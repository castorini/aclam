N19-1000:
  address: Minneapolis, Minnesota
  author:
  - first: Jill
    full: Jill Burstein
    id: jill-burstein
    last: Burstein
  - first: Christy
    full: Christy Doran
    id: christy-doran
    last: Doran
  - first: Thamar
    full: Thamar Solorio
    id: thamar-solorio
    last: Solorio
  author_string: Jill Burstein, Christy Doran, Thamar Solorio
  bibkey: naacl-2019-2019
  bibtype: proceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  month: June
  paper_id: '0'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1000.jpg
  title: 'Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  title_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  url: https://www.aclweb.org/anthology/N19-1000
  year: '2019'
N19-1001:
  abstract: Previous research shows that eye-tracking data contains information about
    the lexical and syntactic properties of text, which can be used to improve natural
    language processing models. In this work, we leverage eye movement features from
    three corpora with recorded gaze information to augment a state-of-the-art neural
    model for named entity recognition (NER) with gaze embeddings. These corpora were
    manually annotated with named entity labels. Moreover, we show how gaze features,
    generalized on word type level, eliminate the need for recorded eye-tracking data
    at test time. The gaze-augmented models for NER using token-level and type-level
    features outperform the baselines. We present the benefits of eye-tracking features
    by evaluating the NER models on both individual datasets as well as in cross-domain
    settings.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/347364761
    type: video
    url: https://vimeo.com/347364761
  author:
  - first: Nora
    full: Nora Hollenstein
    id: nora-hollenstein
    last: Hollenstein
  - first: Ce
    full: Ce Zhang
    id: ce-zhang
    last: Zhang
  author_string: Nora Hollenstein, Ce Zhang
  bibkey: hollenstein-zhang-2019-entity
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1001
  month: June
  page_first: '1'
  page_last: '10'
  pages: "1\u201310"
  paper_id: '1'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1001.jpg
  title: 'Entity Recognition at First Sight: Improving NER with Eye Movement Information'
  title_html: 'Entity Recognition at First Sight: <span class="acl-fixed-case">I</span>mproving
    <span class="acl-fixed-case">NER</span> with Eye Movement Information'
  url: https://www.aclweb.org/anthology/N19-1001
  year: '2019'
N19-1002:
  abstract: "Recent work has shown that LSTMs trained on a generic language modeling\
    \ objective capture syntax-sensitive generalizations such as long-distance number\
    \ agreement. We have however no mechanistic understanding of how they accomplish\
    \ this remarkable feat. Some have conjectured it depends on heuristics that do\
    \ not truly take hierarchical structure into account. We present here a detailed\
    \ study of the inner mechanics of number tracking in LSTMs at the single neuron\
    \ level. We discover that long-distance number information is largely managed\
    \ by two \u201Cnumber units\u201D. Importantly, the behaviour of these units is\
    \ partially controlled by other units independently shown to track syntactic structure.\
    \ We conclude that LSTMs are, to some extent, implementing genuinely syntactic\
    \ processing mechanisms, paving the way to a more general understanding of grammatical\
    \ encoding in LSTMs."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1002.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1002.Supplementary.pdf
  - filename: https://vimeo.com/347368203
    type: video
    url: https://vimeo.com/347368203
  author:
  - first: Yair
    full: Yair Lakretz
    id: yair-lakretz
    last: Lakretz
  - first: German
    full: German Kruszewski
    id: german-kruszewski
    last: Kruszewski
  - first: Theo
    full: Theo Desbordes
    id: theo-desbordes
    last: Desbordes
  - first: Dieuwke
    full: Dieuwke Hupkes
    id: dieuwke-hupkes
    last: Hupkes
  - first: Stanislas
    full: Stanislas Dehaene
    id: stanislas-dehaene
    last: Dehaene
  - first: Marco
    full: Marco Baroni
    id: marco-baroni
    last: Baroni
  author_string: Yair Lakretz, German Kruszewski, Theo Desbordes, Dieuwke Hupkes,
    Stanislas Dehaene, Marco Baroni
  bibkey: lakretz-etal-2019-emergence
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1002
  month: June
  page_first: '11'
  page_last: '20'
  pages: "11\u201320"
  paper_id: '2'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1002.jpg
  title: The emergence of number and syntax units in LSTM language models
  title_html: The emergence of number and syntax units in <span class="acl-fixed-case">LSTM</span>
    language models
  url: https://www.aclweb.org/anthology/N19-1002
  year: '2019'
N19-1003:
  abstract: Self-training is a semi-supervised learning approach for utilizing unlabeled
    data to create better learners. The efficacy of self-training algorithms depends
    on their data sampling techniques. The majority of current sampling techniques
    are based on predetermined policies which may not effectively explore the data
    space or improve model generalizability. In this work, we tackle the above challenges
    by introducing a new data sampling technique based on spaced repetition that dynamically
    samples informative and diverse unlabeled instances with respect to individual
    learner and instance characteristics. The proposed model is specifically effective
    in the context of neural models which can suffer from overfitting and high-variance
    gradients when trained with small amount of labeled data. Our model outperforms
    current semi-supervised learning approaches developed for neural networks on publicly-available
    datasets.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/347373924
    type: video
    url: https://vimeo.com/347373924
  author:
  - first: Hadi
    full: Hadi Amiri
    id: hadi-amiri
    last: Amiri
  author_string: Hadi Amiri
  bibkey: amiri-2019-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1003
  month: June
  page_first: '21'
  page_last: '31'
  pages: "21\u201331"
  paper_id: '3'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1003.jpg
  title: Neural Self-Training through Spaced Repetition
  title_html: Neural Self-Training through Spaced Repetition
  url: https://www.aclweb.org/anthology/N19-1003
  year: '2019'
N19-1004:
  abstract: 'We investigate the extent to which the behavior of neural network language
    models reflects incremental representations of syntactic state. To do so, we employ
    experimental methodologies which were originally developed in the field of psycholinguistics
    to study syntactic representation in the human mind. We examine neural network
    model behavior on sets of artificial sentences containing a variety of syntactically
    complex structures. These sentences not only test whether the networks have a
    representation of syntactic state, they also reveal the specific lexical cues
    that networks use to update these states. We test four models: two publicly available
    LSTM sequence models of English (Jozefowicz et al., 2016; Gulordava et al., 2018)
    trained on large datasets; an RNN Grammar (Dyer et al., 2016) trained on a small,
    parsed dataset; and an LSTM trained on the same small corpus as the RNNG. We find
    evidence for basic syntactic state representations in all models, but only the
    models trained on large datasets are sensitive to subtle lexical cues signaling
    changes in syntactic state.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/347377574
    type: video
    url: https://vimeo.com/347377574
  author:
  - first: Richard
    full: Richard Futrell
    id: richard-futrell
    last: Futrell
  - first: Ethan
    full: Ethan Wilcox
    id: ethan-wilcox
    last: Wilcox
  - first: Takashi
    full: Takashi Morita
    id: takashi-morita
    last: Morita
  - first: Peng
    full: Peng Qian
    id: peng-qian
    last: Qian
  - first: Miguel
    full: Miguel Ballesteros
    id: miguel-ballesteros
    last: Ballesteros
  - first: Roger
    full: Roger Levy
    id: roger-levy
    last: Levy
  author_string: Richard Futrell, Ethan Wilcox, Takashi Morita, Peng Qian, Miguel
    Ballesteros, Roger Levy
  bibkey: futrell-etal-2019-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1004
  month: June
  page_first: '32'
  page_last: '42'
  pages: "32\u201342"
  paper_id: '4'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1004.jpg
  title: 'Neural language models as psycholinguistic subjects: Representations of
    syntactic state'
  title_html: 'Neural language models as psycholinguistic subjects: Representations
    of syntactic state'
  url: https://www.aclweb.org/anthology/N19-1004
  year: '2019'
N19-1005:
  abstract: "Electroencephalography (EEG) recordings of brain activity taken while\
    \ participants read or listen to language are widely used within the cognitive\
    \ neuroscience and psycholinguistics communities as a tool to study language comprehension.\
    \ Several time-locked stereotyped EEG responses to word-presentations \u2013 known\
    \ collectively as event-related potentials (ERPs) \u2013 are thought to be markers\
    \ for semantic or syntactic processes that take place during comprehension. However,\
    \ the characterization of each individual ERP in terms of what features of a stream\
    \ of language trigger the response remains controversial. Improving this characterization\
    \ would make ERPs a more useful tool for studying language comprehension. We take\
    \ a step towards better understanding the ERPs by finetuning a language model\
    \ to predict them. This new approach to analysis shows for the first time that\
    \ all of the ERPs are predictable from embeddings of a stream of language. Prior\
    \ work has only found two of the ERPs to be predictable. In addition to this analysis,\
    \ we examine which ERPs benefit from sharing parameters during joint training.\
    \ We find that two pairs of ERPs previously identified in the literature as being\
    \ related to each other benefit from joint training, while several other pairs\
    \ of ERPs that benefit from joint training are suggestive of potential relationships.\
    \ Extensions of this analysis that further examine what kinds of information in\
    \ the model embeddings relate to each ERP have the potential to elucidate the\
    \ processes involved in human language comprehension."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/347381430
    type: video
    url: https://vimeo.com/347381430
  author:
  - first: Dan
    full: Dan Schwartz
    id: dan-schwartz
    last: Schwartz
  - first: Tom
    full: Tom Mitchell
    id: tom-mitchell
    last: Mitchell
  author_string: Dan Schwartz, Tom Mitchell
  bibkey: schwartz-mitchell-2019-understanding
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1005
  month: June
  page_first: '43'
  page_last: '57'
  pages: "43\u201357"
  paper_id: '5'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1005.jpg
  title: Understanding language-elicited EEG data by predicting it from a fine-tuned
    language model
  title_html: Understanding language-elicited <span class="acl-fixed-case">EEG</span>
    data by predicting it from a fine-tuned language model
  url: https://www.aclweb.org/anthology/N19-1005
  year: '2019'
N19-1006:
  abstract: 'We present a simple approach to improve direct speech-to-text translation
    (ST) when the source language is low-resource: we pre-train the model on a high-resource
    automatic speech recognition (ASR) task, and then fine-tune its parameters for
    ST. We demonstrate that our approach is effective by pre-training on 300 hours
    of English ASR data to improve Spanish English ST from 10.8 to 20.2 BLEU when
    only 20 hours of Spanish-English ST training data are available. Through an ablation
    study, we find that the pre-trained encoder (acoustic model) accounts for most
    of the improvement, despite the fact that the shared language in these tasks is
    the target language text, not the source language audio. Applying this insight,
    we show that pre-training on ASR helps ST even when the ASR language differs from
    both source and target ST languages: pre-training on French ASR also improves
    Spanish-English ST. Finally, we show that the approach improves performance on
    a true low-resource task: pre-training on a combination of English ASR and French
    ASR improves Mboshi-French ST, where only 4 hours of data are available, from
    3.5 to 7.1 BLEU.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1006.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1006.Presentation.pdf
  - filename: https://vimeo.com/353436038
    type: video
    url: https://vimeo.com/353436038
  author:
  - first: Sameer
    full: Sameer Bansal
    id: sameer-bansal
    last: Bansal
  - first: Herman
    full: Herman Kamper
    id: herman-kamper
    last: Kamper
  - first: Karen
    full: Karen Livescu
    id: karen-livescu
    last: Livescu
  - first: Adam
    full: Adam Lopez
    id: adam-lopez
    last: Lopez
  - first: Sharon
    full: Sharon Goldwater
    id: sharon-goldwater
    last: Goldwater
  author_string: Sameer Bansal, Herman Kamper, Karen Livescu, Adam Lopez, Sharon Goldwater
  bibkey: bansal-etal-2019-pre
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1006
  month: June
  page_first: '58'
  page_last: '68'
  pages: "58\u201368"
  paper_id: '6'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1006.jpg
  title: Pre-training on high-resource speech recognition improves low-resource speech-to-text
    translation
  title_html: Pre-training on high-resource speech recognition improves low-resource
    speech-to-text translation
  url: https://www.aclweb.org/anthology/N19-1006
  year: '2019'
N19-1007:
  abstract: In this paper, we deploy binary stochastic neural autoencoder networks
    as models of infant language learning in two typologically unrelated languages
    (Xitsonga and English). We show that the drive to model auditory percepts leads
    to latent clusters that partially align with theory-driven phonemic categories.
    We further evaluate the degree to which theory-driven phonological features are
    encoded in the latent bit patterns, finding that some (e.g. [+-approximant]),
    are well represented by the network in both languages, while others (e.g. [+-spread
    glottis]) are less so. Together, these findings suggest that many reliable cues
    to phonemic structure are immediately available to infants from bottom-up perceptual
    characteristics alone, but that these cues must eventually be supplemented by
    top-down lexical and phonotactic information to achieve adult-like phone discrimination.
    Our results also suggest differences in degree of perceptual availability between
    features, yielding testable predictions as to which features might depend more
    or less heavily on top-down cues during child language acquisition.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/353440477
    type: video
    url: https://vimeo.com/353440477
  author:
  - first: Cory
    full: Cory Shain
    id: cory-shain
    last: Shain
  - first: Micha
    full: Micha Elsner
    id: micha-elsner
    last: Elsner
  author_string: Cory Shain, Micha Elsner
  bibkey: shain-elsner-2019-measuring
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1007
  month: June
  page_first: '69'
  page_last: '85'
  pages: "69\u201385"
  paper_id: '7'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1007.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1007.jpg
  title: Measuring the perceptual availability of phonological features during language
    acquisition using unsupervised binary stochastic autoencoders
  title_html: Measuring the perceptual availability of phonological features during
    language acquisition using unsupervised binary stochastic autoencoders
  url: https://www.aclweb.org/anthology/N19-1007
  year: '2019'
N19-1008:
  abstract: Disfluencies in spontaneous speech are known to be associated with prosodic
    disruptions. However, most algorithms for disfluency detection use only word transcripts.
    Integrating prosodic cues has proved difficult because of the many sources of
    variability affecting the acoustic correlates. This paper introduces a new approach
    to extracting acoustic-prosodic cues using text-based distributional prediction
    of acoustic cues to derive vector z-score features (innovations). We explore both
    early and late fusion techniques for integrating text and prosody, showing gains
    over a high-accuracy text-only model.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/353444632
    type: video
    url: https://vimeo.com/353444632
  author:
  - first: Vicky
    full: Vicky Zayats
    id: vicky-zayats
    last: Zayats
  - first: Mari
    full: Mari Ostendorf
    id: mari-ostendorf
    last: Ostendorf
  author_string: Vicky Zayats, Mari Ostendorf
  bibkey: zayats-ostendorf-2019-giving
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1008
  month: June
  page_first: '86'
  page_last: '95'
  pages: "86\u201395"
  paper_id: '8'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1008.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1008.jpg
  title: 'Giving Attention to the Unexpected: Using Prosody Innovations in Disfluency
    Detection'
  title_html: 'Giving Attention to the Unexpected: Using Prosody Innovations in Disfluency
    Detection'
  url: https://www.aclweb.org/anthology/N19-1008
  year: '2019'
N19-1009:
  abstract: 'We report on adaptation of multilingual end-to-end speech recognition
    models trained on as many as 100 languages. Our findings shed light on the relative
    importance of similarity between the target and pretraining languages along the
    dimensions of phonetics, phonology, language family, geographical location, and
    orthography. In this context, experiments demonstrate the effectiveness of two
    additional pretraining objectives in encouraging language-independent encoder
    representations: a context-independent phoneme objective paired with a language-adversarial
    classification objective.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/353450338
    type: video
    url: https://vimeo.com/353450338
  author:
  - first: Oliver
    full: Oliver Adams
    id: oliver-adams
    last: Adams
  - first: Matthew
    full: Matthew Wiesner
    id: matthew-wiesner
    last: Wiesner
  - first: Shinji
    full: Shinji Watanabe
    id: shinji-watanabe
    last: Watanabe
  - first: David
    full: David Yarowsky
    id: david-yarowsky
    last: Yarowsky
  author_string: Oliver Adams, Matthew Wiesner, Shinji Watanabe, David Yarowsky
  bibkey: adams-etal-2019-massively
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1009
  month: June
  page_first: '96'
  page_last: '108'
  pages: "96\u2013108"
  paper_id: '9'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1009.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1009.jpg
  title: Massively Multilingual Adversarial Speech Recognition
  title_html: Massively Multilingual Adversarial Speech Recognition
  url: https://www.aclweb.org/anthology/N19-1009
  year: '2019'
N19-1010:
  abstract: Simultaneous interpretation, the translation of speech from one language
    to another in real-time, is an inherently difficult and strenuous task. One of
    the greatest challenges faced by interpreters is the accurate translation of difficult
    terminology like proper names, numbers, or other entities. Intelligent computer-assisted
    interpreting (CAI) tools that could analyze the spoken word and detect terms likely
    to be untranslated by an interpreter could reduce translation error and improve
    interpreter performance. In this paper, we propose a task of predicting which
    terminology simultaneous interpreters will leave untranslated, and examine methods
    that perform this task using supervised sequence taggers. We describe a number
    of task-specific features explicitly designed to indicate when an interpreter
    may struggle with translating a word. Experimental results on a newly-annotated
    version of the NAIST Simultaneous Translation Corpus (Shimizu et al., 2014) indicate
    the promise of our proposed method.
  address: Minneapolis, Minnesota
  author:
  - first: Nikolai
    full: Nikolai Vogler
    id: nikolai-vogler
    last: Vogler
  - first: Craig
    full: Craig Stewart
    id: craig-stewart
    last: Stewart
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  author_string: Nikolai Vogler, Craig Stewart, Graham Neubig
  bibkey: vogler-etal-2019-lost
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1010
  month: June
  page_first: '109'
  page_last: '118'
  pages: "109\u2013118"
  paper_id: '10'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1010.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1010.jpg
  title: 'Lost in Interpretation: Predicting Untranslated Terminology in Simultaneous
    Interpretation'
  title_html: '<span class="acl-fixed-case">L</span>ost in <span class="acl-fixed-case">I</span>nterpretation:
    <span class="acl-fixed-case">P</span>redicting <span class="acl-fixed-case">U</span>ntranslated
    <span class="acl-fixed-case">T</span>erminology in <span class="acl-fixed-case">S</span>imultaneous
    <span class="acl-fixed-case">I</span>nterpretation'
  url: https://www.aclweb.org/anthology/N19-1010
  year: '2019'
N19-1011:
  abstract: 'We explore the problem of Audio Captioning: generating natural language
    description for any kind of audio in the wild, which has been surprisingly unexplored
    in previous research. We contribute a large-scale dataset of 46K audio clips with
    human-written text pairs collected via crowdsourcing on the AudioSet dataset.
    Our thorough empirical studies not only show that our collected captions are indeed
    faithful to audio inputs but also discover what forms of audio representation
    and captioning models are effective for the audio captioning. From extensive experiments,
    we also propose two novel components that help improve audio captioning performance:
    the top-down multi-scale encoder and aligned semantic attention.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/353410416
    type: video
    url: https://vimeo.com/353410416
  author:
  - first: Chris Dongjoo
    full: Chris Dongjoo Kim
    id: chris-dongjoo-kim
    last: Kim
  - first: Byeongchang
    full: Byeongchang Kim
    id: byeongchang-kim
    last: Kim
  - first: Hyunmin
    full: Hyunmin Lee
    id: hyunmin-lee
    last: Lee
  - first: Gunhee
    full: Gunhee Kim
    id: gunhee-kim
    last: Kim
  author_string: Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, Gunhee Kim
  bibkey: kim-etal-2019-audiocaps
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1011
  month: June
  page_first: '119'
  page_last: '132'
  pages: "119\u2013132"
  paper_id: '11'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1011.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1011.jpg
  title: 'AudioCaps: Generating Captions for Audios in The Wild'
  title_html: '<span class="acl-fixed-case">A</span>udio<span class="acl-fixed-case">C</span>aps:
    Generating Captions for Audios in The Wild'
  url: https://www.aclweb.org/anthology/N19-1011
  year: '2019'
N19-1012:
  abstract: We introduce, release, and analyze a new dataset, called Humicroedit,
    for research in computational humor. Our publicly available data consists of regular
    English news headlines paired with versions of the same headlines that contain
    simple replacement edits designed to make them funny. We carefully curated crowdsourced
    editors to create funny headlines and judges to score a to a total of 15,095 edited
    headlines, with five judges per headline. The simple edits, usually just a single
    word replacement, mean we can apply straightforward analysis techniques to determine
    what makes our edited headlines humorous. We show how the data support classic
    theories of humor, such as incongruity, superiority, and setup/punchline. Finally,
    we develop baseline classifiers that can predict whether or not an edited headline
    is funny, which is a first step toward automatically generating humorous headlines
    as an approach to creating topical humor.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/353414280
    type: video
    url: https://vimeo.com/353414280
  author:
  - first: Nabil
    full: Nabil Hossain
    id: nabil-hossain
    last: Hossain
  - first: John
    full: John Krumm
    id: john-krumm
    last: Krumm
  - first: Michael
    full: Michael Gamon
    id: michael-gamon
    last: Gamon
  author_string: Nabil Hossain, John Krumm, Michael Gamon
  bibkey: hossain-etal-2019-president
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1012
  month: June
  page_first: '133'
  page_last: '142'
  pages: "133\u2013142"
  paper_id: '12'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1012.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1012.jpg
  title: "\u201CPresident Vows to Cut <Taxes> Hair\u201D: Dataset and Analysis of\
    \ Creative Text Editing for Humorous Headlines"
  title_html: "\u201CPresident Vows to Cut &lt;Taxes&gt; Hair\u201D: Dataset and Analysis\
    \ of Creative Text Editing for Humorous Headlines"
  url: https://www.aclweb.org/anthology/N19-1012
  year: '2019'
N19-1013:
  abstract: We present an approach for generating clarification questions with the
    goal of eliciting new information that would make the given textual context more
    complete. We propose that modeling hypothetical answers (to clarification questions)
    as latent variables can guide our approach into generating more useful clarification
    questions. We develop a Generative Adversarial Network (GAN) where the generator
    is a sequence-to-sequence model and the discriminator is a utility function that
    models the value of updating the context with the answer to the clarification
    question. We evaluate on two datasets, using both automatic metrics and human
    judgments of usefulness, specificity and relevance, showing that our approach
    outperforms both a retrieval-based model and ablations that exclude the utility
    model and the adversarial training.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1013.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1013.Supplementary.pdf
  - filename: https://vimeo.com/353418933
    type: video
    url: https://vimeo.com/353418933
  author:
  - first: Sudha
    full: Sudha Rao
    id: sudha-rao
    last: Rao
  - first: Hal
    full: "Hal Daum\xE9 III"
    id: hal-daume-iii
    last: "Daum\xE9 III"
  author_string: "Sudha Rao, Hal Daum\xE9 III"
  bibkey: rao-daume-iii-2019-answer
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1013
  month: June
  page_first: '143'
  page_last: '155'
  pages: "143\u2013155"
  paper_id: '13'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1013.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1013.jpg
  title: Answer-based Adversarial Training for Generating Clarification Questions
  title_html: <span class="acl-fixed-case">A</span>nswer-based <span class="acl-fixed-case">A</span>dversarial
    <span class="acl-fixed-case">T</span>raining for <span class="acl-fixed-case">G</span>enerating
    <span class="acl-fixed-case">C</span>larification <span class="acl-fixed-case">Q</span>uestions
  url: https://www.aclweb.org/anthology/N19-1013
  year: '2019'
N19-1014:
  abstract: Neural machine translation systems have become state-of-the-art approaches
    for Grammatical Error Correction (GEC) task. In this paper, we propose a copy-augmented
    architecture for the GEC task by copying the unchanged words from the source sentence
    to the target sentence. Since the GEC suffers from not having enough labeled training
    data to achieve high accuracy. We pre-train the copy-augmented architecture with
    a denoising auto-encoder using the unlabeled One Billion Benchmark and make comparisons
    between the fully pre-trained model and a partially pre-trained model. It is the
    first time copying words from the source context and fully pre-training a sequence
    to sequence model are experimented on the GEC task. Moreover, We add token-level
    and sentence-level multi-task learning for the GEC task. The evaluation results
    on the CoNLL-2014 test set show that our approach outperforms all recently published
    state-of-the-art results by a large margin.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/353425373
    type: video
    url: https://vimeo.com/353425373
  author:
  - first: Wei
    full: Wei Zhao
    id: wei-zhao
    last: Zhao
  - first: Liang
    full: Liang Wang
    id: liang-wang
    last: Wang
  - first: Kewei
    full: Kewei Shen
    id: kewei-shen
    last: Shen
  - first: Ruoyu
    full: Ruoyu Jia
    id: ruoyu-jia
    last: Jia
  - first: Jingming
    full: Jingming Liu
    id: jingming-liu
    last: Liu
  author_string: Wei Zhao, Liang Wang, Kewei Shen, Ruoyu Jia, Jingming Liu
  bibkey: zhao-etal-2019-improving
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1014
  month: June
  page_first: '156'
  page_last: '165'
  pages: "156\u2013165"
  paper_id: '14'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1014.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1014.jpg
  title: Improving Grammatical Error Correction via Pre-Training a Copy-Augmented
    Architecture with Unlabeled Data
  title_html: Improving Grammatical Error Correction via Pre-Training a Copy-Augmented
    Architecture with Unlabeled Data
  url: https://www.aclweb.org/anthology/N19-1014
  year: '2019'
N19-1015:
  abstract: We propose a topic-guided variational auto-encoder (TGVAE) model for text
    generation. Distinct from existing variational auto-encoder (VAE) based approaches,
    which assume a simple Gaussian prior for latent code, our model specifies the
    prior as a Gaussian mixture model (GMM) parametrized by a neural topic module.
    Each mixture component corresponds to a latent topic, which provides a guidance
    to generate sentences under the topic. The neural topic module and the VAE-based
    neural sequence module in our model are learned jointly. In particular, a sequence
    of invertible Householder transformations is applied to endow the approximate
    posterior of the latent code with high flexibility during the model inference.
    Experimental results show that our TGVAE outperforms its competitors on both unconditional
    and conditional text generation, which can also generate semantically-meaningful
    sentences with various topics.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/353433493
    type: video
    url: https://vimeo.com/353433493
  author:
  - first: Wenlin
    full: Wenlin Wang
    id: wenlin-wang
    last: Wang
  - first: Zhe
    full: Zhe Gan
    id: zhe-gan
    last: Gan
  - first: Hongteng
    full: Hongteng Xu
    id: hongteng-xu
    last: Xu
  - first: Ruiyi
    full: Ruiyi Zhang
    id: ruiyi-zhang
    last: Zhang
  - first: Guoyin
    full: Guoyin Wang
    id: guoyin-wang
    last: Wang
  - first: Dinghan
    full: Dinghan Shen
    id: dinghan-shen
    last: Shen
  - first: Changyou
    full: Changyou Chen
    id: changyou-chen
    last: Chen
  - first: Lawrence
    full: Lawrence Carin
    id: lawrence-carin
    last: Carin
  author_string: Wenlin Wang, Zhe Gan, Hongteng Xu, Ruiyi Zhang, Guoyin Wang, Dinghan
    Shen, Changyou Chen, Lawrence Carin
  bibkey: wang-etal-2019-topic-guided
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1015
  month: June
  page_first: '166'
  page_last: '177'
  pages: "166\u2013177"
  paper_id: '15'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1015.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1015.jpg
  title: Topic-Guided Variational Auto-Encoder for Text Generation
  title_html: Topic-Guided Variational Auto-Encoder for Text Generation
  url: https://www.aclweb.org/anthology/N19-1015
  year: '2019'
N19-1016:
  abstract: "Constituent parsing has been studied extensively in the last decades.\
    \ Chomsky-Sch\xFCtzenberger parsing as an approach to constituent parsing has\
    \ only been investigated theoretically, yet. It uses the decomposition of a language\
    \ into a regular language, a homomorphism, and a bracket language to divide the\
    \ parsing problem into simpler subproblems. We provide the first implementation\
    \ of Chomsky-Sch\xFCtzenberger parsing. It employs multiple context-free grammars\
    \ and incorporates many refinements to achieve feasibility. We compare its performance\
    \ to state-of-the-art grammar-based parsers."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/360467719
    type: video
    url: https://vimeo.com/360467719
  author:
  - first: Thomas
    full: Thomas Ruprecht
    id: thomas-ruprecht
    last: Ruprecht
  - first: Tobias
    full: Tobias Denkinger
    id: tobias-denkinger
    last: Denkinger
  author_string: Thomas Ruprecht, Tobias Denkinger
  bibkey: ruprecht-denkinger-2019-implementation
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1016
  month: June
  page_first: '178'
  page_last: '191'
  pages: "178\u2013191"
  paper_id: '16'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1016.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1016.jpg
  title: "Implementation of a Chomsky-Sch\xFCtzenberger n-best parser for weighted\
    \ multiple context-free grammars"
  title_html: "Implementation of a <span class=\"acl-fixed-case\">C</span>homsky-Sch\xFC\
    tzenberger n-best parser for weighted multiple context-free grammars"
  url: https://www.aclweb.org/anthology/N19-1016
  year: '2019'
N19-1017:
  abstract: Languages evolve and diverge over time. Their evolutionary history is
    often depicted in the shape of a phylogenetic tree. Assuming parsing models are
    representations of their languages grammars, their evolution should follow a structure
    similar to that of the phylogenetic tree. In this paper, drawing inspiration from
    multi-task learning, we make use of the phylogenetic tree to guide the learning
    of multi-lingual dependency parsers leveraging languages structural similarities.
    Experiments on data from the Universal Dependency project show that phylogenetic
    training is beneficial to low resourced languages and to well furnished languages
    families. As a side product of phylogenetic training, our model is able to perform
    zero-shot parsing of previously unseen languages.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/360481999
    type: video
    url: https://vimeo.com/360481999
  author:
  - first: Mathieu
    full: Mathieu Dehouck
    id: mathieu-dehouck
    last: Dehouck
  - first: Pascal
    full: Pascal Denis
    id: pascal-denis
    last: Denis
  author_string: Mathieu Dehouck, Pascal Denis
  bibkey: dehouck-denis-2019-phylogenic
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1017
  month: June
  page_first: '192'
  page_last: '203'
  pages: "192\u2013203"
  paper_id: '17'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1017.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1017.jpg
  title: Phylogenic Multi-Lingual Dependency Parsing
  title_html: Phylogenic Multi-Lingual Dependency Parsing
  url: https://www.aclweb.org/anthology/N19-1017
  year: '2019'
N19-1018:
  abstract: "We introduce a novel transition system for discontinuous constituency\
    \ parsing. Instead of storing subtrees in a stack \u2013i.e. a data structure\
    \ with linear-time sequential access\u2013 the proposed system uses a set of parsing\
    \ items, with constant-time random access. This change makes it possible to construct\
    \ any discontinuous constituency tree in exactly 4n\u20132 transitions for a sentence\
    \ of length n. At each parsing step, the parser considers every item in the set\
    \ to be combined with a focus item and to construct a new constituent in a bottom-up\
    \ fashion. The parsing strategy is based on the assumption that most syntactic\
    \ structures can be parsed incrementally and that the set \u2013the memory of\
    \ the parser\u2013 remains reasonably small on average. Moreover, we introduce\
    \ a provably correct dynamic oracle for the new transition system, and present\
    \ the first experiments in discontinuous constituency parsing using a dynamic\
    \ oracle. Our parser obtains state-of-the-art results on three English and German\
    \ discontinuous treebanks. transitions for a sentence of length n. At each parsing\
    \ step, the parser considers every item in the set to be combined with a focus\
    \ item and to construct a new constituent in a bottom-up fashion. The parsing\
    \ strategy is based on the assumption that most syntactic structures can be parsed\
    \ incrementally and that the set \u2013the memory of the parser\u2013 remains\
    \ reasonably small on average. Moreover, we introduce a provably correct dynamic\
    \ oracle for the new transition system, and present the first experiments in discontinuous\
    \ constituency parsing using a dynamic oracle. Our parser obtains state-of-the-art\
    \ results on three English and German discontinuous treebanks."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/360494509
    type: video
    url: https://vimeo.com/360494509
  author:
  - first: Maximin
    full: Maximin Coavoux
    id: maximin-coavoux
    last: Coavoux
  - first: Shay B.
    full: Shay B. Cohen
    id: shay-b-cohen
    last: Cohen
  author_string: Maximin Coavoux, Shay B. Cohen
  bibkey: coavoux-cohen-2019-discontinuous
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1018
  month: June
  page_first: '204'
  page_last: '217'
  pages: "204\u2013217"
  paper_id: '18'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1018.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1018.jpg
  title: Discontinuous Constituency Parsing with a Stack-Free Transition System and
    a Dynamic Oracle
  title_html: Discontinuous Constituency Parsing with a Stack-Free Transition System
    and a Dynamic Oracle
  url: https://www.aclweb.org/anthology/N19-1018
  year: '2019'
N19-1019:
  abstract: The performance of Part-of-Speech tagging varies significantly across
    the treebanks of the Universal Dependencies project. This work points out that
    these variations may result from divergences between the annotation of train and
    test sets. We show how the annotation variation principle, introduced by Dickinson
    and Meurers (2003) to automatically detect errors in gold standard, can be used
    to identify inconsistencies between annotations; we also evaluate their impact
    on prediction performance.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/360505435
    type: video
    url: https://vimeo.com/360505435
  author:
  - first: Guillaume
    full: Guillaume Wisniewski
    id: guillaume-wisniewski
    last: Wisniewski
  - first: "Fran\xE7ois"
    full: "Fran\xE7ois Yvon"
    id: francois-yvon
    last: Yvon
  author_string: "Guillaume Wisniewski, Fran\xE7ois Yvon"
  bibkey: wisniewski-yvon-2019-bad
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1019
  month: June
  page_first: '218'
  page_last: '227'
  pages: "218\u2013227"
  paper_id: '19'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1019.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1019.jpg
  title: How Bad are PoS Tagger in Cross-Corpora Settings? Evaluating Annotation Divergence
    in the UD Project.
  title_html: <span class="acl-fixed-case">H</span>ow <span class="acl-fixed-case">B</span>ad
    are <span class="acl-fixed-case">P</span>o<span class="acl-fixed-case">S</span>
    <span class="acl-fixed-case">T</span>agger in <span class="acl-fixed-case">C</span>ross-<span
    class="acl-fixed-case">C</span>orpora <span class="acl-fixed-case">S</span>ettings?
    <span class="acl-fixed-case">E</span>valuating <span class="acl-fixed-case">A</span>nnotation
    <span class="acl-fixed-case">D</span>ivergence in the <span class="acl-fixed-case">UD</span>
    <span class="acl-fixed-case">P</span>roject.
  url: https://www.aclweb.org/anthology/N19-1019
  year: '2019'
N19-1020:
  abstract: The main obstacle to incremental sentence processing arises from right-branching
    constituent structures, which are present in the majority of English sentences,
    as well as optional constituents that adjoin on the right, such as right adjuncts
    and right conjuncts. In CCG, many right-branching derivations can be replaced
    by semantically equivalent left-branching incremental derivations. The problem
    of right-adjunction is more resistant to solution, and has been tackled in the
    past using revealing-based approaches that often rely either on the higher-order
    unification over lambda terms (Pareschi and Steedman,1987) or heuristics over
    dependency representations that do not cover the whole CCGbank (Ambati et al.,
    2015). We propose a new incremental parsing algorithm for CCG following the same
    revealing tradition of work but having a purely syntactic approach that does not
    depend on access to a distinct level of semantic representation. This algorithm
    can cover the whole CCGbank, with greater incrementality and accuracy than previous
    proposals.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/360516550
    type: video
    url: https://vimeo.com/360516550
  author:
  - first: "Milo\u0161"
    full: "Milo\u0161 Stanojevi\u0107"
    id: milos-stanojevic
    last: "Stanojevi\u0107"
  - first: Mark
    full: Mark Steedman
    id: mark-steedman
    last: Steedman
  author_string: "Milo\u0161 Stanojevi\u0107, Mark Steedman"
  bibkey: stanojevic-steedman-2019-ccg
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1020
  month: June
  page_first: '228'
  page_last: '239'
  pages: "228\u2013239"
  paper_id: '20'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1020.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1020.jpg
  title: CCG Parsing Algorithm with Incremental Tree Rotation
  title_html: <span class="acl-fixed-case">CCG</span> Parsing Algorithm with Incremental
    Tree Rotation
  url: https://www.aclweb.org/anthology/N19-1020
  year: '2019'
N19-1021:
  abstract: "Variational autoencoders (VAE) with an auto-regressive decoder have been\
    \ applied for many natural language processing (NLP) tasks. VAE objective consists\
    \ of two terms, the KL regularization term and the reconstruction term, balanced\
    \ by a weighting hyper-parameter \U0001D6FD. One notorious training difficulty\
    \ is that the KL term tends to vanish. In this paper we study different scheduling\
    \ schemes for . One notorious training difficulty is that the KL term tends to\
    \ vanish. In this paper we study different scheduling schemes for \U0001D6FD,\
    \ and show that KL vanishing is caused by the lack of good latent codes in training\
    \ decoder at the beginning of optimization. To remedy the issue, we propose a\
    \ cyclical annealing schedule, which simply repeats the process of increasing\
    \ , and show that KL vanishing is caused by the lack of good latent codes in training\
    \ decoder at the beginning of optimization. To remedy the issue, we propose a\
    \ cyclical annealing schedule, which simply repeats the process of increasing\
    \ \U0001D6FD multiple times. This new procedure allows us to learn more meaningful\
    \ latent codes progressively by leveraging the results of previous learning cycles\
    \ as warm re-restart. The effectiveness of cyclical annealing schedule is validated\
    \ on a broad range of NLP tasks, including language modeling, dialog response\
    \ generation and semi-supervised text classification. multiple times. This new\
    \ procedure allows us to learn more meaningful latent codes progressively by leveraging\
    \ the results of previous learning cycles as warm re-restart. The effectiveness\
    \ of cyclical annealing schedule is validated on a broad range of NLP tasks, including\
    \ language modeling, dialog response generation and semi-supervised text classification."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1021.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1021.Supplementary.pdf
  author:
  - first: Hao
    full: Hao Fu
    id: hao-fu
    last: Fu
  - first: Chunyuan
    full: Chunyuan Li
    id: chunyuan-li
    last: Li
  - first: Xiaodong
    full: Xiaodong Liu
    id: xiaodong-liu
    last: Liu
  - first: Jianfeng
    full: Jianfeng Gao
    id: jianfeng-gao
    last: Gao
  - first: Asli
    full: Asli Celikyilmaz
    id: asli-celikyilmaz
    last: Celikyilmaz
  - first: Lawrence
    full: Lawrence Carin
    id: lawrence-carin
    last: Carin
  author_string: Hao Fu, Chunyuan Li, Xiaodong Liu, Jianfeng Gao, Asli Celikyilmaz,
    Lawrence Carin
  bibkey: fu-etal-2019-cyclical
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1021
  month: June
  page_first: '240'
  page_last: '250'
  pages: "240\u2013250"
  paper_id: '21'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1021.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1021.jpg
  title: 'Cyclical Annealing Schedule: A Simple Approach to Mitigating KL Vanishing'
  title_html: 'Cyclical Annealing Schedule: A Simple Approach to Mitigating <span
    class="acl-fixed-case">KL</span> Vanishing'
  url: https://www.aclweb.org/anthology/N19-1021
  year: '2019'
N19-1022:
  abstract: The current state-of-the-art in neural graph-based parsing uses only approximate
    decoding at the training phase. In this paper aim to understand this result better.
    We show how recurrent models can carry out projective maximum spanning tree decoding.
    This result holds for both current state-of-the-art models for shift-reduce and
    graph-based parsers, projective or not. We also provide the first proof on the
    lower bounds of projective maximum spanning tree decoding.
  address: Minneapolis, Minnesota
  author:
  - first: Natalie
    full: Natalie Schluter
    id: natalie-schluter
    last: Schluter
  author_string: Natalie Schluter
  bibkey: schluter-2019-recurrent
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1022
  month: June
  page_first: '251'
  page_last: '260'
  pages: "251\u2013260"
  paper_id: '22'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1022.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1022.jpg
  title: Recurrent models and lower bounds for projective syntactic decoding
  title_html: Recurrent models and lower bounds for projective syntactic decoding
  url: https://www.aclweb.org/anthology/N19-1022
  year: '2019'
N19-1023:
  abstract: "Ellipsis is a natural language phenomenon where part of a sentence is\
    \ missing and its information must be recovered from its surrounding context,\
    \ as in \u201CCats chase dogs and so do foxes.\u201D. Formal semantics has different\
    \ methods for resolving ellipsis and recovering the missing information, but the\
    \ problem has not been considered for distributional semantics, where words have\
    \ vector embeddings and combinations thereof provide embeddings for sentences.\
    \ In elliptical sentences these combinations go beyond linear as copying of elided\
    \ information is necessary. In this paper, we develop different models for embedding\
    \ VP-elliptical sentences. We extend existing verb disambiguation and sentence\
    \ similarity datasets to ones containing elliptical phrases and evaluate our models\
    \ on these datasets for a variety of non-linear combinations and their linear\
    \ counterparts. We compare results of these compositional models to state of the\
    \ art holistic sentence encoders. Our results show that non-linear addition and\
    \ a non-linear tensor-based composition outperform the naive non-compositional\
    \ baselines and the linear models, and that sentence encoders perform well on\
    \ sentence similarity, but not on verb disambiguation."
  address: Minneapolis, Minnesota
  author:
  - first: Gijs
    full: Gijs Wijnholds
    id: gijs-wijnholds
    last: Wijnholds
  - first: Mehrnoosh
    full: Mehrnoosh Sadrzadeh
    id: mehrnoosh-sadrzadeh
    last: Sadrzadeh
  author_string: Gijs Wijnholds, Mehrnoosh Sadrzadeh
  bibkey: wijnholds-sadrzadeh-2019-evaluating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1023
  month: June
  page_first: '261'
  page_last: '271'
  pages: "261\u2013271"
  paper_id: '23'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1023.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1023.jpg
  title: Evaluating Composition Models for Verb Phrase Elliptical Sentence Embeddings
  title_html: Evaluating Composition Models for Verb Phrase Elliptical Sentence Embeddings
  url: https://www.aclweb.org/anthology/N19-1023
  year: '2019'
N19-1024:
  abstract: We introduce neural finite state transducers (NFSTs), a family of string
    transduction models defining joint and conditional probability distributions over
    pairs of strings. The probability of a string pair is obtained by marginalizing
    over all its accepting paths in a finite state transducer. In contrast to ordinary
    weighted FSTs, however, each path is scored using an arbitrary function such as
    a recurrent neural network, which breaks the usual conditional independence assumption
    (Markov property). NFSTs are more powerful than previous finite-state models with
    neural features (Rastogi et al., 2016.) We present training and inference algorithms
    for locally and globally normalized variants of NFSTs. In experiments on different
    transduction tasks, they compete favorably against seq2seq models while offering
    interpretable paths that correspond to hard monotonic alignments.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1024.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1024.Supplementary.pdf
  author:
  - first: Chu-Cheng
    full: Chu-Cheng Lin
    id: chu-cheng-lin
    last: Lin
  - first: Hao
    full: Hao Zhu
    id: hao-zhu
    last: Zhu
  - first: Matthew R.
    full: Matthew R. Gormley
    id: matthew-r-gormley
    last: Gormley
  - first: Jason
    full: Jason Eisner
    id: jason-eisner
    last: Eisner
  author_string: Chu-Cheng Lin, Hao Zhu, Matthew R. Gormley, Jason Eisner
  bibkey: lin-etal-2019-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1024
  month: June
  page_first: '272'
  page_last: '283'
  pages: "272\u2013283"
  paper_id: '24'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1024.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1024.jpg
  title: 'Neural Finite-State Transducers: Beyond Rational Relations'
  title_html: 'Neural Finite-State Transducers: Beyond Rational Relations'
  url: https://www.aclweb.org/anthology/N19-1024
  year: '2019'
N19-1025:
  abstract: Recurrent Variational Autoencoder has been widely used for language modeling
    and text generation tasks. These models often face a difficult optimization problem,
    also known as KL vanishing, where the posterior easily collapses to the prior
    and model will ignore latent codes in generative tasks. To address this problem,
    we introduce an improved Variational Wasserstein Autoencoder (WAE) with Riemannian
    Normalizing Flow (RNF) for text modeling. The RNF transforms a latent variable
    into a space that respects the geometric characteristics of input space, which
    makes posterior impossible to collapse to the non-informative prior. The Wasserstein
    objective minimizes the distance between marginal distribution and the prior directly
    and therefore does not force the posterior to match the prior. Empirical experiments
    show that our model avoids KL vanishing over a range of datasets and has better
    performance in tasks such as language modeling, likelihood approximation, and
    text generation. Through a series of experiments and analysis over latent space,
    we show that our model learns latent distributions that respect latent space geometry
    and is able to generate sentences that are more diverse.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1025.Software.tar
    type: software
    url: https://www.aclweb.org/anthology/attachments/N19-1025.Software.tar
  - filename: N19-1025.Presentation.pptx
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1025.Presentation.pptx
  author:
  - first: Prince Zizhuang
    full: Prince Zizhuang Wang
    id: prince-zizhuang-wang
    last: Wang
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  author_string: Prince Zizhuang Wang, William Yang Wang
  bibkey: wang-wang-2019-riemannian
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1025
  month: June
  page_first: '284'
  page_last: '294'
  pages: "284\u2013294"
  paper_id: '25'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1025.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1025.jpg
  title: Riemannian Normalizing Flow on Variational Wasserstein Autoencoder for Text
    Modeling
  title_html: <span class="acl-fixed-case">R</span>iemannian Normalizing Flow on Variational
    <span class="acl-fixed-case">W</span>asserstein Autoencoder for Text Modeling
  url: https://www.aclweb.org/anthology/N19-1025
  year: '2019'
N19-1026:
  abstract: Developing bots demands highquality training samples, typically in the
    form of user utterances and their associated intents. Given the fuzzy nature of
    human language, such datasets ideally must cover all possible utterances of each
    single intent. Crowdsourcing has widely been used to collect such inclusive datasets
    by paraphrasing an initial utterance. However, the quality of this approach often
    suffers from various issues, particularly language errors produced by unqualified
    crowd workers. More so, since workers are tasked to write open-ended text, it
    is very challenging to automatically asses the quality of paraphrased utterances.
    In this paper, we investigate common crowdsourced paraphrasing issues, and propose
    an annotated dataset called Para-Quality, for detecting the quality issues. We
    also investigate existing tools and services to provide baselines for detecting
    each category of issues. In all, this work presents a data-driven view of incorrect
    paraphrases during the bot development process, and we pave the way towards automatic
    detection of unqualified paraphrases.
  address: Minneapolis, Minnesota
  author:
  - first: Mohammad-Ali
    full: Mohammad-Ali Yaghoub-Zadeh-Fard
    id: mohammad-ali-yaghoub-zadeh-fard
    last: Yaghoub-Zadeh-Fard
  - first: Boualem
    full: Boualem Benatallah
    id: boualem-benatallah
    last: Benatallah
  - first: Moshe
    full: Moshe Chai Barukh
    id: moshe-chai-barukh
    last: Chai Barukh
  - first: Shayan
    full: Shayan Zamanirad
    id: shayan-zamanirad
    last: Zamanirad
  author_string: Mohammad-Ali Yaghoub-Zadeh-Fard, Boualem Benatallah, Moshe Chai Barukh,
    Shayan Zamanirad
  bibkey: yaghoub-zadeh-fard-etal-2019-study
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1026
  month: June
  page_first: '295'
  page_last: '306'
  pages: "295\u2013306"
  paper_id: '26'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1026.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1026.jpg
  title: A Study of Incorrect Paraphrases in Crowdsourced User Utterances
  title_html: A Study of Incorrect Paraphrases in Crowdsourced User Utterances
  url: https://www.aclweb.org/anthology/N19-1026
  year: '2019'
N19-1027:
  abstract: To bridge the gap between the capabilities of the state-of-the-art in
    factoid question answering (QA) and what users ask, we need large datasets of
    real user questions that capture the various question phenomena users are interested
    in, and the diverse ways in which these questions are formulated. We introduce
    ComQA, a large dataset of real user questions that exhibit different challenging
    aspects such as compositionality, temporal reasoning, and comparisons. ComQA questions
    come from the WikiAnswers community QA platform, which typically contains questions
    that are not satisfactorily answerable by existing search engine technology. Through
    a large crowdsourcing effort, we clean the question dataset, group questions into
    paraphrase clusters, and annotate clusters with their answers. ComQA contains
    11,214 questions grouped into 4,834 paraphrase clusters. We detail the process
    of constructing ComQA, including the measures taken to ensure its high quality
    while making effective use of crowdsourcing. We also present an extensive analysis
    of the dataset and the results achieved by state-of-the-art systems on ComQA,
    demonstrating that our dataset can be a driver of future research on QA.
  address: Minneapolis, Minnesota
  author:
  - first: Abdalghani
    full: Abdalghani Abujabal
    id: abdalghani-abujabal
    last: Abujabal
  - first: Rishiraj
    full: Rishiraj Saha Roy
    id: rishiraj-saha-roy
    last: Saha Roy
  - first: Mohamed
    full: Mohamed Yahya
    id: mohamed-yahya
    last: Yahya
  - first: Gerhard
    full: Gerhard Weikum
    id: gerhard-weikum
    last: Weikum
  author_string: Abdalghani Abujabal, Rishiraj Saha Roy, Mohamed Yahya, Gerhard Weikum
  bibkey: abujabal-etal-2019-comqa
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1027
  month: June
  page_first: '307'
  page_last: '317'
  pages: "307\u2013317"
  paper_id: '27'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1027.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1027.jpg
  title: 'ComQA: A Community-sourced Dataset for Complex Factoid Question Answering
    with Paraphrase Clusters'
  title_html: '<span class="acl-fixed-case">C</span>om<span class="acl-fixed-case">QA</span>:
    A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase
    Clusters'
  url: https://www.aclweb.org/anthology/N19-1027
  year: '2019'
N19-1028:
  abstract: In this paper, we present a new data set, named FreebaseQA, for open-domain
    factoid question answering (QA) tasks over structured knowledge bases, like Freebase.
    The data set is generated by matching trivia-type question-answer pairs with subject-predicate-object
    triples in Freebase. For each collected question-answer pair, we first tag all
    entities in each question and search for relevant predicates that bridge a tagged
    entity with the answer in Freebase. Finally, human annotation is used to remove
    any false positive in these matched triples. Using this method, we are able to
    efficiently generate over 54K matches from about 28K unique questions with minimal
    cost. Our analysis shows that this data set is suitable for model training in
    factoid QA tasks beyond simpler questions since FreebaseQA provides more linguistically
    sophisticated questions than other existing data sets.
  address: Minneapolis, Minnesota
  author:
  - first: Kelvin
    full: Kelvin Jiang
    id: kelvin-jiang
    last: Jiang
  - first: Dekun
    full: Dekun Wu
    id: dekun-wu
    last: Wu
  - first: Hui
    full: Hui Jiang
    id: hui-jiang
    last: Jiang
  author_string: Kelvin Jiang, Dekun Wu, Hui Jiang
  bibkey: jiang-etal-2019-freebaseqa
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1028
  month: June
  page_first: '318'
  page_last: '323'
  pages: "318\u2013323"
  paper_id: '28'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1028.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1028.jpg
  title: 'FreebaseQA: A New Factoid QA Data Set Matching Trivia-Style Question-Answer
    Pairs with Freebase'
  title_html: '<span class="acl-fixed-case">F</span>reebase<span class="acl-fixed-case">QA</span>:
    A New Factoid <span class="acl-fixed-case">QA</span> Data Set Matching Trivia-Style
    Question-Answer Pairs with <span class="acl-fixed-case">F</span>reebase'
  url: https://www.aclweb.org/anthology/N19-1028
  year: '2019'
N19-1029:
  abstract: "Knowledge graph based simple question answering (KBSQA) is a major area\
    \ of research within question answering. Although only dealing with simple questions,\
    \ i.e., questions that can be answered through a single knowledge base (KB) fact,\
    \ this task is neither simple nor close to being solved. Targeting on the two\
    \ main steps, subgraph selection and fact selection, the literature has developed\
    \ sophisticated approaches. However, the importance of subgraph ranking and leveraging\
    \ the subject\u2013relation dependency of a KB fact have not been sufficiently\
    \ explored. Motivated by this, we present a unified framework to describe and\
    \ analyze existing approaches. Using this framework as a starting point we focus\
    \ on two aspects: improving subgraph selection through a novel ranking method,\
    \ and leveraging the subject\u2013relation dependency by proposing a joint scoring\
    \ CNN model with a novel loss function that enforces the well-order of scores.\
    \ Our methods achieve a new state of the art (85.44% in accuracy) on the SimpleQuestions\
    \ dataset."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1029.Poster.pdf
    type: poster
    url: https://www.aclweb.org/anthology/attachments/N19-1029.Poster.pdf
  author:
  - first: Wenbo
    full: Wenbo Zhao
    id: wenbo-zhao
    last: Zhao
  - first: Tagyoung
    full: Tagyoung Chung
    id: tagyoung-chung
    last: Chung
  - first: Anuj
    full: Anuj Goyal
    id: anuj-goyal
    last: Goyal
  - first: Angeliki
    full: Angeliki Metallinou
    id: angeliki-metallinou
    last: Metallinou
  author_string: Wenbo Zhao, Tagyoung Chung, Anuj Goyal, Angeliki Metallinou
  bibkey: zhao-etal-2019-simple
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1029
  month: June
  page_first: '324'
  page_last: '334'
  pages: "324\u2013334"
  paper_id: '29'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1029.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1029.jpg
  title: Simple Question Answering with Subgraph Ranking and Joint-Scoring
  title_html: Simple Question Answering with Subgraph Ranking and Joint-Scoring
  url: https://www.aclweb.org/anthology/N19-1029
  year: '2019'
N19-1030:
  abstract: Open-domain question answering remains a challenging task as it requires
    models that are capable of understanding questions and answers, collecting useful
    information, and reasoning over evidence. Previous work typically formulates this
    task as a reading comprehension or entailment problem given evidence retrieved
    from search engines. However, existing techniques struggle to retrieve indirectly
    related evidence when no directly related evidence is provided, especially for
    complex questions where it is hard to parse precisely what the question asks.
    In this paper we propose a retriever-reader model that learns to attend on essential
    terms during the question answering process. We build (1) an essential term selector
    which first identifies the most important words in a question, then reformulates
    the query and searches for related evidence; and (2) an enhanced reader that distinguishes
    between essential terms and distracting words to predict the answer. We evaluate
    our model on multiple open-domain QA datasets, notably achieving the level of
    the state-of-the-art on the AI2 Reasoning Challenge (ARC) dataset.
  address: Minneapolis, Minnesota
  author:
  - first: Jianmo
    full: Jianmo Ni
    id: jianmo-ni
    last: Ni
  - first: Chenguang
    full: Chenguang Zhu
    id: chenguang-zhu
    last: Zhu
  - first: Weizhu
    full: Weizhu Chen
    id: weizhu-chen
    last: Chen
  - first: Julian
    full: Julian McAuley
    id: julian-mcauley
    last: McAuley
  author_string: Jianmo Ni, Chenguang Zhu, Weizhu Chen, Julian McAuley
  bibkey: ni-etal-2019-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1030
  month: June
  page_first: '335'
  page_last: '344'
  pages: "335\u2013344"
  paper_id: '30'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1030.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1030.jpg
  title: 'Learning to Attend On Essential Terms: An Enhanced Retriever-Reader Model
    for Open-domain Question Answering'
  title_html: 'Learning to Attend On Essential Terms: An Enhanced Retriever-Reader
    Model for Open-domain Question Answering'
  url: https://www.aclweb.org/anthology/N19-1030
  year: '2019'
N19-1031:
  abstract: "In relation extraction for knowledge-based question answering, searching\
    \ from one entity to another entity via a single relation is called \u201Cone\
    \ hop\u201D. In related work, an exhaustive search from all one-hop relations,\
    \ two-hop relations, and so on to the max-hop relations in the knowledge graph\
    \ is necessary but expensive. Therefore, the number of hops is generally restricted\
    \ to two or three. In this paper, we propose UHop, an unrestricted-hop framework\
    \ which relaxes this restriction by use of a transition-based search framework\
    \ to replace the relation-chain-based search one. We conduct experiments on conventional\
    \ 1- and 2-hop questions as well as lengthy questions, including datasets such\
    \ as WebQSP, PathQuestion, and Grid World. Results show that the proposed framework\
    \ enables the ability to halt, works well with state-of-the-art models, achieves\
    \ competitive performance without exhaustive searches, and opens the performance\
    \ gap for long relation paths."
  address: Minneapolis, Minnesota
  author:
  - first: Zi-Yuan
    full: Zi-Yuan Chen
    id: zi-yuan-chen
    last: Chen
  - first: Chih-Hung
    full: Chih-Hung Chang
    id: chih-hung-chang
    last: Chang
  - first: Yi-Pei
    full: Yi-Pei Chen
    id: yi-pei-chen
    last: Chen
  - first: Jijnasa
    full: Jijnasa Nayak
    id: jijnasa-nayak
    last: Nayak
  - first: Lun-Wei
    full: Lun-Wei Ku
    id: lun-wei-ku
    last: Ku
  author_string: Zi-Yuan Chen, Chih-Hung Chang, Yi-Pei Chen, Jijnasa Nayak, Lun-Wei
    Ku
  bibkey: chen-etal-2019-uhop
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1031
  month: June
  page_first: '345'
  page_last: '356'
  pages: "345\u2013356"
  paper_id: '31'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1031.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1031.jpg
  title: 'UHop: An Unrestricted-Hop Relation Extraction Framework for Knowledge-Based
    Question Answering'
  title_html: '<span class="acl-fixed-case">UH</span>op: An Unrestricted-Hop Relation
    Extraction Framework for Knowledge-Based Question Answering'
  url: https://www.aclweb.org/anthology/N19-1031
  year: '2019'
N19-1032:
  abstract: Multi-hop reasoning question answering requires deep comprehension of
    relationships between various documents and queries. We propose a Bi-directional
    Attention Entity Graph Convolutional Network (BAG), leveraging relationships between
    nodes in an entity graph and attention information between a query and the entity
    graph, to solve this task. Graph convolutional networks are used to obtain a relation-aware
    representation of nodes for entity graphs built from documents with multi-level
    features. Bidirectional attention is then applied on graphs and queries to generate
    a query-aware nodes representation, which will be used for the final prediction.
    Experimental evaluation shows BAG achieves state-of-the-art accuracy performance
    on the QAngaroo WIKIHOP dataset.
  address: Minneapolis, Minnesota
  author:
  - first: Yu
    full: Yu Cao
    id: yu-cao
    last: Cao
  - first: Meng
    full: Meng Fang
    id: meng-fang
    last: Fang
  - first: Dacheng
    full: Dacheng Tao
    id: dacheng-tao
    last: Tao
  author_string: Yu Cao, Meng Fang, Dacheng Tao
  bibkey: cao-etal-2019-bag
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1032
  month: June
  page_first: '357'
  page_last: '362'
  pages: "357\u2013362"
  paper_id: '32'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1032.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1032.jpg
  title: 'BAG: Bi-directional Attention Entity Graph Convolutional Network for Multi-hop
    Reasoning Question Answering'
  title_html: '<span class="acl-fixed-case">BAG</span>: Bi-directional Attention Entity
    Graph Convolutional Network for Multi-hop Reasoning Question Answering'
  url: https://www.aclweb.org/anthology/N19-1032
  year: '2019'
N19-1033:
  abstract: In this paper, we propose a novel representation for text documents based
    on aggregating word embedding vectors into document embeddings. Our approach is
    inspired by the Vector of Locally-Aggregated Descriptors used for image representation,
    and it works as follows. First, the word embeddings gathered from a collection
    of documents are clustered by k-means in order to learn a codebook of semnatically-related
    word embeddings. Each word embedding is then associated to its nearest cluster
    centroid (codeword). The Vector of Locally-Aggregated Word Embeddings (VLAWE)
    representation of a document is then computed by accumulating the differences
    between each codeword vector and each word vector (from the document) associated
    to the respective codeword. We plug the VLAWE representation, which is learned
    in an unsupervised manner, into a classifier and show that it is useful for a
    diverse set of text classification tasks. We compare our approach with a broad
    range of recent state-of-the-art methods, demonstrating the effectiveness of our
    approach. Furthermore, we obtain a considerable improvement on the Movie Review
    data set, reporting an accuracy of 93.3%, which represents an absolute gain of
    10% over the state-of-the-art approach.
  address: Minneapolis, Minnesota
  author:
  - first: Radu Tudor
    full: Radu Tudor Ionescu
    id: radu-tudor-ionescu
    last: Ionescu
  - first: Andrei
    full: Andrei Butnaru
    id: andrei-butnaru
    last: Butnaru
  author_string: Radu Tudor Ionescu, Andrei Butnaru
  bibkey: ionescu-butnaru-2019-vector
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1033
  month: June
  page_first: '363'
  page_last: '369'
  pages: "363\u2013369"
  paper_id: '33'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1033.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1033.jpg
  title: 'Vector of Locally-Aggregated Word Embeddings (VLAWE): A Novel Document-level
    Representation'
  title_html: 'Vector of Locally-Aggregated Word Embeddings (<span class="acl-fixed-case">VLAWE</span>):
    A Novel Document-level Representation'
  url: https://www.aclweb.org/anthology/N19-1033
  year: '2019'
N19-1034:
  abstract: Related tasks often have inter-dependence on each other and perform better
    when solved in a joint framework. In this paper, we present a deep multi-task
    learning framework that jointly performs sentiment and emotion analysis both.
    The multi-modal inputs (i.e. text, acoustic and visual frames) of a video convey
    diverse and distinctive information, and usually do not have equal contribution
    in the decision making. We propose a context-level inter-modal attention framework
    for simultaneously predicting the sentiment and expressed emotions of an utterance.
    We evaluate our proposed approach on CMU-MOSEI dataset for multi-modal sentiment
    and emotion analysis. Evaluation results suggest that multi-task learning framework
    offers improvement over the single-task framework. The proposed approach reports
    new state-of-the-art performance for both sentiment analysis and emotion analysis.
  address: Minneapolis, Minnesota
  author:
  - first: Md Shad
    full: Md Shad Akhtar
    id: md-shad-akhtar
    last: Akhtar
  - first: Dushyant
    full: Dushyant Chauhan
    id: dushyant-chauhan
    last: Chauhan
  - first: Deepanway
    full: Deepanway Ghosal
    id: deepanway-ghosal
    last: Ghosal
  - first: Soujanya
    full: Soujanya Poria
    id: soujanya-poria
    last: Poria
  - first: Asif
    full: Asif Ekbal
    id: asif-ekbal
    last: Ekbal
  - first: Pushpak
    full: Pushpak Bhattacharyya
    id: pushpak-bhattacharyya
    last: Bhattacharyya
  author_string: Md Shad Akhtar, Dushyant Chauhan, Deepanway Ghosal, Soujanya Poria,
    Asif Ekbal, Pushpak Bhattacharyya
  bibkey: akhtar-etal-2019-multi
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1034
  month: June
  page_first: '370'
  page_last: '379'
  pages: "370\u2013379"
  paper_id: '34'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1034.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1034.jpg
  title: Multi-task Learning for Multi-modal Emotion Recognition and Sentiment Analysis
  title_html: Multi-task Learning for Multi-modal Emotion Recognition and Sentiment
    Analysis
  url: https://www.aclweb.org/anthology/N19-1034
  year: '2019'
N19-1035:
  abstract: Aspect-based sentiment analysis (ABSA), which aims to identify fine-grained
    opinion polarity towards a specific aspect, is a challenging subtask of sentiment
    analysis (SA). In this paper, we construct an auxiliary sentence from the aspect
    and convert ABSA to a sentence-pair classification task, such as question answering
    (QA) and natural language inference (NLI). We fine-tune the pre-trained model
    from BERT and achieve new state-of-the-art results on SentiHood and SemEval-2014
    Task 4 datasets. The source codes are available at https://github.com/HSLCY/ABSA-BERT-pair.
  address: Minneapolis, Minnesota
  author:
  - first: Chi
    full: Chi Sun
    id: chi-sun
    last: Sun
  - first: Luyao
    full: Luyao Huang
    id: luyao-huang
    last: Huang
  - first: Xipeng
    full: Xipeng Qiu
    id: xipeng-qiu
    last: Qiu
  author_string: Chi Sun, Luyao Huang, Xipeng Qiu
  bibkey: sun-etal-2019-utilizing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1035
  month: June
  page_first: '380'
  page_last: '385'
  pages: "380\u2013385"
  paper_id: '35'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1035.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1035.jpg
  title: Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary
    Sentence
  title_html: Utilizing <span class="acl-fixed-case">BERT</span> for Aspect-Based
    Sentiment Analysis via Constructing Auxiliary Sentence
  url: https://www.aclweb.org/anthology/N19-1035
  year: '2019'
N19-1036:
  abstract: "In this paper, we propose a variational approach to weakly supervised\
    \ document-level multi-aspect sentiment classification. Instead of using user-generated\
    \ ratings or annotations provided by domain experts, we use target-opinion word\
    \ pairs as \u201Csupervision.\u201D These word pairs can be extracted by using\
    \ dependency parsers and simple rules. Our objective is to predict an opinion\
    \ word given a target word while our ultimate goal is to learn a sentiment polarity\
    \ classifier to predict the sentiment polarity of each aspect given a document.\
    \ By introducing a latent variable, i.e., the sentiment polarity, to the objective\
    \ function, we can inject the sentiment polarity classifier to the objective via\
    \ the variational lower bound. We can learn a sentiment polarity classifier by\
    \ optimizing the lower bound. We show that our method can outperform weakly supervised\
    \ baselines on TripAdvisor and BeerAdvocate datasets and can be comparable to\
    \ the state-of-the-art supervised method with hundreds of labels per aspect."
  address: Minneapolis, Minnesota
  author:
  - first: Ziqian
    full: Ziqian Zeng
    id: ziqian-zeng
    last: Zeng
  - first: Wenxuan
    full: Wenxuan Zhou
    id: wenxuan-zhou
    last: Zhou
  - first: Xin
    full: Xin Liu
    id: xin-liu
    last: Liu
  - first: Yangqiu
    full: Yangqiu Song
    id: yangqiu-song
    last: Song
  author_string: Ziqian Zeng, Wenxuan Zhou, Xin Liu, Yangqiu Song
  bibkey: zeng-etal-2019-variational
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1036
  month: June
  page_first: '386'
  page_last: '396'
  pages: "386\u2013396"
  paper_id: '36'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1036.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1036.jpg
  title: A Variational Approach to Weakly Supervised Document-Level Multi-Aspect Sentiment
    Classification
  title_html: A Variational Approach to Weakly Supervised Document-Level Multi-Aspect
    Sentiment Classification
  url: https://www.aclweb.org/anthology/N19-1036
  year: '2019'
N19-1037:
  abstract: 'In this paper, we address three challenges in utterance-level emotion
    recognition in dialogue systems: (1) the same word can deliver different emotions
    in different contexts; (2) some emotions are rarely seen in general dialogues;
    (3) long-range contextual information is hard to be effectively captured. We therefore
    propose a hierarchical Gated Recurrent Unit (HiGRU) framework with a lower-level
    GRU to model the word-level inputs and an upper-level GRU to capture the contexts
    of utterance-level embeddings. Moreover, we promote the framework to two variants,
    Hi-GRU with individual features fusion (HiGRU-f) and HiGRU with self-attention
    and features fusion (HiGRU-sf), so that the word/utterance-level individual inputs
    and the long-range contextual information can be sufficiently utilized. Experiments
    on three dialogue emotion datasets, IEMOCAP, Friends, and EmotionPush demonstrate
    that our proposed Hi-GRU models attain at least 8.7%, 7.5%, 6.0% improvement over
    the state-of-the-art methods on each dataset, respectively. Particularly, by utilizing
    only the textual feature in IEMOCAP, our HiGRU models gain at least 3.8% improvement
    over the state-of-the-art conversational memory network (CMN) with the trimodal
    features of text, video, and audio.'
  address: Minneapolis, Minnesota
  author:
  - first: Wenxiang
    full: Wenxiang Jiao
    id: wenxiang-jiao
    last: Jiao
  - first: Haiqin
    full: Haiqin Yang
    id: haiqin-yang
    last: Yang
  - first: Irwin
    full: Irwin King
    id: irwin-king
    last: King
  - first: Michael R.
    full: Michael R. Lyu
    id: michael-r-lyu
    last: Lyu
  author_string: Wenxiang Jiao, Haiqin Yang, Irwin King, Michael R. Lyu
  bibkey: jiao-etal-2019-higru
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1037
  month: June
  page_first: '397'
  page_last: '406'
  pages: "397\u2013406"
  paper_id: '37'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1037.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1037.jpg
  title: 'HiGRU: Hierarchical Gated Recurrent Units for Utterance-Level Emotion Recognition'
  title_html: '<span class="acl-fixed-case">H</span>i<span class="acl-fixed-case">GRU</span>:
    <span class="acl-fixed-case">H</span>ierarchical Gated Recurrent Units for Utterance-Level
    Emotion Recognition'
  url: https://www.aclweb.org/anthology/N19-1037
  year: '2019'
N19-1038:
  abstract: 'Negation scope detection is widely performed as a supervised learning
    task which relies upon negation labels at word level. This suffers from two key
    drawbacks: (1) such granular annotations are costly and (2) highly subjective,
    since, due to the absence of explicit linguistic resolution rules, human annotators
    often disagree in the perceived negation scopes. To the best of our knowledge,
    our work presents the first approach that eliminates the need for world-level
    negation labels, replacing it instead with document-level sentiment annotations.
    For this, we present a novel strategy for learning fully interpretable negation
    rules via weak supervision: we apply reinforcement learning to find a policy that
    reconstructs negation rules from sentiment predictions at document level. Our
    experiments demonstrate that our approach for weak supervision can effectively
    learn negation rules. Furthermore, an out-of-sample evaluation via sentiment analysis
    reveals consistent improvements (of up to 4.66%) over both a sentiment analysis
    with (i) no negation handling and (ii) the use of word-level annotations from
    humans. Moreover, the inferred negation rules are fully interpretable.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1038.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1038.Supplementary.pdf
  author:
  - first: Nicolas
    full: "Nicolas Pr\xF6llochs"
    id: nicolas-prollochs
    last: "Pr\xF6llochs"
  - first: Stefan
    full: Stefan Feuerriegel
    id: stefan-feuerriegel
    last: Feuerriegel
  - first: Dirk
    full: Dirk Neumann
    id: dirk-neumann
    last: Neumann
  author_string: "Nicolas Pr\xF6llochs, Stefan Feuerriegel, Dirk Neumann"
  bibkey: prollochs-etal-2019-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1038
  month: June
  page_first: '407'
  page_last: '413'
  pages: "407\u2013413"
  paper_id: '38'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1038.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1038.jpg
  title: 'Learning Interpretable Negation Rules via Weak Supervision at Document Level:
    A Reinforcement Learning Approach'
  title_html: 'Learning Interpretable Negation Rules via Weak Supervision at Document
    Level: A Reinforcement Learning Approach'
  url: https://www.aclweb.org/anthology/N19-1038
  year: '2019'
N19-1039:
  abstract: "Unsupervised domain adaptation (UDA) is the task of training a statistical\
    \ model on labeled data from a source domain to achieve better performance on\
    \ data from a target domain, with access to only unlabeled data in the target\
    \ domain. Existing state-of-the-art UDA approaches use neural networks to learn\
    \ representations that are trained to predict the values of subset of important\
    \ features called \u201Cpivot features\u201D on combined data from the source\
    \ and target domains. In this work, we show that it is possible to improve on\
    \ existing neural domain adaptation algorithms by 1) jointly training the representation\
    \ learner with the task learner; and 2) removing the need for heuristically-selected\
    \ \u201Cpivot features.\u201D Our results show competitive performance with a\
    \ simpler model."
  address: Minneapolis, Minnesota
  author:
  - first: Timothy
    full: Timothy Miller
    id: timothy-miller
    last: Miller
  author_string: Timothy Miller
  bibkey: miller-2019-simplified
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1039
  month: June
  page_first: '414'
  page_last: '419'
  pages: "414\u2013419"
  paper_id: '39'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1039.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1039.jpg
  title: Simplified Neural Unsupervised Domain Adaptation
  title_html: Simplified Neural Unsupervised Domain Adaptation
  url: https://www.aclweb.org/anthology/N19-1039
  year: '2019'
N19-1040:
  abstract: Word embeddings learned in two languages can be mapped to a common space
    to produce Bilingual Word Embeddings (BWE). Unsupervised BWE methods learn such
    a mapping without any parallel data. However, these methods are mainly evaluated
    on tasks of word translation or word similarity. We show that these methods fail
    to capture the sentiment information and do not perform well enough on cross-lingual
    sentiment analysis. In this work, we propose UBiSE (Unsupervised Bilingual Sentiment
    Embeddings), which learns sentiment-specific word representations for two languages
    in a common space without any cross-lingual supervision. Our method only requires
    a sentiment corpus in the source language and pretrained monolingual word embeddings
    of both languages. We evaluate our method on three language pairs for cross-lingual
    sentiment analysis. Experimental results show that our method outperforms previous
    unsupervised BWE methods and even supervised BWE methods. Our method succeeds
    for a distant language pair English-Basque.
  address: Minneapolis, Minnesota
  author:
  - first: Yanlin
    full: Yanlin Feng
    id: yanlin-feng
    last: Feng
  - first: Xiaojun
    full: Xiaojun Wan
    id: xiaojun-wan
    last: Wan
  author_string: Yanlin Feng, Xiaojun Wan
  bibkey: feng-wan-2019-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1040
  month: June
  page_first: '420'
  page_last: '429'
  pages: "420\u2013429"
  paper_id: '40'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1040.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1040.jpg
  title: Learning Bilingual Sentiment-Specific Word Embeddings without Cross-lingual
    Supervision
  title_html: Learning Bilingual Sentiment-Specific Word Embeddings without Cross-lingual
    Supervision
  url: https://www.aclweb.org/anthology/N19-1040
  year: '2019'
N19-1041:
  abstract: Regularization of neural machine translation is still a significant problem,
    especially in low-resource settings. To mollify this problem, we propose regressing
    word embeddings (ReWE) as a new regularization technique in a system that is jointly
    trained to predict the next word in the translation (categorical value) and its
    word embedding (continuous value). Such a joint training allows the proposed system
    to learn the distributional properties represented by the word embeddings, empirically
    improving the generalization to unseen sentences. Experiments over three translation
    datasets have showed a consistent improvement over a strong baseline, ranging
    between 0.91 and 2.4 BLEU points, and also a marked improvement over a state-of-the-art
    system.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1041.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1041.Supplementary.pdf
  author:
  - first: Inigo
    full: Inigo Jauregi Unanue
    id: inigo-jauregi-unanue
    last: Jauregi Unanue
  - first: Ehsan
    full: Ehsan Zare Borzeshi
    id: ehsan-zare-borzeshi
    last: Zare Borzeshi
  - first: Nazanin
    full: Nazanin Esmaili
    id: nazanin-esmaili
    last: Esmaili
  - first: Massimo
    full: Massimo Piccardi
    id: massimo-piccardi
    last: Piccardi
  author_string: Inigo Jauregi Unanue, Ehsan Zare Borzeshi, Nazanin Esmaili, Massimo
    Piccardi
  bibkey: jauregi-unanue-etal-2019-rewe
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1041
  month: June
  page_first: '430'
  page_last: '436'
  pages: "430\u2013436"
  paper_id: '41'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1041.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1041.jpg
  title: 'ReWE: Regressing Word Embeddings for Regularization of Neural Machine Translation
    Systems'
  title_html: '<span class="acl-fixed-case">R</span>e<span class="acl-fixed-case">WE</span>:
    Regressing Word Embeddings for Regularization of Neural Machine Translation Systems'
  url: https://www.aclweb.org/anthology/N19-1041
  year: '2019'
N19-1042:
  abstract: "A desideratum of high-quality translation systems is that they preserve\
    \ meaning, in the sense that two sentences with different meanings should not\
    \ translate to one and the same sentence in another language. However, state-of-the-art\
    \ systems often fail in this regard, particularly in cases where the source and\
    \ target languages partition the \u201Cmeaning space\u201D in different ways.\
    \ For instance, \u201CI cut my finger.\u201D and \u201CI cut my finger off.\u201D\
    \ describe different states of the world but are translated to French (by both\
    \ Fairseq and Google Translate) as \u201CJe me suis coup\xE9 le doigt.\u201D,\
    \ which is ambiguous as to whether the finger is detached. More generally, translation\
    \ systems are typically many-to-one (non-injective) functions from source to target\
    \ language, which in many cases results in important distinctions in meaning being\
    \ lost in translation. Building on Bayesian models of informative utterance production,\
    \ we present a method to define a less ambiguous translation system in terms of\
    \ an underlying pre-trained neural sequence-to-sequence model. This method increases\
    \ injectivity, resulting in greater preservation of meaning as measured by improvement\
    \ in cycle-consistency, without impeding translation quality (measured by BLEU\
    \ score)."
  address: Minneapolis, Minnesota
  author:
  - first: Reuben
    full: Reuben Cohn-Gordon
    id: reuben-cohn-gordon
    last: Cohn-Gordon
  - first: Noah
    full: Noah Goodman
    id: noah-goodman
    last: Goodman
  author_string: Reuben Cohn-Gordon, Noah Goodman
  bibkey: cohn-gordon-goodman-2019-lost
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1042
  month: June
  page_first: '437'
  page_last: '441'
  pages: "437\u2013441"
  paper_id: '42'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1042.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1042.jpg
  title: 'Lost in Machine Translation: A Method to Reduce Meaning Loss'
  title_html: 'Lost in Machine Translation: A Method to Reduce Meaning Loss'
  url: https://www.aclweb.org/anthology/N19-1042
  year: '2019'
N19-1043:
  abstract: We aim to better exploit the limited amounts of parallel text available
    in low-resource settings by introducing a differentiable reconstruction loss for
    neural machine translation (NMT). This loss compares original inputs to reconstructed
    inputs, obtained by back-translating translation hypotheses into the input language.
    We leverage differentiable sampling and bi-directional NMT to train models end-to-end,
    without introducing additional parameters. This approach achieves small but consistent
    BLEU improvements on four language pairs in both translation directions, and outperforms
    an alternative differentiable reconstruction strategy based on hidden states.
  address: Minneapolis, Minnesota
  author:
  - first: Xing
    full: Xing Niu
    id: xing-niu
    last: Niu
  - first: Weijia
    full: Weijia Xu
    id: weijia-xu
    last: Xu
  - first: Marine
    full: Marine Carpuat
    id: marine-carpuat
    last: Carpuat
  author_string: Xing Niu, Weijia Xu, Marine Carpuat
  bibkey: niu-etal-2019-bi
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1043
  month: June
  page_first: '442'
  page_last: '448'
  pages: "442\u2013448"
  paper_id: '43'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1043.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1043.jpg
  title: Bi-Directional Differentiable Input Reconstruction for Low-Resource Neural
    Machine Translation
  title_html: Bi-Directional Differentiable Input Reconstruction for Low-Resource
    Neural Machine Translation
  url: https://www.aclweb.org/anthology/N19-1043
  year: '2019'
N19-1044:
  abstract: Leveraging user-provided translation to constrain NMT has practical significance.
    Existing methods can be classified into two main categories, namely the use of
    placeholder tags for lexicon words and the use of hard constraints during decoding.
    Both methods can hurt translation fidelity for various reasons. We investigate
    a data augmentation method, making code-switched training data by replacing source
    phrases with their target translations. Our method does not change the MNT model
    or decoding algorithm, allowing the model to learn lexicon translations by copying
    source-side target words. Extensive experiments show that our method achieves
    consistent improvements over existing approaches, improving translation of constrained
    words without hurting unconstrained words.
  address: Minneapolis, Minnesota
  author:
  - first: Kai
    full: Kai Song
    id: kai-song
    last: Song
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  - first: Heng
    full: Heng Yu
    id: heng-yu
    last: Yu
  - first: Weihua
    full: Weihua Luo
    id: weihua-luo
    last: Luo
  - first: Kun
    full: Kun Wang
    id: kun-wang
    last: Wang
  - first: Min
    full: Min Zhang
    id: min-zhang
    last: Zhang
  author_string: Kai Song, Yue Zhang, Heng Yu, Weihua Luo, Kun Wang, Min Zhang
  bibkey: song-etal-2019-code
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1044
  month: June
  page_first: '449'
  page_last: '459'
  pages: "449\u2013459"
  paper_id: '44'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1044.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1044.jpg
  title: Code-Switching for Enhancing NMT with Pre-Specified Translation
  title_html: Code-Switching for Enhancing <span class="acl-fixed-case">NMT</span>
    with Pre-Specified Translation
  url: https://www.aclweb.org/anthology/N19-1044
  year: '2019'
N19-1045:
  abstract: 'The problem of learning to translate between two vector spaces given
    a set of aligned points arises in several application areas of NLP. Current solutions
    assume that the lexicon which defines the alignment pairs is noise-free. We consider
    the case where the set of aligned points is allowed to contain an amount of noise,
    in the form of incorrect lexicon pairs and show that this arises in practice by
    analyzing the edited dictionaries after the cleaning process. We demonstrate that
    such noise substantially degrades the accuracy of the learned translation when
    using current methods. We propose a model that accounts for noisy pairs. This
    is achieved by introducing a generative model with a compatible iterative EM algorithm.
    The algorithm jointly learns the noise level in the lexicon, finds the set of
    noisy pairs, and learns the mapping between the spaces. We demonstrate the effectiveness
    of our proposed algorithm on two alignment problems: bilingual word embedding
    translation, and mapping between diachronic embedding spaces for recovering the
    semantic shifts of words across time periods.'
  address: Minneapolis, Minnesota
  author:
  - first: Noa
    full: Noa Yehezkel Lubin
    id: noa-yehezkel-lubin
    last: Yehezkel Lubin
  - first: Jacob
    full: Jacob Goldberger
    id: jacob-goldberger
    last: Goldberger
  - first: Yoav
    full: Yoav Goldberg
    id: yoav-goldberg
    last: Goldberg
  author_string: Noa Yehezkel Lubin, Jacob Goldberger, Yoav Goldberg
  bibkey: yehezkel-lubin-etal-2019-aligning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1045
  month: June
  page_first: '460'
  page_last: '465'
  pages: "460\u2013465"
  paper_id: '45'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1045.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1045.jpg
  title: Aligning Vector-spaces with Noisy Supervised Lexicon
  title_html: Aligning Vector-spaces with Noisy Supervised Lexicon
  url: https://www.aclweb.org/anthology/N19-1045
  year: '2019'
N19-1046:
  abstract: Multilayer architectures are currently the gold standard for large-scale
    neural machine translation. Existing works have explored some methods for understanding
    the hidden representations, however, they have not sought to improve the translation
    quality rationally according to their understanding. Towards understanding for
    performance improvement, we first artificially construct a sequence of nested
    relative tasks and measure the feature generalization ability of the learned hidden
    representation over these tasks. Based on our understanding, we then propose to
    regularize the layer-wise representations with all tree-induced tasks. To overcome
    the computational bottleneck resulting from the large number of regularization
    terms, we design efficient approximation methods by selecting a few coarse-to-fine
    tasks for regularization. Extensive experiments on two widely-used datasets demonstrate
    the proposed methods only lead to small extra overheads in training but no additional
    overheads in testing, and achieve consistent improvements (up to +1.3 BLEU) compared
    to the state-of-the-art translation model.
  address: Minneapolis, Minnesota
  author:
  - first: Guanlin
    full: Guanlin Li
    id: guanlin-li
    last: Li
  - first: Lemao
    full: Lemao Liu
    id: lemao-liu
    last: Liu
  - first: Xintong
    full: Xintong Li
    id: xintong-li
    last: Li
  - first: Conghui
    full: Conghui Zhu
    id: conghui-zhu
    last: Zhu
  - first: Tiejun
    full: Tiejun Zhao
    id: tiejun-zhao
    last: Zhao
  - first: Shuming
    full: Shuming Shi
    id: shuming-shi
    last: Shi
  author_string: Guanlin Li, Lemao Liu, Xintong Li, Conghui Zhu, Tiejun Zhao, Shuming
    Shi
  bibkey: li-etal-2019-understanding-improving
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1046
  month: June
  page_first: '466'
  page_last: '477'
  pages: "466\u2013477"
  paper_id: '46'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1046.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1046.jpg
  title: Understanding and Improving Hidden Representations for Neural Machine Translation
  title_html: <span class="acl-fixed-case">U</span>nderstanding and <span class="acl-fixed-case">I</span>mproving
    <span class="acl-fixed-case">H</span>idden <span class="acl-fixed-case">R</span>epresentations
    for <span class="acl-fixed-case">N</span>eural <span class="acl-fixed-case">M</span>achine
    <span class="acl-fixed-case">T</span>ranslation
  url: https://www.aclweb.org/anthology/N19-1046
  year: '2019'
N19-1047:
  abstract: "Syntactic analysis plays an important role in semantic parsing, but the\
    \ nature of this role remains a topic of ongoing debate. The debate has been constrained\
    \ by the scarcity of empirical comparative studies between syntactic and semantic\
    \ schemes, which hinders the development of parsing methods informed by the details\
    \ of target schemes and constructions. We target this gap, and take Universal\
    \ Dependencies (UD) and UCCA as a test case. After abstracting away from differences\
    \ of convention or formalism, we find that most content divergences can be ascribed\
    \ to: (1) UCCA\u2019s distinction between a Scene and a non-Scene; (2) UCCA\u2019\
    s distinction between primary relations, secondary ones and participants; (3)\
    \ different treatment of multi-word expressions, and (4) different treatment of\
    \ inter-clause linkage. We further discuss the long tail of cases where the two\
    \ schemes take markedly different approaches. Finally, we show that the proposed\
    \ comparison methodology can be used for fine-grained evaluation of UCCA parsing,\
    \ highlighting both challenges and potential sources for improvement. The substantial\
    \ differences between the schemes suggest that semantic parsers are likely to\
    \ benefit downstream text understanding applications beyond their syntactic counterparts."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1047.Software.zip
    type: software
    url: https://www.aclweb.org/anthology/attachments/N19-1047.Software.zip
  - filename: N19-1047.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1047.Supplementary.pdf
  - filename: N19-1047.Poster.pdf
    type: poster
    url: https://www.aclweb.org/anthology/attachments/N19-1047.Poster.pdf
  author:
  - first: Daniel
    full: Daniel Hershcovich
    id: daniel-hershcovich
    last: Hershcovich
  - first: Omri
    full: Omri Abend
    id: omri-abend
    last: Abend
  - first: Ari
    full: Ari Rappoport
    id: ari-rappoport
    last: Rappoport
  author_string: Daniel Hershcovich, Omri Abend, Ari Rappoport
  bibkey: hershcovich-etal-2019-content
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1047
  month: June
  page_first: '478'
  page_last: '488'
  pages: "478\u2013488"
  paper_id: '47'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1047.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1047.jpg
  title: Content Differences in Syntactic and Semantic Representation
  title_html: Content Differences in Syntactic and Semantic Representation
  url: https://www.aclweb.org/anthology/N19-1047
  year: '2019'
N19-1048:
  abstract: "Learning high-quality embeddings for rare words is a hard problem because\
    \ of sparse context information. Mimicking (Pinter et al., 2017) has been proposed\
    \ as a solution: given embeddings learned by a standard algorithm, a model is\
    \ first trained to reproduce embeddings of frequent words from their surface form\
    \ and then used to compute embeddings for rare words. In this paper, we introduce\
    \ attentive mimicking: the mimicking model is given access not only to a word\u2019\
    s surface form, but also to all available contexts and learns to attend to the\
    \ most informative and reliable contexts for computing an embedding. In an evaluation\
    \ on four tasks, we show that attentive mimicking outperforms previous work for\
    \ both rare and medium-frequency words. Thus, compared to previous work, attentive\
    \ mimicking improves embeddings for a much larger part of the vocabulary, including\
    \ the medium-frequency range."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1048.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1048.Supplementary.pdf
  author:
  - first: Timo
    full: Timo Schick
    id: timo-schick
    last: Schick
  - first: Hinrich
    full: "Hinrich Sch\xFCtze"
    id: hinrich-schutze
    last: "Sch\xFCtze"
  author_string: "Timo Schick, Hinrich Sch\xFCtze"
  bibkey: schick-schutze-2019-attentive
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1048
  month: June
  page_first: '489'
  page_last: '494'
  pages: "489\u2013494"
  paper_id: '48'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1048.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1048.jpg
  title: 'Attentive Mimicking: Better Word Embeddings by Attending to Informative
    Contexts'
  title_html: 'Attentive Mimicking: Better Word Embeddings by Attending to Informative
    Contexts'
  url: https://www.aclweb.org/anthology/N19-1048
  year: '2019'
N19-1049:
  abstract: "Research in the area of style transfer for text is currently bottlenecked\
    \ by a lack of standard evaluation practices. This paper aims to alleviate this\
    \ issue by experimentally identifying best practices with a Yelp sentiment dataset.\
    \ We specify three aspects of interest (style transfer intensity, content preservation,\
    \ and naturalness) and show how to obtain more reliable measures of them from\
    \ human evaluation than in previous work. We propose a set of metrics for automated\
    \ evaluation and demonstrate that they are more strongly correlated and in agreement\
    \ with human judgment: direction-corrected Earth Mover\u2019s Distance, Word Mover\u2019\
    s Distance on style-masked texts, and adversarial classification for the respective\
    \ aspects. We also show that the three examined models exhibit tradeoffs between\
    \ aspects of interest, demonstrating the importance of evaluating style transfer\
    \ models at specific points of their tradeoff plots. We release software with\
    \ our evaluation metrics to facilitate research."
  address: Minneapolis, Minnesota
  author:
  - first: Remi
    full: Remi Mir
    id: remi-mir
    last: Mir
  - first: Bjarke
    full: Bjarke Felbo
    id: bjarke-felbo
    last: Felbo
  - first: Nick
    full: Nick Obradovich
    id: nick-obradovich
    last: Obradovich
  - first: Iyad
    full: Iyad Rahwan
    id: iyad-rahwan
    last: Rahwan
  author_string: Remi Mir, Bjarke Felbo, Nick Obradovich, Iyad Rahwan
  bibkey: mir-etal-2019-evaluating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1049
  month: June
  page_first: '495'
  page_last: '504'
  pages: "495\u2013504"
  paper_id: '49'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1049.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1049.jpg
  title: Evaluating Style Transfer for Text
  title_html: Evaluating Style Transfer for Text
  url: https://www.aclweb.org/anthology/N19-1049
  year: '2019'
N19-1050:
  abstract: "Bigrams (two-word sequences) hold a special place in semantic composition\
    \ research since they are the smallest unit formed by composing words. A semantic\
    \ relatedness dataset that includes bigrams will thus be useful in the development\
    \ of automatic methods of semantic composition. However, existing relatedness\
    \ datasets only include pairs of unigrams (single words). Further, existing datasets\
    \ were created using rating scales and thus suffer from limitations such as in\
    \ consistent annotations and scale region bias. In this paper, we describe how\
    \ we created a large, fine-grained, bigram relatedness dataset (BiRD), using a\
    \ comparative annotation technique called Best\u2013Worst Scaling. Each of BiRD\u2019\
    s 3,345 English term pairs involves at least one bigram. We show that the relatedness\
    \ scores obtained are highly reliable (split-half reliability r= 0.937). We analyze\
    \ the data to obtain insights into bigram semantic relatedness. Finally, we present\
    \ benchmark experiments on using the relatedness dataset as a testbed to evaluate\
    \ simple unsupervised measures of semantic composition. BiRD is made freely available\
    \ to foster further research on how meaning can be represented and how meaning\
    \ can be composed."
  address: Minneapolis, Minnesota
  author:
  - first: Shima
    full: Shima Asaadi
    id: shima-asaadi
    last: Asaadi
  - first: Saif
    full: Saif Mohammad
    id: saif-mohammad
    last: Mohammad
  - first: Svetlana
    full: Svetlana Kiritchenko
    id: svetlana-kiritchenko
    last: Kiritchenko
  author_string: Shima Asaadi, Saif Mohammad, Svetlana Kiritchenko
  bibkey: asaadi-etal-2019-big
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1050
  month: June
  page_first: '505'
  page_last: '516'
  pages: "505\u2013516"
  paper_id: '50'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1050.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1050.jpg
  title: 'Big BiRD: A Large, Fine-Grained, Bigram Relatedness Dataset for Examining
    Semantic Composition'
  title_html: 'Big <span class="acl-fixed-case">B</span>i<span class="acl-fixed-case">RD</span>:
    A Large, Fine-Grained, Bigram Relatedness Dataset for Examining Semantic Composition'
  url: https://www.aclweb.org/anthology/N19-1050
  year: '2019'
N19-1051:
  abstract: 'In a corpus of data, outliers are either errors: mistakes in the data
    that are counterproductive, or are unique: informative samples that improve model
    robustness. Identifying outliers can lead to better datasets by (1) removing noise
    in datasets and (2) guiding collection of additional data to fill gaps. However,
    the problem of detecting both outlier types has received relatively little attention
    in NLP, particularly for dialog systems. We introduce a simple and effective technique
    for detecting both erroneous and unique samples in a corpus of short texts using
    neural sentence embeddings combined with distance-based outlier detection. We
    also present a novel data collection pipeline built atop our detection technique
    to automatically and iteratively mine unique data samples while discarding erroneous
    samples. Experiments show that our outlier detection technique is effective at
    finding errors while our data collection pipeline yields highly diverse corpora
    that in turn produce more robust intent classification and slot-filling models.'
  address: Minneapolis, Minnesota
  author:
  - first: Stefan
    full: Stefan Larson
    id: stefan-larson
    last: Larson
  - first: Anish
    full: Anish Mahendran
    id: anish-mahendran
    last: Mahendran
  - first: Andrew
    full: Andrew Lee
    id: andrew-lee
    last: Lee
  - first: Jonathan K.
    full: Jonathan K. Kummerfeld
    id: jonathan-k-kummerfeld
    last: Kummerfeld
  - first: Parker
    full: Parker Hill
    id: parker-hill
    last: Hill
  - first: Michael A.
    full: Michael A. Laurenzano
    id: michael-a-laurenzano
    last: Laurenzano
  - first: Johann
    full: Johann Hauswald
    id: johann-hauswald
    last: Hauswald
  - first: Lingjia
    full: Lingjia Tang
    id: lingjia-tang
    last: Tang
  - first: Jason
    full: Jason Mars
    id: jason-mars
    last: Mars
  author_string: Stefan Larson, Anish Mahendran, Andrew Lee, Jonathan K. Kummerfeld,
    Parker Hill, Michael A. Laurenzano, Johann Hauswald, Lingjia Tang, Jason Mars
  bibkey: larson-etal-2019-outlier
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1051
  month: June
  page_first: '517'
  page_last: '527'
  pages: "517\u2013527"
  paper_id: '51'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1051.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1051.jpg
  title: Outlier Detection for Improved Data Quality and Diversity in Dialog Systems
  title_html: Outlier Detection for Improved Data Quality and Diversity in Dialog
    Systems
  url: https://www.aclweb.org/anthology/N19-1051
  year: '2019'
N19-1052:
  abstract: "People often share personal narratives in order to seek advice from others.\
    \ To properly infer the narrator\u2019s intention, one needs to apply a certain\
    \ degree of common sense and social intuition. To test the capabilities of NLP\
    \ systems to recover such intuition, we introduce the new task of inferring what\
    \ is the advice-seeking goal behind a personal narrative. We formulate this as\
    \ a cloze test, where the goal is to identify which of two advice-seeking questions\
    \ was removed from a given narrative. The main challenge in constructing this\
    \ task is finding pairs of semantically plausible advice-seeking questions for\
    \ given narratives. To address this challenge, we devise a method that exploits\
    \ commonalities in experiences people share online to automatically extract pairs\
    \ of questions that are appropriate candidates for the cloze task. This results\
    \ in a dataset of over 20,000 personal narratives, each matched with a pair of\
    \ related advice-seeking questions: one actually intended by the narrator, and\
    \ the other one not. The dataset covers a very broad array of human experiences,\
    \ from dating, to career options, to stolen iPads. We use human annotation to\
    \ determine the degree to which the task relies on common sense and social intuition\
    \ in addition to a semantic understanding of the narrative. By introducing several\
    \ baselines for this new task we demonstrate its feasibility and identify avenues\
    \ for better modeling the intention of the narrator."
  address: Minneapolis, Minnesota
  author:
  - first: Liye
    full: Liye Fu
    id: liye-fu
    last: Fu
  - first: Jonathan P.
    full: Jonathan P. Chang
    id: jonathan-p-chang
    last: Chang
  - first: Cristian
    full: Cristian Danescu-Niculescu-Mizil
    id: cristian-danescu-niculescu-mizil
    last: Danescu-Niculescu-Mizil
  author_string: Liye Fu, Jonathan P. Chang, Cristian Danescu-Niculescu-Mizil
  bibkey: fu-etal-2019-asking
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1052
  month: June
  page_first: '528'
  page_last: '541'
  pages: "528\u2013541"
  paper_id: '52'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1052.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1052.jpg
  title: 'Asking the Right Question: Inferring Advice-Seeking Intentions from Personal
    Narratives'
  title_html: 'Asking the Right Question: Inferring Advice-Seeking Intentions from
    Personal Narratives'
  url: https://www.aclweb.org/anthology/N19-1052
  year: '2019'
N19-1053:
  abstract: "One key consequence of the information revolution is a significant increase\
    \ and a contamination of our information supply. The practice of fact checking\
    \ won\u2019t suffice to eliminate the biases in text data we observe, as the degree\
    \ of factuality alone does not determine whether biases exist in the spectrum\
    \ of opinions visible to us. To better understand controversial issues, one needs\
    \ to view them from a diverse yet comprehensive set of perspectives. For example,\
    \ there are many ways to respond to a claim such as \u201Canimals should have\
    \ lawful rights\u201D, and these responses form a spectrum of perspectives, each\
    \ with a stance relative to this claim and, ideally, with evidence supporting\
    \ it. Inherently, this is a natural language understanding task, and we propose\
    \ to address it as such. Specifically, we propose the task of substantiated perspective\
    \ discovery where, given a claim, a system is expected to discover a diverse set\
    \ of well-corroborated perspectives that take a stance with respect to the claim.\
    \ Each perspective should be substantiated by evidence paragraphs which summarize\
    \ pertinent results and facts. We construct PERSPECTRUM, a dataset of claims,\
    \ perspectives and evidence, making use of online debate websites to create the\
    \ initial data collection, and augmenting it using search engines in order to\
    \ expand and diversify our dataset. We use crowd-sourcing to filter out noise\
    \ and ensure high-quality data. Our dataset contains 1k claims, accompanied with\
    \ pools of 10k and 8k perspective sentences and evidence paragraphs, respectively.\
    \ We provide a thorough analysis of the dataset to highlight key underlying language\
    \ understanding challenges, and show that human baselines across multiple subtasks\
    \ far outperform ma-chine baselines built upon state-of-the-art NLP techniques.\
    \ This poses a challenge and opportunity for the NLP community to address."
  address: Minneapolis, Minnesota
  author:
  - first: Sihao
    full: Sihao Chen
    id: sihao-chen
    last: Chen
  - first: Daniel
    full: Daniel Khashabi
    id: daniel-khashabi
    last: Khashabi
  - first: Wenpeng
    full: Wenpeng Yin
    id: wenpeng-yin
    last: Yin
  - first: Chris
    full: Chris Callison-Burch
    id: chris-callison-burch
    last: Callison-Burch
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Sihao Chen, Daniel Khashabi, Wenpeng Yin, Chris Callison-Burch, Dan
    Roth
  bibkey: chen-etal-2019-seeing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1053
  month: June
  page_first: '542'
  page_last: '557'
  pages: "542\u2013557"
  paper_id: '53'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1053.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1053.jpg
  title: Seeing Things from a Different Angle:Discovering Diverse Perspectives about
    Claims
  title_html: Seeing Things from a Different Angle:Discovering Diverse Perspectives
    about Claims
  url: https://www.aclweb.org/anthology/N19-1053
  year: '2019'
N19-1054:
  abstract: Claims are the central component of an argument. Detecting claims across
    different domains or data sets can often be challenging due to their varying conceptualization.
    We propose to alleviate this problem by fine-tuning a language model using a Reddit
    corpus of 5.5 million opinionated claims. These claims are self-labeled by their
    authors using the internet acronyms IMO/IMHO (in my (humble) opinion). Empirical
    results show that using this approach improves the state of art performance across
    four benchmark argumentation data sets by an average of 4 absolute F1 points in
    claim detection. As these data sets include diverse domains such as social media
    and student essays this improvement demonstrates the robustness of fine-tuning
    on this novel corpus.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1054.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1054.Presentation.pdf
  - filename: https://vimeo.com/353455244
    type: video
    url: https://vimeo.com/353455244
  author:
  - first: Tuhin
    full: Tuhin Chakrabarty
    id: tuhin-chakrabarty
    last: Chakrabarty
  - first: Christopher
    full: Christopher Hidey
    id: christopher-hidey
    last: Hidey
  - first: Kathy
    full: Kathy McKeown
    id: kathleen-mckeown
    last: McKeown
  author_string: Tuhin Chakrabarty, Christopher Hidey, Kathy McKeown
  bibkey: chakrabarty-etal-2019-imho
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1054
  month: June
  page_first: '558'
  page_last: '563'
  pages: "558\u2013563"
  paper_id: '54'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1054.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1054.jpg
  title: IMHO Fine-Tuning Improves Claim Detection
  title_html: <span class="acl-fixed-case">IMHO</span> Fine-Tuning Improves Claim
    Detection
  url: https://www.aclweb.org/anthology/N19-1054
  year: '2019'
N19-1055:
  abstract: Neural network models have recently gained traction for sentence-level
    intent classification and token-based slot-label identification. In many real-world
    scenarios, users have multiple intents in the same utterance, and a token-level
    slot label can belong to more than one intent. We investigate an attention-based
    neural network model that performs multi-label classification for identifying
    multiple intents and produces labels for both intents and slot-labels at the token-level.
    We show state-of-the-art performance for both intent detection and slot-label
    identification by comparing against strong, recently proposed models. Our model
    provides a small but statistically significant improvement of 0.2% on the predominantly
    single-intent ATIS public data set, and 55% intent accuracy improvement on an
    internal multi-intent dataset.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/353455476
    type: video
    url: https://vimeo.com/353455476
  author:
  - first: Rashmi
    full: Rashmi Gangadharaiah
    id: rashmi-gangadharaiah
    last: Gangadharaiah
  - first: Balakrishnan
    full: Balakrishnan Narayanaswamy
    id: balakrishnan-narayanaswamy
    last: Narayanaswamy
  author_string: Rashmi Gangadharaiah, Balakrishnan Narayanaswamy
  bibkey: gangadharaiah-narayanaswamy-2019-joint
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1055
  month: June
  page_first: '564'
  page_last: '569'
  pages: "564\u2013569"
  paper_id: '55'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1055.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1055.jpg
  title: Joint Multiple Intent Detection and Slot Labeling for Goal-Oriented Dialog
  title_html: Joint Multiple Intent Detection and Slot Labeling for Goal-Oriented
    Dialog
  url: https://www.aclweb.org/anthology/N19-1055
  year: '2019'
N19-1056:
  abstract: 'This paper presents a novel crowd-sourced resource for multimodal discourse:
    our resource characterizes inferences in image-text contexts in the domain of
    cooking recipes in the form of coherence relations. Like previous corpora annotating
    discourse structure between text arguments, such as the Penn Discourse Treebank,
    our new corpus aids in establishing a better understanding of natural communication
    and common-sense reasoning, while our findings have implications for a wide range
    of applications, such as understanding and generation of multimodal documents.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/353455577
    type: video
    url: https://vimeo.com/353455577
  author:
  - first: Malihe
    full: Malihe Alikhani
    id: malihe-alikhani
    last: Alikhani
  - first: Sreyasi
    full: Sreyasi Nag Chowdhury
    id: sreyasi-nag-chowdhury
    last: Nag Chowdhury
  - first: Gerard
    full: Gerard de Melo
    id: gerard-de-melo
    last: de Melo
  - first: Matthew
    full: Matthew Stone
    id: matthew-stone
    last: Stone
  author_string: Malihe Alikhani, Sreyasi Nag Chowdhury, Gerard de Melo, Matthew Stone
  bibkey: alikhani-etal-2019-cite
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1056
  month: June
  page_first: '570'
  page_last: '575'
  pages: "570\u2013575"
  paper_id: '56'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1056.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1056.jpg
  title: 'CITE: A Corpus of Image-Text Discourse Relations'
  title_html: '<span class="acl-fixed-case">CITE</span>: A Corpus of Image-Text Discourse
    Relations'
  url: https://www.aclweb.org/anthology/N19-1056
  year: '2019'
N19-1057:
  abstract: "A typical conversation comprises of multiple turns between participants\
    \ where they go back and forth between different topics. At each user turn, dialogue\
    \ state tracking (DST) aims to estimate user\u2019s goal by processing the current\
    \ utterance. However, in many turns, users implicitly refer to the previous goal,\
    \ necessitating the use of relevant dialogue history. Nonetheless, distinguishing\
    \ relevant history is challenging and a popular method of using dialogue recency\
    \ for that is inefficient. We, therefore, propose a novel framework for DST that\
    \ identifies relevant historical context by referring to the past utterances where\
    \ a particular slot-value changes and uses that together with weighted system\
    \ utterance to identify the relevant context. Specifically, we use the current\
    \ user utterance and the most recent system utterance to determine the relevance\
    \ of a system utterance. Empirical analyses show that our method improves joint\
    \ goal accuracy by 2.75% and 2.36% on WoZ 2.0 and Multi-WoZ restaurant domain\
    \ datasets respectively over the previous state-of-the-art GLAD model."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/353455637
    type: video
    url: https://vimeo.com/353455637
  author:
  - first: Sanuj
    full: Sanuj Sharma
    id: sanuj-sharma
    last: Sharma
  - first: Prafulla Kumar
    full: Prafulla Kumar Choubey
    id: prafulla-kumar-choubey
    last: Choubey
  - first: Ruihong
    full: Ruihong Huang
    id: ruihong-huang
    last: Huang
  author_string: Sanuj Sharma, Prafulla Kumar Choubey, Ruihong Huang
  bibkey: sharma-etal-2019-improving
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1057
  month: June
  page_first: '576'
  page_last: '581'
  pages: "576\u2013581"
  paper_id: '57'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1057.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1057.jpg
  title: Improving Dialogue State Tracking by Discerning the Relevant Context
  title_html: Improving Dialogue State Tracking by Discerning the Relevant Context
  url: https://www.aclweb.org/anthology/N19-1057
  year: '2019'
N19-1058:
  abstract: "Visual Dialog is a multimodal task of answering a sequence of questions\
    \ grounded in an image (using the conversation history as context). It entails\
    \ challenges in vision, language, reasoning, and grounding. However, studying\
    \ these subtasks in isolation on large, real datasets is infeasible as it requires\
    \ prohibitively-expensive complete annotation of the \u2018state\u2019 of all\
    \ images and dialogs. We develop CLEVR-Dialog, a large diagnostic dataset for\
    \ studying multi-round reasoning in visual dialog. Specifically, we construct\
    \ a dialog grammar that is grounded in the scene graphs of the images from the\
    \ CLEVR dataset. This combination results in a dataset where all aspects of the\
    \ visual dialog are fully annotated. In total, CLEVR-Dialog contains 5 instances\
    \ of 10-round dialogs for about 85k CLEVR images, totaling to 4.25M question-answer\
    \ pairs. We use CLEVR-Dialog to benchmark performance of standard visual dialog\
    \ models; in particular, on visual coreference resolution (as a function of the\
    \ coreference distance). This is the first analysis of its kind for visual dialog\
    \ models that was not possible without this dataset. We hope the findings from\
    \ CLEVR-Dialog will help inform the development of future models for visual dialog.\
    \ Our code and dataset are publicly available."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/360533677
    type: video
    url: https://vimeo.com/360533677
  author:
  - first: Satwik
    full: Satwik Kottur
    id: satwik-kottur
    last: Kottur
  - first: "Jos\xE9 M. F."
    full: "Jos\xE9 M. F. Moura"
    id: jose-m-f-moura
    last: Moura
  - first: Devi
    full: Devi Parikh
    id: devi-parikh
    last: Parikh
  - first: Dhruv
    full: Dhruv Batra
    id: dhruv-batra
    last: Batra
  - first: Marcus
    full: Marcus Rohrbach
    id: marcus-rohrbach
    last: Rohrbach
  author_string: "Satwik Kottur, Jos\xE9 M. F. Moura, Devi Parikh, Dhruv Batra, Marcus\
    \ Rohrbach"
  bibkey: kottur-etal-2019-clevr
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1058
  month: June
  page_first: '582'
  page_last: '595'
  pages: "582\u2013595"
  paper_id: '58'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1058.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1058.jpg
  title: 'CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog'
  title_html: '<span class="acl-fixed-case">CLEVR</span>-Dialog: A Diagnostic Dataset
    for Multi-Round Reasoning in Visual Dialog'
  url: https://www.aclweb.org/anthology/N19-1058
  year: '2019'
N19-1059:
  abstract: "Most current approaches to metaphor identification use restricted linguistic\
    \ contexts, e.g. by considering only a verb\u2019s arguments or the sentence containing\
    \ a phrase. Inspired by pragmatic accounts of metaphor, we argue that broader\
    \ discourse features are crucial for better metaphor identification. We train\
    \ simple gradient boosting classifiers on representations of an utterance and\
    \ its surrounding discourse learned with a variety of document embedding methods,\
    \ obtaining near state-of-the-art results on the 2018 VU Amsterdam metaphor identification\
    \ task without the complex metaphor-specific features or deep neural architectures\
    \ employed by other systems. A qualitative analysis further confirms the need\
    \ for broader context in metaphor processing."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/360541337
    type: video
    url: https://vimeo.com/360541337
  author:
  - first: Jesse
    full: Jesse Mu
    id: jesse-mu
    last: Mu
  - first: Helen
    full: Helen Yannakoudakis
    id: helen-yannakoudakis
    last: Yannakoudakis
  - first: Ekaterina
    full: Ekaterina Shutova
    id: ekaterina-shutova
    last: Shutova
  author_string: Jesse Mu, Helen Yannakoudakis, Ekaterina Shutova
  bibkey: mu-etal-2019-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1059
  month: June
  page_first: '596'
  page_last: '601'
  pages: "596\u2013601"
  paper_id: '59'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1059.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1059.jpg
  title: 'Learning Outside the Box: Discourse-level Features Improve Metaphor Identification'
  title_html: 'Learning Outside the Box: Discourse-level Features Improve Metaphor
    Identification'
  url: https://www.aclweb.org/anthology/N19-1059
  year: '2019'
N19-1060:
  abstract: We discuss the impact of data bias on abusive language detection. We show
    that classification scores on popular datasets reported in previous work are much
    lower under realistic settings in which this bias is reduced. Such biases are
    most notably observed on datasets that are created by focused sampling instead
    of random sampling. Datasets with a higher proportion of implicit abuse are more
    affected than datasets with a lower proportion.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/347386459
    type: video
    url: https://vimeo.com/347386459
  author:
  - first: Michael
    full: Michael Wiegand
    id: michael-wiegand
    last: Wiegand
  - first: Josef
    full: Josef Ruppenhofer
    id: josef-ruppenhofer
    last: Ruppenhofer
  - first: Thomas
    full: Thomas Kleinbauer
    id: thomas-kleinbauer
    last: Kleinbauer
  author_string: Michael Wiegand, Josef Ruppenhofer, Thomas Kleinbauer
  bibkey: wiegand-etal-2019-detection
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1060
  month: June
  page_first: '602'
  page_last: '608'
  pages: "602\u2013608"
  paper_id: '60'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1060.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1060.jpg
  title: 'Detection of Abusive Language: the Problem of Biased Datasets'
  title_html: '<span class="acl-fixed-case">D</span>etection of <span class="acl-fixed-case">A</span>busive
    <span class="acl-fixed-case">L</span>anguage: the <span class="acl-fixed-case">P</span>roblem
    of <span class="acl-fixed-case">B</span>iased <span class="acl-fixed-case">D</span>atasets'
  url: https://www.aclweb.org/anthology/N19-1060
  year: '2019'
N19-1061:
  abstract: "Word embeddings are widely used in NLP for a vast range of tasks. It\
    \ was shown that word embeddings derived from text corpora reflect gender biases\
    \ in society. This phenomenon is pervasive and consistent across different word\
    \ embedding models, causing serious concern. Several recent works tackle this\
    \ problem, and propose methods for significantly reducing this gender bias in\
    \ word embeddings, demonstrating convincing results. However, we argue that this\
    \ removal is superficial. While the bias is indeed substantially reduced according\
    \ to the provided bias definition, the actual effect is mostly hiding the bias,\
    \ not removing it. The gender bias information is still reflected in the distances\
    \ between \u201Cgender-neutralized\u201D words in the debiased embeddings, and\
    \ can be recovered from them. We present a series of experiments to support this\
    \ claim, for two debiasing methods. We conclude that existing bias removal techniques\
    \ are insufficient, and should not be trusted for providing gender-neutral modeling."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/347389631
    type: video
    url: https://vimeo.com/347389631
  author:
  - first: Hila
    full: Hila Gonen
    id: hila-gonen
    last: Gonen
  - first: Yoav
    full: Yoav Goldberg
    id: yoav-goldberg
    last: Goldberg
  author_string: Hila Gonen, Yoav Goldberg
  bibkey: gonen-goldberg-2019-lipstick
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1061
  month: June
  page_first: '609'
  page_last: '614'
  pages: "609\u2013614"
  paper_id: '61'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1061.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1061.jpg
  title: 'Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in
    Word Embeddings But do not Remove Them'
  title_html: 'Lipstick on a Pig: <span class="acl-fixed-case">D</span>ebiasing Methods
    Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them'
  url: https://www.aclweb.org/anthology/N19-1061
  year: '2019'
N19-1062:
  abstract: Online texts - across genres, registers, domains, and styles - are riddled
    with human stereotypes, expressed in overt or subtle ways. Word embeddings, trained
    on these texts, perpetuate and amplify these stereotypes, and propagate biases
    to machine learning models that use word embeddings as features. In this work,
    we propose a method to debias word embeddings in multiclass settings such as race
    and religion, extending the work of (Bolukbasi et al., 2016) from the binary setting,
    such as binary gender. Next, we propose a novel methodology for the evaluation
    of multiclass debiasing. We demonstrate that our multiclass debiasing is robust
    and maintains the efficacy in standard NLP tasks.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1062.Software.zip
    type: software
    url: https://www.aclweb.org/anthology/attachments/N19-1062.Software.zip
  - filename: https://vimeo.com/347391878
    type: video
    url: https://vimeo.com/347391878
  author:
  - first: Thomas
    full: Thomas Manzini
    id: thomas-manzini
    last: Manzini
  - first: Lim
    full: Lim Yao Chong
    id: lim-yao-chong
    last: Yao Chong
  - first: Alan W
    full: Alan W Black
    id: alan-w-black
    last: Black
  - first: Yulia
    full: Yulia Tsvetkov
    id: yulia-tsvetkov
    last: Tsvetkov
  author_string: Thomas Manzini, Lim Yao Chong, Alan W Black, Yulia Tsvetkov
  bibkey: manzini-etal-2019-black
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1062
  month: June
  page_first: '615'
  page_last: '621'
  pages: "615\u2013621"
  paper_id: '62'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1062.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1062.jpg
  title: 'Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass
    Bias in Word Embeddings'
  title_html: 'Black is to Criminal as Caucasian is to Police: Detecting and Removing
    Multiclass Bias in Word Embeddings'
  url: https://www.aclweb.org/anthology/N19-1062
  year: '2019'
N19-1063:
  abstract: "The Word Embedding Association Test shows that GloVe and word2vec word\
    \ embeddings exhibit human-like implicit biases based on gender, race, and other\
    \ social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable\
    \ text representations has begun to explore sentence-level texts, with some sentence\
    \ encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding\
    \ Association Test to measure bias in sentence encoders. We then test several\
    \ sentence encoders, including state-of-the-art methods such as ELMo and BERT,\
    \ for the social biases studied in prior work and two important biases that are\
    \ difficult or impossible to test at the word level. We observe mixed results\
    \ including suspicious patterns of sensitivity that suggest the test\u2019s assumptions\
    \ may not hold in general. We conclude by proposing directions for future work\
    \ on measuring bias in sentence encoders."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1063.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1063.Supplementary.pdf
  - filename: https://vimeo.com/347394290
    type: video
    url: https://vimeo.com/347394290
  author:
  - first: Chandler
    full: Chandler May
    id: chandler-may
    last: May
  - first: Alex
    full: Alex Wang
    id: alex-wang
    last: Wang
  - first: Shikha
    full: Shikha Bordia
    id: shikha-bordia
    last: Bordia
  - first: Samuel R.
    full: Samuel R. Bowman
    id: samuel-bowman
    last: Bowman
  - first: Rachel
    full: Rachel Rudinger
    id: rachel-rudinger
    last: Rudinger
  author_string: Chandler May, Alex Wang, Shikha Bordia, Samuel R. Bowman, Rachel
    Rudinger
  bibkey: may-etal-2019-measuring
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1063
  month: June
  page_first: '622'
  page_last: '628'
  pages: "622\u2013628"
  paper_id: '63'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1063.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1063.jpg
  title: On Measuring Social Biases in Sentence Encoders
  title_html: On Measuring Social Biases in Sentence Encoders
  url: https://www.aclweb.org/anthology/N19-1063
  year: '2019'
N19-1064:
  abstract: "In this paper, we quantify, analyze and mitigate gender bias exhibited\
    \ in ELMo\u2019s contextualized word vectors. First, we conduct several intrinsic\
    \ analyses and find that (1) training data for ELMo contains significantly more\
    \ male than female entities, (2) the trained ELMo embeddings systematically encode\
    \ gender information and (3) ELMo unequally encodes gender information about male\
    \ and female entities. Then, we show that a state-of-the-art coreference system\
    \ that depends on ELMo inherits its bias and demonstrates significant bias on\
    \ the WinoBias probing corpus. Finally, we explore two methods to mitigate such\
    \ gender bias and show that the bias demonstrated on WinoBias can be eliminated."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/347396468
    type: video
    url: https://vimeo.com/347396468
  author:
  - first: Jieyu
    full: Jieyu Zhao
    id: jieyu-zhao
    last: Zhao
  - first: Tianlu
    full: Tianlu Wang
    id: tianlu-wang
    last: Wang
  - first: Mark
    full: Mark Yatskar
    id: mark-yatskar
    last: Yatskar
  - first: Ryan
    full: Ryan Cotterell
    id: ryan-cotterell
    last: Cotterell
  - first: Vicente
    full: Vicente Ordonez
    id: vicente-ordonez
    last: Ordonez
  - first: Kai-Wei
    full: Kai-Wei Chang
    id: kai-wei-chang
    last: Chang
  author_string: Jieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell, Vicente Ordonez,
    Kai-Wei Chang
  bibkey: zhao-etal-2019-gender
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1064
  month: June
  page_first: '629'
  page_last: '634'
  pages: "629\u2013634"
  paper_id: '64'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1064.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1064.jpg
  title: Gender Bias in Contextualized Word Embeddings
  title_html: Gender Bias in Contextualized Word Embeddings
  url: https://www.aclweb.org/anthology/N19-1064
  year: '2019'
N19-1065:
  abstract: When assigning quantitative labels to a dataset, different methodologies
    may rely on different scales. In particular, when assigning polarities to words
    in a sentiment lexicon, annotators may use binary, categorical, or continuous
    labels. Naturally, it is of interest to unify these labels from disparate scales
    to both achieve maximal coverage over words and to create a single, more robust
    sentiment lexicon while retaining scale coherence. We introduce a generative model
    of sentiment lexica to combine disparate scales into a common latent representation.
    We realize this model with a novel multi-view variational autoencoder (VAE), called
    SentiVAE. We evaluate our approach via a downstream text classification task involving
    nine English-Language sentiment analysis datasets; our representation outperforms
    six individual sentiment lexica, as well as a straightforward combination thereof.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/356020948
    type: video
    url: https://vimeo.com/356020948
  author:
  - first: Alexander Miserlis
    full: Alexander Miserlis Hoyle
    id: alexander-miserlis-hoyle
    last: Hoyle
  - first: Lawrence
    full: Lawrence Wolf-Sonkin
    id: lawrence-wolf-sonkin
    last: Wolf-Sonkin
  - first: Hanna
    full: Hanna Wallach
    id: hanna-wallach
    last: Wallach
  - first: Ryan
    full: Ryan Cotterell
    id: ryan-cotterell
    last: Cotterell
  - first: Isabelle
    full: Isabelle Augenstein
    id: isabelle-augenstein
    last: Augenstein
  author_string: Alexander Miserlis Hoyle, Lawrence Wolf-Sonkin, Hanna Wallach, Ryan
    Cotterell, Isabelle Augenstein
  bibkey: hoyle-etal-2019-combining
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1065
  month: June
  page_first: '635'
  page_last: '640'
  pages: "635\u2013640"
  paper_id: '65'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1065.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1065.jpg
  title: Combining Sentiment Lexica with a Multi-View Variational Autoencoder
  title_html: <span class="acl-fixed-case">C</span>ombining <span class="acl-fixed-case">S</span>entiment
    <span class="acl-fixed-case">L</span>exica with a <span class="acl-fixed-case">M</span>ulti-<span
    class="acl-fixed-case">V</span>iew <span class="acl-fixed-case">V</span>ariational
    <span class="acl-fixed-case">A</span>utoencoder
  url: https://www.aclweb.org/anthology/N19-1065
  year: '2019'
N19-1066:
  abstract: Opinion role labeling (ORL) is an important task for fine-grained opinion
    mining, which identifies important opinion arguments such as holder and target
    for a given opinion trigger. The task is highly correlative with semantic role
    labeling (SRL), which identifies important semantic arguments such as agent and
    patient for a given predicate. As predicate agents and patients usually correspond
    to opinion holders and targets respectively, SRL could be valuable for ORL. In
    this work, we propose a simple and novel method to enhance ORL by utilizing SRL,
    presenting semantic-aware word representations which are learned from SRL. The
    representations are then fed into a baseline neural ORL model as basic inputs.
    We verify the proposed method on a benchmark MPQA corpus. Experimental results
    show that the proposed method is highly effective. In addition, we compare the
    method with two representative methods of SRL integration as well, finding that
    our method can outperform the two methods significantly, achieving 1.47% higher
    F-scores than the better one.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/355756336
    type: video
    url: https://vimeo.com/355756336
  author:
  - first: Meishan
    full: Meishan Zhang
    id: meishan-zhang
    last: Zhang
  - first: Peili
    full: Peili Liang
    id: peili-liang
    last: Liang
  - first: Guohong
    full: Guohong Fu
    id: guohong-fu
    last: Fu
  author_string: Meishan Zhang, Peili Liang, Guohong Fu
  bibkey: zhang-etal-2019-enhancing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1066
  month: June
  page_first: '641'
  page_last: '646'
  pages: "641\u2013646"
  paper_id: '66'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1066.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1066.jpg
  title: Enhancing Opinion Role Labeling with Semantic-Aware Word Representations
    from Semantic Role Labeling
  title_html: Enhancing Opinion Role Labeling with Semantic-Aware Word Representations
    from Semantic Role Labeling
  url: https://www.aclweb.org/anthology/N19-1066
  year: '2019'
N19-1067:
  abstract: The development of a fictional plot is centered around characters who
    closely interact with each other forming dynamic social networks. In literature
    analysis, such networks have mostly been analyzed without particular relation
    types or focusing on roles which the characters take with respect to each other.
    We argue that an important aspect for the analysis of stories and their development
    is the emotion between characters. In this paper, we combine these aspects into
    a unified framework to classify emotional relationships of fictional characters.
    We formalize it as a new task and describe the annotation of a corpus, based on
    fan-fiction short stories. The extraction pipeline which we propose consists of
    character identification (which we treat as given by an oracle here) and the relation
    classification. For the latter, we provide results using several approaches previously
    proposed for relation identification with neural methods. The best result of 0.45
    F1 is achieved with a GRU with character position indicators on the task of predicting
    undirected emotion relations in the associated social network graph.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1067.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1067.Supplementary.pdf
  - filename: N19-1067.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1067.Presentation.pdf
  - filename: https://vimeo.com/355760337
    type: video
    url: https://vimeo.com/355760337
  author:
  - first: Evgeny
    full: Evgeny Kim
    id: evgeny-kim
    last: Kim
  - first: Roman
    full: Roman Klinger
    id: roman-klinger
    last: Klinger
  author_string: Evgeny Kim, Roman Klinger
  bibkey: kim-klinger-2019-frowning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1067
  month: June
  page_first: '647'
  page_last: '653'
  pages: "647\u2013653"
  paper_id: '67'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1067.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1067.jpg
  title: 'Frowning Frodo, Wincing Leia, and a Seriously Great Friendship: Learning
    to Classify Emotional Relationships of Fictional Characters'
  title_html: 'Frowning <span class="acl-fixed-case">F</span>rodo, Wincing <span class="acl-fixed-case">L</span>eia,
    and a Seriously Great Friendship: Learning to Classify Emotional Relationships
    of Fictional Characters'
  url: https://www.aclweb.org/anthology/N19-1067
  year: '2019'
N19-1068:
  abstract: "Authorship verification is the problem of inferring whether two texts\
    \ were written by the same author. For this task, unmasking is one of the most\
    \ robust approaches as of today with the major shortcoming of only being applicable\
    \ to book-length texts. In this paper, we present a generalized unmasking approach\
    \ which allows for authorship verification of texts as short as four printed pages\
    \ with very high precision at an adjustable recall tradeoff. Our generalized approach\
    \ therefore reduces the required material by orders of magnitude, making unmasking\
    \ applicable to authorship cases of more practical proportions. The new approach\
    \ is on par with other state-of-the-art techniques that are optimized for texts\
    \ of this length: it achieves accuracies of 75\u201380%, while also allowing for\
    \ easy adjustment to forensic scenarios that require higher levels of confidence\
    \ in the classification."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/355735934
    type: video
    url: https://vimeo.com/355735934
  author:
  - first: Janek
    full: Janek Bevendorff
    id: janek-bevendorff
    last: Bevendorff
  - first: Benno
    full: Benno Stein
    id: benno-stein
    last: Stein
  - first: Matthias
    full: Matthias Hagen
    id: matthias-hagen
    last: Hagen
  - first: Martin
    full: Martin Potthast
    id: martin-potthast
    last: Potthast
  author_string: Janek Bevendorff, Benno Stein, Matthias Hagen, Martin Potthast
  bibkey: bevendorff-etal-2019-generalizing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1068
  month: June
  page_first: '654'
  page_last: '659'
  pages: "654\u2013659"
  paper_id: '68'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1068.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1068.jpg
  title: Generalizing Unmasking for Short Texts
  title_html: Generalizing Unmasking for Short Texts
  url: https://www.aclweb.org/anthology/N19-1068
  year: '2019'
N19-1069:
  abstract: "The automatic detection of satire vs. regular news is relevant for downstream\
    \ applications (for instance, knowledge base population) and to improve the understanding\
    \ of linguistic characteristics of satire. Recent approaches build upon corpora\
    \ which have been labeled automatically based on article sources. We hypothesize\
    \ that this encourages the models to learn characteristics for different publication\
    \ sources (e.g., \u201CThe Onion\u201D vs. \u201CThe Guardian\u201D) rather than\
    \ characteristics of satire, leading to poor generalization performance to unseen\
    \ publication sources. We therefore propose a novel model for satire detection\
    \ with an adversarial component to control for the confounding variable of publication\
    \ source. On a large novel data set collected from German news (which we make\
    \ available to the research community), we observe comparable satire classification\
    \ performance and, as desired, a considerable drop in publication classification\
    \ performance with adversarial training. Our analysis shows that the adversarial\
    \ component is crucial for the model to learn to pay attention to linguistic properties\
    \ of satire."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1069.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1069.Supplementary.pdf
  - filename: N19-1069.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1069.Presentation.pdf
  - filename: https://vimeo.com/355743083
    type: video
    url: https://vimeo.com/355743083
  author:
  - first: Robert
    full: Robert McHardy
    id: robert-mchardy
    last: McHardy
  - first: Heike
    full: Heike Adel
    id: heike-adel
    last: Adel
  - first: Roman
    full: Roman Klinger
    id: roman-klinger
    last: Klinger
  author_string: Robert McHardy, Heike Adel, Roman Klinger
  bibkey: mchardy-etal-2019-adversarial
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1069
  month: June
  page_first: '660'
  page_last: '665'
  pages: "660\u2013665"
  paper_id: '69'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1069.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1069.jpg
  title: 'Adversarial Training for Satire Detection: Controlling for Confounding Variables'
  title_html: 'Adversarial Training for Satire Detection: Controlling for Confounding
    Variables'
  url: https://www.aclweb.org/anthology/N19-1069
  year: '2019'
N19-1070:
  abstract: "Authors\u2019 keyphrases assigned to scientific articles are essential\
    \ for recognizing content and topic aspects. Most of the proposed supervised and\
    \ unsupervised methods for keyphrase generation are unable to produce terms that\
    \ are valuable but do not appear in the text. In this paper, we explore the possibility\
    \ of considering the keyphrase string as an abstractive summary of the title and\
    \ the abstract. First, we collect, process and release a large dataset of scientific\
    \ paper metadata that contains 2.2 million records. Then we experiment with popular\
    \ text summarization neural architectures. Despite using advanced deep learning\
    \ models, large quantities of data and many days of computation, our systematic\
    \ evaluation on four test datasets reveals that the explored text summarization\
    \ methods could not produce better keyphrases than the simpler unsupervised methods,\
    \ or the existing supervised ones."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/353458816
    type: video
    url: https://vimeo.com/353458816
  author:
  - first: Erion
    full: "Erion \xC7ano"
    id: erion-cano
    last: "\xC7ano"
  - first: "Ond\u0159ej"
    full: "Ond\u0159ej Bojar"
    id: ondrej-bojar
    last: Bojar
  author_string: "Erion \xC7ano, Ond\u0159ej Bojar"
  bibkey: cano-bojar-2019-keyphrase
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1070
  month: June
  page_first: '666'
  page_last: '672'
  pages: "666\u2013672"
  paper_id: '70'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1070.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1070.jpg
  title: 'Keyphrase Generation: A Text Summarization Struggle'
  title_html: 'Keyphrase Generation: A Text Summarization Struggle'
  url: https://www.aclweb.org/anthology/N19-1070
  year: '2019'
N19-1071:
  abstract: "Neural sequence-to-sequence models are currently the dominant approach\
    \ in several natural language processing tasks, but require large parallel corpora.\
    \ We present a sequence-to-sequence-to-sequence autoencoder (SEQ\u02C63), consisting\
    \ of two chained encoder-decoder pairs, with words used as a sequence of discrete\
    \ latent variables. We apply the proposed model to unsupervised abstractive sentence\
    \ compression, where the first and last sequences are the input and reconstructed\
    \ sentences, respectively, while the middle sequence is the compressed sentence.\
    \ Constraining the length of the latent word sequences forces the model to distill\
    \ important information from the input. A pretrained language model, acting as\
    \ a prior over the latent sequences, encourages the compressed sentences to be\
    \ human-readable. Continuous relaxations enable us to sample from categorical\
    \ distributions, allowing gradient-based optimization, unlike alternatives that\
    \ rely on reinforcement learning. The proposed model does not require parallel\
    \ text-summary pairs, achieving promising results in unsupervised sentence compression\
    \ on benchmark datasets."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1071.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1071.Presentation.pdf
  - filename: https://vimeo.com/353462534
    type: video
    url: https://vimeo.com/353462534
  author:
  - first: Christos
    full: Christos Baziotis
    id: christos-baziotis
    last: Baziotis
  - first: Ion
    full: Ion Androutsopoulos
    id: ion-androutsopoulos
    last: Androutsopoulos
  - first: Ioannis
    full: Ioannis Konstas
    id: ioannis-konstas
    last: Konstas
  - first: Alexandros
    full: Alexandros Potamianos
    id: alexandros-potamianos
    last: Potamianos
  author_string: Christos Baziotis, Ion Androutsopoulos, Ioannis Konstas, Alexandros
    Potamianos
  bibkey: baziotis-etal-2019-seq
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1071
  month: June
  page_first: '673'
  page_last: '681'
  pages: "673\u2013681"
  paper_id: '71'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1071.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1071.jpg
  title: "SEQ\u02C63: Differentiable Sequence-to-Sequence-to-Sequence Autoencoder\
    \ for Unsupervised Abstractive Sentence Compression"
  title_html: "<span class=\"acl-fixed-case\">SEQ</span>\u02C63: Differentiable Sequence-to-Sequence-to-Sequence\
    \ Autoencoder for Unsupervised Abstractive Sentence Compression"
  url: https://www.aclweb.org/anthology/N19-1071
  year: '2019'
N19-1072:
  abstract: Conducting a manual evaluation is considered an essential part of summary
    evaluation methodology. Traditionally, the Pyramid protocol, which exhaustively
    compares system summaries to references, has been perceived as very reliable,
    providing objective scores. Yet, due to the high cost of the Pyramid method and
    the required expertise, researchers resorted to cheaper and less thorough manual
    evaluation methods, such as Responsiveness and pairwise comparison, attainable
    via crowdsourcing. We revisit the Pyramid approach, proposing a lightweight sampling-based
    version that is crowdsourcable. We analyze the performance of our method in comparison
    to original expert-based Pyramid evaluations, showing higher correlation relative
    to the common Responsiveness method. We release our crowdsourced Summary-Content-Units,
    along with all crowdsourcing scripts, for future evaluations.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/353467177
    type: video
    url: https://vimeo.com/353467177
  author:
  - first: Ori
    full: Ori Shapira
    id: ori-shapira
    last: Shapira
  - first: David
    full: David Gabay
    id: david-gabay
    last: Gabay
  - first: Yang
    full: Yang Gao
    id: yang-gao
    last: Gao
  - first: Hadar
    full: Hadar Ronen
    id: hadar-ronen
    last: Ronen
  - first: Ramakanth
    full: Ramakanth Pasunuru
    id: ramakanth-pasunuru
    last: Pasunuru
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  - first: Yael
    full: Yael Amsterdamer
    id: yael-amsterdamer
    last: Amsterdamer
  - first: Ido
    full: Ido Dagan
    id: ido-dagan
    last: Dagan
  author_string: Ori Shapira, David Gabay, Yang Gao, Hadar Ronen, Ramakanth Pasunuru,
    Mohit Bansal, Yael Amsterdamer, Ido Dagan
  bibkey: shapira-etal-2019-crowdsourcing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1072
  month: June
  page_first: '682'
  page_last: '687'
  pages: "682\u2013687"
  paper_id: '72'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1072.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1072.jpg
  title: Crowdsourcing Lightweight Pyramids for Manual Summary Evaluation
  title_html: Crowdsourcing Lightweight Pyramids for Manual Summary Evaluation
  url: https://www.aclweb.org/anthology/N19-1072
  year: '2019'
N19-1073:
  abstract: 'Serial recall experiments study the ability of humans to recall words
    in the order in which they occurred. The following serial recall effects are generally
    investigated in studies with humans: word length and frequency, primacy and recency,
    semantic confusion, repetition, and transposition effects. In this research, we
    investigate neural language models in the context of these serial recall effects.
    Our work provides a framework to better understand and analyze neural language
    models and opens a new window to develop accurate language models.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/353471286
    type: video
    url: https://vimeo.com/353471286
  author:
  - first: Hassan
    full: Hassan Hajipoor
    id: hassan-hajipoor
    last: Hajipoor
  - first: Hadi
    full: Hadi Amiri
    id: hadi-amiri
    last: Amiri
  - first: Maseud
    full: Maseud Rahgozar
    id: maseud-rahgozar
    last: Rahgozar
  - first: Farhad
    full: Farhad Oroumchian
    id: farhad-oroumchian
    last: Oroumchian
  author_string: Hassan Hajipoor, Hadi Amiri, Maseud Rahgozar, Farhad Oroumchian
  bibkey: hajipoor-etal-2019-serial
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1073
  month: June
  page_first: '688'
  page_last: '694'
  pages: "688\u2013694"
  paper_id: '73'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1073.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1073.jpg
  title: Serial Recall Effects in Neural Language Modeling
  title_html: Serial Recall Effects in Neural Language Modeling
  url: https://www.aclweb.org/anthology/N19-1073
  year: '2019'
N19-1074:
  abstract: Concept map-based multi-document summarization has recently been proposed
    as a variant of the traditional summarization task with graph-structured summaries.
    As shown by previous work, the grouping of coreferent concept mentions across
    documents is a crucial subtask of it. However, while the current state-of-the-art
    method suggested a new grouping method that was shown to improve the summary quality,
    its use of pairwise comparisons leads to polynomial runtime complexity that prohibits
    the application to large document collections. In this paper, we propose two alternative
    grouping techniques based on locality sensitive hashing, approximate nearest neighbor
    search and a fast clustering algorithm. They exhibit linear and log-linear runtime
    complexity, making them much more scalable. We report experimental results that
    confirm the improved runtime behavior while also showing that the quality of the
    summary concept maps remains comparable.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/353477256
    type: video
    url: https://vimeo.com/353477256
  author:
  - first: Tobias
    full: Tobias Falke
    id: tobias-falke
    last: Falke
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Tobias Falke, Iryna Gurevych
  bibkey: falke-gurevych-2019-fast
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1074
  month: June
  page_first: '695'
  page_last: '700'
  pages: "695\u2013700"
  paper_id: '74'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1074.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1074.jpg
  title: Fast Concept Mention Grouping for Concept Map-based Multi-Document Summarization
  title_html: Fast Concept Mention Grouping for Concept Map-based Multi-Document Summarization
  url: https://www.aclweb.org/anthology/N19-1074
  year: '2019'
N19-1075:
  abstract: We introduce a new syntax-aware model for dependency-based semantic role
    labeling that outperforms syntax-agnostic models for English and Spanish. We use
    a BiLSTM to tag the text with supertags extracted from dependency parses, and
    we feed these supertags, along with words and parts of speech, into a deep highway
    BiLSTM for semantic role labeling. Our model combines the strengths of earlier
    models that performed SRL on the basis of a full dependency parse with more recent
    models that use no syntactic information at all. Our local and non-ensemble model
    achieves state-of-the-art performance on the CoNLL 09 English and Spanish datasets.
    SRL models benefit from syntactic information, and we show that supertagging is
    a simple, powerful, and robust way to incorporate syntax into a neural SRL system.
  address: Minneapolis, Minnesota
  author:
  - first: Jungo
    full: Jungo Kasai
    id: jungo-kasai
    last: Kasai
  - first: Dan
    full: Dan Friedman
    id: dan-friedman
    last: Friedman
  - first: Robert
    full: Robert Frank
    id: robert-frank
    last: Frank
  - first: Dragomir
    full: Dragomir Radev
    id: dragomir-radev
    last: Radev
  - first: Owen
    full: Owen Rambow
    id: owen-rambow
    last: Rambow
  author_string: Jungo Kasai, Dan Friedman, Robert Frank, Dragomir Radev, Owen Rambow
  bibkey: kasai-etal-2019-syntax
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1075
  month: June
  page_first: '701'
  page_last: '709'
  pages: "701\u2013709"
  paper_id: '75'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1075.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1075.jpg
  title: Syntax-aware Neural Semantic Role Labeling with Supertags
  title_html: Syntax-aware Neural Semantic Role Labeling with Supertags
  url: https://www.aclweb.org/anthology/N19-1075
  year: '2019'
N19-1076:
  abstract: We propose a novel transition-based algorithm that straightforwardly parses
    sentences from left to right by building n attachments, with n being the length
    of the input sentence. Similarly to the recent stack-pointer parser by Ma et al.
    (2018), we use the pointer network framework that, given a word, can directly
    point to a position from the sentence. However, our left-to-right approach is
    simpler than the original top-down stack-pointer parser (not requiring a stack)
    and reduces transition sequence length in half, from 2n-1 actions to n. This results
    in a quadratic non-projective parser that runs twice as fast as the original while
    achieving the best accuracy to date on the English PTB dataset (96.04% UAS, 94.43%
    LAS) among fully-supervised single-model dependency parsers, and improves over
    the former top-down transition system in the majority of languages tested.
  address: Minneapolis, Minnesota
  author:
  - first: Daniel
    full: "Daniel Fern\xE1ndez-Gonz\xE1lez"
    id: daniel-fernandez-gonzalez
    last: "Fern\xE1ndez-Gonz\xE1lez"
  - first: Carlos
    full: "Carlos G\xF3mez-Rodr\xEDguez"
    id: carlos-gomez-rodriguez
    last: "G\xF3mez-Rodr\xEDguez"
  author_string: "Daniel Fern\xE1ndez-Gonz\xE1lez, Carlos G\xF3mez-Rodr\xEDguez"
  bibkey: fernandez-gonzalez-gomez-rodriguez-2019-left
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1076
  month: June
  page_first: '710'
  page_last: '716'
  pages: "710\u2013716"
  paper_id: '76'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1076.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1076.jpg
  title: Left-to-Right Dependency Parsing with Pointer Networks
  title_html: Left-to-Right Dependency Parsing with Pointer Networks
  url: https://www.aclweb.org/anthology/N19-1076
  year: '2019'
N19-1077:
  abstract: We recast dependency parsing as a sequence labeling problem, exploring
    several encodings of dependency trees as labels. While dependency parsing by means
    of sequence labeling had been attempted in existing work, results suggested that
    the technique was impractical. We show instead that with a conventional BILSTM-based
    model it is possible to obtain fast and accurate parsers. These parsers are conceptually
    simple, not needing traditional parsing algorithms or auxiliary structures. However,
    experiments on the PTB and a sample of UD treebanks show that they provide a good
    speed-accuracy tradeoff, with results competitive with more complex approaches.
  address: Minneapolis, Minnesota
  author:
  - first: Michalina
    full: Michalina Strzyz
    id: michalina-strzyz
    last: Strzyz
  - first: David
    full: David Vilares
    id: david-vilares
    last: Vilares
  - first: Carlos
    full: "Carlos G\xF3mez-Rodr\xEDguez"
    id: carlos-gomez-rodriguez
    last: "G\xF3mez-Rodr\xEDguez"
  author_string: "Michalina Strzyz, David Vilares, Carlos G\xF3mez-Rodr\xEDguez"
  bibkey: strzyz-etal-2019-viable
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1077
  month: June
  page_first: '717'
  page_last: '723'
  pages: "717\u2013723"
  paper_id: '77'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1077.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1077.jpg
  title: Viable Dependency Parsing as Sequence Labeling
  title_html: Viable Dependency Parsing as Sequence Labeling
  url: https://www.aclweb.org/anthology/N19-1077
  year: '2019'
N19-1078:
  abstract: "Contextual string embeddings are a recent type of contextualized word\
    \ embedding that were shown to yield state-of-the-art results when utilized in\
    \ a range of sequence labeling tasks. They are based on character-level language\
    \ models which treat text as distributions over characters and are capable of\
    \ generating embeddings for any string of characters within any textual context.\
    \ However, such purely character-based approaches struggle to produce meaningful\
    \ embeddings if a rare string is used in a underspecified context. To address\
    \ this drawback, we propose a method in which we dynamically aggregate contextualized\
    \ embeddings of each unique string that we encounter. We then use a pooling operation\
    \ to distill a \u201Dglobal\u201D word representation from all contextualized\
    \ instances. We evaluate these \u201Dpooled contextualized embeddings\u201D on\
    \ common named entity recognition (NER) tasks such as CoNLL-03 and WNUT and show\
    \ that our approach significantly improves the state-of-the-art for NER. We make\
    \ all code and pre-trained models available to the research community for use\
    \ and reproduction."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/360549287
    type: video
    url: https://vimeo.com/360549287
  author:
  - first: Alan
    full: Alan Akbik
    id: alan-akbik
    last: Akbik
  - first: Tanja
    full: Tanja Bergmann
    id: tanja-bergmann
    last: Bergmann
  - first: Roland
    full: Roland Vollgraf
    id: roland-vollgraf
    last: Vollgraf
  author_string: Alan Akbik, Tanja Bergmann, Roland Vollgraf
  bibkey: akbik-etal-2019-pooled
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1078
  month: June
  page_first: '724'
  page_last: '728'
  pages: "724\u2013728"
  paper_id: '78'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1078.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1078.jpg
  title: Pooled Contextualized Embeddings for Named Entity Recognition
  title_html: Pooled Contextualized Embeddings for Named Entity Recognition
  url: https://www.aclweb.org/anthology/N19-1078
  year: '2019'
N19-1079:
  abstract: Supervised approaches to named entity recognition (NER) are largely developed
    based on the assumption that the training data is fully annotated with named entity
    information. However, in practice, annotated data can often be imperfect with
    one typical issue being the training data may contain incomplete annotations.
    We highlight several pitfalls associated with learning under such a setup in the
    context of NER and identify limitations associated with existing approaches, proposing
    a novel yet easy-to-implement approach for recognizing named entities with incomplete
    data annotations. We demonstrate the effectiveness of our approach through extensive
    experiments.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1079.Supplementary.zip
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1079.Supplementary.zip
  - filename: N19-1079.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1079.Presentation.pdf
  - filename: https://vimeo.com/360565437
    type: video
    url: https://vimeo.com/360565437
  author:
  - first: Zhanming
    full: Zhanming Jie
    id: zhanming-jie
    last: Jie
  - first: Pengjun
    full: Pengjun Xie
    id: pengjun-xie
    last: Xie
  - first: Wei
    full: Wei Lu
    id: wei-lu
    last: Lu
  - first: Ruixue
    full: Ruixue Ding
    id: ruixue-ding
    last: Ding
  - first: Linlin
    full: Linlin Li
    id: linlin-li
    last: Li
  author_string: Zhanming Jie, Pengjun Xie, Wei Lu, Ruixue Ding, Linlin Li
  bibkey: jie-etal-2019-better
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1079
  month: June
  page_first: '729'
  page_last: '734'
  pages: "729\u2013734"
  paper_id: '79'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1079.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1079.jpg
  title: Better Modeling of Incomplete Annotations for Named Entity Recognition
  title_html: Better Modeling of Incomplete Annotations for Named Entity Recognition
  url: https://www.aclweb.org/anthology/N19-1079
  year: '2019'
N19-1080:
  abstract: "The goal of event detection (ED) is to detect the occurrences of events\
    \ and categorize them. Previous work solved this task by recognizing and classifying\
    \ event triggers, which is defined as the word or phrase that most clearly expresses\
    \ an event occurrence. As a consequence, existing approaches required both annotated\
    \ triggers and event types in training data. However, triggers are nonessential\
    \ to event detection, and it is time-consuming for annotators to pick out the\
    \ \u201Cmost clearly\u201D word from a given sentence, especially from a long\
    \ sentence. The expensive annotation of training corpus limits the application\
    \ of existing approaches. To reduce manual effort, we explore detecting events\
    \ without triggers. In this work, we propose a novel framework dubbed as Type-aware\
    \ Bias Neural Network with Attention Mechanisms (TBNNAM), which encodes the representation\
    \ of a sentence based on target event types. Experimental results demonstrate\
    \ the effectiveness. Remarkably, the proposed approach even achieves competitive\
    \ performances compared with state-of-the-arts that used annotated triggers."
  address: Minneapolis, Minnesota
  author:
  - first: Shulin
    full: Shulin Liu
    id: shulin-liu
    last: Liu
  - first: Yang
    full: Yang Li
    id: yang-li
    last: Li
  - first: Feng
    full: Feng Zhang
    id: feng-zhang
    last: Zhang
  - first: Tao
    full: Tao Yang
    id: tao-yang
    last: Yang
  - first: Xinpeng
    full: Xinpeng Zhou
    id: xinpeng-zhou
    last: Zhou
  author_string: Shulin Liu, Yang Li, Feng Zhang, Tao Yang, Xinpeng Zhou
  bibkey: liu-etal-2019-event
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1080
  month: June
  page_first: '735'
  page_last: '744'
  pages: "735\u2013744"
  paper_id: '80'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1080.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1080.jpg
  title: Event Detection without Triggers
  title_html: Event Detection without Triggers
  url: https://www.aclweb.org/anthology/N19-1080
  year: '2019'
N19-1081:
  abstract: This paper introduces improved methods for sub-event detection in social
    media streams, by applying neural sequence models not only on the level of individual
    posts, but also directly on the stream level. Current approaches to identify sub-events
    within a given event, such as a goal during a soccer match, essentially do not
    exploit the sequential nature of social media streams. We address this shortcoming
    by framing the sub-event detection problem in social media streams as a sequence
    labeling task and adopt a neural sequence architecture that explicitly accounts
    for the chronological order of posts. Specifically, we (i) establish a neural
    baseline that outperforms a graph-based state-of-the-art method for binary sub-event
    detection (2.7% micro-F1 improvement), as well as (ii) demonstrate superiority
    of a recurrent neural network model on the posts sequence level for labeled sub-events
    (2.4% bin-level F1 improvement over non-sequential models).
  address: Minneapolis, Minnesota
  author:
  - first: Giannis
    full: Giannis Bekoulis
    id: giannis-bekoulis
    last: Bekoulis
  - first: Johannes
    full: Johannes Deleu
    id: johannes-deleu
    last: Deleu
  - first: Thomas
    full: Thomas Demeester
    id: thomas-demeester
    last: Demeester
  - first: Chris
    full: Chris Develder
    id: chris-develder
    last: Develder
  author_string: Giannis Bekoulis, Johannes Deleu, Thomas Demeester, Chris Develder
  bibkey: bekoulis-etal-2019-sub
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1081
  month: June
  page_first: '745'
  page_last: '750'
  pages: "745\u2013750"
  paper_id: '81'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1081.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1081.jpg
  title: Sub-event detection from twitter streams as a sequence labeling problem
  title_html: Sub-event detection from twitter streams as a sequence labeling problem
  url: https://www.aclweb.org/anthology/N19-1081
  year: '2019'
N19-1082:
  abstract: "Most modern Information Extraction (IE) systems are implemented as sequential\
    \ taggers and only model local dependencies. Non-local and non-sequential context\
    \ is, however, a valuable source of information to improve predictions. In this\
    \ paper, we introduce GraphIE, a framework that operates over a graph representing\
    \ a broad set of dependencies between textual units (i.e. words or sentences).\
    \ The algorithm propagates information between connected nodes through graph convolutions,\
    \ generating a richer representation that can be exploited to improve word-level\
    \ predictions. Evaluation on three different tasks \u2014 namely textual, social\
    \ media and visual information extraction \u2014 shows that GraphIE consistently\
    \ outperforms the state-of-the-art sequence tagging model by a significant margin."
  address: Minneapolis, Minnesota
  author:
  - first: Yujie
    full: Yujie Qian
    id: yujie-qian
    last: Qian
  - first: Enrico
    full: Enrico Santus
    id: enrico-santus
    last: Santus
  - first: Zhijing
    full: Zhijing Jin
    id: zhijing-jin
    last: Jin
  - first: Jiang
    full: Jiang Guo
    id: jiang-guo
    last: Guo
  - first: Regina
    full: Regina Barzilay
    id: regina-barzilay
    last: Barzilay
  author_string: Yujie Qian, Enrico Santus, Zhijing Jin, Jiang Guo, Regina Barzilay
  bibkey: qian-etal-2019-graphie
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1082
  month: June
  page_first: '751'
  page_last: '761'
  pages: "751\u2013761"
  paper_id: '82'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1082.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1082.jpg
  title: 'GraphIE: A Graph-Based Framework for Information Extraction'
  title_html: '<span class="acl-fixed-case">G</span>raph<span class="acl-fixed-case">IE</span>:
    A Graph-Based Framework for Information Extraction'
  url: https://www.aclweb.org/anthology/N19-1082
  year: '2019'
N19-1083:
  abstract: 'In this paper, we consider advancing web-scale knowledge extraction and
    alignment by integrating OpenIE extractions in the form of (subject, predicate,
    object) triples with Knowledge Bases (KB). Traditional techniques from universal
    schema and from schema mapping fall in two extremes: either they perform instance-level
    inference relying on embedding for (subject, object) pairs, thus cannot handle
    pairs absent in any existing triples; or they perform predicate-level mapping
    and completely ignore background evidence from individual entities, thus cannot
    achieve satisfying quality. We propose OpenKI to handle sparsity of OpenIE extractions
    by performing instance-level inference: for each entity, we encode the rich information
    in its neighborhood in both KB and OpenIE extractions, and leverage this information
    in relation inference by exploring different methods of aggregation and attention.
    In order to handle unseen entities, our model is designed without creating entity-specific
    parameters. Extensive experiments show that this method not only significantly
    improves state-of-the-art for conventional OpenIE extractions like ReVerb, but
    also boosts the performance on OpenIE from semi-structured data, where new entity
    pairs are abundant and data are fairly sparse.'
  address: Minneapolis, Minnesota
  author:
  - first: Dongxu
    full: Dongxu Zhang
    id: dongxu-zhang
    last: Zhang
  - first: Subhabrata
    full: Subhabrata Mukherjee
    id: subhabrata-mukherjee
    last: Mukherjee
  - first: Colin
    full: Colin Lockard
    id: colin-lockard
    last: Lockard
  - first: Luna
    full: Luna Dong
    id: luna-dong
    last: Dong
  - first: Andrew
    full: Andrew McCallum
    id: andrew-mccallum
    last: McCallum
  author_string: Dongxu Zhang, Subhabrata Mukherjee, Colin Lockard, Luna Dong, Andrew
    McCallum
  bibkey: zhang-etal-2019-openki
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1083
  month: June
  page_first: '762'
  page_last: '772'
  pages: "762\u2013772"
  paper_id: '83'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1083.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1083.jpg
  title: 'OpenKI: Integrating Open Information Extraction and Knowledge Bases with
    Relation Inference'
  title_html: '<span class="acl-fixed-case">O</span>pen<span class="acl-fixed-case">KI</span>:
    <span class="acl-fixed-case">I</span>ntegrating <span class="acl-fixed-case">O</span>pen
    <span class="acl-fixed-case">I</span>nformation <span class="acl-fixed-case">E</span>xtraction
    and <span class="acl-fixed-case">K</span>nowledge <span class="acl-fixed-case">B</span>ases
    with <span class="acl-fixed-case">R</span>elation <span class="acl-fixed-case">I</span>nference'
  url: https://www.aclweb.org/anthology/N19-1083
  year: '2019'
N19-1084:
  abstract: Existing entity typing systems usually exploit the type hierarchy provided
    by knowledge base (KB) schema to model label correlations and thus improve the
    overall performance. Such techniques, however, are not directly applicable to
    more open and practical scenarios where the type set is not restricted by KB schema
    and includes a vast number of free-form types. To model the underlying label correlations
    without access to manually annotated label structures, we introduce a novel label-relational
    inductive bias, represented by a graph propagation layer that effectively encodes
    both global label co-occurrence statistics and word-level similarities. On a large
    dataset with over 10,000 free-form types, the graph-enhanced model equipped with
    an attention-based matching module is able to achieve a much higher recall score
    while maintaining a high-level precision. Specifically, it achieves a 15.3% relative
    F1 improvement and also less inconsistency in the outputs. We further show that
    a simple modification of our proposed graph layer can also improve the performance
    on a conventional and widely-tested dataset that only includes KB-schema types.
  address: Minneapolis, Minnesota
  author:
  - first: Wenhan
    full: Wenhan Xiong
    id: wenhan-xiong
    last: Xiong
  - first: Jiawei
    full: Jiawei Wu
    id: jiawei-wu
    last: Wu
  - first: Deren
    full: Deren Lei
    id: deren-lei
    last: Lei
  - first: Mo
    full: Mo Yu
    id: mo-yu
    last: Yu
  - first: Shiyu
    full: Shiyu Chang
    id: shiyu-chang
    last: Chang
  - first: Xiaoxiao
    full: Xiaoxiao Guo
    id: xiaoxiao-guo
    last: Guo
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  author_string: Wenhan Xiong, Jiawei Wu, Deren Lei, Mo Yu, Shiyu Chang, Xiaoxiao
    Guo, William Yang Wang
  bibkey: xiong-etal-2019-imposing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1084
  month: June
  page_first: '773'
  page_last: '784'
  pages: "773\u2013784"
  paper_id: '84'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1084.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1084.jpg
  title: Imposing Label-Relational Inductive Bias for Extremely Fine-Grained Entity
    Typing
  title_html: Imposing Label-Relational Inductive Bias for Extremely Fine-Grained
    Entity Typing
  url: https://www.aclweb.org/anthology/N19-1084
  year: '2019'
N19-1085:
  abstract: Argument compatibility is a linguistic condition that is frequently incorporated
    into modern event coreference resolution systems. If two event mentions have incompatible
    arguments in any of the argument roles, they cannot be coreferent. On the other
    hand, if these mentions have compatible arguments, then this may be used as information
    towards deciding their coreferent status. One of the key challenges in leveraging
    argument compatibility lies in the paucity of labeled data. In this work, we propose
    a transfer learning framework for event coreference resolution that utilizes a
    large amount of unlabeled data to learn argument compatibility of event mentions.
    In addition, we adopt an interactive inference network based model to better capture
    the compatible and incompatible relations between the context words of event mentions.
    Our experiments on the KBP 2017 English dataset confirm the effectiveness of our
    model in learning argument compatibility, which in turn improves the performance
    of the overall event coreference model.
  address: Minneapolis, Minnesota
  author:
  - first: Yin Jou
    full: Yin Jou Huang
    id: yin-jou-huang
    last: Huang
  - first: Jing
    full: Jing Lu
    id: jing-lu
    last: Lu
  - first: Sadao
    full: Sadao Kurohashi
    id: sadao-kurohashi
    last: Kurohashi
  - first: Vincent
    full: Vincent Ng
    id: vincent-ng
    last: Ng
  author_string: Yin Jou Huang, Jing Lu, Sadao Kurohashi, Vincent Ng
  bibkey: huang-etal-2019-improving
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1085
  month: June
  page_first: '785'
  page_last: '795'
  pages: "785\u2013795"
  paper_id: '85'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1085.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1085.jpg
  title: Improving Event Coreference Resolution by Learning Argument Compatibility
    from Unlabeled Data
  title_html: Improving Event Coreference Resolution by Learning Argument Compatibility
    from Unlabeled Data
  url: https://www.aclweb.org/anthology/N19-1085
  year: '2019'
N19-1086:
  abstract: Conventional approaches to relation extraction usually require a fixed
    set of pre-defined relations. Such requirement is hard to meet in many real applications,
    especially when new data and relations are emerging incessantly and it is computationally
    expensive to store all data and re-train the whole model every time new data and
    relations come in. We formulate such challenging problem as lifelong relation
    extraction and investigate memory-efficient incremental learning methods without
    catastrophically forgetting knowledge learned from previous tasks. We first investigate
    a modified version of the stochastic gradient methods with a replay memory, which
    surprisingly outperforms recent state-of-the-art lifelong learning methods. We
    further propose to improve this approach to alleviate the forgetting problem by
    anchoring the sentence embedding space. Specifically, we utilize an explicit alignment
    model to mitigate the sentence embedding distortion of learned model when training
    on new data and new relations. Experiment results on multiple benchmarks show
    that our proposed method significantly outperforms the state-of-the-art lifelong
    learning approaches.
  address: Minneapolis, Minnesota
  author:
  - first: Hong
    full: Hong Wang
    id: hong-wang
    last: Wang
  - first: Wenhan
    full: Wenhan Xiong
    id: wenhan-xiong
    last: Xiong
  - first: Mo
    full: Mo Yu
    id: mo-yu
    last: Yu
  - first: Xiaoxiao
    full: Xiaoxiao Guo
    id: xiaoxiao-guo
    last: Guo
  - first: Shiyu
    full: Shiyu Chang
    id: shiyu-chang
    last: Chang
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  author_string: Hong Wang, Wenhan Xiong, Mo Yu, Xiaoxiao Guo, Shiyu Chang, William
    Yang Wang
  bibkey: wang-etal-2019-sentence
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1086
  month: June
  page_first: '796'
  page_last: '806'
  pages: "796\u2013806"
  paper_id: '86'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1086.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1086.jpg
  title: Sentence Embedding Alignment for Lifelong Relation Extraction
  title_html: Sentence Embedding Alignment for Lifelong Relation Extraction
  url: https://www.aclweb.org/anthology/N19-1086
  year: '2019'
N19-1087:
  abstract: Fine-grained Entity typing (FGET) is the task of assigning a fine-grained
    type from a hierarchy to entity mentions in the text. As the taxonomy of types
    evolves continuously, it is desirable for an entity typing system to be able to
    recognize novel types without additional training. This work proposes a zero-shot
    entity typing approach that utilizes the type description available from Wikipedia
    to build a distributed semantic representation of the types. During training,
    our system learns to align the entity mentions and their corresponding type representations
    on the known types. At test time, any new type can be incorporated into the system
    given its Wikipedia descriptions. We evaluate our approach on FIGER, a public
    benchmark entity tying dataset. Because the existing test set of FIGER covers
    only a small portion of the fine-grained types, we create a new test set by manually
    annotating a portion of the noisy training data. Our experiments demonstrate the
    effectiveness of the proposed method in recognizing novel types that are not present
    in the training data.
  address: Minneapolis, Minnesota
  author:
  - first: Rasha
    full: Rasha Obeidat
    id: rasha-obeidat
    last: Obeidat
  - first: Xiaoli
    full: Xiaoli Fern
    id: xiaoli-fern
    last: Fern
  - first: Hamed
    full: Hamed Shahbazi
    id: hamed-shahbazi
    last: Shahbazi
  - first: Prasad
    full: Prasad Tadepalli
    id: prasad-tadepalli
    last: Tadepalli
  author_string: Rasha Obeidat, Xiaoli Fern, Hamed Shahbazi, Prasad Tadepalli
  bibkey: obeidat-etal-2019-description
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1087
  month: June
  page_first: '807'
  page_last: '814'
  pages: "807\u2013814"
  paper_id: '87'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1087.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1087.jpg
  title: Description-Based Zero-shot Fine-Grained Entity Typing
  title_html: Description-Based Zero-shot Fine-Grained Entity Typing
  url: https://www.aclweb.org/anthology/N19-1087
  year: '2019'
N19-1088:
  abstract: 'In this paper, we present a method for adversarial decomposition of text
    representation. This method can be used to decompose a representation of an input
    sentence into several independent vectors, each of them responsible for a specific
    aspect of the input sentence. We evaluate the proposed method on two case studies:
    the conversion between different social registers and diachronic language change.
    We show that the proposed method is capable of fine-grained controlled change
    of these aspects of the input sentence. It is also learning a continuous (rather
    than categorical) representation of the style of the sentence, which is more linguistically
    realistic. The model uses adversarial-motivational training and includes a special
    motivational loss, which acts opposite to the discriminator and encourages a better
    decomposition. Furthermore, we evaluate the obtained meaning embeddings on a downstream
    task of paraphrase detection and show that they significantly outperform the embeddings
    of a regular autoencoder.'
  address: Minneapolis, Minnesota
  author:
  - first: Alexey
    full: Alexey Romanov
    id: alexey-romanov
    last: Romanov
  - first: Anna
    full: Anna Rumshisky
    id: anna-rumshisky
    last: Rumshisky
  - first: Anna
    full: Anna Rogers
    id: anna-rogers
    last: Rogers
  - first: David
    full: David Donahue
    id: david-donahue
    last: Donahue
  author_string: Alexey Romanov, Anna Rumshisky, Anna Rogers, David Donahue
  bibkey: romanov-etal-2019-adversarial
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1088
  month: June
  page_first: '815'
  page_last: '825'
  pages: "815\u2013825"
  paper_id: '88'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1088.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1088.jpg
  title: Adversarial Decomposition of Text Representation
  title_html: Adversarial Decomposition of Text Representation
  url: https://www.aclweb.org/anthology/N19-1088
  year: '2019'
N19-1089:
  abstract: "We introduce entity post-modifier generation as an instance of a collaborative\
    \ writing task. Given a sentence about a target entity, the task is to automatically\
    \ generate a post-modifier phrase that provides contextually relevant information\
    \ about the entity. For example, for the sentence, \u201CBarack Obama, _______,\
    \ supported the #MeToo movement.\u201D, the phrase \u201Ca father of two girls\u201D\
    \ is a contextually relevant post-modifier. To this end, we build PoMo, a post-modifier\
    \ dataset created automatically from news articles reflecting a journalistic need\
    \ for incorporating entity information that is relevant to a particular news event.\
    \ PoMo consists of more than 231K sentences with post-modifiers and associated\
    \ facts extracted from Wikidata for around 57K unique entities. We use crowdsourcing\
    \ to show that modeling contextual relevance is necessary for accurate post-modifier\
    \ generation. We adapt a number of existing generation approaches as baselines\
    \ for this dataset. Our results show there is large room for improvement in terms\
    \ of both identifying relevant facts to include (knowing which claims are relevant\
    \ gives a >20% improvement in BLEU score), and generating appropriate post-modifier\
    \ text for the context (providing relevant claims is not sufficient for accurate\
    \ generation). We conduct an error analysis that suggests promising directions\
    \ for future research."
  address: Minneapolis, Minnesota
  author:
  - first: Jun Seok
    full: Jun Seok Kang
    id: jun-seok-kang
    last: Kang
  - first: Robert
    full: Robert Logan
    id: robert-logan
    last: Logan
  - first: Zewei
    full: Zewei Chu
    id: zewei-chu
    last: Chu
  - first: Yang
    full: Yang Chen
    id: yang-chen
    last: Chen
  - first: Dheeru
    full: Dheeru Dua
    id: dheeru-dua
    last: Dua
  - first: Kevin
    full: Kevin Gimpel
    id: kevin-gimpel
    last: Gimpel
  - first: Sameer
    full: Sameer Singh
    id: sameer-singh
    last: Singh
  - first: Niranjan
    full: Niranjan Balasubramanian
    id: niranjan-balasubramanian
    last: Balasubramanian
  author_string: Jun Seok Kang, Robert Logan, Zewei Chu, Yang Chen, Dheeru Dua, Kevin
    Gimpel, Sameer Singh, Niranjan Balasubramanian
  bibkey: kang-etal-2019-pomo
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1089
  month: June
  page_first: '826'
  page_last: '838'
  pages: "826\u2013838"
  paper_id: '89'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1089.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1089.jpg
  title: 'PoMo: Generating Entity-Specific Post-Modifiers in Context'
  title_html: '<span class="acl-fixed-case">P</span>o<span class="acl-fixed-case">M</span>o:
    Generating Entity-Specific Post-Modifiers in Context'
  url: https://www.aclweb.org/anthology/N19-1089
  year: '2019'
N19-1090:
  abstract: 'Lexically-constrained sequence decoding allows for explicit positive
    or negative phrase-based constraints to be placed on target output strings in
    generation tasks such as machine translation or monolingual text rewriting. We
    describe vectorized dynamic beam allocation, which extends work in lexically-constrained
    decoding to work with batching, leading to a five-fold improvement in throughput
    when working with positive constraints. Faster decoding enables faster exploration
    of constraint strategies: we illustrate this via data augmentation experiments
    with a monolingual rewriter applied to the tasks of natural language inference,
    question answering and machine translation, showing improvements in all three.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1090.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1090.Supplementary.pdf
  author:
  - first: J. Edward
    full: J. Edward Hu
    id: j-edward-hu
    last: Hu
  - first: Huda
    full: Huda Khayrallah
    id: huda-khayrallah
    last: Khayrallah
  - first: Ryan
    full: Ryan Culkin
    id: ryan-culkin
    last: Culkin
  - first: Patrick
    full: Patrick Xia
    id: patrick-xia
    last: Xia
  - first: Tongfei
    full: Tongfei Chen
    id: tongfei-chen
    last: Chen
  - first: Matt
    full: Matt Post
    id: matt-post
    last: Post
  - first: Benjamin
    full: Benjamin Van Durme
    id: benjamin-van-durme
    last: Van Durme
  author_string: J. Edward Hu, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei
    Chen, Matt Post, Benjamin Van Durme
  bibkey: hu-etal-2019-improved
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1090
  month: June
  page_first: '839'
  page_last: '850'
  pages: "839\u2013850"
  paper_id: '90'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1090.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1090.jpg
  title: Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting
  title_html: Improved Lexically Constrained Decoding for Translation and Monolingual
    Rewriting
  url: https://www.aclweb.org/anthology/N19-1090
  year: '2019'
N19-1091:
  abstract: "In this paper, we propose an effective deep learning framework for inducing\
    \ courteous behavior in customer care responses. The interaction between a customer\
    \ and the customer care representative contributes substantially to the overall\
    \ customer experience. Thus it is imperative for customer care agents and chatbots\
    \ engaging with humans to be personal, cordial and emphatic to ensure customer\
    \ satisfaction and retention. Our system aims at automatically transforming neutral\
    \ customer care responses into courteous replies. Along with stylistic transfer\
    \ (of courtesy), our system ensures that responses are coherent with the conversation\
    \ history, and generates courteous expressions consistent with the emotional state\
    \ of the customer. Our technique is based on a reinforced pointer-generator model\
    \ for the sequence to sequence task. The model is also conditioned on a hierarchically\
    \ encoded and emotionally aware conversational context. We use real interactions\
    \ on Twitter between customer care professionals and aggrieved customers to create\
    \ a large conversational dataset having both forms of agent responses: \u2018\
    generic\u2019 and \u2018courteous\u2019. We perform quantitative and qualitative\
    \ analyses on established and task-specific metrics, both automatic and human\
    \ evaluation based. Our evaluation shows that the proposed models can generate\
    \ emotionally-appropriate courteous expressions while preserving the content.\
    \ Experimental results also prove that our proposed approach performs better than\
    \ the baseline models."
  address: Minneapolis, Minnesota
  author:
  - first: Hitesh
    full: Hitesh Golchha
    id: hitesh-golchha
    last: Golchha
  - first: Mauajama
    full: Mauajama Firdaus
    id: mauajama-firdaus
    last: Firdaus
  - first: Asif
    full: Asif Ekbal
    id: asif-ekbal
    last: Ekbal
  - first: Pushpak
    full: Pushpak Bhattacharyya
    id: pushpak-bhattacharyya
    last: Bhattacharyya
  author_string: Hitesh Golchha, Mauajama Firdaus, Asif Ekbal, Pushpak Bhattacharyya
  bibkey: golchha-etal-2019-courteously
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1091
  month: June
  page_first: '851'
  page_last: '860'
  pages: "851\u2013860"
  paper_id: '91'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1091.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1091.jpg
  title: 'Courteously Yours: Inducing courteous behavior in Customer Care responses
    using Reinforced Pointer Generator Network'
  title_html: 'Courteously Yours: Inducing courteous behavior in Customer Care responses
    using Reinforced Pointer Generator Network'
  url: https://www.aclweb.org/anthology/N19-1091
  year: '2019'
N19-1092:
  abstract: Metaphor generation attempts to replicate human creativity with language,
    which is an attractive but challengeable text generation task. Previous efforts
    mainly focus on template-based or rule-based methods and result in a lack of linguistic
    subtlety. In order to create novel metaphors, we propose a neural approach to
    metaphor generation and explore the shared inferential structure of a metaphorical
    usage and a literal usage of a verb. Our approach does not require any manually
    annotated metaphors for training. We extract the metaphorically used verbs with
    their metaphorical senses in an unsupervised way and train a neural language model
    from wiki corpus. Then we generate metaphors conveying the assigned metaphorical
    senses with an improved decoding algorithm. Automatic metrics and human evaluations
    demonstrate that our approach can generate metaphors with good readability and
    creativity.
  address: Minneapolis, Minnesota
  author:
  - first: Zhiwei
    full: Zhiwei Yu
    id: zhiwei-yu
    last: Yu
  - first: Xiaojun
    full: Xiaojun Wan
    id: xiaojun-wan
    last: Wan
  author_string: Zhiwei Yu, Xiaojun Wan
  bibkey: yu-wan-2019-avoid
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1092
  month: June
  page_first: '861'
  page_last: '871'
  pages: "861\u2013871"
  paper_id: '92'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1092.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1092.jpg
  title: How to Avoid Sentences Spelling Boring? Towards a Neural Approach to Unsupervised
    Metaphor Generation
  title_html: How to Avoid Sentences Spelling Boring? Towards a Neural Approach to
    Unsupervised Metaphor Generation
  url: https://www.aclweb.org/anthology/N19-1092
  year: '2019'
N19-1093:
  abstract: Linking pronominal expressions to the correct references requires, in
    many cases, better analysis of the contextual information and external knowledge.
    In this paper, we propose a two-layer model for pronoun coreference resolution
    that leverages both context and external knowledge, where a knowledge attention
    mechanism is designed to ensure the model leveraging the appropriate source of
    external knowledge based on different context. Experimental results demonstrate
    the validity and effectiveness of our model, where it outperforms state-of-the-art
    models by a large margin.
  address: Minneapolis, Minnesota
  author:
  - first: Hongming
    full: Hongming Zhang
    id: hongming-zhang
    last: Zhang
  - first: Yan
    full: Yan Song
    id: yan-song
    last: Song
  - first: Yangqiu
    full: Yangqiu Song
    id: yangqiu-song
    last: Song
  author_string: Hongming Zhang, Yan Song, Yangqiu Song
  bibkey: zhang-etal-2019-incorporating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1093
  month: June
  page_first: '872'
  page_last: '881'
  pages: "872\u2013881"
  paper_id: '93'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1093.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1093.jpg
  title: Incorporating Context and External Knowledge for Pronoun Coreference Resolution
  title_html: Incorporating Context and External Knowledge for Pronoun Coreference
    Resolution
  url: https://www.aclweb.org/anthology/N19-1093
  year: '2019'
N19-1094:
  abstract: Commonsense reasoning is fundamental to natural language understanding.
    While traditional methods rely heavily on human-crafted features and knowledge
    bases, we explore learning commonsense knowledge from a large amount of raw text
    via unsupervised learning. We propose two neural network models based on the Deep
    Structured Semantic Models (DSSM) framework to tackle two classic commonsense
    reasoning tasks, Winograd Schema challenges (WSC) and Pronoun Disambiguation (PDP).
    Evaluation shows that the proposed models effectively capture contextual information
    in the sentence and co-reference information between pronouns and nouns, and achieve
    significant improvement over previous state-of-the-art approaches.
  address: Minneapolis, Minnesota
  author:
  - first: Shuohang
    full: Shuohang Wang
    id: shuohang-wang
    last: Wang
  - first: Sheng
    full: Sheng Zhang
    id: sheng-zhang
    last: Zhang
  - first: Yelong
    full: Yelong Shen
    id: yelong-shen
    last: Shen
  - first: Xiaodong
    full: Xiaodong Liu
    id: xiaodong-liu
    last: Liu
  - first: Jingjing
    full: Jingjing Liu
    id: jingjing-liu
    last: Liu
  - first: Jianfeng
    full: Jianfeng Gao
    id: jianfeng-gao
    last: Gao
  - first: Jing
    full: Jing Jiang
    id: jing-jiang
    last: Jiang
  author_string: Shuohang Wang, Sheng Zhang, Yelong Shen, Xiaodong Liu, Jingjing Liu,
    Jianfeng Gao, Jing Jiang
  bibkey: wang-etal-2019-unsupervised
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1094
  month: June
  page_first: '882'
  page_last: '891'
  pages: "882\u2013891"
  paper_id: '94'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1094.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1094.jpg
  title: Unsupervised Deep Structured Semantic Models for Commonsense Reasoning
  title_html: Unsupervised Deep Structured Semantic Models for Commonsense Reasoning
  url: https://www.aclweb.org/anthology/N19-1094
  year: '2019'
N19-1095:
  abstract: Pronouns are often dropped in Chinese sentences, and this happens more
    frequently in conversational genres as their referents can be easily understood
    from context. Recovering dropped pronouns is essential to applications such as
    Information Extraction where the referents of these dropped pronouns need to be
    resolved, or Machine Translation when Chinese is the source language. In this
    work, we present a novel end-to-end neural network model to recover dropped pronouns
    in conversational data. Our model is based on a structured attention mechanism
    that models the referents of dropped pronouns utilizing both sentence-level and
    word-level information. Results on three different conversational genres show
    that our approach achieves a significant improvement over the current state of
    the art.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1095.Software.zip
    type: software
    url: https://www.aclweb.org/anthology/attachments/N19-1095.Software.zip
  author:
  - first: Jingxuan
    full: Jingxuan Yang
    id: jingxuan-yang
    last: Yang
  - first: Jianzhuo
    full: Jianzhuo Tong
    id: jianzhuo-tong
    last: Tong
  - first: Si
    full: Si Li
    id: si-li
    last: Li
  - first: Sheng
    full: Sheng Gao
    id: sheng-gao
    last: Gao
  - first: Jun
    full: Jun Guo
    id: jun-guo
    last: Guo
  - first: Nianwen
    full: Nianwen Xue
    id: nianwen-xue
    last: Xue
  author_string: Jingxuan Yang, Jianzhuo Tong, Si Li, Sheng Gao, Jun Guo, Nianwen
    Xue
  bibkey: yang-etal-2019-recovering
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1095
  month: June
  page_first: '892'
  page_last: '901'
  pages: "892\u2013901"
  paper_id: '95'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1095.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1095.jpg
  title: Recovering dropped pronouns in Chinese conversations via modeling their referents
  title_html: Recovering dropped pronouns in <span class="acl-fixed-case">C</span>hinese
    conversations via modeling their referents
  url: https://www.aclweb.org/anthology/N19-1095
  year: '2019'
N19-1096:
  abstract: Semantic representations in the form of directed acyclic graphs (DAGs)
    have been introduced in recent years, and to model them, we need probabilistic
    models of DAGs. One model that has attracted some attention is the DAG automaton,
    but it has not been studied as a probabilistic model. We show that some DAG automata
    cannot be made into useful probabilistic models by the nearly universal strategy
    of assigning weights to transitions. The problem affects single-rooted, multi-rooted,
    and unbounded-degree variants of DAG automata, and appears to be pervasive. It
    does not affect planar variants, but these are problematic for other reasons.
  address: Minneapolis, Minnesota
  author:
  - first: Ieva
    full: Ieva Vasiljeva
    id: ieva-vasiljeva
    last: Vasiljeva
  - first: Sorcha
    full: Sorcha Gilroy
    id: sorcha-gilroy
    last: Gilroy
  - first: Adam
    full: Adam Lopez
    id: adam-lopez
    last: Lopez
  author_string: Ieva Vasiljeva, Sorcha Gilroy, Adam Lopez
  bibkey: vasiljeva-etal-2019-problem
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1096
  month: June
  page_first: '902'
  page_last: '911'
  pages: "902\u2013911"
  paper_id: '96'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1096.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1096.jpg
  title: The problem with probabilistic DAG automata for semantic graphs
  title_html: The problem with probabilistic <span class="acl-fixed-case">DAG</span>
    automata for semantic graphs
  url: https://www.aclweb.org/anthology/N19-1096
  year: '2019'
N19-1097:
  abstract: "The use of subword-level information (e.g., characters, character n-grams,\
    \ morphemes) has become ubiquitous in modern word representation learning. Its\
    \ importance is attested especially for morphologically rich languages which generate\
    \ a large number of rare words. Despite a steadily increasing interest in such\
    \ subword-informed word representations, their systematic comparative analysis\
    \ across typologically diverse languages and different tasks is still missing.\
    \ In this work, we deliver such a study focusing on the variation of two crucial\
    \ components required for subword-level integration into word representation models:\
    \ 1) segmentation of words into subword units, and 2) subword composition functions\
    \ to obtain final word representations. We propose a general framework for learning\
    \ subword-informed word representations that allows for easy experimentation with\
    \ different segmentation and composition components, also including more advanced\
    \ techniques based on position embeddings and self-attention. Using the unified\
    \ framework, we run experiments over a large number of subword-informed word representation\
    \ configurations (60 in total) on 3 tasks (general and rare word similarity, dependency\
    \ parsing, fine-grained entity typing) for 5 languages representing 3 language\
    \ types. Our main results clearly indicate that there is no \u201Cone-size-fits-all\u201D\
    \ configuration, as performance is both language- and task-dependent. We also\
    \ show that configurations based on unsupervised segmentation (e.g., BPE, Morfessor)\
    \ are sometimes comparable to or even outperform the ones based on supervised\
    \ word segmentation."
  address: Minneapolis, Minnesota
  author:
  - first: Yi
    full: Yi Zhu
    id: yi-zhu
    last: Zhu
  - first: Ivan
    full: "Ivan Vuli\u0107"
    id: ivan-vulic
    last: "Vuli\u0107"
  - first: Anna
    full: Anna Korhonen
    id: anna-korhonen
    last: Korhonen
  author_string: "Yi Zhu, Ivan Vuli\u0107, Anna Korhonen"
  bibkey: zhu-etal-2019-systematic
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1097
  month: June
  page_first: '912'
  page_last: '932'
  pages: "912\u2013932"
  paper_id: '97'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1097.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1097.jpg
  title: A Systematic Study of Leveraging Subword Information for Learning Word Representations
  title_html: A Systematic Study of Leveraging Subword Information for Learning Word
    Representations
  url: https://www.aclweb.org/anthology/N19-1097
  year: '2019'
N19-1098:
  abstract: Pre-trained word vectors are ubiquitous in Natural Language Processing
    applications. In this paper, we show how training word embeddings jointly with
    bigram and even trigram embeddings, results in improved unigram embeddings. We
    claim that training word embeddings along with higher n-gram embeddings helps
    in the removal of the contextual information from the unigrams, resulting in better
    stand-alone word embeddings. We empirically show the validity of our hypothesis
    by outperforming other competing word representation models by a significant margin
    on a wide variety of tasks. We make our models publicly available.
  address: Minneapolis, Minnesota
  author:
  - first: Prakhar
    full: Prakhar Gupta
    id: prakhar-gupta
    last: Gupta
  - first: Matteo
    full: Matteo Pagliardini
    id: matteo-pagliardini
    last: Pagliardini
  - first: Martin
    full: Martin Jaggi
    id: martin-jaggi
    last: Jaggi
  author_string: Prakhar Gupta, Matteo Pagliardini, Martin Jaggi
  bibkey: gupta-etal-2019-better
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1098
  month: June
  page_first: '933'
  page_last: '939'
  pages: "933\u2013939"
  paper_id: '98'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1098.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1098.jpg
  title: Better Word Embeddings by Disentangling Contextual n-Gram Information
  title_html: Better Word Embeddings by Disentangling Contextual n-Gram Information
  url: https://www.aclweb.org/anthology/N19-1098
  year: '2019'
N19-1099:
  abstract: Leveraging domain knowledge is an effective strategy for enhancing the
    quality of inferred low-dimensional representations of documents by topic models.
    In this paper, we develop topic modeling with knowledge graph embedding (TMKGE),
    a Bayesian nonparametric model to employ knowledge graph (KG) embedding in the
    context of topic modeling, for extracting more coherent topics. Specifically,
    we build a hierarchical Dirichlet process (HDP) based model to flexibly borrow
    information from KG to improve the interpretability of topics. An efficient online
    variational inference method based on a stick-breaking construction of HDP is
    developed for TMKGE, making TMKGE suitable for large document corpora and KGs.
    Experiments on three public datasets illustrate the superior performance of TMKGE
    in terms of topic coherence and document classification accuracy, compared to
    state-of-the-art topic modeling methods.
  address: Minneapolis, Minnesota
  author:
  - first: Dingcheng
    full: Dingcheng Li
    id: dingcheng-li
    last: Li
  - first: Siamak
    full: Siamak Zamani
    id: siamak-zamani
    last: Zamani
  - first: Jingyuan
    full: Jingyuan Zhang
    id: jingyuan-zhang
    last: Zhang
  - first: Ping
    full: Ping Li
    id: ping-li
    last: Li
  author_string: Dingcheng Li, Siamak Zamani, Jingyuan Zhang, Ping Li
  bibkey: li-etal-2019-integration
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1099
  month: June
  page_first: '940'
  page_last: '950'
  pages: "940\u2013950"
  paper_id: '99'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1099.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1099.jpg
  title: Integration of Knowledge Graph Embedding Into Topic Modeling with Hierarchical
    Dirichlet Process
  title_html: Integration of Knowledge Graph Embedding Into Topic Modeling with Hierarchical
    <span class="acl-fixed-case">D</span>irichlet Process
  url: https://www.aclweb.org/anthology/N19-1099
  year: '2019'
N19-1100:
  abstract: A large body of research into semantic textual similarity has focused
    on constructing state-of-the-art embeddings using sophisticated modelling, careful
    choice of learning signals and many clever tricks. By contrast, little attention
    has been devoted to similarity measures between these embeddings, with cosine
    similarity being used unquestionably in the majority of cases. In this work, we
    illustrate that for all common word vectors, cosine similarity is essentially
    equivalent to the Pearson correlation coefficient, which provides some justification
    for its use. We thoroughly characterise cases where Pearson correlation (and thus
    cosine similarity) is unfit as similarity measure. Importantly, we show that Pearson
    correlation is appropriate for some word vectors but not others. When it is not
    appropriate, we illustrate how common non-parametric rank correlation coefficients
    can be used instead to significantly improve performance. We support our analysis
    with a series of evaluations on word-level and sentence-level semantic textual
    similarity benchmarks. On the latter, we show that even the simplest averaged
    word vectors compared by rank correlation easily rival the strongest deep representations
    compared by cosine similarity.
  address: Minneapolis, Minnesota
  author:
  - first: Vitalii
    full: Vitalii Zhelezniak
    id: vitalii-zhelezniak
    last: Zhelezniak
  - first: Aleksandar
    full: Aleksandar Savkov
    id: aleksandar-savkov
    last: Savkov
  - first: April
    full: April Shen
    id: april-shen
    last: Shen
  - first: Nils
    full: Nils Hammerla
    id: nils-hammerla
    last: Hammerla
  author_string: Vitalii Zhelezniak, Aleksandar Savkov, April Shen, Nils Hammerla
  bibkey: zhelezniak-etal-2019-correlation
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1100
  month: June
  page_first: '951'
  page_last: '962'
  pages: "951\u2013962"
  paper_id: '100'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1100.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1100.jpg
  title: Correlation Coefficients and Semantic Textual Similarity
  title_html: Correlation Coefficients and Semantic Textual Similarity
  url: https://www.aclweb.org/anthology/N19-1100
  year: '2019'
N19-1101:
  abstract: The task of Natural Language Inference (NLI) is widely modeled as supervised
    sentence pair classification. While there has been a lot of work recently on generating
    explanations of the predictions of classifiers on a single piece of text, there
    have been no attempts to generate explanations of classifiers operating on pairs
    of sentences. In this paper, we show that it is possible to generate token-level
    explanations for NLI without the need for training data explicitly annotated for
    this purpose. We use a simple LSTM architecture and evaluate both LIME and Anchor
    explanations for this task. We compare these to a Multiple Instance Learning (MIL)
    method that uses thresholded attention make token-level predictions. The approach
    we present in this paper is a novel extension of zero-shot single-sentence tagging
    to sentence pairs for NLI. We conduct our experiments on the well-studied SNLI
    dataset that was recently augmented with manually annotation of the tokens that
    explain the entailment relation. We find that our white-box MIL-based method,
    while orders of magnitude faster, does not reach the same accuracy as the black-box
    methods.
  address: Minneapolis, Minnesota
  author:
  - first: James
    full: James Thorne
    id: james-thorne
    last: Thorne
  - first: Andreas
    full: Andreas Vlachos
    id: andreas-vlachos
    last: Vlachos
  - first: Christos
    full: Christos Christodoulopoulos
    id: christos-christodoulopoulos
    last: Christodoulopoulos
  - first: Arpit
    full: Arpit Mittal
    id: arpit-mittal
    last: Mittal
  author_string: James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit
    Mittal
  bibkey: thorne-etal-2019-generating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1101
  month: June
  page_first: '963'
  page_last: '969'
  pages: "963\u2013969"
  paper_id: '101'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1101.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1101.jpg
  title: Generating Token-Level Explanations for Natural Language Inference
  title_html: Generating Token-Level Explanations for Natural Language Inference
  url: https://www.aclweb.org/anthology/N19-1101
  year: '2019'
N19-1102:
  abstract: 'Complex Word Identification (CWI) is the task of identifying which words
    or phrases in a sentence are difficult to understand by a target audience. The
    latest CWI Shared Task released data for two settings: monolingual (i.e. train
    and test in the same language) and cross-lingual (i.e. test in a language not
    seen during training). The best monolingual models relied on language-dependent
    features, which do not generalise in the cross-lingual setting, while the best
    cross-lingual model used neural networks with multi-task learning. In this paper,
    we present monolingual and cross-lingual CWI models that perform as well as (or
    better than) most models submitted to the latest CWI Shared Task. We show that
    carefully selected features and simple learning models can achieve state-of-the-art
    performance, and result in strong baselines for future development in this area.
    Finally, we discuss how inconsistencies in the annotation of the data can explain
    some of the results obtained.'
  address: Minneapolis, Minnesota
  author:
  - first: Pierre
    full: Pierre Finnimore
    id: pierre-finnimore
    last: Finnimore
  - first: Elisabeth
    full: Elisabeth Fritzsch
    id: elisabeth-fritzsch
    last: Fritzsch
  - first: Daniel
    full: Daniel King
    id: daniel-king
    last: King
  - first: Alison
    full: Alison Sneyd
    id: alison-sneyd
    last: Sneyd
  - first: Aneeq
    full: Aneeq Ur Rehman
    id: aneeq-ur-rehman
    last: Ur Rehman
  - first: Fernando
    full: Fernando Alva-Manchego
    id: fernando-alva-manchego
    last: Alva-Manchego
  - first: Andreas
    full: Andreas Vlachos
    id: andreas-vlachos
    last: Vlachos
  author_string: Pierre Finnimore, Elisabeth Fritzsch, Daniel King, Alison Sneyd,
    Aneeq Ur Rehman, Fernando Alva-Manchego, Andreas Vlachos
  bibkey: finnimore-etal-2019-strong
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1102
  month: June
  page_first: '970'
  page_last: '977'
  pages: "970\u2013977"
  paper_id: '102'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1102.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1102.jpg
  title: Strong Baselines for Complex Word Identification across Multiple Languages
  title_html: Strong Baselines for Complex Word Identification across Multiple Languages
  url: https://www.aclweb.org/anthology/N19-1102
  year: '2019'
N19-1103:
  abstract: 'We consider the problem of learning distributed representations for entities
    and relations of multi-relational data so as to predict missing links therein.
    Convolutional neural networks have recently shown their superiority for this problem,
    bringing increased model expressiveness while remaining parameter efficient. Despite
    the success, previous convolution designs fail to model full interactions between
    input entities and relations, which potentially limits the performance of link
    prediction. In this work we introduce ConvR, an adaptive convolutional network
    designed to maximize entity-relation interactions in a convolutional fashion.
    ConvR adaptively constructs convolution filters from relation representations,
    and applies these filters across entity representations to generate convolutional
    features. As such, ConvR enables rich interactions between entity and relation
    representations at diverse regions, and all the convolutional features generated
    will be able to capture such interactions. We evaluate ConvR on multiple benchmark
    datasets. Experimental results show that: (1) ConvR performs substantially better
    than competitive baselines in almost all the metrics and on all the datasets;
    (2) Compared with state-of-the-art convolutional models, ConvR is not only more
    effective but also more efficient. It offers a 7% increase in MRR and a 6% increase
    in Hits@10, while saving 12% in parameter storage.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/353480570
    type: video
    url: https://vimeo.com/353480570
  author:
  - first: Xiaotian
    full: Xiaotian Jiang
    id: xiaotian-jiang
    last: Jiang
  - first: Quan
    full: Quan Wang
    id: quan-wang
    last: Wang
  - first: Bin
    full: Bin Wang
    id: bin-wang
    last: Wang
  author_string: Xiaotian Jiang, Quan Wang, Bin Wang
  bibkey: jiang-etal-2019-adaptive
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1103
  month: June
  page_first: '978'
  page_last: '987'
  pages: "978\u2013987"
  paper_id: '103'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1103.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1103.jpg
  title: Adaptive Convolution for Multi-Relational Learning
  title_html: Adaptive Convolution for Multi-Relational Learning
  url: https://www.aclweb.org/anthology/N19-1103
  year: '2019'
N19-1104:
  abstract: Knowledge graphs have been developed rapidly in recent years and shown
    their usefulness for many artificial intelligence tasks. However, knowledge graphs
    often have lots of missing facts. To solve this problem, many knowledge graph
    embedding models to populate knowledge graphs have been developed and have shown
    outstanding performance these days. However, knowledge graph embedding models
    are so called-black box. Hence, we actually does not know how information of a
    knowledge graph is processed and the models are hard to interpret. In this paper,
    we utilize graph patterns in a knowledge graph to overcome such problems. Our
    proposed model, graph pattern entity ranking Model (GRank), constructs an entity
    ranking system for each graph pattern and evaluate them using a measure for a
    ranking system. By doing so, we can find helpful graph patterns for predicting
    facts. Then we conduct the link prediction tasks on standard data sets to evaluate
    GRank. We show our approach outperforms other state-of-the-art approaches such
    as ComplEx and TorusE on standard metrics such as HITS@n and MRR. Moreover, This
    model is easily interpretable because output facts are described by graph patterns.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/353483121
    type: video
    url: https://vimeo.com/353483121
  author:
  - first: Takuma
    full: Takuma Ebisu
    id: takuma-ebisu
    last: Ebisu
  - first: Ryutaro
    full: Ryutaro Ichise
    id: ryutaro-ichise
    last: Ichise
  author_string: Takuma Ebisu, Ryutaro Ichise
  bibkey: ebisu-ichise-2019-graph
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1104
  month: June
  page_first: '988'
  page_last: '997'
  pages: "988\u2013997"
  paper_id: '104'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1104.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1104.jpg
  title: Graph Pattern Entity Ranking Model for Knowledge Graph Completion
  title_html: Graph Pattern Entity Ranking Model for Knowledge Graph Completion
  url: https://www.aclweb.org/anthology/N19-1104
  year: '2019'
N19-1105:
  abstract: Modern weakly supervised methods for event detection (ED) avoid time-consuming
    human annotation and achieve promising results by learning from auto-labeled data.
    However, these methods typically rely on sophisticated pre-defined rules as well
    as existing instances in knowledge bases for automatic annotation and thus suffer
    from low coverage, topic bias, and data noise. To address these issues, we build
    a large event-related candidate set with good coverage and then apply an adversarial
    training mechanism to iteratively identify those informative instances from the
    candidate set and filter out those noisy ones. The experiments on two real-world
    datasets show that our candidate selection and adversarial training can cooperate
    together to obtain more diverse and accurate training data for ED, and significantly
    outperform the state-of-the-art methods in various weakly supervised scenarios.
    The datasets and source code can be obtained from https://github.com/thunlp/Adv-ED.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/353485593
    type: video
    url: https://vimeo.com/353485593
  author:
  - first: Xiaozhi
    full: Xiaozhi Wang
    id: xiaozhi-wang
    last: Wang
  - first: Xu
    full: Xu Han
    id: xu-han
    last: Han
  - first: Zhiyuan
    full: Zhiyuan Liu
    id: zhiyuan-liu
    last: Liu
  - first: Maosong
    full: Maosong Sun
    id: maosong-sun
    last: Sun
  - first: Peng
    full: Peng Li
    id: peng-li
    last: Li
  author_string: Xiaozhi Wang, Xu Han, Zhiyuan Liu, Maosong Sun, Peng Li
  bibkey: wang-etal-2019-adversarial-training
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1105
  month: June
  page_first: '998'
  page_last: '1008'
  pages: "998\u20131008"
  paper_id: '105'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1105.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1105.jpg
  title: Adversarial Training for Weakly Supervised Event Detection
  title_html: Adversarial Training for Weakly Supervised Event Detection
  url: https://www.aclweb.org/anthology/N19-1105
  year: '2019'
N19-1106:
  abstract: Extreme classification is a classification task on an extremely large
    number of labels (tags). User generated labels for any type of online data can
    be sparing per individual user but intractably large among all users. It would
    be useful to automatically select a smaller, standard set of labels to represent
    the whole label set. We can then solve efficiently the problem of multi-label
    learning with an intractably large number of interdependent labels, such as automatic
    tagging of Wikipedia pages. We propose a submodular maximization framework with
    linear cost to find informative labels which are most relevant to other labels
    yet least redundant with each other. A simple prediction model can then be trained
    on this label subset. Our framework includes both label-label and label-feature
    dependencies, which aims to find the labels with the most representation and prediction
    ability. In addition, to avoid information loss, we extract and predict outlier
    labels with weak dependency on other labels. We apply our model to four standard
    natural language data sets including Bibsonomy entries with users assigned tags,
    web pages with user assigned tags, legal texts with EUROVOC descriptors(A topic
    hierarchy with almost 4000 categories regarding different aspects of European
    law) and Wikipedia pages with tags from social bookmarking as well as news videos
    for automated label detection from a lexicon of semantic concepts. Experimental
    results show that our proposed approach improves label prediction quality, in
    terms of precision and nDCG, by 3% to 5% in three of the 5 tasks and is competitive
    in the others, even with a simple linear prediction model. An ablation study shows
    how different data sets benefit from different aspects of our model, with all
    aspects contributing substantially to at least one data set.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1106.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1106.Supplementary.pdf
  - filename: https://vimeo.com/360593989
    type: video
    url: https://vimeo.com/360593989
  author:
  - first: Elham J.
    full: Elham J. Barezi
    id: elham-j-barezi1
    last: Barezi
  - first: Ian D.
    full: Ian D. Wood
    id: ian-d-wood
    last: Wood
  - first: Pascale
    full: Pascale Fung
    id: pascale-fung
    last: Fung
  - first: Hamid R.
    full: Hamid R. Rabiee
    id: hamid-r-rabiee
    last: Rabiee
  author_string: Elham J. Barezi, Ian D. Wood, Pascale Fung, Hamid R. Rabiee
  bibkey: barezi-etal-2019-submodular
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1106
  month: June
  page_first: '1009'
  page_last: '1018'
  pages: "1009\u20131018"
  paper_id: '106'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1106.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1106.jpg
  title: A Submodular Feature-Aware Framework for Label Subset Selection in Extreme
    Classification Problems
  title_html: A Submodular Feature-Aware Framework for Label Subset Selection in Extreme
    Classification Problems
  url: https://www.aclweb.org/anthology/N19-1106
  year: '2019'
N19-1107:
  abstract: Distant supervision (DS) is an important paradigm for automatically extracting
    relations. It utilizes existing knowledge base to collect examples for the relation
    we intend to extract, and then uses these examples to automatically generate the
    training data. However, the examples collected can be very noisy, and pose significant
    challenge for obtaining high quality labels. Previous work has made remarkable
    progress in predicting the relation from distant supervision, but typically ignores
    the temporal relations among those supervising instances. This paper formulates
    the problem of relation extraction with temporal reasoning and proposes a solution
    to predict whether two given entities participate in a relation at a given time
    spot. For this purpose, we construct a dataset called WIKI-TIME which additionally
    includes the valid period of a certain relation of two entities in the knowledge
    base. We propose a novel neural model to incorporate both the temporal information
    encoding and sequential reasoning. The experimental results show that, compared
    with the best of existing models, our model achieves better performance in both
    WIKI-TIME dataset and the well-studied NYT-10 dataset.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/360608466
    type: video
    url: https://vimeo.com/360608466
  author:
  - first: Jianhao
    full: Jianhao Yan
    id: jianhao-yan
    last: Yan
  - first: Lin
    full: Lin He
    id: lin-he
    last: He
  - first: Ruqin
    full: Ruqin Huang
    id: ruqin-huang
    last: Huang
  - first: Jian
    full: Jian Li
    id: jian-li
    last: Li
  - first: Ying
    full: Ying Liu
    id: ying-liu
    last: Liu
  author_string: Jianhao Yan, Lin He, Ruqin Huang, Jian Li, Ying Liu
  bibkey: yan-etal-2019-relation
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1107
  month: June
  page_first: '1019'
  page_last: '1030'
  pages: "1019\u20131030"
  paper_id: '107'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1107.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1107.jpg
  title: Relation Extraction with Temporal Reasoning Based on Memory Augmented Distant
    Supervision
  title_html: Relation Extraction with Temporal Reasoning Based on Memory Augmented
    Distant Supervision
  url: https://www.aclweb.org/anthology/N19-1107
  year: '2019'
N19-1108:
  abstract: Insufficient or even unavailable training data of emerging classes is
    a big challenge of many classification tasks, including text classification. Recognising
    text documents of classes that have never been seen in the learning stage, so-called
    zero-shot text classification, is therefore difficult and only limited previous
    works tackled this problem. In this paper, we propose a two-phase framework together
    with data augmentation and feature augmentation to solve this problem. Four kinds
    of semantic knowledge (word embeddings, class descriptions, class hierarchy, and
    a general knowledge graph) are incorporated into the proposed framework to deal
    with instances of unseen classes effectively. Experimental results show that each
    and the combination of the two phases achieve the best overall accuracy compared
    with baselines and recent approaches in classifying real-world texts under the
    zero-shot scenario.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1108.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1108.Presentation.pdf
  - filename: https://vimeo.com/355765532
    type: video
    url: https://vimeo.com/355765532
  author:
  - first: Jingqing
    full: Jingqing Zhang
    id: jingqing-zhang
    last: Zhang
  - first: Piyawat
    full: Piyawat Lertvittayakumjorn
    id: piyawat-lertvittayakumjorn
    last: Lertvittayakumjorn
  - first: Yike
    full: Yike Guo
    id: yike-guo
    last: Guo
  author_string: Jingqing Zhang, Piyawat Lertvittayakumjorn, Yike Guo
  bibkey: zhang-etal-2019-integrating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1108
  month: June
  page_first: '1031'
  page_last: '1040'
  pages: "1031\u20131040"
  paper_id: '108'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1108.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1108.jpg
  title: Integrating Semantic Knowledge to Tackle Zero-shot Text Classification
  title_html: Integrating Semantic Knowledge to Tackle Zero-shot Text Classification
  url: https://www.aclweb.org/anthology/N19-1108
  year: '2019'
N19-1109:
  abstract: "A standard word embedding algorithm, such as word2vec and glove, makes\
    \ a strong assumption that words are likely to be semantically related only if\
    \ they co-occur locally within a window of fixed size. However, this strong assumption\
    \ may not capture the semantic association between words that co-occur frequently\
    \ but non-locally within documents. In this paper, we propose a graph-based word\
    \ embedding method, named \u2018word-node2vec\u2019. By relaxing the strong constraint\
    \ of locality, our method is able to capture both the local and non-local co-occurrences.\
    \ Word-node2vec constructs a graph where every node represents a word and an edge\
    \ between two nodes represents a combination of both local (e.g. word2vec) and\
    \ document-level co-occurrences. Our experiments show that word-node2vec outperforms\
    \ word2vec and glove on a range of different tasks, such as predicting word-pair\
    \ similarity, word analogy and concept categorization."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/355773895
    type: video
    url: https://vimeo.com/355773895
  author:
  - first: Procheta
    full: Procheta Sen
    id: procheta-sen
    last: Sen
  - first: Debasis
    full: Debasis Ganguly
    id: debasis-ganguly
    last: Ganguly
  - first: Gareth
    full: Gareth Jones
    id: gareth-jones
    last: Jones
  author_string: Procheta Sen, Debasis Ganguly, Gareth Jones
  bibkey: sen-etal-2019-word
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1109
  month: June
  page_first: '1041'
  page_last: '1051'
  pages: "1041\u20131051"
  paper_id: '109'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1109.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1109.jpg
  title: 'Word-Node2Vec: Improving Word Embedding with Document-Level Non-Local Word
    Co-occurrences'
  title_html: 'Word-<span class="acl-fixed-case">N</span>ode2<span class="acl-fixed-case">V</span>ec:
    Improving Word Embedding with Document-Level Non-Local Word Co-occurrences'
  url: https://www.aclweb.org/anthology/N19-1109
  year: '2019'
N19-1110:
  abstract: In traditional Distributional Semantic Models (DSMs) the multiple senses
    of a polysemous word are conflated into a single vector space representation.
    In this work, we propose a DSM that learns multiple distributional representations
    of a word based on different topics. First, a separate DSM is trained for each
    topic and then each of the topic-based DSMs is aligned to a common vector space.
    Our unsupervised mapping approach is motivated by the hypothesis that words preserving
    their relative distances in different topic semantic sub-spaces constitute robust
    semantic anchors that define the mappings between them. Aligned cross-topic representations
    achieve state-of-the-art results for the task of contextual word similarity. Furthermore,
    evaluation on NLP downstream tasks shows that multiple topic-based embeddings
    outperform single-prototype models.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/355781410
    type: video
    url: https://vimeo.com/355781410
  author:
  - first: Eleftheria
    full: Eleftheria Briakou
    id: eleftheria-briakou
    last: Briakou
  - first: Nikos
    full: Nikos Athanasiou
    id: nikos-athanasiou
    last: Athanasiou
  - first: Alexandros
    full: Alexandros Potamianos
    id: alexandros-potamianos
    last: Potamianos
  author_string: Eleftheria Briakou, Nikos Athanasiou, Alexandros Potamianos
  bibkey: briakou-etal-2019-cross
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1110
  month: June
  page_first: '1052'
  page_last: '1061'
  pages: "1052\u20131061"
  paper_id: '110'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1110.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1110.jpg
  title: Cross-Topic Distributional Semantic Representations Via Unsupervised Mappings
  title_html: Cross-Topic Distributional Semantic Representations Via Unsupervised
    Mappings
  url: https://www.aclweb.org/anthology/N19-1110
  year: '2019'
N19-1111:
  abstract: 'Recent work has attempted to enhance vector space representations using
    information from structured semantic resources. This process, dubbed retrofitting
    (Faruqui et al., 2015), has yielded improvements in word similarity performance.
    Research has largely focused on the retrofitting algorithm, or on the kind of
    structured semantic resources used, but little research has explored why some
    resources perform better than others. We conducted a fine-grained analysis of
    the original retrofitting process, and found that the utility of different lexical
    resources for retrofitting depends on two factors: the coverage of the resource
    and the evaluation metric. Our assessment suggests that the common practice of
    using correlation measures to evaluate increases in performance against full word
    similarity benchmarks 1) obscures the benefits offered by smaller resources, and
    2) overlooks incremental gains in word similarity performance. We propose root-mean-square
    error (RMSE) as an alternative evaluation metric, and demonstrate that correlation
    measures and RMSE sometimes yield opposite conclusions concerning the efficacy
    of retrofitting. This point is illustrated by word vectors retrofitted with novel
    treatments of the FrameNet data (Fillmore and Baker, 2010).'
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1111.Software.txt
    type: software
    url: https://www.aclweb.org/anthology/attachments/N19-1111.Software.txt
  - filename: https://vimeo.com/356031269
    type: video
    url: https://vimeo.com/356031269
  author:
  - first: Dmetri
    full: Dmetri Hayes
    id: dmetri-hayes
    last: Hayes
  author_string: Dmetri Hayes
  bibkey: hayes-2019-just
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1111
  month: June
  page_first: '1062'
  page_last: '1072'
  pages: "1062\u20131072"
  paper_id: '111'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1111.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1111.jpg
  title: What just happened? Evaluating retrofitted distributional word vectors
  title_html: What just happened? <span class="acl-fixed-case">E</span>valuating retrofitted
    distributional word vectors
  url: https://www.aclweb.org/anthology/N19-1111
  year: '2019'
N19-1112:
  abstract: Contextual word representations derived from large-scale neural language
    models are successful across a diverse set of NLP tasks, suggesting that they
    encode useful and transferable features of language. To shed light on the linguistic
    knowledge they capture, we study the representations produced by several recent
    pretrained contextualizers (variants of ELMo, the OpenAI transformer language
    model, and BERT) with a suite of sixteen diverse probing tasks. We find that linear
    models trained on top of frozen contextual representations are competitive with
    state-of-the-art task-specific models in many cases, but fail on tasks requiring
    fine-grained linguistic knowledge (e.g., conjunct identification). To investigate
    the transferability of contextual word representations, we quantify differences
    in the transferability of individual layers within contextualizers, especially
    between recurrent neural networks (RNNs) and transformers. For instance, higher
    layers of RNNs are more task-specific, while transformer layers do not exhibit
    the same monotonic trend. In addition, to better understand what makes contextual
    word representations transferable, we compare language model pretraining with
    eleven supervised pretraining tasks. For any given task, pretraining on a closely
    related task yields better performance than language model pretraining (which
    is better on average) when the pretraining dataset is fixed. However, language
    model pretraining on more data gives the best results.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/355787267
    type: video
    url: https://vimeo.com/355787267
  author:
  - first: Nelson F.
    full: Nelson F. Liu
    id: nelson-f-liu
    last: Liu
  - first: Matt
    full: Matt Gardner
    id: matt-gardner
    last: Gardner
  - first: Yonatan
    full: Yonatan Belinkov
    id: yonatan-belinkov
    last: Belinkov
  - first: Matthew E.
    full: Matthew E. Peters
    id: matthew-e-peters
    last: Peters
  - first: Noah A.
    full: Noah A. Smith
    id: noah-a-smith
    last: Smith
  author_string: Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters,
    Noah A. Smith
  bibkey: liu-etal-2019-linguistic
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1112
  month: June
  page_first: '1073'
  page_last: '1094'
  pages: "1073\u20131094"
  paper_id: '112'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1112.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1112.jpg
  title: Linguistic Knowledge and Transferability of Contextual Representations
  title_html: Linguistic Knowledge and Transferability of Contextual Representations
  url: https://www.aclweb.org/anthology/N19-1112
  year: '2019'
N19-1113:
  abstract: 'We address part-of-speech (POS) induction by maximizing the mutual information
    between the induced label and its context. We focus on two training objectives
    that are amenable to stochastic gradient descent (SGD): a novel generalization
    of the classical Brown clustering objective and a recently proposed variational
    lower bound. While both objectives are subject to noise in gradient updates, we
    show through analysis and experiments that the variational lower bound is robust
    whereas the generalized Brown objective is vulnerable. We obtain strong performance
    on a multitude of datasets and languages with a simple architecture that encodes
    morphology and context.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/364657734
    type: video
    url: https://vimeo.com/364657734
  author:
  - first: Karl
    full: Karl Stratos
    id: karl-stratos
    last: Stratos
  author_string: Karl Stratos
  bibkey: stratos-2019-mutual
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1113
  month: June
  page_first: '1095'
  page_last: '1104'
  pages: "1095\u20131104"
  paper_id: '113'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1113.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1113.jpg
  title: Mutual Information Maximization for Simple and Accurate Part-Of-Speech Induction
  title_html: Mutual Information Maximization for Simple and Accurate Part-Of-Speech
    Induction
  url: https://www.aclweb.org/anthology/N19-1113
  year: '2019'
N19-1114:
  abstract: Recurrent neural network grammars (RNNG) are generative models of language
    which jointly model syntax and surface structure by incrementally generating a
    syntax tree and sentence in a top-down, left-to-right order. Supervised RNNGs
    achieve strong language modeling and parsing performance, but require an annotated
    corpus of parse trees. In this work, we experiment with unsupervised learning
    of RNNGs. Since directly marginalizing over the space of latent trees is intractable,
    we instead apply amortized variational inference. To maximize the evidence lower
    bound, we develop an inference network parameterized as a neural CRF constituency
    parser. On language modeling, unsupervised RNNGs perform as well their supervised
    counterparts on benchmarks in English and Chinese. On constituency grammar induction,
    they are competitive with recent neural language models that induce tree structures
    from words through attention mechanisms.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/364668925
    type: video
    url: https://vimeo.com/364668925
  author:
  - first: Yoon
    full: Yoon Kim
    id: yoon-kim
    last: Kim
  - first: Alexander
    full: Alexander Rush
    id: alexander-m-rush
    last: Rush
  - first: Lei
    full: Lei Yu
    id: lei-yu
    last: Yu
  - first: Adhiguna
    full: Adhiguna Kuncoro
    id: adhiguna-kuncoro
    last: Kuncoro
  - first: Chris
    full: Chris Dyer
    id: chris-dyer
    last: Dyer
  - first: "G\xE1bor"
    full: "G\xE1bor Melis"
    id: gabor-melis
    last: Melis
  author_string: "Yoon Kim, Alexander Rush, Lei Yu, Adhiguna Kuncoro, Chris Dyer,\
    \ G\xE1bor Melis"
  bibkey: kim-etal-2019-unsupervised
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1114
  month: June
  page_first: '1105'
  page_last: '1117'
  pages: "1105\u20131117"
  paper_id: '114'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1114.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1114.jpg
  title: Unsupervised Recurrent Neural Network Grammars
  title_html: Unsupervised Recurrent Neural Network Grammars
  url: https://www.aclweb.org/anthology/N19-1114
  year: '2019'
N19-1115:
  abstract: "There has been considerable attention devoted to models that learn to\
    \ jointly infer an expression\u2019s syntactic structure and its semantics. Yet,\
    \ Nangia and Bowman (2018) has recently shown that the current best systems fail\
    \ to learn the correct parsing strategy on mathematical expressions generated\
    \ from a simple context-free grammar. In this work, we present a recursive model\
    \ inspired by Choi et al. (2018) that reaches near perfect accuracy on this task.\
    \ Our model is composed of two separated modules for syntax and semantics. They\
    \ are cooperatively trained with standard continuous and discrete optimisation\
    \ schemes. Our model does not require any linguistic structure for supervision,\
    \ and its recursive nature allows for out-of-domain generalisation. Additionally,\
    \ our approach performs competitively on several natural language tasks, such\
    \ as Natural Language Inference and Sentiment Analysis."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1115.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1115.Presentation.pdf
  - filename: https://vimeo.com/364675378
    type: video
    url: https://vimeo.com/364675378
  author:
  - first: Serhii
    full: Serhii Havrylov
    id: serhii-havrylov
    last: Havrylov
  - first: "Germ\xE1n"
    full: "Germ\xE1n Kruszewski"
    id: german-kruszewski
    last: Kruszewski
  - first: Armand
    full: Armand Joulin
    id: armand-joulin
    last: Joulin
  author_string: "Serhii Havrylov, Germ\xE1n Kruszewski, Armand Joulin"
  bibkey: havrylov-etal-2019-cooperative
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1115
  month: June
  page_first: '1118'
  page_last: '1128'
  pages: "1118\u20131128"
  paper_id: '115'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1115.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1115.jpg
  title: Cooperative Learning of Disjoint Syntax and Semantics
  title_html: Cooperative Learning of Disjoint Syntax and Semantics
  url: https://www.aclweb.org/anthology/N19-1115
  year: '2019'
N19-1116:
  abstract: We introduce the deep inside-outside recursive autoencoder (DIORA), a
    fully-unsupervised method for discovering syntax that simultaneously learns representations
    for constituents within the induced tree. Our approach predicts each word in an
    input sentence conditioned on the rest of the sentence. During training we use
    dynamic programming to consider all possible binary trees over the sentence, and
    for inference we use the CKY algorithm to extract the highest scoring parse. DIORA
    outperforms previously reported results for unsupervised binary constituency parsing
    on the benchmark WSJ dataset.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/364678824
    type: video
    url: https://vimeo.com/364678824
  author:
  - first: Andrew
    full: Andrew Drozdov
    id: andrew-drozdov
    last: Drozdov
  - first: Patrick
    full: Patrick Verga
    id: patrick-verga
    last: Verga
  - first: Mohit
    full: Mohit Yadav
    id: mohit-yadav
    last: Yadav
  - first: Mohit
    full: Mohit Iyyer
    id: mohit-iyyer
    last: Iyyer
  - first: Andrew
    full: Andrew McCallum
    id: andrew-mccallum
    last: McCallum
  author_string: Andrew Drozdov, Patrick Verga, Mohit Yadav, Mohit Iyyer, Andrew McCallum
  bibkey: drozdov-etal-2019-unsupervised-latent
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1116
  month: June
  page_first: '1129'
  page_last: '1141'
  pages: "1129\u20131141"
  paper_id: '116'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1116.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1116.jpg
  title: Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Auto-Encoders
  title_html: Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive
    Auto-Encoders
  url: https://www.aclweb.org/anthology/N19-1116
  year: '2019'
N19-1117:
  abstract: Traditional language models are unable to efficiently model entity names
    observed in text. All but the most popular named entities appear infrequently
    in text providing insufficient context. Recent efforts have recognized that context
    can be generalized between entity names that share the same type (e.g., person
    or location) and have equipped language models with an access to external knowledge
    base (KB). Our Knowledge-Augmented Language Model (KALM) continues this line of
    work by augmenting a traditional model with a KB. Unlike previous methods, however,
    we train with an end-to-end predictive objective optimizing the perplexity of
    text. We do not require any additional information such as named entity tags.
    In addition to improving language modeling performance, KALM learns to recognize
    named entities in an entirely unsupervised way by using entity type information
    latent in the model. On a Named Entity Recognition (NER) task, KALM achieves performance
    comparable with state-of-the-art supervised models. Our work demonstrates that
    named entities (and possibly other types of world knowledge) can be modeled successfully
    using predictive learning and training on large corpora of text without any additional
    information.
  address: Minneapolis, Minnesota
  author:
  - first: Angli
    full: Angli Liu
    id: angli-liu
    last: Liu
  - first: Jingfei
    full: Jingfei Du
    id: jingfei-du
    last: Du
  - first: Veselin
    full: Veselin Stoyanov
    id: veselin-stoyanov
    last: Stoyanov
  author_string: Angli Liu, Jingfei Du, Veselin Stoyanov
  bibkey: liu-etal-2019-knowledge-augmented
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1117
  month: June
  page_first: '1142'
  page_last: '1150'
  pages: "1142\u20131150"
  paper_id: '117'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1117.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1117.jpg
  title: Knowledge-Augmented Language Model and Its Application to Unsupervised Named-Entity
    Recognition
  title_html: Knowledge-Augmented Language Model and Its Application to Unsupervised
    Named-Entity Recognition
  url: https://www.aclweb.org/anthology/N19-1117
  year: '2019'
N19-1118:
  abstract: Syntax has been demonstrated highly effective in neural machine translation
    (NMT). Previous NMT models integrate syntax by representing 1-best tree outputs
    from a well-trained parsing system, e.g., the representative Tree-RNN and Tree-Linearization
    methods, which may suffer from error propagation. In this work, we propose a novel
    method to integrate source-side syntax implicitly for NMT. The basic idea is to
    use the intermediate hidden representations of a well-trained end-to-end dependency
    parser, which are referred to as syntax-aware word representations (SAWRs). Then,
    we simply concatenate such SAWRs with ordinary word embeddings to enhance basic
    NMT models. The method can be straightforwardly integrated into the widely-used
    sequence-to-sequence (Seq2Seq) NMT models. We start with a representative RNN-based
    Seq2Seq baseline system, and test the effectiveness of our proposed method on
    two benchmark datasets of the Chinese-English and English-Vietnamese translation
    tasks, respectively. Experimental results show that the proposed approach is able
    to bring significant BLEU score improvements on the two datasets compared with
    the baseline, 1.74 points for Chinese-English translation and 0.80 point for English-Vietnamese
    translation, respectively. In addition, the approach also outperforms the explicit
    Tree-RNN and Tree-Linearization methods.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/347403902
    type: video
    url: https://vimeo.com/347403902
  author:
  - first: Meishan
    full: Meishan Zhang
    id: meishan-zhang
    last: Zhang
  - first: Zhenghua
    full: Zhenghua Li
    id: zhenghua-li
    last: Li
  - first: Guohong
    full: Guohong Fu
    id: guohong-fu
    last: Fu
  - first: Min
    full: Min Zhang
    id: min-zhang
    last: Zhang
  author_string: Meishan Zhang, Zhenghua Li, Guohong Fu, Min Zhang
  bibkey: zhang-etal-2019-syntax-enhanced
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1118
  month: June
  page_first: '1151'
  page_last: '1161'
  pages: "1151\u20131161"
  paper_id: '118'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1118.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1118.jpg
  title: Syntax-Enhanced Neural Machine Translation with Syntax-Aware Word Representations
  title_html: Syntax-Enhanced Neural Machine Translation with Syntax-Aware Word Representations
  url: https://www.aclweb.org/anthology/N19-1118
  year: '2019'
N19-1119:
  abstract: Current state-of-the-art NMT systems use large neural networks that are
    not only slow to train, but also often require many heuristics and optimization
    tricks, such as specialized learning rate schedules and large batch sizes. This
    is undesirable as it requires extensive hyperparameter tuning. In this paper,
    we propose a curriculum learning framework for NMT that reduces training time,
    reduces the need for specialized heuristics or large batch sizes, and results
    in overall better performance. Our framework consists of a principled way of deciding
    which training samples are shown to the model at different times during training,
    based on the estimated difficulty of a sample and the current competence of the
    model. Filtering training samples in this manner prevents the model from getting
    stuck in bad local optima, making it converge faster and reach a better solution
    than the common approach of uniformly sampling training examples. Furthermore,
    the proposed method can be easily applied to existing NMT models by simply modifying
    their input data pipelines. We show that our framework can help improve the training
    time and the performance of both recurrent neural network models and Transformers,
    achieving up to a 70% decrease in training time, while at the same time obtaining
    accuracy improvements of up to 2.2 BLEU.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/347406566
    type: video
    url: https://vimeo.com/347406566
  author:
  - first: Emmanouil Antonios
    full: Emmanouil Antonios Platanios
    id: emmanouil-antonios-platanios
    last: Platanios
  - first: Otilia
    full: Otilia Stretcu
    id: otilia-stretcu
    last: Stretcu
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  - first: Barnabas
    full: Barnabas Poczos
    id: barnabas-poczos
    last: Poczos
  - first: Tom
    full: Tom Mitchell
    id: tom-mitchell
    last: Mitchell
  author_string: Emmanouil Antonios Platanios, Otilia Stretcu, Graham Neubig, Barnabas
    Poczos, Tom Mitchell
  bibkey: platanios-etal-2019-competence
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1119
  month: June
  page_first: '1162'
  page_last: '1172'
  pages: "1162\u20131172"
  paper_id: '119'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1119.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1119.jpg
  title: Competence-based Curriculum Learning for Neural Machine Translation
  title_html: Competence-based Curriculum Learning for Neural Machine Translation
  url: https://www.aclweb.org/anthology/N19-1119
  year: '2019'
N19-1120:
  abstract: The overreliance on large parallel corpora significantly limits the applicability
    of machine translation systems to the majority of language pairs. Back-translation
    has been dominantly used in previous approaches for unsupervised neural machine
    translation, where pseudo sentence pairs are generated to train the models with
    a reconstruction loss. However, the pseudo sentences are usually of low quality
    as translation errors accumulate during training. To avoid this fundamental issue,
    we propose an alternative but more effective approach, extract-edit, to extract
    and then edit real sentences from the target monolingual corpora. Furthermore,
    we introduce a comparative translation loss to evaluate the translated target
    sentences and thus train the unsupervised translation systems. Experiments show
    that the proposed approach consistently outperforms the previous state-of-the-art
    unsupervised machine translation systems across two benchmarks (English-French
    and English-German) and two low-resource language pairs (English-Romanian and
    English-Russian) by more than 2 (up to 3.63) BLEU points.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/347408805
    type: video
    url: https://vimeo.com/347408805
  author:
  - first: Jiawei
    full: Jiawei Wu
    id: jiawei-wu
    last: Wu
  - first: Xin
    full: Xin Wang
    id: xin-wang
    last: Wang
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  author_string: Jiawei Wu, Xin Wang, William Yang Wang
  bibkey: wu-etal-2019-extract
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1120
  month: June
  page_first: '1173'
  page_last: '1183'
  pages: "1173\u20131183"
  paper_id: '120'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1120.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1120.jpg
  title: 'Extract and Edit: An Alternative to Back-Translation for Unsupervised Neural
    Machine Translation'
  title_html: 'Extract and Edit: An Alternative to Back-Translation for Unsupervised
    Neural Machine Translation'
  url: https://www.aclweb.org/anthology/N19-1120
  year: '2019'
N19-1121:
  abstract: "Generalization and reliability of multilingual translation often highly\
    \ depend on the amount of available parallel data for each language pair of interest.\
    \ In this paper, we focus on zero-shot generalization\u2014a challenging setup\
    \ that tests models on translation directions they have not been optimized for\
    \ at training time. To solve the problem, we (i) reformulate multilingual translation\
    \ as probabilistic inference, (ii) define the notion of zero-shot consistency\
    \ and show why standard training often results in models unsuitable for zero-shot\
    \ tasks, and (iii) introduce a consistent agreement-based training method that\
    \ encourages the model to produce equivalent translations of parallel sentences\
    \ in auxiliary languages. We test our multilingual NMT models on multiple public\
    \ zero-shot translation benchmarks (IWSLT17, UN corpus, Europarl) and show that\
    \ agreement-based learning often results in 2-3 BLEU zero-shot improvement over\
    \ strong baselines without any loss in performance on supervised translation directions."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/347411013
    type: video
    url: https://vimeo.com/347411013
  author:
  - first: Maruan
    full: Maruan Al-Shedivat
    id: maruan-al-shedivat
    last: Al-Shedivat
  - first: Ankur
    full: Ankur Parikh
    id: ankur-parikh
    last: Parikh
  author_string: Maruan Al-Shedivat, Ankur Parikh
  bibkey: al-shedivat-parikh-2019-consistency
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1121
  month: June
  page_first: '1184'
  page_last: '1197'
  pages: "1184\u20131197"
  paper_id: '121'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1121.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1121.jpg
  title: Consistency by Agreement in Zero-Shot Neural Machine Translation
  title_html: Consistency by Agreement in Zero-Shot Neural Machine Translation
  url: https://www.aclweb.org/anthology/N19-1121
  year: '2019'
N19-1122:
  abstract: "Recently, the Transformer model that is based solely on attention mechanisms,\
    \ has advanced the state-of-the-art on various machine translation tasks. However,\
    \ recent studies reveal that the lack of recurrence modeling hinders its further\
    \ improvement of translation capacity. In response to this problem, we propose\
    \ to directly model recurrence for Transformer with an additional recurrence encoder.\
    \ In addition to the standard recurrent neural network, we introduce a novel attentive\
    \ recurrent network to leverage the strengths of both attention models and recurrent\
    \ networks. Experimental results on the widely-used WMT14 English\u21D2German\
    \ and WMT17 Chinese\u21D2English translation tasks demonstrate the effectiveness\
    \ of the proposed approach. Our studies also reveal that the proposed model benefits\
    \ from a short-cut that bridges the source and target sequences with a single\
    \ recurrent layer, which outperforms its deep counterpart."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/347413361
    type: video
    url: https://vimeo.com/347413361
  author:
  - first: Jie
    full: Jie Hao
    id: jie-hao
    last: Hao
  - first: Xing
    full: Xing Wang
    id: xing-wang
    last: Wang
  - first: Baosong
    full: Baosong Yang
    id: baosong-yang
    last: Yang
  - first: Longyue
    full: Longyue Wang
    id: longyue-wang
    last: Wang
  - first: Jinfeng
    full: Jinfeng Zhang
    id: jinfeng-zhang
    last: Zhang
  - first: Zhaopeng
    full: Zhaopeng Tu
    id: zhaopeng-tu
    last: Tu
  author_string: Jie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, Zhaopeng
    Tu
  bibkey: hao-etal-2019-modeling
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1122
  month: June
  page_first: '1198'
  page_last: '1207'
  pages: "1198\u20131207"
  paper_id: '122'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1122.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1122.jpg
  title: Modeling Recurrence for Transformer
  title_html: Modeling Recurrence for Transformer
  url: https://www.aclweb.org/anthology/N19-1122
  year: '2019'
N19-1123:
  abstract: Defining action spaces for conversational agents and optimizing their
    decision-making process with reinforcement learning is an enduring challenge.
    Common practice has been to use handcrafted dialog acts, or the output vocabulary,
    e.g. in neural encoder decoders, as the action spaces. Both have their own limitations.
    This paper proposes a novel latent action framework that treats the action spaces
    of an end-to-end dialog agent as latent variables and develops unsupervised methods
    in order to induce its own action space from the data. Comprehensive experiments
    are conducted examining both continuous and discrete action types and two different
    optimization methods based on stochastic variational inference. Results show that
    the proposed latent actions achieve superior empirical performance improvement
    over previous word-level policy gradient methods on both DealOrNoDeal and MultiWoz
    dialogs. Our detailed analysis also provides insights about various latent variable
    approaches for policy learning and can serve as a foundation for developing better
    latent actions in future research.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/360620730
    type: video
    url: https://vimeo.com/360620730
  author:
  - first: Tiancheng
    full: Tiancheng Zhao
    id: tiancheng-zhao
    last: Zhao
  - first: Kaige
    full: Kaige Xie
    id: kaige-xie
    last: Xie
  - first: Maxine
    full: Maxine Eskenazi
    id: maxine-eskenazi
    last: Eskenazi
  author_string: Tiancheng Zhao, Kaige Xie, Maxine Eskenazi
  bibkey: zhao-etal-2019-rethinking
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1123
  month: June
  page_first: '1208'
  page_last: '1218'
  pages: "1208\u20131218"
  paper_id: '123'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1123.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1123.jpg
  title: Rethinking Action Spaces for Reinforcement Learning in End-to-end Dialog
    Agents with Latent Variable Models
  title_html: Rethinking Action Spaces for Reinforcement Learning in End-to-end Dialog
    Agents with Latent Variable Models
  url: https://www.aclweb.org/anthology/N19-1123
  year: '2019'
N19-1124:
  abstract: Traditional generative dialogue models generate responses solely from
    input queries. Such information is insufficient for generating a specific response
    since a certain query could be answered in multiple ways. Recently, researchers
    have attempted to fill the information gap by exploiting information retrieval
    techniques. For a given query, similar dialogues are retrieved from the entire
    training data and considered as an additional knowledge source. While the use
    of retrieval may harvest extensive information, the generative models could be
    overwhelmed, leading to unsatisfactory performance. In this paper, we propose
    a new framework which exploits retrieval results via a skeleton-to-response paradigm.
    At first, a skeleton is extracted from the retrieved dialogues. Then, both the
    generated skeleton and the original query are used for response generation via
    a novel response generator. Experimental results show that our approach significantly
    improves the informativeness of the generated responses
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/360630558
    type: video
    url: https://vimeo.com/360630558
  author:
  - first: Deng
    full: Deng Cai
    id: deng-cai
    last: Cai
  - first: Yan
    full: Yan Wang
    id: yan-wang
    last: Wang
  - first: Wei
    full: Wei Bi
    id: wei-bi
    last: Bi
  - first: Zhaopeng
    full: Zhaopeng Tu
    id: zhaopeng-tu
    last: Tu
  - first: Xiaojiang
    full: Xiaojiang Liu
    id: xiaojiang-liu
    last: Liu
  - first: Wai
    full: Wai Lam
    id: wai-lam
    last: Lam
  - first: Shuming
    full: Shuming Shi
    id: shuming-shi
    last: Shi
  author_string: Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, Wai Lam,
    Shuming Shi
  bibkey: cai-etal-2019-skeleton
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1124
  month: June
  page_first: '1219'
  page_last: '1228'
  pages: "1219\u20131228"
  paper_id: '124'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1124.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1124.jpg
  title: 'Skeleton-to-Response: Dialogue Generation Guided by Retrieval Memory'
  title_html: 'Skeleton-to-Response: Dialogue Generation Guided by Retrieval Memory'
  url: https://www.aclweb.org/anthology/N19-1124
  year: '2019'
N19-1125:
  abstract: Although recent neural conversation models have shown great potential,
    they often generate bland and generic responses. While various approaches have
    been explored to diversify the output of the conversation model, the improvement
    often comes at the cost of decreased relevance. In this paper, we propose a SpaceFusion
    model to jointly optimize diversity and relevance that essentially fuses the latent
    space of a sequence-to-sequence model and that of an autoencoder model by leveraging
    novel regularization terms. As a result, our approach induces a latent space in
    which the distance and direction from the predicted response vector roughly match
    the relevance and diversity, respectively. This property also lends itself well
    to an intuitive visualization of the latent space. Both automatic and human evaluation
    results demonstrate that the proposed approach brings significant improvement
    compared to strong baselines in both diversity and relevance.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/360643280
    type: video
    url: https://vimeo.com/360643280
  author:
  - first: Xiang
    full: Xiang Gao
    id: xiang-gao
    last: Gao
  - first: Sungjin
    full: Sungjin Lee
    id: sungjin-lee
    last: Lee
  - first: Yizhe
    full: Yizhe Zhang
    id: yizhe-zhang
    last: Zhang
  - first: Chris
    full: Chris Brockett
    id: chris-brockett
    last: Brockett
  - first: Michel
    full: Michel Galley
    id: michel-galley
    last: Galley
  - first: Jianfeng
    full: Jianfeng Gao
    id: jianfeng-gao
    last: Gao
  - first: Bill
    full: Bill Dolan
    id: bill-dolan
    last: Dolan
  author_string: Xiang Gao, Sungjin Lee, Yizhe Zhang, Chris Brockett, Michel Galley,
    Jianfeng Gao, Bill Dolan
  bibkey: gao-etal-2019-jointly
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1125
  month: June
  page_first: '1229'
  page_last: '1238'
  pages: "1229\u20131238"
  paper_id: '125'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1125.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1125.jpg
  title: Jointly Optimizing Diversity and Relevance in Neural Response Generation
  title_html: Jointly Optimizing Diversity and Relevance in Neural Response Generation
  url: https://www.aclweb.org/anthology/N19-1125
  year: '2019'
N19-1126:
  abstract: "The Knowledge Base (KB) used for real-world applications, such as booking\
    \ a movie or restaurant reservation, keeps changing over time. End-to-end neural\
    \ networks trained for these task-oriented dialogs are expected to be immune to\
    \ any changes in the KB. However, existing approaches breakdown when asked to\
    \ handle such changes. We propose an encoder-decoder architecture (BoSsNet) with\
    \ a novel Bag-of-Sequences (BoSs) memory, which facilitates the disentangled learning\
    \ of the response\u2019s language model and its knowledge incorporation. Consequently,\
    \ the KB can be modified with new knowledge without a drop in interpretability.\
    \ We find that BoSsNeT outperforms state-of-the-art models, with considerable\
    \ improvements (>10%) on bAbI OOV test sets and other human-human datasets. We\
    \ also systematically modify existing datasets to measure disentanglement and\
    \ show BoSsNeT to be robust to KB modifications."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1126.Software.zip
    type: software
    url: https://www.aclweb.org/anthology/attachments/N19-1126.Software.zip
  - filename: https://vimeo.com/360661118
    type: video
    url: https://vimeo.com/360661118
  author:
  - first: Dinesh
    full: Dinesh Raghu
    id: dinesh-raghu
    last: Raghu
  - first: Nikhil
    full: Nikhil Gupta
    id: nikhil-gupta
    last: Gupta
  - first: ''
    full: Mausam
    id: mausam
    last: Mausam
  author_string: Dinesh Raghu, Nikhil Gupta, Mausam
  bibkey: raghu-etal-2019-disentangling
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1126
  month: June
  page_first: '1239'
  page_last: '1255'
  pages: "1239\u20131255"
  paper_id: '126'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1126.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1126.jpg
  title: Disentangling Language and Knowledge in Task-Oriented Dialogs
  title_html: <span class="acl-fixed-case">D</span>isentangling <span class="acl-fixed-case">L</span>anguage
    and <span class="acl-fixed-case">K</span>nowledge in <span class="acl-fixed-case">T</span>ask-<span
    class="acl-fixed-case">O</span>riented <span class="acl-fixed-case">D</span>ialogs
  url: https://www.aclweb.org/anthology/N19-1126
  year: '2019'
N19-1127:
  abstract: "Neural networks equipped with self-attention have parallelizable computation,\
    \ light-weight structure, and the ability to capture both long-range and local\
    \ dependencies. Further, their expressive power and performance can be boosted\
    \ by using a vector to measure pairwise dependency, but this requires to expand\
    \ the alignment matrix to a tensor, which results in memory and computation bottlenecks.\
    \ In this paper, we propose a novel attention mechanism called \u201CMulti-mask\
    \ Tensorized Self-Attention\u201D (MTSA), which is as fast and as memory-efficient\
    \ as a CNN, but significantly outperforms previous CNN-/RNN-/attention-based models.\
    \ MTSA 1) captures both pairwise (token2token) and global (source2token) dependencies\
    \ by a novel compatibility function composed of dot-product and additive attentions,\
    \ 2) uses a tensor to represent the feature-wise alignment scores for better expressive\
    \ power but only requires parallelizable matrix multiplications, and 3) combines\
    \ multi-head with multi-dimensional attentions, and applies a distinct positional\
    \ mask to each head (subspace), so the memory and computation can be distributed\
    \ to multiple heads, each with sequential information encoded independently. The\
    \ experiments show that a CNN/RNN-free model based on MTSA achieves state-of-the-art\
    \ or competitive performance on nine NLP benchmarks with compelling memory- and\
    \ time-efficiency."
  address: Minneapolis, Minnesota
  author:
  - first: Tao
    full: Tao Shen
    id: tao-shen
    last: Shen
  - first: Tianyi
    full: Tianyi Zhou
    id: tianyi-zhou
    last: Zhou
  - first: Guodong
    full: Guodong Long
    id: guodong-long
    last: Long
  - first: Jing
    full: Jing Jiang
    id: jing-jiang
    last: Jiang
  - first: Chengqi
    full: Chengqi Zhang
    id: chengqi-zhang
    last: Zhang
  author_string: Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Chengqi Zhang
  bibkey: shen-etal-2019-tensorized
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1127
  month: June
  page_first: '1256'
  page_last: '1266'
  pages: "1256\u20131266"
  paper_id: '127'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1127.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1127.jpg
  title: 'Tensorized Self-Attention: Efficiently Modeling Pairwise and Global Dependencies
    Together'
  title_html: 'Tensorized Self-Attention: Efficiently Modeling Pairwise and Global
    Dependencies Together'
  url: https://www.aclweb.org/anthology/N19-1127
  year: '2019'
N19-1128:
  abstract: "By design, word embeddings are unable to model the dynamic nature of\
    \ words\u2019 semantics, i.e., the property of words to correspond to potentially\
    \ different meanings. To address this limitation, dozens of specialized meaning\
    \ representation techniques such as sense or contextualized embeddings have been\
    \ proposed. However, despite the popularity of research on this topic, very few\
    \ evaluation benchmarks exist that specifically focus on the dynamic semantics\
    \ of words. In this paper we show that existing models have surpassed the performance\
    \ ceiling of the standard evaluation dataset for the purpose, i.e., Stanford Contextual\
    \ Word Similarity, and highlight its shortcomings. To address the lack of a suitable\
    \ benchmark, we put forward a large-scale Word in Context dataset, called WiC,\
    \ based on annotations curated by experts, for generic evaluation of context-sensitive\
    \ representations. WiC is released in https://pilehvar.github.io/wic/."
  address: Minneapolis, Minnesota
  author:
  - first: Mohammad Taher
    full: Mohammad Taher Pilehvar
    id: mohammad-taher-pilehvar
    last: Pilehvar
  - first: Jose
    full: Jose Camacho-Collados
    id: jose-camacho-collados
    last: Camacho-Collados
  author_string: Mohammad Taher Pilehvar, Jose Camacho-Collados
  bibkey: pilehvar-camacho-collados-2019-wic
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1128
  month: June
  page_first: '1267'
  page_last: '1273'
  pages: "1267\u20131273"
  paper_id: '128'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1128.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1128.jpg
  title: 'WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning
    Representations'
  title_html: '<span class="acl-fixed-case">W</span>i<span class="acl-fixed-case">C</span>:
    the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations'
  url: https://www.aclweb.org/anthology/N19-1128
  year: '2019'
N19-1129:
  abstract: "Peer review is a core element of the scientific process, particularly\
    \ in conference-centered fields such as ML and NLP. However, only few studies\
    \ have evaluated its properties empirically. Aiming to fill this gap, we present\
    \ a corpus that contains over 4k reviews and 1.2k author responses from ACL-2018.\
    \ We quantitatively and qualitatively assess the corpus. This includes a pilot\
    \ study on paper weaknesses given by reviewers and on quality of author responses.\
    \ We then focus on the role of the rebuttal phase, and propose a novel task to\
    \ predict after-rebuttal (i.e., final) scores from initial reviews and author\
    \ responses. Although author responses do have a marginal (and statistically significant)\
    \ influence on the final scores, especially for borderline papers, our results\
    \ suggest that a reviewer\u2019s final score is largely determined by her initial\
    \ score and the distance to the other reviewers\u2019 initial scores. In this\
    \ context, we discuss the conformity bias inherent to peer reviewing, a bias that\
    \ has largely been overlooked in previous research. We hope our analyses will\
    \ help better assess the usefulness of the rebuttal phase in NLP conferences."
  address: Minneapolis, Minnesota
  author:
  - first: Yang
    full: Yang Gao
    id: yang-gao
    last: Gao
  - first: Steffen
    full: Steffen Eger
    id: steffen-eger
    last: Eger
  - first: Ilia
    full: Ilia Kuznetsov
    id: ilia-kuznetsov
    last: Kuznetsov
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  - first: Yusuke
    full: Yusuke Miyao
    id: yusuke-miyao
    last: Miyao
  author_string: Yang Gao, Steffen Eger, Ilia Kuznetsov, Iryna Gurevych, Yusuke Miyao
  bibkey: gao-etal-2019-rebuttal
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1129
  month: June
  page_first: '1274'
  page_last: '1290'
  pages: "1274\u20131290"
  paper_id: '129'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1129.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1129.jpg
  title: Does My Rebuttal Matter? Insights from a Major NLP Conference
  title_html: Does My Rebuttal Matter? Insights from a Major <span class="acl-fixed-case">NLP</span>
    Conference
  url: https://www.aclweb.org/anthology/N19-1129
  year: '2019'
N19-1130:
  abstract: "Literary critics often attempt to uncover meaning in a single work of\
    \ literature through careful reading and analysis. Applying natural language processing\
    \ methods to aid in such literary analyses remains a challenge in digital humanities.\
    \ While most previous work focuses on \u201Cdistant reading\u201D by algorithmically\
    \ discovering high-level patterns from large collections of literary works, here\
    \ we sharpen the focus of our methods to a single literary theory about Italo\
    \ Calvino\u2019s postmodern novel Invisible Cities, which consists of 55 short\
    \ descriptions of imaginary cities. Calvino has provided a classification of these\
    \ cities into eleven thematic groups, but literary scholars disagree as to how\
    \ trustworthy his categorization is. Due to the unique structure of this novel,\
    \ we can computationally weigh in on this debate: we leverage pretrained contextualized\
    \ representations to embed each city\u2019s description and use unsupervised methods\
    \ to cluster these embeddings. Additionally, we compare results of our computational\
    \ approach to similarity judgments generated by human readers. Our work is a first\
    \ step towards incorporating natural language processing into literary criticism."
  address: Minneapolis, Minnesota
  author:
  - first: Shufan
    full: Shufan Wang
    id: shufan-wang
    last: Wang
  - first: Mohit
    full: Mohit Iyyer
    id: mohit-iyyer
    last: Iyyer
  author_string: Shufan Wang, Mohit Iyyer
  bibkey: wang-iyyer-2019-casting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1130
  month: June
  page_first: '1291'
  page_last: '1297'
  pages: "1291\u20131297"
  paper_id: '130'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1130.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1130.jpg
  title: 'Casting Light on Invisible Cities: Computationally Engaging with Literary
    Criticism'
  title_html: '<span class="acl-fixed-case">C</span>asting <span class="acl-fixed-case">L</span>ight
    on <span class="acl-fixed-case">I</span>nvisible <span class="acl-fixed-case">C</span>ities:
    <span class="acl-fixed-case">C</span>omputationally <span class="acl-fixed-case">E</span>ngaging
    with <span class="acl-fixed-case">L</span>iterary <span class="acl-fixed-case">C</span>riticism'
  url: https://www.aclweb.org/anthology/N19-1130
  year: '2019'
N19-1131:
  abstract: Existing paraphrase identification datasets lack sentence pairs that have
    high lexical overlap without being paraphrases. Models trained on such data fail
    to distinguish pairs like flights from New York to Florida and flights from Florida
    to New York. This paper introduces PAWS (Paraphrase Adversaries from Word Scrambling),
    a new dataset with 108,463 well-formed paraphrase and non-paraphrase pairs with
    high lexical overlap. Challenging pairs are generated by controlled word swapping
    and back translation, followed by fluency and paraphrase judgments by human raters.
    State-of-the-art models trained on existing datasets have dismal performance on
    PAWS (<40% accuracy); however, including PAWS training data for these models improves
    their accuracy to 85% while maintaining performance on existing tasks. In contrast,
    models that do not capture non-local contextual information fail even with PAWS
    training examples. As such, PAWS provides an effective instrument for driving
    further progress on models that better exploit structure, context, and pairwise
    comparisons.
  address: Minneapolis, Minnesota
  author:
  - first: Yuan
    full: Yuan Zhang
    id: yuan-zhang
    last: Zhang
  - first: Jason
    full: Jason Baldridge
    id: jason-baldridge
    last: Baldridge
  - first: Luheng
    full: Luheng He
    id: luheng-he
    last: He
  author_string: Yuan Zhang, Jason Baldridge, Luheng He
  bibkey: zhang-etal-2019-paws
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1131
  month: June
  page_first: '1298'
  page_last: '1308'
  pages: "1298\u20131308"
  paper_id: '131'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1131.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1131.jpg
  title: 'PAWS: Paraphrase Adversaries from Word Scrambling'
  title_html: '<span class="acl-fixed-case">PAWS</span>: Paraphrase Adversaries from
    Word Scrambling'
  url: https://www.aclweb.org/anthology/N19-1131
  year: '2019'
N19-1132:
  abstract: "This study explores the necessity of performing cross-corpora evaluation\
    \ for grammatical error correction (GEC) models. GEC models have been previously\
    \ evaluated based on a single commonly applied corpus: the CoNLL-2014 benchmark.\
    \ However, the evaluation remains incomplete because the task difficulty varies\
    \ depending on the test corpus and conditions such as the proficiency levels of\
    \ the writers and essay topics. To overcome this limitation, we evaluate the performance\
    \ of several GEC models, including NMT-based (LSTM, CNN, and transformer) and\
    \ an SMT-based model, against various learner corpora (CoNLL-2013, CoNLL-2014,\
    \ FCE, JFLEG, ICNALE, and KJ). Evaluation results reveal that the models\u2019\
    \ rankings considerably vary depending on the corpus, indicating that single-corpus\
    \ evaluation is insufficient for GEC models."
  address: Minneapolis, Minnesota
  author:
  - first: Masato
    full: Masato Mita
    id: masato-mita
    last: Mita
  - first: Tomoya
    full: Tomoya Mizumoto
    id: tomoya-mizumoto
    last: Mizumoto
  - first: Masahiro
    full: Masahiro Kaneko
    id: masahiro-kaneko
    last: Kaneko
  - first: Ryo
    full: Ryo Nagata
    id: ryo-nagata
    last: Nagata
  - first: Kentaro
    full: Kentaro Inui
    id: kentaro-inui
    last: Inui
  author_string: Masato Mita, Tomoya Mizumoto, Masahiro Kaneko, Ryo Nagata, Kentaro
    Inui
  bibkey: mita-etal-2019-cross
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1132
  month: June
  page_first: '1309'
  page_last: '1314'
  pages: "1309\u20131314"
  paper_id: '132'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1132.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1132.jpg
  title: "Cross-Corpora Evaluation and Analysis of Grammatical Error Correction Models\
    \ \u2014 Is Single-Corpus Evaluation Enough?"
  title_html: "Cross-Corpora Evaluation and Analysis of Grammatical Error Correction\
    \ Models \u2014 Is Single-Corpus Evaluation Enough?"
  url: https://www.aclweb.org/anthology/N19-1132
  year: '2019'
N19-1133:
  abstract: Although Transformer has achieved great successes on many NLP tasks, its
    heavy structure with fully-connected attention connections leads to dependencies
    on large training data. In this paper, we present Star-Transformer, a lightweight
    alternative by careful sparsification. To reduce model complexity, we replace
    the fully-connected structure with a star-shaped topology, in which every two
    non-adjacent nodes are connected through a shared relay node. Thus, complexity
    is reduced from quadratic to linear, while preserving the capacity to capture
    both local composition and long-range dependency. The experiments on four tasks
    (22 datasets) show that Star-Transformer achieved significant improvements against
    the standard Transformer for the modestly sized datasets.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1133.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1133.Supplementary.pdf
  author:
  - first: Qipeng
    full: Qipeng Guo
    id: qipeng-guo
    last: Guo
  - first: Xipeng
    full: Xipeng Qiu
    id: xipeng-qiu
    last: Qiu
  - first: Pengfei
    full: Pengfei Liu
    id: pengfei-liu
    last: Liu
  - first: Yunfan
    full: Yunfan Shao
    id: yunfan-shao
    last: Shao
  - first: Xiangyang
    full: Xiangyang Xue
    id: xiangyang-xue
    last: Xue
  - first: Zheng
    full: Zheng Zhang
    id: zheng-zhang
    last: Zhang
  author_string: Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue,
    Zheng Zhang
  bibkey: guo-etal-2019-star
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1133
  month: June
  page_first: '1315'
  page_last: '1325'
  pages: "1315\u20131325"
  paper_id: '133'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1133.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1133.jpg
  title: Star-Transformer
  title_html: Star-Transformer
  url: https://www.aclweb.org/anthology/N19-1133
  year: '2019'
N19-1134:
  abstract: We address the problem of speech act recognition (SAR) in asynchronous
    conversations (forums, emails). Unlike synchronous conversations (e.g., meetings,
    phone), asynchronous domains lack large labeled datasets to train an effective
    SAR model. In this paper, we propose methods to effectively leverage abundant
    unlabeled conversational data and the available labeled data from synchronous
    domains. We carry out our research in three main steps. First, we introduce a
    neural architecture based on hierarchical LSTMs and conditional random fields
    (CRF) for SAR, and show that our method outperforms existing methods when trained
    on in-domain data only. Second, we improve our initial SAR models by semi-supervised
    learning in the form of pretrained word embeddings learned from a large unlabeled
    conversational corpus. Finally, we employ adversarial training to improve the
    results further by leveraging the labeled data from synchronous domains and by
    explicitly modeling the distributional shift in two domains.
  address: Minneapolis, Minnesota
  author:
  - first: Tasnim
    full: Tasnim Mohiuddin
    id: muhammad-tasnim-mohiuddin
    last: Mohiuddin
  - first: Thanh-Tung
    full: Thanh-Tung Nguyen
    id: thanh-tung-nguyen
    last: Nguyen
  - first: Shafiq
    full: Shafiq Joty
    id: shafiq-joty
    last: Joty
  author_string: Tasnim Mohiuddin, Thanh-Tung Nguyen, Shafiq Joty
  bibkey: mohiuddin-etal-2019-adaptation
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1134
  month: June
  page_first: '1326'
  page_last: '1336'
  pages: "1326\u20131336"
  paper_id: '134'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1134.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1134.jpg
  title: Adaptation of Hierarchical Structured Models for Speech Act Recognition in
    Asynchronous Conversation
  title_html: Adaptation of Hierarchical Structured Models for Speech Act Recognition
    in Asynchronous Conversation
  url: https://www.aclweb.org/anthology/N19-1134
  year: '2019'
N19-1135:
  abstract: Advances in the automated detection of offensive Internet postings make
    this mechanism very attractive to social media companies, who are increasingly
    under pressure to monitor and action activity on their sites. However, these advances
    also have important implications as a threat to the fundamental right of free
    expression. In this article, we analyze which Twitter posts could actually be
    deemed offenses under German criminal law. German law follows the deductive method
    of the Roman law tradition based on abstract rules as opposed to the inductive
    reasoning in Anglo-American common law systems. This allows us to show how legal
    conclusions can be reached and implemented without relying on existing court decisions.
    We present a data annotation schema, consisting of a series of binary decisions,
    for determining whether a specific post would constitute a criminal offense. This
    schema serves as a step towards an inexpensive creation of a sufficient amount
    of data for an automated classification. We find that the majority of posts deemed
    offensive actually do not constitute a criminal offense and still contribute to
    public discourse. Furthermore, laymen can provide sufficiently reliable data to
    an expert reference but are, for instance, more lenient in the interpretation
    of what constitutes a disparaging statement.
  address: Minneapolis, Minnesota
  author:
  - first: Frederike
    full: Frederike Zufall
    id: frederike-zufall
    last: Zufall
  - first: Tobias
    full: Tobias Horsmann
    id: tobias-horsmann
    last: Horsmann
  - first: Torsten
    full: Torsten Zesch
    id: torsten-zesch
    last: Zesch
  author_string: Frederike Zufall, Tobias Horsmann, Torsten Zesch
  bibkey: zufall-etal-2019-legal
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1135
  month: June
  page_first: '1337'
  page_last: '1347'
  pages: "1337\u20131347"
  paper_id: '135'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1135.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1135.jpg
  title: 'From legal to technical concept: Towards an automated classification of
    German political Twitter postings as criminal offenses'
  title_html: 'From legal to technical concept: Towards an automated classification
    of <span class="acl-fixed-case">G</span>erman political Twitter postings as criminal
    offenses'
  url: https://www.aclweb.org/anthology/N19-1135
  year: '2019'
N19-1136:
  abstract: We propose a novel attention network for document annotation with user-generated
    tags. The network is designed according to the human reading and annotation behaviour.
    Usually, users try to digest the title and obtain a rough idea about the topic
    first, and then read the content of the document. Present research shows that
    the title metadata could largely affect the social annotation. To better utilise
    this information, we design a framework that separates the title from the content
    of a document and apply a title-guided attention mechanism over each sentence
    in the content. We also propose two semantic-based loss regularisers that enforce
    the output of the network to conform to label semantics, i.e. similarity and subsumption.
    We analyse each part of the proposed system with two real-world open datasets
    on publication and question annotation. The integrated approach, Joint Multi-label
    Attention Network (JMAN), significantly outperformed the Bidirectional Gated Recurrent
    Unit (Bi-GRU) by around 13%-26% and the Hierarchical Attention Network (HAN) by
    around 4%-12% on both datasets, with around 10%-30% reduction of training time.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1136.Poster.pdf
    type: poster
    url: https://www.aclweb.org/anthology/attachments/N19-1136.Poster.pdf
  author:
  - first: Hang
    full: Hang Dong
    id: hang-dong
    last: Dong
  - first: Wei
    full: Wei Wang
    id: wei-wang
    last: Wang
  - first: Kaizhu
    full: Kaizhu Huang
    id: kaizhu-huang
    last: Huang
  - first: Frans
    full: Frans Coenen
    id: frans-coenen
    last: Coenen
  author_string: Hang Dong, Wei Wang, Kaizhu Huang, Frans Coenen
  bibkey: dong-etal-2019-joint
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1136
  month: June
  page_first: '1348'
  page_last: '1354'
  pages: "1348\u20131354"
  paper_id: '136'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1136.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1136.jpg
  title: Joint Multi-Label Attention Networks for Social Text Annotation
  title_html: Joint Multi-Label Attention Networks for Social Text Annotation
  url: https://www.aclweb.org/anthology/N19-1136
  year: '2019'
N19-1137:
  abstract: The advent of micro-blogging sites has paved the way for researchers to
    collect and analyze huge volumes of data in recent years. Twitter, being one of
    the leading social networking sites worldwide, provides a great opportunity to
    its users for expressing their states of mind via short messages which are called
    tweets. The urgency of identifying emotions and sentiments conveyed through tweets
    has led to several research works. It provides a great way to understand human
    psychology and impose a challenge to researchers to analyze their content easily.
    In this paper, we propose a novel use of a multi-channel convolutional neural
    architecture which can effectively use different emotion and sentiment indicators
    such as hashtags, emoticons and emojis that are present in the tweets and improve
    the performance of emotion and sentiment identification. We also investigate the
    incorporation of different lexical features in the neural network model and its
    effect on the emotion and sentiment identification task. We analyze our model
    on some standard datasets and compare its effectiveness with existing techniques.
  address: Minneapolis, Minnesota
  author:
  - first: Jumayel
    full: Jumayel Islam
    id: jumayel-islam
    last: Islam
  - first: Robert E.
    full: Robert E. Mercer
    id: robert-e-mercer
    last: Mercer
  - first: Lu
    full: Lu Xiao
    id: lu-xiao
    last: Xiao
  author_string: Jumayel Islam, Robert E. Mercer, Lu Xiao
  bibkey: islam-etal-2019-multi
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1137
  month: June
  page_first: '1355'
  page_last: '1365'
  pages: "1355\u20131365"
  paper_id: '137'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1137.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1137.jpg
  title: Multi-Channel Convolutional Neural Network for Twitter Emotion and Sentiment
    Recognition
  title_html: Multi-Channel Convolutional Neural Network for Twitter Emotion and Sentiment
    Recognition
  url: https://www.aclweb.org/anthology/N19-1137
  year: '2019'
N19-1138:
  abstract: It is very critical to analyze messages shared over social networks for
    cyber threat intelligence and cyber-crime prevention. In this study, we propose
    a method that leverages both domain-specific word embeddings and task-specific
    features to detect cyber security events from tweets. Our model employs a convolutional
    neural network (CNN) and a long short-term memory (LSTM) recurrent neural network
    which takes word level meta-embeddings as inputs and incorporates contextual embeddings
    to classify noisy short text. We collected a new dataset of cyber security related
    tweets from Twitter and manually annotated a subset of 2K of them. We experimented
    with this dataset and concluded that the proposed model outperforms both traditional
    and neural baselines. The results suggest that our method works well for detecting
    cyber security events from noisy short text.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1138.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1138.Supplementary.pdf
  author:
  - first: Semih
    full: Semih Yagcioglu
    id: semih-yagcioglu
    last: Yagcioglu
  - first: Mehmet Saygin
    full: Mehmet Saygin Seyfioglu
    id: mehmet-saygin-seyfioglu
    last: Seyfioglu
  - first: Begum
    full: Begum Citamak
    id: begum-citamak
    last: Citamak
  - first: Batuhan
    full: Batuhan Bardak
    id: batuhan-bardak
    last: Bardak
  - first: Seren
    full: Seren Guldamlasioglu
    id: seren-guldamlasioglu
    last: Guldamlasioglu
  - first: Azmi
    full: Azmi Yuksel
    id: azmi-yuksel
    last: Yuksel
  - first: Emin Islam
    full: Emin Islam Tatli
    id: emin-islam-tatli
    last: Tatli
  author_string: Semih Yagcioglu, Mehmet Saygin Seyfioglu, Begum Citamak, Batuhan
    Bardak, Seren Guldamlasioglu, Azmi Yuksel, Emin Islam Tatli
  bibkey: yagcioglu-etal-2019-detecting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1138
  month: June
  page_first: '1366'
  page_last: '1372'
  pages: "1366\u20131372"
  paper_id: '138'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1138.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1138.jpg
  title: Detecting Cybersecurity Events from Noisy Short Text
  title_html: Detecting Cybersecurity Events from Noisy Short Text
  url: https://www.aclweb.org/anthology/N19-1138
  year: '2019'
N19-1139:
  abstract: Adversarial examples are important for understanding the behavior of neural
    models, and can improve their robustness through adversarial training. Recent
    work in natural language processing generated adversarial examples by assuming
    white-box access to the attacked model, and optimizing the input directly against
    it (Ebrahimi et al., 2018). In this work, we show that the knowledge implicit
    in the optimization procedure can be distilled into another more efficient neural
    network. We train a model to emulate the behavior of a white-box attack and show
    that it generalizes well across examples. Moreover, it reduces adversarial example
    generation time by 19x-39x. We also show that our approach transfers to a black-box
    setting, by attacking The Google Perspective API and exposing its vulnerability.
    Our attack flips the API-predicted label in 42% of the generated examples, while
    humans maintain high-accuracy in predicting the gold label.
  address: Minneapolis, Minnesota
  author:
  - first: Yotam
    full: Yotam Gil
    id: yotam-gil
    last: Gil
  - first: Yoav
    full: Yoav Chai
    id: yoav-chai
    last: Chai
  - first: Or
    full: Or Gorodissky
    id: or-gorodissky
    last: Gorodissky
  - first: Jonathan
    full: Jonathan Berant
    id: jonathan-berant
    last: Berant
  author_string: Yotam Gil, Yoav Chai, Or Gorodissky, Jonathan Berant
  bibkey: gil-etal-2019-white
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1139
  month: June
  page_first: '1373'
  page_last: '1379'
  pages: "1373\u20131379"
  paper_id: '139'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1139.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1139.jpg
  title: 'White-to-Black: Efficient Distillation of Black-Box Adversarial Attacks'
  title_html: 'White-to-Black: Efficient Distillation of Black-Box Adversarial Attacks'
  url: https://www.aclweb.org/anthology/N19-1139
  year: '2019'
N19-1140:
  abstract: "Breaking cybersecurity events are shared across a range of websites,\
    \ including security blogs (FireEye, Kaspersky, etc.), in addition to social media\
    \ platforms such as Facebook and Twitter. In this paper, we investigate methods\
    \ to analyze the severity of cybersecurity threats based on the language that\
    \ is used to describe them online. A corpus of 6,000 tweets describing software\
    \ vulnerabilities is annotated with authors\u2019 opinions toward their severity.\
    \ We show that our corpus supports the development of automatic classifiers with\
    \ high precision for this task. Furthermore, we demonstrate the value of analyzing\
    \ users\u2019 opinions about the severity of threats reported online as an early\
    \ indicator of important software vulnerabilities. We present a simple, yet effective\
    \ method for linking software vulnerabilities reported in tweets to Common Vulnerabilities\
    \ and Exposures (CVEs) in the National Vulnerability Database (NVD). Using our\
    \ predicted severity scores, we show that it is possible to achieve a Precision@50\
    \ of 0.86 when forecasting high severity vulnerabilities, significantly outperforming\
    \ a baseline that is based on tweet volume. Finally we show how reports of severe\
    \ vulnerabilities online are predictive of real-world exploits."
  address: Minneapolis, Minnesota
  author:
  - first: Shi
    full: Shi Zong
    id: shi-zong
    last: Zong
  - first: Alan
    full: Alan Ritter
    id: alan-ritter
    last: Ritter
  - first: Graham
    full: Graham Mueller
    id: graham-mueller
    last: Mueller
  - first: Evan
    full: Evan Wright
    id: evan-wright
    last: Wright
  author_string: Shi Zong, Alan Ritter, Graham Mueller, Evan Wright
  bibkey: zong-etal-2019-analyzing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1140
  month: June
  page_first: '1380'
  page_last: '1390'
  pages: "1380\u20131390"
  paper_id: '140'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1140.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1140.jpg
  title: Analyzing the Perceived Severity of Cybersecurity Threats Reported on Social
    Media
  title_html: Analyzing the Perceived Severity of Cybersecurity Threats Reported on
    Social Media
  url: https://www.aclweb.org/anthology/N19-1140
  year: '2019'
N19-1141:
  abstract: Deep-learning-based models have been successfully applied to the problem
    of detecting fake news on social media. While the correlations among news articles
    have been shown to be effective cues for online news analysis, existing deep-learning-based
    methods often ignore this information and only consider each news article individually.
    To overcome this limitation, we develop a graph-theoretic method that inherits
    the power of deep learning while at the same time utilizing the correlations among
    the articles. We formulate fake news detection as an inference problem in a Markov
    random field (MRF) which can be solved by the iterative mean-field algorithm.
    We then unfold the mean-field algorithm into hidden layers that are composed of
    common neural network operations. By integrating these hidden layers on top of
    a deep network, which produces the MRF potentials, we obtain our deep MRF model
    for fake news detection. Experimental results on well-known datasets show that
    the proposed model improves upon various state-of-the-art models.
  address: Minneapolis, Minnesota
  author:
  - first: Duc Minh
    full: Duc Minh Nguyen
    id: duc-minh-nguyen
    last: Nguyen
  - first: Tien Huu
    full: Tien Huu Do
    id: tien-huu-do
    last: Do
  - first: Robert
    full: Robert Calderbank
    id: robert-calderbank
    last: Calderbank
  - first: Nikos
    full: Nikos Deligiannis
    id: nikos-deligiannis
    last: Deligiannis
  author_string: Duc Minh Nguyen, Tien Huu Do, Robert Calderbank, Nikos Deligiannis
  bibkey: nguyen-etal-2019-fake
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1141
  month: June
  page_first: '1391'
  page_last: '1400'
  pages: "1391\u20131400"
  paper_id: '141'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1141.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1141.jpg
  title: Fake News Detection using Deep Markov Random Fields
  title_html: Fake News Detection using Deep <span class="acl-fixed-case">M</span>arkov
    Random Fields
  url: https://www.aclweb.org/anthology/N19-1141
  year: '2019'
N19-1142:
  abstract: In online discussion fora, speakers often make arguments for or against
    something, say birth control, by highlighting certain aspects of the topic. In
    social science, this is referred to as issue framing. In this paper, we introduce
    a new issue frame annotated corpus of online discussions. We explore to what extent
    models trained to detect issue frames in newswire and social media can be transferred
    to the domain of discussion fora, using a combination of multi-task and adversarial
    training, assuming only unlabeled training data in the target domain.
  address: Minneapolis, Minnesota
  author:
  - first: Mareike
    full: Mareike Hartmann
    id: mareike-hartmann
    last: Hartmann
  - first: Tallulah
    full: Tallulah Jansen
    id: tallulah-jansen
    last: Jansen
  - first: Isabelle
    full: Isabelle Augenstein
    id: isabelle-augenstein
    last: Augenstein
  - first: Anders
    full: "Anders S\xF8gaard"
    id: anders-sogaard
    last: "S\xF8gaard"
  author_string: "Mareike Hartmann, Tallulah Jansen, Isabelle Augenstein, Anders S\xF8\
    gaard"
  bibkey: hartmann-etal-2019-issue
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1142
  month: June
  page_first: '1401'
  page_last: '1407'
  pages: "1401\u20131407"
  paper_id: '142'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1142.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1142.jpg
  title: Issue Framing in Online Discussion Fora
  title_html: Issue Framing in Online Discussion Fora
  url: https://www.aclweb.org/anthology/N19-1142
  year: '2019'
N19-1143:
  abstract: We present Vector of Locally Aggregated Embeddings (VLAE) for effective
    and, ultimately, lossless representation of textual content. Our model encodes
    each input text by effectively identifying and integrating the representations
    of its semantically-relevant parts. The proposed model generates high quality
    representation of textual content and improves the classification performance
    of current state-of-the-art deep averaging networks across several text classification
    tasks.
  address: Minneapolis, Minnesota
  author:
  - first: Hadi
    full: Hadi Amiri
    id: hadi-amiri
    last: Amiri
  - first: Mitra
    full: Mitra Mohtarami
    id: mitra-mohtarami
    last: Mohtarami
  author_string: Hadi Amiri, Mitra Mohtarami
  bibkey: amiri-mohtarami-2019-vector
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1143
  month: June
  page_first: '1408'
  page_last: '1414'
  pages: "1408\u20131414"
  paper_id: '143'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1143.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1143.jpg
  title: Vector of Locally Aggregated Embeddings for Text Representation
  title_html: Vector of Locally Aggregated Embeddings for Text Representation
  url: https://www.aclweb.org/anthology/N19-1143
  year: '2019'
N19-1144:
  abstract: As offensive content has become pervasive in social media, there has been
    much research in identifying potentially offensive messages. However, previous
    work on this topic did not consider the problem as a whole, but rather focused
    on detecting very specific types of offensive content, e.g., hate speech, cyberbulling,
    or cyber-aggression. In contrast, here we target several different kinds of offensive
    content. In particular, we model the task hierarchically, identifying the type
    and the target of offensive messages in social media. For this purpose, we complied
    the Offensive Language Identification Dataset (OLID), a new dataset with tweets
    annotated for offensive content using a fine-grained three-layer annotation scheme,
    which we make publicly available. We discuss the main similarities and differences
    between OLID and pre-existing datasets for hate speech identification, aggression
    detection, and similar tasks. We further experiment with and we compare the performance
    of different machine learning models on OLID.
  address: Minneapolis, Minnesota
  author:
  - first: Marcos
    full: Marcos Zampieri
    id: marcos-zampieri
    last: Zampieri
  - first: Shervin
    full: Shervin Malmasi
    id: shervin-malmasi
    last: Malmasi
  - first: Preslav
    full: Preslav Nakov
    id: preslav-nakov
    last: Nakov
  - first: Sara
    full: Sara Rosenthal
    id: sara-rosenthal
    last: Rosenthal
  - first: Noura
    full: Noura Farra
    id: noura-farra
    last: Farra
  - first: Ritesh
    full: Ritesh Kumar
    id: ritesh-kumar
    last: Kumar
  author_string: Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal,
    Noura Farra, Ritesh Kumar
  bibkey: zampieri-etal-2019-predicting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1144
  month: June
  page_first: '1415'
  page_last: '1420'
  pages: "1415\u20131420"
  paper_id: '144'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1144.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1144.jpg
  title: Predicting the Type and Target of Offensive Posts in Social Media
  title_html: Predicting the Type and Target of Offensive Posts in Social Media
  url: https://www.aclweb.org/anthology/N19-1144
  year: '2019'
N19-1145:
  abstract: 'Event extraction for the biomedical domain is more challenging than that
    in the general news domain since it requires broader acquisition of domain-specific
    knowledge and deeper understanding of complex contexts. To better encode contextual
    information and external background knowledge, we propose a novel knowledge base
    (KB)-driven tree-structured long short-term memory networks (Tree-LSTM) framework,
    incorporating two new types of features: (1) dependency structures to capture
    wide contexts; (2) entity properties (types and category descriptions) from external
    ontologies via entity linking. We evaluate our approach on the BioNLP shared task
    with Genia dataset and achieve a new state-of-the-art result. In addition, both
    quantitative and qualitative studies demonstrate the advancement of the Tree-LSTM
    and the external knowledge representation for biomedical event extraction.'
  address: Minneapolis, Minnesota
  author:
  - first: Diya
    full: Diya Li
    id: diya-li
    last: Li
  - first: Lifu
    full: Lifu Huang
    id: lifu-huang
    last: Huang
  - first: Heng
    full: Heng Ji
    id: heng-ji
    last: Ji
  - first: Jiawei
    full: Jiawei Han
    id: jiawei-han
    last: Han
  author_string: Diya Li, Lifu Huang, Heng Ji, Jiawei Han
  bibkey: li-etal-2019-biomedical
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1145
  month: June
  page_first: '1421'
  page_last: '1430'
  pages: "1421\u20131430"
  paper_id: '145'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1145.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1145.jpg
  title: Biomedical Event Extraction based on Knowledge-driven Tree-LSTM
  title_html: Biomedical Event Extraction based on Knowledge-driven Tree-<span class="acl-fixed-case">LSTM</span>
  url: https://www.aclweb.org/anthology/N19-1145
  year: '2019'
N19-1146:
  abstract: Linguistic features have shown promising applications for detecting various
    cognitive impairments. To improve detection accuracies, increasing the amount
    of data or the number of linguistic features have been two applicable approaches.
    However, acquiring additional clinical data can be expensive, and hand-crafting
    features is burdensome. In this paper, we take a third approach, proposing Consensus
    Networks (CNs), a framework to classify after reaching agreements between modalities.
    We divide linguistic features into non-overlapping subsets according to their
    modalities, and let neural networks learn low-dimensional representations that
    agree with each other. These representations are passed into a classifier network.
    All neural networks are optimized iteratively. In this paper, we also present
    two methods that improve the performance of CNs. We then present ablation studies
    to illustrate the effectiveness of modality division. To understand further what
    happens in CNs, we visualize the representations during training. Overall, using
    all of the 413 linguistic features, our models significantly outperform traditional
    classifiers, which are used by the state-of-the-art papers.
  address: Minneapolis, Minnesota
  author:
  - first: Zining
    full: Zining Zhu
    id: zining-zhu
    last: Zhu
  - first: Jekaterina
    full: Jekaterina Novikova
    id: jekaterina-novikova
    last: Novikova
  - first: Frank
    full: Frank Rudzicz
    id: frank-rudzicz
    last: Rudzicz
  author_string: Zining Zhu, Jekaterina Novikova, Frank Rudzicz
  bibkey: zhu-etal-2019-detecting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1146
  month: June
  page_first: '1431'
  page_last: '1441'
  pages: "1431\u20131441"
  paper_id: '146'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1146.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1146.jpg
  title: Detecting cognitive impairments by agreeing on interpretations of linguistic
    features
  title_html: Detecting cognitive impairments by agreeing on interpretations of linguistic
    features
  url: https://www.aclweb.org/anthology/N19-1146
  year: '2019'
N19-1147:
  abstract: Relation extraction (RE) aims to label relations between groups of marked
    entities in raw text. Most current RE models learn context-aware representations
    of the target entities that are then used to establish relation between them.
    This works well for intra-sentence RE, and we call them first-order relations.
    However, this methodology can sometimes fail to capture complex and long dependencies.
    To address this, we hypothesize that at times the target entities can be connected
    via a context token. We refer to such indirect relations as second-order relations,
    and describe an efficient implementation for computing them. These second-order
    relation scores are then combined with first-order relation scores to obtain final
    relation scores. Our empirical results show that the proposed method leads to
    state-of-the-art performance over two biomedical datasets.
  address: Minneapolis, Minnesota
  author:
  - first: Gaurav
    full: Gaurav Singh
    id: gaurav-singh-tomar
    last: Singh
  - first: Parminder
    full: Parminder Bhatia
    id: parminder-bhatia
    last: Bhatia
  author_string: Gaurav Singh, Parminder Bhatia
  bibkey: singh-bhatia-2019-relation
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1147
  month: June
  page_first: '1442'
  page_last: '1447'
  pages: "1442\u20131447"
  paper_id: '147'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1147.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1147.jpg
  title: Relation Extraction using Explicit Context Conditioning
  title_html: Relation Extraction using Explicit Context Conditioning
  url: https://www.aclweb.org/anthology/N19-1147
  year: '2019'
N19-1148:
  abstract: The recent surge of text-based online counseling applications enables
    us to collect and analyze interactions between counselors and clients. A dataset
    of those interactions can be used to learn to automatically classify the client
    utterances into categories that help counselors in diagnosing client status and
    predicting counseling outcome. With proper anonymization, we collect counselor-client
    dialogues, define meaningful categories of client utterances with professional
    counselors, and develop a novel neural network model for classifying the client
    utterances. The central idea of our model, ConvMFiT, is a pre-trained conversation
    model which consists of a general language model built from an out-of-domain corpus
    and two role-specific language models built from unlabeled in-domain dialogues.
    The classification result shows that ConvMFiT outperforms state-of-the-art comparison
    models. Further, the attention weights in the learned model confirm that the model
    finds expected linguistic patterns for each category.
  address: Minneapolis, Minnesota
  author:
  - first: Sungjoon
    full: Sungjoon Park
    id: sungjoon-park
    last: Park
  - first: Donghyun
    full: Donghyun Kim
    id: donghyun-kim
    last: Kim
  - first: Alice
    full: Alice Oh
    id: alice-oh
    last: Oh
  author_string: Sungjoon Park, Donghyun Kim, Alice Oh
  bibkey: park-etal-2019-conversation
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1148
  month: June
  page_first: '1448'
  page_last: '1459'
  pages: "1448\u20131459"
  paper_id: '148'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1148.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1148.jpg
  title: Conversation Model Fine-Tuning for Classifying Client Utterances in Counseling
    Dialogues
  title_html: Conversation Model Fine-Tuning for Classifying Client Utterances in
    Counseling Dialogues
  url: https://www.aclweb.org/anthology/N19-1148
  year: '2019'
N19-1149:
  abstract: Word vectors and Language Models (LMs) pretrained on a large amount of
    unlabelled data can dramatically improve various Natural Language Processing (NLP)
    tasks. However, the measure and impact of similarity between pretraining data
    and target task data are left to intuition. We propose three cost-effective measures
    to quantify different aspects of similarity between source pretraining and target
    task data. We demonstrate that these measures are good predictors of the usefulness
    of pretrained models for Named Entity Recognition (NER) over 30 data pairs. Results
    also suggest that pretrained LMs are more effective and more predictable than
    pretrained word vectors, but pretrained word vectors are better when pretraining
    data is dissimilar.
  address: Minneapolis, Minnesota
  author:
  - first: Xiang
    full: Xiang Dai
    id: xiang-dai
    last: Dai
  - first: Sarvnaz
    full: Sarvnaz Karimi
    id: sarvnaz-karimi
    last: Karimi
  - first: Ben
    full: Ben Hachey
    id: ben-hachey
    last: Hachey
  - first: Cecile
    full: Cecile Paris
    id: cecile-paris
    last: Paris
  author_string: Xiang Dai, Sarvnaz Karimi, Ben Hachey, Cecile Paris
  bibkey: dai-etal-2019-using
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1149
  month: June
  page_first: '1460'
  page_last: '1470'
  pages: "1460\u20131470"
  paper_id: '149'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1149.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1149.jpg
  title: Using Similarity Measures to Select Pretraining Data for NER
  title_html: Using Similarity Measures to Select Pretraining Data for <span class="acl-fixed-case">NER</span>
  url: https://www.aclweb.org/anthology/N19-1149
  year: '2019'
N19-1150:
  abstract: "Modern NLP systems require high-quality annotated data. For specialized\
    \ domains, expert annotations may be prohibitively expensive; the alternative\
    \ is to rely on crowdsourcing to reduce costs at the risk of introducing noise.\
    \ In this paper we demonstrate that directly modeling instance difficulty can\
    \ be used to improve model performance and to route instances to appropriate annotators.\
    \ Our difficulty prediction model combines two learned representations: a \u2018\
    universal\u2019 encoder trained on out of domain data, and a task-specific encoder.\
    \ Experiments on a complex biomedical information extraction task using expert\
    \ and lay annotators show that: (i) simply excluding from the training data instances\
    \ predicted to be difficult yields a small boost in performance; (ii) using difficulty\
    \ scores to weight instances during training provides further, consistent gains;\
    \ (iii) assigning instances predicted to be difficult to domain experts is an\
    \ effective strategy for task routing. Further, our experiments confirm the expectation\
    \ that for such domain-specific tasks expert annotations are of much higher quality\
    \ and preferable to obtain if practical and that augmenting small amounts of expert\
    \ data with a larger set of lay annotations leads to further improvements in model\
    \ performance."
  address: Minneapolis, Minnesota
  author:
  - first: Yinfei
    full: Yinfei Yang
    id: yinfei-yang
    last: Yang
  - first: Oshin
    full: Oshin Agarwal
    id: oshin-agarwal
    last: Agarwal
  - first: Chris
    full: Chris Tar
    id: chris-tar
    last: Tar
  - first: Byron C.
    full: Byron C. Wallace
    id: byron-c-wallace
    last: Wallace
  - first: Ani
    full: Ani Nenkova
    id: ani-nenkova
    last: Nenkova
  author_string: Yinfei Yang, Oshin Agarwal, Chris Tar, Byron C. Wallace, Ani Nenkova
  bibkey: yang-etal-2019-predicting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1150
  month: June
  page_first: '1471'
  page_last: '1480'
  pages: "1471\u20131480"
  paper_id: '150'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1150.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1150.jpg
  title: Predicting Annotation Difficulty to Improve Task Routing and Model Performance
    for Biomedical Information Extraction
  title_html: Predicting Annotation Difficulty to Improve Task Routing and Model Performance
    for Biomedical Information Extraction
  url: https://www.aclweb.org/anthology/N19-1150
  year: '2019'
N19-1151:
  abstract: Nowadays social media platforms are the most popular way for people to
    share information, from work issues to personal matters. For example, people with
    health disorders tend to share their concerns for advice, support or simply to
    relieve suffering. This provides a great opportunity to proactively detect these
    users and refer them as soon as possible to professional help. We propose a new
    representation called Bag of Sub-Emotions (BoSE), which represents social media
    documents by a set of fine-grained emotions automatically generated using a lexical
    resource of emotions and subword embeddings. The proposed representation is evaluated
    in the task of depression detection. The results are encouraging; the usage of
    fine-grained emotions improved the results from a representation based on the
    core emotions and obtained competitive results in comparison to state of the art
    approaches.
  address: Minneapolis, Minnesota
  author:
  - first: Mario Ezra
    full: "Mario Ezra Arag\xF3n"
    id: mario-ezra-aragon
    last: "Arag\xF3n"
  - first: Adrian Pastor
    full: "Adrian Pastor L\xF3pez-Monroy"
    id: adrian-pastor-lopez-monroy
    last: "L\xF3pez-Monroy"
  - first: Luis Carlos
    full: "Luis Carlos Gonz\xE1lez-Gurrola"
    id: luis-carlos-gonzalez-gurrola
    last: "Gonz\xE1lez-Gurrola"
  - first: Manuel
    full: "Manuel Montes-y-G\xF3mez"
    id: manuel-montes
    last: "Montes-y-G\xF3mez"
  author_string: "Mario Ezra Arag\xF3n, Adrian Pastor L\xF3pez-Monroy, Luis Carlos\
    \ Gonz\xE1lez-Gurrola, Manuel Montes-y-G\xF3mez"
  bibkey: aragon-etal-2019-detecting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1151
  month: June
  page_first: '1481'
  page_last: '1486'
  pages: "1481\u20131486"
  paper_id: '151'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1151.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1151.jpg
  title: Detecting Depression in Social Media using Fine-Grained Emotions
  title_html: Detecting Depression in Social Media using Fine-Grained Emotions
  url: https://www.aclweb.org/anthology/N19-1151
  year: '2019'
N19-1152:
  abstract: Human phenotype-gene relations are fundamental to fully understand the
    origin of some phenotypic abnormalities and their associated diseases. Biomedical
    literature is the most comprehensive source of these relations, however, we need
    Relation Extraction tools to automatically recognize them. Most of these tools
    require an annotated corpus and to the best of our knowledge, there is no corpus
    available annotated with human phenotype-gene relations. This paper presents the
    Phenotype-Gene Relations (PGR) corpus, a silver standard corpus of human phenotype
    and gene annotations and their relations. The corpus consists of 1712 abstracts,
    5676 human phenotype annotations, 13835 gene annotations, and 4283 relations.
    We generated this corpus using Named-Entity Recognition tools, whose results were
    partially evaluated by eight curators, obtaining a precision of 87.01%. By using
    the corpus we were able to obtain promising results with two state-of-the-art
    deep learning tools, namely 78.05% of precision. The PGR corpus was made publicly
    available to the research community.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1152.Poster.pdf
    type: poster
    url: https://www.aclweb.org/anthology/attachments/N19-1152.Poster.pdf
  author:
  - first: Diana
    full: Diana Sousa
    id: diana-sousa
    last: Sousa
  - first: Andre
    full: Andre Lamurias
    id: andre-lamurias
    last: Lamurias
  - first: Francisco M.
    full: Francisco M. Couto
    id: francisco-m-couto
    last: Couto
  author_string: Diana Sousa, Andre Lamurias, Francisco M. Couto
  bibkey: sousa-etal-2019-silver
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1152
  month: June
  page_first: '1487'
  page_last: '1492'
  pages: "1487\u20131492"
  paper_id: '152'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1152.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1152.jpg
  title: A Silver Standard Corpus of Human Phenotype-Gene Relations
  title_html: A Silver Standard Corpus of Human Phenotype-Gene Relations
  url: https://www.aclweb.org/anthology/N19-1152
  year: '2019'
N19-1153:
  abstract: 'Lemmatization of standard languages is concerned with (i) abstracting
    over morphological differences and (ii) resolving token-lemma ambiguities of inflected
    words in order to map them to a dictionary headword. In the present paper we aim
    to improve lemmatization performance on a set of non-standard historical languages
    in which the difficulty is increased by an additional aspect (iii): spelling variation
    due to lacking orthographic standards. We approach lemmatization as a string-transduction
    task with an Encoder-Decoder architecture which we enrich with sentence information
    using a hierarchical sentence encoder. We show significant improvements over the
    state-of-the-art by fine-tuning the sentence encodings to jointly optimize a bidirectional
    language model loss. Crucially, our architecture does not require POS or morphological
    annotations, which are not always available for historical corpora. Additionally,
    we also test the proposed model on a set of typologically diverse standard languages
    showing results on par or better than a model without fine-tuned sentence representations
    and previous state-of-the-art systems. Finally, to encourage future work on processing
    of non-standard varieties, we release the dataset of non-standard languages underlying
    the present study, which is based on openly accessible sources.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/360688359
    type: video
    url: https://vimeo.com/360688359
  author:
  - first: Enrique
    full: Enrique Manjavacas
    id: enrique-manjavacas
    last: Manjavacas
  - first: "\xC1kos"
    full: "\xC1kos K\xE1d\xE1r"
    id: akos-kadar
    last: "K\xE1d\xE1r"
  - first: Mike
    full: Mike Kestemont
    id: mike-kestemont
    last: Kestemont
  author_string: "Enrique Manjavacas, \xC1kos K\xE1d\xE1r, Mike Kestemont"
  bibkey: manjavacas-etal-2019-improving
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1153
  month: June
  page_first: '1493'
  page_last: '1503'
  pages: "1493\u20131503"
  paper_id: '153'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1153.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1153.jpg
  title: Improving Lemmatization of Non-Standard Languages with Joint Learning
  title_html: Improving Lemmatization of Non-Standard Languages with Joint Learning
  url: https://www.aclweb.org/anthology/N19-1153
  year: '2019'
N19-1154:
  abstract: Recent work has shown that contextualized word representations derived
    from neural machine translation are a viable alternative to such from simple word
    predictions tasks. This is because the internal understanding that needs to be
    built in order to be able to translate from one language to another is much more
    comprehensive. Unfortunately, computational and memory limitations as of present
    prevent NMT models from using large word vocabularies, and thus alternatives such
    as subword units (BPE and morphological segmentations) and characters have been
    used. Here we study the impact of using different kinds of units on the quality
    of the resulting representations when used to model morphology, syntax, and semantics.
    We found that while representations derived from subwords are slightly better
    for modeling syntax, character-based representations are superior for modeling
    morphology and are also more robust to noisy input.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/360694967
    type: video
    url: https://vimeo.com/360694967
  author:
  - first: Nadir
    full: Nadir Durrani
    id: nadir-durrani
    last: Durrani
  - first: Fahim
    full: Fahim Dalvi
    id: fahim-dalvi
    last: Dalvi
  - first: Hassan
    full: Hassan Sajjad
    id: hassan-sajjad
    last: Sajjad
  - first: Yonatan
    full: Yonatan Belinkov
    id: yonatan-belinkov
    last: Belinkov
  - first: Preslav
    full: Preslav Nakov
    id: preslav-nakov
    last: Nakov
  author_string: Nadir Durrani, Fahim Dalvi, Hassan Sajjad, Yonatan Belinkov, Preslav
    Nakov
  bibkey: durrani-etal-2019-one
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1154
  month: June
  page_first: '1504'
  page_last: '1516'
  pages: "1504\u20131516"
  paper_id: '154'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1154.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1154.jpg
  title: 'One Size Does Not Fit All: Comparing NMT Representations of Different Granularities'
  title_html: 'One Size Does Not Fit All: Comparing <span class="acl-fixed-case">NMT</span>
    Representations of Different Granularities'
  url: https://www.aclweb.org/anthology/N19-1154
  year: '2019'
N19-1155:
  abstract: English verbs have multiple forms. For instance, talk may also appear
    as talks, talked or talking, depending on the context. The NLP task of lemmatization
    seeks to map these diverse forms back to a canonical one, known as the lemma.
    We present a simple joint neural model for lemmatization and morphological tagging
    that achieves state-of-the-art results on 20 languages from the Universal Dependencies
    corpora. Our paper describes the model in addition to training and decoding procedures.
    Error analysis indicates that joint morphological tagging and lemmatization is
    especially helpful in low-resource lemmatization and languages that display a
    larger degree of morphological complexity.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/360705702
    type: video
    url: https://vimeo.com/360705702
  author:
  - first: Chaitanya
    full: Chaitanya Malaviya
    id: chaitanya-malaviya
    last: Malaviya
  - first: Shijie
    full: Shijie Wu
    id: shijie-wu
    last: Wu
  - first: Ryan
    full: Ryan Cotterell
    id: ryan-cotterell
    last: Cotterell
  author_string: Chaitanya Malaviya, Shijie Wu, Ryan Cotterell
  bibkey: malaviya-etal-2019-simple
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1155
  month: June
  page_first: '1517'
  page_last: '1528'
  pages: "1517\u20131528"
  paper_id: '155'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1155.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1155.jpg
  title: A Simple Joint Model for Improved Contextual Neural Lemmatization
  title_html: A Simple Joint Model for Improved Contextual Neural Lemmatization
  url: https://www.aclweb.org/anthology/N19-1155
  year: '2019'
N19-1156:
  abstract: "In the principles-and-parameters framework, the structural features of\
    \ languages depend on parameters that may be toggled on or off, with a single\
    \ parameter often dictating the status of multiple features. The implied covariance\
    \ between features inspires our probabilisation of this line of linguistic inquiry\u2014\
    we develop a generative model of language based on exponential-family matrix factorisation.\
    \ By modelling all languages and features within the same architecture, we show\
    \ how structural similarities between languages can be exploited to predict typological\
    \ features with near-perfect accuracy, outperforming several baselines on the\
    \ task of predicting held-out features. Furthermore, we show that language embeddings\
    \ pre-trained on monolingual text allow for generalisation to unobserved languages.\
    \ This finding has clear practical and also theoretical implications: the results\
    \ confirm what linguists have hypothesised, i.e. that there are significant correlations\
    \ between typological features and languages."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1156.Presentation.pptx
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1156.Presentation.pptx
  - filename: https://vimeo.com/360714960
    type: video
    url: https://vimeo.com/360714960
  author:
  - first: Johannes
    full: Johannes Bjerva
    id: johannes-bjerva
    last: Bjerva
  - first: Yova
    full: Yova Kementchedjhieva
    id: yova-kementchedjhieva
    last: Kementchedjhieva
  - first: Ryan
    full: Ryan Cotterell
    id: ryan-cotterell
    last: Cotterell
  - first: Isabelle
    full: Isabelle Augenstein
    id: isabelle-augenstein
    last: Augenstein
  author_string: Johannes Bjerva, Yova Kementchedjhieva, Ryan Cotterell, Isabelle
    Augenstein
  bibkey: bjerva-etal-2019-probabilistic
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1156
  month: June
  page_first: '1529'
  page_last: '1540'
  pages: "1529\u20131540"
  paper_id: '156'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1156.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1156.jpg
  title: A Probabilistic Generative Model of Linguistic Typology
  title_html: A Probabilistic Generative Model of Linguistic Typology
  url: https://www.aclweb.org/anthology/N19-1156
  year: '2019'
N19-1157:
  abstract: "Brown and Exchange word clusters have long been successfully used as\
    \ word representations in Natural Language Processing (NLP) systems. Their success\
    \ has been attributed to their seeming ability to represent both semantic and\
    \ syntactic information. Using corpora representing several language families,\
    \ we test the hypothesis that Brown and Exchange word clusters are highly effective\
    \ at encoding morphosyntactic information. Our experiments show that word clusters\
    \ are highly capable at distinguishing Parts of Speech. We show that increases\
    \ in Average Mutual Information, the clustering algorithms\u2019 optimization\
    \ goal, are highly correlated with improvements in encoding of morphosyntactic\
    \ information. Our results provide empirical evidence that downstream NLP systems\
    \ addressing tasks dependent on morphosyntactic information can benefit from word\
    \ cluster features."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1157.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1157.Supplementary.pdf
  - filename: https://vimeo.com/360726205
    type: video
    url: https://vimeo.com/360726205
  author:
  - first: Manuel R.
    full: Manuel R. Ciosici
    id: manuel-r-ciosici
    last: Ciosici
  - first: Leon
    full: Leon Derczynski
    id: leon-derczynski
    last: Derczynski
  - first: Ira
    full: Ira Assent
    id: ira-assent
    last: Assent
  author_string: Manuel R. Ciosici, Leon Derczynski, Ira Assent
  bibkey: ciosici-etal-2019-quantifying
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1157
  month: June
  page_first: '1541'
  page_last: '1550'
  pages: "1541\u20131550"
  paper_id: '157'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1157.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1157.jpg
  title: Quantifying the morphosyntactic content of Brown Clusters
  title_html: Quantifying the morphosyntactic content of Brown Clusters
  url: https://www.aclweb.org/anthology/N19-1157
  year: '2019'
N19-1158:
  abstract: We introduce a theoretical analysis of crosslingual transfer in probabilistic
    topic models. By formulating posterior inference through Gibbs sampling as a process
    of language transfer, we propose a new measure that quantifies the loss of knowledge
    across languages during this process. This measure enables us to derive a PAC-Bayesian
    bound that elucidates the factors affecting model quality, both during training
    and in downstream applications. We provide experimental validation of the analysis
    on a diverse set of five languages, and discuss best practices for data collection
    and model design based on our analysis.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1158.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1158.Supplementary.pdf
  - filename: https://vimeo.com/364703295
    type: video
    url: https://vimeo.com/364703295
  author:
  - first: Shudong
    full: Shudong Hao
    id: shudong-hao
    last: Hao
  - first: Michael J.
    full: Michael J. Paul
    id: michael-paul
    last: Paul
  author_string: Shudong Hao, Michael J. Paul
  bibkey: hao-paul-2019-analyzing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1158
  month: June
  page_first: '1551'
  page_last: '1565'
  pages: "1551\u20131565"
  paper_id: '158'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1158.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1158.jpg
  title: Analyzing Bayesian Crosslingual Transfer in Topic Models
  title_html: Analyzing <span class="acl-fixed-case">B</span>ayesian Crosslingual
    Transfer in Topic Models
  url: https://www.aclweb.org/anthology/N19-1158
  year: '2019'
N19-1159:
  abstract: The need for tree structure modelling on top of sequence modelling is
    an open issue in neural dependency parsing. We investigate the impact of adding
    a tree layer on top of a sequential model by recursively composing subtree representations
    (composition) in a transition-based parser that uses features extracted by a BiLSTM.
    Composition seems superfluous with such a model, suggesting that BiLSTMs capture
    information about subtrees. We perform model ablations to tease out the conditions
    under which composition helps. When ablating the backward LSTM, performance drops
    and composition does not recover much of the gap. When ablating the forward LSTM,
    performance drops less dramatically and composition recovers a substantial part
    of the gap, indicating that a forward LSTM and composition capture similar information.
    We take the backward LSTM to be related to lookahead features and the forward
    LSTM to the rich history-based features both crucial for transition-based parsers.
    To capture history-based information, composition is better than a forward LSTM
    on its own, but it is even better to have a forward LSTM as part of a BiLSTM.
    We correlate results with language properties, showing that the improved lookahead
    of a backward LSTM is especially important for head-final languages.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1159.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1159.Presentation.pdf
  - filename: https://vimeo.com/364704101
    type: video
    url: https://vimeo.com/364704101
  author:
  - first: Miryam
    full: Miryam de Lhoneux
    id: miryam-de-lhoneux
    last: de Lhoneux
  - first: Miguel
    full: Miguel Ballesteros
    id: miguel-ballesteros
    last: Ballesteros
  - first: Joakim
    full: Joakim Nivre
    id: joakim-nivre
    last: Nivre
  author_string: Miryam de Lhoneux, Miguel Ballesteros, Joakim Nivre
  bibkey: de-lhoneux-etal-2019-recursive
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1159
  month: June
  page_first: '1566'
  page_last: '1576'
  pages: "1566\u20131576"
  paper_id: '159'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1159.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1159.jpg
  title: Recursive Subtree Composition in LSTM-Based Dependency Parsing
  title_html: Recursive Subtree Composition in <span class="acl-fixed-case">LSTM</span>-Based
    Dependency Parsing
  url: https://www.aclweb.org/anthology/N19-1159
  year: '2019'
N19-1160:
  abstract: 'Combinatory categorial grammars are linguistically motivated and useful
    for semantic parsing, but costly to acquire in a supervised way and difficult
    to acquire in an unsupervised way. We propose an alternative making use of cross-lingual
    learning: an existing source-language parser is used together with a parallel
    corpus to induce a grammar and parsing model for a target language. On the PASCAL
    benchmark, cross-lingual CCG induction outperforms CCG induction from gold-standard
    POS tags on 3 out of 8 languages, and unsupervised CCG induction on 6 out of 8
    languages. We also show that cross-lingually induced CCGs reflect syntactic properties
    of the target languages.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1160.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1160.Presentation.pdf
  - filename: https://vimeo.com/364705576
    type: video
    url: https://vimeo.com/364705576
  author:
  - first: Kilian
    full: Kilian Evang
    id: kilian-evang
    last: Evang
  author_string: Kilian Evang
  bibkey: evang-2019-cross
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1160
  month: June
  page_first: '1577'
  page_last: '1587'
  pages: "1577\u20131587"
  paper_id: '160'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1160.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1160.jpg
  title: Cross-lingual CCG Induction
  title_html: Cross-lingual <span class="acl-fixed-case">CCG</span> Induction
  url: https://www.aclweb.org/anthology/N19-1160
  year: '2019'
N19-1161:
  abstract: Recent approaches to cross-lingual word embedding have generally been
    based on linear transformations between the sets of embedding vectors in the two
    languages. In this paper, we propose an approach that instead expresses the two
    monolingual embedding spaces as probability densities defined by a Gaussian mixture
    model, and matches the two densities using a method called normalizing flow. The
    method requires no explicit supervision, and can be learned with only a seed dictionary
    of words that have identical strings. We argue that this formulation has several
    intuitively attractive properties, particularly with the respect to improving
    robustness and generalization to mappings between difficult language pairs or
    word pairs. On a benchmark data set of bilingual lexicon induction and cross-lingual
    word similarity, our approach can achieve competitive or superior performance
    compared to state-of-the-art published results, with particularly strong results
    being found on etymologically distant and/or morphologically rich languages.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/364706803
    type: video
    url: https://vimeo.com/364706803
  author:
  - first: Chunting
    full: Chunting Zhou
    id: chunting-zhou
    last: Zhou
  - first: Xuezhe
    full: Xuezhe Ma
    id: xuezhe-ma
    last: Ma
  - first: Di
    full: Di Wang
    id: di-wang
    last: Wang
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  author_string: Chunting Zhou, Xuezhe Ma, Di Wang, Graham Neubig
  bibkey: zhou-etal-2019-density
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1161
  month: June
  page_first: '1588'
  page_last: '1598'
  pages: "1588\u20131598"
  paper_id: '161'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1161.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1161.jpg
  title: Density Matching for Bilingual Word Embedding
  title_html: Density Matching for Bilingual Word Embedding
  url: https://www.aclweb.org/anthology/N19-1161
  year: '2019'
N19-1162:
  abstract: We introduce a novel method for multilingual transfer that utilizes deep
    contextual embeddings, pretrained in an unsupervised fashion. While contextual
    embeddings have been shown to yield richer representations of meaning compared
    to their static counterparts, aligning them poses a challenge due to their dynamic
    nature. To this end, we construct context-independent variants of the original
    monolingual spaces and utilize their mapping to derive an alignment for the context-dependent
    spaces. This mapping readily supports processing of a target language, improving
    transfer by context-aware embeddings. Our experimental results demonstrate the
    effectiveness of this approach for zero-shot and few-shot learning of dependency
    parsing. Specifically, our method consistently outperforms the previous state-of-the-art
    on 6 tested languages, yielding an improvement of 6.8 LAS points on average.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/364708233
    type: video
    url: https://vimeo.com/364708233
  author:
  - first: Tal
    full: Tal Schuster
    id: tal-schuster
    last: Schuster
  - first: Ori
    full: Ori Ram
    id: ori-ram
    last: Ram
  - first: Regina
    full: Regina Barzilay
    id: regina-barzilay
    last: Barzilay
  - first: Amir
    full: Amir Globerson
    id: amir-globerson
    last: Globerson
  author_string: Tal Schuster, Ori Ram, Regina Barzilay, Amir Globerson
  bibkey: schuster-etal-2019-cross
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1162
  month: June
  page_first: '1599'
  page_last: '1613'
  pages: "1599\u20131613"
  paper_id: '162'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1162.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1162.jpg
  title: Cross-Lingual Alignment of Contextual Word Embeddings, with Applications
    to Zero-shot Dependency Parsing
  title_html: Cross-Lingual Alignment of Contextual Word Embeddings, with Applications
    to Zero-shot Dependency Parsing
  url: https://www.aclweb.org/anthology/N19-1162
  year: '2019'
N19-1163:
  abstract: "Rumours can spread quickly through social media, and malicious ones can\
    \ bring about significant economical and social impact. Motivated by this, our\
    \ paper focuses on the task of rumour detection; particularly, we are interested\
    \ in understanding how early we can detect them. Although there are numerous studies\
    \ on rumour detection, few are concerned with the timing of the detection. A successfully-detected\
    \ malicious rumour can still cause significant damage if it isn\u2019t detected\
    \ in a timely manner, and so timing is crucial. To address this, we present a\
    \ novel methodology for early rumour detection. Our model treats social media\
    \ posts (e.g. tweets) as a data stream and integrates reinforcement learning to\
    \ learn the number minimum number of posts required before we classify an event\
    \ as a rumour. Experiments on Twitter and Weibo demonstrate that our model identifies\
    \ rumours earlier than state-of-the-art systems while maintaining a comparable\
    \ accuracy."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/364683415
    type: video
    url: https://vimeo.com/364683415
  author:
  - first: Kaimin
    full: Kaimin Zhou
    id: kaimin-zhou
    last: Zhou
  - first: Chang
    full: Chang Shu
    id: chang-shu
    last: Shu
  - first: Binyang
    full: Binyang Li
    id: binyang-li
    last: Li
  - first: Jey Han
    full: Jey Han Lau
    id: jey-han-lau
    last: Lau
  author_string: Kaimin Zhou, Chang Shu, Binyang Li, Jey Han Lau
  bibkey: zhou-etal-2019-early
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1163
  month: June
  page_first: '1614'
  page_last: '1623'
  pages: "1614\u20131623"
  paper_id: '163'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1163.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1163.jpg
  title: Early Rumour Detection
  title_html: Early Rumour Detection
  url: https://www.aclweb.org/anthology/N19-1163
  year: '2019'
N19-1164:
  abstract: Automatic hashtag annotation plays an important role in content understanding
    for microblog posts. To date, progress made in this field has been restricted
    to phrase selection from limited candidates, or word-level hashtag discovery using
    topic models. Different from previous work considering hashtags to be inseparable,
    our work is the first effort to annotate hashtags with a novel sequence generation
    framework via viewing the hashtag as a short sequence of words. Moreover, to address
    the data sparsity issue in processing short microblog posts, we propose to jointly
    model the target posts and the conversation contexts initiated by them with bidirectional
    attention. Extensive experimental results on two large-scale datasets, newly collected
    from English Twitter and Chinese Weibo, show that our model significantly outperforms
    state-of-the-art models based on classification. Further studies demonstrate our
    ability to effectively generate rare and even unseen hashtags, which is however
    not possible for most existing methods.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/364687803
    type: video
    url: https://vimeo.com/364687803
  author:
  - first: Yue
    full: Yue Wang
    id: yue-wang
    last: Wang
  - first: Jing
    full: Jing Li
    id: jing-li
    last: Li
  - first: Irwin
    full: Irwin King
    id: irwin-king
    last: King
  - first: Michael R.
    full: Michael R. Lyu
    id: michael-r-lyu
    last: Lyu
  - first: Shuming
    full: Shuming Shi
    id: shuming-shi
    last: Shi
  author_string: Yue Wang, Jing Li, Irwin King, Michael R. Lyu, Shuming Shi
  bibkey: wang-etal-2019-microblog
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1164
  month: June
  page_first: '1624'
  page_last: '1633'
  pages: "1624\u20131633"
  paper_id: '164'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1164.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1164.jpg
  title: Microblog Hashtag Generation via Encoding Conversation Contexts
  title_html: Microblog Hashtag Generation via Encoding Conversation Contexts
  url: https://www.aclweb.org/anthology/N19-1164
  year: '2019'
N19-1165:
  abstract: "Visual modifications to text are often used to obfuscate offensive comments\
    \ in social media (e.g., \u201C!d10t\u201D) or as a writing style (\u201C1337\u201D\
    \ in \u201Cleet speak\u201D), among other scenarios. We consider this as a new\
    \ type of adversarial attack in NLP, a setting to which humans are very robust,\
    \ as our experiments with both simple and more difficult visual perturbations\
    \ demonstrate. We investigate the impact of visual adversarial attacks on current\
    \ NLP systems on character-, word-, and sentence-level tasks, showing that both\
    \ neural and non-neural models are, in contrast to humans, extremely sensitive\
    \ to such attacks, suffering performance decreases of up to 82%. We then explore\
    \ three shielding methods\u2014visual character embeddings, adversarial training,\
    \ and rule-based recovery\u2014which substantially improve the robustness of the\
    \ models. However, the shielding methods still fall behind performances achieved\
    \ in non-attack scenarios, which demonstrates the difficulty of dealing with visual\
    \ attacks."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1165.Presentation.pptx
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1165.Presentation.pptx
  - filename: https://vimeo.com/364693490
    type: video
    url: https://vimeo.com/364693490
  author:
  - first: Steffen
    full: Steffen Eger
    id: steffen-eger
    last: Eger
  - first: "G\xF6zde G\xFCl"
    full: "G\xF6zde G\xFCl \u015Eahin"
    id: gozde-gul-sahin
    last: "\u015Eahin"
  - first: Andreas
    full: "Andreas R\xFCckl\xE9"
    id: andreas-ruckle
    last: "R\xFCckl\xE9"
  - first: Ji-Ung
    full: Ji-Ung Lee
    id: ji-ung-lee
    last: Lee
  - first: Claudia
    full: Claudia Schulz
    id: claudia-schulz
    last: Schulz
  - first: Mohsen
    full: Mohsen Mesgar
    id: mohsen-mesgar
    last: Mesgar
  - first: Krishnkant
    full: Krishnkant Swarnkar
    id: krishnkant-swarnkar
    last: Swarnkar
  - first: Edwin
    full: Edwin Simpson
    id: edwin-simpson
    last: Simpson
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: "Steffen Eger, G\xF6zde G\xFCl \u015Eahin, Andreas R\xFCckl\xE9,\
    \ Ji-Ung Lee, Claudia Schulz, Mohsen Mesgar, Krishnkant Swarnkar, Edwin Simpson,\
    \ Iryna Gurevych"
  bibkey: eger-etal-2019-text
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1165
  month: June
  page_first: '1634'
  page_last: '1647'
  pages: "1634\u20131647"
  paper_id: '165'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1165.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1165.jpg
  title: 'Text Processing Like Humans Do: Visually Attacking and Shielding NLP Systems'
  title_html: 'Text Processing Like Humans Do: Visually Attacking and Shielding <span
    class="acl-fixed-case">NLP</span> Systems'
  url: https://www.aclweb.org/anthology/N19-1165
  year: '2019'
N19-1166:
  abstract: "Controversial posts are those that split the preferences of a community,\
    \ receiving both significant positive and significant negative feedback. Our inclusion\
    \ of the word \u201Ccommunity\u201D here is deliberate: what is controversial\
    \ to some audiences may not be so to others. Using data from several different\
    \ communities on reddit.com, we predict the ultimate controversiality of posts,\
    \ leveraging features drawn from both the textual content and the tree structure\
    \ of the early comments that initiate the discussion. We find that even when only\
    \ a handful of comments are available, e.g., the first 5 comments made within\
    \ 15 minutes of the original post, discussion features often add predictive capacity\
    \ to strong content-and- rate only baselines. Additional experiments on domain\
    \ transfer suggest that conversation- structure features often generalize to other\
    \ communities better than conversation-content features do."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1166.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1166.Presentation.pdf
  - filename: https://vimeo.com/364697819
    type: video
    url: https://vimeo.com/364697819
  author:
  - first: Jack
    full: Jack Hessel
    id: jack-hessel
    last: Hessel
  - first: Lillian
    full: Lillian Lee
    id: lillian-lee
    last: Lee
  author_string: Jack Hessel, Lillian Lee
  bibkey: hessel-lee-2019-somethings
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1166
  month: June
  page_first: '1648'
  page_last: '1659'
  pages: "1648\u20131659"
  paper_id: '166'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1166.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1166.jpg
  title: "Something\u2019s Brewing! Early Prediction of Controversy-causing Posts\
    \ from Discussion Features"
  title_html: "Something\u2019s Brewing! Early Prediction of Controversy-causing Posts\
    \ from Discussion Features"
  url: https://www.aclweb.org/anthology/N19-1166
  year: '2019'
N19-1167:
  abstract: "Understanding the dynamics of international politics is important yet\
    \ challenging for civilians. In this work, we explore unsupervised neural models\
    \ to infer relations between nations from news articles. We extend existing models\
    \ by incorporating shallow linguistics information and propose a new automatic\
    \ evaluation metric that aligns relationship dynamics with manually annotated\
    \ key events. As understanding international relations requires carefully analyzing\
    \ complex relationships, we conduct in-person human evaluations with three groups\
    \ of participants. Overall, humans prefer the outputs of our model and give insightful\
    \ feedback that suggests future directions for human-centered models. Furthermore,\
    \ our model reveals interesting regional differences in news coverage. For instance,\
    \ with respect to US-China relations, Singaporean media focus more on \u201Cstrengthening\u201D\
    \ and \u201Cpurchasing\u201D, while US media focus more on \u201Ccriticizing\u201D\
    \ and \u201Cdenouncing\u201D."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1167.Supplementary.zip
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1167.Supplementary.zip
  - filename: https://vimeo.com/364700832
    type: video
    url: https://vimeo.com/364700832
  author:
  - first: Xiaochuang
    full: Xiaochuang Han
    id: xiaochuang-han
    last: Han
  - first: Eunsol
    full: Eunsol Choi
    id: eunsol-choi
    last: Choi
  - first: Chenhao
    full: Chenhao Tan
    id: chenhao-tan
    last: Tan
  author_string: Xiaochuang Han, Eunsol Choi, Chenhao Tan
  bibkey: han-etal-2019-permanent
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1167
  month: June
  page_first: '1660'
  page_last: '1676'
  pages: "1660\u20131676"
  paper_id: '167'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1167.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1167.jpg
  title: 'No Permanent Friends or Enemies: Tracking Relationships between Nations
    from News'
  title_html: 'No Permanent <span class="acl-fixed-case">F</span>riends or Enemies:
    Tracking Relationships between Nations from News'
  url: https://www.aclweb.org/anthology/N19-1167
  year: '2019'
N19-1168:
  abstract: Titles of short sections within long documents support readers by guiding
    their focus towards relevant passages and by providing anchor-points that help
    to understand the progression of the document. The positive effects of section
    titles are even more pronounced when measured on readers with less developed reading
    abilities, for example in communities with limited labeled text resources. We,
    therefore, aim to develop techniques to generate section titles in low-resource
    environments. In particular, we present an extractive pipeline for section title
    generation by first selecting the most salient sentence and then applying deletion-based
    compression. Our compression approach is based on a Semi-Markov Conditional Random
    Field that leverages unsupervised word-representations such as ELMo or BERT, eliminating
    the need for a complex encoder-decoder architecture. The results show that this
    approach leads to competitive performance with sequence-to-sequence models with
    high resources, while strongly outperforming it with low resources. In a human-subject
    study across subjects with varying reading abilities, we find that our section
    titles improve the speed of completing comprehension tasks while retaining similar
    accuracy.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/361580764
    type: video
    url: https://vimeo.com/361580764
  author:
  - first: Sebastian
    full: Sebastian Gehrmann
    id: sebastian-gehrmann
    last: Gehrmann
  - first: Steven
    full: Steven Layne
    id: steven-layne
    last: Layne
  - first: Franck
    full: Franck Dernoncourt
    id: franck-dernoncourt
    last: Dernoncourt
  author_string: Sebastian Gehrmann, Steven Layne, Franck Dernoncourt
  bibkey: gehrmann-etal-2019-improving
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1168
  month: June
  page_first: '1677'
  page_last: '1688'
  pages: "1677\u20131688"
  paper_id: '168'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1168.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1168.jpg
  title: Improving Human Text Comprehension through Semi-Markov CRF-based Neural Section
    Title Generation
  title_html: Improving Human Text Comprehension through Semi-<span class="acl-fixed-case">M</span>arkov
    <span class="acl-fixed-case">CRF</span>-based Neural Section Title Generation
  url: https://www.aclweb.org/anthology/N19-1168
  year: '2019'
N19-1169:
  abstract: How can we measure whether a natural language generation system produces
    both high quality and diverse outputs? Human evaluation captures quality but not
    diversity, as it does not catch models that simply plagiarize from the training
    set. On the other hand, statistical evaluation (i.e., perplexity) captures diversity
    but not quality, as models that occasionally emit low quality samples would be
    insufficiently penalized. In this paper, we propose a unified framework which
    evaluates both diversity and quality, based on the optimal error rate of predicting
    whether a sentence is human- or machine-generated. We demonstrate that this error
    rate can be efficiently estimated by combining human and statistical evaluation,
    using an evaluation metric which we call HUSE. On summarization and chit-chat
    dialogue, we show that (i) HUSE detects diversity defects which fool pure human
    evaluation and that (ii) techniques such as annealing for improving quality actually
    decrease HUSE due to decreased diversity.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/359681098
    type: video
    url: https://vimeo.com/359681098
  author:
  - first: Tatsunori
    full: Tatsunori Hashimoto
    id: tatsunori-hashimoto
    last: Hashimoto
  - first: Hugh
    full: Hugh Zhang
    id: hugh-zhang
    last: Zhang
  - first: Percy
    full: Percy Liang
    id: percy-liang
    last: Liang
  author_string: Tatsunori Hashimoto, Hugh Zhang, Percy Liang
  bibkey: hashimoto-etal-2019-unifying
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1169
  month: June
  page_first: '1689'
  page_last: '1701'
  pages: "1689\u20131701"
  paper_id: '169'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1169.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1169.jpg
  title: Unifying Human and Statistical Evaluation for Natural Language Generation
  title_html: Unifying Human and Statistical Evaluation for Natural Language Generation
  url: https://www.aclweb.org/anthology/N19-1169
  year: '2019'
N19-1170:
  abstract: "A good conversation requires balance \u2013 between simplicity and detail;\
    \ staying on topic and changing it; asking questions and answering them. Although\
    \ dialogue agents are commonly evaluated via human judgments of overall quality,\
    \ the relationship between quality and these individual factors is less well-studied.\
    \ In this work, we examine two controllable neural text generation methods, conditional\
    \ training and weighted decoding, in order to control four important attributes\
    \ for chit-chat dialogue: repetition, specificity, response-relatedness and question-asking.\
    \ We conduct a large-scale human evaluation to measure the effect of these control\
    \ parameters on multi-turn interactive conversations on the PersonaChat task.\
    \ We provide a detailed analysis of their relationship to high-level aspects of\
    \ conversation, and show that by controlling combinations of these variables our\
    \ models obtain clear improvements in human quality judgments."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1170.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1170.Presentation.pdf
  - filename: N19-1170.Poster.pdf
    type: poster
    url: https://www.aclweb.org/anthology/attachments/N19-1170.Poster.pdf
  - filename: https://vimeo.com/359676375
    type: video
    url: https://vimeo.com/359676375
  author:
  - first: Abigail
    full: Abigail See
    id: abigail-see
    last: See
  - first: Stephen
    full: Stephen Roller
    id: stephen-roller
    last: Roller
  - first: Douwe
    full: Douwe Kiela
    id: douwe-kiela
    last: Kiela
  - first: Jason
    full: Jason Weston
    id: jason-weston
    last: Weston
  author_string: Abigail See, Stephen Roller, Douwe Kiela, Jason Weston
  bibkey: see-etal-2019-makes
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1170
  month: June
  page_first: '1702'
  page_last: '1723'
  pages: "1702\u20131723"
  paper_id: '170'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1170.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1170.jpg
  title: What makes a good conversation? How controllable attributes affect human
    judgments
  title_html: What makes a good conversation? How controllable attributes affect human
    judgments
  url: https://www.aclweb.org/anthology/N19-1170
  year: '2019'
N19-1171:
  abstract: 'Globally normalized neural sequence models are considered superior to
    their locally normalized equivalents because they may ameliorate the effects of
    label bias. However, when considering high-capacity neural parametrizations that
    condition on the whole input sequence, both model classes are theoretically equivalent
    in terms of the distributions they are capable of representing. Thus, the practical
    advantage of global normalization in the context of modern neural methods remains
    unclear. In this paper, we attempt to shed light on this problem through an empirical
    study. We extend an approach for search-aware training via a continuous relaxation
    of beam search (Goyal et al., 2017b) in order to enable training of globally normalized
    recurrent sequence models through simple backpropagation. We then use this technique
    to conduct an empirical study of the interaction between global normalization,
    high-capacity encoders, and search-aware optimization. We observe that in the
    context of inexact search, globally normalized neural models are still more effective
    than their locally normalized counterparts. Further, since our training approach
    is sensitive to warm-starting with pre-trained models, we also propose a novel
    initialization strategy based on self-normalization for pre-training globally
    normalized models. We perform analysis of our approach on two tasks: CCG supertagging
    and Machine Translation, and demonstrate the importance of global normalization
    under different conditions while using search-aware training.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/359674784
    type: video
    url: https://vimeo.com/359674784
  author:
  - first: Kartik
    full: Kartik Goyal
    id: kartik-goyal
    last: Goyal
  - first: Chris
    full: Chris Dyer
    id: chris-dyer
    last: Dyer
  - first: Taylor
    full: Taylor Berg-Kirkpatrick
    id: taylor-berg-kirkpatrick
    last: Berg-Kirkpatrick
  author_string: Kartik Goyal, Chris Dyer, Taylor Berg-Kirkpatrick
  bibkey: goyal-etal-2019-empirical
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1171
  month: June
  page_first: '1724'
  page_last: '1733'
  pages: "1724\u20131733"
  paper_id: '171'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1171.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1171.jpg
  title: An Empirical Investigation of Global and Local Normalization for Recurrent
    Neural Sequence Models Using a Continuous Relaxation to Beam Search
  title_html: <span class="acl-fixed-case">A</span>n Empirical Investigation of Global
    and Local Normalization for Recurrent Neural Sequence Models Using a Continuous
    Relaxation to Beam Search
  url: https://www.aclweb.org/anthology/N19-1171
  year: '2019'
N19-1172:
  abstract: "We tackle the problem of generating a pun sentence given a pair of homophones\
    \ (e.g., \u201Cdied\u201D and \u201Cdyed\u201D). Puns are by their very nature\
    \ statistically anomalous and not amenable to most text generation methods that\
    \ are supervised by a large corpus. In this paper, we propose an unsupervised\
    \ approach to pun generation based on lots of raw (unhumorous) text and a surprisal\
    \ principle. Specifically, we posit that in a pun sentence, there is a strong\
    \ association between the pun word (e.g., \u201Cdyed\u201D) and the distant context,\
    \ but a strong association between the alternative word (e.g., \u201Cdied\u201D\
    ) and the immediate context. We instantiate the surprisal principle in two ways:\
    \ (i) as a measure based on the ratio of probabilities given by a language model,\
    \ and (ii) a retrieve-and-edit approach based on words suggested by a skip-gram\
    \ model. Based on human evaluation, our retrieve-and-edit approach generates puns\
    \ successfully 30% of the time, doubling the success rate of a neural generation\
    \ baseline."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/359670150
    type: video
    url: https://vimeo.com/359670150
  author:
  - first: He
    full: He He
    id: he-he
    last: He
  - first: Nanyun
    full: Nanyun Peng
    id: nanyun-peng
    last: Peng
  - first: Percy
    full: Percy Liang
    id: percy-liang
    last: Liang
  author_string: He He, Nanyun Peng, Percy Liang
  bibkey: he-etal-2019-pun
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1172
  month: June
  page_first: '1734'
  page_last: '1744'
  pages: "1734\u20131744"
  paper_id: '172'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1172.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1172.jpg
  title: Pun Generation with Surprise
  title_html: Pun Generation with Surprise
  url: https://www.aclweb.org/anthology/N19-1172
  year: '2019'
N19-1173:
  abstract: 'In this paper, we conceptualize single-document extractive summarization
    as a tree induction problem. In contrast to previous approaches which have relied
    on linguistically motivated document representations to generate summaries, our
    model induces a multi-root dependency tree while predicting the output summary.
    Each root node in the tree is a summary sentence, and the subtrees attached to
    it are sentences whose content relates to or explains the summary sentence. We
    design a new iterative refinement algorithm: it induces the trees through repeatedly
    refining the structures predicted by previous iterations. We demonstrate experimentally
    on two benchmark datasets that our summarizer performs competitively against state-of-the-art
    methods.'
  address: Minneapolis, Minnesota
  author:
  - first: Yang
    full: Yang Liu
    id: yang-liu-edinburgh
    last: Liu
  - first: Ivan
    full: Ivan Titov
    id: ivan-titov
    last: Titov
  - first: Mirella
    full: Mirella Lapata
    id: mirella-lapata
    last: Lapata
  author_string: Yang Liu, Ivan Titov, Mirella Lapata
  bibkey: liu-etal-2019-single
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1173
  month: June
  page_first: '1745'
  page_last: '1755'
  pages: "1745\u20131755"
  paper_id: '173'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1173.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1173.jpg
  title: Single Document Summarization as Tree Induction
  title_html: Single Document Summarization as Tree Induction
  url: https://www.aclweb.org/anthology/N19-1173
  year: '2019'
N19-1174:
  abstract: Understanding contrastive opinions is a key component of argument generation.
    Central to an argument is the claim, a statement that is in dispute. Generating
    a counter-argument then requires generating a response in contrast to the main
    claim of the original argument. To generate contrastive claims, we create a corpus
    of Reddit comment pairs self-labeled by posters using the acronym FTFY (fixed
    that for you). We then train neural models on these pairs to edit the original
    claim and produce a new claim with a different view. We demonstrate significant
    improvement over a sequence-to-sequence baseline in BLEU score and a human evaluation
    for fluency, coherence, and contrast.
  address: Minneapolis, Minnesota
  author:
  - first: Christopher
    full: Christopher Hidey
    id: christopher-hidey
    last: Hidey
  - first: Kathy
    full: Kathy McKeown
    id: kathleen-mckeown
    last: McKeown
  author_string: Christopher Hidey, Kathy McKeown
  bibkey: hidey-mckeown-2019-fixed
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1174
  month: June
  page_first: '1756'
  page_last: '1767'
  pages: "1756\u20131767"
  paper_id: '174'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1174.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1174.jpg
  title: 'Fixed That for You: Generating Contrastive Claims with Semantic Edits'
  title_html: 'Fixed That for You: Generating Contrastive Claims with Semantic Edits'
  url: https://www.aclweb.org/anthology/N19-1174
  year: '2019'
N19-1175:
  abstract: Deception often takes place during everyday conversations, yet conversational
    dialogues remain largely unexplored by current work on automatic deception detection.
    In this paper, we address the task of detecting multimodal deceptive cues during
    conversational dialogues. We introduce a multimodal dataset containing deceptive
    conversations between participants playing the Box of Lies game from The Tonight
    Show Starring Jimmy Fallon, in which they try to guess whether an object description
    provided by their opponent is deceptive or not. We conduct annotations of multimodal
    communication behaviors, including facial and linguistic behaviors, and derive
    several learning features based on these annotations. Initial classification experiments
    show promising results, performing well above both a random and a human baseline,
    and reaching up to 69% accuracy in distinguishing deceptive and truthful behaviors.
  address: Minneapolis, Minnesota
  author:
  - first: Felix
    full: Felix Soldner
    id: felix-soldner
    last: Soldner
  - first: "Ver\xF3nica"
    full: "Ver\xF3nica P\xE9rez-Rosas"
    id: veronica-perez-rosas
    last: "P\xE9rez-Rosas"
  - first: Rada
    full: Rada Mihalcea
    id: rada-mihalcea
    last: Mihalcea
  author_string: "Felix Soldner, Ver\xF3nica P\xE9rez-Rosas, Rada Mihalcea"
  bibkey: soldner-etal-2019-box
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1175
  month: June
  page_first: '1768'
  page_last: '1777'
  pages: "1768\u20131777"
  paper_id: '175'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1175.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1175.jpg
  title: 'Box of Lies: Multimodal Deception Detection in Dialogues'
  title_html: 'Box of Lies: Multimodal Deception Detection in Dialogues'
  url: https://www.aclweb.org/anthology/N19-1175
  year: '2019'
N19-1176:
  abstract: 'We present a corpus of anaphoric information (coreference) crowdsourced
    through a game-with-a-purpose. The corpus, containing annotations for about 108,000
    markables, is one of the largest corpora for coreference for English, and one
    of the largest crowdsourced NLP corpora, but its main feature is the large number
    of judgments per markable: 20 on average, and over 2.2M in total. This characteristic
    makes the corpus a unique resource for the study of disagreements on anaphoric
    interpretation. A second distinctive feature is its rich annotation scheme, covering
    singletons, expletives, and split-antecedent plurals. Finally, the corpus also
    comes with labels inferred using a recently proposed probabilistic model of annotation
    for coreference. The labels are of high quality and make it possible to successfully
    train a state of the art coreference resolver, including training on singletons
    and non-referring expressions. The annotation model can also result in more than
    one label, or no label, being proposed for a markable, thus serving as a baseline
    method for automatically identifying ambiguous markables. A preliminary analysis
    of the results is presented.'
  address: Minneapolis, Minnesota
  author:
  - first: Massimo
    full: Massimo Poesio
    id: massimo-poesio
    last: Poesio
  - first: Jon
    full: Jon Chamberlain
    id: jon-chamberlain
    last: Chamberlain
  - first: Silviu
    full: Silviu Paun
    id: silviu-paun
    last: Paun
  - first: Juntao
    full: Juntao Yu
    id: juntao-yu
    last: Yu
  - first: Alexandra
    full: Alexandra Uma
    id: alexandra-uma
    last: Uma
  - first: Udo
    full: Udo Kruschwitz
    id: udo-kruschwitz
    last: Kruschwitz
  author_string: Massimo Poesio, Jon Chamberlain, Silviu Paun, Juntao Yu, Alexandra
    Uma, Udo Kruschwitz
  bibkey: poesio-etal-2019-crowdsourced
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1176
  month: June
  page_first: '1778'
  page_last: '1789'
  pages: "1778\u20131789"
  paper_id: '176'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1176.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1176.jpg
  title: A Crowdsourced Corpus of Multiple Judgments and Disagreement on Anaphoric
    Interpretation
  title_html: A Crowdsourced Corpus of Multiple Judgments and Disagreement on Anaphoric
    Interpretation
  url: https://www.aclweb.org/anthology/N19-1176
  year: '2019'
N19-1177:
  abstract: The study of argumentation and the development of argument mining tools
    depends on the availability of annotated data, which is challenging to obtain
    in sufficient quantity and quality. We present a method that breaks down a popular
    but relatively complex discourse-level argument annotation scheme into a simpler,
    iterative procedure that can be applied even by untrained annotators. We apply
    this method in a crowdsourcing setup and report on the reliability of the annotations
    obtained. The source code for a tool implementing our annotation method, as well
    as the sample data we obtained (4909 gold-standard annotations across 982 documents),
    are freely released to the research community. These are intended to serve the
    needs of qualitative research into argumentation, as well as of data-driven approaches
    to argument mining.
  address: Minneapolis, Minnesota
  author:
  - first: Tristan
    full: Tristan Miller
    id: tristan-miller
    last: Miller
  - first: Maria
    full: Maria Sukhareva
    id: maria-sukhareva
    last: Sukhareva
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Tristan Miller, Maria Sukhareva, Iryna Gurevych
  bibkey: miller-etal-2019-streamlined
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1177
  month: June
  page_first: '1790'
  page_last: '1796'
  pages: "1790\u20131796"
  paper_id: '177'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1177.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1177.jpg
  title: A Streamlined Method for Sourcing Discourse-level Argumentation Annotations
    from the Crowd
  title_html: A Streamlined Method for Sourcing Discourse-level Argumentation Annotations
    from the Crowd
  url: https://www.aclweb.org/anthology/N19-1177
  year: '2019'
N19-1178:
  abstract: "Learning a shared dialog structure from a set of task-oriented dialogs\
    \ is an important challenge in computational linguistics. The learned dialog structure\
    \ can shed light on how to analyze human dialogs, and more importantly contribute\
    \ to the design and evaluation of dialog systems. We propose to extract dialog\
    \ structures using a modified VRNN model with discrete latent vectors. Different\
    \ from existing HMM-based models, our model is based on variational-autoencoder\
    \ (VAE). Such model is able to capture more dynamics in dialogs beyond the surface\
    \ forms of the language. We find that qualitatively, our method extracts meaningful\
    \ dialog structure, and quantitatively, outperforms previous models on the ability\
    \ to predict unseen data. We further evaluate the model\u2019s effectiveness in\
    \ a downstream task, the dialog system building task. Experiments show that, by\
    \ integrating the learned dialog structure into the reward function design, the\
    \ model converges faster and to a better outcome in a reinforcement learning setting."
  address: Minneapolis, Minnesota
  author:
  - first: Weiyan
    full: Weiyan Shi
    id: weiyan-shi
    last: Shi
  - first: Tiancheng
    full: Tiancheng Zhao
    id: tiancheng-zhao
    last: Zhao
  - first: Zhou
    full: Zhou Yu
    id: zhou-yu
    last: Yu
  author_string: Weiyan Shi, Tiancheng Zhao, Zhou Yu
  bibkey: shi-etal-2019-unsupervised
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1178
  month: June
  page_first: '1797'
  page_last: '1807'
  pages: "1797\u20131807"
  paper_id: '178'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1178.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1178.jpg
  title: Unsupervised Dialog Structure Learning
  title_html: Unsupervised Dialog Structure Learning
  url: https://www.aclweb.org/anthology/N19-1178
  year: '2019'
N19-1179:
  abstract: 'We aim to comprehensively identify all the event causal relations in
    a document, both within a sentence and across sentences, which is important for
    reconstructing pivotal event structures. The challenges we identified are two:
    1) event causal relations are sparse among all possible event pairs in a document,
    in addition, 2) few causal relations are explicitly stated. Both challenges are
    especially true for identifying causal relations between events across sentences.
    To address these challenges, we model rich aspects of document-level causal structures
    for achieving comprehensive causal relation identification. The causal structures
    include heavy involvements of document-level main events in causal relations as
    well as several types of fine-grained constraints that capture implications from
    certain sentential syntactic relations and discourse relations as well as interactions
    between event causal relations and event coreference relations. Our experimental
    results show that modeling the global and fine-grained aspects of causal structures
    using Integer Linear Programming (ILP) greatly improves the performance of causal
    relation identification, especially in identifying cross-sentence causal relations.'
  address: Minneapolis, Minnesota
  author:
  - first: Lei
    full: Lei Gao
    id: lei-gao
    last: Gao
  - first: Prafulla Kumar
    full: Prafulla Kumar Choubey
    id: prafulla-kumar-choubey
    last: Choubey
  - first: Ruihong
    full: Ruihong Huang
    id: ruihong-huang
    last: Huang
  author_string: Lei Gao, Prafulla Kumar Choubey, Ruihong Huang
  bibkey: gao-etal-2019-modeling
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1179
  month: June
  page_first: '1808'
  page_last: '1817'
  pages: "1808\u20131817"
  paper_id: '179'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1179.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1179.jpg
  title: Modeling Document-level Causal Structures for Event Causal Relation Identification
  title_html: Modeling Document-level Causal Structures for Event Causal Relation
    Identification
  url: https://www.aclweb.org/anthology/N19-1179
  year: '2019'
N19-1180:
  abstract: Utilizing reviews to learn user and item representations is useful for
    recommender systems. Existing methods usually merge all reviews from the same
    user or for the same item into a long document. However, different reviews, sentences
    and even words usually have different informativeness for modeling users and items.
    In this paper, we propose a hierarchical user and item representation model with
    three-tier attention to learn user and item representations from reviews for recommendation.
    Our model contains three major components, i.e., a sentence encoder to learn sentence
    representations from words, a review encoder to learn review representations from
    sentences, and a user/item encoder to learn user/item representations from reviews.
    In addition, we incorporate a three-tier attention network in our model to select
    important words, sentences and reviews. Besides, we combine the user and item
    representations learned from the reviews with user and item embeddings based on
    IDs as the final representations to capture the latent factors of individual users
    and items. Extensive experiments on four benchmark datasets validate the effectiveness
    of our approach.
  address: Minneapolis, Minnesota
  author:
  - first: Chuhan
    full: Chuhan Wu
    id: chuhan-wu
    last: Wu
  - first: Fangzhao
    full: Fangzhao Wu
    id: fangzhao-wu
    last: Wu
  - first: Junxin
    full: Junxin Liu
    id: junxin-liu
    last: Liu
  - first: Yongfeng
    full: Yongfeng Huang
    id: yongfeng-huang
    last: Huang
  author_string: Chuhan Wu, Fangzhao Wu, Junxin Liu, Yongfeng Huang
  bibkey: wu-etal-2019-hierarchical-user
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1180
  month: June
  page_first: '1818'
  page_last: '1826'
  pages: "1818\u20131826"
  paper_id: '180'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1180.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1180.jpg
  title: Hierarchical User and Item Representation with Three-Tier Attention for Recommendation
  title_html: Hierarchical User and Item Representation with Three-Tier Attention
    for Recommendation
  url: https://www.aclweb.org/anthology/N19-1180
  year: '2019'
N19-1181:
  abstract: The prevalent way to estimate the similarity of two documents based on
    word embeddings is to apply the cosine similarity measure to the two centroids
    obtained from the embedding vectors associated with the words in each document.
    Motivated by an industrial application from the domain of youth marketing, where
    this approach produced only mediocre results, we propose an alternative way of
    combining the word vectors using matrix norms. The evaluation shows superior results
    for most of the investigated matrix norms in comparison to both the classical
    cosine measure and several other document similarity estimates.
  address: Minneapolis, Minnesota
  author:
  - first: Tim
    full: "Tim vor der Br\xFCck"
    id: tim-vor-der-bruck
    last: "vor der Br\xFCck"
  - first: Marc
    full: Marc Pouly
    id: marc-pouly
    last: Pouly
  author_string: "Tim vor der Br\xFCck, Marc Pouly"
  bibkey: vor-der-bruck-pouly-2019-text
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1181
  month: June
  page_first: '1827'
  page_last: '1836'
  pages: "1827\u20131836"
  paper_id: '181'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1181.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1181.jpg
  title: Text Similarity Estimation Based on Word Embeddings and Matrix Norms for
    Targeted Marketing
  title_html: Text Similarity Estimation Based on Word Embeddings and Matrix Norms
    for Targeted Marketing
  url: https://www.aclweb.org/anthology/N19-1181
  year: '2019'
N19-1182:
  abstract: "Graph Convolutional Networks (GCNs) are a class of spectral clustering\
    \ techniques that leverage localized convolution filters to perform supervised\
    \ classification directly on graphical structures. While such methods model nodes\u2019\
    \ local pairwise importance, they lack the capability to model global importance\
    \ relative to other nodes of the graph. This causes such models to miss critical\
    \ information in tasks where global ranking is a key component for the task, such\
    \ as in keyphrase extraction. We address this shortcoming by allowing the proper\
    \ incorporation of global information into the GCN family of models through the\
    \ use of scaled node weights. In the context of keyphrase extraction, incorporating\
    \ global random walk scores obtained from TextRank boosts performance significantly.\
    \ With our proposed method, we achieve state-of-the-art results, bettering a strong\
    \ baseline by an absolute 2% increase in F1 score."
  address: Minneapolis, Minnesota
  author:
  - first: Animesh
    full: Animesh Prasad
    id: animesh-prasad
    last: Prasad
  - first: Min-Yen
    full: Min-Yen Kan
    id: min-yen-kan
    last: Kan
  author_string: Animesh Prasad, Min-Yen Kan
  bibkey: prasad-kan-2019-glocal
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1182
  month: June
  page_first: '1837'
  page_last: '1846'
  pages: "1837\u20131846"
  paper_id: '182'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1182.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1182.jpg
  title: 'Glocal: Incorporating Global Information in Local Convolution for Keyphrase
    Extraction'
  title_html: '<span class="acl-fixed-case">G</span>local: Incorporating Global Information
    in Local Convolution for Keyphrase Extraction'
  url: https://www.aclweb.org/anthology/N19-1182
  year: '2019'
N19-1183:
  abstract: The structured output framework provides a helpful tool for learning to
    rank problems. In this paper, we propose a structured output approach which regards
    rankings as latent variables. Our approach addresses the complex optimization
    of Mean Average Precision (MAP) ranking metric. We provide an inference procedure
    to find the max-violating ranking based on the decomposition of the corresponding
    loss. The results of our experiments on WikiQA and TREC13 datasets show that our
    reranking based on structured prediction is a promising research direction.
  address: Minneapolis, Minnesota
  author:
  - first: Iryna
    full: Iryna Haponchyk
    id: iryna-haponchyk
    last: Haponchyk
  - first: Alessandro
    full: Alessandro Moschitti
    id: alessandro-moschitti
    last: Moschitti
  author_string: Iryna Haponchyk, Alessandro Moschitti
  bibkey: haponchyk-moschitti-2019-study
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1183
  month: June
  page_first: '1847'
  page_last: '1857'
  pages: "1847\u20131857"
  paper_id: '183'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1183.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1183.jpg
  title: A Study of Latent Structured Prediction Approaches to Passage Reranking
  title_html: A Study of Latent Structured Prediction Approaches to Passage Reranking
  url: https://www.aclweb.org/anthology/N19-1183
  year: '2019'
N19-1184:
  abstract: "In relation extraction with distant supervision, noisy labels make it\
    \ difficult to train quality models. Previous neural models addressed this problem\
    \ using an attention mechanism that attends to sentences that are likely to express\
    \ the relations. We improve such models by combining the distant supervision data\
    \ with an additional directly-supervised data, which we use as supervision for\
    \ the attention weights. We find that joint training on both types of supervision\
    \ leads to a better model because it improves the model\u2019s ability to identify\
    \ noisy sentences. In addition, we find that sigmoidal attention weights with\
    \ max pooling achieves better performance over the commonly used weighted average\
    \ attention in this setup. Our proposed method achieves a new state-of-the-art\
    \ result on the widely used FB-NYT dataset."
  address: Minneapolis, Minnesota
  author:
  - first: Iz
    full: Iz Beltagy
    id: iz-beltagy
    last: Beltagy
  - first: Kyle
    full: Kyle Lo
    id: kyle-lo
    last: Lo
  - first: Waleed
    full: Waleed Ammar
    id: waleed-ammar
    last: Ammar
  author_string: Iz Beltagy, Kyle Lo, Waleed Ammar
  bibkey: beltagy-etal-2019-combining
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1184
  month: June
  page_first: '1858'
  page_last: '1867'
  pages: "1858\u20131867"
  paper_id: '184'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1184.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1184.jpg
  title: Combining Distant and Direct Supervision for Neural Relation Extraction
  title_html: Combining Distant and Direct Supervision for Neural Relation Extraction
  url: https://www.aclweb.org/anthology/N19-1184
  year: '2019'
N19-1185:
  abstract: Stance detection in twitter aims at mining user stances expressed in a
    tweet towards a single or multiple target entities. To tackle this problem, most
    of the prior studies have been explored the traditional deep learning models,
    e.g., LSTM and GRU. However, in compared to these traditional approaches, recently
    proposed densely connected Bi-LSTM and nested LSTMs architectures effectively
    address the vanishing-gradient and overfitting problems as well as dealing with
    long-term dependencies. In this paper, we propose a neural ensemble model that
    adopts the strengths of these two LSTM variants to learn better long-term dependencies,
    where each module coupled with an attention mechanism that amplifies the contribution
    of important elements in the final representation. We also employ a multi-kernel
    convolution on top of them to extract the higher-level tweet representations.
    Results of extensive experiments on single and multi-target stance detection datasets
    show that our proposed method achieves substantial improvement over the current
    state-of-the-art deep learning based methods.
  address: Minneapolis, Minnesota
  author:
  - first: Umme Aymun
    full: Umme Aymun Siddiqua
    id: umme-aymun-siddiqua
    last: Siddiqua
  - first: Abu Nowshed
    full: Abu Nowshed Chy
    id: abu-nowshed-chy
    last: Chy
  - first: Masaki
    full: Masaki Aono
    id: masaki-aono
    last: Aono
  author_string: Umme Aymun Siddiqua, Abu Nowshed Chy, Masaki Aono
  bibkey: siddiqua-etal-2019-tweet
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1185
  month: June
  page_first: '1868'
  page_last: '1873'
  pages: "1868\u20131873"
  paper_id: '185'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1185.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1185.jpg
  title: Tweet Stance Detection Using an Attention based Neural Ensemble Model
  title_html: <span class="acl-fixed-case">T</span>weet Stance Detection Using an
    Attention based Neural Ensemble Model
  url: https://www.aclweb.org/anthology/N19-1185
  year: '2019'
N19-1186:
  abstract: "We propose a new automatic evaluation metric for machine translation.\
    \ Our proposed metric is obtained by adjusting the Earth Mover\u2019s Distance\
    \ (EMD) to the evaluation task. The EMD measure is used to obtain the distance\
    \ between two probability distributions consisting of some signatures having a\
    \ feature and a weight. We use word embeddings, sentence-level tf-idf, and cosine\
    \ similarity between two word embeddings, respectively, as the features, weight,\
    \ and the distance between two features. Results show that our proposed metric\
    \ can evaluate machine translation based on word meaning. Moreover, for distance,\
    \ cosine similarity and word position information are used to address word-order\
    \ differences. We designate this metric as Word Embedding-Based automatic MT evaluation\
    \ using Word Position Information (WE_WPI). A meta-evaluation using WMT16 metrics\
    \ shared task set indicates that our WE_WPI achieves the highest correlation with\
    \ human judgment among several representative metrics."
  address: Minneapolis, Minnesota
  author:
  - first: Hiroshi
    full: "Hiroshi Echizen\u2019ya"
    id: hiroshi-echizen-ya
    last: "Echizen\u2019ya"
  - first: Kenji
    full: Kenji Araki
    id: kenji-araki
    last: Araki
  - first: Eduard
    full: Eduard Hovy
    id: eduard-hovy
    last: Hovy
  author_string: "Hiroshi Echizen\u2019ya, Kenji Araki, Eduard Hovy"
  bibkey: echizenya-etal-2019-word
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1186
  month: June
  page_first: '1874'
  page_last: '1883'
  pages: "1874\u20131883"
  paper_id: '186'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1186.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1186.jpg
  title: Word Embedding-Based Automatic MT Evaluation Metric using Word Position Information
  title_html: Word Embedding-Based Automatic <span class="acl-fixed-case">MT</span>
    Evaluation Metric using Word Position Information
  url: https://www.aclweb.org/anthology/N19-1186
  year: '2019'
N19-1187:
  abstract: "Beam search optimization (Wiseman and Rush, 2016) resolves many issues\
    \ in neural machine translation. However, this method lacks principled stopping\
    \ criteria and does not learn how to stop during training, and the model naturally\
    \ prefers longer hypotheses during the testing time in practice since they use\
    \ the raw score instead of the probability-based score. We propose a novel ranking\
    \ method which enables an optimal beam search stop- ping criteria. We further\
    \ introduce a structured prediction loss function which penalizes suboptimal finished\
    \ candidates produced by beam search during training. Experiments of neural machine\
    \ translation on both synthetic data and real languages (German\u2192English and\
    \ Chinese\u2192English) demonstrate our pro- posed methods lead to better length\
    \ and BLEU score."
  address: Minneapolis, Minnesota
  author:
  - first: Mingbo
    full: Mingbo Ma
    id: mingbo-ma
    last: Ma
  - first: Renjie
    full: Renjie Zheng
    id: renjie-zheng
    last: Zheng
  - first: Liang
    full: Liang Huang
    id: liang-huang
    last: Huang
  author_string: Mingbo Ma, Renjie Zheng, Liang Huang
  bibkey: ma-etal-2019-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1187
  month: June
  page_first: '1884'
  page_last: '1889'
  pages: "1884\u20131889"
  paper_id: '187'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1187.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1187.jpg
  title: Learning to Stop in Structured Prediction for Neural Machine Translation
  title_html: Learning to Stop in Structured Prediction for Neural Machine Translation
  url: https://www.aclweb.org/anthology/N19-1187
  year: '2019'
N19-1188:
  abstract: 'Recent research has discovered that a shared bilingual word embedding
    space can be induced by projecting monolingual word embedding spaces from two
    languages using a self-learning paradigm without any bilingual supervision. However,
    it has also been shown that for distant language pairs such fully unsupervised
    self-learning methods are unstable and often get stuck in poor local optima due
    to reduced isomorphism between starting monolingual spaces. In this work, we propose
    a new robust framework for learning unsupervised multilingual word embeddings
    that mitigates the instability issues. We learn a shared multilingual embedding
    space for a variable number of languages by incrementally adding new languages
    one by one to the current multilingual space. Through the gradual language addition
    the method can leverage the interdependencies between the new language and all
    other languages in the current multilingual space. We find that it is beneficial
    to project more distant languages later in the iterative process. Our fully unsupervised
    multilingual embedding spaces yield results that are on par with the state-of-the-art
    methods in the bilingual lexicon induction (BLI) task, and simultaneously obtain
    state-of-the-art scores on two downstream tasks: multilingual document classification
    and multilingual dependency parsing, outperforming even supervised baselines.
    This finding also accentuates the need to establish evaluation protocols for cross-lingual
    word embeddings beyond the omnipresent intrinsic BLI task in future work.'
  address: Minneapolis, Minnesota
  author:
  - first: Geert
    full: Geert Heyman
    id: geert-heyman
    last: Heyman
  - first: Bregt
    full: Bregt Verreet
    id: bregt-verreet
    last: Verreet
  - first: Ivan
    full: "Ivan Vuli\u0107"
    id: ivan-vulic
    last: "Vuli\u0107"
  - first: Marie-Francine
    full: Marie-Francine Moens
    id: marie-francine-moens
    last: Moens
  author_string: "Geert Heyman, Bregt Verreet, Ivan Vuli\u0107, Marie-Francine Moens"
  bibkey: heyman-etal-2019-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1188
  month: June
  page_first: '1890'
  page_last: '1902'
  pages: "1890\u20131902"
  paper_id: '188'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1188.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1188.jpg
  title: Learning Unsupervised Multilingual Word Embeddings with Incremental Multilingual
    Hubs
  title_html: Learning Unsupervised Multilingual Word Embeddings with Incremental
    Multilingual Hubs
  url: https://www.aclweb.org/anthology/N19-1188
  year: '2019'
N19-1189:
  abstract: We introduce a curriculum learning approach to adapt generic neural machine
    translation models to a specific domain. Samples are grouped by their similarities
    to the domain of interest and each group is fed to the training algorithm with
    a particular schedule. This approach is simple to implement on top of any neural
    framework or architecture, and consistently outperforms both unadapted and adapted
    baselines in experiments with two distinct domains and two language pairs.
  address: Minneapolis, Minnesota
  author:
  - first: Xuan
    full: Xuan Zhang
    id: xuan-zhang
    last: Zhang
  - first: Pamela
    full: Pamela Shapiro
    id: pamela-shapiro
    last: Shapiro
  - first: Gaurav
    full: Gaurav Kumar
    id: gaurav-kumar
    last: Kumar
  - first: Paul
    full: Paul McNamee
    id: paul-mcnamee
    last: McNamee
  - first: Marine
    full: Marine Carpuat
    id: marine-carpuat
    last: Carpuat
  - first: Kevin
    full: Kevin Duh
    id: kevin-duh
    last: Duh
  author_string: Xuan Zhang, Pamela Shapiro, Gaurav Kumar, Paul McNamee, Marine Carpuat,
    Kevin Duh
  bibkey: zhang-etal-2019-curriculum
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1189
  month: June
  page_first: '1903'
  page_last: '1915'
  pages: "1903\u20131915"
  paper_id: '189'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1189.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1189.jpg
  title: Curriculum Learning for Domain Adaptation in Neural Machine Translation
  title_html: Curriculum Learning for Domain Adaptation in Neural Machine Translation
  url: https://www.aclweb.org/anthology/N19-1189
  year: '2019'
N19-1190:
  abstract: Modern Machine Translation (MT) systems perform remarkably well on clean,
    in-domain text. However most of the human generated text, particularly in the
    realm of social media, is full of typos, slang, dialect, idiolect and other noise
    which can have a disastrous impact on the accuracy of MT. In this paper we propose
    methods to enhance the robustness of MT systems by emulating naturally occurring
    noise in otherwise clean data. Synthesizing noise in this manner we are ultimately
    able to make a vanilla MT system more resilient to naturally occurring noise,
    partially mitigating loss in accuracy resulting therefrom.
  address: Minneapolis, Minnesota
  author:
  - first: Vaibhav
    full: Vaibhav Vaibhav
    id: vaibhav-vaibhav
    last: Vaibhav
  - first: Sumeet
    full: Sumeet Singh
    id: sumeet-singh
    last: Singh
  - first: Craig
    full: Craig Stewart
    id: craig-stewart
    last: Stewart
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  author_string: Vaibhav Vaibhav, Sumeet Singh, Craig Stewart, Graham Neubig
  bibkey: vaibhav-etal-2019-improving
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1190
  month: June
  page_first: '1916'
  page_last: '1920'
  pages: "1916\u20131920"
  paper_id: '190'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1190.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1190.jpg
  title: Improving Robustness of Machine Translation with Synthetic Noise
  title_html: Improving Robustness of Machine Translation with Synthetic Noise
  url: https://www.aclweb.org/anthology/N19-1190
  year: '2019'
N19-1191:
  abstract: Neural Networks trained with gradient descent are known to be susceptible
    to catastrophic forgetting caused by parameter shift during the training process.
    In the context of Neural Machine Translation (NMT) this results in poor performance
    on heterogeneous datasets and on sub-tasks like rare phrase translation. On the
    other hand, non-parametric approaches are immune to forgetting, perfectly complementing
    the generalization ability of NMT. However, attempts to combine non-parametric
    or retrieval based approaches with NMT have only been successful on narrow domains,
    possibly due to over-reliance on sentence level retrieval. We propose a novel
    n-gram level retrieval approach that relies on local phrase level similarities,
    allowing us to retrieve neighbors that are useful for translation even when overall
    sentence similarity is low. We complement this with an expressive neural network,
    allowing our model to extract information from the noisy retrieved context. We
    evaluate our Semi-parametric NMT approach on a heterogeneous dataset composed
    of WMT, IWSLT, JRC-Acquis and OpenSubtitles, and demonstrate gains on all 4 evaluation
    sets. The Semi-parametric nature of our approach also opens the door for non-parametric
    domain adaptation, demonstrating strong inference-time adaptation performance
    on new domains without the need for any parameter updates.
  address: Minneapolis, Minnesota
  author:
  - first: Ankur
    full: Ankur Bapna
    id: ankur-bapna
    last: Bapna
  - first: Orhan
    full: Orhan Firat
    id: orhan-firat
    last: Firat
  author_string: Ankur Bapna, Orhan Firat
  bibkey: bapna-firat-2019-non
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1191
  month: June
  page_first: '1921'
  page_last: '1931'
  pages: "1921\u20131931"
  paper_id: '191'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1191.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1191.jpg
  title: Non-Parametric Adaptation for Neural Machine Translation
  title_html: Non-Parametric Adaptation for Neural Machine Translation
  url: https://www.aclweb.org/anthology/N19-1191
  year: '2019'
N19-1192:
  abstract: Current predominant neural machine translation (NMT) models often have
    a deep structure with large amounts of parameters, making these models hard to
    train and easily suffering from over-fitting. A common practice is to utilize
    a validation set to evaluate the training process and select the best checkpoint.
    Average and ensemble techniques on checkpoints can lead to further performance
    improvement. However, as these methods do not affect the training process, the
    system performance is restricted to the checkpoints generated in original training
    procedure. In contrast, we propose an online knowledge distillation method. Our
    method on-the-fly generates a teacher model from checkpoints, guiding the training
    process to obtain better performance. Experiments on several datasets and language
    pairs show steady improvement over a strong self-attention-based baseline system.
    We also provide analysis on data-limited setting against over-fitting. Furthermore,
    our method leads to an improvement in a machine reading experiment as well.
  address: Minneapolis, Minnesota
  author:
  - first: Hao-Ran
    full: Hao-Ran Wei
    id: hao-ran-wei
    last: Wei
  - first: Shujian
    full: Shujian Huang
    id: shujian-huang
    last: Huang
  - first: Ran
    full: Ran Wang
    id: ran-wang
    last: Wang
  - first: Xin-yu
    full: Xin-yu Dai
    id: xinyu-dai
    last: Dai
  - first: Jiajun
    full: Jiajun Chen
    id: jiajun-chen
    last: Chen
  author_string: Hao-Ran Wei, Shujian Huang, Ran Wang, Xin-yu Dai, Jiajun Chen
  bibkey: wei-etal-2019-online
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1192
  month: June
  page_first: '1932'
  page_last: '1941'
  pages: "1932\u20131941"
  paper_id: '192'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1192.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1192.jpg
  title: Online Distilling from Checkpoints for Neural Machine Translation
  title_html: Online Distilling from Checkpoints for Neural Machine Translation
  url: https://www.aclweb.org/anthology/N19-1192
  year: '2019'
N19-1193:
  abstract: Training models to map natural language instructions to programs, given
    target world supervision only, requires searching for good programs at training
    time. Search is commonly done using beam search in the space of partial programs
    or program trees, but as the length of the instructions grows finding a good program
    becomes difficult. In this work, we propose a search algorithm that uses the target
    world state, known at training time, to train a critic network that predicts the
    expected reward of every search state. We then score search states on the beam
    by interpolating their expected reward with the likelihood of programs represented
    by the search state. Moreover, we search not in the space of programs but in a
    more compressed state of program executions, augmented with recent entities and
    actions. On the SCONE dataset, we show that our algorithm dramatically improves
    performance on all three domains compared to standard beam search and other baselines.
  address: Minneapolis, Minnesota
  author:
  - first: Dor
    full: Dor Muhlgay
    id: dor-muhlgay
    last: Muhlgay
  - first: Jonathan
    full: Jonathan Herzig
    id: jonathan-herzig
    last: Herzig
  - first: Jonathan
    full: Jonathan Berant
    id: jonathan-berant
    last: Berant
  author_string: Dor Muhlgay, Jonathan Herzig, Jonathan Berant
  bibkey: muhlgay-etal-2019-value
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1193
  month: June
  page_first: '1942'
  page_last: '1954'
  pages: "1942\u20131954"
  paper_id: '193'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1193.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1193.jpg
  title: Value-based Search in Execution Space for Mapping Instructions to Programs
  title_html: Value-based Search in Execution Space for Mapping Instructions to Programs
  url: https://www.aclweb.org/anthology/N19-1193
  year: '2019'
N19-1194:
  abstract: We propose a new visual grounding task called Visual Query Detection (VQD).
    In VQD, the task is to localize a variable number of objects in an image where
    the objects are specified in natural language. VQD is related to visual referring
    expression comprehension, where the task is to localize only one object. We propose
    the first algorithms for VQD, and we evaluate them on both visual referring expression
    datasets and our new VQDv1 dataset.
  address: Minneapolis, Minnesota
  author:
  - first: Manoj
    full: Manoj Acharya
    id: manoj-acharya
    last: Acharya
  - first: Karan
    full: Karan Jariwala
    id: karan-jariwala
    last: Jariwala
  - first: Christopher
    full: Christopher Kanan
    id: christopher-kanan
    last: Kanan
  author_string: Manoj Acharya, Karan Jariwala, Christopher Kanan
  bibkey: acharya-etal-2019-vqd
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1194
  month: June
  page_first: '1955'
  page_last: '1961'
  pages: "1955\u20131961"
  paper_id: '194'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1194.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1194.jpg
  title: 'VQD: Visual Query Detection In Natural Scenes'
  title_html: '<span class="acl-fixed-case">VQD</span>: Visual Query Detection In
    Natural Scenes'
  url: https://www.aclweb.org/anthology/N19-1194
  year: '2019'
N19-1195:
  abstract: "Over the last few years, there has been growing interest in learning\
    \ models for physically grounded language understanding tasks, such as the popular\
    \ blocks world domain. These works typically view this problem as a single-step\
    \ process, in which a human operator gives an instruction and an automated agent\
    \ is evaluated on its ability to execute it. In this paper we take the first step\
    \ towards increasing the bandwidth of this interaction, and suggest a protocol\
    \ for including advice, high-level observations about the task, which can help\
    \ constrain the agent\u2019s prediction. We evaluate our approach on the blocks\
    \ world task, and show that even simple advice can help lead to significant performance\
    \ improvements. To help reduce the effort involved in supplying the advice, we\
    \ also explore model self-generated advice which can still improve results."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1195.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1195.Supplementary.pdf
  author:
  - first: Nikhil
    full: Nikhil Mehta
    id: nikhil-mehta
    last: Mehta
  - first: Dan
    full: Dan Goldwasser
    id: dan-goldwasser
    last: Goldwasser
  author_string: Nikhil Mehta, Dan Goldwasser
  bibkey: mehta-goldwasser-2019-improving
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1195
  month: June
  page_first: '1962'
  page_last: '1967'
  pages: "1962\u20131967"
  paper_id: '195'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1195.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1195.jpg
  title: Improving Natural Language Interaction with Robots Using Advice
  title_html: Improving Natural Language Interaction with Robots Using Advice
  url: https://www.aclweb.org/anthology/N19-1195
  year: '2019'
N19-1196:
  abstract: We present a novel method for mapping unrestricted text to knowledge graph
    entities by framing the task as a sequence-to-sequence problem. Specifically,
    given the encoded state of an input text, our decoder directly predicts paths
    in the knowledge graph, starting from the root and ending at the the target node
    following hypernym-hyponym relationships. In this way, and in contrast to other
    text-to-entity mapping systems, our model outputs hierarchically structured predictions
    that are fully interpretable in the context of the underlying ontology, in an
    end-to-end manner. We present a proof-of-concept experiment with encouraging results,
    comparable to those of state-of-the-art systems.
  address: Minneapolis, Minnesota
  author:
  - first: Victor
    full: Victor Prokhorov
    id: victor-prokhorov
    last: Prokhorov
  - first: Mohammad Taher
    full: Mohammad Taher Pilehvar
    id: mohammad-taher-pilehvar
    last: Pilehvar
  - first: Nigel
    full: Nigel Collier
    id: nigel-collier
    last: Collier
  author_string: Victor Prokhorov, Mohammad Taher Pilehvar, Nigel Collier
  bibkey: prokhorov-etal-2019-generating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1196
  month: June
  page_first: '1968'
  page_last: '1976'
  pages: "1968\u20131976"
  paper_id: '196'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1196.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1196.jpg
  title: Generating Knowledge Graph Paths from Textual Definitions using Sequence-to-Sequence
    Models
  title_html: Generating Knowledge Graph Paths from Textual Definitions using Sequence-to-Sequence
    Models
  url: https://www.aclweb.org/anthology/N19-1196
  year: '2019'
N19-1197:
  abstract: We demonstrate the surprising strength of unimodal baselines in multimodal
    domains, and make concrete recommendations for best practices in future research.
    Where existing work often compares against random or majority class baselines,
    we argue that unimodal approaches better capture and reflect dataset biases and
    therefore provide an important comparison when assessing the performance of multimodal
    techniques. We present unimodal ablations on three recent datasets in visual navigation
    and QA, seeing an up to 29% absolute gain in performance over published baselines.
  address: Minneapolis, Minnesota
  author:
  - first: Jesse
    full: Jesse Thomason
    id: jesse-thomason
    last: Thomason
  - first: Daniel
    full: Daniel Gordon
    id: daniel-gordon
    last: Gordon
  - first: Yonatan
    full: Yonatan Bisk
    id: yonatan-bisk
    last: Bisk
  author_string: Jesse Thomason, Daniel Gordon, Yonatan Bisk
  bibkey: thomason-etal-2019-shifting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1197
  month: June
  page_first: '1977'
  page_last: '1983'
  pages: "1977\u20131983"
  paper_id: '197'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1197.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1197.jpg
  title: 'Shifting the Baseline: Single Modality Performance on Visual Navigation
    & QA'
  title_html: 'Shifting the Baseline: Single Modality Performance on Visual Navigation
    &amp; <span class="acl-fixed-case">QA</span>'
  url: https://www.aclweb.org/anthology/N19-1197
  year: '2019'
N19-1198:
  abstract: The task of retrieving clips within videos based on a given natural language
    query requires cross-modal reasoning over multiple frames. Prior approaches such
    as sliding window classifiers are inefficient, while text-clip similarity driven
    ranking-based approaches such as segment proposal networks are far more complicated.
    In order to select the most relevant video clip corresponding to the given text
    description, we propose a novel extractive approach that predicts the start and
    end frames by leveraging cross-modal interactions between the text and video -
    this removes the need to retrieve and re-rank multiple proposal segments. Using
    recurrent networks we encode the two modalities into a joint representation which
    is then used in different variants of start-end frame predictor networks. Through
    extensive experimentation and ablative analysis, we demonstrate that our simple
    and elegant approach significantly outperforms state of the art on two datasets
    and has comparable performance on a third.
  address: Minneapolis, Minnesota
  author:
  - first: Soham
    full: Soham Ghosh
    id: soham-ghosh
    last: Ghosh
  - first: Anuva
    full: Anuva Agarwal
    id: anuva-agarwal
    last: Agarwal
  - first: Zarana
    full: Zarana Parekh
    id: zarana-parekh
    last: Parekh
  - first: Alexander
    full: Alexander Hauptmann
    id: alexander-g-hauptmann
    last: Hauptmann
  author_string: Soham Ghosh, Anuva Agarwal, Zarana Parekh, Alexander Hauptmann
  bibkey: ghosh-etal-2019-excl
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1198
  month: June
  page_first: '1984'
  page_last: '1990'
  pages: "1984\u20131990"
  paper_id: '198'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1198.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1198.jpg
  title: 'ExCL: Extractive Clip Localization Using Natural Language Descriptions'
  title_html: '<span class="acl-fixed-case">E</span>x<span class="acl-fixed-case">CL</span>:
    <span class="acl-fixed-case">E</span>xtractive <span class="acl-fixed-case">C</span>lip
    <span class="acl-fixed-case">L</span>ocalization <span class="acl-fixed-case">U</span>sing
    <span class="acl-fixed-case">N</span>atural <span class="acl-fixed-case">L</span>anguage
    <span class="acl-fixed-case">D</span>escriptions'
  url: https://www.aclweb.org/anthology/N19-1198
  year: '2019'
N19-1199:
  abstract: "Machine learning has shown promise for automatic detection of Alzheimer\u2019\
    s disease (AD) through speech; however, efforts are hampered by a scarcity of\
    \ data, especially in languages other than English. We propose a method to learn\
    \ a correspondence between independently engineered lexicosyntactic features in\
    \ two languages, using a large parallel corpus of out-of-domain movie dialogue\
    \ data. We apply it to dementia detection in Mandarin Chinese, and demonstrate\
    \ that our method outperforms both unilingual and machine translation-based baselines.\
    \ This appears to be the first study that transfers feature domains in detecting\
    \ cognitive decline."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/354235335
    type: video
    url: https://vimeo.com/354235335
  author:
  - first: Bai
    full: Bai Li
    id: bai-li
    last: Li
  - first: Yi-Te
    full: Yi-Te Hsu
    id: yi-te-hsu
    last: Hsu
  - first: Frank
    full: Frank Rudzicz
    id: frank-rudzicz
    last: Rudzicz
  author_string: Bai Li, Yi-Te Hsu, Frank Rudzicz
  bibkey: li-etal-2019-detecting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1199
  month: June
  page_first: '1991'
  page_last: '1997'
  pages: "1991\u20131997"
  paper_id: '199'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1199.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1199.jpg
  title: Detecting dementia in Mandarin Chinese using transfer learning from a parallel
    corpus
  title_html: Detecting dementia in <span class="acl-fixed-case">M</span>andarin <span
    class="acl-fixed-case">C</span>hinese using transfer learning from a parallel
    corpus
  url: https://www.aclweb.org/anthology/N19-1199
  year: '2019'
N19-1200:
  abstract: Recent work has shown that visual context improves cross-lingual sense
    disambiguation for nouns. We extend this line of work to the more challenging
    task of cross-lingual verb sense disambiguation, introducing the MultiSense dataset
    of 9,504 images annotated with English, German, and Spanish verbs. Each image
    in MultiSense is annotated with an English verb and its translation in German
    or Spanish. We show that cross-lingual verb sense disambiguation models benefit
    from visual context, compared to unimodal baselines. We also show that the verb
    sense predicted by our best disambiguation model can improve the results of a
    text-only machine translation system when used for a multimodal translation task.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/354228781
    type: video
    url: https://vimeo.com/354228781
  author:
  - first: Spandana
    full: Spandana Gella
    id: spandana-gella
    last: Gella
  - first: Desmond
    full: Desmond Elliott
    id: desmond-elliott
    last: Elliott
  - first: Frank
    full: Frank Keller
    id: frank-keller
    last: Keller
  author_string: Spandana Gella, Desmond Elliott, Frank Keller
  bibkey: gella-etal-2019-cross
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1200
  month: June
  page_first: '1998'
  page_last: '2004'
  pages: "1998\u20132004"
  paper_id: '200'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1200.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1200.jpg
  title: Cross-lingual Visual Verb Sense Disambiguation
  title_html: Cross-lingual Visual Verb Sense Disambiguation
  url: https://www.aclweb.org/anthology/N19-1200
  year: '2019'
N19-1201:
  abstract: "Language identification for code-switching (CS), the phenomenon of alternating\
    \ between two or more languages in conversations, has traditionally been approached\
    \ under the assumption of a single language per token. However, if at least one\
    \ language is morphologically rich, a large number of words can be composed of\
    \ morphemes from more than one language (intra-word CS). In this paper, we extend\
    \ the language identification task to the subword-level, such that it includes\
    \ splitting mixed words while tagging each part with a language ID. We further\
    \ propose a model for this task, which is based on a segmental recurrent neural\
    \ network. In experiments on a new Spanish\u2013Wixarika dataset and on an adapted\
    \ German\u2013Turkish dataset, our proposed model performs slightly better than\
    \ or roughly on par with our best baseline, respectively. Considering only mixed\
    \ words, however, it strongly outperforms all baselines."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/354264673
    type: video
    url: https://vimeo.com/354264673
  author:
  - first: Manuel
    full: Manuel Mager
    id: manuel-mager
    last: Mager
  - first: "\xD6zlem"
    full: "\xD6zlem \xC7etino\u011Flu"
    id: ozlem-cetinoglu
    last: "\xC7etino\u011Flu"
  - first: Katharina
    full: Katharina Kann
    id: katharina-kann
    last: Kann
  author_string: "Manuel Mager, \xD6zlem \xC7etino\u011Flu, Katharina Kann"
  bibkey: mager-etal-2019-subword
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1201
  month: June
  page_first: '2005'
  page_last: '2011'
  pages: "2005\u20132011"
  paper_id: '201'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1201.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1201.jpg
  title: Subword-Level Language Identification for Intra-Word Code-Switching
  title_html: Subword-Level Language Identification for Intra-Word Code-Switching
  url: https://www.aclweb.org/anthology/N19-1201
  year: '2019'
N19-1202:
  abstract: 'Current research on spoken language translation (SLT) has to confront
    with the scarcity of sizeable and publicly available training corpora. This problem
    hinders the adoption of neural end-to-end approaches, which represent the state
    of the art in the two parent tasks of SLT: automatic speech recognition and machine
    translation. To fill this gap, we created MuST-C, a multilingual speech translation
    corpus whose size and quality will facilitate the training of end-to-end systems
    for SLT from English into 8 languages. For each target language, MuST-C comprises
    at least 385 hours of audio recordings from English TED Talks, which are automatically
    aligned at the sentence level with their manual transcriptions and translations.
    Together with a description of the corpus creation methodology (scalable to add
    new data and cover new languages), we provide an empirical verification of its
    quality and SLT results computed with a state-of-the-art approach on each language
    direction.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1202.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1202.Supplementary.pdf
  - filename: https://vimeo.com/354264090
    type: video
    url: https://vimeo.com/354264090
  author:
  - first: Mattia A.
    full: Mattia A. Di Gangi
    id: mattia-a-di-gangi
    last: Di Gangi
  - first: Roldano
    full: Roldano Cattoni
    id: roldano-cattoni
    last: Cattoni
  - first: Luisa
    full: Luisa Bentivogli
    id: luisa-bentivogli
    last: Bentivogli
  - first: Matteo
    full: Matteo Negri
    id: matteo-negri
    last: Negri
  - first: Marco
    full: Marco Turchi
    id: marco-turchi
    last: Turchi
  author_string: Mattia A. Di Gangi, Roldano Cattoni, Luisa Bentivogli, Matteo Negri,
    Marco Turchi
  bibkey: di-gangi-etal-2019-must
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1202
  month: June
  page_first: '2012'
  page_last: '2017'
  pages: "2012\u20132017"
  paper_id: '202'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1202.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1202.jpg
  title: 'MuST-C: a Multilingual Speech Translation Corpus'
  title_html: '<span class="acl-fixed-case">M</span>u<span class="acl-fixed-case">ST</span>-<span
    class="acl-fixed-case">C</span>: a <span class="acl-fixed-case">M</span>ultilingual
    <span class="acl-fixed-case">S</span>peech <span class="acl-fixed-case">T</span>ranslation
    <span class="acl-fixed-case">C</span>orpus'
  url: https://www.aclweb.org/anthology/N19-1202
  year: '2019'
N19-1203:
  abstract: "Critical to natural language generation is the production of correctly\
    \ inflected text. In this paper, we isolate the task of predicting a fully inflected\
    \ sentence from its partially lemmatized version. Unlike traditional morphological\
    \ inflection or surface realization, our task input does not provide \u201Cgold\u201D\
    \ tags that specify what morphological features to realize on each lemmatized\
    \ word; rather, such features must be inferred from sentential context. We develop\
    \ a neural hybrid graphical model that explicitly reconstructs morphological features\
    \ before predicting the inflected forms, and compare this to a system that directly\
    \ predicts the inflected forms without relying on any morphological annotation.\
    \ We experiment on several typologically diverse languages from the Universal\
    \ Dependencies treebanks, showing the utility of incorporating linguistically-motivated\
    \ latent variables into NLP models."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/354264026
    type: video
    url: https://vimeo.com/354264026
  author:
  - first: Ekaterina
    full: Ekaterina Vylomova
    id: ekaterina-vylomova
    last: Vylomova
  - first: Ryan
    full: Ryan Cotterell
    id: ryan-cotterell
    last: Cotterell
  - first: Trevor
    full: Trevor Cohn
    id: trevor-cohn
    last: Cohn
  - first: Timothy
    full: Timothy Baldwin
    id: timothy-baldwin
    last: Baldwin
  - first: Jason
    full: Jason Eisner
    id: jason-eisner
    last: Eisner
  author_string: Ekaterina Vylomova, Ryan Cotterell, Trevor Cohn, Timothy Baldwin,
    Jason Eisner
  bibkey: vylomova-etal-2019-contextualization
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1203
  month: June
  page_first: '2018'
  page_last: '2024'
  pages: "2018\u20132024"
  paper_id: '203'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1203.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1203.jpg
  title: Contextualization of Morphological Inflection
  title_html: Contextualization of Morphological Inflection
  url: https://www.aclweb.org/anthology/N19-1203
  year: '2019'
N19-1204:
  abstract: 'We present a robust neural abstractive summarization system for cross-lingual
    summarization. We construct summarization corpora for documents automatically
    translated from three low-resource languages, Somali, Swahili, and Tagalog, using
    machine translation and the New York Times summarization corpus. We train three
    language-specific abstractive summarizers and evaluate on documents originally
    written in the source languages, as well as on a fourth, unseen language: Arabic.
    Our systems achieve significantly higher fluency than a standard copy-attention
    summarizer on automatically translated input documents, as well as comparable
    content selection.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/356048544
    type: video
    url: https://vimeo.com/356048544
  author:
  - first: Jessica
    full: Jessica Ouyang
    id: jessica-ouyang
    last: Ouyang
  - first: Boya
    full: Boya Song
    id: boya-song
    last: Song
  - first: Kathy
    full: Kathy McKeown
    id: kathleen-mckeown
    last: McKeown
  author_string: Jessica Ouyang, Boya Song, Kathy McKeown
  bibkey: ouyang-etal-2019-robust
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1204
  month: June
  page_first: '2025'
  page_last: '2031'
  pages: "2025\u20132031"
  paper_id: '204'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1204.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1204.jpg
  title: A Robust Abstractive System for Cross-Lingual Summarization
  title_html: A Robust Abstractive System for Cross-Lingual Summarization
  url: https://www.aclweb.org/anthology/N19-1204
  year: '2019'
N19-1205:
  abstract: "The explicit use of syntactic information has been proved useful for\
    \ neural machine translation (NMT). However, previous methods resort to either\
    \ tree-structured neural networks or long linearized sequences, both of which\
    \ are inefficient. Neural syntactic distance (NSD) enables us to represent a constituent\
    \ tree using a sequence whose length is identical to the number of words in the\
    \ sentence. NSD has been used for constituent parsing, but not in machine translation.\
    \ We propose five strategies to improve NMT with NSD. Experiments show that it\
    \ is not trivial to improve NMT with NSD; however, the proposed strategies are\
    \ shown to improve translation performance of the baseline model (+2.1 (En\u2013\
    Ja), +1.3 (Ja\u2013En), +1.2 (En\u2013Ch), and +1.0 (Ch\u2013En) BLEU)."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/355792799
    type: video
    url: https://vimeo.com/355792799
  author:
  - first: Chunpeng
    full: Chunpeng Ma
    id: chunpeng-ma
    last: Ma
  - first: Akihiro
    full: Akihiro Tamura
    id: akihiro-tamura
    last: Tamura
  - first: Masao
    full: Masao Utiyama
    id: masao-utiyama
    last: Utiyama
  - first: Eiichiro
    full: Eiichiro Sumita
    id: eiichiro-sumita
    last: Sumita
  - first: Tiejun
    full: Tiejun Zhao
    id: tiejun-zhao
    last: Zhao
  author_string: Chunpeng Ma, Akihiro Tamura, Masao Utiyama, Eiichiro Sumita, Tiejun
    Zhao
  bibkey: ma-etal-2019-improving
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1205
  month: June
  page_first: '2032'
  page_last: '2037'
  pages: "2032\u20132037"
  paper_id: '205'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1205.pdf
  publisher: Association for Computational Linguistics
  revision:
  - id: '1'
    url: https://www.aclweb.org/anthology/N19-1205v1.pdf
    value: N19-1205v1
  - explanation: Tiejun Zhao is marked as the "corresponding author".
    id: '2'
    url: https://www.aclweb.org/anthology/N19-1205v2.pdf
    value: N19-1205v2
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1205.jpg
  title: Improving Neural Machine Translation with Neural Syntactic Distance
  title_html: Improving Neural Machine Translation with Neural Syntactic Distance
  url: https://www.aclweb.org/anthology/N19-1205
  year: '2019'
N19-1206:
  abstract: Incremental domain adaptation, in which a system learns from the correct
    output for each input immediately after making its prediction for that input,
    can dramatically improve system performance for interactive machine translation.
    Users of interactive systems are sensitive to the speed of adaptation and how
    often a system repeats mistakes, despite being corrected. Adaptation is most commonly
    assessed using corpus-level BLEU- or TER-derived metrics that do not explicitly
    take adaptation speed into account. We find that these metrics often do not capture
    immediate adaptation effects, such as zero-shot and one-shot learning of domain-specific
    lexical items. To this end, we propose new metrics that directly evaluate immediate
    adaptation performance for machine translation. We use these metrics to choose
    the most suitable adaptation method from a range of different adaptation techniques
    for neural machine translation systems.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1206.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1206.Presentation.pdf
  - filename: https://vimeo.com/355794917
    type: video
    url: https://vimeo.com/355794917
  author:
  - first: Patrick
    full: Patrick Simianer
    id: patrick-simianer
    last: Simianer
  - first: Joern
    full: Joern Wuebker
    id: joern-wuebker
    last: Wuebker
  - first: John
    full: John DeNero
    id: john-denero
    last: DeNero
  author_string: Patrick Simianer, Joern Wuebker, John DeNero
  bibkey: simianer-etal-2019-measuring
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1206
  month: June
  page_first: '2038'
  page_last: '2046'
  pages: "2038\u20132046"
  paper_id: '206'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1206.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1206.jpg
  title: Measuring Immediate Adaptation Performance for Neural Machine Translation
  title_html: Measuring Immediate Adaptation Performance for Neural Machine Translation
  url: https://www.aclweb.org/anthology/N19-1206
  year: '2019'
N19-1207:
  abstract: 'Despite some empirical success at correcting exposure bias in machine
    translation, scheduled sampling algorithms suffer from a major drawback: they
    incorrectly assume that words in the reference translations and in sampled sequences
    are aligned at each time step. Our new differentiable sampling algorithm addresses
    this issue by optimizing the probability that the reference can be aligned with
    the sampled output, based on a soft alignment predicted by the model itself. As
    a result, the output distribution at each time step is evaluated with respect
    to the whole predicted sequence. Experiments on IWSLT translation tasks show that
    our approach improves BLEU compared to maximum likelihood and scheduled sampling
    baselines. In addition, our approach is simpler to train with no need for sampling
    schedule and yields models that achieve larger improvements with smaller beam
    sizes.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/355796702
    type: video
    url: https://vimeo.com/355796702
  author:
  - first: Weijia
    full: Weijia Xu
    id: weijia-xu
    last: Xu
  - first: Xing
    full: Xing Niu
    id: xing-niu
    last: Niu
  - first: Marine
    full: Marine Carpuat
    id: marine-carpuat
    last: Carpuat
  author_string: Weijia Xu, Xing Niu, Marine Carpuat
  bibkey: xu-etal-2019-differentiable
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1207
  month: June
  page_first: '2047'
  page_last: '2053'
  pages: "2047\u20132053"
  paper_id: '207'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1207.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1207.jpg
  title: Differentiable Sampling with Flexible Reference Word Order for Neural Machine
    Translation
  title_html: Differentiable Sampling with Flexible Reference Word Order for Neural
    Machine Translation
  url: https://www.aclweb.org/anthology/N19-1207
  year: '2019'
N19-1208:
  abstract: We consider the problem of making efficient use of heterogeneous training
    data in neural machine translation (NMT). Specifically, given a training dataset
    with a sentence-level feature such as noise, we seek an optimal curriculum, or
    order for presenting examples to the system during training. Our curriculum framework
    allows examples to appear an arbitrary number of times, and thus generalizes data
    weighting, filtering, and fine-tuning schemes. Rather than relying on prior knowledge
    to design a curriculum, we use reinforcement learning to learn one automatically,
    jointly with the NMT system, in the course of a single training run. We show that
    this approach can beat uniform baselines on Paracrawl and WMT English-to-French
    datasets by +3.4 and +1.3 BLEU respectively. Additionally, we match the performance
    of strong filtering baselines and hand-designed, state-of-the-art curricula.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/355798547
    type: video
    url: https://vimeo.com/355798547
  author:
  - first: Gaurav
    full: Gaurav Kumar
    id: gaurav-kumar
    last: Kumar
  - first: George
    full: George Foster
    id: george-foster
    last: Foster
  - first: Colin
    full: Colin Cherry
    id: colin-cherry
    last: Cherry
  - first: Maxim
    full: Maxim Krikun
    id: maxim-krikun
    last: Krikun
  author_string: Gaurav Kumar, George Foster, Colin Cherry, Maxim Krikun
  bibkey: kumar-etal-2019-reinforcement
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1208
  month: June
  page_first: '2054'
  page_last: '2061'
  pages: "2054\u20132061"
  paper_id: '208'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1208.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1208.jpg
  title: Reinforcement Learning based Curriculum Optimization for Neural Machine Translation
  title_html: Reinforcement Learning based Curriculum Optimization for Neural Machine
    Translation
  url: https://www.aclweb.org/anthology/N19-1208
  year: '2019'
N19-1209:
  abstract: "Continued training is an effective method for domain adaptation in neural\
    \ machine translation. However, in-domain gains from adaptation come at the expense\
    \ of general-domain performance. In this work, we interpret the drop in general-domain\
    \ performance as catastrophic forgetting of general-domain knowledge. To mitigate\
    \ it, we adapt Elastic Weight Consolidation (EWC)\u2014a machine learning method\
    \ for learning a new task without forgetting previous tasks. Our method retains\
    \ the majority of general-domain performance lost in continued training without\
    \ degrading in-domain performance, outperforming the previous state-of-the-art.\
    \ We also explore the full range of general-domain performance available when\
    \ some in-domain degradation is acceptable."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/356056256
    type: video
    url: https://vimeo.com/356056256
  author:
  - first: Brian
    full: Brian Thompson
    id: brian-thompson
    last: Thompson
  - first: Jeremy
    full: Jeremy Gwinnup
    id: jeremy-gwinnup
    last: Gwinnup
  - first: Huda
    full: Huda Khayrallah
    id: huda-khayrallah
    last: Khayrallah
  - first: Kevin
    full: Kevin Duh
    id: kevin-duh
    last: Duh
  - first: Philipp
    full: Philipp Koehn
    id: philipp-koehn
    last: Koehn
  author_string: Brian Thompson, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, Philipp
    Koehn
  bibkey: thompson-etal-2019-overcoming
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1209
  month: June
  page_first: '2062'
  page_last: '2068'
  pages: "2062\u20132068"
  paper_id: '209'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1209.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1209.jpg
  title: Overcoming Catastrophic Forgetting During Domain Adaptation of Neural Machine
    Translation
  title_html: Overcoming Catastrophic Forgetting During Domain Adaptation of Neural
    Machine Translation
  url: https://www.aclweb.org/anthology/N19-1209
  year: '2019'
N19-1210:
  abstract: We present the first exploration of meaning shift over short periods of
    time in online communities using distributional representations. We create a small
    annotated dataset and use it to assess the performance of a standard model for
    meaning shift detection on short-term meaning shift. We find that the model has
    problems distinguishing meaning shift from referential phenomena, and propose
    a measure of contextual variability to remedy this.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/354246126
    type: video
    url: https://vimeo.com/354246126
  author:
  - first: Marco
    full: Marco Del Tredici
    id: marco-del-tredici
    last: Del Tredici
  - first: Raquel
    full: "Raquel Fern\xE1ndez"
    id: raquel-fernandez
    last: "Fern\xE1ndez"
  - first: Gemma
    full: Gemma Boleda
    id: gemma-boleda
    last: Boleda
  author_string: "Marco Del Tredici, Raquel Fern\xE1ndez, Gemma Boleda"
  bibkey: del-tredici-etal-2019-short
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1210
  month: June
  page_first: '2069'
  page_last: '2075'
  pages: "2069\u20132075"
  paper_id: '210'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1210.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1210.jpg
  title: 'Short-Term Meaning Shift: A Distributional Exploration'
  title_html: 'Short-Term Meaning Shift: A Distributional Exploration'
  url: https://www.aclweb.org/anthology/N19-1210
  year: '2019'
N19-1211:
  abstract: "We examine the new task of detecting derogatory compounds (e.g. \u201C\
    curry muncher\u201D). Derogatory compounds are much more difficult to detect than\
    \ derogatory unigrams (e.g. \u201Cidiot\u201D) since they are more sparsely represented\
    \ in lexical resources previously found effective for this task (e.g. Wiktionary).\
    \ We propose an unsupervised classification approach that incorporates linguistic\
    \ properties of compounds. It mostly depends on a simple distributional representation.\
    \ We compare our approach against previously established methods proposed for\
    \ extracting derogatory unigrams."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/354243652
    type: video
    url: https://vimeo.com/354243652
  author:
  - first: Michael
    full: Michael Wiegand
    id: michael-wiegand
    last: Wiegand
  - first: Maximilian
    full: Maximilian Wolf
    id: maximilian-wolf
    last: Wolf
  - first: Josef
    full: Josef Ruppenhofer
    id: josef-ruppenhofer
    last: Ruppenhofer
  author_string: Michael Wiegand, Maximilian Wolf, Josef Ruppenhofer
  bibkey: wiegand-etal-2019-detecting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1211
  month: June
  page_first: '2076'
  page_last: '2081'
  pages: "2076\u20132081"
  paper_id: '211'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1211.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1211.jpg
  title: "Detecting Derogatory Compounds \u2013 An Unsupervised Approach"
  title_html: "<span class=\"acl-fixed-case\">D</span>etecting <span class=\"acl-fixed-case\"\
    >D</span>erogatory <span class=\"acl-fixed-case\">C</span>ompounds \u2013 <span\
    \ class=\"acl-fixed-case\">A</span>n <span class=\"acl-fixed-case\">U</span>nsupervised\
    \ <span class=\"acl-fixed-case\">A</span>pproach"
  url: https://www.aclweb.org/anthology/N19-1211
  year: '2019'
N19-1212:
  abstract: "Collaborative filtering (CF) is a core technique for recommender systems.\
    \ Traditional CF approaches exploit user-item relations (e.g., clicks, likes,\
    \ and views) only and hence they suffer from the data sparsity issue. Items are\
    \ usually associated with unstructured text such as article abstracts and product\
    \ reviews. We develop a Personalized Neural Embedding (PNE) framework to exploit\
    \ both interactions and words seamlessly. We learn such embeddings of users, items,\
    \ and words jointly, and predict user preferences on items based on these learned\
    \ representations. PNE estimates the probability that a user will like an item\
    \ by two terms\u2014behavior factors and semantic factors. On two real-world datasets,\
    \ PNE shows better performance than four state-of-the-art baselines in terms of\
    \ three metrics. We also show that PNE learns meaningful word embeddings by visualization."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/354241518
    type: video
    url: https://vimeo.com/354241518
  author:
  - first: Guangneng
    full: Guangneng Hu
    id: guangneng-hu
    last: Hu
  author_string: Guangneng Hu
  bibkey: hu-2019-personalized
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1212
  month: June
  page_first: '2082'
  page_last: '2088'
  pages: "2082\u20132088"
  paper_id: '212'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1212.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1212.jpg
  title: Personalized Neural Embeddings for Collaborative Filtering with Text
  title_html: Personalized Neural Embeddings for Collaborative Filtering with Text
  url: https://www.aclweb.org/anthology/N19-1212
  year: '2019'
N19-1213:
  abstract: A growing number of state-of-the-art transfer learning methods employ
    language models pretrained on large generic corpora. In this paper we present
    a conceptually simple and effective transfer learning approach that addresses
    the problem of catastrophic forgetting. Specifically, we combine the task-specific
    optimization function with an auxiliary language model objective, which is adjusted
    during the training process. This preserves language regularities captured by
    language models, while enabling sufficient adaptation for solving the target task.
    Our method does not require pretraining or finetuning separate components of the
    network and we train our models end-to-end in a single step. We present results
    on a variety of challenging affective and text classification tasks, surpassing
    well established transfer learning methods with greater level of complexity.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/354239263
    type: video
    url: https://vimeo.com/354239263
  author:
  - first: Alexandra
    full: Alexandra Chronopoulou
    id: alexandra-chronopoulou
    last: Chronopoulou
  - first: Christos
    full: Christos Baziotis
    id: christos-baziotis
    last: Baziotis
  - first: Alexandros
    full: Alexandros Potamianos
    id: alexandros-potamianos
    last: Potamianos
  author_string: Alexandra Chronopoulou, Christos Baziotis, Alexandros Potamianos
  bibkey: chronopoulou-etal-2019-embarrassingly
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1213
  month: June
  page_first: '2089'
  page_last: '2095'
  pages: "2089\u20132095"
  paper_id: '213'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1213.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1213.jpg
  title: An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language
    Models
  title_html: An Embarrassingly Simple Approach for Transfer Learning from Pretrained
    Language Models
  url: https://www.aclweb.org/anthology/N19-1213
  year: '2019'
N19-1214:
  abstract: 'Tweets are short messages that often include specialized language such
    as hashtags and emojis. In this paper, we present a simple strategy to process
    emojis: replace them with their natural language description and use pretrained
    word embeddings as normally done with standard words. We show that this strategy
    is more effective than using pretrained emoji embeddings for tweet classification.
    Specifically, we obtain new state-of-the-art results in irony detection and sentiment
    analysis despite our neural network is simpler than previous proposals.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/354236787
    type: video
    url: https://vimeo.com/354236787
  author:
  - first: Abhishek
    full: Abhishek Singh
    id: abhishek-singh
    last: Singh
  - first: Eduardo
    full: Eduardo Blanco
    id: eduardo-blanco
    last: Blanco
  - first: Wei
    full: Wei Jin
    id: wei-jin
    last: Jin
  author_string: Abhishek Singh, Eduardo Blanco, Wei Jin
  bibkey: singh-etal-2019-incorporating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1214
  month: June
  page_first: '2096'
  page_last: '2101'
  pages: "2096\u20132101"
  paper_id: '214'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1214.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1214.jpg
  title: Incorporating Emoji Descriptions Improves Tweet Classification
  title_html: Incorporating Emoji Descriptions Improves Tweet Classification
  url: https://www.aclweb.org/anthology/N19-1214
  year: '2019'
N19-1215:
  abstract: "There exist biases in individual\u2019s language use; the same word (e.g.,\
    \ cool) is used for expressing different meanings (e.g., temperature range) or\
    \ different words (e.g., cloudy, hazy) are used for describing the same meaning.\
    \ In this study, we propose a method of modeling such personal biases in word\
    \ meanings (hereafter, semantic variations) with personalized word embeddings\
    \ obtained by solving a task on subjective text while regarding words used by\
    \ different individuals as different words. To prevent personalized word embeddings\
    \ from being contaminated by other irrelevant biases, we solve a task of identifying\
    \ a review-target (objective output) from a given review. To stabilize the training\
    \ of this extreme multi-class classification, we perform a multi-task learning\
    \ with metadata identification. Experimental results with reviews retrieved from\
    \ RateBeer confirmed that the obtained personalized word embeddings improved the\
    \ accuracy of sentiment analysis as well as the target task. Analysis of the obtained\
    \ personalized word embeddings revealed trends in semantic variations related\
    \ to frequent and adjective words."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/359682958
    type: video
    url: https://vimeo.com/359682958
  author:
  - first: Daisuke
    full: Daisuke Oba
    id: daisuke-oba
    last: Oba
  - first: Naoki
    full: Naoki Yoshinaga
    id: naoki-yoshinaga
    last: Yoshinaga
  - first: Shoetsu
    full: Shoetsu Sato
    id: shoetsu-sato
    last: Sato
  - first: Satoshi
    full: Satoshi Akasaki
    id: satoshi-akasaki
    last: Akasaki
  - first: Masashi
    full: Masashi Toyoda
    id: masashi-toyoda
    last: Toyoda
  author_string: Daisuke Oba, Naoki Yoshinaga, Shoetsu Sato, Satoshi Akasaki, Masashi
    Toyoda
  bibkey: oba-etal-2019-modeling
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1215
  month: June
  page_first: '2102'
  page_last: '2108'
  pages: "2102\u20132108"
  paper_id: '215'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1215.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1215.jpg
  title: Modeling Personal Biases in Language Use by Inducing Personalized Word Embeddings
  title_html: Modeling Personal Biases in Language Use by Inducing Personalized Word
    Embeddings
  url: https://www.aclweb.org/anthology/N19-1215
  year: '2019'
N19-1216:
  abstract: 'In the context of fake news, bias, and propaganda, we study two important
    but relatively under-explored problems: (i) trustworthiness estimation (on a 3-point
    scale) and (ii) political ideology detection (left/right bias on a 7-point scale)
    of entire news outlets, as opposed to evaluating individual articles. In particular,
    we propose a multi-task ordinal regression framework that models the two problems
    jointly. This is motivated by the observation that hyper-partisanship is often
    linked to low trustworthiness, e.g., appealing to emotions rather than sticking
    to the facts, while center media tend to be generally more impartial and trustworthy.
    We further use several auxiliary tasks, modeling centrality, hyper-partisanship,
    as well as left-vs.-right bias on a coarse-grained scale. The evaluation results
    show sizable performance gains by the joint models over models that target the
    problems in isolation.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/355802649
    type: video
    url: https://vimeo.com/355802649
  author:
  - first: Ramy
    full: Ramy Baly
    id: ramy-baly
    last: Baly
  - first: Georgi
    full: Georgi Karadzhov
    id: georgi-karadzhov
    last: Karadzhov
  - first: Abdelrhman
    full: Abdelrhman Saleh
    id: abdelrhman-saleh
    last: Saleh
  - first: James
    full: James Glass
    id: james-glass
    last: Glass
  - first: Preslav
    full: Preslav Nakov
    id: preslav-nakov
    last: Nakov
  author_string: Ramy Baly, Georgi Karadzhov, Abdelrhman Saleh, James Glass, Preslav
    Nakov
  bibkey: baly-etal-2019-multi
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1216
  month: June
  page_first: '2109'
  page_last: '2116'
  pages: "2109\u20132116"
  paper_id: '216'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1216.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1216.jpg
  title: Multi-Task Ordinal Regression for Jointly Predicting the Trustworthiness
    and the Leading Political Ideology of News Media
  title_html: Multi-Task Ordinal Regression for Jointly Predicting the Trustworthiness
    and the Leading Political Ideology of News Media
  url: https://www.aclweb.org/anthology/N19-1216
  year: '2019'
N19-1217:
  abstract: A pun is a form of wordplay for an intended humorous or rhetorical effect,
    where a word suggests two or more meanings by exploiting polysemy (homographic
    pun) or phonological similarity to another word (heterographic pun). This paper
    presents an approach that addresses pun detection and pun location jointly from
    a sequence labeling perspective. We employ a new tagging scheme such that the
    model is capable of performing such a joint task, where useful structural information
    can be properly captured. We show that our proposed model is effective in handling
    both homographic and heterographic puns. Empirical results on the benchmark datasets
    demonstrate that our approach can achieve new state-of-the-art results.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/355805085
    type: video
    url: https://vimeo.com/355805085
  author:
  - first: Yanyan
    full: Yanyan Zou
    id: yanyan-zou
    last: Zou
  - first: Wei
    full: Wei Lu
    id: wei-lu
    last: Lu
  author_string: Yanyan Zou, Wei Lu
  bibkey: zou-lu-2019-joint
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1217
  month: June
  page_first: '2117'
  page_last: '2123'
  pages: "2117\u20132123"
  paper_id: '217'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1217.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1217.jpg
  title: Joint Detection and Location of English Puns
  title_html: Joint Detection and Location of <span class="acl-fixed-case">E</span>nglish
    Puns
  url: https://www.aclweb.org/anthology/N19-1217
  year: '2019'
N19-1218:
  abstract: "We explore the challenge of action prediction from textual descriptions\
    \ of scenes, a testbed to approximate whether text inference can be used to predict\
    \ upcoming actions. As a case of study, we consider the world of the Harry Potter\
    \ fantasy novels and inferring what spell will be cast next given a fragment of\
    \ a story. Spells act as keywords that abstract actions (e.g. \u2018Alohomora\u2019\
    \ to open a door) and denote a response to the environment. This idea is used\
    \ to automatically build HPAC, a corpus containing 82,836 samples and 85 actions.\
    \ We then evaluate different baselines. Among the tested models, an LSTM-based\
    \ approach obtains the best performance for frequent actions and large scene descriptions,\
    \ but approaches such as logistic regression behave well on infrequent actions."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/355806868
    type: video
    url: https://vimeo.com/355806868
  author:
  - first: David
    full: David Vilares
    id: david-vilares
    last: Vilares
  - first: Carlos
    full: "Carlos G\xF3mez-Rodr\xEDguez"
    id: carlos-gomez-rodriguez
    last: "G\xF3mez-Rodr\xEDguez"
  author_string: "David Vilares, Carlos G\xF3mez-Rodr\xEDguez"
  bibkey: vilares-gomez-rodriguez-2019-harry
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1218
  month: June
  page_first: '2124'
  page_last: '2130'
  pages: "2124\u20132130"
  paper_id: '218'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1218.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1218.jpg
  title: Harry Potter and the Action Prediction Challenge from Natural Language
  title_html: <span class="acl-fixed-case">H</span>arry <span class="acl-fixed-case">P</span>otter
    and the Action Prediction Challenge from Natural Language
  url: https://www.aclweb.org/anthology/N19-1218
  year: '2019'
N19-1219:
  abstract: Peer-review plays a critical role in the scientific writing and publication
    ecosystem. To assess the efficiency and efficacy of the reviewing process, one
    essential element is to understand and evaluate the reviews themselves. In this
    work, we study the content and structure of peer reviews under the argument mining
    framework, through automatically detecting (1) the argumentative propositions
    put forward by reviewers, and (2) their types (e.g., evaluating the work or making
    suggestions for improvement). We first collect 14.2K reviews from major machine
    learning and natural language processing venues. 400 reviews are annotated with
    10,386 propositions and corresponding types of Evaluation, Request, Fact, Reference,
    or Quote. We then train state-of-the-art proposition segmentation and classification
    models on the data to evaluate their utilities and identify new challenges for
    this new domain, motivating future directions for argument mining. Further experiments
    show that proposition usage varies across venues in amount, type, and topic.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1219.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1219.Supplementary.pdf
  - filename: https://vimeo.com/355808962
    type: video
    url: https://vimeo.com/355808962
  author:
  - first: Xinyu
    full: Xinyu Hua
    id: xinyu-hua
    last: Hua
  - first: Mitko
    full: Mitko Nikolov
    id: mitko-nikolov
    last: Nikolov
  - first: Nikhil
    full: Nikhil Badugu
    id: nikhil-badugu
    last: Badugu
  - first: Lu
    full: Lu Wang
    id: lu-wang
    last: Wang
  author_string: Xinyu Hua, Mitko Nikolov, Nikhil Badugu, Lu Wang
  bibkey: hua-etal-2019-argument-mining
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1219
  month: June
  page_first: '2131'
  page_last: '2137'
  pages: "2131\u20132137"
  paper_id: '219'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1219.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1219.jpg
  title: Argument Mining for Understanding Peer Reviews
  title_html: Argument Mining for Understanding Peer Reviews
  url: https://www.aclweb.org/anthology/N19-1219
  year: '2019'
N19-1220:
  abstract: "We present a new dataset comprised of 210,532 tokens evenly drawn from\
    \ 100 different English-language literary texts annotated for ACE entity categories\
    \ (person, location, geo-political entity, facility, organization, and vehicle).\
    \ These categories include non-named entities (such as \u201Cthe boy\u201D, \u201C\
    the kitchen\u201D) and nested structure (such as [[the cook]\u2019s sister]).\
    \ In contrast to existing datasets built primarily on news (focused on geo-political\
    \ entities and organizations), literary texts offer strikingly different distributions\
    \ of entity categories, with much stronger emphasis on people and description\
    \ of settings. We present empirical results demonstrating the performance of nested\
    \ entity recognition models in this domain; training natively on in-domain literary\
    \ data yields an improvement of over 20 absolute points in F-score (from 45.7\
    \ to 68.3), and mitigates a disparate impact in performance for male and female\
    \ entities present in models trained on news data."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/356050927
    type: video
    url: https://vimeo.com/356050927
  author:
  - first: David
    full: David Bamman
    id: david-bamman
    last: Bamman
  - first: Sejal
    full: Sejal Popat
    id: sejal-popat
    last: Popat
  - first: Sheng
    full: Sheng Shen
    id: sheng-shen
    last: Shen
  author_string: David Bamman, Sejal Popat, Sheng Shen
  bibkey: bamman-etal-2019-annotated
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1220
  month: June
  page_first: '2138'
  page_last: '2144'
  pages: "2138\u20132144"
  paper_id: '220'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1220.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1220.jpg
  title: An annotated dataset of literary entities
  title_html: An annotated dataset of literary entities
  url: https://www.aclweb.org/anthology/N19-1220
  year: '2019'
N19-1221:
  abstract: "Abuse on the Internet represents a significant societal problem of our\
    \ time. Previous research on automated abusive language detection in Twitter has\
    \ shown that community-based profiling of users is a promising technique for this\
    \ task. However, existing approaches only capture shallow properties of online\
    \ communities by modeling follower\u2013following relationships. In contrast,\
    \ working with graph convolutional networks (GCNs), we present the first approach\
    \ that captures not only the structure of online communities but also the linguistic\
    \ behavior of the users within them. We show that such a heterogeneous graph-structured\
    \ modeling of communities significantly advances the current state of the art\
    \ in abusive language detection."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/355811189
    type: video
    url: https://vimeo.com/355811189
  author:
  - first: Pushkar
    full: Pushkar Mishra
    id: pushkar-mishra
    last: Mishra
  - first: Marco
    full: Marco Del Tredici
    id: marco-del-tredici
    last: Del Tredici
  - first: Helen
    full: Helen Yannakoudakis
    id: helen-yannakoudakis
    last: Yannakoudakis
  - first: Ekaterina
    full: Ekaterina Shutova
    id: ekaterina-shutova
    last: Shutova
  author_string: Pushkar Mishra, Marco Del Tredici, Helen Yannakoudakis, Ekaterina
    Shutova
  bibkey: mishra-etal-2019-abusive
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1221
  month: June
  page_first: '2145'
  page_last: '2150'
  pages: "2145\u20132150"
  paper_id: '221'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1221.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1221.jpg
  title: Abusive Language Detection with Graph Convolutional Networks
  title_html: <span class="acl-fixed-case">A</span>busive <span class="acl-fixed-case">L</span>anguage
    <span class="acl-fixed-case">D</span>etection with <span class="acl-fixed-case">G</span>raph
    <span class="acl-fixed-case">C</span>onvolutional <span class="acl-fixed-case">N</span>etworks
  url: https://www.aclweb.org/anthology/N19-1221
  year: '2019'
N19-1222:
  abstract: Meaning conflation deficiency is one of the main limiting factors of word
    representations which, given their widespread use at the core of many NLP systems,
    can lead to inaccurate semantic understanding of the input text and inevitably
    hamper the performance. Sense representations target this problem. However, their
    potential impact has rarely been investigated in downstream NLP applications.
    Through a set of experiments on a state-of-the-art reverse dictionary system based
    on neural networks, we show that a simple adjustment aimed at addressing the meaning
    conflation deficiency can lead to substantial improvements.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/364724563
    type: video
    url: https://vimeo.com/364724563
  author:
  - first: Mohammad Taher
    full: Mohammad Taher Pilehvar
    id: mohammad-taher-pilehvar
    last: Pilehvar
  author_string: Mohammad Taher Pilehvar
  bibkey: pilehvar-2019-importance
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1222
  month: June
  page_first: '2151'
  page_last: '2156'
  pages: "2151\u20132156"
  paper_id: '222'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1222.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1222.jpg
  title: 'On the Importance of Distinguishing Word Meaning Representations: A Case
    Study on Reverse Dictionary Mapping'
  title_html: 'On the Importance of Distinguishing Word Meaning Representations: A
    Case Study on Reverse Dictionary Mapping'
  url: https://www.aclweb.org/anthology/N19-1222
  year: '2019'
N19-1223:
  abstract: 'Generating from Abstract Meaning Representation (AMR) is an underspecified
    problem, as many syntactic decisions are not specified by the semantic graph.
    To explicitly account for this variation, we break down generating from AMR into
    two steps: first generate a syntactic structure, and then generate the surface
    form. We show that decomposing the generation process this way leads to state-of-the-art
    single model performance generating from AMR without additional unlabelled data.
    We also demonstrate that we can generate meaning-preserving syntactic paraphrases
    of the same AMR graph, as judged by humans.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/364735719
    type: video
    url: https://vimeo.com/364735719
  author:
  - first: Kris
    full: Kris Cao
    id: kris-cao
    last: Cao
  - first: Stephen
    full: Stephen Clark
    id: stephen-clark
    last: Clark
  author_string: Kris Cao, Stephen Clark
  bibkey: cao-clark-2019-factorising
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1223
  month: June
  page_first: '2157'
  page_last: '2163'
  pages: "2157\u20132163"
  paper_id: '223'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1223.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1223.jpg
  title: Factorising AMR generation through syntax
  title_html: Factorising <span class="acl-fixed-case">AMR</span> generation through
    syntax
  url: https://www.aclweb.org/anthology/N19-1223
  year: '2019'
N19-1224:
  abstract: We present a resource for the task of FrameNet semantic frame disambiguation
    of over 5,000 word-sentence pairs from the Wikipedia corpus. The annotations were
    collected using a novel crowdsourcing approach with multiple workers per sentence
    to capture inter-annotator disagreement. In contrast to the typical approach of
    attributing the best single frame to each word, we provide a list of frames with
    disagreement-based scores that express the confidence with which each frame applies
    to the word. This is based on the idea that inter-annotator disagreement is at
    least partly caused by ambiguity that is inherent to the text and frames. We have
    found many examples where the semantics of individual frames overlap sufficiently
    to make them acceptable alternatives for interpreting a sentence. We have argued
    that ignoring this ambiguity creates an overly arbitrary target for training and
    evaluating natural language processing systems - if humans cannot agree, why would
    we expect the correct answer from a machine to be any different? To process this
    data we also utilized an expanded lemma-set provided by the Framester system,
    which merges FN with WordNet to enhance coverage. Our dataset includes annotations
    of 1,000 sentence-word pairs whose lemmas are not part of FN. Finally we present
    metrics for evaluating frame disambiguation systems that account for ambiguity.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1224.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1224.Supplementary.pdf
  - filename: N19-1224.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1224.Presentation.pdf
  - filename: https://vimeo.com/364709844
    type: video
    url: https://vimeo.com/364709844
  author:
  - first: Anca
    full: Anca Dumitrache
    id: anca-dumitrache
    last: Dumitrache
  - first: Lora
    full: Lora Aroyo
    id: lora-aroyo
    last: Aroyo
  - first: Chris
    full: Chris Welty
    id: chris-welty
    last: Welty
  author_string: Anca Dumitrache, Lora Aroyo, Chris Welty
  bibkey: dumitrache-etal-2019-crowdsourced
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1224
  month: June
  page_first: '2164'
  page_last: '2170'
  pages: "2164\u20132170"
  paper_id: '224'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1224.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1224.jpg
  title: A Crowdsourced Frame Disambiguation Corpus with Ambiguity
  title_html: A Crowdsourced Frame Disambiguation Corpus with Ambiguity
  url: https://www.aclweb.org/anthology/N19-1224
  year: '2019'
N19-1225:
  abstract: "Several datasets have recently been constructed to expose brittleness\
    \ in models trained on existing benchmarks. While model performance on these challenge\
    \ datasets is significantly lower compared to the original benchmark, it is unclear\
    \ what particular weaknesses they reveal. For example, a challenge dataset may\
    \ be difficult because it targets phenomena that current models cannot capture,\
    \ or because it simply exploits blind spots in a model\u2019s specific training\
    \ set. We introduce inoculation by fine-tuning, a new analysis method for studying\
    \ challenge datasets by exposing models (the metaphorical patient) to a small\
    \ amount of data from the challenge dataset (a metaphorical pathogen) and assessing\
    \ how well they can adapt. We apply our method to analyze the NLI \u201Cstress\
    \ tests\u201D (Naik et al., 2018) and the Adversarial SQuAD dataset (Jia and Liang,\
    \ 2017). We show that after slight exposure, some of these datasets are no longer\
    \ challenging, while others remain difficult. Our results indicate that failures\
    \ on challenge datasets may lead to very different conclusions about models, training\
    \ datasets, and the challenge datasets themselves."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/364712933
    type: video
    url: https://vimeo.com/364712933
  author:
  - first: Nelson F.
    full: Nelson F. Liu
    id: nelson-f-liu
    last: Liu
  - first: Roy
    full: Roy Schwartz
    id: roy-schwartz
    last: Schwartz
  - first: Noah A.
    full: Noah A. Smith
    id: noah-a-smith
    last: Smith
  author_string: Nelson F. Liu, Roy Schwartz, Noah A. Smith
  bibkey: liu-etal-2019-inoculation
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1225
  month: June
  page_first: '2171'
  page_last: '2179'
  pages: "2171\u20132179"
  paper_id: '225'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1225.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1225.jpg
  title: 'Inoculation by Fine-Tuning: A Method for Analyzing Challenge Datasets'
  title_html: 'Inoculation by Fine-Tuning: A Method for Analyzing Challenge Datasets'
  url: https://www.aclweb.org/anthology/N19-1225
  year: '2019'
N19-1226:
  abstract: In this paper, we introduce an embedding model, named CapsE, exploring
    a capsule network to model relationship triples (subject, relation, object). Our
    CapsE represents each triple as a 3-column matrix where each column vector represents
    the embedding of an element in the triple. This 3-column matrix is then fed to
    a convolution layer where multiple filters are operated to generate different
    feature maps. These feature maps are reconstructed into corresponding capsules
    which are then routed to another capsule to produce a continuous vector. The length
    of this vector is used to measure the plausibility score of the triple. Our proposed
    CapsE obtains better performance than previous state-of-the-art embedding models
    for knowledge graph completion on two benchmark datasets WN18RR and FB15k-237,
    and outperforms strong search personalization baselines on SEARCH17.
  address: Minneapolis, Minnesota
  author:
  - first: Dai Quoc
    full: Dai Quoc Nguyen
    id: dai-quoc-nguyen
    last: Nguyen
  - first: Thanh
    full: Thanh Vu
    id: thanh-vu
    last: Vu
  - first: Tu Dinh
    full: Tu Dinh Nguyen
    id: tu-dinh-nguyen
    last: Nguyen
  - first: Dat Quoc
    full: Dat Quoc Nguyen
    id: dat-quoc-nguyen
    last: Nguyen
  - first: Dinh
    full: Dinh Phung
    id: dinh-phung
    last: Phung
  author_string: Dai Quoc Nguyen, Thanh Vu, Tu Dinh Nguyen, Dat Quoc Nguyen, Dinh
    Phung
  bibkey: nguyen-etal-2019-capsule
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1226
  month: June
  page_first: '2180'
  page_last: '2189'
  pages: "2180\u20132189"
  paper_id: '226'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1226.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1226.jpg
  title: A Capsule Network-based Embedding Model for Knowledge Graph Completion and
    Search Personalization
  title_html: A Capsule Network-based Embedding Model for Knowledge Graph Completion
    and Search Personalization
  url: https://www.aclweb.org/anthology/N19-1226
  year: '2019'
N19-1227:
  abstract: For many structured learning tasks, the data annotation process is complex
    and costly. Existing annotation schemes usually aim at acquiring completely annotated
    structures, under the common perception that partial structures are of low quality
    and could hurt the learning process. This paper questions this common perception,
    motivated by the fact that structures consist of interdependent sets of variables.
    Thus, given a fixed budget, partly annotating each structure may provide the same
    level of supervision, while allowing for more structures to be annotated. We provide
    an information theoretic formulation for this perspective and use it, in the context
    of three diverse structured learning tasks, to show that learning from partial
    structures can sometimes outperform learning from complete ones. Our findings
    may provide important insights into structured data annotation schemes and could
    support progress in learning protocols for structured tasks.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1227.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1227.Supplementary.pdf
  author:
  - first: Qiang
    full: Qiang Ning
    id: qiang-ning
    last: Ning
  - first: Hangfeng
    full: Hangfeng He
    id: hangfeng-he
    last: He
  - first: Chuchu
    full: Chuchu Fan
    id: chuchu-fan
    last: Fan
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Qiang Ning, Hangfeng He, Chuchu Fan, Dan Roth
  bibkey: ning-etal-2019-partial
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1227
  month: June
  page_first: '2190'
  page_last: '2200'
  pages: "2190\u20132200"
  paper_id: '227'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1227.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1227.jpg
  title: "Partial Or Complete, That\u2019s The Question"
  title_html: "Partial Or Complete, That\u2019s The Question"
  url: https://www.aclweb.org/anthology/N19-1227
  year: '2019'
N19-1228:
  abstract: In Community-based Question Answering system(CQA), Answer Selection(AS)
    is a critical task, which focuses on finding a suitable answer within a list of
    candidate answers. For neural network models, the key issue is how to model the
    representations of QA text pairs and calculate the interactions between them.
    We propose a Sequential Attention with Keyword Mask model(SAKM) for CQA to imitate
    human reading behavior. Question and answer text regard each other as context
    within keyword-mask attention when encoding the representations, and repeat multiple
    times(hops) in a sequential style. So the QA pairs capture features and information
    from both question text and answer text, interacting and improving vector representations
    iteratively through hops. The flexibility of the model allows to extract meaningful
    keywords from the sentences and enhance diverse mutual information. We perform
    on answer selection tasks and multi-level answer ranking tasks. Experiment results
    demonstrate the superiority of our proposed model on community-based QA datasets.
  address: Minneapolis, Minnesota
  author:
  - first: Jianxin
    full: Jianxin Yang
    id: jianxin-yang
    last: Yang
  - first: Wenge
    full: Wenge Rong
    id: wenge-rong
    last: Rong
  - first: Libin
    full: Libin Shi
    id: libin-shi
    last: Shi
  - first: Zhang
    full: Zhang Xiong
    id: zhang-xiong
    last: Xiong
  author_string: Jianxin Yang, Wenge Rong, Libin Shi, Zhang Xiong
  bibkey: yang-etal-2019-sequential
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1228
  month: June
  page_first: '2201'
  page_last: '2211'
  pages: "2201\u20132211"
  paper_id: '228'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1228.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1228.jpg
  title: Sequential Attention with Keyword Mask Model for Community-based Question
    Answering
  title_html: <span class="acl-fixed-case">S</span>equential <span class="acl-fixed-case">A</span>ttention
    with <span class="acl-fixed-case">K</span>eyword <span class="acl-fixed-case">M</span>ask
    <span class="acl-fixed-case">M</span>odel for <span class="acl-fixed-case">C</span>ommunity-based
    <span class="acl-fixed-case">Q</span>uestion <span class="acl-fixed-case">A</span>nswering
  url: https://www.aclweb.org/anthology/N19-1228
  year: '2019'
N19-1229:
  abstract: "This paper explores the problem of ranking short social media posts with\
    \ respect to user queries using neural networks. Instead of starting with a complex\
    \ architecture, we proceed from the bottom up and examine the effectiveness of\
    \ a simple, word-level Siamese architecture augmented with attention-based mechanisms\
    \ for capturing semantic \u201Csoft\u201D matches between query and post tokens.\
    \ Extensive experiments on datasets from the TREC Microblog Tracks show that our\
    \ simple models not only achieve better effectiveness than existing approaches\
    \ that are far more complex or exploit a more diverse set of relevance signals,\
    \ but are also much faster."
  address: Minneapolis, Minnesota
  author:
  - first: Peng
    full: Peng Shi
    id: peng-shi
    last: Shi
  - first: Jinfeng
    full: Jinfeng Rao
    id: jinfeng-rao
    last: Rao
  - first: Jimmy
    full: Jimmy Lin
    id: jimmy-lin
    last: Lin
  author_string: Peng Shi, Jinfeng Rao, Jimmy Lin
  bibkey: shi-etal-2019-simple
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1229
  month: June
  page_first: '2212'
  page_last: '2217'
  pages: "2212\u20132217"
  paper_id: '229'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1229.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1229.jpg
  title: Simple Attention-Based Representation Learning for Ranking Short Social Media
    Posts
  title_html: Simple Attention-Based Representation Learning for Ranking Short Social
    Media Posts
  url: https://www.aclweb.org/anthology/N19-1229
  year: '2019'
N19-1230:
  abstract: The recently released FEVER dataset provided benchmark results on a fact-checking
    task in which given a factual claim, the system must extract textual evidence
    (sets of sentences from Wikipedia pages) that support or refute the claim. In
    this paper, we present a completely task-agnostic pipelined system, AttentiveChecker,
    consisting of three homogeneous Bi-Directional Attention Flow (BIDAF) networks,
    which are multi-layer hierarchical networks that represent the context at different
    levels of granularity. We are the first to apply to this task a bi-directional
    attention flow mechanism to obtain a query-aware context representation without
    early summarization. AttentiveChecker can be used to perform document retrieval,
    sentence selection, and claim verification. Experiments on the FEVER dataset indicate
    that AttentiveChecker is able to achieve the state-of-the-art results on the FEVER
    test set.
  address: Minneapolis, Minnesota
  author:
  - first: Santosh
    full: Santosh Tokala
    id: santosh-tokala
    last: Tokala
  - first: Vishal
    full: Vishal G
    id: vishal-g
    last: G
  - first: Avirup
    full: Avirup Saha
    id: avirup-saha
    last: Saha
  - first: Niloy
    full: Niloy Ganguly
    id: niloy-ganguly
    last: Ganguly
  author_string: Santosh Tokala, Vishal G, Avirup Saha, Niloy Ganguly
  bibkey: tokala-etal-2019-attentivechecker
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1230
  month: June
  page_first: '2218'
  page_last: '2222'
  pages: "2218\u20132222"
  paper_id: '230'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1230.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1230.jpg
  title: 'AttentiveChecker: A Bi-Directional Attention Flow Mechanism for Fact Verification'
  title_html: '<span class="acl-fixed-case">A</span>ttentive<span class="acl-fixed-case">C</span>hecker:
    A Bi-Directional Attention Flow Mechanism for Fact Verification'
  url: https://www.aclweb.org/anthology/N19-1230
  year: '2019'
N19-1231:
  abstract: "Scholars in inter-disciplinary fields like the Digital Humanities are\
    \ increasingly interested in semantic annotation of specialized corpora. Yet,\
    \ under-resourced languages, imperfect or noisily structured data, and user-specific\
    \ classification tasks make it difficult to meet their needs using off-the-shelf\
    \ models. Manual annotation of large corpora from scratch, meanwhile, can be prohibitively\
    \ expensive. Thus, we propose an active learning solution for named entity recognition,\
    \ attempting to maximize a custom model\u2019s improvement per additional unit\
    \ of manual annotation. Our system robustly handles any domain or user-defined\
    \ label set and requires no external resources, enabling quality named entity\
    \ recognition for Humanities corpora where such resources are not available. Evaluating\
    \ on typologically disparate languages and datasets, we reduce required annotation\
    \ by 20-60% and greatly outperform a competitive active learning baseline."
  address: Minneapolis, Minnesota
  author:
  - first: Alexander
    full: Alexander Erdmann
    id: alexander-erdmann
    last: Erdmann
  - first: David Joseph
    full: David Joseph Wrisley
    id: david-joseph-wrisley
    last: Wrisley
  - first: Benjamin
    full: Benjamin Allen
    id: benjamin-allen
    last: Allen
  - first: Christopher
    full: Christopher Brown
    id: christopher-brown
    last: Brown
  - first: Sophie
    full: "Sophie Cohen-Bod\xE9n\xE8s"
    id: sophie-cohen-bodenes
    last: "Cohen-Bod\xE9n\xE8s"
  - first: Micha
    full: Micha Elsner
    id: micha-elsner
    last: Elsner
  - first: Yukun
    full: Yukun Feng
    id: yukun-feng
    last: Feng
  - first: Brian
    full: Brian Joseph
    id: brian-joseph
    last: Joseph
  - first: "B\xE9atrice"
    full: "B\xE9atrice Joyeux-Prunel"
    id: beatrice-joyeux-prunel
    last: Joyeux-Prunel
  - first: Marie-Catherine
    full: Marie-Catherine de Marneffe
    id: marie-catherine-de-marneffe
    last: de Marneffe
  author_string: "Alexander Erdmann, David Joseph Wrisley, Benjamin Allen, Christopher\
    \ Brown, Sophie Cohen-Bod\xE9n\xE8s, Micha Elsner, Yukun Feng, Brian Joseph, B\xE9\
    atrice Joyeux-Prunel, Marie-Catherine de Marneffe"
  bibkey: erdmann-etal-2019-practical
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1231
  month: June
  page_first: '2223'
  page_last: '2234'
  pages: "2223\u20132234"
  paper_id: '231'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1231.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1231.jpg
  title: Practical, Efficient, and Customizable Active Learning for Named Entity Recognition
    in the Digital Humanities
  title_html: Practical, Efficient, and Customizable Active Learning for Named Entity
    Recognition in the Digital Humanities
  url: https://www.aclweb.org/anthology/N19-1231
  year: '2019'
N19-1232:
  abstract: Learning to hash via generative model has become a powerful paradigm for
    fast similarity search in documents retrieval. To get binary representation (i.e.,
    hash codes), the discrete distribution prior (i.e., Bernoulli Distribution) is
    applied to train the variational autoencoder (VAE). However, the discrete stochastic
    layer is usually incompatible with the backpropagation in the training stage,
    and thus causes a gradient flow problem because of non-differentiable operators.
    The reparameterization trick of sampling from a discrete distribution usually
    inc non-differentiable operators. In this paper, we propose a method, Doc2hash,
    that solves the gradient flow problem of the discrete stochastic layer by using
    continuous relaxation on priors, and trains the generative model in an end-to-end
    manner to generate hash codes. In qualitative and quantitative experiments, we
    show the proposed model outperforms other state-of-art methods.
  address: Minneapolis, Minnesota
  author:
  - first: Yifei
    full: Yifei Zhang
    id: yifei-zhang
    last: Zhang
  - first: Hao
    full: Hao Zhu
    id: hao-zhu
    last: Zhu
  author_string: Yifei Zhang, Hao Zhu
  bibkey: zhang-zhu-2019-doc2hash
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1232
  month: June
  page_first: '2235'
  page_last: '2240'
  pages: "2235\u20132240"
  paper_id: '232'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1232.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1232.jpg
  title: 'Doc2hash: Learning Discrete Latent variables for Documents Retrieval'
  title_html: '<span class="acl-fixed-case">D</span>oc2hash: Learning Discrete Latent
    variables for Documents Retrieval'
  url: https://www.aclweb.org/anthology/N19-1232
  year: '2019'
N19-1233:
  abstract: "Generative Adversarial Networks (GANs) are a promising approach for text\
    \ generation that, unlike traditional language models (LM), does not suffer from\
    \ the problem of \u201Cexposure bias\u201D. However, A major hurdle for understanding\
    \ the potential of GANs for text generation is the lack of a clear evaluation\
    \ metric. In this work, we propose to approximate the distribution of text generated\
    \ by a GAN, which permits evaluating them with traditional probability-based LM\
    \ metrics. We apply our approximation procedure on several GAN-based models and\
    \ show that they currently perform substantially worse than state-of-the-art LMs.\
    \ Our evaluation procedure promotes better understanding of the relation between\
    \ GANs and LMs, and can accelerate progress in GAN-based text generation."
  address: Minneapolis, Minnesota
  author:
  - first: Guy
    full: Guy Tevet
    id: guy-tevet
    last: Tevet
  - first: Gavriel
    full: Gavriel Habib
    id: gavriel-habib
    last: Habib
  - first: Vered
    full: Vered Shwartz
    id: vered-shwartz
    last: Shwartz
  - first: Jonathan
    full: Jonathan Berant
    id: jonathan-berant
    last: Berant
  author_string: Guy Tevet, Gavriel Habib, Vered Shwartz, Jonathan Berant
  bibkey: tevet-etal-2019-evaluating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1233
  month: June
  page_first: '2241'
  page_last: '2247'
  pages: "2241\u20132247"
  paper_id: '233'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1233.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1233.jpg
  title: Evaluating Text GANs as Language Models
  title_html: Evaluating Text <span class="acl-fixed-case">GAN</span>s as Language
    Models
  url: https://www.aclweb.org/anthology/N19-1233
  year: '2019'
N19-1234:
  abstract: Text generation with generative adversarial networks (GANs) can be divided
    into the text-based and code-based categories according to the type of signals
    used for discrimination. In this work, we introduce a novel text-based approach
    called Soft-GAN to effectively exploit GAN setup for text generation. We demonstrate
    how autoencoders (AEs) can be used for providing a continuous representation of
    sentences, which we will refer to as soft-text. This soft representation will
    be used in GAN discrimination to synthesize similar soft-texts. We also propose
    hybrid latent code and text-based GAN (LATEXT-GAN) approaches with one or more
    discriminators, in which a combination of the latent code and the soft-text is
    used for GAN discriminations. We perform a number of subjective and objective
    experiments on two well-known datasets (SNLI and Image COCO) to validate our techniques.
    We discuss the results using several evaluation metrics and show that the proposed
    techniques outperform the traditional GAN-based text-generation methods.
  address: Minneapolis, Minnesota
  author:
  - first: Md. Akmal
    full: Md. Akmal Haidar
    id: md-akmal-haidar
    last: Haidar
  - first: Mehdi
    full: Mehdi Rezagholizadeh
    id: mehdi-rezagholizadeh
    last: Rezagholizadeh
  - first: Alan Do
    full: Alan Do Omri
    id: alan-do-omri
    last: Omri
  - first: Ahmad
    full: Ahmad Rashid
    id: ahmad-rashid
    last: Rashid
  author_string: Md. Akmal Haidar, Mehdi Rezagholizadeh, Alan Do Omri, Ahmad Rashid
  bibkey: haidar-etal-2019-latent
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1234
  month: June
  page_first: '2248'
  page_last: '2258'
  pages: "2248\u20132258"
  paper_id: '234'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1234.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1234.jpg
  title: Latent Code and Text-based Generative Adversarial Networks for Soft-text
    Generation
  title_html: Latent Code and Text-based Generative Adversarial Networks for Soft-text
    Generation
  url: https://www.aclweb.org/anthology/N19-1234
  year: '2019'
N19-1235:
  abstract: We propose neural models to generate high-quality text from structured
    representations based on Minimal Recursion Semantics (MRS). MRS is a rich semantic
    representation that encodes more precise semantic detail than other representations
    such as Abstract Meaning Representation (AMR). We show that a sequence-to-sequence
    model that maps a linearization of Dependency MRS, a graph-based representation
    of MRS, to text can achieve a BLEU score of 66.11 when trained on gold data. The
    performance of the model can be improved further using a high-precision, broad
    coverage grammar-based parser to generate a large silver training corpus, achieving
    a final BLEU score of 77.17 on the full test set, and 83.37 on the subset of test
    data most closely matching the silver data domain. Our results suggest that MRS-based
    representations are a good choice for applications that need both structured semantics
    and the ability to produce natural language text as output.
  address: Minneapolis, Minnesota
  author:
  - first: Valerie
    full: Valerie Hajdik
    id: valerie-hajdik
    last: Hajdik
  - first: Jan
    full: Jan Buys
    id: jan-buys
    last: Buys
  - first: Michael Wayne
    full: Michael Wayne Goodman
    id: michael-wayne-goodman
    last: Goodman
  - first: Emily M.
    full: Emily M. Bender
    id: emily-m-bender
    last: Bender
  author_string: Valerie Hajdik, Jan Buys, Michael Wayne Goodman, Emily M. Bender
  bibkey: hajdik-etal-2019-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1235
  month: June
  page_first: '2259'
  page_last: '2266'
  pages: "2259\u20132266"
  paper_id: '235'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1235.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1235.jpg
  title: Neural Text Generation from Rich Semantic Representations
  title_html: Neural Text Generation from Rich Semantic Representations
  url: https://www.aclweb.org/anthology/N19-1235
  year: '2019'
N19-1236:
  abstract: "Data-to-text generation can be conceptually divided into two parts: ordering\
    \ and structuring the information (planning), and generating fluent language describing\
    \ the information (realization). Modern neural generation systems conflate these\
    \ two steps into a single end-to-end differentiable system. We propose to split\
    \ the generation process into a symbolic text-planning stage that is faithful\
    \ to the input, followed by a neural generation stage that focuses only on realization.\
    \ For training a plan-to-text generator, we present a method for matching reference\
    \ texts to their corresponding text plans. For inference time, we describe a method\
    \ for selecting high-quality text plans for new inputs. We implement and evaluate\
    \ our approach on the WebNLG benchmark. Our results demonstrate that decoupling\
    \ text planning from neural realization indeed improves the system\u2019s reliability\
    \ and adequacy while maintaining fluent output. We observe improvements both in\
    \ BLEU scores and in manual evaluations. Another benefit of our approach is the\
    \ ability to output diverse realizations of the same input, paving the way to\
    \ explicit control over the generated text structure."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1236.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1236.Supplementary.pdf
  author:
  - first: Amit
    full: Amit Moryossef
    id: amit-moryossef
    last: Moryossef
  - first: Yoav
    full: Yoav Goldberg
    id: yoav-goldberg
    last: Goldberg
  - first: Ido
    full: Ido Dagan
    id: ido-dagan
    last: Dagan
  author_string: Amit Moryossef, Yoav Goldberg, Ido Dagan
  bibkey: moryossef-etal-2019-step
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1236
  month: June
  page_first: '2267'
  page_last: '2277'
  pages: "2267\u20132277"
  paper_id: '236'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1236.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1236.jpg
  title: 'Step-by-Step: Separating Planning from Realization in Neural Data-to-Text
    Generation'
  title_html: '<span class="acl-fixed-case">S</span>tep-by-Step: <span class="acl-fixed-case">S</span>eparating
    Planning from Realization in Neural Data-to-Text Generation'
  url: https://www.aclweb.org/anthology/N19-1236
  year: '2019'
N19-1237:
  abstract: Recent approaches to question generation have used modifications to a
    Seq2Seq architecture inspired by advances in machine translation. Models are trained
    using teacher forcing to optimise only the one-step-ahead prediction. However,
    at test time, the model is asked to generate a whole sequence, causing errors
    to propagate through the generation process (exposure bias). A number of authors
    have suggested that optimising for rewards less tightly coupled to the training
    data might counter this mismatch. We therefore optimise directly for various objectives
    beyond simply replicating the ground truth questions, including a novel approach
    using an adversarial discriminator that seeks to generate questions that are indistinguishable
    from real examples. We confirm that training with policy gradient methods leads
    to increases in the metrics used as rewards. We perform a human evaluation, and
    show that although these metrics have previously been assumed to be good proxies
    for question quality, they are poorly aligned with human judgement and the model
    simply learns to exploit the weaknesses of the reward source.
  address: Minneapolis, Minnesota
  author:
  - first: Tom
    full: Tom Hosking
    id: tom-hosking
    last: Hosking
  - first: Sebastian
    full: Sebastian Riedel
    id: sebastian-riedel
    last: Riedel
  author_string: Tom Hosking, Sebastian Riedel
  bibkey: hosking-riedel-2019-evaluating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1237
  month: June
  page_first: '2278'
  page_last: '2283'
  pages: "2278\u20132283"
  paper_id: '237'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1237.pdf
  publisher: Association for Computational Linguistics
  revision:
  - id: '1'
    url: https://www.aclweb.org/anthology/N19-1237v1.pdf
    value: N19-1237v1
  - explanation: No description of the changes were recorded.
    id: '2'
    url: https://www.aclweb.org/anthology/N19-1237v2.pdf
    value: N19-1237v2
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1237.jpg
  title: Evaluating Rewards for Question Generation Models
  title_html: Evaluating Rewards for Question Generation Models
  url: https://www.aclweb.org/anthology/N19-1237
  year: '2019'
N19-1238:
  abstract: Generating texts which express complex ideas spanning multiple sentences
    requires a structured representation of their content (document plan), but these
    representations are prohibitively expensive to manually produce. In this work,
    we address the problem of generating coherent multi-sentence texts from the output
    of an information extraction system, and in particular a knowledge graph. Graphical
    knowledge representations are ubiquitous in computing, but pose a significant
    challenge for text generation techniques due to their non-hierarchical nature,
    collapsing of long-distance dependencies, and structural variety. We introduce
    a novel graph transforming encoder which can leverage the relational structure
    of such knowledge graphs without imposing linearization or hierarchical constraints.
    Incorporated into an encoder-decoder setup, we provide an end-to-end trainable
    system for graph-to-text generation that we apply to the domain of scientific
    text. Automatic and human evaluations show that our technique produces more informative
    texts which exhibit better document structure than competitive encoder-decoder
    methods.
  address: Minneapolis, Minnesota
  author:
  - first: Rik
    full: Rik Koncel-Kedziorski
    id: rik-koncel-kedziorski
    last: Koncel-Kedziorski
  - first: Dhanush
    full: Dhanush Bekal
    id: dhanush-bekal
    last: Bekal
  - first: Yi
    full: Yi Luan
    id: yi-luan
    last: Luan
  - first: Mirella
    full: Mirella Lapata
    id: mirella-lapata
    last: Lapata
  - first: Hannaneh
    full: Hannaneh Hajishirzi
    id: hannaneh-hajishirzi
    last: Hajishirzi
  author_string: Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, Hannaneh
    Hajishirzi
  bibkey: koncel-kedziorski-etal-2019-text
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1238
  month: June
  page_first: '2284'
  page_last: '2293'
  pages: "2284\u20132293"
  paper_id: '238'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1238.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1238.jpg
  title: Text Generation from Knowledge Graphs with Graph Transformers
  title_html: <span class="acl-fixed-case">T</span>ext <span class="acl-fixed-case">G</span>eneration
    from <span class="acl-fixed-case">K</span>nowledge <span class="acl-fixed-case">G</span>raphs
    with <span class="acl-fixed-case">G</span>raph <span class="acl-fixed-case">T</span>ransformers
  url: https://www.aclweb.org/anthology/N19-1238
  year: '2019'
N19-1239:
  abstract: Open Information Extraction (OpenIE) extracts meaningful structured tuples
    from free-form text. Most previous work on OpenIE considers extracting data from
    one sentence at a time. We describe NeurON, a system for extracting tuples from
    question-answer pairs. One of the main motivations for NeurON is to be able to
    extend knowledge bases in a way that considers precisely the information that
    users care about. NeurON addresses several challenges. First, an answer text is
    often hard to understand without knowing the question, and second, relevant information
    can span multiple sentences. To address these, NeurON formulates extraction as
    a multi-source sequence-to-sequence learning task, wherein it combines distributed
    representations of a question and an answer to generate knowledge facts. We describe
    experiments on two real-world datasets that demonstrate that NeurON can find a
    significant number of new and interesting facts to extend a knowledge base compared
    to state-of-the-art OpenIE methods.
  address: Minneapolis, Minnesota
  author:
  - first: Nikita
    full: Nikita Bhutani
    id: nikita-bhutani
    last: Bhutani
  - first: Yoshihiko
    full: Yoshihiko Suhara
    id: yoshihiko-suhara
    last: Suhara
  - first: Wang-Chiew
    full: Wang-Chiew Tan
    id: wang-chiew-tan
    last: Tan
  - first: Alon
    full: Alon Halevy
    id: alon-halevy
    last: Halevy
  - first: H. V.
    full: H. V. Jagadish
    id: h-v-jagadish
    last: Jagadish
  author_string: Nikita Bhutani, Yoshihiko Suhara, Wang-Chiew Tan, Alon Halevy, H.
    V. Jagadish
  bibkey: bhutani-etal-2019-open
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1239
  month: June
  page_first: '2294'
  page_last: '2305'
  pages: "2294\u20132305"
  paper_id: '239'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1239.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1239.jpg
  title: Open Information Extraction from Question-Answer Pairs
  title_html: Open Information Extraction from Question-Answer Pairs
  url: https://www.aclweb.org/anthology/N19-1239
  year: '2019'
N19-1240:
  abstract: Most research in reading comprehension has focused on answering questions
    based on individual documents or even single paragraphs. We introduce a neural
    model which integrates and reasons relying on information spread within documents
    and across multiple documents. We frame it as an inference problem on a graph.
    Mentions of entities are nodes of this graph while edges encode relations between
    different mentions (e.g., within- and cross-document co-reference). Graph convolutional
    networks (GCNs) are applied to these graphs and trained to perform multi-step
    reasoning. Our Entity-GCN method is scalable and compact, and it achieves state-of-the-art
    results on a multi-document question answering dataset, WikiHop (Welbl et al.,
    2018).
  address: Minneapolis, Minnesota
  author:
  - first: Nicola
    full: Nicola De Cao
    id: nicola-de-cao
    last: De Cao
  - first: Wilker
    full: Wilker Aziz
    id: wilker-aziz
    last: Aziz
  - first: Ivan
    full: Ivan Titov
    id: ivan-titov
    last: Titov
  author_string: Nicola De Cao, Wilker Aziz, Ivan Titov
  bibkey: de-cao-etal-2019-question
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1240
  month: June
  page_first: '2306'
  page_last: '2317'
  pages: "2306\u20132317"
  paper_id: '240'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1240.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1240.jpg
  title: Question Answering by Reasoning Across Documents with Graph Convolutional
    Networks
  title_html: Question Answering by Reasoning Across Documents with Graph Convolutional
    Networks
  url: https://www.aclweb.org/anthology/N19-1240
  year: '2019'
N19-1241:
  abstract: "We compare three new datasets for question answering: SQuAD 2.0, QuAC,\
    \ and CoQA, along several of their new features: (1) unanswerable questions, (2)\
    \ multi-turn interactions, and (3) abstractive answers.We show that the datasets\
    \ provide complementary coverage of the first two aspects, but weak coverage of\
    \ the third.Because of the datasets\u2019 structural similarity, a single extractive\
    \ model can be easily adapted to any of the datasets and we show improved baseline\
    \ results on both SQuAD 2.0 and CoQA. Despite the similarity, models trained on\
    \ one dataset are ineffective on another dataset, but we find moderate performance\
    \ improvement through pretraining. To encourage cross-evaluation, we release code\
    \ for conversion between datasets."
  address: Minneapolis, Minnesota
  author:
  - first: Mark
    full: Mark Yatskar
    id: mark-yatskar
    last: Yatskar
  author_string: Mark Yatskar
  bibkey: yatskar-2019-qualitative
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1241
  month: June
  page_first: '2318'
  page_last: '2323'
  pages: "2318\u20132323"
  paper_id: '241'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1241.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1241.jpg
  title: A Qualitative Comparison of CoQA, SQuAD 2.0 and QuAC
  title_html: A Qualitative Comparison of <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">QA</span>, <span class="acl-fixed-case">SQ</span>u<span
    class="acl-fixed-case">AD</span> 2.0 and <span class="acl-fixed-case">Q</span>u<span
    class="acl-fixed-case">AC</span>
  url: https://www.aclweb.org/anthology/N19-1241
  year: '2019'
N19-1242:
  abstract: Question-answering plays an important role in e-commerce as it allows
    potential customers to actively seek crucial information about products or services
    to help their purchase decision making. Inspired by the recent success of machine
    reading comprehension (MRC) on formal documents, this paper explores the potential
    of turning customer reviews into a large source of knowledge that can be exploited
    to answer user questions. We call this problem Review Reading Comprehension (RRC).
    To the best of our knowledge, no existing work has been done on RRC. In this work,
    we first build an RRC dataset called ReviewRC based on a popular benchmark for
    aspect-based sentiment analysis. Since ReviewRC has limited training examples
    for RRC (and also for aspect-based sentiment analysis), we then explore a novel
    post-training approach on the popular language model BERT to enhance the performance
    of fine-tuning of BERT for RRC. To show the generality of the approach, the proposed
    post-training is also applied to some other review-based tasks such as aspect
    extraction and aspect sentiment classification in aspect-based sentiment analysis.
    Experimental results demonstrate that the proposed post-training is highly effective.
  address: Minneapolis, Minnesota
  author:
  - first: Hu
    full: Hu Xu
    id: hu-xu
    last: Xu
  - first: Bing
    full: Bing Liu
    id: bing-liu
    last: Liu
  - first: Lei
    full: Lei Shu
    id: lei-shu
    last: Shu
  - first: Philip
    full: Philip Yu
    id: philip-s-yu
    last: Yu
  author_string: Hu Xu, Bing Liu, Lei Shu, Philip Yu
  bibkey: xu-etal-2019-bert
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1242
  month: June
  page_first: '2324'
  page_last: '2335'
  pages: "2324\u20132335"
  paper_id: '242'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1242.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1242.jpg
  title: BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment
    Analysis
  title_html: <span class="acl-fixed-case">BERT</span> Post-Training for Review Reading
    Comprehension and Aspect-based Sentiment Analysis
  url: https://www.aclweb.org/anthology/N19-1242
  year: '2019'
N19-1243:
  abstract: Short texts challenge NLP tasks such as named entity recognition, disambiguation,
    linking and relation inference because they do not provide sufficient context
    or are partially malformed (e.g. wrt. capitalization, long tail entities, implicit
    relations). In this work, we present the Falcon approach which effectively maps
    entities and relations within a short text to its mentions of a background knowledge
    graph. Falcon overcomes the challenges of short text using a light-weight linguistic
    approach relying on a background knowledge graph. Falcon performs joint entity
    and relation linking of a short text by leveraging several fundamental principles
    of English morphology (e.g. compounding, headword identification) and utilizes
    an extended knowledge graph created by merging entities and relations from various
    knowledge sources. It uses the context of entities for finding relations and does
    not require training data. Our empirical study using several standard benchmarks
    and datasets show that Falcon significantly outperforms state-of-the-art entity
    and relation linking for short text query inventories.
  address: Minneapolis, Minnesota
  author:
  - first: Ahmad
    full: Ahmad Sakor
    id: ahmad-sakor
    last: Sakor
  - first: Isaiah
    full: "Isaiah Onando Mulang\u2019"
    id: isaiah-onando-mulang1
    last: "Onando Mulang\u2019"
  - first: Kuldeep
    full: Kuldeep Singh
    id: kuldeep-singh
    last: Singh
  - first: Saeedeh
    full: Saeedeh Shekarpour
    id: saeedeh-shekarpour
    last: Shekarpour
  - first: Maria
    full: Maria Esther Vidal
    id: maria-esther-vidal
    last: Esther Vidal
  - first: Jens
    full: Jens Lehmann
    id: jens-lehmann
    last: Lehmann
  - first: "S\xF6ren"
    full: "S\xF6ren Auer"
    id: soren-auer
    last: Auer
  author_string: "Ahmad Sakor, Isaiah Onando Mulang\u2019, Kuldeep Singh, Saeedeh\
    \ Shekarpour, Maria Esther Vidal, Jens Lehmann, S\xF6ren Auer"
  bibkey: sakor-etal-2019-old
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1243
  month: June
  page_first: '2336'
  page_last: '2346'
  pages: "2336\u20132346"
  paper_id: '243'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1243.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1243.jpg
  title: 'Old is Gold: Linguistic Driven Approach for Entity and Relation Linking
    of Short Text'
  title_html: 'Old is Gold: Linguistic Driven Approach for Entity and Relation Linking
    of Short Text'
  url: https://www.aclweb.org/anthology/N19-1243
  year: '2019'
N19-1244:
  abstract: Our goal is procedural text comprehension, namely tracking how the properties
    of entities (e.g., their location) change with time given a procedural text (e.g.,
    a paragraph about photosynthesis, a recipe). This task is challenging as the world
    is changing throughout the text, and despite recent advances, current systems
    still struggle with this task. Our approach is to leverage the fact that, for
    many procedural texts, multiple independent descriptions are readily available,
    and that predictions from them should be consistent (label consistency). We present
    a new learning framework that leverages label consistency during training, allowing
    consistency bias to be built into the model. Evaluation on a standard benchmark
    dataset for procedural text, ProPara (Dalvi et al., 2018), shows that our approach
    significantly improves prediction performance (F1) over prior state-of-the-art
    systems.
  address: Minneapolis, Minnesota
  author:
  - first: Xinya
    full: Xinya Du
    id: xinya-du
    last: Du
  - first: Bhavana
    full: Bhavana Dalvi
    id: bhavana-dalvi
    last: Dalvi
  - first: Niket
    full: Niket Tandon
    id: niket-tandon
    last: Tandon
  - first: Antoine
    full: Antoine Bosselut
    id: antoine-bosselut
    last: Bosselut
  - first: Wen-tau
    full: Wen-tau Yih
    id: wen-tau-yih
    last: Yih
  - first: Peter
    full: Peter Clark
    id: peter-clark
    last: Clark
  - first: Claire
    full: Claire Cardie
    id: claire-cardie
    last: Cardie
  author_string: Xinya Du, Bhavana Dalvi, Niket Tandon, Antoine Bosselut, Wen-tau
    Yih, Peter Clark, Claire Cardie
  bibkey: du-etal-2019-consistent
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1244
  month: June
  page_first: '2347'
  page_last: '2356'
  pages: "2347\u20132356"
  paper_id: '244'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1244.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1244.jpg
  title: Be Consistent! Improving Procedural Text Comprehension using Label Consistency
  title_html: Be Consistent! Improving Procedural Text Comprehension using Label Consistency
  url: https://www.aclweb.org/anthology/N19-1244
  year: '2019'
N19-1245:
  abstract: We introduce a large-scale dataset of math word problems and an interpretable
    neural math problem solver by learning to map problems to their operation programs.
    Due to annotation challenges, current datasets in this domain have been either
    relatively small in scale or did not offer precise operational annotations over
    diverse problem types. We introduce a new representation language to model operation
    programs corresponding to each math problem that aim to improve both the performance
    and the interpretability of the learned models. Using this representation language,
    we significantly enhance the AQUA-RAT dataset with fully-specified operational
    programs. We additionally introduce a neural sequence-to-program model with automatic
    problem categorization. Our experiments show improvements over competitive baselines
    in our dataset as well as the AQUA-RAT dataset. The results are still lower than
    human performance indicating that the dataset poses new challenges for future
    research. Our dataset is available at https://math-qa.github.io/math-QA/
  address: Minneapolis, Minnesota
  author:
  - first: Aida
    full: Aida Amini
    id: aida-amini
    last: Amini
  - first: Saadia
    full: Saadia Gabriel
    id: saadia-gabriel
    last: Gabriel
  - first: Shanchuan
    full: Shanchuan Lin
    id: shanchuan-lin
    last: Lin
  - first: Rik
    full: Rik Koncel-Kedziorski
    id: rik-koncel-kedziorski
    last: Koncel-Kedziorski
  - first: Yejin
    full: Yejin Choi
    id: yejin-choi
    last: Choi
  - first: Hannaneh
    full: Hannaneh Hajishirzi
    id: hannaneh-hajishirzi
    last: Hajishirzi
  author_string: Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski,
    Yejin Choi, Hannaneh Hajishirzi
  bibkey: amini-etal-2019-mathqa
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1245
  month: June
  page_first: '2357'
  page_last: '2367'
  pages: "2357\u20132367"
  paper_id: '245'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1245.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1245.jpg
  title: 'MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based
    Formalisms'
  title_html: '<span class="acl-fixed-case">M</span>ath<span class="acl-fixed-case">QA</span>:
    Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms'
  url: https://www.aclweb.org/anthology/N19-1245
  year: '2019'
N19-1246:
  abstract: Reading comprehension has recently seen rapid progress, with systems matching
    humans on the most popular datasets for the task. However, a large body of work
    has highlighted the brittleness of these systems, showing that there is much work
    left to be done. We introduce a new reading comprehension benchmark, DROP, which
    requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced,
    adversarially-created, 55k-question benchmark, a system must resolve references
    in a question, perhaps to multiple input positions, and perform discrete operations
    over them (such as addition, counting, or sorting). These operations require a
    much more comprehensive understanding of the content of paragraphs, as they remove
    the paraphrase-and-entity-typing shortcuts available in prior datasets. We apply
    state-of-the-art methods from both the reading comprehension and semantic parsing
    literatures on this dataset and show that the best systems only achieve 38.4%
    F1 on our generalized accuracy metric, while expert human performance is 96%.
    We additionally present a new model that combines reading comprehension methods
    with simple numerical reasoning to achieve 51% F1.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1246.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1246.Supplementary.pdf
  author:
  - first: Dheeru
    full: Dheeru Dua
    id: dheeru-dua
    last: Dua
  - first: Yizhong
    full: Yizhong Wang
    id: yizhong-wang
    last: Wang
  - first: Pradeep
    full: Pradeep Dasigi
    id: pradeep-dasigi
    last: Dasigi
  - first: Gabriel
    full: Gabriel Stanovsky
    id: gabriel-stanovsky
    last: Stanovsky
  - first: Sameer
    full: Sameer Singh
    id: sameer-singh
    last: Singh
  - first: Matt
    full: Matt Gardner
    id: matt-gardner
    last: Gardner
  author_string: Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer
    Singh, Matt Gardner
  bibkey: dua-etal-2019-drop
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1246
  month: June
  page_first: '2368'
  page_last: '2378'
  pages: "2368\u20132378"
  paper_id: '246'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1246.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1246.jpg
  title: 'DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over
    Paragraphs'
  title_html: '<span class="acl-fixed-case">DROP</span>: A Reading Comprehension Benchmark
    Requiring Discrete Reasoning Over Paragraphs'
  url: https://www.aclweb.org/anthology/N19-1246
  year: '2019'
N19-1247:
  abstract: A recently proposed lattice model has demonstrated that words in character
    sequence can provide rich word boundary information for character-based Chinese
    NER model. In this model, word information is integrated into a shortcut path
    between the start and the end characters of the word. However, the existence of
    shortcut path may cause the model to degenerate into a partial word-based model,
    which will suffer from word segmentation errors. Furthermore, the lattice model
    can not be trained in batches due to its DAG structure. In this paper, we propose
    a novel word-character LSTM(WC-LSTM) model to add word information into the start
    or the end character of the word, alleviating the influence of word segmentation
    errors while obtaining the word boundary information. Four different strategies
    are explored in our model to encode word information into a fixed-sized representation
    for efficient batch training. Experiments on benchmark datasets show that our
    proposed model outperforms other state-of-the-arts models.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1247.Supplementary.zip
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1247.Supplementary.zip
  author:
  - first: Wei
    full: Wei Liu
    id: wei-liu
    last: Liu
  - first: Tongge
    full: Tongge Xu
    id: tongge-xu
    last: Xu
  - first: Qinghua
    full: Qinghua Xu
    id: qinghua-xu
    last: Xu
  - first: Jiayu
    full: Jiayu Song
    id: jiayu-song
    last: Song
  - first: Yueran
    full: Yueran Zu
    id: yueran-zu
    last: Zu
  author_string: Wei Liu, Tongge Xu, Qinghua Xu, Jiayu Song, Yueran Zu
  bibkey: liu-etal-2019-encoding
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1247
  month: June
  page_first: '2379'
  page_last: '2389'
  pages: "2379\u20132389"
  paper_id: '247'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1247.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1247.jpg
  title: An Encoding Strategy Based Word-Character LSTM for Chinese NER
  title_html: An Encoding Strategy Based Word-Character <span class="acl-fixed-case">LSTM</span>
    for <span class="acl-fixed-case">C</span>hinese <span class="acl-fixed-case">NER</span>
  url: https://www.aclweb.org/anthology/N19-1247
  year: '2019'
N19-1248:
  abstract: Arabic text is typically written without short vowels (or diacritics).
    However, their presence is required for properly verbalizing Arabic and is hence
    essential for applications such as text to speech. There are two types of diacritics,
    namely core-word diacritics and case-endings. Most previous works on automatic
    Arabic diacritic recovery rely on a large number of manually engineered features,
    particularly for case-endings. In this work, we present a unified character level
    sequence-to-sequence deep learning model that recovers both types of diacritics
    without the use of explicit feature engineering. Specifically, we employ a standard
    neural machine translation setup on overlapping windows of words (broken down
    into characters), and then we use voting to select the most likely diacritized
    form of a word. The proposed model outperforms all previous state-of-the-art systems.
    Our best settings achieve a word error rate (WER) of 4.49% compared to the state-of-the-art
    of 12.25% on a standard dataset.
  address: Minneapolis, Minnesota
  author:
  - first: Hamdy
    full: Hamdy Mubarak
    id: hamdy-mubarak
    last: Mubarak
  - first: Ahmed
    full: Ahmed Abdelali
    id: ahmed-abdelali
    last: Abdelali
  - first: Hassan
    full: Hassan Sajjad
    id: hassan-sajjad
    last: Sajjad
  - first: Younes
    full: Younes Samih
    id: younes-samih
    last: Samih
  - first: Kareem
    full: Kareem Darwish
    id: kareem-darwish
    last: Darwish
  author_string: Hamdy Mubarak, Ahmed Abdelali, Hassan Sajjad, Younes Samih, Kareem
    Darwish
  bibkey: mubarak-etal-2019-highly
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1248
  month: June
  page_first: '2390'
  page_last: '2395'
  pages: "2390\u20132395"
  paper_id: '248'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1248.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1248.jpg
  title: Highly Effective Arabic Diacritization using Sequence to Sequence Modeling
  title_html: Highly Effective <span class="acl-fixed-case">A</span>rabic Diacritization
    using Sequence to Sequence Modeling
  url: https://www.aclweb.org/anthology/N19-1248
  year: '2019'
N19-1249:
  abstract: Multi-task learning (MTL) has been studied recently for sequence labeling.
    Typically, auxiliary tasks are selected specifically in order to improve the performance
    of a target task. Jointly learning multiple tasks in a way that benefit all of
    them simultaneously can increase the utility of MTL. In order to do so, we propose
    a new LSTM cell which contains both shared parameters that can learn from all
    tasks, and task-specific parameters that can learn task-specific information.
    We name it a Shared-Cell Long-Short Term Memory SC-LSTM. Experimental results
    on three sequence labeling benchmarks (named-entity recognition, text chunking,
    and part-of-speech tagging) demonstrate the effectiveness of our SC-LSTM cell.
  address: Minneapolis, Minnesota
  author:
  - first: Peng
    full: Peng Lu
    id: peng-lu
    last: Lu
  - first: Ting
    full: Ting Bai
    id: ting-bai
    last: Bai
  - first: Philippe
    full: Philippe Langlais
    id: philippe-langlais
    last: Langlais
  author_string: Peng Lu, Ting Bai, Philippe Langlais
  bibkey: lu-etal-2019-sc
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1249
  month: June
  page_first: '2396'
  page_last: '2406'
  pages: "2396\u20132406"
  paper_id: '249'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1249.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1249.jpg
  title: 'SC-LSTM: Learning Task-Specific Representations in Multi-Task Learning for
    Sequence Labeling'
  title_html: '<span class="acl-fixed-case">SC</span>-<span class="acl-fixed-case">LSTM</span>:
    Learning Task-Specific Representations in Multi-Task Learning for Sequence Labeling'
  url: https://www.aclweb.org/anthology/N19-1249
  year: '2019'
N19-1250:
  abstract: 'Distantly-labeled data can be used to scale up training of statistical
    models, but it is typically noisy and that noise can vary with the distant labeling
    technique. In this work, we propose a two-stage procedure for handling this type
    of data: denoise it with a learned model, then train our final model on clean
    and denoised distant data with standard supervised training. Our denoising approach
    consists of two parts. First, a filtering function discards examples from the
    distantly labeled data that are wholly unusable. Second, a relabeling function
    repairs noisy labels for the retained examples. Each of these components is a
    model trained on synthetically-noised examples generated from a small manually-labeled
    set. We investigate this approach on the ultra-fine entity typing task of Choi
    et al. (2018). Our baseline model is an extension of their model with pre-trained
    ELMo representations, which already achieves state-of-the-art performance. Adding
    distant data that has been denoised with our learned models gives further performance
    gains over this base model, outperforming models trained on raw distant data or
    heuristically-denoised distant data.'
  address: Minneapolis, Minnesota
  author:
  - first: Yasumasa
    full: Yasumasa Onoe
    id: yasumasa-onoe
    last: Onoe
  - first: Greg
    full: Greg Durrett
    id: greg-durrett
    last: Durrett
  author_string: Yasumasa Onoe, Greg Durrett
  bibkey: onoe-durrett-2019-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1250
  month: June
  page_first: '2407'
  page_last: '2417'
  pages: "2407\u20132417"
  paper_id: '250'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1250.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1250.jpg
  title: Learning to Denoise Distantly-Labeled Data for Entity Typing
  title_html: Learning to Denoise Distantly-Labeled Data for Entity Typing
  url: https://www.aclweb.org/anthology/N19-1250
  year: '2019'
N19-1251:
  abstract: 'While rule-based detection of subject-verb agreement (SVA) errors is
    sensitive to syntactic parsing errors and irregularities and exceptions to the
    main rules, neural sequential labelers have a tendency to overfit their training
    data. We observe that rule-based error generation is less sensitive to syntactic
    parsing errors and irregularities than error detection and explore a simple, yet
    efficient approach to getting the best of both worlds: We train neural sequential
    labelers on the combination of large volumes of silver standard data, obtained
    through rule-based error generation, and gold standard data. We show that our
    simple protocol leads to more robust detection of SVA errors on both in-domain
    and out-of-domain data, as well as in the context of other errors and long-distance
    dependencies; and across four standard benchmarks, the induced model on average
    achieves a new state of the art.'
  address: Minneapolis, Minnesota
  author:
  - first: Simon
    full: Simon Flachs
    id: simon-flachs
    last: Flachs
  - first: "Oph\xE9lie"
    full: "Oph\xE9lie Lacroix"
    id: ophelie-lacroix
    last: Lacroix
  - first: Marek
    full: Marek Rei
    id: marek-rei
    last: Rei
  - first: Helen
    full: Helen Yannakoudakis
    id: helen-yannakoudakis
    last: Yannakoudakis
  - first: Anders
    full: "Anders S\xF8gaard"
    id: anders-sogaard
    last: "S\xF8gaard"
  author_string: "Simon Flachs, Oph\xE9lie Lacroix, Marek Rei, Helen Yannakoudakis,\
    \ Anders S\xF8gaard"
  bibkey: flachs-etal-2019-simple
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1251
  month: June
  page_first: '2418'
  page_last: '2427'
  pages: "2418\u20132427"
  paper_id: '251'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1251.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1251.jpg
  title: A Simple and Robust Approach to Detecting Subject-Verb Agreement Errors
  title_html: A Simple and Robust Approach to Detecting Subject-Verb Agreement Errors
  url: https://www.aclweb.org/anthology/N19-1251
  year: '2019'
N19-1252:
  abstract: "Unsupervised part of speech (POS) tagging is often framed as a clustering\
    \ problem, but practical taggers need to ground their clusters as well. Grounding\
    \ generally requires reference labeled data, a luxury a low-resource language\
    \ might not have. In this work, we describe an approach for low-resource unsupervised\
    \ POS tagging that yields fully grounded output and requires no labeled training\
    \ data. We find the classic method of Brown et al. (1992) clusters well in our\
    \ use case and employ a decipherment-based approach to grounding. This approach\
    \ presumes a sequence of cluster IDs is a \u2018ciphertext\u2019 and seeks a POS\
    \ tag-to-cluster ID mapping that will reveal the POS sequence. We show intrinsically\
    \ that, despite the difficulty of the task, we obtain reasonable performance across\
    \ a variety of languages. We also show extrinsically that incorporating our POS\
    \ tagger into a name tagger leads to state-of-the-art tagging performance in Sinhalese\
    \ and Kinyarwanda, two languages with nearly no labeled POS data available. We\
    \ further demonstrate our tagger\u2019s utility by incorporating it into a true\
    \ \u2018zero-resource\u2019 variant of the MALOPA (Ammar et al., 2016) dependency\
    \ parser model that removes the current reliance on multilingual resources and\
    \ gold POS tags for new languages. Experiments show that including our tagger\
    \ makes up much of the accuracy lost when gold POS tags are unavailable."
  address: Minneapolis, Minnesota
  author:
  - first: Ronald
    full: Ronald Cardenas
    id: ronald-cardenas
    last: Cardenas
  - first: Ying
    full: Ying Lin
    id: ying-lin
    last: Lin
  - first: Heng
    full: Heng Ji
    id: heng-ji
    last: Ji
  - first: Jonathan
    full: Jonathan May
    id: jonathan-may
    last: May
  author_string: Ronald Cardenas, Ying Lin, Heng Ji, Jonathan May
  bibkey: cardenas-etal-2019-grounded
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1252
  month: June
  page_first: '2428'
  page_last: '2439'
  pages: "2428\u20132439"
  paper_id: '252'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1252.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1252.jpg
  title: A Grounded Unsupervised Universal Part-of-Speech Tagger for Low-Resource
    Languages
  title_html: A Grounded Unsupervised Universal Part-of-Speech Tagger for Low-Resource
    Languages
  url: https://www.aclweb.org/anthology/N19-1252
  year: '2019'
N19-1253:
  abstract: Different languages might have different word orders. In this paper, we
    investigate crosslingual transfer and posit that an orderagnostic model will perform
    better when transferring to distant foreign languages. To test our hypothesis,
    we train dependency parsers on an English corpus and evaluate their transfer performance
    on 30 other languages. Specifically, we compare encoders and decoders based on
    Recurrent Neural Networks (RNNs) and modified self-attentive architectures. The
    former relies on sequential information while the latter is more flexible at modeling
    word order. Rigorous experiments and detailed analysis shows that RNN-based architectures
    transfer well to languages that are close to English, while self-attentive models
    have better overall cross-lingual transferability and perform especially well
    on distant languages.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1253.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1253.Supplementary.pdf
  author:
  - first: Wasi
    full: Wasi Ahmad
    id: wasi-ahmad
    last: Ahmad
  - first: Zhisong
    full: Zhisong Zhang
    id: zhisong-zhang
    last: Zhang
  - first: Xuezhe
    full: Xuezhe Ma
    id: xuezhe-ma
    last: Ma
  - first: Eduard
    full: Eduard Hovy
    id: eduard-hovy
    last: Hovy
  - first: Kai-Wei
    full: Kai-Wei Chang
    id: kai-wei-chang
    last: Chang
  - first: Nanyun
    full: Nanyun Peng
    id: nanyun-peng
    last: Peng
  author_string: Wasi Ahmad, Zhisong Zhang, Xuezhe Ma, Eduard Hovy, Kai-Wei Chang,
    Nanyun Peng
  bibkey: ahmad-etal-2019-difficulties
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1253
  month: June
  page_first: '2440'
  page_last: '2452'
  pages: "2440\u20132452"
  paper_id: '253'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1253.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1253.jpg
  title: 'On Difficulties of Cross-Lingual Transfer with Order Differences: A Case
    Study on Dependency Parsing'
  title_html: 'On Difficulties of Cross-Lingual Transfer with Order Differences: A
    Case Study on Dependency Parsing'
  url: https://www.aclweb.org/anthology/N19-1253
  year: '2019'
N19-1254:
  abstract: We propose a generative model for a sentence that uses two latent variables,
    with one intended to represent the syntax of the sentence and the other to represent
    its semantics. We show we can achieve better disentanglement between semantic
    and syntactic representations by training with multiple losses, including losses
    that exploit aligned paraphrastic sentences and word-order information. We evaluate
    our models on standard semantic similarity tasks and novel syntactic similarity
    tasks. Empirically, we find that the model with the best performing syntactic
    and semantic representations also gives rise to the most disentangled representations.
  address: Minneapolis, Minnesota
  author:
  - first: Mingda
    full: Mingda Chen
    id: mingda-chen
    last: Chen
  - first: Qingming
    full: Qingming Tang
    id: qingming-tang
    last: Tang
  - first: Sam
    full: Sam Wiseman
    id: sam-wiseman
    last: Wiseman
  - first: Kevin
    full: Kevin Gimpel
    id: kevin-gimpel
    last: Gimpel
  author_string: Mingda Chen, Qingming Tang, Sam Wiseman, Kevin Gimpel
  bibkey: chen-etal-2019-multi-task
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1254
  month: June
  page_first: '2453'
  page_last: '2464'
  pages: "2453\u20132464"
  paper_id: '254'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1254.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1254.jpg
  title: A Multi-Task Approach for Disentangling Syntax and Semantics in Sentence
    Representations
  title_html: A Multi-Task Approach for Disentangling Syntax and Semantics in Sentence
    Representations
  url: https://www.aclweb.org/anthology/N19-1254
  year: '2019'
N19-1255:
  abstract: Unsupervised document representation learning is an important task providing
    pre-trained features for NLP applications. Unlike most previous work which learn
    the embedding based on self-prediction of the surface of text, we explicitly exploit
    the inter-document information and directly model the relations of documents in
    embedding space with a discriminative network and a novel objective. Extensive
    experiments on both small and large public datasets show the competitiveness of
    the proposed method. In evaluations on standard document classification, our model
    has errors that are 5 to 13% lower than state-of-the-art unsupervised embedding
    models. The reduction in error is even more pronounced in scarce label setting.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/355814096
    type: video
    url: https://vimeo.com/355814096
  author:
  - first: Hong-You
    full: Hong-You Chen
    id: hong-you-chen
    last: Chen
  - first: Chin-Hua
    full: Chin-Hua Hu
    id: chin-hua-hu
    last: Hu
  - first: Leila
    full: Leila Wehbe
    id: leila-wehbe
    last: Wehbe
  - first: Shou-De
    full: Shou-De Lin
    id: shou-de-lin
    last: Lin
  author_string: Hong-You Chen, Chin-Hua Hu, Leila Wehbe, Shou-De Lin
  bibkey: chen-etal-2019-self
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1255
  month: June
  page_first: '2465'
  page_last: '2474'
  pages: "2465\u20132474"
  paper_id: '255'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1255.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1255.jpg
  title: Self-Discriminative Learning for Unsupervised Document Embedding
  title_html: Self-Discriminative Learning for Unsupervised Document Embedding
  url: https://www.aclweb.org/anthology/N19-1255
  year: '2019'
N19-1256:
  abstract: In this paper, we present an adaptive convolution for text classification
    to give flexibility to convolutional neural networks (CNNs). Unlike traditional
    convolutions which utilize the same set of filters regardless of different inputs,
    the adaptive convolution employs adaptively generated convolutional filters conditioned
    on inputs. We achieve this by attaching filter-generating networks, which are
    carefully designed to generate input-specific filters, to convolution blocks in
    existing CNNs. We show the efficacy of our approach in existing CNNs based on
    the performance evaluation. Our evaluation indicates that all of our baselines
    achieve performance improvements with adaptive convolutions as much as up to 2.6
    percentage point in seven benchmark text classification datasets.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1256.Software.zip
    type: software
    url: https://www.aclweb.org/anthology/attachments/N19-1256.Software.zip
  author:
  - first: Byung-Ju
    full: Byung-Ju Choi
    id: byung-ju-choi
    last: Choi
  - first: Jun-Hyung
    full: Jun-Hyung Park
    id: jun-hyung-park
    last: Park
  - first: SangKeun
    full: SangKeun Lee
    id: sangkeun-lee
    last: Lee
  author_string: Byung-Ju Choi, Jun-Hyung Park, SangKeun Lee
  bibkey: choi-etal-2019-adaptive
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1256
  month: June
  page_first: '2475'
  page_last: '2485'
  pages: "2475\u20132485"
  paper_id: '256'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1256.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1256.jpg
  title: Adaptive Convolution for Text Classification
  title_html: Adaptive Convolution for Text Classification
  url: https://www.aclweb.org/anthology/N19-1256
  year: '2019'
N19-1257:
  abstract: 'Aspect-based sentiment analysis involves the recognition of so called
    opinion target expressions (OTEs). To automatically extract OTEs, supervised learning
    algorithms are usually employed which are trained on manually annotated corpora.
    The creation of these corpora is labor-intensive and sufficiently large datasets
    are therefore usually only available for a very narrow selection of languages
    and domains. In this work, we address the lack of available annotated data for
    specific languages by proposing a zero-shot cross-lingual approach for the extraction
    of opinion target expressions. We leverage multilingual word embeddings that share
    a common vector space across various languages and incorporate these into a convolutional
    neural network architecture for OTE extraction. Our experiments with 5 languages
    give promising results: We can successfully train a model on annotated data of
    a source language and perform accurate prediction on a target language without
    ever using any annotated samples in that target language. Depending on the source
    and target language pairs, we reach performances in a zero-shot regime of up to
    77% of a model trained on target language data. Furthermore, we can increase this
    performance up to 87% of a baseline model trained on target language data by performing
    cross-lingual learning from multiple source languages.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/359684150
    type: video
    url: https://vimeo.com/359684150
  author:
  - first: Soufian
    full: Soufian Jebbara
    id: soufian-jebbara
    last: Jebbara
  - first: Philipp
    full: Philipp Cimiano
    id: philipp-cimiano
    last: Cimiano
  author_string: Soufian Jebbara, Philipp Cimiano
  bibkey: jebbara-cimiano-2019-zero
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1257
  month: June
  page_first: '2486'
  page_last: '2495'
  pages: "2486\u20132495"
  paper_id: '257'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1257.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1257.jpg
  title: Zero-Shot Cross-Lingual Opinion Target Extraction
  title_html: <span class="acl-fixed-case">Z</span>ero-Shot Cross-Lingual Opinion
    Target Extraction
  url: https://www.aclweb.org/anthology/N19-1257
  year: '2019'
N19-1258:
  abstract: Cross-domain sentiment classification aims to predict sentiment polarity
    on a target domain utilizing a classifier learned from a source domain. Most existing
    adversarial learning methods focus on aligning the global marginal distribution
    by fooling a domain discriminator, without taking category-specific decision boundaries
    into consideration, which can lead to the mismatch of category-level features.
    In this work, we propose an adversarial category alignment network (ACAN), which
    attempts to enhance category consistency between the source domain and the target
    domain. Specifically, we increase the discrepancy of two polarity classifiers
    to provide diverse views, locating ambiguous features near the decision boundaries.
    Then the generator learns to create better features away from the category boundaries
    by minimizing this discrepancy. Experimental results on benchmark datasets show
    that the proposed method can achieve state-of-the-art performance and produce
    more discriminative features.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1258.Presentation.pptx
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1258.Presentation.pptx
  - filename: https://vimeo.com/355822833
    type: video
    url: https://vimeo.com/355822833
  author:
  - first: Xiaoye
    full: Xiaoye Qu
    id: xiaoye-qu
    last: Qu
  - first: Zhikang
    full: Zhikang Zou
    id: zhikang-zou
    last: Zou
  - first: Yu
    full: Yu Cheng
    id: yu-cheng
    last: Cheng
  - first: Yang
    full: Yang Yang
    id: yang-yang
    last: Yang
  - first: Pan
    full: Pan Zhou
    id: pan-zhou
    last: Zhou
  author_string: Xiaoye Qu, Zhikang Zou, Yu Cheng, Yang Yang, Pan Zhou
  bibkey: qu-etal-2019-adversarial
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1258
  month: June
  page_first: '2496'
  page_last: '2508'
  pages: "2496\u20132508"
  paper_id: '258'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1258.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1258.jpg
  title: Adversarial Category Alignment Network for Cross-domain Sentiment Classification
  title_html: Adversarial Category Alignment Network for Cross-domain Sentiment Classification
  url: https://www.aclweb.org/anthology/N19-1258
  year: '2019'
N19-1259:
  abstract: Opinion target extraction and opinion words extraction are two fundamental
    subtasks in Aspect Based Sentiment Analysis (ABSA). Recently, many methods have
    made progress on these two tasks. However, few works aim at extracting opinion
    targets and opinion words as pairs. In this paper, we propose a novel sequence
    labeling subtask for ABSA named TOWE (Target-oriented Opinion Words Extraction),
    which aims at extracting the corresponding opinion words for a given opinion target.
    A target-fused sequence labeling neural network model is designed to perform this
    task. The opinion target information is well encoded into context by an Inward-Outward
    LSTM. Then left and right contexts of the opinion target and the global context
    are combined to find the corresponding opinion words. We build four datasets for
    TOWE based on several popular ABSA benchmarks from laptop and restaurant reviews.
    The experimental results show that our proposed model outperforms the other compared
    methods significantly. We believe that our work may not only be helpful for downstream
    sentiment analysis task, but can also be used for pair-wise opinion summarization.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/355827037
    type: video
    url: https://vimeo.com/355827037
  author:
  - first: Zhifang
    full: Zhifang Fan
    id: zhifang-fan
    last: Fan
  - first: Zhen
    full: Zhen Wu
    id: zhen-wu
    last: Wu
  - first: Xin-Yu
    full: Xin-Yu Dai
    id: xinyu-dai
    last: Dai
  - first: Shujian
    full: Shujian Huang
    id: shujian-huang
    last: Huang
  - first: Jiajun
    full: Jiajun Chen
    id: jiajun-chen
    last: Chen
  author_string: Zhifang Fan, Zhen Wu, Xin-Yu Dai, Shujian Huang, Jiajun Chen
  bibkey: fan-etal-2019-target
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1259
  month: June
  page_first: '2509'
  page_last: '2518'
  pages: "2509\u20132518"
  paper_id: '259'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1259.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1259.jpg
  title: Target-oriented Opinion Words Extraction with Target-fused Neural Sequence
    Labeling
  title_html: Target-oriented Opinion Words Extraction with Target-fused Neural Sequence
    Labeling
  url: https://www.aclweb.org/anthology/N19-1259
  year: '2019'
N19-1260:
  abstract: 'We address the problem of abstractive summarization in two directions:
    proposing a novel dataset and a new model. First, we collect Reddit TIFU dataset,
    consisting of 120K posts from the online discussion forum Reddit. We use such
    informal crowd-generated posts as text source, in contrast with existing datasets
    that mostly use formal documents as source such as news articles. Thus, our dataset
    could less suffer from some biases that key sentences usually located at the beginning
    of the text and favorable summary candidates are already inside the text in similar
    forms. Second, we propose a novel abstractive summarization model named multi-level
    memory networks (MMN), equipped with multi-level memory to store the information
    of text from different levels of abstraction. With quantitative evaluation and
    user studies via Amazon Mechanical Turk, we show the Reddit TIFU dataset is highly
    abstractive and the MMN outperforms the state-of-the-art summarization models.'
  address: Minneapolis, Minnesota
  author:
  - first: Byeongchang
    full: Byeongchang Kim
    id: byeongchang-kim
    last: Kim
  - first: Hyunwoo
    full: Hyunwoo Kim
    id: hyunwoo-kim
    last: Kim
  - first: Gunhee
    full: Gunhee Kim
    id: gunhee-kim
    last: Kim
  author_string: Byeongchang Kim, Hyunwoo Kim, Gunhee Kim
  bibkey: kim-etal-2019-abstractive
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1260
  month: June
  page_first: '2519'
  page_last: '2531'
  pages: "2519\u20132531"
  paper_id: '260'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1260.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1260.jpg
  title: Abstractive Summarization of Reddit Posts with Multi-level Memory Networks
  title_html: Abstractive Summarization of <span class="acl-fixed-case">R</span>eddit
    Posts with Multi-level Memory Networks
  url: https://www.aclweb.org/anthology/N19-1260
  year: '2019'
N19-1261:
  abstract: Automating the assessment of learner summary provides a useful tool for
    assessing learner reading comprehension. We present a summarization task for evaluating
    non-native reading comprehension and propose three novel approaches to automatically
    assess the learner summaries. We evaluate our models on two datasets we created
    and show that our models outperform traditional approaches that rely on exact
    word match on this task. Our best model produces quality assessments close to
    professional examiners.
  address: Minneapolis, Minnesota
  author:
  - first: Menglin
    full: Menglin Xia
    id: menglin-xia
    last: Xia
  - first: Ekaterina
    full: Ekaterina Kochmar
    id: ekaterina-kochmar
    last: Kochmar
  - first: Ted
    full: Ted Briscoe
    id: ted-briscoe
    last: Briscoe
  author_string: Menglin Xia, Ekaterina Kochmar, Ted Briscoe
  bibkey: xia-etal-2019-automatic
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1261
  month: June
  page_first: '2532'
  page_last: '2542'
  pages: "2532\u20132542"
  paper_id: '261'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1261.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1261.jpg
  title: Automatic learner summary assessment for reading comprehension
  title_html: Automatic learner summary assessment for reading comprehension
  url: https://www.aclweb.org/anthology/N19-1261
  year: '2019'
N19-1262:
  abstract: Neural sequence-to-sequence models have been successfully applied to text
    compression. However, these models were trained on huge automatically induced
    parallel corpora, which are only available for a few domains and tasks. In this
    paper, we propose a novel interactive setup to neural text compression that enables
    transferring a model to new domains and compression tasks with minimal human supervision.
    This is achieved by employing active learning, which intelligently samples from
    a large pool of unlabeled data. Using this setup, we can successfully adapt a
    model trained on small data of 40k samples for a headline generation task to a
    general text compression dataset at an acceptable compression quality with just
    500 sampled instances annotated by a human.
  address: Minneapolis, Minnesota
  author:
  - first: Avinesh
    full: Avinesh P.V.S
    id: avinesh-p-v-s
    last: P.V.S
  - first: Christian M.
    full: Christian M. Meyer
    id: christian-m-meyer
    last: Meyer
  author_string: Avinesh P.V.S, Christian M. Meyer
  bibkey: p-v-s-meyer-2019-data
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1262
  month: June
  page_first: '2543'
  page_last: '2554'
  pages: "2543\u20132554"
  paper_id: '262'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1262.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1262.jpg
  title: Data-efficient Neural Text Compression with Interactive Learning
  title_html: Data-efficient Neural Text Compression with Interactive Learning
  url: https://www.aclweb.org/anthology/N19-1262
  year: '2019'
N19-1263:
  abstract: "We propose a novel conditioned text generation model. It draws inspiration\
    \ from traditional template-based text generation techniques, where the source\
    \ provides the content (i.e., what to say), and the template influences how to\
    \ say it. Building on the successful encoder-decoder paradigm, it first encodes\
    \ the content representation from the given input text; to produce the output,\
    \ it retrieves exemplar text from the training data as \u201Csoft templates,\u201D\
    \ which are then used to construct an exemplar-specific decoder. We evaluate the\
    \ proposed model on abstractive text summarization and data-to-text generation.\
    \ Empirical results show that this model achieves strong performance and outperforms\
    \ comparable baselines."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1263.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1263.Supplementary.pdf
  author:
  - first: Hao
    full: Hao Peng
    id: hao-peng
    last: Peng
  - first: Ankur
    full: Ankur Parikh
    id: ankur-parikh
    last: Parikh
  - first: Manaal
    full: Manaal Faruqui
    id: manaal-faruqui
    last: Faruqui
  - first: Bhuwan
    full: Bhuwan Dhingra
    id: bhuwan-dhingra
    last: Dhingra
  - first: Dipanjan
    full: Dipanjan Das
    id: dipanjan-das
    last: Das
  author_string: Hao Peng, Ankur Parikh, Manaal Faruqui, Bhuwan Dhingra, Dipanjan
    Das
  bibkey: peng-etal-2019-text
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1263
  month: June
  page_first: '2555'
  page_last: '2565'
  pages: "2555\u20132565"
  paper_id: '263'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1263.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1263.jpg
  title: Text Generation with Exemplar-based Adaptive Decoding
  title_html: Text Generation with Exemplar-based Adaptive Decoding
  url: https://www.aclweb.org/anthology/N19-1263
  year: '2019'
N19-1264:
  abstract: Highlighting while reading is a natural behavior for people to track salient
    content of a document. It would be desirable to teach an extractive summarizer
    to do the same. However, a major obstacle to the development of a supervised summarizer
    is the lack of ground-truth. Manual annotation of extraction units is cost-prohibitive,
    whereas acquiring labels by automatically aligning human abstracts and source
    documents can yield inferior results. In this paper we describe a novel framework
    to guide a supervised, extractive summarization system with question-answering
    rewards. We argue that quality summaries should serve as document surrogates to
    answer important questions, and such question-answer pairs can be conveniently
    obtained from human abstracts. The system learns to promote summaries that are
    informative, fluent, and perform competitively on question-answering. Our results
    compare favorably with those reported by strong summarization baselines as evaluated
    by automatic metrics and human assessors.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1264.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1264.Presentation.pdf
  author:
  - first: Kristjan
    full: Kristjan Arumae
    id: kristjan-arumae
    last: Arumae
  - first: Fei
    full: Fei Liu
    id: fei-liu-utdallas
    last: Liu
  author_string: Kristjan Arumae, Fei Liu
  bibkey: arumae-liu-2019-guiding
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1264
  month: June
  page_first: '2566'
  page_last: '2577'
  pages: "2566\u20132577"
  paper_id: '264'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1264.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1264.jpg
  title: Guiding Extractive Summarization with Question-Answering Rewards
  title_html: Guiding Extractive Summarization with Question-Answering Rewards
  url: https://www.aclweb.org/anthology/N19-1264
  year: '2019'
N19-1265:
  abstract: We propose a grounded dialogue state encoder which addresses a foundational
    issue on how to integrate visual grounding with dialogue system components. As
    a test-bed, we focus on the GuessWhat?! game, a two-player game where the goal
    is to identify an object in a complex visual scene by asking a sequence of yes/no
    questions. Our visually-grounded encoder leverages synergies between guessing
    and asking questions, as it is trained jointly using multi-task learning. We further
    enrich our model via a cooperative learning regime. We show that the introduction
    of both the joint architecture and cooperative learning lead to accuracy improvements
    over the baseline system. We compare our approach to an alternative system which
    extends the baseline with reinforcement learning. Our in-depth analysis shows
    that the linguistic skills of the two models differ dramatically, despite approaching
    comparable performance levels. This points at the importance of analyzing the
    linguistic output of competing systems beyond numeric comparison solely based
    on task success.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/361582708
    type: video
    url: https://vimeo.com/361582708
  author:
  - first: Ravi
    full: Ravi Shekhar
    id: ravi-shekhar
    last: Shekhar
  - first: Aashish
    full: Aashish Venkatesh
    id: aashish-venkatesh
    last: Venkatesh
  - first: Tim
    full: "Tim Baumg\xE4rtner"
    id: tim-baumgartner
    last: "Baumg\xE4rtner"
  - first: Elia
    full: Elia Bruni
    id: elia-bruni
    last: Bruni
  - first: Barbara
    full: Barbara Plank
    id: barbara-plank
    last: Plank
  - first: Raffaella
    full: Raffaella Bernardi
    id: raffaella-bernardi
    last: Bernardi
  - first: Raquel
    full: "Raquel Fern\xE1ndez"
    id: raquel-fernandez
    last: "Fern\xE1ndez"
  author_string: "Ravi Shekhar, Aashish Venkatesh, Tim Baumg\xE4rtner, Elia Bruni,\
    \ Barbara Plank, Raffaella Bernardi, Raquel Fern\xE1ndez"
  bibkey: shekhar-etal-2019-beyond
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1265
  month: June
  page_first: '2578'
  page_last: '2587'
  pages: "2578\u20132587"
  paper_id: '265'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1265.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1265.jpg
  title: 'Beyond task success: A closer look at jointly learning to see, ask, and
    GuessWhat'
  title_html: 'Beyond task success: A closer look at jointly learning to see, ask,
    and <span class="acl-fixed-case">G</span>uess<span class="acl-fixed-case">W</span>hat'
  url: https://www.aclweb.org/anthology/N19-1265
  year: '2019'
N19-1266:
  abstract: Visual Dialog is a multi-modal task that requires a model to participate
    in a multi-turn human dialog grounded on an image, and generate correct, human-like
    responses. In this paper, we propose a novel Adversarial Multi-modal Feature Encoding
    (AMFE) framework for effective and robust auxiliary training of visual dialog
    systems. AMFE can force the language-encoding part of a model to generate hidden
    states in a distribution closely related to the distribution of real-world images,
    resulting in language features containing general knowledge from both modalities
    by nature, which can help generate both more correct and more general responses
    with reasonably low time cost. Experimental results show that AMFE can steadily
    bring performance gains to different models on different scales of data. Our method
    outperforms both the supervised learning baselines and other fine-tuning methods,
    achieving state-of-the-art results on most metrics of VisDial v0.5/v0.9 generative
    tasks.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/361585429
    type: video
    url: https://vimeo.com/361585429
  author:
  - first: Yiqun
    full: Yiqun Yao
    id: yiqun-yao
    last: Yao
  - first: Jiaming
    full: Jiaming Xu
    id: jiaming-xu
    last: Xu
  - first: Bo
    full: Bo Xu
    id: bo-xu
    last: Xu
  author_string: Yiqun Yao, Jiaming Xu, Bo Xu
  bibkey: yao-etal-2019-world
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1266
  month: June
  page_first: '2588'
  page_last: '2598'
  pages: "2588\u20132598"
  paper_id: '266'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1266.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1266.jpg
  title: 'The World in My Mind: Visual Dialog with Adversarial Multi-modal Feature
    Encoding'
  title_html: 'The World in My Mind: Visual Dialog with Adversarial Multi-modal Feature
    Encoding'
  url: https://www.aclweb.org/anthology/N19-1266
  year: '2019'
N19-1267:
  abstract: Human language is a rich multimodal signal consisting of spoken words,
    facial expressions, body gestures, and vocal intonations. Learning representations
    for these spoken utterances is a complex research problem due to the presence
    of multiple heterogeneous sources of information. Recent advances in multimodal
    learning have followed the general trend of building more complex models that
    utilize various attention, memory and recurrent components. In this paper, we
    propose two simple but strong baselines to learn embeddings of multimodal utterances.
    The first baseline assumes a conditional factorization of the utterance into unimodal
    factors. Each unimodal factor is modeled using the simple form of a likelihood
    function obtained via a linear transformation of the embedding. We show that the
    optimal embedding can be derived in closed form by taking a weighted average of
    the unimodal features. In order to capture richer representations, our second
    baseline extends the first by factorizing into unimodal, bimodal, and trimodal
    factors, while retaining simplicity and efficiency during learning and inference.
    From a set of experiments across two tasks, we show strong performance on both
    supervised and semi-supervised multimodal prediction, as well as significant (10
    times) speedups over neural models during inference. Overall, we believe that
    our strong baseline models offer new benchmarking options for future research
    in multimodal learning.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1267.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1267.Supplementary.pdf
  - filename: https://vimeo.com/364226255
    type: video
    url: https://vimeo.com/364226255
  author:
  - first: Paul Pu
    full: Paul Pu Liang
    id: paul-pu-liang
    last: Liang
  - first: Yao Chong
    full: Yao Chong Lim
    id: yao-chong-lim
    last: Lim
  - first: Yao-Hung Hubert
    full: Yao-Hung Hubert Tsai
    id: yao-hung-hubert-tsai
    last: Tsai
  - first: Ruslan
    full: Ruslan Salakhutdinov
    id: ruslan-salakhutdinov
    last: Salakhutdinov
  - first: Louis-Philippe
    full: Louis-Philippe Morency
    id: louis-philippe-morency
    last: Morency
  author_string: Paul Pu Liang, Yao Chong Lim, Yao-Hung Hubert Tsai, Ruslan Salakhutdinov,
    Louis-Philippe Morency
  bibkey: liang-etal-2019-strong
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1267
  month: June
  page_first: '2599'
  page_last: '2609'
  pages: "2599\u20132609"
  paper_id: '267'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1267.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1267.jpg
  title: Strong and Simple Baselines for Multimodal Utterance Embeddings
  title_html: Strong and Simple Baselines for Multimodal Utterance Embeddings
  url: https://www.aclweb.org/anthology/N19-1267
  year: '2019'
N19-1268:
  abstract: "A grand goal in AI is to build a robot that can accurately navigate based\
    \ on natural language instructions, which requires the agent to perceive the scene,\
    \ understand and ground language, and act in the real-world environment. One key\
    \ challenge here is to learn to navigate in new environments that are unseen during\
    \ training. Most of the existing approaches perform dramatically worse in unseen\
    \ environments as compared to seen ones. In this paper, we present a generalizable\
    \ navigational agent. Our agent is trained in two stages. The first stage is training\
    \ via mixed imitation and reinforcement learning, combining the benefits from\
    \ both off-policy and on-policy optimization. The second stage is fine-tuning\
    \ via newly-introduced \u2018unseen\u2019 triplets (environment, path, instruction).\
    \ To generate these unseen triplets, we propose a simple but effective \u2018\
    environmental dropout\u2019 method to mimic unseen environments, which overcomes\
    \ the problem of limited seen environment variability. Next, we apply semi-supervised\
    \ learning (via back-translation) on these dropout environments to generate new\
    \ paths and instructions. Empirically, we show that our agent is substantially\
    \ better at generalizability when fine-tuned with these triplets, outperforming\
    \ the state-of-art approaches by a large margin on the private unseen test set\
    \ of the Room-to-Room task, and achieving the top rank on the leaderboard."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/364229428
    type: video
    url: https://vimeo.com/364229428
  author:
  - first: Hao
    full: Hao Tan
    id: hao-tan
    last: Tan
  - first: Licheng
    full: Licheng Yu
    id: licheng-yu
    last: Yu
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  author_string: Hao Tan, Licheng Yu, Mohit Bansal
  bibkey: tan-etal-2019-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1268
  month: June
  page_first: '2610'
  page_last: '2621'
  pages: "2610\u20132621"
  paper_id: '268'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1268.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1268.jpg
  title: 'Learning to Navigate Unseen Environments: Back Translation with Environmental
    Dropout'
  title_html: 'Learning to Navigate Unseen Environments: Back Translation with Environmental
    Dropout'
  url: https://www.aclweb.org/anthology/N19-1268
  year: '2019'
N19-1269:
  abstract: Recent work in neural generation has attracted significant interest in
    controlling the form of text, such as style, persona, and politeness. However,
    there has been less work on controlling neural text generation for content. This
    paper introduces the notion of Content Transfer for long-form text generation,
    where the task is to generate a next sentence in a document that both fits its
    context and is grounded in a content-rich external textual source such as a news
    story. Our experiments on Wikipedia data show significant improvements against
    competitive baselines. As another contribution of this paper, we release a benchmark
    dataset of 640k Wikipedia referenced sentences paired with the source articles
    to encourage exploration of this new task.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/364740187
    type: video
    url: https://vimeo.com/364740187
  author:
  - first: Shrimai
    full: Shrimai Prabhumoye
    id: shrimai-prabhumoye
    last: Prabhumoye
  - first: Chris
    full: Chris Quirk
    id: chris-quirk
    last: Quirk
  - first: Michel
    full: Michel Galley
    id: michel-galley
    last: Galley
  author_string: Shrimai Prabhumoye, Chris Quirk, Michel Galley
  bibkey: prabhumoye-etal-2019-towards
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1269
  month: June
  page_first: '2622'
  page_last: '2632'
  pages: "2622\u20132632"
  paper_id: '269'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1269.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1269.jpg
  title: Towards Content Transfer through Grounded Text Generation
  title_html: Towards Content Transfer through Grounded Text Generation
  url: https://www.aclweb.org/anthology/N19-1269
  year: '2019'
N19-1270:
  abstract: 'Reading strategies have been shown to improve comprehension levels, especially
    for readers lacking adequate prior knowledge. Just as the process of knowledge
    accumulation is time-consuming for human readers, it is resource-demanding to
    impart rich general domain knowledge into a deep language model via pre-training.
    Inspired by reading strategies identified in cognitive science, and given limited
    computational resources - just a pre-trained model and a fixed number of training
    instances - we propose three general strategies aimed to improve non-extractive
    machine reading comprehension (MRC): (i) BACK AND FORTH READING that considers
    both the original and reverse order of an input sequence, (ii) HIGHLIGHTING, which
    adds a trainable embedding to the text embedding of tokens that are relevant to
    the question and candidate answers, and (iii) SELF-ASSESSMENT that generates practice
    questions and candidate answers directly from the text in an unsupervised manner.
    By fine-tuning a pre-trained language model (Radford et al., 2018) with our proposed
    strategies on the largest general domain multiple-choice MRC dataset RACE, we
    obtain a 5.8% absolute increase in accuracy over the previous best result achieved
    by the same pre-trained model fine-tuned on RACE without the use of strategies.
    We further fine-tune the resulting model on a target MRC task, leading to an absolute
    improvement of 6.2% in average accuracy over previous state-of-the-art approaches
    on six representative non-extractive MRC datasets from different domains (i.e.,
    ARC, OpenBookQA, MCTest, SemEval-2018 Task 11, ROCStories, and MultiRC). These
    results demonstrate the effectiveness of our proposed strategies and the versatility
    and general applicability of our fine-tuned models that incorporate these strategies.
    Core code is available at https://github.com/nlpdata/strategy/.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/364746823
    type: video
    url: https://vimeo.com/364746823
  author:
  - first: Kai
    full: Kai Sun
    id: kai-sun
    last: Sun
  - first: Dian
    full: Dian Yu
    id: dian-yu
    last: Yu
  - first: Dong
    full: Dong Yu
    id: dong-yu
    last: Yu
  - first: Claire
    full: Claire Cardie
    id: claire-cardie
    last: Cardie
  author_string: Kai Sun, Dian Yu, Dong Yu, Claire Cardie
  bibkey: sun-etal-2019-improving
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1270
  month: June
  page_first: '2633'
  page_last: '2643'
  pages: "2633\u20132643"
  paper_id: '270'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1270.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1270.jpg
  title: Improving Machine Reading Comprehension with General Reading Strategies
  title_html: Improving Machine Reading Comprehension with General Reading Strategies
  url: https://www.aclweb.org/anthology/N19-1270
  year: '2019'
N19-1271:
  abstract: We propose a multi-task learning framework to learn a joint Machine Reading
    Comprehension (MRC) model that can be applied to a wide range of MRC tasks in
    different domains. Inspired by recent ideas of data selection in machine translation,
    we develop a novel sample re-weighting scheme to assign sample-specific weights
    to the loss. Empirical study shows that our approach can be applied to many existing
    MRC models. Combined with contextual representations from pre-trained language
    models (such as ELMo), we achieve new state-of-the-art results on a set of MRC
    benchmark datasets. We release our code at https://github.com/xycforgithub/MultiTask-MRC.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/364750438
    type: video
    url: https://vimeo.com/364750438
  author:
  - first: Yichong
    full: Yichong Xu
    id: yichong-xu
    last: Xu
  - first: Xiaodong
    full: Xiaodong Liu
    id: xiaodong-liu
    last: Liu
  - first: Yelong
    full: Yelong Shen
    id: yelong-shen
    last: Shen
  - first: Jingjing
    full: Jingjing Liu
    id: jingjing-liu
    last: Liu
  - first: Jianfeng
    full: Jianfeng Gao
    id: jianfeng-gao
    last: Gao
  author_string: Yichong Xu, Xiaodong Liu, Yelong Shen, Jingjing Liu, Jianfeng Gao
  bibkey: xu-etal-2019-multi
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1271
  month: June
  page_first: '2644'
  page_last: '2655'
  pages: "2644\u20132655"
  paper_id: '271'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1271.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1271.jpg
  title: Multi-task Learning with Sample Re-weighting for Machine Reading Comprehension
  title_html: Multi-task Learning with Sample Re-weighting for Machine Reading Comprehension
  url: https://www.aclweb.org/anthology/N19-1271
  year: '2019'
N19-1272:
  abstract: Solving math word problems is a challenging task that requires accurate
    natural language understanding to bridge natural language texts and math expressions.
    Motivated by the intuition about how human generates the equations given the problem
    texts, this paper presents a neural approach to automatically solve math word
    problems by operating symbols according to their semantic meanings in texts. This
    paper views the process of generating equation as a bridge between the semantic
    world and the symbolic world, where the proposed neural math solver is based on
    an encoder-decoder framework. In the proposed model, the encoder is designed to
    understand the semantics of problems, and the decoder focuses on tracking semantic
    meanings of the generated symbols and then deciding which symbol to generate next.
    The preliminary experiments are conducted in a dataset Math23K, and our model
    significantly outperforms both the state-of-the-art single model and the best
    non-retrieval-based model over about 10% accuracy, demonstrating the effectiveness
    of bridging the symbolic and semantic worlds from math word problems.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1272.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1272.Presentation.pdf
  - filename: https://vimeo.com/361688938
    type: video
    url: https://vimeo.com/361688938
  author:
  - first: Ting-Rui
    full: Ting-Rui Chiang
    id: ting-rui-chiang
    last: Chiang
  - first: Yun-Nung
    full: Yun-Nung Chen
    id: yun-nung-chen
    last: Chen
  author_string: Ting-Rui Chiang, Yun-Nung Chen
  bibkey: chiang-chen-2019-semantically
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1272
  month: June
  page_first: '2656'
  page_last: '2668'
  pages: "2656\u20132668"
  paper_id: '272'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1272.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1272.jpg
  title: Semantically-Aligned Equation Generation for Solving and Reasoning Math Word
    Problems
  title_html: Semantically-Aligned Equation Generation for Solving and Reasoning Math
    Word Problems
  url: https://www.aclweb.org/anthology/N19-1272
  year: '2019'
N19-1273:
  abstract: 'Training semantic parsers from question-answer pairs typically involves
    searching over an exponentially large space of logical forms, and an unguided
    search can easily be misled by spurious logical forms that coincidentally evaluate
    to the correct answer. We propose a novel iterative training algorithm that alternates
    between searching for consistent logical forms and maximizing the marginal likelihood
    of the retrieved ones. This training scheme lets us iteratively train models that
    provide guidance to subsequent ones to search for logical forms of increasing
    complexity, thus dealing with the problem of spuriousness. We evaluate these techniques
    on two hard datasets: WikiTableQuestions (WTQ) and Cornell Natural Language Visual
    Reasoning (NLVR), and show that our training algorithm outperforms the previous
    best systems, on WTQ in a comparable setting, and on NLVR with significantly less
    supervision.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1273.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1273.Presentation.pdf
  - filename: https://vimeo.com/361691015
    type: video
    url: https://vimeo.com/361691015
  author:
  - first: Pradeep
    full: Pradeep Dasigi
    id: pradeep-dasigi
    last: Dasigi
  - first: Matt
    full: Matt Gardner
    id: matt-gardner
    last: Gardner
  - first: Shikhar
    full: Shikhar Murty
    id: shikhar-murty
    last: Murty
  - first: Luke
    full: Luke Zettlemoyer
    id: luke-zettlemoyer
    last: Zettlemoyer
  - first: Eduard
    full: Eduard Hovy
    id: eduard-hovy
    last: Hovy
  author_string: Pradeep Dasigi, Matt Gardner, Shikhar Murty, Luke Zettlemoyer, Eduard
    Hovy
  bibkey: dasigi-etal-2019-iterative
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1273
  month: June
  page_first: '2669'
  page_last: '2680'
  pages: "2669\u20132680"
  paper_id: '273'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1273.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1273.jpg
  title: Iterative Search for Weakly Supervised Semantic Parsing
  title_html: Iterative Search for Weakly Supervised Semantic Parsing
  url: https://www.aclweb.org/anthology/N19-1273
  year: '2019'
N19-1274:
  abstract: "We propose a simple, fast, and mostly-unsupervised approach for non-factoid\
    \ question answering (QA) called Alignment over Heterogeneous Embeddings (AHE).\
    \ AHE simply aligns each word in the question and candidate answer with the most\
    \ similar word in the retrieved supporting paragraph, and weighs each alignment\
    \ score with the inverse document frequency of the corresponding question/answer\
    \ term. AHE\u2019s similarity function operates over embeddings that model the\
    \ underlying text at different levels of abstraction: character (FLAIR), word\
    \ (BERT and GloVe), and sentence (InferSent), where the latter is the only supervised\
    \ component in the proposed approach. Despite its simplicity and lack of supervision,\
    \ AHE obtains a new state-of-the-art performance on the \u201CEasy\u201D partition\
    \ of the AI2 Reasoning Challenge (ARC) dataset (64.6% accuracy), top-two performance\
    \ on the \u201CChallenge\u201D partition of ARC (34.1%), and top-three performance\
    \ on the WikiQA dataset (74.08% MRR), outperforming many other complex, supervised\
    \ approaches. Our error analysis indicates that alignments over character, word,\
    \ and sentence embeddings capture substantially different semantic information.\
    \ We exploit this with a simple meta-classifier that learns how much to trust\
    \ the predictions over each representation, which further improves the performance\
    \ of unsupervised AHE."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/361693315
    type: video
    url: https://vimeo.com/361693315
  author:
  - first: Vikas
    full: Vikas Yadav
    id: vikas-yadav
    last: Yadav
  - first: Steven
    full: Steven Bethard
    id: steven-bethard
    last: Bethard
  - first: Mihai
    full: Mihai Surdeanu
    id: mihai-surdeanu
    last: Surdeanu
  author_string: Vikas Yadav, Steven Bethard, Mihai Surdeanu
  bibkey: yadav-etal-2019-alignment
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1274
  month: June
  page_first: '2681'
  page_last: '2691'
  pages: "2681\u20132691"
  paper_id: '274'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1274.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1274.jpg
  title: Alignment over Heterogeneous Embeddings for Question Answering
  title_html: Alignment over Heterogeneous Embeddings for Question Answering
  url: https://www.aclweb.org/anthology/N19-1274
  year: '2019'
N19-1275:
  abstract: 'We introduce a new method to tag Multiword Expressions (MWEs) using a
    linguistically interpretable language-independent deep learning architecture.
    We specifically target discontinuity, an under-explored aspect that poses a significant
    challenge to computational treatment of MWEs. Two neural architectures are explored:
    Graph Convolutional Network (GCN) and multi-head self-attention. GCN leverages
    dependency parse information, and self-attention attends to long-range relations.
    We finally propose a combined model that integrates complementary information
    from both, through a gating mechanism. The experiments on a standard multilingual
    dataset for verbal MWEs show that our model outperforms the baselines not only
    in the case of discontinuous MWEs but also in overall F-score.'
  address: Minneapolis, Minnesota
  author:
  - first: Omid
    full: Omid Rohanian
    id: omid-rohanian
    last: Rohanian
  - first: Shiva
    full: Shiva Taslimipoor
    id: shiva-taslimipoor
    last: Taslimipoor
  - first: Samaneh
    full: Samaneh Kouchaki
    id: samaneh-kouchaki
    last: Kouchaki
  - first: Le An
    full: Le An Ha
    id: le-an-ha
    last: Ha
  - first: Ruslan
    full: Ruslan Mitkov
    id: ruslan-mitkov
    last: Mitkov
  author_string: Omid Rohanian, Shiva Taslimipoor, Samaneh Kouchaki, Le An Ha, Ruslan
    Mitkov
  bibkey: rohanian-etal-2019-bridging
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1275
  month: June
  page_first: '2692'
  page_last: '2698'
  pages: "2692\u20132698"
  paper_id: '275'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1275.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1275.jpg
  title: 'Bridging the Gap: Attending to Discontinuity in Identification of Multiword
    Expressions'
  title_html: '<span class="acl-fixed-case">B</span>ridging the Gap: <span class="acl-fixed-case">A</span>ttending
    to Discontinuity in Identification of Multiword Expressions'
  url: https://www.aclweb.org/anthology/N19-1275
  year: '2019'
N19-1276:
  abstract: Neural network models have been actively applied to word segmentation,
    especially Chinese, because of the ability to minimize the effort in feature engineering.
    Typical segmentation models are categorized as character-based, for conducting
    exact inference, or word-based, for utilizing word-level information. We propose
    a character-based model utilizing word information to leverage the advantages
    of both types of models. Our model learns the importance of multiple candidate
    words for a character on the basis of an attention mechanism, and makes use of
    it for segmentation decisions. The experimental results show that our model achieves
    better performance than the state-of-the-art models on both Japanese and Chinese
    benchmark datasets.
  address: Minneapolis, Minnesota
  author:
  - first: Shohei
    full: Shohei Higashiyama
    id: shohei-higashiyama
    last: Higashiyama
  - first: Masao
    full: Masao Utiyama
    id: masao-utiyama
    last: Utiyama
  - first: Eiichiro
    full: Eiichiro Sumita
    id: eiichiro-sumita
    last: Sumita
  - first: Masao
    full: Masao Ideuchi
    id: masao-ideuchi
    last: Ideuchi
  - first: Yoshiaki
    full: Yoshiaki Oida
    id: yoshiaki-oida
    last: Oida
  - first: Yohei
    full: Yohei Sakamoto
    id: yohei-sakamoto
    last: Sakamoto
  - first: Isaac
    full: Isaac Okada
    id: isaac-okada
    last: Okada
  author_string: Shohei Higashiyama, Masao Utiyama, Eiichiro Sumita, Masao Ideuchi,
    Yoshiaki Oida, Yohei Sakamoto, Isaac Okada
  bibkey: higashiyama-etal-2019-incorporating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1276
  month: June
  page_first: '2699'
  page_last: '2709'
  pages: "2699\u20132709"
  paper_id: '276'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1276.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1276.jpg
  title: Incorporating Word Attention into Character-Based Word Segmentation
  title_html: Incorporating Word Attention into Character-Based Word Segmentation
  url: https://www.aclweb.org/anthology/N19-1276
  year: '2019'
N19-1277:
  abstract: 'Chinese is a logographic writing system, and the shape of Chinese characters
    contain rich syntactic and semantic information. In this paper, we propose a model
    to learn Chinese word embeddings via three-level composition: (1) a convolutional
    neural network to extract the intra-character compositionality from the visual
    shape of a character; (2) a recurrent neural network with self-attention to compose
    character representation into word embeddings; (3) the Skip-Gram framework to
    capture non-compositionality directly from the contextual information. Evaluations
    demonstrate the superior performance of our model on four tasks: word similarity,
    sentiment analysis, named entity recognition and part-of-speech tagging.'
  address: Minneapolis, Minnesota
  author:
  - first: Chi
    full: Chi Sun
    id: chi-sun
    last: Sun
  - first: Xipeng
    full: Xipeng Qiu
    id: xipeng-qiu
    last: Qiu
  - first: Xuanjing
    full: Xuanjing Huang
    id: xuan-jing-huang
    last: Huang
  author_string: Chi Sun, Xipeng Qiu, Xuanjing Huang
  bibkey: sun-etal-2019-vcwe
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1277
  month: June
  page_first: '2710'
  page_last: '2719'
  pages: "2710\u20132719"
  paper_id: '277'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1277.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1277.jpg
  title: 'VCWE: Visual Character-Enhanced Word Embeddings'
  title_html: '<span class="acl-fixed-case">VCWE</span>: Visual Character-Enhanced
    Word Embeddings'
  url: https://www.aclweb.org/anthology/N19-1277
  year: '2019'
N19-1278:
  abstract: We investigate subword information for Chinese word segmentation, by integrating
    sub word embeddings trained using byte-pair encoding into a Lattice LSTM (LaLSTM)
    network over a character sequence. Experiments on standard benchmark show that
    subword information brings significant gains over strong character-based segmentation
    models. To our knowledge, this is the first research on the effectiveness of subwords
    on neural word segmentation.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1278.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1278.Supplementary.pdf
  author:
  - first: Jie
    full: Jie Yang
    id: jie-yang
    last: Yang
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  - first: Shuailong
    full: Shuailong Liang
    id: shuailong-liang
    last: Liang
  author_string: Jie Yang, Yue Zhang, Shuailong Liang
  bibkey: yang-etal-2019-subword
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1278
  month: June
  page_first: '2720'
  page_last: '2725'
  pages: "2720\u20132725"
  paper_id: '278'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1278.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1278.jpg
  title: Subword Encoding in Lattice LSTM for Chinese Word Segmentation
  title_html: Subword Encoding in Lattice <span class="acl-fixed-case">LSTM</span>
    for <span class="acl-fixed-case">C</span>hinese Word Segmentation
  url: https://www.aclweb.org/anthology/N19-1278
  year: '2019'
N19-1279:
  abstract: Cross-domain Chinese Word Segmentation (CWS) remains a challenge despite
    recent progress in neural-based CWS. The limited amount of annotated data in the
    target domain has been the key obstacle to a satisfactory performance. In this
    paper, we propose a semi-supervised word-based approach to improving cross-domain
    CWS given a baseline segmenter. Particularly, our model only deploys word embeddings
    trained on raw text in the target domain, discarding complex hand-crafted features
    and domain-specific dictionaries. Innovative subsampling and negative sampling
    methods are proposed to derive word embeddings optimized for CWS. We conduct experiments
    on five datasets in special domains, covering domains in novels, medicine, and
    patent. Results show that our model can obviously improve cross-domain CWS, especially
    in the segmentation of domain-specific noun entities. The word F-measure increases
    by over 3.0% on four datasets, outperforming state-of-the-art semi-supervised
    and unsupervised cross-domain CWS approaches with a large margin. We make our
    data and code available on Github.
  address: Minneapolis, Minnesota
  author:
  - first: Yuxiao
    full: Yuxiao Ye
    id: yuxiao-ye
    last: Ye
  - first: Weikang
    full: Weikang Li
    id: weigang-li
    last: Li
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  - first: Likun
    full: Likun Qiu
    id: likun-qiu
    last: Qiu
  - first: Jian
    full: Jian Sun
    id: jian-sun
    last: Sun
  author_string: Yuxiao Ye, Weikang Li, Yue Zhang, Likun Qiu, Jian Sun
  bibkey: ye-etal-2019-improving
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1279
  month: June
  page_first: '2726'
  page_last: '2735'
  pages: "2726\u20132735"
  paper_id: '279'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1279.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1279.jpg
  title: Improving Cross-Domain Chinese Word Segmentation with Word Embeddings
  title_html: Improving Cross-Domain <span class="acl-fixed-case">C</span>hinese Word
    Segmentation with Word Embeddings
  url: https://www.aclweb.org/anthology/N19-1279
  year: '2019'
N19-1280:
  abstract: 'Character-level models of tokens have been shown to be effective at dealing
    with within-token noise and out-of-vocabulary words. However, they often still
    rely on correct token boundaries. In this paper, we propose to eliminate the need
    for tokenizers with an end-to-end character-level semi-Markov conditional random
    field. It uses neural networks for its character and segment representations.
    We demonstrate its effectiveness in multilingual settings and when token boundaries
    are noisy: It matches state-of-the-art part-of-speech taggers for various languages
    and significantly outperforms them on a noisy English version of a benchmark dataset.
    Our code and the noisy dataset are publicly available at http://cistern.cis.lmu.de/semiCRF.'
  address: Minneapolis, Minnesota
  author:
  - first: Apostolos
    full: Apostolos Kemos
    id: apostolos-kemos
    last: Kemos
  - first: Heike
    full: Heike Adel
    id: heike-adel
    last: Adel
  - first: Hinrich
    full: "Hinrich Sch\xFCtze"
    id: hinrich-schutze
    last: "Sch\xFCtze"
  author_string: "Apostolos Kemos, Heike Adel, Hinrich Sch\xFCtze"
  bibkey: kemos-etal-2019-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1280
  month: June
  page_first: '2736'
  page_last: '2743'
  pages: "2736\u20132743"
  paper_id: '280'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1280.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1280.jpg
  title: Neural Semi-Markov Conditional Random Fields for Robust Character-Based Part-of-Speech
    Tagging
  title_html: Neural Semi-<span class="acl-fixed-case">M</span>arkov Conditional Random
    Fields for Robust Character-Based Part-of-Speech Tagging
  url: https://www.aclweb.org/anthology/N19-1280
  year: '2019'
N19-1281:
  abstract: 'For languages without natural word boundaries, like Japanese and Chinese,
    word segmentation is a prerequisite for downstream analysis. For Japanese, segmentation
    is often done jointly with part of speech tagging, and this process is usually
    referred to as morphological analysis. Morphological analyzers are trained on
    data hand-annotated with segmentation boundaries and part of speech tags. A segmentation
    dictionary or character n-gram information is also provided as additional inputs
    to the model. Incorporating this extra information makes models large. Modern
    neural morphological analyzers can consume gigabytes of memory. We propose a compact
    alternative to these cumbersome approaches which do not rely on any externally
    provided n-gram or word representations. The model uses only unigram character
    embeddings, encodes them using either stacked bi-LSTM or a self-attention network,
    and independently infers both segmentation and part of speech information. The
    model is trained in an end-to-end and semi-supervised fashion, on labels produced
    by a state-of-the-art analyzer. We demonstrate that the proposed technique rivals
    performance of a previous dictionary-based state-of-the-art approach and can even
    surpass it when training with the combination of human-annotated and automatically-annotated
    data. Our model itself is significantly smaller than the dictionary-based one:
    it uses less than 15 megabytes of space.'
  address: Minneapolis, Minnesota
  author:
  - first: Arseny
    full: Arseny Tolmachev
    id: arseny-tolmachev
    last: Tolmachev
  - first: Daisuke
    full: Daisuke Kawahara
    id: daisuke-kawahara
    last: Kawahara
  - first: Sadao
    full: Sadao Kurohashi
    id: sadao-kurohashi
    last: Kurohashi
  author_string: Arseny Tolmachev, Daisuke Kawahara, Sadao Kurohashi
  bibkey: tolmachev-etal-2019-shrinking
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1281
  month: June
  page_first: '2744'
  page_last: '2755'
  pages: "2744\u20132755"
  paper_id: '281'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1281.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1281.jpg
  title: Shrinking Japanese Morphological Analyzers With Neural Networks and Semi-supervised
    Learning
  title_html: Shrinking <span class="acl-fixed-case">J</span>apanese Morphological
    Analyzers With Neural Networks and Semi-supervised Learning
  url: https://www.aclweb.org/anthology/N19-1281
  year: '2019'
N19-1282:
  abstract: This paper studies the performance of a neural self-attentive parser on
    transcribed speech. Speech presents parsing challenges that do not appear in written
    text, such as the lack of punctuation and the presence of speech disfluencies
    (including filled pauses, repetitions, corrections, etc.). Disfluencies are especially
    problematic for conventional syntactic parsers, which typically fail to find any
    EDITED disfluency nodes at all. This motivated the development of special disfluency
    detection systems, and special mechanisms added to parsers specifically to handle
    disfluencies. However, we show here that neural parsers can find EDITED disfluency
    nodes, and the best neural parsers find them with an accuracy surpassing that
    of specialized disfluency detection systems, thus making these specialized mechanisms
    unnecessary. This paper also investigates a modified loss function that puts more
    weight on EDITED nodes. It also describes tree-transformations that simplify the
    disfluency detection task by providing alternative encodings of disfluencies and
    syntactic information.
  address: Minneapolis, Minnesota
  author:
  - first: Paria
    full: Paria Jamshid Lou
    id: paria-jamshid-lou
    last: Jamshid Lou
  - first: Yufei
    full: Yufei Wang
    id: yufei-wang
    last: Wang
  - first: Mark
    full: Mark Johnson
    id: mark-johnson
    last: Johnson
  author_string: Paria Jamshid Lou, Yufei Wang, Mark Johnson
  bibkey: jamshid-lou-etal-2019-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1282
  month: June
  page_first: '2756'
  page_last: '2765'
  pages: "2756\u20132765"
  paper_id: '282'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1282.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1282.jpg
  title: Neural Constituency Parsing of Speech Transcripts
  title_html: Neural Constituency Parsing of Speech Transcripts
  url: https://www.aclweb.org/anthology/N19-1282
  year: '2019'
N19-1283:
  abstract: Conversational context information, higher-level knowledge that spans
    across sentences, can help to recognize a long conversation. However, existing
    speech recognition models are typically built at a sentence level, and thus it
    may not capture important conversational context information. The recent progress
    in end-to-end speech recognition enables integrating context with other available
    information (e.g., acoustic, linguistic resources) and directly recognizing words
    from speech. In this work, we present a direct acoustic-to-word, end-to-end speech
    recognition model capable of utilizing the conversational context to better process
    long conversations. We evaluate our proposed approach on the Switchboard conversational
    speech corpus and show that our system outperforms a standard end-to-end speech
    recognition system.
  address: Minneapolis, Minnesota
  author:
  - first: Suyoun
    full: Suyoun Kim
    id: suyoun-kim
    last: Kim
  - first: Florian
    full: Florian Metze
    id: florian-metze
    last: Metze
  author_string: Suyoun Kim, Florian Metze
  bibkey: kim-metze-2019-acoustic
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1283
  month: June
  page_first: '2766'
  page_last: '2771'
  pages: "2766\u20132771"
  paper_id: '283'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1283.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1283.jpg
  title: Acoustic-to-Word Models with Conversational Context Information
  title_html: Acoustic-to-Word Models with Conversational Context Information
  url: https://www.aclweb.org/anthology/N19-1283
  year: '2019'
N19-1284:
  abstract: Individual differences in speakers are reflected in their language use
    as well as in their interests and opinions. Characterizing these differences can
    be useful in human-computer interaction, as well as analysis of human-human conversations.
    In this work, we introduce a neural model for learning a dynamically updated speaker
    embedding in a conversational context. Initial model training is unsupervised,
    using context-sensitive language generation as an objective, with the context
    being the conversation history. Further fine-tuning can leverage task-dependent
    supervised training. The learned neural representation of speakers is shown to
    be useful for content ranking in a socialbot and dialog act prediction in human-human
    conversations.
  address: Minneapolis, Minnesota
  author:
  - first: Hao
    full: Hao Cheng
    id: hao-cheng
    last: Cheng
  - first: Hao
    full: Hao Fang
    id: hao-fang
    last: Fang
  - first: Mari
    full: Mari Ostendorf
    id: mari-ostendorf
    last: Ostendorf
  author_string: Hao Cheng, Hao Fang, Mari Ostendorf
  bibkey: cheng-etal-2019-dynamic
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1284
  month: June
  page_first: '2772'
  page_last: '2785'
  pages: "2772\u20132785"
  paper_id: '284'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1284.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1284.jpg
  title: A Dynamic Speaker Model for Conversational Interactions
  title_html: A Dynamic Speaker Model for Conversational Interactions
  url: https://www.aclweb.org/anthology/N19-1284
  year: '2019'
N19-1285:
  abstract: "Spoken language translation applications for speech suffer due to conversational\
    \ speech phenomena, particularly the presence of disfluencies. With the rise of\
    \ end-to-end speech translation models, processing steps such as disfluency removal\
    \ that were previously an intermediate step between speech recognition and machine\
    \ translation need to be incorporated into model architectures. We use a sequence-to-sequence\
    \ model to translate from noisy, disfluent speech to fluent text with disfluencies\
    \ removed using the recently collected \u2018copy-edited\u2019 references for\
    \ the Fisher Spanish-English dataset. We are able to directly generate fluent\
    \ translations and introduce considerations about how to evaluate success on this\
    \ task. This work provides a baseline for a new task, implicitly removing disfluencies\
    \ in end-to-end translation of conversational speech."
  address: Minneapolis, Minnesota
  author:
  - first: Elizabeth
    full: Elizabeth Salesky
    id: elizabeth-salesky
    last: Salesky
  - first: Matthias
    full: Matthias Sperber
    id: matthias-sperber
    last: Sperber
  - first: Alexander
    full: Alexander Waibel
    id: alex-waibel
    last: Waibel
  author_string: Elizabeth Salesky, Matthias Sperber, Alexander Waibel
  bibkey: salesky-etal-2019-fluent
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1285
  month: June
  page_first: '2786'
  page_last: '2792'
  pages: "2786\u20132792"
  paper_id: '285'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1285.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1285.jpg
  title: Fluent Translations from Disfluent Speech in End-to-End Speech Translation
  title_html: Fluent Translations from Disfluent Speech in End-to-End Speech Translation
  url: https://www.aclweb.org/anthology/N19-1285
  year: '2019'
N19-1286:
  abstract: Recently, relation classification has gained much success by exploiting
    deep neural networks. In this paper, we propose a new model effectively combining
    Segment-level Attention-based Convolutional Neural Networks (SACNNs) and Dependency-based
    Recurrent Neural Networks (DepRNNs). While SACNNs allow the model to selectively
    focus on the important information segment from the raw sequence, DepRNNs help
    to handle the long-distance relations from the shortest dependency path of relation
    entities. Experiments on the SemEval-2010 Task 8 dataset show that our model is
    comparable to the state-of-the-art without using any external lexical features.
  address: Minneapolis, Minnesota
  author:
  - first: Van-Hien
    full: Van-Hien Tran
    id: van-hien-tran
    last: Tran
  - first: Van-Thuy
    full: Van-Thuy Phi
    id: van-thuy-phi
    last: Phi
  - first: Hiroyuki
    full: Hiroyuki Shindo
    id: hiroyuki-shindo
    last: Shindo
  - first: Yuji
    full: Yuji Matsumoto
    id: yuji-matsumoto
    last: Matsumoto
  author_string: Van-Hien Tran, Van-Thuy Phi, Hiroyuki Shindo, Yuji Matsumoto
  bibkey: tran-etal-2019-relation
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1286
  month: June
  page_first: '2793'
  page_last: '2798'
  pages: "2793\u20132798"
  paper_id: '286'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1286.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1286.jpg
  title: Relation Classification Using Segment-Level Attention-based CNN and Dependency-based
    RNN
  title_html: Relation Classification Using Segment-Level Attention-based <span class="acl-fixed-case">CNN</span>
    and Dependency-based <span class="acl-fixed-case">RNN</span>
  url: https://www.aclweb.org/anthology/N19-1286
  year: '2019'
N19-1287:
  abstract: Document-level event factuality identification is an important subtask
    in event factuality and is crucial for discourse understanding in Natural Language
    Processing (NLP). Previous studies mainly suffer from the scarcity of suitable
    corpus and effective methods. To solve these two issues, we first construct a
    corpus annotated with both document- and sentence-level event factuality information
    on both English and Chinese texts. Then we present an LSTM neural network based
    on adversarial training with both intra- and inter-sequence attentions to identify
    document-level event factuality. Experimental results show that our neural network
    model can outperform various baselines on the constructed corpus.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1287.Software.zip
    type: software
    url: https://www.aclweb.org/anthology/attachments/N19-1287.Software.zip
  author:
  - first: Zhong
    full: Zhong Qian
    id: zhong-qian
    last: Qian
  - first: Peifeng
    full: Peifeng Li
    id: peifeng-li
    last: Li
  - first: Qiaoming
    full: Qiaoming Zhu
    id: qiaoming-zhu
    last: Zhu
  - first: Guodong
    full: Guodong Zhou
    id: guodong-zhou
    last: Zhou
  author_string: Zhong Qian, Peifeng Li, Qiaoming Zhu, Guodong Zhou
  bibkey: qian-etal-2019-document
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1287
  month: June
  page_first: '2799'
  page_last: '2809'
  pages: "2799\u20132809"
  paper_id: '287'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1287.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1287.jpg
  title: Document-Level Event Factuality Identification via Adversarial Neural Network
  title_html: Document-Level Event Factuality Identification via Adversarial Neural
    Network
  url: https://www.aclweb.org/anthology/N19-1287
  year: '2019'
N19-1288:
  abstract: This paper presents a neural relation extraction method to deal with the
    noisy training data generated by distant supervision. Previous studies mainly
    focus on sentence-level de-noising by designing neural networks with intra-bag
    attentions. In this paper, both intra-bag and inter-bag attentions are considered
    in order to deal with the noise at sentence-level and bag-level respectively.
    First, relation-aware bag representations are calculated by weighting sentence
    embeddings using intra-bag attentions. Here, each possible relation is utilized
    as the query for attention calculation instead of only using the target relation
    in conventional methods. Furthermore, the representation of a group of bags in
    the training set which share the same relation label is calculated by weighting
    bag representations using a similarity-based inter-bag attention module. Finally,
    a bag group is utilized as a training sample when building our relation extractor.
    Experimental results on the New York Times dataset demonstrate the effectiveness
    of our proposed intra-bag and inter-bag attention modules. Our method also achieves
    better relation extraction accuracy than state-of-the-art methods on this dataset.
  address: Minneapolis, Minnesota
  author:
  - first: Zhi-Xiu
    full: Zhi-Xiu Ye
    id: zhi-xiu-ye
    last: Ye
  - first: Zhen-Hua
    full: Zhen-Hua Ling
    id: zhen-hua-ling
    last: Ling
  author_string: Zhi-Xiu Ye, Zhen-Hua Ling
  bibkey: ye-ling-2019-distant
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1288
  month: June
  page_first: '2810'
  page_last: '2819'
  pages: "2810\u20132819"
  paper_id: '288'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1288.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1288.jpg
  title: Distant Supervision Relation Extraction with Intra-Bag and Inter-Bag Attentions
  title_html: Distant Supervision Relation Extraction with Intra-Bag and Inter-Bag
    Attentions
  url: https://www.aclweb.org/anthology/N19-1288
  year: '2019'
N19-1289:
  abstract: 'Extreme Multi-label classification (XML) is an important yet challenging
    machine learning task, that assigns to each instance its most relevant candidate
    labels from an extremely large label collection, where the numbers of labels,
    features and instances could be thousands or millions. XML is more and more on
    demand in the Internet industries, accompanied with the increasing business scale
    / scope and data accumulation. The extremely large label collections yield challenges
    such as computational complexity, inter-label dependency and noisy labeling. Many
    methods have been proposed to tackle these challenges, based on different mathematical
    formulations. In this paper, we propose a deep learning XML method, with a word-vector-based
    self-attention, followed by a ranking-based AutoEncoder architecture. The proposed
    method has three major advantages: 1) the autoencoder simultaneously considers
    the inter-label dependencies and the feature-label dependencies, by projecting
    labels and features onto a common embedding space; 2) the ranking loss not only
    improves the training efficiency and accuracy but also can be extended to handle
    noisy labeled data; 3) the efficient attention mechanism improves feature representation
    by highlighting feature importance. Experimental results on benchmark datasets
    show the proposed method is competitive to state-of-the-art methods.'
  address: Minneapolis, Minnesota
  author:
  - first: Bingyu
    full: Bingyu Wang
    id: bingyu-wang
    last: Wang
  - first: Li
    full: Li Chen
    id: li-chen
    last: Chen
  - first: Wei
    full: Wei Sun
    id: wei-sun
    last: Sun
  - first: Kechen
    full: Kechen Qin
    id: kechen-qin
    last: Qin
  - first: Kefeng
    full: Kefeng Li
    id: kefeng-li
    last: Li
  - first: Hui
    full: Hui Zhou
    id: hui-zhou
    last: Zhou
  author_string: Bingyu Wang, Li Chen, Wei Sun, Kechen Qin, Kefeng Li, Hui Zhou
  bibkey: wang-etal-2019-ranking
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1289
  month: June
  page_first: '2820'
  page_last: '2830'
  pages: "2820\u20132830"
  paper_id: '289'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1289.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1289.jpg
  title: Ranking-Based Autoencoder for Extreme Multi-label Classification
  title_html: Ranking-Based Autoencoder for Extreme Multi-label Classification
  url: https://www.aclweb.org/anthology/N19-1289
  year: '2019'
N19-1290:
  abstract: This paper provides a new way to improve the efficiency of the REINFORCE
    training process. We apply it to the task of instance selection in distant supervision.
    Modeling the instance selection in one bag as a sequential decision process, a
    reinforcement learning agent is trained to determine whether an instance is valuable
    or not and construct a new bag with less noisy instances. However unbiased methods,
    such as REINFORCE, could usually take much time to train. This paper adopts posterior
    regularization (PR) to integrate some domain-specific rules in instance selection
    using REINFORCE. As the experiment results show, this method remarkably improves
    the performance of the relation classifier trained on cleaned distant supervision
    dataset as well as the efficiency of the REINFORCE training.
  address: Minneapolis, Minnesota
  author:
  - first: Qi
    full: Qi Zhang
    id: qi-zhang
    last: Zhang
  - first: Siliang
    full: Siliang Tang
    id: siliang-tang
    last: Tang
  - first: Xiang
    full: Xiang Ren
    id: xiang-ren
    last: Ren
  - first: Fei
    full: Fei Wu
    id: fei-wu
    last: Wu
  - first: Shiliang
    full: Shiliang Pu
    id: shiliang-pu
    last: Pu
  - first: Yueting
    full: Yueting Zhuang
    id: yueting-zhuang
    last: Zhuang
  author_string: Qi Zhang, Siliang Tang, Xiang Ren, Fei Wu, Shiliang Pu, Yueting Zhuang
  bibkey: zhang-etal-2019-posterior
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1290
  month: June
  page_first: '2831'
  page_last: '2835'
  pages: "2831\u20132835"
  paper_id: '290'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1290.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1290.jpg
  title: Posterior-regularized REINFORCE for Instance Selection in Distant Supervision
  title_html: Posterior-regularized <span class="acl-fixed-case">REINFORCE</span>
    for Instance Selection in Distant Supervision
  url: https://www.aclweb.org/anthology/N19-1290
  year: '2019'
N19-1291:
  abstract: "The bigger the corpus, the more topics it can potentially support. To\
    \ truly make full use of massive text corpora, a topic model inference algorithm\
    \ must therefore scale efficiently in 1) documents and 2) topics, while 3) achieving\
    \ accurate inference. Previous methods have achieved two out of three of these\
    \ criteria simultaneously, but never all three at once. In this paper, we develop\
    \ an online inference algorithm for topic models which leverages stochasticity\
    \ to scale well in the number of documents, sparsity to scale well in the number\
    \ of topics, and which operates in the collapsed representation of the topic model\
    \ for improved accuracy and run-time performance. We use a Monte Carlo inner loop\
    \ in the online setting to approximate the collapsed variational Bayes updates\
    \ in a sparse and efficient way, which we accomplish via the MetropolisHastings\
    \ Walker method. We showcase our algorithm on LDA and the recently proposed mixed\
    \ membership skip-gram topic model. Our method requires only amortized O(kd) computation\
    \ per word token instead of computation per word token instead of O(K) operations,\
    \ where the number of topics occurring for a particular document operations, where\
    \ the number of topics occurring for a particular document kd\u226A the total\
    \ number of topics in the corpus the total number of topics in the corpus K, to\
    \ converge to a high-quality solution., to converge to a high-quality solution."
  address: Minneapolis, Minnesota
  author:
  - first: Rashidul
    full: Rashidul Islam
    id: rashidul-islam
    last: Islam
  - first: James
    full: James Foulds
    id: james-foulds
    last: Foulds
  author_string: Rashidul Islam, James Foulds
  bibkey: islam-foulds-2019-scalable
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1291
  month: June
  page_first: '2836'
  page_last: '2845'
  pages: "2836\u20132845"
  paper_id: '291'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1291.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1291.jpg
  title: Scalable Collapsed Inference for High-Dimensional Topic Models
  title_html: Scalable Collapsed Inference for High-Dimensional Topic Models
  url: https://www.aclweb.org/anthology/N19-1291
  year: '2019'
N19-1292:
  abstract: In this paper, we present a novel integrated approach for keyphrase generation
    (KG). Unlike previous works which are purely extractive or generative, we first
    propose a new multi-task learning framework that jointly learns an extractive
    model and a generative model. Besides extracting keyphrases, the output of the
    extractive model is also employed to rectify the copy probability distribution
    of the generative model, such that the generative model can better identify important
    contents from the given document. Moreover, we retrieve similar documents with
    the given document from training data and use their associated keyphrases as external
    knowledge for the generative model to produce more accurate keyphrases. For further
    exploiting the power of extraction and retrieval, we propose a neural-based merging
    module to combine and re-rank the predicted keyphrases from the enhanced generative
    model, the extractive model, and the retrieved keyphrases. Experiments on the
    five KG benchmarks demonstrate that our integrated approach outperforms the state-of-the-art
    methods.
  address: Minneapolis, Minnesota
  author:
  - first: Wang
    full: Wang Chen
    id: wang-chen
    last: Chen
  - first: Hou Pong
    full: Hou Pong Chan
    id: hou-pong-chan
    last: Chan
  - first: Piji
    full: Piji Li
    id: piji-li
    last: Li
  - first: Lidong
    full: Lidong Bing
    id: lidong-bing
    last: Bing
  - first: Irwin
    full: Irwin King
    id: irwin-king
    last: King
  author_string: Wang Chen, Hou Pong Chan, Piji Li, Lidong Bing, Irwin King
  bibkey: chen-etal-2019-integrated
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1292
  month: June
  page_first: '2846'
  page_last: '2856'
  pages: "2846\u20132856"
  paper_id: '292'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1292.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1292.jpg
  title: An Integrated Approach for Keyphrase Generation via Exploring the Power of
    Retrieval and Extraction
  title_html: An Integrated Approach for Keyphrase Generation via Exploring the Power
    of Retrieval and Extraction
  url: https://www.aclweb.org/anthology/N19-1292
  year: '2019'
N19-1293:
  abstract: Text analytics is a useful tool for studying malware behavior and tracking
    emerging threats. The task of automated malware attribute identification based
    on cybersecurity texts is very challenging due to a large number of malware attribute
    labels and a small number of training instances. In this paper, we propose a novel
    feature learning method to leverage diverse knowledge sources such as small amount
    of human annotations, unlabeled text and specifications about malware attribute
    labels. Our evaluation has demonstrated the effectiveness of our method over the
    state-of-the-art malware attribute prediction systems.
  address: Minneapolis, Minnesota
  author:
  - first: Arpita
    full: Arpita Roy
    id: arpita-roy
    last: Roy
  - first: Youngja
    full: Youngja Park
    id: youngja-park
    last: Park
  - first: Shimei
    full: Shimei Pan
    id: shimei-pan
    last: Pan
  author_string: Arpita Roy, Youngja Park, Shimei Pan
  bibkey: roy-etal-2019-predicting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1293
  month: June
  page_first: '2857'
  page_last: '2861'
  pages: "2857\u20132861"
  paper_id: '293'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1293.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1293.jpg
  title: Predicting Malware Attributes from Cybersecurity Texts
  title_html: Predicting Malware Attributes from Cybersecurity Texts
  url: https://www.aclweb.org/anthology/N19-1293
  year: '2019'
N19-1294:
  abstract: Recently, distant supervision has gained great success on Fine-grained
    Entity Typing (FET). Despite its efficiency in reducing manual labeling efforts,
    it also brings the challenge of dealing with false entity type labels, as distant
    supervision assigns labels in a context-agnostic manner. Existing works alleviated
    this issue with partial-label loss, but usually suffer from confirmation bias,
    which means the classifier fit a pseudo data distribution given by itself. In
    this work, we propose to regularize distantly supervised models with Compact Latent
    Space Clustering (CLSC) to bypass this problem and effectively utilize noisy data
    yet. Our proposed method first dynamically constructs a similarity graph of different
    entity mentions; infer the labels of noisy instances via label propagation. Based
    on the inferred labels, mention embeddings are updated accordingly to encourage
    entity mentions with close semantics to form a compact cluster in the embedding
    space, thus leading to better classification performance. Extensive experiments
    on standard benchmarks show that our CLSC model consistently outperforms state-of-the-art
    distantly supervised entity typing systems by a significant margin.
  address: Minneapolis, Minnesota
  author:
  - first: Bo
    full: Bo Chen
    id: bo-chen
    last: Chen
  - first: Xiaotao
    full: Xiaotao Gu
    id: xiaotao-gu
    last: Gu
  - first: Yufeng
    full: Yufeng Hu
    id: yufeng-hu
    last: Hu
  - first: Siliang
    full: Siliang Tang
    id: siliang-tang
    last: Tang
  - first: Guoping
    full: Guoping Hu
    id: guoping-hu
    last: Hu
  - first: Yueting
    full: Yueting Zhuang
    id: yueting-zhuang
    last: Zhuang
  - first: Xiang
    full: Xiang Ren
    id: xiang-ren
    last: Ren
  author_string: Bo Chen, Xiaotao Gu, Yufeng Hu, Siliang Tang, Guoping Hu, Yueting
    Zhuang, Xiang Ren
  bibkey: chen-etal-2019-improving-distantly
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1294
  month: June
  page_first: '2862'
  page_last: '2872'
  pages: "2862\u20132872"
  paper_id: '294'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1294.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1294.jpg
  title: Improving Distantly-supervised Entity Typing with Compact Latent Space Clustering
  title_html: Improving Distantly-supervised Entity Typing with Compact Latent Space
    Clustering
  url: https://www.aclweb.org/anthology/N19-1294
  year: '2019'
N19-1295:
  abstract: "When constructing models that learn from noisy labels produced by multiple\
    \ annotators, it is important to accurately estimate the reliability of annotators.\
    \ Annotators may provide labels of inconsistent quality due to their varying expertise\
    \ and reliability in a domain. Previous studies have mostly focused on estimating\
    \ each annotator\u2019s overall reliability on the entire annotation task. However,\
    \ in practice, the reliability of an annotator may depend on each specific instance.\
    \ Only a limited number of studies have investigated modelling per-instance reliability\
    \ and these only considered binary labels. In this paper, we propose an unsupervised\
    \ model which can handle both binary and multi-class labels. It can automatically\
    \ estimate the per-instance reliability of each annotator and the correct label\
    \ for each instance. We specify our model as a probabilistic model which incorporates\
    \ neural networks to model the dependency between latent variables and instances.\
    \ For evaluation, the proposed method is applied to both synthetic and real data,\
    \ including two labelling tasks: text classification and textual entailment. Experimental\
    \ results demonstrate our novel method can not only accurately estimate the reliability\
    \ of annotators across different instances, but also achieve superior performance\
    \ in predicting the correct labels and detecting the least reliable annotators\
    \ compared to state-of-the-art baselines."
  address: Minneapolis, Minnesota
  author:
  - first: Maolin
    full: Maolin Li
    id: maolin-li
    last: Li
  - first: Arvid
    full: "Arvid Fahlstr\xF6m Myrman"
    id: arvid-fahlstrom-myrman
    last: "Fahlstr\xF6m Myrman"
  - first: Tingting
    full: Tingting Mu
    id: tingting-mu
    last: Mu
  - first: Sophia
    full: Sophia Ananiadou
    id: sophia-ananiadou
    last: Ananiadou
  author_string: "Maolin Li, Arvid Fahlstr\xF6m Myrman, Tingting Mu, Sophia Ananiadou"
  bibkey: li-etal-2019-modelling
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1295
  month: June
  page_first: '2873'
  page_last: '2883'
  pages: "2873\u20132883"
  paper_id: '295'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1295.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1295.jpg
  title: Modelling Instance-Level Annotator Reliability for Natural Language Labelling
    Tasks
  title_html: Modelling Instance-Level Annotator Reliability for Natural Language
    Labelling Tasks
  url: https://www.aclweb.org/anthology/N19-1295
  year: '2019'
N19-1296:
  abstract: This paper explores a new natural languageprocessing task, review-driven
    multi-label musicstyle classification. This task requires systemsto identify multiple
    styles of music basedon its reviews on websites. The biggest challengelies in
    the complicated relations of musicstyles. To tackle this problem, we proposea
    novel deep learning approach to automaticallylearn and exploit style correlations.Experiment
    results show that our approachachieves large improvements over baselines onthe
    proposed dataset. Furthermore, the visualizedanalysis shows that our approach
    performswell in capturing style correlations.
  address: Minneapolis, Minnesota
  author:
  - first: Guangxiang
    full: Guangxiang Zhao
    id: guangxiang-zhao
    last: Zhao
  - first: Jingjing
    full: Jingjing Xu
    id: jingjing-xu
    last: Xu
  - first: Qi
    full: Qi Zeng
    id: qi-zeng
    last: Zeng
  - first: Xuancheng
    full: Xuancheng Ren
    id: xuancheng-ren
    last: Ren
  - first: Xu
    full: Xu Sun
    id: xu-sun
    last: Sun
  author_string: Guangxiang Zhao, Jingjing Xu, Qi Zeng, Xuancheng Ren, Xu Sun
  bibkey: zhao-etal-2019-review
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1296
  month: June
  page_first: '2884'
  page_last: '2891'
  pages: "2884\u20132891"
  paper_id: '296'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1296.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1296.jpg
  title: Review-Driven Multi-Label Music Style Classification by Exploiting Style
    Correlations
  title_html: Review-Driven Multi-Label Music Style Classification by Exploiting Style
    Correlations
  url: https://www.aclweb.org/anthology/N19-1296
  year: '2019'
N19-1297:
  abstract: During the past few decades, knowledge bases (KBs) have experienced rapid
    growth. Nevertheless, most KBs still suffer from serious incompletion. Researchers
    proposed many tasks such as knowledge base completion and relation prediction
    to help build the representation of KBs. However, there are some issues unsettled
    towards enriching the KBs. Knowledge base completion and relation prediction assume
    that we know two elements of the fact triples and we are going to predict the
    missing one. This assumption is too restricted in practice and prevents it from
    discovering new facts directly. To address this issue, we propose a new task,
    namely, fact discovery from knowledge base. This task only requires that we know
    the head entity and the goal is to discover facts associated with the head entity.
    To tackle this new problem, we propose a novel framework that decomposes the discovery
    problem into several facet discovery components. We also propose a novel auto-encoder
    based facet component to estimate some facets of the fact. Besides, we propose
    a feedback learning component to share the information between each facet. We
    evaluate our framework using a benchmark dataset and the experimental results
    show that our framework achieves promising results. We also conduct an extensive
    analysis of our framework in discovering different kinds of facts. The source
    code of this paper can be obtained from https://github.com/thunlp/FFD.
  address: Minneapolis, Minnesota
  author:
  - first: Zihao
    full: Zihao Fu
    id: zihao-fu
    last: Fu
  - first: Yankai
    full: Yankai Lin
    id: yankai-lin
    last: Lin
  - first: Zhiyuan
    full: Zhiyuan Liu
    id: zhiyuan-liu
    last: Liu
  - first: Wai
    full: Wai Lam
    id: wai-lam
    last: Lam
  author_string: Zihao Fu, Yankai Lin, Zhiyuan Liu, Wai Lam
  bibkey: fu-etal-2019-fact
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1297
  month: June
  page_first: '2892'
  page_last: '2901'
  pages: "2892\u20132901"
  paper_id: '297'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1297.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1297.jpg
  title: Fact Discovery from Knowledge Base via Facet Decomposition
  title_html: Fact Discovery from Knowledge Base via Facet Decomposition
  url: https://www.aclweb.org/anthology/N19-1297
  year: '2019'
N19-1298:
  abstract: To extract the relationship between two entities in a sentence, two common
    approaches are (1) using their shortest dependency path (SDP) and (2) using an
    attention model to capture a context-based representation of the sentence. Each
    approach suffers from its own disadvantage of either missing or redundant information.
    In this work, we propose a novel model that combines the advantages of these two
    approaches. This is based on the basic information in the SDP enhanced with information
    selected by several attention mechanisms with kernel filters, namely RbSP (Richer-but-Smarter
    SDP). To exploit the representation behind the RbSP structure effectively, we
    develop a combined deep neural model with a LSTM network on word sequences and
    a CNN on RbSP. Experimental results on the SemEval-2010 dataset demonstrate improved
    performance over competitive baselines. The data and source code are available
    at https://github.com/catcd/RbSP.
  address: Minneapolis, Minnesota
  author:
  - first: Duy-Cat
    full: Duy-Cat Can
    id: duy-cat-can
    last: Can
  - first: Hoang-Quynh
    full: Hoang-Quynh Le
    id: hoang-quynh-le
    last: Le
  - first: Quang-Thuy
    full: Quang-Thuy Ha
    id: quang-thuy-ha
    last: Ha
  - first: Nigel
    full: Nigel Collier
    id: nigel-collier
    last: Collier
  author_string: Duy-Cat Can, Hoang-Quynh Le, Quang-Thuy Ha, Nigel Collier
  bibkey: can-etal-2019-richer
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1298
  month: June
  page_first: '2902'
  page_last: '2912'
  pages: "2902\u20132912"
  paper_id: '298'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1298.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1298.jpg
  title: A Richer-but-Smarter Shortest Dependency Path with Attentive Augmentation
    for Relation Extraction
  title_html: A Richer-but-Smarter Shortest Dependency Path with Attentive Augmentation
    for Relation Extraction
  url: https://www.aclweb.org/anthology/N19-1298
  year: '2019'
N19-1299:
  abstract: When answering natural language questions over knowledge bases (KBs),
    different question components and KB aspects play different roles. However, most
    existing embedding-based methods for knowledge base question answering (KBQA)
    ignore the subtle inter-relationships between the question and the KB (e.g., entity
    types, relation paths and context). In this work, we propose to directly model
    the two-way flow of interactions between the questions and the KB via a novel
    Bidirectional Attentive Memory Network, called BAMnet. Requiring no external resources
    and only very few hand-crafted features, on the WebQuestions benchmark, our method
    significantly outperforms existing information-retrieval based methods, and remains
    competitive with (hand-crafted) semantic parsing based methods. Also, since we
    use attention mechanisms, our method offers better interpretability compared to
    other baselines.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/356071812
    type: video
    url: https://vimeo.com/356071812
  author:
  - first: Yu
    full: Yu Chen
    id: yu-chen
    last: Chen
  - first: Lingfei
    full: Lingfei Wu
    id: lingfei-wu
    last: Wu
  - first: Mohammed J.
    full: Mohammed J. Zaki
    id: mohammed-j-zaki
    last: Zaki
  author_string: Yu Chen, Lingfei Wu, Mohammed J. Zaki
  bibkey: chen-etal-2019-bidirectional
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1299
  month: June
  page_first: '2913'
  page_last: '2923'
  pages: "2913\u20132923"
  paper_id: '299'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1299.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1299.jpg
  title: Bidirectional Attentive Memory Networks for Question Answering over Knowledge
    Bases
  title_html: Bidirectional Attentive Memory Networks for Question Answering over
    Knowledge Bases
  url: https://www.aclweb.org/anthology/N19-1299
  year: '2019'
N19-1300:
  abstract: "In this paper we study yes/no questions that are naturally occurring\
    \ \u2014 meaning that they are generated in unprompted and unconstrained settings.\
    \ We build a reading comprehension dataset, BoolQ, of such questions, and show\
    \ that they are unexpectedly challenging. They often query for complex, non-factoid\
    \ information, and require difficult entailment-like inference to solve. We also\
    \ explore the effectiveness of a range of transfer learning baselines. We find\
    \ that transferring from entailment data is more effective than transferring from\
    \ paraphrase or extractive QA data, and that it, surprisingly, continues to be\
    \ very beneficial even when starting from massive pre-trained language models\
    \ such as BERT. Our best method trains BERT on MultiNLI and then re-trains it\
    \ on our train set. It achieves 80.4% accuracy compared to 90% accuracy of human\
    \ annotators (and 62% majority-baseline), leaving a significant gap for future\
    \ work."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/356079637
    type: video
    url: https://vimeo.com/356079637
  author:
  - first: Christopher
    full: Christopher Clark
    id: christopher-clark
    last: Clark
  - first: Kenton
    full: Kenton Lee
    id: kenton-lee
    last: Lee
  - first: Ming-Wei
    full: Ming-Wei Chang
    id: ming-wei-chang
    last: Chang
  - first: Tom
    full: Tom Kwiatkowski
    id: tom-kwiatkowski
    last: Kwiatkowski
  - first: Michael
    full: Michael Collins
    id: michael-collins
    last: Collins
  - first: Kristina
    full: Kristina Toutanova
    id: kristina-toutanova
    last: Toutanova
  author_string: Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
    Collins, Kristina Toutanova
  bibkey: clark-etal-2019-boolq
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1300
  month: June
  page_first: '2924'
  page_last: '2936'
  pages: "2924\u20132936"
  paper_id: '300'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1300.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1300.jpg
  title: 'BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions'
  title_html: '<span class="acl-fixed-case">B</span>ool<span class="acl-fixed-case">Q</span>:
    Exploring the Surprising Difficulty of Natural Yes/No Questions'
  url: https://www.aclweb.org/anthology/N19-1300
  year: '2019'
N19-1301:
  abstract: Traditional Key-value Memory Neural Networks (KV-MemNNs) are proved to
    be effective to support shallow reasoning over a collection of documents in domain
    specific Question Answering or Reading Comprehension tasks. However, extending
    KV-MemNNs to Knowledge Based Question Answering (KB-QA) is not trivia, which should
    properly decompose a complex question into a sequence of queries against the memory,
    and update the query representations to support multi-hop reasoning over the memory.
    In this paper, we propose a novel mechanism to enable conventional KV-MemNNs models
    to perform interpretable reasoning for complex questions. To achieve this, we
    design a new query updating strategy to mask previously-addressed memory information
    from the query representations, and introduce a novel STOP strategy to avoid invalid
    or repeated memory reading without strong annotation signals. This also enables
    KV-MemNNs to produce structured queries and work in a semantic parsing fashion.
    Experimental results on benchmark datasets show that our solution, trained with
    question-answer pairs only, can provide conventional KV-MemNNs models with better
    reasoning abilities on complex questions, and achieve state-of-art performances.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/356088995
    type: video
    url: https://vimeo.com/356088995
  author:
  - first: Kun
    full: Kun Xu
    id: kun-xu
    last: Xu
  - first: Yuxuan
    full: Yuxuan Lai
    id: yuxuan-lai
    last: Lai
  - first: Yansong
    full: Yansong Feng
    id: yansong-feng
    last: Feng
  - first: Zhiguo
    full: Zhiguo Wang
    id: zhiguo-wang
    last: Wang
  author_string: Kun Xu, Yuxuan Lai, Yansong Feng, Zhiguo Wang
  bibkey: xu-etal-2019-enhancing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1301
  month: June
  page_first: '2937'
  page_last: '2947'
  pages: "2937\u20132947"
  paper_id: '301'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1301.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1301.jpg
  title: Enhancing Key-Value Memory Neural Networks for Knowledge Based Question Answering
  title_html: Enhancing Key-Value Memory Neural Networks for Knowledge Based Question
    Answering
  url: https://www.aclweb.org/anthology/N19-1301
  year: '2019'
N19-1302:
  abstract: Question Answering (QA) naturally reduces to an entailment problem, namely,
    verifying whether some text entails the answer to a question. However, for multi-hop
    QA tasks, which require reasoning with multiple sentences, it remains unclear
    how best to utilize entailment models pre-trained on large scale datasets such
    as SNLI, which are based on sentence pairs. We introduce Multee, a general architecture
    that can effectively use entailment models for multi-hop QA tasks. Multee uses
    (i) a local module that helps locate important sentences, thereby avoiding distracting
    information, and (ii) a global module that aggregates information by effectively
    incorporating importance weights. Importantly, we show that both modules can use
    entailment functions pre-trained on a large scale NLI datasets. We evaluate performance
    on MultiRC and OpenBookQA, two multihop QA datasets. When using an entailment
    function pre-trained on NLI datasets, Multee outperforms QA models trained only
    on the target QA datasets and the OpenAI transformer models.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/356097901
    type: video
    url: https://vimeo.com/356097901
  author:
  - first: Harsh
    full: Harsh Trivedi
    id: harsh-trivedi
    last: Trivedi
  - first: Heeyoung
    full: Heeyoung Kwon
    id: heeyoung-kwon
    last: Kwon
  - first: Tushar
    full: Tushar Khot
    id: tushar-khot
    last: Khot
  - first: Ashish
    full: Ashish Sabharwal
    id: ashish-sabharwal
    last: Sabharwal
  - first: Niranjan
    full: Niranjan Balasubramanian
    id: niranjan-balasubramanian
    last: Balasubramanian
  author_string: Harsh Trivedi, Heeyoung Kwon, Tushar Khot, Ashish Sabharwal, Niranjan
    Balasubramanian
  bibkey: trivedi-etal-2019-repurposing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1302
  month: June
  page_first: '2948'
  page_last: '2958'
  pages: "2948\u20132958"
  paper_id: '302'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1302.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1302.jpg
  title: Repurposing Entailment for Multi-Hop Question Answering Tasks
  title_html: Repurposing Entailment for Multi-Hop Question Answering Tasks
  url: https://www.aclweb.org/anthology/N19-1302
  year: '2019'
N19-1303:
  abstract: "Language is gendered if the context surrounding a mention is suggestive\
    \ of a particular binary gender for that mention. Detecting the different ways\
    \ in which language is gendered is an important task since gendered language can\
    \ bias NLP models (such as for coreference resolution). This task is challenging\
    \ since genderedness is often expressed in subtle ways. Existing approaches need\
    \ considerable annotation efforts for each language, domain, and author, and often\
    \ require handcrafted lexicons and features. Additionally, these approaches do\
    \ not provide a quantifiable measure of how gendered the text is, nor are they\
    \ applicable at the fine-grained mention level. In this paper, we use existing\
    \ NLP pipelines to automatically annotate gender of mentions in the text. On corpora\
    \ labeled using this method, we train a supervised classifier to predict the gender\
    \ of any mention from its context and evaluate it on unseen text. The model confidence\
    \ for a mention\u2019s gender can be used as a proxy to indicate the level of\
    \ genderedness of the context. We test this gendered language detector on movie\
    \ summaries, movie reviews, news articles, and fiction novels, achieving an AUC-ROC\
    \ of up to 0.71, and observe that the model predictions agree with human judgments\
    \ collected for this task. We also provide examples of detected gendered sentences\
    \ from aforementioned domains."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1303.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1303.Presentation.pdf
  - filename: https://vimeo.com/361708034
    type: video
    url: https://vimeo.com/361708034
  author:
  - first: ''
    full: Ananya
    id: ananya
    last: Ananya
  - first: Nitya
    full: Nitya Parthasarthi
    id: nitya-parthasarthi
    last: Parthasarthi
  - first: Sameer
    full: Sameer Singh
    id: sameer-singh
    last: Singh
  author_string: Ananya, Nitya Parthasarthi, Sameer Singh
  bibkey: ananya-etal-2019-genderquant
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1303
  month: June
  page_first: '2959'
  page_last: '2969'
  pages: "2959\u20132969"
  paper_id: '303'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1303.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1303.jpg
  title: 'GenderQuant: Quantifying Mention-Level Genderedness'
  title_html: '<span class="acl-fixed-case">G</span>ender<span class="acl-fixed-case">Q</span>uant:
    Quantifying Mention-Level Genderedness'
  url: https://www.aclweb.org/anthology/N19-1303
  year: '2019'
N19-1304:
  abstract: "We provide an NLP framework to uncover four linguistic dimensions of\
    \ political polarization in social media: topic choice, framing, affect and illocutionary\
    \ force. We quantify these aspects with existing lexical methods, and propose\
    \ clustering of tweet embeddings as a means to identify salient topics for analysis\
    \ across events; human evaluations show that our approach generates more cohesive\
    \ topics than traditional LDA-based models. We apply our methods to study 4.4M\
    \ tweets on 21 mass shootings. We provide evidence that the discussion of these\
    \ events is highly polarized politically and that this polarization is primarily\
    \ driven by partisan differences in framing rather than topic choice. We identify\
    \ framing devices, such as grounding and the contrasting use of the terms \u201C\
    terrorist\u201D and \u201Ccrazy\u201D, that contribute to polarization. Results\
    \ pertaining to topic choice, affect and illocutionary force suggest that Republicans\
    \ focus more on the shooter and event-specific facts (news) while Democrats focus\
    \ more on the victims and call for policy changes. Our work contributes to a deeper\
    \ understanding of the way group divisions manifest in language and to computational\
    \ methods for studying them."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/359689303
    type: video
    url: https://vimeo.com/359689303
  author:
  - first: Dorottya
    full: Dorottya Demszky
    id: dorottya-demszky
    last: Demszky
  - first: Nikhil
    full: Nikhil Garg
    id: nikhil-garg
    last: Garg
  - first: Rob
    full: Rob Voigt
    id: rob-voigt
    last: Voigt
  - first: James
    full: James Zou
    id: james-zou
    last: Zou
  - first: Jesse
    full: Jesse Shapiro
    id: jesse-shapiro
    last: Shapiro
  - first: Matthew
    full: Matthew Gentzkow
    id: matthew-gentzkow
    last: Gentzkow
  - first: Dan
    full: Dan Jurafsky
    id: dan-jurafsky
    last: Jurafsky
  author_string: Dorottya Demszky, Nikhil Garg, Rob Voigt, James Zou, Jesse Shapiro,
    Matthew Gentzkow, Dan Jurafsky
  bibkey: demszky-etal-2019-analyzing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1304
  month: June
  page_first: '2970'
  page_last: '3005'
  pages: "2970\u20133005"
  paper_id: '304'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1304.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1304.jpg
  title: 'Analyzing Polarization in Social Media: Method and Application to Tweets
    on 21 Mass Shootings'
  title_html: 'Analyzing Polarization in Social Media: Method and Application to Tweets
    on 21 Mass Shootings'
  url: https://www.aclweb.org/anthology/N19-1304
  year: '2019'
N19-1305:
  abstract: Existing computational models to understand hate speech typically frame
    the problem as a simple classification task, bypassing the understanding of hate
    symbols (e.g., 14 words, kigy) and their secret connotations. In this paper, we
    propose a novel task of deciphering hate symbols. To do this, we leveraged the
    Urban Dictionary and collected a new, symbol-rich Twitter corpus of hate speech.
    We investigate neural network latent context models for deciphering hate symbols.
    More specifically, we study Sequence-to-Sequence models and show how they are
    able to crack the ciphers based on context. Furthermore, we propose a novel Variational
    Decipher and show how it can generalize better to unseen hate symbols in a more
    challenging testing setting.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/359687876
    type: video
    url: https://vimeo.com/359687876
  author:
  - first: Jing
    full: Jing Qian
    id: jing-qian
    last: Qian
  - first: Mai
    full: Mai ElSherief
    id: mai-elsherief
    last: ElSherief
  - first: Elizabeth
    full: Elizabeth Belding
    id: elizabeth-belding
    last: Belding
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  author_string: Jing Qian, Mai ElSherief, Elizabeth Belding, William Yang Wang
  bibkey: qian-etal-2019-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1305
  month: June
  page_first: '3006'
  page_last: '3015'
  pages: "3006\u20133015"
  paper_id: '305'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1305.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1305.jpg
  title: Learning to Decipher Hate Symbols
  title_html: Learning to Decipher Hate Symbols
  url: https://www.aclweb.org/anthology/N19-1305
  year: '2019'
N19-1306:
  abstract: "We propose a distance supervised relation extraction approach for long-tailed,\
    \ imbalanced data which is prevalent in real-world settings. Here, the challenge\
    \ is to learn accurate \u201Cfew-shot\u201D models for classes existing at the\
    \ tail of the class distribution, for which little data is available. Inspired\
    \ by the rich semantic correlations between classes at the long tail and those\
    \ at the head, we take advantage of the knowledge from data-rich classes at the\
    \ head of the distribution to boost the performance of the data-poor classes at\
    \ the tail. First, we propose to leverage implicit relational knowledge among\
    \ class labels from knowledge graph embeddings and learn explicit relational knowledge\
    \ using graph convolution networks. Second, we integrate that relational knowledge\
    \ into relation extraction model by coarse-to-fine knowledge-aware attention mechanism.\
    \ We demonstrate our results for a large-scale benchmark dataset which show that\
    \ our approach significantly outperforms other baselines, especially for long-tail\
    \ relations."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/355830579
    type: video
    url: https://vimeo.com/355830579
  author:
  - first: Ningyu
    full: Ningyu Zhang
    id: ningyu-zhang
    last: Zhang
  - first: Shumin
    full: Shumin Deng
    id: shumin-deng
    last: Deng
  - first: Zhanlin
    full: Zhanlin Sun
    id: zhanlin-sun
    last: Sun
  - first: Guanying
    full: Guanying Wang
    id: guanying-wang
    last: Wang
  - first: Xi
    full: Xi Chen
    id: xi-chen
    last: Chen
  - first: Wei
    full: Wei Zhang
    id: wei-zhang
    last: Zhang
  - first: Huajun
    full: Huajun Chen
    id: huajun-chen
    last: Chen
  author_string: Ningyu Zhang, Shumin Deng, Zhanlin Sun, Guanying Wang, Xi Chen, Wei
    Zhang, Huajun Chen
  bibkey: zhang-etal-2019-long
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1306
  month: June
  page_first: '3016'
  page_last: '3025'
  pages: "3016\u20133025"
  paper_id: '306'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1306.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1306.jpg
  title: Long-tail Relation Extraction via Knowledge Graph Embeddings and Graph Convolution
    Networks
  title_html: Long-tail Relation Extraction via Knowledge Graph Embeddings and Graph
    Convolution Networks
  url: https://www.aclweb.org/anthology/N19-1306
  year: '2019'
N19-1307:
  abstract: Distant supervision has been widely used in relation extraction tasks
    without hand-labeled datasets recently. However, the automatically constructed
    datasets comprise numbers of wrongly labeled negative instances due to the incompleteness
    of knowledge bases, which is neglected by current distant supervised methods resulting
    in seriously misleading in both training and testing processes. To address this
    issue, we propose a novel semi-distant supervision approach for relation extraction
    by constructing a small accurate dataset and properly leveraging numerous instances
    without relation labels. In our approach, we construct accurate instances by both
    knowledge base and entity descriptions determined to avoid wrong negative labeling
    and further utilize unlabeled instances sufficiently using generative adversarial
    network (GAN) framework. Experimental results on real-world datasets show that
    our approach can achieve significant improvements in distant supervised relation
    extraction over strong baselines.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/355832634
    type: video
    url: https://vimeo.com/355832634
  author:
  - first: Pengshuai
    full: Pengshuai Li
    id: pengshuai-li
    last: Li
  - first: Xinsong
    full: Xinsong Zhang
    id: xinsong-zhang
    last: Zhang
  - first: Weijia
    full: Weijia Jia
    id: weijia-jia
    last: Jia
  - first: Hai
    full: Hai Zhao
    id: hai-zhao
    last: Zhao
  author_string: Pengshuai Li, Xinsong Zhang, Weijia Jia, Hai Zhao
  bibkey: li-etal-2019-gan
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1307
  month: June
  page_first: '3026'
  page_last: '3035'
  pages: "3026\u20133035"
  paper_id: '307'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1307.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1307.jpg
  title: GAN Driven Semi-distant Supervision for Relation Extraction
  title_html: <span class="acl-fixed-case">GAN</span> Driven Semi-distant Supervision
    for Relation Extraction
  url: https://www.aclweb.org/anthology/N19-1307
  year: '2019'
N19-1308:
  abstract: We introduce a general framework for several information extraction tasks
    that share span representations using dynamically constructed span graphs. The
    graphs are dynamically constructed by selecting the most confident entity spans
    and linking these nodes with confidence-weighted relation types and coreferences.
    The dynamic span graph allow coreference and relation type confidences to propagate
    through the graph to iteratively refine the span representations. This is unlike
    previous multi-task frameworks for information extraction in which the only interaction
    between tasks is in the shared first-layer LSTM. Our framework significantly outperforms
    state-of-the-art on multiple information extraction tasks across multiple datasets
    reflecting different domains. We further observe that the span enumeration approach
    is good at detecting nested span entities, with significant F1 score improvement
    on the ACE dataset.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/355835368
    type: video
    url: https://vimeo.com/355835368
  author:
  - first: Yi
    full: Yi Luan
    id: yi-luan
    last: Luan
  - first: Dave
    full: Dave Wadden
    id: dave-wadden
    last: Wadden
  - first: Luheng
    full: Luheng He
    id: luheng-he
    last: He
  - first: Amy
    full: Amy Shah
    id: amy-shah
    last: Shah
  - first: Mari
    full: Mari Ostendorf
    id: mari-ostendorf
    last: Ostendorf
  - first: Hannaneh
    full: Hannaneh Hajishirzi
    id: hannaneh-hajishirzi
    last: Hajishirzi
  author_string: Yi Luan, Dave Wadden, Luheng He, Amy Shah, Mari Ostendorf, Hannaneh
    Hajishirzi
  bibkey: luan-etal-2019-general
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1308
  month: June
  page_first: '3036'
  page_last: '3046'
  pages: "3036\u20133046"
  paper_id: '308'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1308.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1308.jpg
  title: A general framework for information extraction using dynamic span graphs
  title_html: A general framework for information extraction using dynamic span graphs
  url: https://www.aclweb.org/anthology/N19-1308
  year: '2019'
N19-1309:
  abstract: Open Information Extraction (OpenIE), the problem of harvesting triples
    from natural language text whose predicate relations are not aligned to any pre-defined
    ontology, has been a popular subject of research for the last decade. However,
    this research has largely ignored the vast quantity of facts available in semi-structured
    webpages. In this paper, we define the problem of OpenIE from semi-structured
    websites to extract such facts, and present an approach for solving it. We also
    introduce a labeled evaluation dataset to motivate research in this area. Given
    a semi-structured website and a set of seed facts for some relations existing
    on its pages, we employ a semi-supervised label propagation technique to automatically
    create training data for the relations present on the site. We then use this training
    data to learn a classifier for relation extraction. Experimental results of this
    method on our new benchmark dataset obtained a precision of over 70%. A larger
    scale extraction experiment on 31 websites in the movie vertical resulted in the
    extraction of over 2 million triples.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/355837778
    type: video
    url: https://vimeo.com/355837778
  author:
  - first: Colin
    full: Colin Lockard
    id: colin-lockard
    last: Lockard
  - first: Prashant
    full: Prashant Shiralkar
    id: prashant-shiralkar
    last: Shiralkar
  - first: Xin Luna
    full: Xin Luna Dong
    id: xin-luna-dong
    last: Dong
  author_string: Colin Lockard, Prashant Shiralkar, Xin Luna Dong
  bibkey: lockard-etal-2019-openceres
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1309
  month: June
  page_first: '3047'
  page_last: '3056'
  pages: "3047\u20133056"
  paper_id: '309'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1309.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1309.jpg
  title: 'OpenCeres: When Open Information Extraction Meets the Semi-Structured Web'
  title_html: 'Open<span class="acl-fixed-case">C</span>eres: <span class="acl-fixed-case">W</span>hen
    Open Information Extraction Meets the Semi-Structured Web'
  url: https://www.aclweb.org/anthology/N19-1309
  year: '2019'
N19-1310:
  abstract: We present an approach to minimally supervised relation extraction that
    combines the benefits of learned representations and structured learning, and
    accurately predicts sentence-level relation mentions given only proposition-level
    supervision from a KB. By explicitly reasoning about missing data during learning,
    our approach enables large-scale training of 1D convolutional neural networks
    while mitigating the issue of label noise inherent in distant supervision. Our
    approach achieves state-of-the-art results on minimally supervised sentential
    relation extraction, outperforming a number of baselines, including a competitive
    approach that uses the attention layer of a purely neural model.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/359696937
    type: video
    url: https://vimeo.com/359696937
  author:
  - first: Fan
    full: Fan Bai
    id: fan-bai
    last: Bai
  - first: Alan
    full: Alan Ritter
    id: alan-ritter
    last: Ritter
  author_string: Fan Bai, Alan Ritter
  bibkey: bai-ritter-2019-structured
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1310
  month: June
  page_first: '3057'
  page_last: '3069'
  pages: "3057\u20133069"
  paper_id: '310'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1310.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1310.jpg
  title: Structured Minimally Supervised Learning for Neural Relation Extraction
  title_html: <span class="acl-fixed-case">S</span>tructured <span class="acl-fixed-case">M</span>inimally
    <span class="acl-fixed-case">S</span>upervised <span class="acl-fixed-case">L</span>earning
    for <span class="acl-fixed-case">N</span>eural <span class="acl-fixed-case">R</span>elation
    <span class="acl-fixed-case">E</span>xtraction
  url: https://www.aclweb.org/anthology/N19-1310
  year: '2019'
N19-1311:
  abstract: Neural Machine Translation (NMT) systems are known to degrade when confronted
    with noisy data, especially when the system is trained only on clean data. In
    this paper, we show that augmenting training data with sentences containing artificially-introduced
    grammatical errors can make the system more robust to such errors. In combination
    with an automatic grammar error correction system, we can recover 1.0 BLEU out
    of 2.4 BLEU lost due to grammatical errors. We also present a set of Spanish translations
    of the JFLEG grammar error correction corpus, which allows for testing NMT robustness
    to real grammatical errors.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1311.Software.zip
    type: software
    url: https://www.aclweb.org/anthology/attachments/N19-1311.Software.zip
  - filename: https://vimeo.com/361713720
    type: video
    url: https://vimeo.com/361713720
  author:
  - first: Antonios
    full: Antonios Anastasopoulos
    id: antonios-anastasopoulos
    last: Anastasopoulos
  - first: Alison
    full: Alison Lui
    id: alison-lui
    last: Lui
  - first: Toan Q.
    full: Toan Q. Nguyen
    id: toan-q-nguyen
    last: Nguyen
  - first: David
    full: David Chiang
    id: david-chiang
    last: Chiang
  author_string: Antonios Anastasopoulos, Alison Lui, Toan Q. Nguyen, David Chiang
  bibkey: anastasopoulos-etal-2019-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1311
  month: June
  page_first: '3070'
  page_last: '3080'
  pages: "3070\u20133080"
  paper_id: '311'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1311.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1311.jpg
  title: Neural Machine Translation of Text from Non-Native Speakers
  title_html: Neural Machine Translation of Text from Non-Native Speakers
  url: https://www.aclweb.org/anthology/N19-1311
  year: '2019'
N19-1312:
  abstract: In domain adaptation for neural machine translation, translation performance
    can benefit from separating features into domain-specific features and common
    features. In this paper, we propose a method to explicitly model the two kinds
    of information in the encoder-decoder framework so as to exploit out-of-domain
    data in in-domain training. In our method, we maintain a private encoder and a
    private decoder for each domain which are used to model domain-specific information.
    In the meantime, we introduce a common encoder and a common decoder shared by
    all the domains which can only have domain-independent information flow through.
    Besides, we add a discriminator to the shared encoder and employ adversarial training
    for the whole model to reinforce the performance of information separation and
    machine translation simultaneously. Experiment results show that our method can
    outperform competitive baselines greatly on multiple data sets.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/361719030
    type: video
    url: https://vimeo.com/361719030
  author:
  - first: Shuhao
    full: Shuhao Gu
    id: shuhao-gu
    last: Gu
  - first: Yang
    full: Yang Feng
    id: yang-feng
    last: Feng
  - first: Qun
    full: Qun Liu
    id: qun-liu
    last: Liu
  author_string: Shuhao Gu, Yang Feng, Qun Liu
  bibkey: gu-etal-2019-improving
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1312
  month: June
  page_first: '3081'
  page_last: '3091'
  pages: "3081\u20133091"
  paper_id: '312'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1312.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1312.jpg
  title: Improving Domain Adaptation Translation with Domain Invariant and Specific
    Information
  title_html: Improving Domain Adaptation Translation with Domain Invariant and Specific
    Information
  url: https://www.aclweb.org/anthology/N19-1312
  year: '2019'
N19-1313:
  abstract: Despite the progress made in sentence-level NMT, current systems still
    fall short at achieving fluent, good quality translation for a full document.
    Recent works in context-aware NMT consider only a few previous sentences as context
    and may not scale to entire documents. To this end, we propose a novel and scalable
    top-down approach to hierarchical attention for context-aware NMT which uses sparse
    attention to selectively focus on relevant sentences in the document context and
    then attends to key words in those sentences. We also propose single-level attention
    approaches based on sentence or word-level information in the context. The document-level
    context representation, produced from these attention modules, is integrated into
    the encoder or decoder of the Transformer model depending on whether we use monolingual
    or bilingual context. Our experiments and evaluation on English-German datasets
    in different document MT settings show that our selective attention approach not
    only significantly outperforms context-agnostic baselines but also surpasses context-aware
    baselines in most cases.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1313.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1313.Presentation.pdf
  - filename: https://vimeo.com/361725345
    type: video
    url: https://vimeo.com/361725345
  author:
  - first: Sameen
    full: Sameen Maruf
    id: sameen-maruf
    last: Maruf
  - first: "Andr\xE9 F. T."
    full: "Andr\xE9 F. T. Martins"
    id: andre-f-t-martins
    last: Martins
  - first: Gholamreza
    full: Gholamreza Haffari
    id: gholamreza-haffari
    last: Haffari
  author_string: "Sameen Maruf, Andr\xE9 F. T. Martins, Gholamreza Haffari"
  bibkey: maruf-etal-2019-selective
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1313
  month: June
  page_first: '3092'
  page_last: '3102'
  pages: "3092\u20133102"
  paper_id: '313'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1313.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1313.jpg
  title: Selective Attention for Context-aware Neural Machine Translation
  title_html: Selective Attention for Context-aware Neural Machine Translation
  url: https://www.aclweb.org/anthology/N19-1313
  year: '2019'
N19-1314:
  abstract: "Adversarial examples \u2014 perturbations to the input of a model that\
    \ elicit large changes in the output \u2014 have been shown to be an effective\
    \ way of assessing the robustness of sequence-to-sequence (seq2seq) models. However,\
    \ these perturbations only indicate weaknesses in the model if they do not change\
    \ the input so significantly that it legitimately results in changes in the expected\
    \ output. This fact has largely been ignored in the evaluations of the growing\
    \ body of related literature. Using the example of untargeted attacks on machine\
    \ translation (MT), we propose a new evaluation framework for adversarial attacks\
    \ on seq2seq models that takes the semantic equivalence of the pre- and post-perturbation\
    \ input into account. Using this framework, we demonstrate that existing methods\
    \ may not preserve meaning in general, breaking the aforementioned assumption\
    \ that source side perturbations should not result in changes in the expected\
    \ output. We further use this framework to demonstrate that adding additional\
    \ constraints on attacks allows for adversarial perturbations that are more meaning-preserving,\
    \ but nonetheless largely change the output sequence. Finally, we show that performing\
    \ untargeted adversarial training with meaning-preserving attacks is beneficial\
    \ to the model in terms of adversarial robustness, without hurting test performance.\
    \ A toolkit implementing our evaluation framework is released at https://github.com/pmichel31415/teapot-nlp."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1314.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1314.Presentation.pdf
  - filename: https://vimeo.com/356119183
    type: video
    url: https://vimeo.com/356119183
  author:
  - first: Paul
    full: Paul Michel
    id: paul-michel
    last: Michel
  - first: Xian
    full: Xian Li
    id: xian-li
    last: Li
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  - first: Juan
    full: Juan Pino
    id: juan-pino
    last: Pino
  author_string: Paul Michel, Xian Li, Graham Neubig, Juan Pino
  bibkey: michel-etal-2019-evaluation
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1314
  month: June
  page_first: '3103'
  page_last: '3114'
  pages: "3103\u20133114"
  paper_id: '314'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1314.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1314.jpg
  title: On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models
  title_html: On Evaluation of Adversarial Perturbations for Sequence-to-Sequence
    Models
  url: https://www.aclweb.org/anthology/N19-1314
  year: '2019'
N19-1315:
  abstract: A major obstacle in reinforcement learning-based sentence generation is
    the large action space whose size is equal to the vocabulary size of the target-side
    language. To improve the efficiency of reinforcement learning, we present a novel
    approach for reducing the action space based on dynamic vocabulary prediction.
    Our method first predicts a fixed-size small vocabulary for each input to generate
    its target sentence. The input-specific vocabularies are then used at supervised
    and reinforcement learning steps, and also at test time. In our experiments on
    six machine translation and two image captioning datasets, our method achieves
    faster reinforcement learning (~2.7x faster) with less GPU memory (~2.3x less)
    than the full-vocabulary counterpart. We also show that our method more effectively
    receives rewards with fewer iterations of supervised pre-training.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1315.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1315.Supplementary.pdf
  - filename: https://vimeo.com/356125366
    type: video
    url: https://vimeo.com/356125366
  author:
  - first: Kazuma
    full: Kazuma Hashimoto
    id: kazuma-hashimoto
    last: Hashimoto
  - first: Yoshimasa
    full: Yoshimasa Tsuruoka
    id: yoshimasa-tsuruoka
    last: Tsuruoka
  author_string: Kazuma Hashimoto, Yoshimasa Tsuruoka
  bibkey: hashimoto-tsuruoka-2019-accelerated
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1315
  month: June
  page_first: '3115'
  page_last: '3125'
  pages: "3115\u20133125"
  paper_id: '315'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1315.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1315.jpg
  title: Accelerated Reinforcement Learning for Sentence Generation by Vocabulary
    Prediction
  title_html: Accelerated Reinforcement Learning for Sentence Generation by Vocabulary
    Prediction
  url: https://www.aclweb.org/anthology/N19-1315
  year: '2019'
N19-1316:
  abstract: "The uncertainty measurement of classifiers\u2019 predictions is especially\
    \ important in applications such as medical diagnoses that need to ensure limited\
    \ human resources can focus on the most uncertain predictions returned by machine\
    \ learning models. However, few existing uncertainty models attempt to improve\
    \ overall prediction accuracy where human resources are involved in the text classification\
    \ task. In this paper, we propose a novel neural-network-based model that applies\
    \ a new dropout-entropy method for uncertainty measurement. We also design a metric\
    \ learning method on feature representations, which can boost the performance\
    \ of dropout-based uncertainty methods with smaller prediction variance in accurate\
    \ prediction trials. Extensive experiments on real-world data sets demonstrate\
    \ that our method can achieve a considerable improvement in overall prediction\
    \ accuracy compared to existing approaches. In particular, our model improved\
    \ the accuracy from 0.78 to 0.92 when 30% of the most uncertain predictions were\
    \ handed over to human experts in \u201C20NewsGroup\u201D data."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/347415373
    type: video
    url: https://vimeo.com/347415373
  author:
  - first: Xuchao
    full: Xuchao Zhang
    id: xuchao-zhang
    last: Zhang
  - first: Fanglan
    full: Fanglan Chen
    id: fanglan-chen
    last: Chen
  - first: Chang-Tien
    full: Chang-Tien Lu
    id: chang-tien-lu
    last: Lu
  - first: Naren
    full: Naren Ramakrishnan
    id: naren-ramakrishnan
    last: Ramakrishnan
  author_string: Xuchao Zhang, Fanglan Chen, Chang-Tien Lu, Naren Ramakrishnan
  bibkey: zhang-etal-2019-mitigating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1316
  month: June
  page_first: '3126'
  page_last: '3136'
  pages: "3126\u20133136"
  paper_id: '316'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1316.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1316.jpg
  title: Mitigating Uncertainty in Document Classification
  title_html: Mitigating Uncertainty in Document Classification
  url: https://www.aclweb.org/anthology/N19-1316
  year: '2019'
N19-1317:
  abstract: Sentence simplification is the task of rewriting texts so they are easier
    to understand. Recent research has applied sequence-to-sequence (Seq2Seq) models
    to this task, focusing largely on training-time improvements via reinforcement
    learning and memory augmentation. One of the main problems with applying generic
    Seq2Seq models for simplification is that these models tend to copy directly from
    the original sentence, resulting in outputs that are relatively long and complex.
    We aim to alleviate this issue through the use of two main techniques. First,
    we incorporate content word complexities, as predicted with a leveled word complexity
    model, into our loss function during training. Second, we generate a large set
    of diverse candidate simplifications at test time, and rerank these to promote
    fluency, adequacy, and simplicity. Here, we measure simplicity through a novel
    sentence complexity model. These extensions allow our models to perform competitively
    with state-of-the-art systems while generating simpler sentences. We report standard
    automatic and human evaluation metrics.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1317.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1317.Supplementary.pdf
  - filename: https://vimeo.com/347417188
    type: video
    url: https://vimeo.com/347417188
  author:
  - first: Reno
    full: Reno Kriz
    id: reno-kriz
    last: Kriz
  - first: "Jo\xE3o"
    full: "Jo\xE3o Sedoc"
    id: joao-sedoc
    last: Sedoc
  - first: Marianna
    full: Marianna Apidianaki
    id: marianna-apidianaki
    last: Apidianaki
  - first: Carolina
    full: Carolina Zheng
    id: carolina-zheng
    last: Zheng
  - first: Gaurav
    full: Gaurav Kumar
    id: gaurav-kumar
    last: Kumar
  - first: Eleni
    full: Eleni Miltsakaki
    id: eleni-miltsakaki
    last: Miltsakaki
  - first: Chris
    full: Chris Callison-Burch
    id: chris-callison-burch
    last: Callison-Burch
  author_string: "Reno Kriz, Jo\xE3o Sedoc, Marianna Apidianaki, Carolina Zheng, Gaurav\
    \ Kumar, Eleni Miltsakaki, Chris Callison-Burch"
  bibkey: kriz-etal-2019-complexity
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1317
  month: June
  page_first: '3137'
  page_last: '3147'
  pages: "3137\u20133147"
  paper_id: '317'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1317.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1317.jpg
  title: Complexity-Weighted Loss and Diverse Reranking for Sentence Simplification
  title_html: Complexity-Weighted Loss and Diverse Reranking for Sentence Simplification
  url: https://www.aclweb.org/anthology/N19-1317
  year: '2019'
N19-1318:
  abstract: Users participate in online discussion forums to learn from others and
    share their knowledge with the community. They often start a thread with a question
    or by sharing their new findings on a certain topic. We find that, unlike Community
    Question Answering, where questions are mostly factoid based, the threads in a
    forum are often open-ended (e.g., asking for recommendations from others) without
    a single correct answer. In this paper, we address the task of identifying helpful
    posts in a forum thread to help users comprehend long running discussion threads,
    which often contain repetitive or irrelevant posts. We propose a recurrent neural
    network based architecture to model (i) the relevance of a post regarding the
    original post starting the thread and (ii) the novelty it brings to the discussion,
    compared to the previous posts in the thread. Experimental results on different
    types of online forum datasets show that our model significantly outperforms the
    state-of-the-art neural network models for text classification.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/347421182
    type: video
    url: https://vimeo.com/347421182
  author:
  - first: Kishaloy
    full: Kishaloy Halder
    id: kishaloy-halder
    last: Halder
  - first: Min-Yen
    full: Min-Yen Kan
    id: min-yen-kan
    last: Kan
  - first: Kazunari
    full: Kazunari Sugiyama
    id: kazunari-sugiyama
    last: Sugiyama
  author_string: Kishaloy Halder, Min-Yen Kan, Kazunari Sugiyama
  bibkey: halder-etal-2019-predicting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1318
  month: June
  page_first: '3148'
  page_last: '3157'
  pages: "3148\u20133157"
  paper_id: '318'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1318.pdf
  publisher: Association for Computational Linguistics
  revision:
  - id: '1'
    url: https://www.aclweb.org/anthology/N19-1318v1.pdf
    value: N19-1318v1
  - explanation: No description of the changes were recorded.
    id: '2'
    url: https://www.aclweb.org/anthology/N19-1318v2.pdf
    value: N19-1318v2
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1318.jpg
  title: 'Predicting Helpful Posts in Open-Ended Discussion Forums: A Neural Architecture'
  title_html: 'Predicting Helpful Posts in Open-Ended Discussion Forums: A Neural
    Architecture'
  url: https://www.aclweb.org/anthology/N19-1318
  year: '2019'
N19-1319:
  abstract: Training data for text classification is often limited in practice, especially
    for applications with many output classes or involving many related classification
    problems. This means classifiers must generalize from limited evidence, but the
    manner and extent of generalization is task dependent. Current practice primarily
    relies on pre-trained word embeddings to map words unseen in training to similar
    seen ones. Unfortunately, this squishes many components of meaning into highly
    restricted capacity. Our alternative begins with sparse pre-trained representations
    derived from unlabeled parsed corpora; based on the available training data, we
    select features that offers the relevant generalizations. This produces task-specific
    semantic vectors; here, we show that a feed-forward network over these vectors
    is especially effective in low-data scenarios, compared to existing state-of-the-art
    methods. By further pairing this network with a convolutional neural network,
    we keep this edge in low data scenarios and remain competitive when using full
    training sets.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/347423311
    type: video
    url: https://vimeo.com/347423311
  author:
  - first: Abhijit
    full: Abhijit Mahabal
    id: abhijit-mahabal
    last: Mahabal
  - first: Jason
    full: Jason Baldridge
    id: jason-baldridge
    last: Baldridge
  - first: Burcu
    full: Burcu Karagol Ayan
    id: burcu-karagol-ayan1
    last: Karagol Ayan
  - first: Vincent
    full: Vincent Perot
    id: vincent-perot
    last: Perot
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Abhijit Mahabal, Jason Baldridge, Burcu Karagol Ayan, Vincent Perot,
    Dan Roth
  bibkey: mahabal-etal-2019-text
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1319
  month: June
  page_first: '3158'
  page_last: '3167'
  pages: "3158\u20133167"
  paper_id: '319'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1319.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1319.jpg
  title: Text Classification with Few Examples using Controlled Generalization
  title_html: Text Classification with Few Examples using Controlled Generalization
  url: https://www.aclweb.org/anthology/N19-1319
  year: '2019'
N19-1320:
  abstract: "Text style transfer rephrases a text from a source style (e.g., informal)\
    \ to a target style (e.g., formal) while keeping its original meaning. Despite\
    \ the success existing works have achieved using a parallel corpus for the two\
    \ styles, transferring text style has proven significantly more challenging when\
    \ there is no parallel training corpus. In this paper, we address this challenge\
    \ by using a reinforcement-learning-based generator-evaluator architecture. Our\
    \ generator employs an attention-based encoder-decoder to transfer a sentence\
    \ from the source style to the target style. Our evaluator is an adversarially\
    \ trained style discriminator with semantic and syntactic constraints that score\
    \ the generated sentence for style, meaning preservation, and fluency. Experimental\
    \ results on two different style transfer tasks\u2013sentiment transfer, and formality\
    \ transfer\u2013show that our model outperforms state-of-the-art approaches.Furthermore,\
    \ we perform a manual evaluation that demonstrates the effectiveness of the proposed\
    \ method using subjective metrics of generated text quality."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1320.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1320.Supplementary.pdf
  - filename: https://vimeo.com/347425210
    type: video
    url: https://vimeo.com/347425210
  author:
  - first: Hongyu
    full: Hongyu Gong
    id: hongyu-gong
    last: Gong
  - first: Suma
    full: Suma Bhat
    id: suma-bhat
    last: Bhat
  - first: Lingfei
    full: Lingfei Wu
    id: lingfei-wu
    last: Wu
  - first: JinJun
    full: JinJun Xiong
    id: jinjun-xiong
    last: Xiong
  - first: Wen-mei
    full: Wen-mei Hwu
    id: wen-mei-hwu1
    last: Hwu
  author_string: Hongyu Gong, Suma Bhat, Lingfei Wu, JinJun Xiong, Wen-mei Hwu
  bibkey: gong-etal-2019-reinforcement
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1320
  month: June
  page_first: '3168'
  page_last: '3180'
  pages: "3168\u20133180"
  paper_id: '320'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1320.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1320.jpg
  title: Reinforcement Learning Based Text Style Transfer without Parallel Training
    Corpus
  title_html: Reinforcement Learning Based Text Style Transfer without Parallel Training
    Corpus
  url: https://www.aclweb.org/anthology/N19-1320
  year: '2019'
N19-1321:
  abstract: We present an adaptation of RNN sequence models to the problem of multi-label
    classification for text, where the target is a set of labels, not a sequence.
    Previous such RNN models define probabilities for sequences but not for sets;
    attempts to obtain a set probability are after-thoughts of the network design,
    including pre-specifying the label order, or relating the sequence probability
    to the set probability in ad hoc ways. Our formulation is derived from a principled
    notion of set probability, as the sum of probabilities of corresponding permutation
    sequences for the set. We provide a new training objective that maximizes this
    set probability, and a new prediction objective that finds the most probable set
    on a test document. These new objectives are theoretically appealing because they
    give the RNN model freedom to discover the best label order, which often is the
    natural one (but different among documents). We develop efficient procedures to
    tackle the computation difficulties involved in training and prediction. Experiments
    on benchmark datasets demonstrate that we outperform state-of-the-art methods
    for this task.
  address: Minneapolis, Minnesota
  author:
  - first: Kechen
    full: Kechen Qin
    id: kechen-qin
    last: Qin
  - first: Cheng
    full: Cheng Li
    id: cheng-li
    last: Li
  - first: Virgil
    full: Virgil Pavlu
    id: virgil-pavlu
    last: Pavlu
  - first: Javed
    full: Javed Aslam
    id: javed-aslam
    last: Aslam
  author_string: Kechen Qin, Cheng Li, Virgil Pavlu, Javed Aslam
  bibkey: qin-etal-2019-adapting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1321
  month: June
  page_first: '3181'
  page_last: '3190'
  pages: "3181\u20133190"
  paper_id: '321'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1321.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1321.jpg
  title: Adapting RNN Sequence Prediction Model to Multi-label Set Prediction
  title_html: Adapting <span class="acl-fixed-case">RNN</span> Sequence Prediction
    Model to Multi-label Set Prediction
  url: https://www.aclweb.org/anthology/N19-1321
  year: '2019'
N19-1322:
  abstract: 'Grapheme to phoneme (G2P) conversion is an integral part in various text
    and speech processing systems, such as: Text to Speech system, Speech Recognition
    system, etc. The existing methodologies for G2P conversion in Bangla language
    are mostly rule-based. However, data-driven approaches have proved their superiority
    over rule-based approaches for large-scale G2P conversion in other languages,
    such as: English, German, etc. As the performance of data-driven approaches for
    G2P conversion depend largely on pronunciation lexicon on which the system is
    trained, in this paper, we investigate on developing an improved training lexicon
    by identifying and categorizing the critical cases in Bangla language and include
    those critical cases in training lexicon for developing a robust G2P conversion
    system in Bangla language. Additionally, we have incorporated nasal vowels in
    our proposed phoneme list. Our methodology outperforms other state-of-the-art
    approaches for G2P conversion in Bangla language.'
  address: Minneapolis, Minnesota
  author:
  - first: Sudipta Saha
    full: Sudipta Saha Shubha
    id: sudipta-saha-shubha
    last: Shubha
  - first: Nafis
    full: Nafis Sadeq
    id: nafis-sadeq
    last: Sadeq
  - first: Shafayat
    full: Shafayat Ahmed
    id: shafayat-ahmed
    last: Ahmed
  - first: Md. Nahidul
    full: Md. Nahidul Islam
    id: md-nahidul-islam
    last: Islam
  - first: Muhammad Abdullah
    full: Muhammad Abdullah Adnan
    id: muhammad-abdullah-adnan
    last: Adnan
  - first: Md. Yasin Ali
    full: Md. Yasin Ali Khan
    id: md-yasin-ali-khan
    last: Khan
  - first: Mohammad Zuberul
    full: Mohammad Zuberul Islam
    id: mohammad-zuberul-islam
    last: Islam
  author_string: Sudipta Saha Shubha, Nafis Sadeq, Shafayat Ahmed, Md. Nahidul Islam,
    Muhammad Abdullah Adnan, Md. Yasin Ali Khan, Mohammad Zuberul Islam
  bibkey: shubha-etal-2019-customizing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1322
  month: June
  page_first: '3191'
  page_last: '3200'
  pages: "3191\u20133200"
  paper_id: '322'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1322.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1322.jpg
  title: Customizing Grapheme-to-Phoneme System for Non-Trivial Transcription Problems
    in Bangla Language
  title_html: Customizing Grapheme-to-Phoneme System for Non-Trivial Transcription
    Problems in <span class="acl-fixed-case">B</span>angla Language
  url: https://www.aclweb.org/anthology/N19-1322
  year: '2019'
N19-1323:
  abstract: Knowledge Bases (KBs) require constant updating to reflect changes to
    the world they represent. For general purpose KBs, this is often done through
    Relation Extraction (RE), the task of predicting KB relations expressed in text
    mentioning entities known to the KB. One way to improve RE is to use KB Embeddings
    (KBE) for link prediction. However, despite clear connections between RE and KBE,
    little has been done toward properly unifying these models systematically. We
    help close the gap with a framework that unifies the learning of RE and KBE models
    leading to significant improvements over the state-of-the-art in RE. The code
    is available at https://github.com/billy-inn/HRERE.
  address: Minneapolis, Minnesota
  author:
  - first: Peng
    full: Peng Xu
    id: peng-xu
    last: Xu
  - first: Denilson
    full: Denilson Barbosa
    id: denilson-barbosa
    last: Barbosa
  author_string: Peng Xu, Denilson Barbosa
  bibkey: xu-barbosa-2019-connecting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1323
  month: June
  page_first: '3201'
  page_last: '3206'
  pages: "3201\u20133206"
  paper_id: '323'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1323.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1323.jpg
  title: Connecting Language and Knowledge with Heterogeneous Representations for
    Neural Relation Extraction
  title_html: Connecting Language and Knowledge with Heterogeneous Representations
    for Neural Relation Extraction
  url: https://www.aclweb.org/anthology/N19-1323
  year: '2019'
N19-1324:
  abstract: We propose a new type of representation learning method that models words,
    phrases and sentences seamlessly. Our method does not depend on word segmentation
    and any human-annotated resources (e.g., word dictionaries), yet it is very effective
    for noisy corpora written in unsegmented languages such as Chinese and Japanese.
    The main idea of our method is to ignore word boundaries completely (i.e., segmentation-free),
    and construct representations for all character n-grams in a raw corpus with embeddings
    of compositional sub--grams in a raw corpus with embeddings of compositional sub-n-grams.
    Although the idea is simple, our experiments on various benchmarks and real-world
    datasets show the efficacy of our proposal.-grams. Although the idea is simple,
    our experiments on various benchmarks and real-world datasets show the efficacy
    of our proposal.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1324.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1324.Supplementary.pdf
  author:
  - first: Geewook
    full: Geewook Kim
    id: geewook-kim
    last: Kim
  - first: Kazuki
    full: Kazuki Fukui
    id: kazuki-fukui
    last: Fukui
  - first: Hidetoshi
    full: Hidetoshi Shimodaira
    id: hidetoshi-shimodaira
    last: Shimodaira
  author_string: Geewook Kim, Kazuki Fukui, Hidetoshi Shimodaira
  bibkey: kim-etal-2019-segmentation
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1324
  month: June
  page_first: '3207'
  page_last: '3215'
  pages: "3207\u20133215"
  paper_id: '324'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1324.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1324.jpg
  title: Segmentation-free compositional n-gram embedding-gram embedding
  title_html: Segmentation-free compositional <span class="tex-math">n</span>-gram
    embedding
  url: https://www.aclweb.org/anthology/N19-1324
  year: '2019'
N19-1325:
  abstract: Distant supervision has obtained great progress on relation classification
    task. However, it still suffers from noisy labeling problem. Different from previous
    works that underutilize noisy data which inherently characterize the property
    of classification, in this paper, we propose RCEND, a novel framework to enhance
    Relation Classification by Exploiting Noisy Data. First, an instance discriminator
    with reinforcement learning is designed to split the noisy data into correctly
    labeled data and incorrectly labeled data. Second, we learn a robust relation
    classifier in semi-supervised learning way, whereby the correctly and incorrectly
    labeled data are treated as labeled and unlabeled data respectively. The experimental
    results show that our method outperforms the state-of-the-art models.
  address: Minneapolis, Minnesota
  author:
  - first: Kaijia
    full: Kaijia Yang
    id: kaijia-yang
    last: Yang
  - first: Liang
    full: Liang He
    id: liang-he
    last: He
  - first: Xin-yu
    full: Xin-yu Dai
    id: xinyu-dai
    last: Dai
  - first: Shujian
    full: Shujian Huang
    id: shujian-huang
    last: Huang
  - first: Jiajun
    full: Jiajun Chen
    id: jiajun-chen
    last: Chen
  author_string: Kaijia Yang, Liang He, Xin-yu Dai, Shujian Huang, Jiajun Chen
  bibkey: yang-etal-2019-exploiting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1325
  month: June
  page_first: '3216'
  page_last: '3225'
  pages: "3216\u20133225"
  paper_id: '325'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1325.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1325.jpg
  title: Exploiting Noisy Data in Distant Supervision Relation Classification
  title_html: Exploiting Noisy Data in Distant Supervision Relation Classification
  url: https://www.aclweb.org/anthology/N19-1325
  year: '2019'
N19-1326:
  abstract: In this paper we present a method to learn word embeddings that are resilient
    to misspellings. Existing word embeddings have limited applicability to malformed
    texts, which contain a non-negligible amount of out-of-vocabulary words. We propose
    a method combining FastText with subwords and a supervised task of learning misspelling
    patterns. In our method, misspellings of each word are embedded close to their
    correct variants. We train these embeddings on a new dataset we are releasing
    publicly. Finally, we experimentally show the advantages of this approach on both
    intrinsic and extrinsic NLP tasks using public test sets.
  address: Minneapolis, Minnesota
  author:
  - first: Aleksandra
    full: Aleksandra Piktus
    id: aleksandra-piktus
    last: Piktus
  - first: Necati Bora
    full: Necati Bora Edizel
    id: necati-bora-edizel
    last: Edizel
  - first: Piotr
    full: Piotr Bojanowski
    id: piotr-bojanowski
    last: Bojanowski
  - first: Edouard
    full: Edouard Grave
    id: edouard-grave
    last: Grave
  - first: Rui
    full: Rui Ferreira
    id: rui-ferreira
    last: Ferreira
  - first: Fabrizio
    full: Fabrizio Silvestri
    id: fabrizio-silvestri
    last: Silvestri
  author_string: Aleksandra Piktus, Necati Bora Edizel, Piotr Bojanowski, Edouard
    Grave, Rui Ferreira, Fabrizio Silvestri
  bibkey: piktus-etal-2019-misspelling
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1326
  month: June
  page_first: '3226'
  page_last: '3234'
  pages: "3226\u20133234"
  paper_id: '326'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1326.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1326.jpg
  title: Misspelling Oblivious Word Embeddings
  title_html: Misspelling Oblivious Word Embeddings
  url: https://www.aclweb.org/anthology/N19-1326
  year: '2019'
N19-1327:
  abstract: We address relation extraction as an analogy problem by proposing a novel
    approach to learn representations of relations expressed by their textual mentions.
    In our assumption, if two pairs of entities belong to the same relation, then
    those two pairs are analogous. Following this idea, we collect a large set of
    analogous pairs by matching triples in knowledge bases with web-scale corpora
    through distant supervision. We leverage this dataset to train a hierarchical
    siamese network in order to learn entity-entity embeddings which encode relational
    information through the different linguistic paraphrasing expressing the same
    relation. We evaluate our model in a one-shot learning task by showing a promising
    generalization capability in order to classify unseen relation types, which makes
    this approach suitable to perform automatic knowledge base population with minimal
    supervision. Moreover, the model can be used to generate pre-trained embeddings
    which provide a valuable signal when integrated into an existing neural-based
    model by outperforming the state-of-the-art methods on a downstream relation extraction
    task.
  address: Minneapolis, Minnesota
  author:
  - first: Gaetano
    full: Gaetano Rossiello
    id: gaetano-rossiello
    last: Rossiello
  - first: Alfio
    full: Alfio Gliozzo
    id: alfio-gliozzo
    last: Gliozzo
  - first: Robert
    full: Robert Farrell
    id: robert-farrell
    last: Farrell
  - first: Nicolas
    full: Nicolas Fauceglia
    id: nicolas-r-fauceglia
    last: Fauceglia
  - first: Michael
    full: Michael Glass
    id: michael-glass
    last: Glass
  author_string: Gaetano Rossiello, Alfio Gliozzo, Robert Farrell, Nicolas Fauceglia,
    Michael Glass
  bibkey: rossiello-etal-2019-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1327
  month: June
  page_first: '3235'
  page_last: '3245'
  pages: "3235\u20133245"
  paper_id: '327'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1327.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1327.jpg
  title: Learning Relational Representations by Analogy using Hierarchical Siamese
    Networks
  title_html: Learning Relational Representations by Analogy using Hierarchical <span
    class="acl-fixed-case">S</span>iamese Networks
  url: https://www.aclweb.org/anthology/N19-1327
  year: '2019'
N19-1328:
  abstract: Because large, human-annotated datasets suffer from labeling errors, it
    is crucial to be able to train deep neural networks in the presence of label noise.
    While training image classification models with label noise have received much
    attention, training text classification models have not. In this paper, we propose
    an approach to training deep networks that is robust to label noise. This approach
    introduces a non-linear processing layer (noise model) that models the statistics
    of the label noise into a convolutional neural network (CNN) architecture. The
    noise model and the CNN weights are learned jointly from noisy training data,
    which prevents the model from overfitting to erroneous labels. Through extensive
    experiments on several text classification datasets, we show that this approach
    enables the CNN to learn better sentence representations and is robust even to
    extreme label noise. We find that proper initialization and regularization of
    this noise model is critical. Further, by contrast to results focusing on large
    batch sizes for mitigating label noise for image classification, we find that
    altering the batch size does not have much effect on classification performance.
  address: Minneapolis, Minnesota
  author:
  - first: Ishan
    full: Ishan Jindal
    id: ishan-jindal
    last: Jindal
  - first: Daniel
    full: Daniel Pressel
    id: daniel-pressel
    last: Pressel
  - first: Brian
    full: Brian Lester
    id: brian-lester
    last: Lester
  - first: Matthew
    full: Matthew Nokleby
    id: matthew-nokleby
    last: Nokleby
  author_string: Ishan Jindal, Daniel Pressel, Brian Lester, Matthew Nokleby
  bibkey: jindal-etal-2019-effective
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1328
  month: June
  page_first: '3246'
  page_last: '3256'
  pages: "3246\u20133256"
  paper_id: '328'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1328.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1328.jpg
  title: An Effective Label Noise Model for DNN Text Classification
  title_html: An Effective Label Noise Model for <span class="acl-fixed-case">DNN</span>
    Text Classification
  url: https://www.aclweb.org/anthology/N19-1328
  year: '2019'
N19-1329:
  abstract: Research has shown that neural models implicitly encode linguistic features,
    but there has been no research showing how these encodings arise as the models
    are trained. We present the first study on the learning dynamics of neural language
    models, using a simple and flexible analysis method called Singular Vector Canonical
    Correlation Analysis (SVCCA), which enables us to compare learned representations
    across time and across models, without the need to evaluate directly on annotated
    data. We probe the evolution of syntactic, semantic, and topic representations,
    finding, for example, that part-of-speech is learned earlier than topic; that
    recurrent layers become more similar to those of a tagger during training; and
    embedding layers less similar. Our results and methods could inform better learning
    algorithms for NLP models, possibly to incorporate linguistic information more
    effectively.
  address: Minneapolis, Minnesota
  author:
  - first: Naomi
    full: Naomi Saphra
    id: naomi-saphra
    last: Saphra
  - first: Adam
    full: Adam Lopez
    id: adam-lopez
    last: Lopez
  author_string: Naomi Saphra, Adam Lopez
  bibkey: saphra-lopez-2019-understanding
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1329
  month: June
  page_first: '3257'
  page_last: '3267'
  pages: "3257\u20133267"
  paper_id: '329'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1329.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1329.jpg
  title: Understanding Learning Dynamics Of Language Models with SVCCA
  title_html: Understanding Learning Dynamics Of Language Models with <span class="acl-fixed-case">SVCCA</span>
  url: https://www.aclweb.org/anthology/N19-1329
  year: '2019'
N19-1330:
  abstract: Recurrent neural network language models (RNNLM) form a valuable foundation
    for many NLP systems, but training the models can be computationally expensive,
    and may take days to train on a large corpus. We explore a technique that uses
    large corpus n-gram statistics as a regularizer for training a neural network
    LM on a smaller corpus. In experiments with the Billion-Word and Wikitext corpora,
    we show that the technique is effective, and more time-efficient than simply training
    on a larger sequential corpus. We also introduce new strategies for selecting
    the most informative n-grams, and show that these boost efficiency.
  address: Minneapolis, Minnesota
  author:
  - first: Yiben
    full: Yiben Yang
    id: yiben-yang
    last: Yang
  - first: Ji-Ping
    full: Ji-Ping Wang
    id: ji-ping-wang
    last: Wang
  - first: Doug
    full: Doug Downey
    id: doug-downey
    last: Downey
  author_string: Yiben Yang, Ji-Ping Wang, Doug Downey
  bibkey: yang-etal-2019-using
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1330
  month: June
  page_first: '3268'
  page_last: '3273'
  pages: "3268\u20133273"
  paper_id: '330'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1330.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1330.jpg
  title: Using Large Corpus N-gram Statistics to Improve Recurrent Neural Language
    Models
  title_html: Using Large Corpus N-gram Statistics to Improve Recurrent Neural Language
    Models
  url: https://www.aclweb.org/anthology/N19-1330
  year: '2019'
N19-1331:
  abstract: 'Distributed representations of sentences have become ubiquitous in natural
    language processing tasks. In this paper, we consider a continual learning scenario
    for sentence representations: Given a sequence of corpora, we aim to optimize
    the sentence encoder with respect to the new corpus while maintaining its accuracy
    on the old corpora. To address this problem, we propose to initialize sentence
    encoders with the help of corpus-independent features, and then sequentially update
    sentence encoders using Boolean operations of conceptor matrices to learn corpus-dependent
    features. We evaluate our approach on semantic textual similarity tasks and show
    that our proposed sentence encoder can continually learn features from new corpora
    while retaining its competence on previously encountered corpora.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1331.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1331.Supplementary.pdf
  author:
  - first: Tianlin
    full: Tianlin Liu
    id: tianlin-liu
    last: Liu
  - first: Lyle
    full: Lyle Ungar
    id: lyle-ungar
    last: Ungar
  - first: "Jo\xE3o"
    full: "Jo\xE3o Sedoc"
    id: joao-sedoc
    last: Sedoc
  author_string: "Tianlin Liu, Lyle Ungar, Jo\xE3o Sedoc"
  bibkey: liu-etal-2019-continual
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1331
  month: June
  page_first: '3274'
  page_last: '3279'
  pages: "3274\u20133279"
  paper_id: '331'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1331.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1331.jpg
  title: Continual Learning for Sentence Representations Using Conceptors
  title_html: Continual Learning for Sentence Representations Using Conceptors
  url: https://www.aclweb.org/anthology/N19-1331
  year: '2019'
N19-1332:
  abstract: Unsupervised relation discovery aims to discover new relations from a
    given text corpus without annotated data. However, it does not consider existing
    human annotated knowledge bases even when they are relevant to the relations to
    be discovered. In this paper, we study the problem of how to use out-of-relation
    knowledge bases to supervise the discovery of unseen relations, where out-of-relation
    means that relations to discover from the text corpus and those in knowledge bases
    are not overlapped. We construct a set of constraints between entity pairs based
    on the knowledge base embedding and then incorporate constraints into the relation
    discovery by a variational auto-encoder based algorithm. Experiments show that
    our new approach can improve the state-of-the-art relation discovery performance
    by a large margin.
  address: Minneapolis, Minnesota
  author:
  - first: Yan
    full: Yan Liang
    id: yan-liang
    last: Liang
  - first: Xin
    full: Xin Liu
    id: xin-liu
    last: Liu
  - first: Jianwen
    full: Jianwen Zhang
    id: jianwen-zhang
    last: Zhang
  - first: Yangqiu
    full: Yangqiu Song
    id: yangqiu-song
    last: Song
  author_string: Yan Liang, Xin Liu, Jianwen Zhang, Yangqiu Song
  bibkey: liang-etal-2019-relation
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1332
  month: June
  page_first: '3280'
  page_last: '3290'
  pages: "3280\u20133290"
  paper_id: '332'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1332.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1332.jpg
  title: Relation Discovery with Out-of-Relation Knowledge Base as Supervision
  title_html: Relation Discovery with Out-of-Relation Knowledge Base as Supervision
  url: https://www.aclweb.org/anthology/N19-1332
  year: '2019'
N19-1333:
  abstract: "Grammatical Error Correction (GEC) has been recently modeled using the\
    \ sequence-to-sequence framework. However, unlike sequence transduction problems\
    \ such as machine translation, GEC suffers from the lack of plentiful parallel\
    \ data. We describe two approaches for generating large parallel datasets for\
    \ GEC using publicly available Wikipedia data. The first method extracts source-target\
    \ pairs from Wikipedia edit histories with minimal filtration heuristics while\
    \ the second method introduces noise into Wikipedia sentences via round-trip translation\
    \ through bridge languages. Both strategies yield similar sized parallel corpora\
    \ containing around 4B tokens. We employ an iterative decoding strategy that is\
    \ tailored to the loosely supervised nature of our constructed corpora. We demonstrate\
    \ that neural GEC models trained using either type of corpora give similar performance.\
    \ Fine-tuning these models on the Lang-8 corpus and ensembling allows us to surpass\
    \ the state of the art on both the CoNLL \u201814 benchmark and the JFLEG task.\
    \ We present systematic analysis that compares the two approaches to data generation\
    \ and highlights the effectiveness of ensembling."
  address: Minneapolis, Minnesota
  author:
  - first: Jared
    full: Jared Lichtarge
    id: jared-lichtarge
    last: Lichtarge
  - first: Chris
    full: Chris Alberti
    id: chris-alberti
    last: Alberti
  - first: Shankar
    full: Shankar Kumar
    id: shankar-kumar
    last: Kumar
  - first: Noam
    full: Noam Shazeer
    id: noam-shazeer
    last: Shazeer
  - first: Niki
    full: Niki Parmar
    id: niki-parmar
    last: Parmar
  - first: Simon
    full: Simon Tong
    id: simon-tong
    last: Tong
  author_string: Jared Lichtarge, Chris Alberti, Shankar Kumar, Noam Shazeer, Niki
    Parmar, Simon Tong
  bibkey: lichtarge-etal-2019-corpora
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1333
  month: June
  page_first: '3291'
  page_last: '3301'
  pages: "3291\u20133301"
  paper_id: '333'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1333.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1333.jpg
  title: Corpora Generation for Grammatical Error Correction
  title_html: Corpora Generation for Grammatical Error Correction
  url: https://www.aclweb.org/anthology/N19-1333
  year: '2019'
N19-1334:
  abstract: "State-of-the-art LSTM language models trained on large corpora learn\
    \ sequential contingencies in impressive detail, and have been shown to acquire\
    \ a number of non-local grammatical dependencies with some success. Here we investigate\
    \ whether supervision with hierarchical structure enhances learning of a range\
    \ of grammatical dependencies, a question that has previously been addressed only\
    \ for subject-verb agreement. Using controlled experimental methods from psycholinguistics,\
    \ we compare the performance of word-based LSTM models versus Recurrent Neural\
    \ Network Grammars (RNNGs) (Dyer et al. 2016) which represent hierarchical syntactic\
    \ structure and use neural control to deploy it in left-to-right processing, on\
    \ two classes of non-local grammatical dependencies in English\u2014Negative Polarity\
    \ licensing and Filler-Gap Dependencies\u2014tested in a range of configurations.\
    \ Using the same training data for both models, we find that the RNNG outperforms\
    \ the LSTM on both types of grammatical dependencies and even learns many of the\
    \ Island Constraints on the filler-gap dependency. Structural supervision thus\
    \ provides data efficiency advantages over purely string-based training of neural\
    \ language models in acquiring human-like generalizations about non-local grammatical\
    \ dependencies."
  address: Minneapolis, Minnesota
  author:
  - first: Ethan
    full: Ethan Wilcox
    id: ethan-wilcox
    last: Wilcox
  - first: Peng
    full: Peng Qian
    id: peng-qian
    last: Qian
  - first: Richard
    full: Richard Futrell
    id: richard-futrell
    last: Futrell
  - first: Miguel
    full: Miguel Ballesteros
    id: miguel-ballesteros
    last: Ballesteros
  - first: Roger
    full: Roger Levy
    id: roger-levy
    last: Levy
  author_string: Ethan Wilcox, Peng Qian, Richard Futrell, Miguel Ballesteros, Roger
    Levy
  bibkey: wilcox-etal-2019-structural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1334
  month: June
  page_first: '3302'
  page_last: '3312'
  pages: "3302\u20133312"
  paper_id: '334'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1334.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1334.jpg
  title: Structural Supervision Improves Learning of Non-Local Grammatical Dependencies
  title_html: Structural Supervision Improves Learning of Non-Local Grammatical Dependencies
  url: https://www.aclweb.org/anthology/N19-1334
  year: '2019'
N19-1335:
  abstract: "Exact structured inference with neural network scoring functions is computationally\
    \ challenging but several methods have been proposed for approximating inference.\
    \ One approach is to perform gradient descent with respect to the output structure\
    \ directly (Belanger and McCallum, 2016). Another approach, proposed recently,\
    \ is to train a neural network (an \u201Cinference network\u201D) to perform inference\
    \ (Tu and Gimpel, 2018). In this paper, we compare these two families of inference\
    \ methods on three sequence labeling datasets. We choose sequence labeling because\
    \ it permits us to use exact inference as a benchmark in terms of speed, accuracy,\
    \ and search error. Across datasets, we demonstrate that inference networks achieve\
    \ a better speed/accuracy/search error trade-off than gradient descent, while\
    \ also being faster than exact inference at similar accuracy levels. We find further\
    \ benefit by combining inference networks and gradient descent, using the former\
    \ to provide a warm start for the latter."
  address: Minneapolis, Minnesota
  author:
  - first: Lifu
    full: Lifu Tu
    id: lifu-tu
    last: Tu
  - first: Kevin
    full: Kevin Gimpel
    id: kevin-gimpel
    last: Gimpel
  author_string: Lifu Tu, Kevin Gimpel
  bibkey: tu-gimpel-2019-benchmarking
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1335
  month: June
  page_first: '3313'
  page_last: '3324'
  pages: "3313\u20133324"
  paper_id: '335'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1335.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1335.jpg
  title: Benchmarking Approximate Inference Methods for Neural Structured Prediction
  title_html: Benchmarking Approximate Inference Methods for Neural Structured Prediction
  url: https://www.aclweb.org/anthology/N19-1335
  year: '2019'
N19-1336:
  abstract: Recent research has demonstrated that goal-oriented dialogue agents trained
    on large datasets can achieve striking performance when interacting with human
    users. In real world applications, however, it is important to ensure that the
    agent performs smoothly interacting with not only regular users but also those
    malicious ones who would attack the system through interactions in order to achieve
    goals for their own advantage. In this paper, we develop algorithms to evaluate
    the robustness of a dialogue agent by carefully designed attacks using adversarial
    agents. Those attacks are performed in both black-box and white-box settings.
    Furthermore, we demonstrate that adversarial training using our attacks can significantly
    improve the robustness of a goal-oriented dialogue system. On a case-study of
    the negotiation agent developed by (Lewis et al., 2017), our attacks reduced the
    average advantage of rewards between the attacker and the trained RL-based agent
    from 2.68 to -5.76 on a scale from -10 to 10 for randomized goals. Moreover, we
    show that with the adversarial training, we are able to improve the robustness
    of negotiation agents by 1.5 points on average against all our attacks.
  address: Minneapolis, Minnesota
  author:
  - first: Minhao
    full: Minhao Cheng
    id: minhao-cheng
    last: Cheng
  - first: Wei
    full: Wei Wei
    id: wei-wei
    last: Wei
  - first: Cho-Jui
    full: Cho-Jui Hsieh
    id: cho-jui-hsieh
    last: Hsieh
  author_string: Minhao Cheng, Wei Wei, Cho-Jui Hsieh
  bibkey: cheng-etal-2019-evaluating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1336
  month: June
  page_first: '3325'
  page_last: '3335'
  pages: "3325\u20133335"
  paper_id: '336'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1336.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1336.jpg
  title: 'Evaluating and Enhancing the Robustness of Dialogue Systems: A Case Study
    on a Negotiation Agent'
  title_html: 'Evaluating and Enhancing the Robustness of Dialogue Systems: A Case
    Study on a Negotiation Agent'
  url: https://www.aclweb.org/anthology/N19-1336
  year: '2019'
N19-1337:
  abstract: 'Representing entities and relations in an embedding space is a well-studied
    approach for machine learning on relational data. Existing approaches, however,
    primarily focus on improving accuracy and overlook other aspects such as robustness
    and interpretability. In this paper, we propose adversarial modifications for
    link prediction models: identifying the fact to add into or remove from the knowledge
    graph that changes the prediction for a target fact after the model is retrained.
    Using these single modifications of the graph, we identify the most influential
    fact for a predicted link and evaluate the sensitivity of the model to the addition
    of fake facts. We introduce an efficient approach to estimate the effect of such
    modifications by approximating the change in the embeddings when the knowledge
    graph changes. To avoid the combinatorial search over all possible facts, we train
    a network to decode embeddings to their corresponding graph components, allowing
    the use of gradient-based optimization to identify the adversarial modification.
    We use these techniques to evaluate the robustness of link prediction models (by
    measuring sensitivity to additional facts), study interpretability through the
    facts most responsible for predictions (by identifying the most influential neighbors),
    and detect incorrect facts in the knowledge base.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1337.Poster.pdf
    type: poster
    url: https://www.aclweb.org/anthology/attachments/N19-1337.Poster.pdf
  author:
  - first: Pouya
    full: Pouya Pezeshkpour
    id: pouya-pezeshkpour
    last: Pezeshkpour
  - first: Yifan
    full: Yifan Tian
    id: yifan-tian
    last: Tian
  - first: Sameer
    full: Sameer Singh
    id: sameer-singh
    last: Singh
  author_string: Pouya Pezeshkpour, Yifan Tian, Sameer Singh
  bibkey: pezeshkpour-etal-2019-investigating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1337
  month: June
  page_first: '3336'
  page_last: '3347'
  pages: "3336\u20133347"
  paper_id: '337'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1337.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1337.jpg
  title: Investigating Robustness and Interpretability of Link Prediction via Adversarial
    Modifications
  title_html: Investigating Robustness and Interpretability of Link Prediction via
    Adversarial Modifications
  url: https://www.aclweb.org/anthology/N19-1337
  year: '2019'
N19-1339:
  abstract: Neural word representations are at the core of many state-of-the-art natural
    language processing models. A widely used approach is to pre-train, store and
    look up word or character embedding matrices. While useful, such representations
    occupy huge memory making it hard to deploy on-device and often do not generalize
    to unknown words due to vocabulary pruning. In this paper, we propose a skip-gram
    based architecture coupled with Locality-Sensitive Hashing (LSH) projections to
    learn efficient dynamically computable representations. Our model does not need
    to store lookup tables as representations are computed on-the-fly and require
    low memory footprint. The representations can be trained in an unsupervised fashion
    and can be easily transferred to other NLP tasks. For qualitative evaluation,
    we analyze the nearest neighbors of the word representations and discover semantically
    similar words even with misspellings. For quantitative evaluation, we plug our
    transferable projections into a simple LSTM and run it on multiple NLP tasks and
    show how our transferable projections achieve better performance compared to prior
    work.
  address: Minneapolis, Minnesota
  author:
  - first: Chinnadhurai
    full: Chinnadhurai Sankar
    id: chinnadhurai-sankar
    last: Sankar
  - first: Sujith
    full: Sujith Ravi
    id: sujith-ravi
    last: Ravi
  - first: Zornitsa
    full: Zornitsa Kozareva
    id: zornitsa-kozareva
    last: Kozareva
  author_string: Chinnadhurai Sankar, Sujith Ravi, Zornitsa Kozareva
  bibkey: sankar-etal-2019-transferable
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1339
  month: June
  page_first: '3355'
  page_last: '3360'
  pages: "3355\u20133360"
  paper_id: '339'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1339.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1339.jpg
  title: Transferable Neural Projection Representations
  title_html: Transferable Neural Projection Representations
  url: https://www.aclweb.org/anthology/N19-1339
  year: '2019'
N19-1340:
  abstract: Semantic role labeling (SRL) is a task to recognize all the predicate-argument
    pairs of a sentence, which has been in a performance improvement bottleneck after
    a series of latest works were presented. This paper proposes a novel syntax-agnostic
    SRL model enhanced by the proposed associated memory network (AMN), which makes
    use of inter-sentence attention of label-known associated sentences as a kind
    of memory to further enhance dependency-based SRL. In detail, we use sentences
    and their labels from train dataset as an associated memory cue to help label
    the target sentence. Furthermore, we compare several associated sentences selecting
    strategies and label merging methods in AMN to find and utilize the label of associated
    sentences while attending them. By leveraging the attentive memory from known
    training data, Our full model reaches state-of-the-art on CoNLL-2009 benchmark
    datasets for syntax-agnostic setting, showing a new effective research line of
    SRL enhancement other than exploiting external resources such as well pre-trained
    language models.
  address: Minneapolis, Minnesota
  author:
  - first: Chaoyu
    full: Chaoyu Guan
    id: chaoyu-guan
    last: Guan
  - first: Yuhao
    full: Yuhao Cheng
    id: yuhao-cheng
    last: Cheng
  - first: Hai
    full: Hai Zhao
    id: hai-zhao
    last: Zhao
  author_string: Chaoyu Guan, Yuhao Cheng, Hai Zhao
  bibkey: guan-etal-2019-semantic
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1340
  month: June
  page_first: '3361'
  page_last: '3371'
  pages: "3361\u20133371"
  paper_id: '340'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1340.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1340.jpg
  title: Semantic Role Labeling with Associated Memory Network
  title_html: Semantic Role Labeling with Associated Memory Network
  url: https://www.aclweb.org/anthology/N19-1340
  year: '2019'
N19-1341:
  abstract: 'Sequence tagging models for constituent parsing are faster, but less
    accurate than other types of parsers. In this work, we address the following weaknesses
    of such constituent parsers: (a) high error rates around closing brackets of long
    constituents, (b) large label sets, leading to sparsity, and (c) error propagation
    arising from greedy decoding. To effectively close brackets, we train a model
    that learns to switch between tagging schemes. To reduce sparsity, we decompose
    the label set and use multi-task learning to jointly learn to predict sublabels.
    Finally, we mitigate issues from greedy decoding through auxiliary losses and
    sentence-level fine-tuning with policy gradient. Combining these techniques, we
    clearly surpass the performance of sequence tagging constituent parsers on the
    English and Chinese Penn Treebanks, and reduce their parsing time even further.
    On the SPMRL datasets, we observe even greater improvements across the board,
    including a new state of the art on Basque, Hebrew, Polish and Swedish.'
  address: Minneapolis, Minnesota
  author:
  - first: David
    full: David Vilares
    id: david-vilares
    last: Vilares
  - first: Mostafa
    full: Mostafa Abdou
    id: mostafa-abdou
    last: Abdou
  - first: Anders
    full: "Anders S\xF8gaard"
    id: anders-sogaard
    last: "S\xF8gaard"
  author_string: "David Vilares, Mostafa Abdou, Anders S\xF8gaard"
  bibkey: vilares-etal-2019-better
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1341
  month: June
  page_first: '3372'
  page_last: '3383'
  pages: "3372\u20133383"
  paper_id: '341'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1341.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1341.jpg
  title: Better, Faster, Stronger Sequence Tagging Constituent Parsers
  title_html: Better, Faster, Stronger Sequence Tagging Constituent Parsers
  url: https://www.aclweb.org/anthology/N19-1341
  year: '2019'
N19-1342:
  abstract: Named entity recognition (NER) in Chinese is essential but difficult because
    of the lack of natural delimiters. Therefore, Chinese Word Segmentation (CWS)
    is usually considered as the first step for Chinese NER. However, models based
    on word-level embeddings and lexicon features often suffer from segmentation errors
    and out-of-vocabulary (OOV) words. In this paper, we investigate a Convolutional
    Attention Network called CAN for Chinese NER, which consists of a character-based
    convolutional neural network (CNN) with local-attention layer and a gated recurrent
    unit (GRU) with global self-attention layer to capture the information from adjacent
    characters and sentence contexts. Also, compared to other models, not depending
    on any external resources like lexicons and employing small size of char embeddings
    make our model more practical. Extensive experimental results show that our approach
    outperforms state-of-the-art methods without word embedding and external lexicon
    resources on different domain datasets including Weibo, MSRA and Chinese Resume
    NER dataset.
  address: Minneapolis, Minnesota
  author:
  - first: Yuying
    full: Yuying Zhu
    id: yuying-zhu
    last: Zhu
  - first: Guoxin
    full: Guoxin Wang
    id: guoxin-wang
    last: Wang
  author_string: Yuying Zhu, Guoxin Wang
  bibkey: zhu-wang-2019-ner
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1342
  month: June
  page_first: '3384'
  page_last: '3393'
  pages: "3384\u20133393"
  paper_id: '342'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1342.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1342.jpg
  title: 'CAN-NER: Convolutional Attention Network for Chinese Named Entity Recognition'
  title_html: '<span class="acl-fixed-case">CAN</span>-<span class="acl-fixed-case">NER</span>:
    <span class="acl-fixed-case">C</span>onvolutional <span class="acl-fixed-case">A</span>ttention
    <span class="acl-fixed-case">N</span>etwork for <span class="acl-fixed-case">C</span>hinese
    <span class="acl-fixed-case">N</span>amed <span class="acl-fixed-case">E</span>ntity
    <span class="acl-fixed-case">R</span>ecognition'
  url: https://www.aclweb.org/anthology/N19-1342
  year: '2019'
N19-1343:
  abstract: We propose a simple and accurate model for coordination boundary identification.
    Our model decomposes the task into three sub-tasks during training; finding a
    coordinator, identifying inside boundaries of a pair of conjuncts, and selecting
    outside boundaries of it. For inference, we make use of probabilities of coordinators
    and conjuncts in the CKY parsing to find the optimal combination of coordinate
    structures. Experimental results demonstrate that our model achieves state-of-the-art
    results, ensuring that the global structure of coordinations is consistent.
  address: Minneapolis, Minnesota
  author:
  - first: Hiroki
    full: Hiroki Teranishi
    id: hiroki-teranishi
    last: Teranishi
  - first: Hiroyuki
    full: Hiroyuki Shindo
    id: hiroyuki-shindo
    last: Shindo
  - first: Yuji
    full: Yuji Matsumoto
    id: yuji-matsumoto
    last: Matsumoto
  author_string: Hiroki Teranishi, Hiroyuki Shindo, Yuji Matsumoto
  bibkey: teranishi-etal-2019-decomposed
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1343
  month: June
  page_first: '3394'
  page_last: '3403'
  pages: "3394\u20133403"
  paper_id: '343'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1343.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1343.jpg
  title: Decomposed Local Models for Coordinate Structure Parsing
  title_html: Decomposed Local Models for Coordinate Structure Parsing
  url: https://www.aclweb.org/anthology/N19-1343
  year: '2019'
N19-1344:
  abstract: An event-noun is a noun that has an argument structure similar to a predicate.
    Recent works, including those considered state-of-the-art, ignore event-nouns
    or build a single model for solving both Japanese predicate argument structure
    analysis (PASA) and event-noun argument structure analysis (ENASA). However, because
    there are interactions between predicates and event-nouns, it is not sufficient
    to target only predicates. To address this problem, we present a multi-task learning
    method for PASA and ENASA. Our multi-task models improved the performance of both
    tasks compared to a single-task model by sharing knowledge from each task. Moreover,
    in PASA, our models achieved state-of-the-art results in overall F1 scores on
    the NAIST Text Corpus. In addition, this is the first work to employ neural networks
    in ENASA.
  address: Minneapolis, Minnesota
  author:
  - first: Hikaru
    full: Hikaru Omori
    id: hikaru-omori
    last: Omori
  - first: Mamoru
    full: Mamoru Komachi
    id: mamoru-komachi
    last: Komachi
  author_string: Hikaru Omori, Mamoru Komachi
  bibkey: omori-komachi-2019-multi
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1344
  month: June
  page_first: '3404'
  page_last: '3414'
  pages: "3404\u20133414"
  paper_id: '344'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1344.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1344.jpg
  title: Multi-Task Learning for Japanese Predicate Argument Structure Analysis
  title_html: Multi-Task Learning for <span class="acl-fixed-case">J</span>apanese
    Predicate Argument Structure Analysis
  url: https://www.aclweb.org/anthology/N19-1344
  year: '2019'
N19-1345:
  abstract: The performance of a Part-of-speech (POS) tagger is highly dependent on
    the domain of the processed text, and for many domains there is no or only very
    little training data available. This work addresses the problem of POS tagging
    noisy user-generated text using a neural network. We propose an architecture that
    trains an out-of-domain model on a large newswire corpus, and transfers those
    weights by using them as a prior for a model trained on the target domain (a data-set
    of German Tweets) for which there is very little annotations available. The neural
    network has a standard bidirectional LSTM at its core. However, we find it crucial
    to also encode a set of task-specific features, and to obtain reliable (source-domain
    and target-domain) word representations. Experiments with different regularization
    techniques such as early stopping, dropout and fine-tuning the domain adaptation
    prior weights are conducted. Our best model uses external weights from the out-of-domain
    model, as well as feature embeddings, pre-trained word and sub-word embeddings
    and achieves a tagging accuracy of slightly over 90%, improving on the previous
    state of the art for this task.
  address: Minneapolis, Minnesota
  author:
  - first: Luisa
    full: "Luisa M\xE4rz"
    id: luisa-marz
    last: "M\xE4rz"
  - first: Dietrich
    full: Dietrich Trautmann
    id: dietrich-trautmann
    last: Trautmann
  - first: Benjamin
    full: Benjamin Roth
    id: benjamin-roth
    last: Roth
  author_string: "Luisa M\xE4rz, Dietrich Trautmann, Benjamin Roth"
  bibkey: marz-etal-2019-domain
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1345
  month: June
  page_first: '3415'
  page_last: '3420'
  pages: "3415\u20133420"
  paper_id: '345'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1345.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1345.jpg
  title: Domain adaptation for part-of-speech tagging of noisy user-generated text
  title_html: Domain adaptation for part-of-speech tagging of noisy user-generated
    text
  url: https://www.aclweb.org/anthology/N19-1345
  year: '2019'
N19-1346:
  abstract: "This paper introduces a new task \u2013 Chinese address parsing \u2013\
    \ the task of mapping Chinese addresses into semantically meaningful chunks. While\
    \ it is possible to model this problem using a conventional sequence labelling\
    \ approach, our observation is that there exist complex dependencies between labels\
    \ that cannot be readily captured by a simple linear-chain structure. We investigate\
    \ neural structured prediction models with latent variables to capture such rich\
    \ structural information within Chinese addresses. We create and publicly release\
    \ a new dataset consisting of 15K Chinese addresses, and conduct extensive experiments\
    \ on the dataset to investigate the model effectiveness and robustness. We release\
    \ our code and data at http://statnlp.org/research/sp."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1346.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1346.Supplementary.pdf
  author:
  - first: Hao
    full: Hao Li
    id: hao-li
    last: Li
  - first: Wei
    full: Wei Lu
    id: wei-lu
    last: Lu
  - first: Pengjun
    full: Pengjun Xie
    id: pengjun-xie
    last: Xie
  - first: Linlin
    full: Linlin Li
    id: linlin-li
    last: Li
  author_string: Hao Li, Wei Lu, Pengjun Xie, Linlin Li
  bibkey: li-etal-2019-neural-chinese
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1346
  month: June
  page_first: '3421'
  page_last: '3431'
  pages: "3421\u20133431"
  paper_id: '346'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1346.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1346.jpg
  title: Neural Chinese Address Parsing
  title_html: Neural <span class="acl-fixed-case">C</span>hinese Address Parsing
  url: https://www.aclweb.org/anthology/N19-1346
  year: '2019'
N19-1347:
  abstract: On the one hand, nowadays, fake news articles are easily propagated through
    various online media platforms and have become a grand threat to the trustworthiness
    of information. On the other hand, our understanding of the language of fake news
    is still minimal. Incorporating hierarchical discourse-level structure of fake
    and real news articles is one crucial step toward a better understanding of how
    these articles are structured. Nevertheless, this has rarely been investigated
    in the fake news detection domain and faces tremendous challenges. First, existing
    methods for capturing discourse-level structure rely on annotated corpora which
    are not available for fake news datasets. Second, how to extract out useful information
    from such discovered structures is another challenge. To address these challenges,
    we propose Hierarchical Discourse-level Structure for Fake news detection. HDSF
    learns and constructs a discourse-level structure for fake/real news articles
    in an automated and data-driven manner. Moreover, we identify insightful structure-related
    properties, which can explain the discovered structures and boost our understating
    of fake news. Conducted experiments show the effectiveness of the proposed approach.
    Further structural analysis suggests that real and fake news present substantial
    differences in the hierarchical discourse-level structures.
  address: Minneapolis, Minnesota
  author:
  - first: Hamid
    full: Hamid Karimi
    id: hamid-karimi
    last: Karimi
  - first: Jiliang
    full: Jiliang Tang
    id: jiliang-tang
    last: Tang
  author_string: Hamid Karimi, Jiliang Tang
  bibkey: karimi-tang-2019-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1347
  month: June
  page_first: '3432'
  page_last: '3442'
  pages: "3432\u20133442"
  paper_id: '347'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1347.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1347.jpg
  title: Learning Hierarchical Discourse-level Structure for Fake News Detection
  title_html: Learning Hierarchical Discourse-level Structure for Fake News Detection
  url: https://www.aclweb.org/anthology/N19-1347
  year: '2019'
N19-1348:
  abstract: 'Sentence fusion is the task of joining several independent sentences
    into a single coherent text. Current datasets for sentence fusion are small and
    insufficient for training modern neural models. In this paper, we propose a method
    for automatically-generating fusion examples from raw text and present DiscoFuse,
    a large scale dataset for discourse-based sentence fusion. We author a set of
    rules for identifying a diverse set of discourse phenomena in raw text, and decomposing
    the text into two independent sentences. We apply our approach on two document
    collections: Wikipedia and Sports articles, yielding 60 million fusion examples
    annotated with discourse information required to reconstruct the fused text. We
    develop a sequence-to-sequence model on DiscoFuse and thoroughly analyze its strengths
    and weaknesses with respect to the various discourse phenomena, using both automatic
    as well as human evaluation. Finally, we conduct transfer learning experiments
    with WebSplit, a recent dataset for text simplification. We show that pretraining
    on DiscoFuse substantially improves performance on WebSplit when viewed as a sentence
    fusion task.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/364767763
    type: video
    url: https://vimeo.com/364767763
  author:
  - first: Mor
    full: Mor Geva
    id: mor-geva
    last: Geva
  - first: Eric
    full: Eric Malmi
    id: eric-malmi
    last: Malmi
  - first: Idan
    full: Idan Szpektor
    id: idan-szpektor
    last: Szpektor
  - first: Jonathan
    full: Jonathan Berant
    id: jonathan-berant
    last: Berant
  author_string: Mor Geva, Eric Malmi, Idan Szpektor, Jonathan Berant
  bibkey: geva-etal-2019-discofuse
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1348
  month: June
  page_first: '3443'
  page_last: '3455'
  pages: "3443\u20133455"
  paper_id: '348'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1348.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1348.jpg
  title: 'DiscoFuse: A Large-Scale Dataset for Discourse-Based Sentence Fusion'
  title_html: '<span class="acl-fixed-case">D</span>isco<span class="acl-fixed-case">F</span>use:
    A Large-Scale Dataset for Discourse-Based Sentence Fusion'
  url: https://www.aclweb.org/anthology/N19-1348
  year: '2019'
N19-1349:
  abstract: "Sequence-to-sequence models for open-domain dialogue generation tend\
    \ to favor generic, uninformative responses. Past work has focused on word frequency-based\
    \ approaches to improving specificity, such as penalizing responses with only\
    \ common words. In this work, we examine whether specificity is solely a frequency-related\
    \ notion and find that more linguistically-driven specificity measures are better\
    \ suited to improving response informativeness. However, we find that forcing\
    \ a sequence-to-sequence model to be more specific can expose a host of other\
    \ problems in the responses, including flawed discourse and implausible semantics.\
    \ We rerank our model\u2019s outputs using externally-trained classifiers targeting\
    \ each of these identified factors. Experiments show that our final model using\
    \ linguistically motivated specificity and plausibility reranking improves the\
    \ informativeness, reasonableness, and grammatically of responses."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/364782035
    type: video
    url: https://vimeo.com/364782035
  author:
  - first: Wei-Jen
    full: Wei-Jen Ko
    id: wei-jen-ko
    last: Ko
  - first: Greg
    full: Greg Durrett
    id: greg-durrett
    last: Durrett
  - first: Junyi Jessy
    full: Junyi Jessy Li
    id: junyi-jessy-li
    last: Li
  author_string: Wei-Jen Ko, Greg Durrett, Junyi Jessy Li
  bibkey: ko-etal-2019-linguistically
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1349
  month: June
  page_first: '3456'
  page_last: '3466'
  pages: "3456\u20133466"
  paper_id: '349'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1349.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1349.jpg
  title: Linguistically-Informed Specificity and Semantic Plausibility for Dialogue
    Generation
  title_html: Linguistically-Informed Specificity and Semantic Plausibility for Dialogue
    Generation
  url: https://www.aclweb.org/anthology/N19-1349
  year: '2019'
N19-1350:
  abstract: When reading a text, it is common to become stuck on unfamiliar words
    and phrases, such as polysemous words with novel senses, rarely used idioms, internet
    slang, or emerging entities. If we humans cannot figure out the meaning of those
    expressions from the immediate local context, we consult dictionaries for definitions
    or search documents or the web to find other global context to help in interpretation.
    Can machines help us do this work? Which type of context is more important for
    machines to solve the problem? To answer these questions, we undertake a task
    of describing a given phrase in natural language based on its local and global
    contexts. To solve this task, we propose a neural description model that consists
    of two context encoders and a description decoder. In contrast to the existing
    methods for non-standard English explanation [Ni+ 2017] and definition generation
    [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues
    from both local and global contexts. Experimental results on three existing datasets
    (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created
    from Wikipedia demonstrate the effectiveness of our method over previous work.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/364799403
    type: video
    url: https://vimeo.com/364799403
  author:
  - first: Shonosuke
    full: Shonosuke Ishiwatari
    id: shonosuke-ishiwatari
    last: Ishiwatari
  - first: Hiroaki
    full: Hiroaki Hayashi
    id: hiroaki-hayashi
    last: Hayashi
  - first: Naoki
    full: Naoki Yoshinaga
    id: naoki-yoshinaga
    last: Yoshinaga
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  - first: Shoetsu
    full: Shoetsu Sato
    id: shoetsu-sato
    last: Sato
  - first: Masashi
    full: Masashi Toyoda
    id: masashi-toyoda
    last: Toyoda
  - first: Masaru
    full: Masaru Kitsuregawa
    id: masaru-kitsuregawa
    last: Kitsuregawa
  author_string: Shonosuke Ishiwatari, Hiroaki Hayashi, Naoki Yoshinaga, Graham Neubig,
    Shoetsu Sato, Masashi Toyoda, Masaru Kitsuregawa
  bibkey: ishiwatari-etal-2019-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1350
  month: June
  page_first: '3467'
  page_last: '3476'
  pages: "3467\u20133476"
  paper_id: '350'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1350.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1350.jpg
  title: Learning to Describe Unknown Phrases with Local and Global Contexts
  title_html: Learning to Describe Unknown Phrases with Local and Global Contexts
  url: https://www.aclweb.org/anthology/N19-1350
  year: '2019'
N19-1351:
  abstract: "Current state of the art systems in NLP heavily rely on manually annotated\
    \ datasets, which are expensive to construct. Very little work adequately exploits\
    \ unannotated data \u2013 such as discourse markers between sentences \u2013 mainly\
    \ because of data sparseness and ineffective extraction methods. In the present\
    \ work, we propose a method to automatically discover sentence pairs with relevant\
    \ discourse markers, and apply it to massive amounts of data. Our resulting dataset\
    \ contains 174 discourse markers with at least 10k examples each, even for rare\
    \ markers such as \u201Ccoincidentally\u201D or \u201Camazingly\u201D. We use\
    \ the resulting data as supervision for learning transferable sentence embeddings.\
    \ In addition, we show that even though sentence representation learning through\
    \ prediction of discourse marker yields state of the art results across different\
    \ transfer tasks, it\u2019s not clear that our models made use of the semantic\
    \ relation between sentences, thus leaving room for further improvements."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/364816868
    type: video
    url: https://vimeo.com/364816868
  author:
  - first: Damien
    full: Damien Sileo
    id: damien-sileo
    last: Sileo
  - first: Tim
    full: Tim Van De Cruys
    id: tim-van-de-cruys
    last: Van De Cruys
  - first: Camille
    full: Camille Pradel
    id: camille-pradel
    last: Pradel
  - first: Philippe
    full: Philippe Muller
    id: philippe-muller
    last: Muller
  author_string: Damien Sileo, Tim Van De Cruys, Camille Pradel, Philippe Muller
  bibkey: sileo-etal-2019-mining
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1351
  month: June
  page_first: '3477'
  page_last: '3486'
  pages: "3477\u20133486"
  paper_id: '351'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1351.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1351.jpg
  title: Mining Discourse Markers for Unsupervised Sentence Representation Learning
  title_html: Mining Discourse Markers for Unsupervised Sentence Representation Learning
  url: https://www.aclweb.org/anthology/N19-1351
  year: '2019'
N19-1352:
  abstract: 'With the rapid development in deep learning, deep neural networks have
    been widely adopted in many real-life natural language applications. Under deep
    neural networks, a pre-defined vocabulary is required to vectorize text inputs.
    The canonical approach to select pre-defined vocabulary is based on the word frequency,
    where a threshold is selected to cut off the long tail distribution. However,
    we observed that such a simple approach could easily lead to under-sized vocabulary
    or over-sized vocabulary issues. Therefore, we are interested in understanding
    how the end-task classification accuracy is related to the vocabulary size and
    what is the minimum required vocabulary size to achieve a specific performance.
    In this paper, we provide a more sophisticated variational vocabulary dropout
    (VVD) based on variational dropout to perform vocabulary selection, which can
    intelligently select the subset of the vocabulary to achieve the required performance.
    To evaluate different algorithms on the newly proposed vocabulary selection problem,
    we propose two new metrics: Area Under Accuracy-Vocab Curve and Vocab Size under
    X% Accuracy Drop. Through extensive experiments on various NLP classification
    tasks, our variational framework is shown to significantly outperform the frequency-based
    and other selection baselines on these metrics.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/347426957
    type: video
    url: https://vimeo.com/347426957
  author:
  - first: Wenhu
    full: Wenhu Chen
    id: wenhu-chen
    last: Chen
  - first: Yu
    full: Yu Su
    id: yu-su
    last: Su
  - first: Yilin
    full: Yilin Shen
    id: yilin-shen
    last: Shen
  - first: Zhiyu
    full: Zhiyu Chen
    id: zhiyu-chen
    last: Chen
  - first: Xifeng
    full: Xifeng Yan
    id: xifeng-yan
    last: Yan
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  author_string: Wenhu Chen, Yu Su, Yilin Shen, Zhiyu Chen, Xifeng Yan, William Yang
    Wang
  bibkey: chen-etal-2019-large
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1352
  month: June
  page_first: '3487'
  page_last: '3497'
  pages: "3487\u20133497"
  paper_id: '352'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1352.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1352.jpg
  title: How Large a Vocabulary Does Text Classification Need? A Variational Approach
    to Vocabulary Selection
  title_html: How Large a Vocabulary Does Text Classification Need? A Variational
    Approach to Vocabulary Selection
  url: https://www.aclweb.org/anthology/N19-1352
  year: '2019'
N19-1353:
  abstract: 'The idea of subword-based word embeddings has been proposed in the literature,
    mainly for solving the out-of-vocabulary (OOV) word problem observed in standard
    word-based word embeddings. In this paper, we propose a method of reconstructing
    pre-trained word embeddings using subword information that can effectively represent
    a large number of subword embeddings in a considerably small fixed space. The
    key techniques of our method are twofold: memory-shared embeddings and a variant
    of the key-value-query self-attention mechanism. Our experiments show that our
    reconstructed subword-based embeddings can successfully imitate well-trained word
    embeddings in a small fixed space while preventing quality degradation across
    several linguistic benchmark datasets, and can simultaneously predict effective
    embeddings of OOV words. We also demonstrate the effectiveness of our reconstruction
    method when we apply them to downstream tasks.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1353.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1353.Supplementary.pdf
  - filename: https://vimeo.com/347430247
    type: video
    url: https://vimeo.com/347430247
  author:
  - first: Shota
    full: Shota Sasaki
    id: shota-sasaki
    last: Sasaki
  - first: Jun
    full: Jun Suzuki
    id: jun-suzuki
    last: Suzuki
  - first: Kentaro
    full: Kentaro Inui
    id: kentaro-inui
    last: Inui
  author_string: Shota Sasaki, Jun Suzuki, Kentaro Inui
  bibkey: sasaki-etal-2019-subword
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1353
  month: June
  page_first: '3498'
  page_last: '3508'
  pages: "3498\u20133508"
  paper_id: '353'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1353.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1353.jpg
  title: Subword-based Compact Reconstruction of Word Embeddings
  title_html: <span class="acl-fixed-case">S</span>ubword-based <span class="acl-fixed-case">C</span>ompact
    <span class="acl-fixed-case">R</span>econstruction of <span class="acl-fixed-case">W</span>ord
    <span class="acl-fixed-case">E</span>mbeddings
  url: https://www.aclweb.org/anthology/N19-1353
  year: '2019'
N19-1354:
  abstract: 'While neural dependency parsers provide state-of-the-art accuracy for
    several languages, they still rely on large amounts of costly labeled training
    data. We demonstrate that in the small data regime, where uncertainty around parameter
    estimation and model prediction matters the most, Bayesian neural modeling is
    very effective. In order to overcome the computational and statistical costs of
    the approximate inference step in this framework, we utilize an efficient sampling
    procedure via stochastic gradient Langevin dynamics to generate samples from the
    approximated posterior. Moreover, we show that our Bayesian neural parser can
    be further improved when integrated into a multi-task parsing and POS tagging
    framework, designed to minimize task interference via an adversarial procedure.
    When trained and tested on 6 languages with less than 5k training instances, our
    parser consistently outperforms the strong bilstm baseline (Kiperwasser and Goldberg,
    2016). Compared with the biaffine parser (Dozat et al., 2017) our model achieves
    an improvement of up to 3% for Vietnames and Irish, while our multi-task model
    achieves an improvement of up to 9% across five languages: Farsi, Russian, Turkish,
    Vietnamese, and Irish.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/361752492
    type: video
    url: https://vimeo.com/361752492
  author:
  - first: Ehsan
    full: Ehsan Shareghi
    id: ehsan-shareghi
    last: Shareghi
  - first: Yingzhen
    full: Yingzhen Li
    id: yingzhen-li
    last: Li
  - first: Yi
    full: Yi Zhu
    id: yi-zhu
    last: Zhu
  - first: Roi
    full: Roi Reichart
    id: roi-reichart
    last: Reichart
  - first: Anna
    full: Anna Korhonen
    id: anna-korhonen
    last: Korhonen
  author_string: Ehsan Shareghi, Yingzhen Li, Yi Zhu, Roi Reichart, Anna Korhonen
  bibkey: shareghi-etal-2019-bayesian
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1354
  month: June
  page_first: '3509'
  page_last: '3519'
  pages: "3509\u20133519"
  paper_id: '354'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1354.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1354.jpg
  title: Bayesian Learning for Neural Dependency Parsing
  title_html: <span class="acl-fixed-case">B</span>ayesian Learning for Neural Dependency
    Parsing
  url: https://www.aclweb.org/anthology/N19-1354
  year: '2019'
N19-1355:
  abstract: Multi-task learning (MTL) has achieved success over a wide range of problems,
    where the goal is to improve the performance of a primary task using a set of
    relevant auxiliary tasks. However, when the usefulness of the auxiliary tasks
    w.r.t. the primary task is not known a priori, the success of MTL models depends
    on the correct choice of these auxiliary tasks and also a balanced mixing ratio
    of these tasks during alternate training. These two problems could be resolved
    via manual intuition or hyper-parameter tuning over all combinatorial task choices,
    but this introduces inductive bias or is not scalable when the number of candidate
    auxiliary tasks is very large. To address these issues, we present AutoSeM, a
    two-stage MTL pipeline, where the first stage automatically selects the most useful
    auxiliary tasks via a Beta-Bernoulli multi-armed bandit with Thompson Sampling,
    and the second stage learns the training mixing ratio of these selected auxiliary
    tasks via a Gaussian Process based Bayesian optimization framework. We conduct
    several MTL experiments on the GLUE language understanding tasks, and show that
    our AutoSeM framework can successfully find relevant auxiliary tasks and automatically
    learn their mixing ratio, achieving significant performance boosts on several
    primary tasks. Finally, we present ablations for each stage of AutoSeM and analyze
    the learned auxiliary task choices.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1355.Software.pdf
    type: software
    url: https://www.aclweb.org/anthology/attachments/N19-1355.Software.pdf
  - filename: https://vimeo.com/361755239
    type: video
    url: https://vimeo.com/361755239
  author:
  - first: Han
    full: Han Guo
    id: han-guo
    last: Guo
  - first: Ramakanth
    full: Ramakanth Pasunuru
    id: ramakanth-pasunuru
    last: Pasunuru
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  author_string: Han Guo, Ramakanth Pasunuru, Mohit Bansal
  bibkey: guo-etal-2019-autosem
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1355
  month: June
  page_first: '3520'
  page_last: '3531'
  pages: "3520\u20133531"
  paper_id: '355'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1355.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1355.jpg
  title: 'AutoSeM: Automatic Task Selection and Mixing in Multi-Task Learning'
  title_html: '<span class="acl-fixed-case">A</span>uto<span class="acl-fixed-case">S</span>e<span
    class="acl-fixed-case">M</span>: Automatic Task Selection and Mixing in Multi-Task
    Learning'
  url: https://www.aclweb.org/anthology/N19-1355
  year: '2019'
N19-1356:
  abstract: "How do typological properties such as word order and morphological case\
    \ marking affect the ability of neural sequence models to acquire the syntax of\
    \ a language? Cross-linguistic comparisons of RNNs\u2019 syntactic performance\
    \ (e.g., on subject-verb agreement prediction) are complicated by the fact that\
    \ any two languages differ in multiple typological properties, as well as by differences\
    \ in training corpus. We propose a paradigm that addresses these issues: we create\
    \ synthetic versions of English, which differ from English in one or more typological\
    \ parameters, and generate corpora for those languages based on a parsed English\
    \ corpus. We report a series of experiments in which RNNs were trained to predict\
    \ agreement features for verbs in each of those synthetic languages. Among other\
    \ findings, (1) performance was higher in subject-verb-object order (as in English)\
    \ than in subject-object-verb order (as in Japanese), suggesting that RNNs have\
    \ a recency bias; (2) predicting agreement with both subject and object (polypersonal\
    \ agreement) improves over predicting each separately, suggesting that underlying\
    \ syntactic knowledge transfers across the two tasks; and (3) overt morphological\
    \ case makes agreement prediction significantly easier, regardless of word order."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/347430311
    type: video
    url: https://vimeo.com/347430311
  author:
  - first: Shauli
    full: Shauli Ravfogel
    id: shauli-ravfogel
    last: Ravfogel
  - first: Yoav
    full: Yoav Goldberg
    id: yoav-goldberg
    last: Goldberg
  - first: Tal
    full: Tal Linzen
    id: tal-linzen
    last: Linzen
  author_string: Shauli Ravfogel, Yoav Goldberg, Tal Linzen
  bibkey: ravfogel-etal-2019-studying
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1356
  month: June
  page_first: '3532'
  page_last: '3542'
  pages: "3532\u20133542"
  paper_id: '356'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1356.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1356.jpg
  title: Studying the Inductive Biases of RNNs with Synthetic Variations of Natural
    Languages
  title_html: Studying the Inductive Biases of <span class="acl-fixed-case">RNN</span>s
    with Synthetic Variations of Natural Languages
  url: https://www.aclweb.org/anthology/N19-1356
  year: '2019'
N19-1357:
  abstract: "Attention mechanisms have seen wide adoption in neural NLP models. In\
    \ addition to improving predictive performance, these are often touted as affording\
    \ transparency: models equipped with attention provide a distribution over attended-to\
    \ input units, and this is often presented (at least implicitly) as communicating\
    \ the relative importance of inputs. However, it is unclear what relationship\
    \ exists between attention weights and model outputs. In this work we perform\
    \ extensive experiments across a variety of NLP tasks that aim to assess the degree\
    \ to which attention weights provide meaningful \u201Cexplanations\u201D for predictions.\
    \ We find that they largely do not. For example, learned attention weights are\
    \ frequently uncorrelated with gradient-based measures of feature importance,\
    \ and one can identify very different attention distributions that nonetheless\
    \ yield equivalent predictions. Our findings show that standard attention modules\
    \ do not provide meaningful explanations and should not be treated as though they\
    \ do."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/359703968
    type: video
    url: https://vimeo.com/359703968
  author:
  - first: Sarthak
    full: Sarthak Jain
    id: sarthak-jain
    last: Jain
  - first: Byron C.
    full: Byron C. Wallace
    id: byron-c-wallace
    last: Wallace
  author_string: Sarthak Jain, Byron C. Wallace
  bibkey: jain-wallace-2019-attention
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1357
  month: June
  page_first: '3543'
  page_last: '3556'
  pages: "3543\u20133556"
  paper_id: '357'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1357.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1357.jpg
  title: Attention is not Explanation
  title_html: <span class="acl-fixed-case">A</span>ttention is not <span class="acl-fixed-case">E</span>xplanation
  url: https://www.aclweb.org/anthology/N19-1357
  year: '2019'
N19-1358:
  abstract: Text-based adventure games provide a platform on which to explore reinforcement
    learning in the context of a combinatorial action space, such as natural language.
    We present a deep reinforcement learning architecture that represents the game
    state as a knowledge graph which is learned during exploration. This graph is
    used to prune the action space, enabling more efficient exploration. The question
    of which action to take can be reduced to a question-answering task, a form of
    transfer learning that pre-trains certain parts of our architecture. In experiments
    using the TextWorld framework, we show that our proposed technique can learn a
    control policy faster than baseline alternatives. We have also open-sourced our
    code at https://github.com/rajammanabrolu/KG-DQN.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/359702665
    type: video
    url: https://vimeo.com/359702665
  author:
  - first: Prithviraj
    full: Prithviraj Ammanabrolu
    id: prithviraj-ammanabrolu
    last: Ammanabrolu
  - first: Mark
    full: Mark Riedl
    id: mark-riedl
    last: Riedl
  author_string: Prithviraj Ammanabrolu, Mark Riedl
  bibkey: ammanabrolu-riedl-2019-playing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1358
  month: June
  page_first: '3557'
  page_last: '3565'
  pages: "3557\u20133565"
  paper_id: '358'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1358.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1358.jpg
  title: Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning
  title_html: Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning
  url: https://www.aclweb.org/anthology/N19-1358
  year: '2019'
N19-1359:
  abstract: Multi-head attention is appealing for its ability to jointly extract different
    types of information from multiple representation subspaces. Concerning the information
    aggregation, a common practice is to use a concatenation followed by a linear
    transformation, which may not fully exploit the expressiveness of multi-head attention.
    In this work, we propose to improve the information aggregation for multi-head
    attention with a more powerful routing-by-agreement algorithm. Specifically, the
    routing algorithm iteratively updates the proportion of how much a part (i.e.
    the distinct information learned from a specific subspace) should be assigned
    to a whole (i.e. the final output representation), based on the agreement between
    parts and wholes. Experimental results on linguistic probing tasks and machine
    translation tasks prove the superiority of the advanced information aggregation
    over the standard linear transformation.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/359701494
    type: video
    url: https://vimeo.com/359701494
  author:
  - first: Jian
    full: Jian Li
    id: jian-li
    last: Li
  - first: Baosong
    full: Baosong Yang
    id: baosong-yang
    last: Yang
  - first: Zi-Yi
    full: Zi-Yi Dou
    id: zi-yi-dou
    last: Dou
  - first: Xing
    full: Xing Wang
    id: xing-wang
    last: Wang
  - first: Michael R.
    full: Michael R. Lyu
    id: michael-r-lyu
    last: Lyu
  - first: Zhaopeng
    full: Zhaopeng Tu
    id: zhaopeng-tu
    last: Tu
  author_string: Jian Li, Baosong Yang, Zi-Yi Dou, Xing Wang, Michael R. Lyu, Zhaopeng
    Tu
  bibkey: li-etal-2019-information
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1359
  month: June
  page_first: '3566'
  page_last: '3575'
  pages: "3566\u20133575"
  paper_id: '359'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1359.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1359.jpg
  title: Information Aggregation for Multi-Head Attention with Routing-by-Agreement
  title_html: Information Aggregation for Multi-Head Attention with Routing-by-Agreement
  url: https://www.aclweb.org/anthology/N19-1359
  year: '2019'
N19-1360:
  abstract: "We describe a new semantic parsing setting that allows users to query\
    \ the system using both natural language questions and actions within a graphical\
    \ user interface. Multiple time series belonging to an entity of interest are\
    \ stored in a database and the user interacts with the system to obtain a better\
    \ understanding of the entity\u2019s state and behavior, entailing sequences of\
    \ actions and questions whose answers may depend on previous factual or navigational\
    \ interactions. We design an LSTM-based encoder-decoder architecture that models\
    \ context dependency through copying mechanisms and multiple levels of attention\
    \ over inputs and previous outputs. When trained to predict tokens using supervised\
    \ learning, the proposed architecture substantially outperforms standard sequence\
    \ generation baselines. Training the architecture using policy gradient leads\
    \ to further improvements in performance, reaching a sequence-level accuracy of\
    \ 88.7% on artificial data and 74.8% on real data."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1360.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1360.Supplementary.pdf
  - filename: https://vimeo.com/359699975
    type: video
    url: https://vimeo.com/359699975
  author:
  - first: Charles
    full: Charles Chen
    id: charles-chen-jr
    last: Chen
  - first: Razvan
    full: Razvan Bunescu
    id: razvan-bunescu
    last: Bunescu
  author_string: Charles Chen, Razvan Bunescu
  bibkey: chen-bunescu-2019-context
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1360
  month: June
  page_first: '3576'
  page_last: '3585'
  pages: "3576\u20133585"
  paper_id: '360'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1360.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1360.jpg
  title: Context Dependent Semantic Parsing over Temporally Structured Data
  title_html: Context Dependent Semantic Parsing over Temporally Structured Data
  url: https://www.aclweb.org/anthology/N19-1360
  year: '2019'
N19-1361:
  abstract: 'Identifying the intent of a citation in scientific papers (e.g., background
    information, use of methods, comparing results) is critical for machine reading
    of individual publications and automated analysis of the scientific literature.
    We propose structural scaffolds, a multitask model to incorporate structural information
    of scientific papers into citations for effective classification of citation intents.
    Our model achieves a new state-of-the-art on an existing ACL anthology dataset
    (ACL-ARC) with a 13.3% absolute increase in F1 score, without relying on external
    linguistic resources or hand-engineered features as done in existing methods.
    In addition, we introduce a new dataset of citation intents (SciCite) which is
    more than five times larger and covers multiple scientific domains compared with
    existing datasets. Our code and data are available at: https://github.com/allenai/scicite.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/359698633
    type: video
    url: https://vimeo.com/359698633
  author:
  - first: Arman
    full: Arman Cohan
    id: arman-cohan
    last: Cohan
  - first: Waleed
    full: Waleed Ammar
    id: waleed-ammar
    last: Ammar
  - first: Madeleine
    full: Madeleine van Zuylen
    id: madeleine-van-zuylen
    last: van Zuylen
  - first: Field
    full: Field Cady
    id: field-cady
    last: Cady
  author_string: Arman Cohan, Waleed Ammar, Madeleine van Zuylen, Field Cady
  bibkey: cohan-etal-2019-structural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1361
  month: June
  page_first: '3586'
  page_last: '3596'
  pages: "3586\u20133596"
  paper_id: '361'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1361.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1361.jpg
  title: Structural Scaffolds for Citation Intent Classification in Scientific Publications
  title_html: Structural Scaffolds for Citation Intent Classification in Scientific
    Publications
  url: https://www.aclweb.org/anthology/N19-1361
  year: '2019'
N19-1362:
  abstract: "Reasoning about implied relationships (e.g. paraphrastic, common sense,\
    \ encyclopedic) between pairs of words is crucial for many cross-sentence inference\
    \ problems. This paper proposes new methods for learning and using embeddings\
    \ of word pairs that implicitly represent background knowledge about such relationships.\
    \ Our pairwise embeddings are computed as a compositional function of each word\u2019\
    s representation, which is learned by maximizing the pointwise mutual information\
    \ (PMI) with the contexts in which the the two words co-occur. We add these representations\
    \ to the cross-sentence attention layer of existing inference models (e.g. BiDAF\
    \ for QA, ESIM for NLI), instead of extending or replacing existing word embeddings.\
    \ Experiments show a gain of 2.7% on the recently released SQuAD 2.0 and 1.3%\
    \ on MultiNLI. Our representations also aid in better generalization with gains\
    \ of around 6-7% on adversarial SQuAD datasets, and 8.8% on the adversarial entailment\
    \ test set by Glockner et al. (2018)."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/356133444
    type: video
    url: https://vimeo.com/356133444
  author:
  - first: Mandar
    full: Mandar Joshi
    id: mandar-joshi
    last: Joshi
  - first: Eunsol
    full: Eunsol Choi
    id: eunsol-choi
    last: Choi
  - first: Omer
    full: Omer Levy
    id: omer-levy
    last: Levy
  - first: Daniel
    full: Daniel Weld
    id: daniel-s-weld
    last: Weld
  - first: Luke
    full: Luke Zettlemoyer
    id: luke-zettlemoyer
    last: Zettlemoyer
  author_string: Mandar Joshi, Eunsol Choi, Omer Levy, Daniel Weld, Luke Zettlemoyer
  bibkey: joshi-etal-2019-pair2vec
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1362
  month: June
  page_first: '3597'
  page_last: '3608'
  pages: "3597\u20133608"
  paper_id: '362'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1362.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1362.jpg
  title: 'pair2vec: Compositional Word-Pair Embeddings for Cross-Sentence Inference'
  title_html: 'pair2vec: Compositional Word-Pair Embeddings for Cross-Sentence Inference'
  url: https://www.aclweb.org/anthology/N19-1362
  year: '2019'
N19-1363:
  abstract: Inducing diversity in the task of paraphrasing is an important problem
    in NLP with applications in data augmentation and conversational agents. Previous
    paraphrasing approaches have mainly focused on the issue of generating semantically
    similar paraphrases while paying little attention towards diversity. In fact,
    most of the methods rely solely on top-k beam search sequences to obtain a set
    of paraphrases. The resulting set, however, contains many structurally similar
    sentences. In this work, we focus on the task of obtaining highly diverse paraphrases
    while not compromising on paraphrasing quality. We provide a novel formulation
    of the problem in terms of monotone submodular function maximization, specifically
    targeted towards the task of paraphrasing. Additionally, we demonstrate the effectiveness
    of our method for data augmentation on multiple tasks such as intent classification
    and paraphrase recognition. In order to drive further research, we have made the
    source code available.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1363.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1363.Supplementary.pdf
  - filename: N19-1363.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1363.Presentation.pdf
  - filename: https://vimeo.com/356143764
    type: video
    url: https://vimeo.com/356143764
  author:
  - first: Ashutosh
    full: Ashutosh Kumar
    id: ashutosh-kumar
    last: Kumar
  - first: Satwik
    full: Satwik Bhattamishra
    id: satwik-bhattamishra
    last: Bhattamishra
  - first: Manik
    full: Manik Bhandari
    id: manik-bhandari
    last: Bhandari
  - first: Partha
    full: Partha Talukdar
    id: partha-talukdar
    last: Talukdar
  author_string: Ashutosh Kumar, Satwik Bhattamishra, Manik Bhandari, Partha Talukdar
  bibkey: kumar-etal-2019-submodular
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1363
  month: June
  page_first: '3609'
  page_last: '3619'
  pages: "3609\u20133619"
  paper_id: '363'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1363.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1363.jpg
  title: Submodular Optimization-based Diverse Paraphrasing and its Effectiveness
    in Data Augmentation
  title_html: Submodular Optimization-based Diverse Paraphrasing and its Effectiveness
    in Data Augmentation
  url: https://www.aclweb.org/anthology/N19-1363
  year: '2019'
N19-1364:
  abstract: "Modeling what makes a request persuasive - eliciting the desired response\
    \ from a reader - is critical to the study of propaganda, behavioral economics,\
    \ and advertising. Yet current models can\u2019t quantify the persuasiveness of\
    \ requests or extract successful persuasive strategies. Building on theories of\
    \ persuasion, we propose a neural network to quantify persuasiveness and identify\
    \ the persuasive strategies in advocacy requests. Our semi-supervised hierarchical\
    \ neural network model is supervised by the number of people persuaded to take\
    \ actions and partially supervised at the sentence level with human-labeled rhetorical\
    \ strategies. Our method outperforms several baselines, uncovers persuasive strategies\
    \ - offering increased interpretability of persuasive speech - and has applications\
    \ for other situations with document-level supervision but only partial sentence\
    \ supervision."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/356153695
    type: video
    url: https://vimeo.com/356153695
  author:
  - first: Diyi
    full: Diyi Yang
    id: diyi-yang
    last: Yang
  - first: Jiaao
    full: Jiaao Chen
    id: jiaao-chen
    last: Chen
  - first: Zichao
    full: Zichao Yang
    id: zichao-yang
    last: Yang
  - first: Dan
    full: Dan Jurafsky
    id: dan-jurafsky
    last: Jurafsky
  - first: Eduard
    full: Eduard Hovy
    id: eduard-hovy
    last: Hovy
  author_string: Diyi Yang, Jiaao Chen, Zichao Yang, Dan Jurafsky, Eduard Hovy
  bibkey: yang-etal-2019-lets
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1364
  month: June
  page_first: '3620'
  page_last: '3630'
  pages: "3620\u20133630"
  paper_id: '364'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1364.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1364.jpg
  title: "Let\u2019s Make Your Request More Persuasive: Modeling Persuasive Strategies\
    \ via Semi-Supervised Neural Nets on Crowdfunding Platforms"
  title_html: "Let\u2019s Make Your Request More Persuasive: Modeling Persuasive Strategies\
    \ via Semi-Supervised Neural Nets on Crowdfunding Platforms"
  url: https://www.aclweb.org/anthology/N19-1364
  year: '2019'
N19-1365:
  abstract: "We introduce Recursive Routing Networks (RRNs), which are modular, adaptable\
    \ models that learn effectively in diverse environments. RRNs consist of a set\
    \ of functions, typically organized into a grid, and a meta-learner decision-making\
    \ component called the router. The model jointly optimizes the parameters of the\
    \ functions and the meta-learner\u2019s policy for routing inputs through those\
    \ functions. RRNs can be incorporated into existing architectures in a number\
    \ of ways; we explore adding them to word representation layers, recurrent network\
    \ hidden layers, and classifier layers. Our evaluation task is natural language\
    \ inference (NLI). Using the MultiNLI corpus, we show that an RRN\u2019s routing\
    \ decisions reflect the high-level genre structure of that corpus. To show that\
    \ RRNs can learn to specialize to more fine-grained semantic distinctions, we\
    \ introduce a new corpus of NLI examples involving implicative predicates, and\
    \ show that the model components become fine-tuned to the inferential signatures\
    \ that are characteristic of these predicates."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/356167288
    type: video
    url: https://vimeo.com/356167288
  author:
  - first: Ignacio
    full: Ignacio Cases
    id: ignacio-cases
    last: Cases
  - first: Clemens
    full: Clemens Rosenbaum
    id: clemens-rosenbaum
    last: Rosenbaum
  - first: Matthew
    full: Matthew Riemer
    id: matthew-riemer
    last: Riemer
  - first: Atticus
    full: Atticus Geiger
    id: atticus-geiger
    last: Geiger
  - first: Tim
    full: Tim Klinger
    id: tim-klinger
    last: Klinger
  - first: Alex
    full: Alex Tamkin
    id: alex-tamkin
    last: Tamkin
  - first: Olivia
    full: Olivia Li
    id: olivia-li
    last: Li
  - first: Sandhini
    full: Sandhini Agarwal
    id: sandhini-agarwal
    last: Agarwal
  - first: Joshua D.
    full: Joshua D. Greene
    id: joshua-d-greene
    last: Greene
  - first: Dan
    full: Dan Jurafsky
    id: dan-jurafsky
    last: Jurafsky
  - first: Christopher
    full: Christopher Potts
    id: christopher-potts
    last: Potts
  - first: Lauri
    full: Lauri Karttunen
    id: lauri-karttunen
    last: Karttunen
  author_string: Ignacio Cases, Clemens Rosenbaum, Matthew Riemer, Atticus Geiger,
    Tim Klinger, Alex Tamkin, Olivia Li, Sandhini Agarwal, Joshua D. Greene, Dan Jurafsky,
    Christopher Potts, Lauri Karttunen
  bibkey: cases-etal-2019-recursive
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1365
  month: June
  page_first: '3631'
  page_last: '3648'
  pages: "3631\u20133648"
  paper_id: '365'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1365.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1365.jpg
  title: 'Recursive Routing Networks: Learning to Compose Modules for Language Understanding'
  title_html: 'Recursive Routing Networks: Learning to Compose Modules for Language
    Understanding'
  url: https://www.aclweb.org/anthology/N19-1365
  year: '2019'
N19-1366:
  abstract: AMR-to-text generation is a problem recently introduced to the NLP community,
    in which the goal is to generate sentences from Abstract Meaning Representation
    (AMR) graphs. Sequence-to-sequence models can be used to this end by converting
    the AMR graphs to strings. Approaching the problem while working directly with
    graphs requires the use of graph-to-sequence models that encode the AMR graph
    into a vector representation. Such encoding has been shown to be beneficial in
    the past, and unlike sequential encoding, it allows us to explicitly capture reentrant
    structures in the AMR graphs. We investigate the extent to which reentrancies
    (nodes with multiple parents) have an impact on AMR-to-text generation by comparing
    graph encoders to tree encoders, where reentrancies are not preserved. We show
    that improvements in the treatment of reentrancies and long-range dependencies
    contribute to higher overall scores for graph encoders. Our best model achieves
    24.40 BLEU on LDC2015E86, outperforming the state of the art by 1.1 points and
    24.54 BLEU on LDC2017T10, outperforming the state of the art by 1.24 points.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1366.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1366.Presentation.pdf
  - filename: https://vimeo.com/356184145
    type: video
    url: https://vimeo.com/356184145
  author:
  - first: Marco
    full: Marco Damonte
    id: marco-damonte
    last: Damonte
  - first: Shay B.
    full: Shay B. Cohen
    id: shay-b-cohen
    last: Cohen
  author_string: Marco Damonte, Shay B. Cohen
  bibkey: damonte-cohen-2019-structural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1366
  month: June
  page_first: '3649'
  page_last: '3658'
  pages: "3649\u20133658"
  paper_id: '366'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1366.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1366.jpg
  title: Structural Neural Encoders for AMR-to-text Generation
  title_html: Structural Neural Encoders for <span class="acl-fixed-case">AMR</span>-to-text
    Generation
  url: https://www.aclweb.org/anthology/N19-1366
  year: '2019'
N19-1367:
  abstract: There is growing evidence that changes in speech and language may be early
    markers of dementia, but much of the previous NLP work in this area has been limited
    by the size of the available datasets. Here, we compare several methods of domain
    adaptation to augment a small French dataset of picture descriptions (n = 57)
    with a much larger English dataset (n = 550), for the task of automatically distinguishing
    participants with dementia from controls. The first challenge is to identify a
    set of features that transfer across languages; in addition to previously used
    features based on information units, we introduce a new set of features to model
    the order in which information units are produced by dementia patients and controls.
    These concept-based language model features improve classification performance
    in both English and French separately, and the best result (AUC = 0.89) is achieved
    using the multilingual training set with a combination of information and language
    model features.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/359709530
    type: video
    url: https://vimeo.com/359709530
  author:
  - first: Kathleen C.
    full: Kathleen C. Fraser
    id: kathleen-c-fraser
    last: Fraser
  - first: Nicklas
    full: Nicklas Linz
    id: nicklas-linz
    last: Linz
  - first: Bai
    full: Bai Li
    id: bai-li
    last: Li
  - first: Kristina
    full: Kristina Lundholm Fors
    id: kristina-lundholm-fors
    last: Lundholm Fors
  - first: Frank
    full: Frank Rudzicz
    id: frank-rudzicz
    last: Rudzicz
  - first: Alexandra
    full: "Alexandra K\xF6nig"
    id: alexandra-konig
    last: "K\xF6nig"
  - first: Jan
    full: Jan Alexandersson
    id: jan-alexandersson
    last: Alexandersson
  - first: Philippe
    full: Philippe Robert
    id: philippe-robert
    last: Robert
  - first: Dimitrios
    full: Dimitrios Kokkinakis
    id: dimitrios-kokkinakis
    last: Kokkinakis
  author_string: "Kathleen C. Fraser, Nicklas Linz, Bai Li, Kristina Lundholm Fors,\
    \ Frank Rudzicz, Alexandra K\xF6nig, Jan Alexandersson, Philippe Robert, Dimitrios\
    \ Kokkinakis"
  bibkey: fraser-etal-2019-multilingual
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1367
  month: June
  page_first: '3659'
  page_last: '3670'
  pages: "3659\u20133670"
  paper_id: '367'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1367.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1367.jpg
  title: "Multilingual prediction of Alzheimer\u2019s disease through domain adaptation\
    \ and concept-based language modelling"
  title_html: "Multilingual prediction of <span class=\"acl-fixed-case\">A</span>lzheimer\u2019\
    s disease through domain adaptation and concept-based language modelling"
  url: https://www.aclweb.org/anthology/N19-1367
  year: '2019'
N19-1368:
  abstract: "To make machines better understand sentiments, research needs to move\
    \ from polarity identification to understanding the reasons that underlie the\
    \ expression of sentiment. Categorizing the goals or needs of humans is one way\
    \ to explain the expression of sentiment in text. Humans are good at understanding\
    \ situations described in natural language and can easily connect them to the\
    \ character\u2019s psychological needs using commonsense knowledge. We present\
    \ a novel method to extract, rank, filter and select multi-hop relation paths\
    \ from a commonsense knowledge resource to interpret the expression of sentiment\
    \ in terms of their underlying human needs. We efficiently integrate the acquired\
    \ knowledge paths in a neural model that interfaces context representations with\
    \ knowledge using a gated attention mechanism. We assess the model\u2019s performance\
    \ on a recently published dataset for categorizing human needs. Selectively integrating\
    \ knowledge paths boosts performance and establishes a new state-of-the-art. Our\
    \ model offers interpretability through the learned attention map over commonsense\
    \ knowledge paths. Human evaluation highlights the relevance of the encoded knowledge."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1368.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1368.Supplementary.pdf
  - filename: N19-1368.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1368.Presentation.pdf
  - filename: https://vimeo.com/361758446
    type: video
    url: https://vimeo.com/361758446
  author:
  - first: Debjit
    full: Debjit Paul
    id: debjit-paul
    last: Paul
  - first: Anette
    full: Anette Frank
    id: anette-frank
    last: Frank
  author_string: Debjit Paul, Anette Frank
  bibkey: paul-frank-2019-ranking
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1368
  month: June
  page_first: '3671'
  page_last: '3681'
  pages: "3671\u20133681"
  paper_id: '368'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1368.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1368.jpg
  title: Ranking and Selecting Multi-Hop Knowledge Paths to Better Predict Human Needs
  title_html: Ranking and Selecting Multi-Hop Knowledge Paths to Better Predict Human
    Needs
  url: https://www.aclweb.org/anthology/N19-1368
  year: '2019'
N19-1369:
  abstract: Incorporating domain knowledge is vital in building successful natural
    language processing (NLP) applications. Many times, cross-domain application of
    a tool results in poor performance as the tool does not account for domain-specific
    attributes. The clinical domain is challenging in this aspect due to specialized
    medical terms and nomenclature, shorthand notation, fragmented text, and a variety
    of writing styles used by different medical units. Temporal resolution is an NLP
    task that, in general, is domain-agnostic because temporal information is represented
    using a limited lexicon. However, domain-specific aspects of temporal resolution
    are present in clinical texts. Here we explore parsing issues that arose when
    running our system, a tool built on Newswire text, on clinical notes in the THYME
    corpus. Many parsing issues were straightforward to correct; however, a few code
    changes resulted in a cascading series of parsing errors that had to be resolved
    before an improvement in performance was observed, revealing the complexity temporal
    resolution and rule-based parsing. Our system now outperforms current state-of-the-art
    systems on the THYME corpus with little change in its performance on Newswire
    texts.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/359707022
    type: video
    url: https://vimeo.com/359707022
  author:
  - first: Amy
    full: Amy Olex
    id: amy-olex
    last: Olex
  - first: Luke
    full: Luke Maffey
    id: luke-maffey
    last: Maffey
  - first: Bridget
    full: Bridget McInnes
    id: bridget-mcinnes
    last: McInnes
  author_string: Amy Olex, Luke Maffey, Bridget McInnes
  bibkey: olex-etal-2019-nlp
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1369
  month: June
  page_first: '3682'
  page_last: '3692'
  pages: "3682\u20133692"
  paper_id: '369'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1369.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1369.jpg
  title: 'NLP Whack-A-Mole: Challenges in Cross-Domain Temporal Expression Extraction'
  title_html: '<span class="acl-fixed-case">NLP</span> Whack-A-Mole: <span class="acl-fixed-case">C</span>hallenges
    in Cross-Domain Temporal Expression Extraction'
  url: https://www.aclweb.org/anthology/N19-1369
  year: '2019'
N19-1370:
  abstract: "Most information extraction methods focus on binary relations expressed\
    \ within single sentences. In high-value domains, however, n-ary relations are\
    \ of great demand (e.g., drug-gene-mutation interactions in precision oncology).\
    \ Such relations often involve entity mentions that are far apart in the document,\
    \ yet existing work on cross-sentence relation extraction is generally confined\
    \ to small text spans (e.g., three consecutive sentences), which severely limits\
    \ recall. In this paper, we propose a novel multiscale neural architecture for\
    \ document-level n-ary relation extraction. Our system combines representations\
    \ learned over various text spans throughout the document and across the subrelation\
    \ hierarchy. Widening the system\u2019s purview to the entire document maximizes\
    \ potential recall. Moreover, by integrating weak signals across the document,\
    \ multiscale modeling increases precision, even in the presence of noisy labels\
    \ from distant supervision. Experiments on biomedical machine reading show that\
    \ our approach substantially outperforms previous n-ary relation extraction methods."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/359705969
    type: video
    url: https://vimeo.com/359705969
  author:
  - first: Robin
    full: Robin Jia
    id: robin-jia
    last: Jia
  - first: Cliff
    full: Cliff Wong
    id: cliff-wong
    last: Wong
  - first: Hoifung
    full: Hoifung Poon
    id: hoifung-poon
    last: Poon
  author_string: Robin Jia, Cliff Wong, Hoifung Poon
  bibkey: jia-etal-2019-document
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1370
  month: June
  page_first: '3693'
  page_last: '3704'
  pages: "3693\u20133704"
  paper_id: '370'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1370.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1370.jpg
  title: Document-Level N-ary Relation Extraction with Multiscale Representation Learning
  title_html: Document-Level N-ary Relation Extraction with Multiscale Representation
    Learning
  url: https://www.aclweb.org/anthology/N19-1370
  year: '2019'
N19-1371:
  abstract: "How do we know if a particular medical treatment actually works? Ideally\
    \ one would consult all available evidence from relevant clinical trials. Unfortunately,\
    \ such results are primarily disseminated in natural language scientific articles,\
    \ imposing substantial burden on those trying to make sense of them. In this paper,\
    \ we present a new task and corpus for making this unstructured published scientific\
    \ evidence actionable. The task entails inferring reported findings from a full-text\
    \ article describing randomized controlled trials (RCT) with respect to a given\
    \ intervention, comparator, and outcome of interest, e.g., inferring if a given\
    \ article provides evidence supporting the use of aspirin to reduce risk of stroke,\
    \ as compared to placebo. We present a new corpus for this task comprising 10,000+\
    \ prompts coupled with full-text articles describing RCTs. Results using a suite\
    \ of baseline models \u2014 ranging from heuristic (rule-based) approaches to\
    \ attentive neural architectures \u2014 demonstrate the difficulty of the task,\
    \ which we believe largely owes to the lengthy, technical input texts. To facilitate\
    \ further work on this important, challenging problem we make the corpus, documentation,\
    \ a website and leaderboard, and all source code for baselines and evaluation\
    \ publicly available."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1371.Software.tar
    type: software
    url: https://www.aclweb.org/anthology/attachments/N19-1371.Software.tar
  - filename: https://vimeo.com/359705097
    type: video
    url: https://vimeo.com/359705097
  author:
  - first: Eric
    full: Eric Lehman
    id: eric-lehman
    last: Lehman
  - first: Jay
    full: Jay DeYoung
    id: jay-deyoung
    last: DeYoung
  - first: Regina
    full: Regina Barzilay
    id: regina-barzilay
    last: Barzilay
  - first: Byron C.
    full: Byron C. Wallace
    id: byron-c-wallace
    last: Wallace
  author_string: Eric Lehman, Jay DeYoung, Regina Barzilay, Byron C. Wallace
  bibkey: lehman-etal-2019-inferring
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1371
  month: June
  page_first: '3705'
  page_last: '3717'
  pages: "3705\u20133717"
  paper_id: '371'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1371.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1371.jpg
  title: Inferring Which Medical Treatments Work from Reports of Clinical Trials
  title_html: Inferring Which Medical Treatments Work from Reports of Clinical Trials
  url: https://www.aclweb.org/anthology/N19-1371
  year: '2019'
N19-1372:
  abstract: To capture salient contextual information for spoken language understanding
    (SLU) of a dialogue, we propose time-aware models that automatically learn the
    latent time-decay function of the history without a manual time-decay function.
    We also propose a method to identify and label the current speaker to improve
    the SLU accuracy. In experiments on the benchmark dataset used in Dialog State
    Tracking Challenge 4, the proposed models achieved significantly higher F1 scores
    than the state-of-the-art contextual models. Finally, we analyze the effectiveness
    of the introduced models in detail. The analysis demonstrates that the proposed
    methods were effective to improve SLU accuracy individually.
  address: Minneapolis, Minnesota
  author:
  - first: Jonggu
    full: Jonggu Kim
    id: jonggu-kim
    last: Kim
  - first: Jong-Hyeok
    full: Jong-Hyeok Lee
    id: jong-hyeok-lee
    last: Lee
  author_string: Jonggu Kim, Jong-Hyeok Lee
  bibkey: kim-lee-2019-decay
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1372
  month: June
  page_first: '3718'
  page_last: '3726'
  pages: "3718\u20133726"
  paper_id: '372'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1372.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1372.jpg
  title: Decay-Function-Free Time-Aware Attention to Context and Speaker Indicator
    for Spoken Language Understanding
  title_html: Decay-Function-Free Time-Aware Attention to Context and Speaker Indicator
    for Spoken Language Understanding
  url: https://www.aclweb.org/anthology/N19-1372
  year: '2019'
N19-1373:
  abstract: Recent work in Dialogue Act classification has treated the task as a sequence
    labeling problem using hierarchical deep neural networks. We build on this prior
    work by leveraging the effectiveness of a context-aware self-attention mechanism
    coupled with a hierarchical recurrent neural network. We conduct extensive evaluations
    on standard Dialogue Act classification datasets and show significant improvement
    over state-of-the-art results on the Switchboard Dialogue Act (SwDA) Corpus. We
    also investigate the impact of different utterance-level representation learning
    methods and show that our method is effective at capturing utterance-level semantic
    text representations while maintaining high accuracy.
  address: Minneapolis, Minnesota
  author:
  - first: Vipul
    full: Vipul Raheja
    id: vipul-raheja
    last: Raheja
  - first: Joel
    full: Joel Tetreault
    id: joel-tetreault
    last: Tetreault
  author_string: Vipul Raheja, Joel Tetreault
  bibkey: raheja-tetreault-2019-dialogue
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1373
  month: June
  page_first: '3727'
  page_last: '3733'
  pages: "3727\u20133733"
  paper_id: '373'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1373.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1373.jpg
  title: Dialogue Act Classification with Context-Aware Self-Attention
  title_html: <span class="acl-fixed-case">D</span>ialogue <span class="acl-fixed-case">A</span>ct
    <span class="acl-fixed-case">C</span>lassification with <span class="acl-fixed-case">C</span>ontext-<span
    class="acl-fixed-case">A</span>ware <span class="acl-fixed-case">S</span>elf-<span
    class="acl-fixed-case">A</span>ttention
  url: https://www.aclweb.org/anthology/N19-1373
  year: '2019'
N19-1374:
  abstract: 'The majority of current systems for end-to-end dialog generation focus
    on response quality without an explicit control over the affective content of
    the responses. In this paper, we present an affect-driven dialog system, which
    generates emotional responses in a controlled manner using a continuous representation
    of emotions. The system achieves this by modeling emotions at a word and sequence
    level using: (1) a vector representation of the desired emotion, (2) an affect
    regularizer, which penalizes neutral words, and (3) an affect sampling method,
    which forces the neural network to generate diverse words that are emotionally
    relevant. During inference, we use a re-ranking procedure that aims to extract
    the most emotionally relevant responses using a human-in-the-loop optimization
    process. We study the performance of our system in terms of both quantitative
    (BLEU score and response diversity), and qualitative (emotional appropriateness)
    measures.'
  address: Minneapolis, Minnesota
  author:
  - first: Pierre
    full: Pierre Colombo
    id: pierre-colombo
    last: Colombo
  - first: Wojciech
    full: Wojciech Witon
    id: wojciech-witon
    last: Witon
  - first: Ashutosh
    full: Ashutosh Modi
    id: ashutosh-modi
    last: Modi
  - first: James
    full: James Kennedy
    id: james-kennedy
    last: Kennedy
  - first: Mubbasir
    full: Mubbasir Kapadia
    id: mubbasir-kapadia
    last: Kapadia
  author_string: Pierre Colombo, Wojciech Witon, Ashutosh Modi, James Kennedy, Mubbasir
    Kapadia
  bibkey: colombo-etal-2019-affect
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1374
  month: June
  page_first: '3734'
  page_last: '3743'
  pages: "3734\u20133743"
  paper_id: '374'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1374.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1374.jpg
  title: Affect-Driven Dialog Generation
  title_html: Affect-Driven Dialog Generation
  url: https://www.aclweb.org/anthology/N19-1374
  year: '2019'
N19-1375:
  abstract: Recent end-to-end task oriented dialog systems use memory architectures
    to incorporate external knowledge in their dialogs. Current work makes simplifying
    assumptions about the structure of the knowledge base, such as the use of triples
    to represent knowledge, and combines dialog utterances (context) as well as knowledge
    base (KB) results as part of the same memory. This causes an explosion in the
    memory size, and makes the reasoning over memory harder. In addition, such a memory
    design forces hierarchical properties of the data to be fit into a triple structure
    of memory. This requires the memory reader to infer relationships across otherwise
    connected attributes. In this paper we relax the strong assumptions made by existing
    architectures and separate memories used for modeling dialog context and KB results.
    Instead of using triples to store KB results, we introduce a novel multi-level
    memory architecture consisting of cells for each query and their corresponding
    results. The multi-level memory first addresses queries, followed by results and
    finally each key-value pair within a result. We conduct detailed experiments on
    three publicly available task oriented dialog data sets and we find that our method
    conclusively outperforms current state-of-the-art models. We report a 15-25% increase
    in both entity F1 and BLEU scores.
  address: Minneapolis, Minnesota
  author:
  - first: Revanth
    full: Revanth Gangi Reddy
    id: revanth-gangi-reddy
    last: Gangi Reddy
  - first: Danish
    full: Danish Contractor
    id: danish-contractor
    last: Contractor
  - first: Dinesh
    full: Dinesh Raghu
    id: dinesh-raghu
    last: Raghu
  - first: Sachindra
    full: Sachindra Joshi
    id: sachindra-joshi
    last: Joshi
  author_string: Revanth Gangi Reddy, Danish Contractor, Dinesh Raghu, Sachindra Joshi
  bibkey: gangi-reddy-etal-2019-multi
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1375
  month: June
  page_first: '3744'
  page_last: '3754'
  pages: "3744\u20133754"
  paper_id: '375'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1375.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1375.jpg
  title: Multi-Level Memory for Task Oriented Dialogs
  title_html: Multi-Level Memory for Task Oriented Dialogs
  url: https://www.aclweb.org/anthology/N19-1375
  year: '2019'
N19-1376:
  abstract: Success of deep learning techniques have renewed the interest in development
    of dialogue systems. However, current systems struggle to have consistent long
    term conversations with the users and fail to build rapport. Topic spotting, the
    task of automatically inferring the topic of a conversation, has been shown to
    be helpful in making dialog system more engaging and efficient. We propose a hierarchical
    model with self attention for topic spotting. Experiments on the Switchboard corpus
    show the superior performance of our model over previously proposed techniques
    for topic spotting and deep models for text classification. Additionally, in contrast
    to offline processing of dialog, we also analyze the performance of our model
    in a more realistic setting i.e. in an online setting where the topic is identified
    in real time as the dialog progresses. Results show that our model is able to
    generalize even with limited information in the online setting.
  address: Minneapolis, Minnesota
  author:
  - first: Pooja
    full: Pooja Chitkara
    id: pooja-chitkara
    last: Chitkara
  - first: Ashutosh
    full: Ashutosh Modi
    id: ashutosh-modi
    last: Modi
  - first: Pravalika
    full: Pravalika Avvaru
    id: pravalika-avvaru
    last: Avvaru
  - first: Sepehr
    full: Sepehr Janghorbani
    id: sepehr-janghorbani
    last: Janghorbani
  - first: Mubbasir
    full: Mubbasir Kapadia
    id: mubbasir-kapadia
    last: Kapadia
  author_string: Pooja Chitkara, Ashutosh Modi, Pravalika Avvaru, Sepehr Janghorbani,
    Mubbasir Kapadia
  bibkey: chitkara-etal-2019-topic
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1376
  month: June
  page_first: '3755'
  page_last: '3761'
  pages: "3755\u20133761"
  paper_id: '376'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1376.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1376.jpg
  title: Topic Spotting using Hierarchical Networks with Self Attention
  title_html: Topic Spotting using Hierarchical Networks with Self Attention
  url: https://www.aclweb.org/anthology/N19-1376
  year: '2019'
N19-1377:
  abstract: 'We consider neural language generation under a novel problem setting:
    generating the words of a sentence according to the order of their first appearance
    in its lexicalized PCFG parse tree, in a depth-first, left-to-right manner. Unlike
    previous tree-based language generation methods, our approach is both (i) top-down
    and (ii) explicitly generating syntactic structure at the same time. In addition,
    our method combines neural model with symbolic approach: word choice at each step
    is constrained by its predicted syntactic function. We applied our model to the
    task of dialog response generation, and found it significantly improves over sequence-to-sequence
    baseline, in terms of diversity and relevance. We also investigated the effect
    of lexicalization on language generation, and found that lexicalization schemes
    that give priority to content words have certain advantages over those focusing
    on dependency relations.'
  address: Minneapolis, Minnesota
  author:
  - first: Wenchao
    full: Wenchao Du
    id: wenchao-du
    last: Du
  - first: Alan W
    full: Alan W Black
    id: alan-w-black
    last: Black
  author_string: Wenchao Du, Alan W Black
  bibkey: du-black-2019-top
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1377
  month: June
  page_first: '3762'
  page_last: '3771'
  pages: "3762\u20133771"
  paper_id: '377'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1377.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1377.jpg
  title: Top-Down Structurally-Constrained Neural Response Generation with Lexicalized
    Probabilistic Context-Free Grammar
  title_html: Top-Down Structurally-Constrained Neural Response Generation with Lexicalized
    Probabilistic Context-Free Grammar
  url: https://www.aclweb.org/anthology/N19-1377
  year: '2019'
N19-1378:
  abstract: Humans use language to refer to entities in the external world. Motivated
    by this, in recent years several models that incorporate a bias towards learning
    entity representations have been proposed. Such entity-centric models have shown
    empirical success, but we still know little about why. In this paper we analyze
    the behavior of two recently proposed entity-centric models in a referential task,
    Entity Linking in Multi-party Dialogue (SemEval 2018 Task 4). We show that these
    models outperform the state of the art on this task, and that they do better on
    lower frequency entities than a counterpart model that is not entity-centric,
    with the same model size. We argue that making models entity-centric naturally
    fosters good architectural decisions. However, we also show that these models
    do not really build entity representations and that they make poor use of linguistic
    context. These negative results underscore the need for model analysis, to test
    whether the motivations for particular architectures are borne out in how models
    behave when deployed.
  address: Minneapolis, Minnesota
  author:
  - first: Laura
    full: Laura Aina
    id: laura-aina
    last: Aina
  - first: Carina
    full: Carina Silberer
    id: carina-silberer
    last: Silberer
  - first: Ionut-Teodor
    full: Ionut-Teodor Sorodoc
    id: ionut-sorodoc
    last: Sorodoc
  - first: Matthijs
    full: Matthijs Westera
    id: matthijs-westera
    last: Westera
  - first: Gemma
    full: Gemma Boleda
    id: gemma-boleda
    last: Boleda
  author_string: Laura Aina, Carina Silberer, Ionut-Teodor Sorodoc, Matthijs Westera,
    Gemma Boleda
  bibkey: aina-etal-2019-entity
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1378
  month: June
  page_first: '3772'
  page_last: '3783'
  pages: "3772\u20133783"
  paper_id: '378'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1378.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1378.jpg
  title: What do Entity-Centric Models Learn? Insights from Entity Linking in Multi-Party
    Dialogue
  title_html: What do Entity-Centric Models Learn? Insights from Entity Linking in
    Multi-Party Dialogue
  url: https://www.aclweb.org/anthology/N19-1378
  year: '2019'
N19-1379:
  abstract: Domain classification is the task to map spoken language utterances to
    one of the natural language understanding domains in intelligent personal digital
    assistants (IPDAs). This is observed in mainstream IPDAs in industry and third-party
    domains are developed to enhance the capability of the IPDAs. As more and more
    new domains are developed very frequently, how to continuously accommodate the
    new domains still remains challenging. Moreover, if one wants to use personalized
    information dynamically for better domain classification, it is infeasible to
    directly adopt existing continual learning approaches. In this paper, we propose
    CoNDA, a neural-based approach for continuous domain adaption with normalization
    and regularization. Unlike existing methods that often conduct full model parameter
    update, CoNDA only updates the necessary parameters in the model for the new domains.
    Empirical evaluation shows that CoNDA achieves high accuracy on both the accommodated
    new domains and the existing known domains for which input samples come with personal
    information, and outperforms the baselines by a large margin.
  address: Minneapolis, Minnesota
  author:
  - first: Han
    full: Han Li
    id: han-li
    last: Li
  - first: Jihwan
    full: Jihwan Lee
    id: jihwan-lee
    last: Lee
  - first: Sidharth
    full: Sidharth Mudgal
    id: sidharth-mudgal
    last: Mudgal
  - first: Ruhi
    full: Ruhi Sarikaya
    id: ruhi-sarikaya
    last: Sarikaya
  - first: Young-Bum
    full: Young-Bum Kim
    id: young-bum-kim
    last: Kim
  author_string: Han Li, Jihwan Lee, Sidharth Mudgal, Ruhi Sarikaya, Young-Bum Kim
  bibkey: li-etal-2019-continuous
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1379
  month: June
  page_first: '3784'
  page_last: '3794'
  pages: "3784\u20133794"
  paper_id: '379'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1379.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1379.jpg
  title: Continuous Learning for Large-scale Personalized Domain Classification
  title_html: Continuous Learning for Large-scale Personalized Domain Classification
  url: https://www.aclweb.org/anthology/N19-1379
  year: '2019'
N19-1380:
  abstract: 'One of the first steps in the utterance interpretation pipeline of many
    task-oriented conversational AI systems is to identify user intents and the corresponding
    slots. Since data collection for machine learning models for this task is time-consuming,
    it is desirable to make use of existing data in a high-resource language to train
    models in low-resource languages. However, development of such models has largely
    been hindered by the lack of multilingual training data. In this paper, we present
    a new data set of 57k annotated utterances in English (43k), Spanish (8.6k) and
    Thai (5k) across the domains weather, alarm, and reminder. We use this data set
    to evaluate three different cross-lingual transfer methods: (1) translating the
    training data, (2) using cross-lingual pre-trained embeddings, and (3) a novel
    method of using a multilingual machine translation encoder as contextual word
    representations. We find that given several hundred training examples in the the
    target language, the latter two methods outperform translating the training data.
    Further, in very low-resource settings, multilingual contextual word representations
    give better results than using cross-lingual static embeddings. We also compare
    the cross-lingual methods to using monolingual resources in the form of contextual
    ELMo representations and find that given just small amounts of target language
    data, this method outperforms all cross-lingual methods, which highlights the
    need for more sophisticated cross-lingual methods.'
  address: Minneapolis, Minnesota
  author:
  - first: Sebastian
    full: Sebastian Schuster
    id: sebastian-schuster
    last: Schuster
  - first: Sonal
    full: Sonal Gupta
    id: sonal-gupta
    last: Gupta
  - first: Rushin
    full: Rushin Shah
    id: rushin-shah
    last: Shah
  - first: Mike
    full: Mike Lewis
    id: mike-lewis
    last: Lewis
  author_string: Sebastian Schuster, Sonal Gupta, Rushin Shah, Mike Lewis
  bibkey: schuster-etal-2019-cross-lingual
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1380
  month: June
  page_first: '3795'
  page_last: '3805'
  pages: "3795\u20133805"
  paper_id: '380'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1380.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1380.jpg
  title: Cross-lingual Transfer Learning for Multilingual Task Oriented Dialog
  title_html: Cross-lingual Transfer Learning for Multilingual Task Oriented Dialog
  url: https://www.aclweb.org/anthology/N19-1380
  year: '2019'
N19-1381:
  abstract: Evaluating open-domain dialogue systems is difficult due to the diversity
    of possible correct answers. Automatic metrics such as BLEU correlate weakly with
    human annotations, resulting in a significant bias across different models and
    datasets. Some researchers resort to human judgment experimentation for assessing
    response quality, which is expensive, time consuming, and not scalable. Moreover,
    judges tend to evaluate a small number of dialogues, meaning that minor differences
    in evaluation configuration may lead to dissimilar results. In this paper, we
    present interpretable metrics for evaluating topic coherence by making use of
    distributed sentence representations. Furthermore, we introduce calculable approximations
    of human judgment based on conversational coherence by adopting state-of-the-art
    entailment techniques. Results show that our metrics can be used as a surrogate
    for human judgment, making it easy to evaluate dialogue systems on large-scale
    datasets and allowing an unbiased estimate for the quality of the responses.
  address: Minneapolis, Minnesota
  author:
  - first: Nouha
    full: Nouha Dziri
    id: nouha-dziri
    last: Dziri
  - first: Ehsan
    full: Ehsan Kamalloo
    id: ehsan-kamalloo
    last: Kamalloo
  - first: Kory
    full: Kory Mathewson
    id: kory-mathewson
    last: Mathewson
  - first: Osmar
    full: Osmar Zaiane
    id: osmar-r-zaiane
    last: Zaiane
  author_string: Nouha Dziri, Ehsan Kamalloo, Kory Mathewson, Osmar Zaiane
  bibkey: dziri-etal-2019-evaluating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1381
  month: June
  page_first: '3806'
  page_last: '3812'
  pages: "3806\u20133812"
  paper_id: '381'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1381.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1381.jpg
  title: Evaluating Coherence in Dialogue Systems using Entailment
  title_html: Evaluating Coherence in Dialogue Systems using Entailment
  url: https://www.aclweb.org/anthology/N19-1381
  year: '2019'
N19-1382:
  abstract: Recent advances in Question Answering have lead to the development of
    very complex models which compute rich representations for query and documents
    by capturing all pairwise interactions between query and document words. This
    makes these models expensive in space and time, and in practice one has to restrict
    the length of the documents that can be fed to these models. Such models have
    also been recently employed for the task of predicting dialog responses from available
    background documents (e.g., Holl-E dataset). However, here the documents are longer,
    thereby rendering these complex models infeasible except in select restricted
    settings. In order to overcome this, we use standard simple models which do not
    capture all pairwise interactions, but learn to emulate certain characteristics
    of a complex teacher network. Specifically, we first investigate the conicity
    of representations learned by a complex model and observe that it is significantly
    lower than that of simpler models. Based on this insight, we modify the simple
    architecture to mimic this characteristic. We go further by using knowledge distillation
    approaches, where the simple model acts as a student and learns to match the output
    from the complex teacher network. We experiment with the Holl-E dialog data set
    and show that by mimicking characteristics and matching outputs from a teacher,
    even a simple network can give improved performance.
  address: Minneapolis, Minnesota
  author:
  - first: Siddhartha
    full: Siddhartha Arora
    id: siddhartha-arora
    last: Arora
  - first: Mitesh M.
    full: Mitesh M. Khapra
    id: mitesh-m-khapra
    last: Khapra
  - first: Harish G.
    full: Harish G. Ramaswamy
    id: harish-g-ramaswamy
    last: Ramaswamy
  author_string: Siddhartha Arora, Mitesh M. Khapra, Harish G. Ramaswamy
  bibkey: arora-etal-2019-knowledge
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1382
  month: June
  page_first: '3813'
  page_last: '3822'
  pages: "3813\u20133822"
  paper_id: '382'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1382.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1382.jpg
  title: On Knowledge distillation from complex networks for response prediction
  title_html: On Knowledge distillation from complex networks for response prediction
  url: https://www.aclweb.org/anthology/N19-1382
  year: '2019'
N19-1383:
  abstract: 'We focus on improving name tagging for low-resource languages using annotations
    from related languages. Previous studies either directly project annotations from
    a source language to a target language using cross-lingual representations or
    use a shared encoder in a multitask network to transfer knowledge. These approaches
    inevitably introduce noise to the target language annotation due to mismatched
    source-target sentence structures. To effectively transfer the resources, we develop
    a new neural architecture that leverages multi-level adversarial transfer: (1)
    word-level adversarial training, which projects source language words into the
    same semantic space as those of the target language without using any parallel
    corpora or bilingual gazetteers, and (2) sentence-level adversarial training,
    which yields language-agnostic sequential features. Our neural architecture outperforms
    previous approaches on CoNLL data sets. Moreover, on 10 low-resource languages,
    our approach achieves up to 16% absolute F-score gain over all high-performing
    baselines on cross-lingual transfer without using any target-language resources.'
  address: Minneapolis, Minnesota
  author:
  - first: Lifu
    full: Lifu Huang
    id: lifu-huang
    last: Huang
  - first: Heng
    full: Heng Ji
    id: heng-ji
    last: Ji
  - first: Jonathan
    full: Jonathan May
    id: jonathan-may
    last: May
  author_string: Lifu Huang, Heng Ji, Jonathan May
  bibkey: huang-etal-2019-cross
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1383
  month: June
  page_first: '3823'
  page_last: '3833'
  pages: "3823\u20133833"
  paper_id: '383'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1383.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1383.jpg
  title: Cross-lingual Multi-Level Adversarial Transfer to Enhance Low-Resource Name
    Tagging
  title_html: Cross-lingual Multi-Level Adversarial Transfer to Enhance Low-Resource
    Name Tagging
  url: https://www.aclweb.org/anthology/N19-1383
  year: '2019'
N19-1384:
  abstract: 'In neural machine translation (NMT), monolingual data are usually exploited
    through a so-called back-translation: sentences in the target language are translated
    into the source language to synthesize new parallel data. While this method provides
    more training data to better model the target language, on the source side, it
    only exploits translations that the NMT system is already able to generate using
    a model trained on existing parallel data. In this work, we assume that new translation
    knowledge can be extracted from monolingual data, without relying at all on existing
    parallel data. We propose a new algorithm for extracting from monolingual data
    what we call partial translations: pairs of source and target sentences that contain
    sequences of tokens that are translations of each other. Our algorithm is fully
    unsupervised and takes only source and target monolingual data as input. Our empirical
    evaluation points out that our partial translations can be used in combination
    with back-translation to further improve NMT models. Furthermore, while partial
    translations are particularly useful for low-resource language pairs, they can
    also be successfully exploited in resource-rich scenarios to improve translation
    quality.'
  address: Minneapolis, Minnesota
  author:
  - first: Benjamin
    full: Benjamin Marie
    id: benjamin-marie
    last: Marie
  - first: Atsushi
    full: Atsushi Fujita
    id: atsushi-fujita
    last: Fujita
  author_string: Benjamin Marie, Atsushi Fujita
  bibkey: marie-fujita-2019-unsupervised-extraction
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1384
  month: June
  page_first: '3834'
  page_last: '3844'
  pages: "3834\u20133844"
  paper_id: '384'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1384.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1384.jpg
  title: Unsupervised Extraction of Partial Translations for Neural Machine Translation
  title_html: Unsupervised Extraction of Partial Translations for Neural Machine Translation
  url: https://www.aclweb.org/anthology/N19-1384
  year: '2019'
N19-1385:
  abstract: We describe a cross-lingual transfer method for dependency parsing that
    takes into account the problem of word order differences between source and target
    languages. Our model only relies on the Bible, a considerably smaller parallel
    data than the commonly used parallel data in transfer methods. We use the concatenation
    of projected trees from the Bible corpus, and the gold-standard treebanks in multiple
    source languages along with cross-lingual word representations. We demonstrate
    that reordering the source treebanks before training on them for a target language
    improves the accuracy of languages outside the European language family. Our experiments
    on 68 treebanks (38 languages) in the Universal Dependencies corpus achieve a
    high accuracy for all languages. Among them, our experiments on 16 treebanks of
    12 non-European languages achieve an average UAS absolute improvement of 3.3%
    over a state-of-the-art method.
  address: Minneapolis, Minnesota
  author:
  - first: Mohammad Sadegh
    full: Mohammad Sadegh Rasooli
    id: mohammad-sadegh-rasooli
    last: Rasooli
  - first: Michael
    full: Michael Collins
    id: michael-collins
    last: Collins
  author_string: Mohammad Sadegh Rasooli, Michael Collins
  bibkey: rasooli-collins-2019-low
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1385
  month: June
  page_first: '3845'
  page_last: '3856'
  pages: "3845\u20133856"
  paper_id: '385'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1385.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1385.jpg
  title: Low-Resource Syntactic Transfer with Unsupervised Source Reordering
  title_html: Low-Resource Syntactic Transfer with Unsupervised Source Reordering
  url: https://www.aclweb.org/anthology/N19-1385
  year: '2019'
N19-1386:
  abstract: Adversarial training has shown impressive success in learning bilingual
    dictionary without any parallel data by mapping monolingual embeddings to a shared
    space. However, recent work has shown superior performance for non-adversarial
    methods in more challenging language pairs. In this work, we revisit adversarial
    autoencoder for unsupervised word translation and propose two novel extensions
    to it that yield more stable training and improved results. Our method includes
    regularization terms to enforce cycle consistency and input reconstruction, and
    puts the target encoders as an adversary against the corresponding discriminator.
    Extensive experimentations with European, non-European and low-resource languages
    show that our method is more robust and achieves better performance than recently
    proposed adversarial and non-adversarial approaches.
  address: Minneapolis, Minnesota
  author:
  - first: Tasnim
    full: Tasnim Mohiuddin
    id: muhammad-tasnim-mohiuddin
    last: Mohiuddin
  - first: Shafiq
    full: Shafiq Joty
    id: shafiq-joty
    last: Joty
  author_string: Tasnim Mohiuddin, Shafiq Joty
  bibkey: mohiuddin-joty-2019-revisiting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1386
  month: June
  page_first: '3857'
  page_last: '3867'
  pages: "3857\u20133867"
  paper_id: '386'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1386.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1386.jpg
  title: Revisiting Adversarial Autoencoder for Unsupervised Word Translation with
    Cycle Consistency and Improved Training
  title_html: Revisiting Adversarial Autoencoder for Unsupervised Word Translation
    with Cycle Consistency and Improved Training
  url: https://www.aclweb.org/anthology/N19-1386
  year: '2019'
N19-1387:
  abstract: Transfer learning approaches for Neural Machine Translation (NMT) train
    a NMT model on an assisting language-target language pair (parent model) which
    is later fine-tuned for the source language-target language pair of interest (child
    model), with the target language being the same. In many cases, the assisting
    language has a different word order from the source language. We show that divergent
    word order adversely limits the benefits from transfer learning when little to
    no parallel corpus between the source and target language is available. To bridge
    this divergence, we propose to pre-order the assisting language sentences to match
    the word order of the source language and train the parent model. Our experiments
    on many language pairs show that bridging the word order gap leads to significant
    improvement in the translation quality in extremely low-resource scenarios.
  address: Minneapolis, Minnesota
  author:
  - first: Rudra
    full: Rudra Murthy
    id: rudra-murthy
    last: Murthy
  - first: Anoop
    full: Anoop Kunchukuttan
    id: anoop-kunchukuttan
    last: Kunchukuttan
  - first: Pushpak
    full: Pushpak Bhattacharyya
    id: pushpak-bhattacharyya
    last: Bhattacharyya
  author_string: Rudra Murthy, Anoop Kunchukuttan, Pushpak Bhattacharyya
  bibkey: murthy-etal-2019-addressing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1387
  month: June
  page_first: '3868'
  page_last: '3873'
  pages: "3868\u20133873"
  paper_id: '387'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1387.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1387.jpg
  title: Addressing word-order Divergence in Multilingual Neural Machine Translation
    for extremely Low Resource Languages
  title_html: Addressing word-order Divergence in Multilingual Neural Machine Translation
    for extremely Low Resource Languages
  url: https://www.aclweb.org/anthology/N19-1387
  year: '2019'
N19-1388:
  abstract: Multilingual Neural Machine Translation enables training a single model
    that supports translation from multiple source languages into multiple target
    languages. We perform extensive experiments in training massively multilingual
    NMT models, involving up to 103 distinct languages and 204 translation directions
    simultaneously. We explore different setups for training such models and analyze
    the trade-offs between translation quality and various modeling decisions. We
    report results on the publicly available TED talks multilingual corpus where we
    show that massively multilingual many-to-many models are effective in low resource
    settings, outperforming the previous state-of-the-art while supporting up to 59
    languages in 116 translation directions in a single model. Our experiments on
    a large-scale dataset with 103 languages, 204 trained directions and up to one
    million examples per direction also show promising results, surpassing strong
    bilingual baselines and encouraging future work on massively multilingual NMT.
  address: Minneapolis, Minnesota
  author:
  - first: Roee
    full: Roee Aharoni
    id: roee-aharoni
    last: Aharoni
  - first: Melvin
    full: Melvin Johnson
    id: melvin-johnson
    last: Johnson
  - first: Orhan
    full: Orhan Firat
    id: orhan-firat
    last: Firat
  author_string: Roee Aharoni, Melvin Johnson, Orhan Firat
  bibkey: aharoni-etal-2019-massively
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1388
  month: June
  page_first: '3874'
  page_last: '3884'
  pages: "3874\u20133884"
  paper_id: '388'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1388.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1388.jpg
  title: Massively Multilingual Neural Machine Translation
  title_html: Massively Multilingual Neural Machine Translation
  url: https://www.aclweb.org/anthology/N19-1388
  year: '2019'
N19-1389:
  abstract: "There is no consensus on the state-of-the-art approach to historical\
    \ text normalization. Many techniques have been proposed, including rule-based\
    \ methods, distance metrics, character-based statistical machine translation,\
    \ and neural encoder\u2013decoder models, but studies have used different datasets,\
    \ different evaluation methods, and have come to different conclusions. This paper\
    \ presents the largest study of historical text normalization done so far. We\
    \ critically survey the existing literature and report experiments on eight languages,\
    \ comparing systems spanning all categories of proposed normalization techniques,\
    \ analysing the effect of training data quantity, and using different evaluation\
    \ methods. The datasets and scripts are made publicly available."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1389.Software.zip
    type: software
    url: https://www.aclweb.org/anthology/attachments/N19-1389.Software.zip
  - filename: N19-1389.Poster.pdf
    type: poster
    url: https://www.aclweb.org/anthology/attachments/N19-1389.Poster.pdf
  author:
  - first: Marcel
    full: Marcel Bollmann
    id: marcel-bollmann
    last: Bollmann
  author_string: Marcel Bollmann
  bibkey: bollmann-2019-large
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1389
  month: June
  page_first: '3885'
  page_last: '3898'
  pages: "3885\u20133898"
  paper_id: '389'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1389.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1389.jpg
  title: A Large-Scale Comparison of Historical Text Normalization Systems
  title_html: A Large-Scale Comparison of Historical Text Normalization Systems
  url: https://www.aclweb.org/anthology/N19-1389
  year: '2019'
N19-1390:
  abstract: "It is well-known that distributional semantic approaches have difficulty\
    \ in distinguishing between synonyms and antonyms (Grefenstette, 1992; Pad\xF3\
    \ and Lapata, 2003). Recent work has shown that supervision available in English\
    \ for this task (e.g., lexical resources) can be transferred to other languages\
    \ via cross-lingual word embeddings. However, this kind of transfer misses monolingual\
    \ distributional information available in a target language, such as contrast\
    \ relations that are indicative of antonymy (e.g. hot ... while ... cold). In\
    \ this work, we improve the transfer by exploiting monolingual information, expressed\
    \ in the form of co-occurrences with discourse markers that convey contrast. Our\
    \ approach makes use of less than a dozen markers, which can easily be obtained\
    \ for many languages. Compared to a baseline using only cross-lingual embeddings,\
    \ we show absolute improvements of 4\u201310% F1-score in Vietnamese and Hindi."
  address: Minneapolis, Minnesota
  author:
  - first: Michael
    full: Michael Roth
    id: michael-roth
    last: Roth
  - first: Shyam
    full: Shyam Upadhyay
    id: shyam-upadhyay
    last: Upadhyay
  author_string: Michael Roth, Shyam Upadhyay
  bibkey: roth-upadhyay-2019-combining
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1390
  month: June
  page_first: '3899'
  page_last: '3905'
  pages: "3899\u20133905"
  paper_id: '390'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1390.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1390.jpg
  title: "Combining Discourse Markers and Cross-lingual Embeddings for Synonym\u2013\
    Antonym Classification"
  title_html: "Combining Discourse Markers and Cross-lingual Embeddings for Synonym\u2013\
    Antonym Classification"
  url: https://www.aclweb.org/anthology/N19-1390
  year: '2019'
N19-1391:
  abstract: Cross-lingual word vectors are typically obtained by fitting an orthogonal
    matrix that maps the entries of a bilingual dictionary from a source to a target
    vector space. Word vectors, however, are most commonly used for sentence or document-level
    representations that are calculated as the weighted average of word embeddings.
    In this paper, we propose an alternative to word-level mapping that better reflects
    sentence-level cross-lingual similarity. We incorporate context in the transformation
    matrix by directly mapping the averaged embeddings of aligned sentences in a parallel
    corpus. We also implement cross-lingual mapping of deep contextualized word embeddings
    using parallel sentences with word alignments. In our experiments, both approaches
    resulted in cross-lingual sentence embeddings that outperformed context-independent
    word mapping in sentence translation retrieval. Furthermore, the sentence-level
    transformation could be used for word-level mapping without loss in word translation
    quality.
  address: Minneapolis, Minnesota
  author:
  - first: Hanan
    full: Hanan Aldarmaki
    id: hanan-aldarmaki
    last: Aldarmaki
  - first: Mona
    full: Mona Diab
    id: mona-diab
    last: Diab
  author_string: Hanan Aldarmaki, Mona Diab
  bibkey: aldarmaki-diab-2019-context
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1391
  month: June
  page_first: '3906'
  page_last: '3911'
  pages: "3906\u20133911"
  paper_id: '391'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1391.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1391.jpg
  title: Context-Aware Cross-Lingual Mapping
  title_html: Context-Aware Cross-Lingual Mapping
  url: https://www.aclweb.org/anthology/N19-1391
  year: '2019'
N19-1392:
  abstract: We introduce Rosita, a method to produce multilingual contextual word
    representations by training a single language model on text from multiple languages.
    Our method combines the advantages of contextual word representations with those
    of multilingual representation learning. We produce language models from dissimilar
    language pairs (English/Arabic and English/Chinese) and use them in dependency
    parsing, semantic role labeling, and named entity recognition, with comparisons
    to monolingual and non-contextual variants. Our results provide further evidence
    for the benefits of polyglot learning, in which representations are shared across
    multiple languages.
  address: Minneapolis, Minnesota
  author:
  - first: Phoebe
    full: Phoebe Mulcaire
    id: phoebe-mulcaire
    last: Mulcaire
  - first: Jungo
    full: Jungo Kasai
    id: jungo-kasai
    last: Kasai
  - first: Noah A.
    full: Noah A. Smith
    id: noah-a-smith
    last: Smith
  author_string: Phoebe Mulcaire, Jungo Kasai, Noah A. Smith
  bibkey: mulcaire-etal-2019-polyglot
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1392
  month: June
  page_first: '3912'
  page_last: '3918'
  pages: "3912\u20133918"
  paper_id: '392'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1392.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1392.jpg
  title: Polyglot Contextual Representations Improve Crosslingual Transfer
  title_html: Polyglot Contextual Representations Improve Crosslingual Transfer
  url: https://www.aclweb.org/anthology/N19-1392
  year: '2019'
N19-1393:
  abstract: The existence of universal models to describe the syntax of languages
    has been debated for decades. The availability of resources such as the Universal
    Dependencies treebanks and the World Atlas of Language Structures make it possible
    to study the plausibility of universal grammar from the perspective of dependency
    parsing. Our work investigates the use of high-level language descriptions in
    the form of typological features for multilingual dependency parsing. Our experiments
    on multilingual parsing for 40 languages show that typological information can
    indeed guide parsers to share information between similar languages beyond simple
    language identification.
  address: Minneapolis, Minnesota
  author:
  - first: Manon
    full: Manon Scholivet
    id: manon-scholivet
    last: Scholivet
  - first: Franck
    full: Franck Dary
    id: franck-dary
    last: Dary
  - first: Alexis
    full: Alexis Nasr
    id: alexis-nasr
    last: Nasr
  - first: Benoit
    full: Benoit Favre
    id: benoit-favre
    last: Favre
  - first: Carlos
    full: Carlos Ramisch
    id: carlos-ramisch
    last: Ramisch
  author_string: Manon Scholivet, Franck Dary, Alexis Nasr, Benoit Favre, Carlos Ramisch
  bibkey: scholivet-etal-2019-typological
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1393
  month: June
  page_first: '3919'
  page_last: '3930'
  pages: "3919\u20133930"
  paper_id: '393'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1393.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1393.jpg
  title: Typological Features for Multilingual Delexicalised Dependency Parsing
  title_html: Typological Features for Multilingual Delexicalised Dependency Parsing
  url: https://www.aclweb.org/anthology/N19-1393
  year: '2019'
N19-1394:
  abstract: "Source Code Summarization is the task of writing short, natural language\
    \ descriptions of source code. The main use for these descriptions is in software\
    \ documentation e.g. the one-sentence Java method descriptions in JavaDocs. Code\
    \ summarization is rapidly becoming a popular research problem, but progress is\
    \ restrained due to a lack of suitable datasets. In addition, a lack of community\
    \ standards for creating datasets leads to confusing and unreproducible research\
    \ results \u2013 we observe swings in performance of more than 33% due only to\
    \ changes in dataset design. In this paper, we make recommendations for these\
    \ standards from experimental results. We release a dataset based on prior work\
    \ of over 2.1m pairs of Java methods and one sentence method descriptions from\
    \ over 28k Java projects. We describe the dataset and point out key differences\
    \ from natural language data, to guide and support future researchers."
  address: Minneapolis, Minnesota
  author:
  - first: Alexander
    full: Alexander LeClair
    id: alexander-leclair
    last: LeClair
  - first: Collin
    full: Collin McMillan
    id: collin-mcmillan
    last: McMillan
  author_string: Alexander LeClair, Collin McMillan
  bibkey: leclair-mcmillan-2019-recommendations
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1394
  month: June
  page_first: '3931'
  page_last: '3937'
  pages: "3931\u20133937"
  paper_id: '394'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1394.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1394.jpg
  title: Recommendations for Datasets for Source Code Summarization
  title_html: Recommendations for Datasets for Source Code Summarization
  url: https://www.aclweb.org/anthology/N19-1394
  year: '2019'
N19-1395:
  abstract: Recent work in the field of automatic summarization and headline generation
    focuses on maximizing ROUGE scores for various news datasets. We present an alternative,
    extrinsic, evaluation metric for this task, Answering Performance for Evaluation
    of Summaries. APES utilizes recent progress in the field of reading-comprehension
    to quantify the ability of a summary to answer a set of manually created questions
    regarding central entities in the source article. We first analyze the strength
    of this metric by comparing it to known manual evaluation metrics. We then present
    an end-to-end neural abstractive model that maximizes APES, while increasing ROUGE
    scores to competitive results.
  address: Minneapolis, Minnesota
  author:
  - first: Matan
    full: Matan Eyal
    id: matan-eyal
    last: Eyal
  - first: Tal
    full: Tal Baumel
    id: tal-baumel
    last: Baumel
  - first: Michael
    full: Michael Elhadad
    id: michael-elhadad
    last: Elhadad
  author_string: Matan Eyal, Tal Baumel, Michael Elhadad
  bibkey: eyal-etal-2019-question
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1395
  month: June
  page_first: '3938'
  page_last: '3948'
  pages: "3938\u20133948"
  paper_id: '395'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1395.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1395.jpg
  title: Question Answering as an Automatic Evaluation Metric for News Article Summarization
  title_html: Question Answering as an Automatic Evaluation Metric for News Article
    Summarization
  url: https://www.aclweb.org/anthology/N19-1395
  year: '2019'
N19-1396:
  abstract: Neural abstractive summarizers generate summary texts using a language
    model conditioned on the input source text, and have recently achieved high ROUGE
    scores on benchmark summarization datasets. We investigate how they achieve this
    performance with respect to human-written gold-standard abstracts, and whether
    the systems are able to understand deeper syntactic and semantic structures. We
    generate a set of contrastive summaries which are perturbed, deficient versions
    of human-written summaries, and test whether existing neural summarizers score
    them more highly than the human-written summaries. We analyze their performance
    on different datasets and find that these systems fail to understand the source
    text, in a majority of the cases.
  address: Minneapolis, Minnesota
  author:
  - first: Krtin
    full: Krtin Kumar
    id: krtin-kumar
    last: Kumar
  - first: Jackie Chi Kit
    full: Jackie Chi Kit Cheung
    id: jackie-chi-kit-cheung
    last: Cheung
  author_string: Krtin Kumar, Jackie Chi Kit Cheung
  bibkey: kumar-cheung-2019-understanding
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1396
  month: June
  page_first: '3949'
  page_last: '3954'
  pages: "3949\u20133954"
  paper_id: '396'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1396.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1396.jpg
  title: Understanding the Behaviour of Neural Abstractive Summarizers using Contrastive
    Examples
  title_html: <span class="acl-fixed-case">U</span>nderstanding the <span class="acl-fixed-case">B</span>ehaviour
    of <span class="acl-fixed-case">N</span>eural <span class="acl-fixed-case">A</span>bstractive
    <span class="acl-fixed-case">S</span>ummarizers using <span class="acl-fixed-case">C</span>ontrastive
    <span class="acl-fixed-case">E</span>xamples
  url: https://www.aclweb.org/anthology/N19-1396
  year: '2019'
N19-1397:
  abstract: We present a new neural model for text summarization that first extracts
    sentences from a document and then compresses them. The pro-posed model offers
    a balance that sidesteps thedifficulties in abstractive methods while gener-ating
    more concise summaries than extractivemethods. In addition, our model dynamically
    determines the length of the output summary based on the gold summaries it observes
    during training and does not require length constraints typical to extractive
    summarization. The model achieves state-of-the-art results on the CNN/DailyMail
    and Newsroom datasets, improving over current extractive and abstractive methods.
    Human evaluations demonstratethat our model generates concise and informa-tive
    summaries. We also make available a new dataset of oracle compressive summaries
    derived automatically from the CNN/DailyMailreference summaries.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1397.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1397.Supplementary.pdf
  author:
  - first: Afonso
    full: Afonso Mendes
    id: alfonso-mendes
    last: Mendes
  - first: Shashi
    full: Shashi Narayan
    id: shashi-narayan
    last: Narayan
  - first: "Sebasti\xE3o"
    full: "Sebasti\xE3o Miranda"
    id: sebastiao-miranda
    last: Miranda
  - first: Zita
    full: Zita Marinho
    id: zita-marinho
    last: Marinho
  - first: "Andr\xE9 F. T."
    full: "Andr\xE9 F. T. Martins"
    id: andre-f-t-martins
    last: Martins
  - first: Shay B.
    full: Shay B. Cohen
    id: shay-b-cohen
    last: Cohen
  author_string: "Afonso Mendes, Shashi Narayan, Sebasti\xE3o Miranda, Zita Marinho,\
    \ Andr\xE9 F. T. Martins, Shay B. Cohen"
  bibkey: mendes-etal-2019-jointly
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1397
  month: June
  page_first: '3955'
  page_last: '3966'
  pages: "3955\u20133966"
  paper_id: '397'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1397.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1397.jpg
  title: Jointly Extracting and Compressing Documents with Summary State Representations
  title_html: Jointly Extracting and Compressing Documents with Summary State Representations
  url: https://www.aclweb.org/anthology/N19-1397
  year: '2019'
N19-1398:
  abstract: In this work, we define the task of teaser generation and provide an evaluation
    benchmark and baseline systems for the process of generating teasers. A teaser
    is a short reading suggestion for an article that is illustrative and includes
    curiosity-arousing elements to entice potential readers to read particular news
    items. Teasers are one of the main vehicles for transmitting news to social media
    users. We compile a novel dataset of teasers by systematically accumulating tweets
    and selecting those that conform to the teaser definition. We have compared a
    number of neural abstractive architectures on the task of teaser generation and
    the overall best performing system is See et al. seq2seq with pointer network.
  address: Minneapolis, Minnesota
  author:
  - first: Sanjeev Kumar
    full: Sanjeev Kumar Karn
    id: sanjeev-kumar-karn
    last: Karn
  - first: Mark
    full: Mark Buckley
    id: mark-buckley
    last: Buckley
  - first: Ulli
    full: Ulli Waltinger
    id: ulli-waltinger
    last: Waltinger
  - first: Hinrich
    full: "Hinrich Sch\xFCtze"
    id: hinrich-schutze
    last: "Sch\xFCtze"
  author_string: "Sanjeev Kumar Karn, Mark Buckley, Ulli Waltinger, Hinrich Sch\xFC\
    tze"
  bibkey: karn-etal-2019-news
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1398
  month: June
  page_first: '3967'
  page_last: '3977'
  pages: "3967\u20133977"
  paper_id: '398'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1398.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1398.jpg
  title: News Article Teaser Tweets and How to Generate Them
  title_html: News Article Teaser Tweets and How to Generate Them
  url: https://www.aclweb.org/anthology/N19-1398
  year: '2019'
N19-1399:
  abstract: Cross-referencing, which links passages of text to other related passages,
    can be a valuable study aid for facilitating comprehension of a text. However,
    cross-referencing requires first, a comprehensive thematic knowledge of the entire
    corpus, and second, a focused search through the corpus specifically to find such
    useful connections. Due to this, cross-reference resources are prohibitively expensive
    and exist only for the most well-studied texts (e.g. religious texts). We develop
    a topic-based system for automatically producing candidate cross-references which
    can be easily verified by human annotators. Our system utilizes fine-grained topic
    modeling with thousands of highly nuanced and specific topics to identify verse
    pairs which are topically related. We demonstrate that our system can be cost
    effective compared to having annotators acquire the expertise necessary to produce
    cross-reference resources unaided.
  address: Minneapolis, Minnesota
  author:
  - first: Jeffrey
    full: Jeffrey Lund
    id: jeffrey-lund
    last: Lund
  - first: Piper
    full: Piper Armstrong
    id: piper-armstrong
    last: Armstrong
  - first: Wilson
    full: Wilson Fearn
    id: wilson-fearn
    last: Fearn
  - first: Stephen
    full: Stephen Cowley
    id: stephen-cowley
    last: Cowley
  - first: Emily
    full: Emily Hales
    id: emily-hales
    last: Hales
  - first: Kevin
    full: Kevin Seppi
    id: kevin-seppi
    last: Seppi
  author_string: Jeffrey Lund, Piper Armstrong, Wilson Fearn, Stephen Cowley, Emily
    Hales, Kevin Seppi
  bibkey: lund-etal-2019-cross
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1399
  month: June
  page_first: '3978'
  page_last: '3987'
  pages: "3978\u20133987"
  paper_id: '399'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1399.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1399.jpg
  title: Cross-referencing Using Fine-grained Topic Modeling
  title_html: Cross-referencing Using Fine-grained Topic Modeling
  url: https://www.aclweb.org/anthology/N19-1399
  year: '2019'
N19-1400:
  abstract: "In our everyday chit-chat, there is a conversation initiator, who proactively\
    \ casts an initial utterance to start chatting. However, most existing conversation\
    \ systems cannot play this role. Previous studies on conversation systems assume\
    \ that the user always initiates conversation, and have placed emphasis on how\
    \ to respond to the given user\u2019s utterance. As a result, existing conversation\
    \ systems become passive. Namely they continue waiting until being spoken to by\
    \ the users. In this paper, we consider the system as a conversation initiator\
    \ and propose a novel task of generating the initial utterance in open-domain\
    \ non-task-oriented conversation. Here, in order not to make users bored, it is\
    \ necessary to generate diverse utterances to initiate conversation without relying\
    \ on boilerplate utterances like greetings. To this end, we propose to generate\
    \ initial utterance by summarizing and chatting about news articles, which provide\
    \ fresh and various contents everyday. To address the lack of the training data\
    \ for this task, we constructed a novel large-scale dataset through crowd-sourcing.\
    \ We also analyzed the dataset in detail to examine how humans initiate conversations\
    \ (the dataset will be released to facilitate future research activities). We\
    \ present several approaches to conversation initiation including information\
    \ retrieval based and generation based models. Experimental results showed that\
    \ the proposed models trained on our dataset performed reasonably well and outperformed\
    \ baselines that utilize automatically collected training data in both automatic\
    \ and manual evaluation."
  address: Minneapolis, Minnesota
  author:
  - first: Satoshi
    full: Satoshi Akasaki
    id: satoshi-akasaki
    last: Akasaki
  - first: Nobuhiro
    full: Nobuhiro Kaji
    id: nobuhiro-kaji
    last: Kaji
  author_string: Satoshi Akasaki, Nobuhiro Kaji
  bibkey: akasaki-kaji-2019-conversation
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1400
  month: June
  page_first: '3988'
  page_last: '3998'
  pages: "3988\u20133998"
  paper_id: '400'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1400.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1400.jpg
  title: Conversation Initiation by Diverse News Contents Introduction
  title_html: Conversation Initiation by Diverse News Contents Introduction
  url: https://www.aclweb.org/anthology/N19-1400
  year: '2019'
N19-1401:
  abstract: Neural encoder-decoder models have been successful in natural language
    generation tasks. However, real applications of abstractive summarization must
    consider an additional constraint that a generated summary should not exceed a
    desired length. In this paper, we propose a simple but effective extension of
    a sinusoidal positional encoding (Vaswani et al., 2017) so that a neural encoder-decoder
    model preserves the length constraint. Unlike previous studies that learn length
    embeddings, the proposed method can generate a text of any length even if the
    target length is unseen in training data. The experimental results show that the
    proposed method is able not only to control generation length but also improve
    ROUGE scores.
  address: Minneapolis, Minnesota
  author:
  - first: Sho
    full: Sho Takase
    id: sho-takase
    last: Takase
  - first: Naoaki
    full: Naoaki Okazaki
    id: naoaki-okazaki
    last: Okazaki
  author_string: Sho Takase, Naoaki Okazaki
  bibkey: takase-okazaki-2019-positional
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1401
  month: June
  page_first: '3999'
  page_last: '4004'
  pages: "3999\u20134004"
  paper_id: '401'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1401.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1401.jpg
  title: Positional Encoding to Control Output Sequence Length
  title_html: Positional Encoding to Control Output Sequence Length
  url: https://www.aclweb.org/anthology/N19-1401
  year: '2019'
N19-1402:
  abstract: "To improve the training efficiency of hierarchical recurrent models without\
    \ compromising their performance, we propose a strategy named as \u201Cthe lower\
    \ the simpler\u201D, which is to simplify the baseline models by making the lower\
    \ layers simpler than the upper layers. We carry out this strategy to simplify\
    \ two typical hierarchical recurrent models, namely Hierarchical Recurrent Encoder-Decoder\
    \ (HRED) and R-NET, whose basic building block is GRU. Specifically, we propose\
    \ Scalar Gated Unit (SGU), which is a simplified variant of GRU, and use it to\
    \ replace the GRUs at the middle layers of HRED and R-NET. Besides, we also use\
    \ Fixed-size Ordinally-Forgetting Encoding (FOFE), which is an efficient encoding\
    \ method without any trainable parameter, to replace the GRUs at the bottom layers\
    \ of HRED and R-NET. The experimental results show that the simplified HRED and\
    \ the simplified R-NET contain significantly less trainable parameters, consume\
    \ significantly less training time, and achieve slightly better performance than\
    \ their baseline models."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/356204320
    type: video
    url: https://vimeo.com/356204320
  author:
  - first: Chao
    full: Chao Wang
    id: chao-wang
    last: Wang
  - first: Hui
    full: Hui Jiang
    id: hui-jiang
    last: Jiang
  author_string: Chao Wang, Hui Jiang
  bibkey: wang-jiang-2019-lower
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1402
  month: June
  page_first: '4005'
  page_last: '4009'
  pages: "4005\u20134009"
  paper_id: '402'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1402.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1402.jpg
  title: 'The Lower The Simpler: Simplifying Hierarchical Recurrent Models'
  title_html: 'The Lower The Simpler: Simplifying Hierarchical Recurrent Models'
  url: https://www.aclweb.org/anthology/N19-1402
  year: '2019'
N19-1403:
  abstract: While evaluating an answer choice for Reading Comprehension task, other
    answer choices available for the question and the answers of related questions
    about the same paragraph often provide valuable information. In this paper, we
    propose a method to leverage the natural language relations between the answer
    choices, such as entailment and contradiction, to improve the performance of machine
    comprehension. We use a stand-alone question answering (QA) system to perform
    QA task and a Natural Language Inference (NLI) system to identify the relations
    between the choice pairs. Then we perform inference using an Integer Linear Programming
    (ILP)-based relational framework to re-evaluate the decisions made by the standalone
    QA system in light of the relations identified by the NLI system. We also propose
    a multitask learning model that learns both the tasks jointly.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1403.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1403.Presentation.pdf
  - filename: https://vimeo.com/361763231
    type: video
    url: https://vimeo.com/361763231
  author:
  - first: Rajkumar
    full: Rajkumar Pujari
    id: rajkumar-pujari
    last: Pujari
  - first: Dan
    full: Dan Goldwasser
    id: dan-goldwasser
    last: Goldwasser
  author_string: Rajkumar Pujari, Dan Goldwasser
  bibkey: pujari-goldwasser-2019-using
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1403
  month: June
  page_first: '4010'
  page_last: '4015'
  pages: "4010\u20134015"
  paper_id: '403'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1403.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1403.jpg
  title: Using Natural Language Relations between Answer Choices for Machine Comprehension
  title_html: Using Natural Language Relations between Answer Choices for Machine
    Comprehension
  url: https://www.aclweb.org/anthology/N19-1403
  year: '2019'
N19-1404:
  abstract: "Deep learning has emerged as a compelling solution to many NLP tasks\
    \ with remarkable performances. However, due to their opacity, such models are\
    \ hard to interpret and trust. Recent work on explaining deep models has introduced\
    \ approaches to provide insights toward the model\u2019s behaviour and predictions,\
    \ which are helpful for assessing the reliability of the model\u2019s predictions.\
    \ However, such methods do not improve the model\u2019s reliability. In this paper,\
    \ we aim to teach the model to make the right prediction for the right reason\
    \ by providing explanation training and ensuring the alignment of the model\u2019\
    s explanation with the ground truth explanation. Our experimental results on multiple\
    \ tasks and datasets demonstrate the effectiveness of the proposed method, which\
    \ produces more reliable predictions while delivering better results compared\
    \ to traditionally trained models."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/361773751
    type: video
    url: https://vimeo.com/361773751
  author:
  - first: Reza
    full: Reza Ghaeini
    id: reza-ghaeini
    last: Ghaeini
  - first: Xiaoli
    full: Xiaoli Fern
    id: xiaoli-fern
    last: Fern
  - first: Hamed
    full: Hamed Shahbazi
    id: hamed-shahbazi
    last: Shahbazi
  - first: Prasad
    full: Prasad Tadepalli
    id: prasad-tadepalli
    last: Tadepalli
  author_string: Reza Ghaeini, Xiaoli Fern, Hamed Shahbazi, Prasad Tadepalli
  bibkey: ghaeini-etal-2019-saliency
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1404
  month: June
  page_first: '4016'
  page_last: '4025'
  pages: "4016\u20134025"
  paper_id: '404'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1404.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1404.jpg
  title: 'Saliency Learning: Teaching the Model Where to Pay Attention'
  title_html: '<span class="acl-fixed-case">S</span>aliency <span class="acl-fixed-case">L</span>earning:
    <span class="acl-fixed-case">T</span>eaching the <span class="acl-fixed-case">M</span>odel
    <span class="acl-fixed-case">W</span>here to <span class="acl-fixed-case">P</span>ay
    <span class="acl-fixed-case">A</span>ttention'
  url: https://www.aclweb.org/anthology/N19-1404
  year: '2019'
N19-1405:
  abstract: Learning multi-hop reasoning has been a key challenge for reading comprehension
    models, leading to the design of datasets that explicitly focus on it. Ideally,
    a model should not be able to perform well on a multi-hop question answering task
    without doing multi-hop reasoning. In this paper, we investigate two recently
    proposed datasets, WikiHop and HotpotQA. First, we explore sentence-factored models
    for these tasks; by design, these models cannot do multi-hop reasoning, but they
    are still able to solve a large number of examples in both datasets. Furthermore,
    we find spurious correlations in the unmasked version of WikiHop, which make it
    easy to achieve high performance considering only the questions and answers. Finally,
    we investigate one key difference between these datasets, namely span-based vs.
    multiple-choice formulations of the QA task. Multiple-choice versions of both
    datasets can be easily gamed, and two models we examine only marginally exceed
    a baseline in this setting. Overall, while these datasets are useful testbeds,
    high-performing models may not be learning as much multi-hop reasoning as previously
    thought.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/361800281
    type: video
    url: https://vimeo.com/361800281
  author:
  - first: Jifan
    full: Jifan Chen
    id: jifan-chen
    last: Chen
  - first: Greg
    full: Greg Durrett
    id: greg-durrett
    last: Durrett
  author_string: Jifan Chen, Greg Durrett
  bibkey: chen-durrett-2019-understanding
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1405
  month: June
  page_first: '4026'
  page_last: '4032'
  pages: "4026\u20134032"
  paper_id: '405'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1405.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1405.jpg
  title: Understanding Dataset Design Choices for Multi-hop Reasoning
  title_html: Understanding Dataset Design Choices for Multi-hop Reasoning
  url: https://www.aclweb.org/anthology/N19-1405
  year: '2019'
N19-1406:
  abstract: Grammatical error correction (GEC) is one of the areas in natural language
    processing in which purely neural models have not yet superseded more traditional
    symbolic models. Hybrid systems combining phrase-based statistical machine translation
    (SMT) and neural sequence models are currently among the most effective approaches
    to GEC. However, both SMT and neural sequence-to-sequence models require large
    amounts of annotated data. Language model based GEC (LM-GEC) is a promising alternative
    which does not rely on annotated training data. We show how to improve LM-GEC
    by applying modelling techniques based on finite state transducers. We report
    further gains by rescoring with neural language models. We show that our methods
    developed for LM-GEC can also be used with SMT systems if annotated training data
    is available. Our best system outperforms the best published result on the CoNLL-2014
    test set, and achieves far better relative improvements over the SMT baselines
    than previous hybrid systems.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/359719163
    type: video
    url: https://vimeo.com/359719163
  author:
  - first: Felix
    full: Felix Stahlberg
    id: felix-stahlberg
    last: Stahlberg
  - first: Christopher
    full: Christopher Bryant
    id: christopher-bryant
    last: Bryant
  - first: Bill
    full: Bill Byrne
    id: bill-byrne
    last: Byrne
  author_string: Felix Stahlberg, Christopher Bryant, Bill Byrne
  bibkey: stahlberg-etal-2019-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1406
  month: June
  page_first: '4033'
  page_last: '4039'
  pages: "4033\u20134039"
  paper_id: '406'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1406.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1406.jpg
  title: Neural Grammatical Error Correction with Finite State Transducers
  title_html: Neural Grammatical Error Correction with Finite State Transducers
  url: https://www.aclweb.org/anthology/N19-1406
  year: '2019'
N19-1407:
  abstract: Self-attention networks (SANs) have drawn increasing interest due to their
    high parallelization in computation and flexibility in modeling dependencies.
    SANs can be further enhanced with multi-head attention by allowing the model to
    attend to information from different representation subspaces. In this work, we
    propose novel convolutional self-attention networks, which offer SANs the abilities
    to 1) strengthen dependencies among neighboring elements, and 2) model the interaction
    between features extracted by multiple attention heads. Experimental results of
    machine translation on different language pairs and model settings show that our
    approach outperforms both the strong Transformer baseline and other existing models
    on enhancing the locality of SANs. Comparing with prior studies, the proposed
    model is parameter free in terms of introducing no more parameters.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/359716954
    type: video
    url: https://vimeo.com/359716954
  author:
  - first: Baosong
    full: Baosong Yang
    id: baosong-yang
    last: Yang
  - first: Longyue
    full: Longyue Wang
    id: longyue-wang
    last: Wang
  - first: Derek F.
    full: Derek F. Wong
    id: derek-f-wong
    last: Wong
  - first: Lidia S.
    full: Lidia S. Chao
    id: lidia-s-chao
    last: Chao
  - first: Zhaopeng
    full: Zhaopeng Tu
    id: zhaopeng-tu
    last: Tu
  author_string: Baosong Yang, Longyue Wang, Derek F. Wong, Lidia S. Chao, Zhaopeng
    Tu
  bibkey: yang-etal-2019-convolutional
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1407
  month: June
  page_first: '4040'
  page_last: '4045'
  pages: "4040\u20134045"
  paper_id: '407'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1407.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1407.jpg
  title: Convolutional Self-Attention Networks
  title_html: Convolutional Self-Attention Networks
  url: https://www.aclweb.org/anthology/N19-1407
  year: '2019'
N19-1408:
  abstract: 'Neural network models for many NLP tasks have grown increasingly complex
    in recent years, making training and deployment more difficult. A number of recent
    papers have questioned the necessity of such architectures and found that well-executed,
    simpler models are quite effective. We show that this is also the case for document
    classification: in a large-scale reproducibility study of several recent neural
    models, we find that a simple BiLSTM architecture with appropriate regularization
    yields accuracy and F1 that are either competitive or exceed the state of the
    art on four standard benchmark datasets. Surprisingly, our simple model is able
    to achieve these results without attention mechanisms. While these regularization
    techniques, borrowed from language modeling, are not novel, to our knowledge we
    are the first to apply them in this context. Our work provides an open-source
    platform and the foundation for future work in document classification.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/359714536
    type: video
    url: https://vimeo.com/359714536
  author:
  - first: Ashutosh
    full: Ashutosh Adhikari
    id: ashutosh-adhikari
    last: Adhikari
  - first: Achyudh
    full: Achyudh Ram
    id: achyudh-ram
    last: Ram
  - first: Raphael
    full: Raphael Tang
    id: raphael-tang
    last: Tang
  - first: Jimmy
    full: Jimmy Lin
    id: jimmy-lin
    last: Lin
  author_string: Ashutosh Adhikari, Achyudh Ram, Raphael Tang, Jimmy Lin
  bibkey: adhikari-etal-2019-rethinking
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1408
  month: June
  page_first: '4046'
  page_last: '4051'
  pages: "4046\u20134051"
  paper_id: '408'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1408.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1408.jpg
  title: Rethinking Complex Neural Network Architectures for Document Classification
  title_html: Rethinking Complex Neural Network Architectures for Document Classification
  url: https://www.aclweb.org/anthology/N19-1408
  year: '2019'
N19-1409:
  abstract: Pre-trained language model representations have been successful in a wide
    range of language understanding tasks. In this paper, we examine different strategies
    to integrate pre-trained representations into sequence to sequence models and
    apply it to neural machine translation and abstractive summarization. We find
    that pre-trained representations are most effective when added to the encoder
    network which slows inference by only 14%. Our experiments in machine translation
    show gains of up to 5.3 BLEU in a simulated resource-poor setup. While returns
    diminish with more labeled data, we still observe improvements when millions of
    sentence-pairs are available. Finally, on abstractive summarization we achieve
    a new state of the art on the full text version of CNN/DailyMail.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1409.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-1409.Supplementary.pdf
  - filename: https://vimeo.com/364836786
    type: video
    url: https://vimeo.com/364836786
  author:
  - first: Sergey
    full: Sergey Edunov
    id: sergey-edunov
    last: Edunov
  - first: Alexei
    full: Alexei Baevski
    id: alexei-baevski
    last: Baevski
  - first: Michael
    full: Michael Auli
    id: michael-auli
    last: Auli
  author_string: Sergey Edunov, Alexei Baevski, Michael Auli
  bibkey: edunov-etal-2019-pre
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1409
  month: June
  page_first: '4052'
  page_last: '4059'
  pages: "4052\u20134059"
  paper_id: '409'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1409.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1409.jpg
  title: Pre-trained language model representations for language generation
  title_html: Pre-trained language model representations for language generation
  url: https://www.aclweb.org/anthology/N19-1409
  year: '2019'
N19-1410:
  abstract: 'We improve the informativeness of models for conditional text generation
    using techniques from computational pragmatics. These techniques formulate language
    production as a game between speakers and listeners, in which a speaker should
    generate output text that a listener can use to correctly identify the original
    input that the text describes. While such approaches are widely used in cognitive
    science and grounded language learning, they have received less attention for
    more standard language generation tasks. We consider two pragmatic modeling methods
    for text generation: one where pragmatics is imposed by information preservation,
    and another where pragmatics is imposed by explicit modeling of distractors. We
    find that these methods improve the performance of strong existing systems for
    abstractive summarization and generation from structured meaning representations.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1410.Presentation.pptx
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1410.Presentation.pptx
  - filename: https://vimeo.com/364848233
    type: video
    url: https://vimeo.com/364848233
  author:
  - first: Sheng
    full: Sheng Shen
    id: sheng-shen
    last: Shen
  - first: Daniel
    full: Daniel Fried
    id: daniel-fried
    last: Fried
  - first: Jacob
    full: Jacob Andreas
    id: jacob-andreas
    last: Andreas
  - first: Dan
    full: Dan Klein
    id: dan-klein
    last: Klein
  author_string: Sheng Shen, Daniel Fried, Jacob Andreas, Dan Klein
  bibkey: shen-etal-2019-pragmatically
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1410
  month: June
  page_first: '4060'
  page_last: '4067'
  pages: "4060\u20134067"
  paper_id: '410'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1410.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1410.jpg
  title: Pragmatically Informative Text Generation
  title_html: Pragmatically Informative Text Generation
  url: https://www.aclweb.org/anthology/N19-1410
  year: '2019'
N19-1411:
  abstract: The variational autoencoder (VAE) imposes a probabilistic distribution
    (typically Gaussian) on the latent space and penalizes the Kullback-Leibler (KL)
    divergence between the posterior and prior. In NLP, VAEs are extremely difficult
    to train due to the problem of KL collapsing to zero. One has to implement various
    heuristics such as KL weight annealing and word dropout in a carefully engineered
    manner to successfully train a VAE for text. In this paper, we propose to use
    the Wasserstein autoencoder (WAE) for probabilistic sentence generation, where
    the encoder could be either stochastic or deterministic. We show theoretically
    and empirically that, in the original WAE, the stochastically encoded Gaussian
    distribution tends to become a Dirac-delta function, and we propose a variant
    of WAE that encourages the stochasticity of the encoder. Experimental results
    show that the latent space learned by WAE exhibits properties of continuity and
    smoothness as in VAEs, while simultaneously achieving much higher BLEU scores
    for sentence reconstruction.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/364869536
    type: video
    url: https://vimeo.com/364869536
  author:
  - first: Hareesh
    full: Hareesh Bahuleyan
    id: hareesh-bahuleyan
    last: Bahuleyan
  - first: Lili
    full: Lili Mou
    id: lili-mou
    last: Mou
  - first: Hao
    full: Hao Zhou
    id: hao-zhou
    last: Zhou
  - first: Olga
    full: Olga Vechtomova
    id: olga-vechtomova
    last: Vechtomova
  author_string: Hareesh Bahuleyan, Lili Mou, Hao Zhou, Olga Vechtomova
  bibkey: bahuleyan-etal-2019-stochastic
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1411
  month: June
  page_first: '4068'
  page_last: '4076'
  pages: "4068\u20134076"
  paper_id: '411'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1411.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1411.jpg
  title: Stochastic Wasserstein Autoencoder for Probabilistic Sentence Generation
  title_html: Stochastic <span class="acl-fixed-case">W</span>asserstein Autoencoder
    for Probabilistic Sentence Generation
  url: https://www.aclweb.org/anthology/N19-1411
  year: '2019'
N19-1412:
  abstract: "Understanding procedural language requires reasoning about both hierarchical\
    \ and temporal relations between events. For example, \u201Cboiling pasta\u201D\
    \ is a sub-event of \u201Cmaking a pasta dish\u201D, typically happens before\
    \ \u201Cdraining pasta,\u201D and requires the use of omitted tools (e.g. a strainer,\
    \ sink...). While people are able to choose when and how to use abstract versus\
    \ concrete instructions, the NLP community lacks corpora and tasks for evaluating\
    \ if our models can do the same. In this paper, we introduce KidsCook, a parallel\
    \ script corpus, as well as a cloze task which matches video captions with missing\
    \ procedural details. Experimental results show that state-of-the-art models struggle\
    \ at this task, which requires inducing functional commonsense knowledge not explicitly\
    \ stated in text."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/364884144
    type: video
    url: https://vimeo.com/364884144
  author:
  - first: Yonatan
    full: Yonatan Bisk
    id: yonatan-bisk
    last: Bisk
  - first: Jan
    full: Jan Buys
    id: jan-buys
    last: Buys
  - first: Karl
    full: Karl Pichotta
    id: karl-pichotta
    last: Pichotta
  - first: Yejin
    full: Yejin Choi
    id: yejin-choi
    last: Choi
  author_string: Yonatan Bisk, Jan Buys, Karl Pichotta, Yejin Choi
  bibkey: bisk-etal-2019-benchmarking
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1412
  month: June
  page_first: '4077'
  page_last: '4085'
  pages: "4077\u20134085"
  paper_id: '412'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1412.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1412.jpg
  title: Benchmarking Hierarchical Script Knowledge
  title_html: Benchmarking Hierarchical Script Knowledge
  url: https://www.aclweb.org/anthology/N19-1412
  year: '2019'
N19-1413:
  abstract: "A number of psycholinguistic studies have factorially manipulated words\u2019\
    \ contextual predictabilities and corpus frequencies and shown separable effects\
    \ of each on measures of human sentence processing, a pattern which has been used\
    \ to support distinct mechanisms underlying prediction on the one hand and lexical\
    \ retrieval on the other. This paper examines the generalizability of this finding\
    \ to more realistic conditions of sentence processing by studying effects of frequency\
    \ and predictability in three large-scale naturalistic reading corpora. Results\
    \ show significant effects of word frequency and predictability in isolation but\
    \ no effect of frequency over and above predictability, and thus do not provide\
    \ evidence of distinct mechanisms. The non-replication of separable effects in\
    \ a naturalistic setting raises doubts about the existence of such a distinction\
    \ in everyday sentence comprehension. Instead, these results are consistent with\
    \ previous claims that apparent effects of frequency are underlyingly effects\
    \ of predictability."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/359727110
    type: video
    url: https://vimeo.com/359727110
  author:
  - first: Cory
    full: Cory Shain
    id: cory-shain
    last: Shain
  author_string: Cory Shain
  bibkey: shain-2019-large
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1413
  month: June
  page_first: '4086'
  page_last: '4094'
  pages: "4086\u20134094"
  paper_id: '413'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1413.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1413.jpg
  title: A large-scale study of the effects of word frequency and predictability in
    naturalistic reading
  title_html: A large-scale study of the effects of word frequency and predictability
    in naturalistic reading
  url: https://www.aclweb.org/anthology/N19-1413
  year: '2019'
N19-1414:
  abstract: "This paper presents three hybrid models that directly combine latent\
    \ Dirichlet allocation and word embedding for distinguishing between speakers\
    \ with and without Alzheimer\u2019s disease from transcripts of picture descriptions.\
    \ Two of our models get F-scores over the current state-of-the-art using automatic\
    \ methods on the DementiaBank dataset."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/359723663
    type: video
    url: https://vimeo.com/359723663
  author:
  - first: Akshay
    full: Akshay Budhkar
    id: akshay-budhkar
    last: Budhkar
  - first: Frank
    full: Frank Rudzicz
    id: frank-rudzicz
    last: Rudzicz
  author_string: Akshay Budhkar, Frank Rudzicz
  bibkey: budhkar-rudzicz-2019-augmenting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1414
  month: June
  page_first: '4095'
  page_last: '4099'
  pages: "4095\u20134099"
  paper_id: '414'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1414.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1414.jpg
  title: Augmenting word2vec with latent Dirichlet allocation within a clinical application
  title_html: Augmenting word2vec with latent <span class="acl-fixed-case">D</span>irichlet
    allocation within a clinical application
  url: https://www.aclweb.org/anthology/N19-1414
  year: '2019'
N19-1415:
  abstract: While idiosyncrasies of the Chinese classifier system have been a richly
    studied topic among linguists (Adams and Conklin, 1973; Erbaugh, 1986; Lakoff,
    1986), not much work has been done to quantify them with statistical methods.
    In this paper, we introduce an information-theoretic approach to measuring idiosyncrasy;
    we examine how much the uncertainty in Mandarin Chinese classifiers can be reduced
    by knowing semantic information about the nouns that the classifiers modify. Using
    the empirical distribution of classifiers from the parsed Chinese Gigaword corpus
    (Graff et al., 2005), we compute the mutual information (in bits) between the
    distribution over classifiers and distributions over other linguistic quantities.
    We investigate whether semantic classes of nouns and adjectives differ in how
    much they reduce uncertainty in classifier choice, and find that it is not fully
    idiosyncratic; while there are no obvious trends for the majority of semantic
    classes, shape nouns reduce uncertainty in classifier choice the most.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/359721173
    type: video
    url: https://vimeo.com/359721173
  author:
  - first: Shijia
    full: Shijia Liu
    id: shijia-liu
    last: Liu
  - first: Hongyuan
    full: Hongyuan Mei
    id: hongyuan-mei
    last: Mei
  - first: Adina
    full: Adina Williams
    id: adina-williams
    last: Williams
  - first: Ryan
    full: Ryan Cotterell
    id: ryan-cotterell
    last: Cotterell
  author_string: Shijia Liu, Hongyuan Mei, Adina Williams, Ryan Cotterell
  bibkey: liu-etal-2019-idiosyncrasies
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1415
  month: June
  page_first: '4100'
  page_last: '4106'
  pages: "4100\u20134106"
  paper_id: '415'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1415.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1415.jpg
  title: On the Idiosyncrasies of the Mandarin Chinese Classifier System
  title_html: On the Idiosyncrasies of the <span class="acl-fixed-case">M</span>andarin
    <span class="acl-fixed-case">C</span>hinese Classifier System
  url: https://www.aclweb.org/anthology/N19-1415
  year: '2019'
N19-1416:
  abstract: Fine-tuning neural networks is widely used to transfer valuable knowledge
    from high-resource to low-resource domains. In a standard fine-tuning scheme,
    source and target problems are trained using the same architecture. Although capable
    of adapting to new domains, pre-trained units struggle with learning uncommon
    target-specific patterns. In this paper, we propose to augment the target-network
    with normalised, weighted and randomly initialised units that beget a better adaptation
    while maintaining the valuable source knowledge. Our experiments on POS tagging
    of social media texts (Tweets domain) demonstrate that our method achieves state-of-the-art
    performances on 3 commonly used datasets.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/361815756
    type: video
    url: https://vimeo.com/361815756
  author:
  - first: Sara
    full: Sara Meftah
    id: sara-meftah
    last: Meftah
  - first: Youssef
    full: Youssef Tamaazousti
    id: youssef-tamaazousti
    last: Tamaazousti
  - first: Nasredine
    full: Nasredine Semmar
    id: nasredine-semmar
    last: Semmar
  - first: Hassane
    full: Hassane Essafi
    id: hassane-essafi
    last: Essafi
  - first: Fatiha
    full: Fatiha Sadat
    id: fatiha-sadat
    last: Sadat
  author_string: Sara Meftah, Youssef Tamaazousti, Nasredine Semmar, Hassane Essafi,
    Fatiha Sadat
  bibkey: meftah-etal-2019-joint
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1416
  month: June
  page_first: '4107'
  page_last: '4112'
  pages: "4107\u20134112"
  paper_id: '416'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1416.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1416.jpg
  title: Joint Learning of Pre-Trained and Random Units for Domain Adaptation in Part-of-Speech
    Tagging
  title_html: Joint Learning of Pre-Trained and Random Units for Domain Adaptation
    in Part-of-Speech Tagging
  url: https://www.aclweb.org/anthology/N19-1416
  year: '2019'
N19-1417:
  abstract: 'In recent years neural language models (LMs) have set the state-of-the-art
    performance for several benchmarking datasets. While the reasons for their success
    and their computational demand are well-documented, a comparison between neural
    models and more recent developments in n-gram models is neglected. In this paper,
    we examine the recent progress in n-gram literature, running experiments on 50
    languages covering all morphological language families. Experimental results illustrate
    that a simple extension of Modified Kneser-Ney outperforms an lstm language model
    on 42 languages while a word-level Bayesian n-gram LM (Shareghi et al., 2017)
    outperforms the character-aware neural model (Kim et al., 2016) on average across
    all languages, and its extension which explicitly injects linguistic knowledge
    (Gerz et al., 2018) on 8 languages. Further experiments on larger Europarl datasets
    for 3 languages indicate that neural architectures are able to outperform computationally
    much cheaper n-gram models: n-gram training is up to 15,000x quicker. Our experiments
    illustrate that standalone n-gram models lend themselves as natural choices for
    resource-lean or morphologically rich languages, while the recent progress has
    significantly improved their accuracy.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/361819010
    type: video
    url: https://vimeo.com/361819010
  author:
  - first: Ehsan
    full: Ehsan Shareghi
    id: ehsan-shareghi
    last: Shareghi
  - first: Daniela
    full: Daniela Gerz
    id: daniela-gerz
    last: Gerz
  - first: Ivan
    full: "Ivan Vuli\u0107"
    id: ivan-vulic
    last: "Vuli\u0107"
  - first: Anna
    full: Anna Korhonen
    id: anna-korhonen
    last: Korhonen
  author_string: "Ehsan Shareghi, Daniela Gerz, Ivan Vuli\u0107, Anna Korhonen"
  bibkey: shareghi-etal-2019-show
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1417
  month: June
  page_first: '4113'
  page_last: '4118'
  pages: "4113\u20134118"
  paper_id: '417'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1417.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1417.jpg
  title: 'Show Some Love to Your n-grams: A Bit of Progress and Stronger n-gram Language
    Modeling Baselines'
  title_html: 'Show Some Love to Your n-grams: A Bit of Progress and Stronger n-gram
    Language Modeling Baselines'
  url: https://www.aclweb.org/anthology/N19-1417
  year: '2019'
N19-1418:
  abstract: Lemmatization aims to reduce the sparse data problem by relating the inflected
    forms of a word to its dictionary form. Using context can help, both for unseen
    and ambiguous words. Yet most context-sensitive approaches require full lemma-annotated
    sentences for training, which may be scarce or unavailable in low-resource languages.
    In addition (as shown here), in a low-resource setting, a lemmatizer can learn
    more from n labeled examples of distinct words (types) than from n (contiguous)
    labeled tokens, since the latter contain far fewer distinct types. To combine
    the efficiency of type-based learning with the benefits of context, we propose
    a way to train a context-sensitive lemmatizer with little or no labeled corpus
    data, using inflection tables from the UniMorph project and raw text examples
    from Wikipedia that provide sentence contexts for the unambiguous UniMorph examples.
    Despite these being unambiguous examples, the model successfully generalizes from
    them, leading to improved results (both overall, and especially on unseen words)
    in comparison to a baseline that does not use context.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1418.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1418.Presentation.pdf
  - filename: https://vimeo.com/361822826
    type: video
    url: https://vimeo.com/361822826
  author:
  - first: Toms
    full: Toms Bergmanis
    id: toms-bergmanis
    last: Bergmanis
  - first: Sharon
    full: Sharon Goldwater
    id: sharon-goldwater
    last: Goldwater
  author_string: Toms Bergmanis, Sharon Goldwater
  bibkey: bergmanis-goldwater-2019-training
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1418
  month: June
  page_first: '4119'
  page_last: '4128'
  pages: "4119\u20134128"
  paper_id: '418'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1418.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1418.jpg
  title: Training Data Augmentation for Context-Sensitive Neural Lemmatizer Using
    Inflection Tables and Raw Text
  title_html: <span class="acl-fixed-case">T</span>raining <span class="acl-fixed-case">D</span>ata
    <span class="acl-fixed-case">A</span>ugmentation for <span class="acl-fixed-case">C</span>ontext-<span
    class="acl-fixed-case">S</span>ensitive <span class="acl-fixed-case">N</span>eural
    <span class="acl-fixed-case">L</span>emmatizer <span class="acl-fixed-case">U</span>sing
    <span class="acl-fixed-case">I</span>nflection <span class="acl-fixed-case">T</span>ables
    and <span class="acl-fixed-case">R</span>aw <span class="acl-fixed-case">T</span>ext
  url: https://www.aclweb.org/anthology/N19-1418
  year: '2019'
N19-1419:
  abstract: "Recent work has improved our ability to detect linguistic knowledge in\
    \ word representations. However, current methods for detecting syntactic knowledge\
    \ do not test whether syntax trees are represented in their entirety. In this\
    \ work, we propose a structural probe, which evaluates whether syntax trees are\
    \ embedded in a linear transformation of a neural network\u2019s word representation\
    \ space. The probe identifies a linear transformation under which squared L2 distance\
    \ encodes the distance between words in the parse tree, and one in which squared\
    \ L2 norm encodes depth in the parse tree. Using our probe, we show that such\
    \ transformations exist for both ELMo and BERT but not in baselines, providing\
    \ evidence that entire syntax trees are embedded implicitly in deep models\u2019\
    \ vector geometry."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/361827125
    type: video
    url: https://vimeo.com/361827125
  author:
  - first: John
    full: John Hewitt
    id: john-hewitt
    last: Hewitt
  - first: Christopher D.
    full: Christopher D. Manning
    id: christopher-d-manning
    last: Manning
  author_string: John Hewitt, Christopher D. Manning
  bibkey: hewitt-manning-2019-structural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1419
  month: June
  page_first: '4129'
  page_last: '4138'
  pages: "4129\u20134138"
  paper_id: '419'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1419.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1419.jpg
  title: A Structural Probe for Finding Syntax in Word Representations
  title_html: <span class="acl-fixed-case">A</span> Structural Probe for Finding Syntax
    in Word Representations
  url: https://www.aclweb.org/anthology/N19-1419
  year: '2019'
N19-1420:
  abstract: This paper seeks to model human language by the mathematical framework
    of quantum physics. With the well-designed mathematical formulations in quantum
    physics, this framework unifies different linguistic units in a single complex-valued
    vector space, e.g. words as particles in quantum states and sentences as mixed
    systems. A complex-valued network is built to implement this framework for semantic
    matching. With well-constrained complex-valued components, the network admits
    interpretations to explicit physical meanings. The proposed complex-valued network
    for matching (CNM) achieves comparable performances to strong CNN and RNN baselines
    on two benchmarking question answering (QA) datasets.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/365160781
    type: video
    url: https://vimeo.com/365160781
  author:
  - first: Qiuchi
    full: Qiuchi Li
    id: qiuchi-li
    last: Li
  - first: Benyou
    full: Benyou Wang
    id: benyou-wang
    last: Wang
  - first: Massimo
    full: Massimo Melucci
    id: massimo-melucci
    last: Melucci
  author_string: Qiuchi Li, Benyou Wang, Massimo Melucci
  bibkey: li-etal-2019-cnm
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1420
  month: June
  page_first: '4139'
  page_last: '4148'
  pages: "4139\u20134148"
  paper_id: '420'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1420.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1420.jpg
  title: 'CNM: An Interpretable Complex-valued Network for Matching'
  title_html: '<span class="acl-fixed-case">CNM</span>: An Interpretable Complex-valued
    Network for Matching'
  url: https://www.aclweb.org/anthology/N19-1420
  year: '2019'
N19-1421:
  abstract: 'When answering a question, people often draw upon their rich world knowledge
    in addition to the particular context. Recent work has focused primarily on answering
    questions given some relevant document or context, and required very little general
    background. To investigate question answering with prior knowledge, we present
    CommonsenseQA: a challenging new dataset for commonsense question answering. To
    capture common sense beyond associations, we extract from ConceptNet (Speer et
    al., 2017) multiple target concepts that have the same semantic relation to a
    single source concept. Crowd-workers are asked to author multiple-choice questions
    that mention the source concept and discriminate in turn between each of the target
    concepts. This encourages workers to create questions with complex semantics that
    often require prior knowledge. We create 12,247 questions through this procedure
    and demonstrate the difficulty of our task with a large number of strong baselines.
    Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56%
    accuracy, well below human performance, which is 89%.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/365154751
    type: video
    url: https://vimeo.com/365154751
  author:
  - first: Alon
    full: Alon Talmor
    id: alon-talmor
    last: Talmor
  - first: Jonathan
    full: Jonathan Herzig
    id: jonathan-herzig
    last: Herzig
  - first: Nicholas
    full: Nicholas Lourie
    id: nicholas-lourie
    last: Lourie
  - first: Jonathan
    full: Jonathan Berant
    id: jonathan-berant
    last: Berant
  author_string: Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant
  bibkey: talmor-etal-2019-commonsenseqa
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1421
  month: June
  page_first: '4149'
  page_last: '4158'
  pages: "4149\u20134158"
  paper_id: '421'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1421.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1421.jpg
  title: 'CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge'
  title_html: '<span class="acl-fixed-case">C</span>ommonsense<span class="acl-fixed-case">QA</span>:
    A Question Answering Challenge Targeting Commonsense Knowledge'
  url: https://www.aclweb.org/anthology/N19-1421
  year: '2019'
N19-1422:
  abstract: Current work on multimodal machine translation (MMT) has suggested that
    the visual modality is either unnecessary or only marginally beneficial. We posit
    that this is a consequence of the very simple, short and repetitive sentences
    used in the only available dataset for the task (Multi30K), rendering the source
    text sufficient as context. In the general case, however, we believe that it is
    possible to combine visual and textual information in order to ground translations.
    In this paper we probe the contribution of the visual modality to state-of-the-art
    MMT models by conducting a systematic analysis where we partially deprive the
    models from source-side textual context. Our results show that under limited textual
    context, models are capable of leveraging the visual input to generate better
    translations. This contradicts the current belief that MMT models disregard the
    visual modality because of either the quality of the image features or the way
    they are integrated into the model.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-1422.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-1422.Presentation.pdf
  - filename: https://vimeo.com/365146894
    type: video
    url: https://vimeo.com/365146894
  author:
  - first: Ozan
    full: Ozan Caglayan
    id: ozan-caglayan
    last: Caglayan
  - first: Pranava
    full: Pranava Madhyastha
    id: pranava-swaroop-madhyastha
    last: Madhyastha
  - first: Lucia
    full: Lucia Specia
    id: lucia-specia
    last: Specia
  - first: "Lo\xEFc"
    full: "Lo\xEFc Barrault"
    id: loic-barrault
    last: Barrault
  author_string: "Ozan Caglayan, Pranava Madhyastha, Lucia Specia, Lo\xEFc Barrault"
  bibkey: caglayan-etal-2019-probing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1422
  month: June
  page_first: '4159'
  page_last: '4170'
  pages: "4159\u20134170"
  paper_id: '422'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1422.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1422.jpg
  title: Probing the Need for Visual Context in Multimodal Machine Translation
  title_html: Probing the Need for Visual Context in Multimodal Machine Translation
  url: https://www.aclweb.org/anthology/N19-1422
  year: '2019'
N19-1423:
  abstract: We introduce a new language representation model called BERT, which stands
    for Bidirectional Encoder Representations from Transformers. Unlike recent language
    representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed
    to pre-train deep bidirectional representations from unlabeled text by jointly
    conditioning on both left and right context in all layers. As a result, the pre-trained
    BERT model can be fine-tuned with just one additional output layer to create state-of-the-art
    models for a wide range of tasks, such as question answering and language inference,
    without substantial task-specific architecture modifications. BERT is conceptually
    simple and empirically powerful. It obtains new state-of-the-art results on eleven
    natural language processing tasks, including pushing the GLUE score to 80.5 (7.7
    point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement),
    SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement)
    and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/365139010
    type: video
    url: https://vimeo.com/365139010
  author:
  - first: Jacob
    full: Jacob Devlin
    id: jacob-devlin
    last: Devlin
  - first: Ming-Wei
    full: Ming-Wei Chang
    id: ming-wei-chang
    last: Chang
  - first: Kenton
    full: Kenton Lee
    id: kenton-lee
    last: Lee
  - first: Kristina
    full: Kristina Toutanova
    id: kristina-toutanova
    last: Toutanova
  author_string: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova
  bibkey: devlin-etal-2019-bert
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1423
  month: June
  page_first: '4171'
  page_last: '4186'
  pages: "4171\u20134186"
  paper_id: '423'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1423.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1423.jpg
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
  title_html: '<span class="acl-fixed-case">BERT</span>: Pre-training of Deep Bidirectional
    Transformers for Language Understanding'
  url: https://www.aclweb.org/anthology/N19-1423
  year: '2019'
N19-1424:
  abstract: "There is a growing body of work that proposes methods for mitigating\
    \ bias in machine learning systems. These methods typically rely on access to\
    \ protected attributes such as race, gender, or age. However, this raises two\
    \ significant challenges: (1) protected attributes may not be available or it\
    \ may not be legal to use them, and (2) it is often desirable to simultaneously\
    \ consider multiple protected attributes, as well as their intersections. In the\
    \ context of mitigating bias in occupation classification, we propose a method\
    \ for discouraging correlation between the predicted probability of an individual\u2019\
    s true occupation and a word embedding of their name. This method leverages the\
    \ societal biases that are encoded in word embeddings, eliminating the need for\
    \ access to protected attributes. Crucially, it only requires access to individuals\u2019\
    \ names at training time and not at deployment time. We evaluate two variations\
    \ of our proposed method using a large-scale dataset of online biographies. We\
    \ find that both variations simultaneously reduce race and gender biases, with\
    \ almost no reduction in the classifier\u2019s overall true positive rate."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/365132300
    type: video
    url: https://vimeo.com/365132300
  author:
  - first: Alexey
    full: Alexey Romanov
    id: alexey-romanov
    last: Romanov
  - first: Maria
    full: Maria De-Arteaga
    id: maria-de-arteaga
    last: De-Arteaga
  - first: Hanna
    full: Hanna Wallach
    id: hanna-wallach
    last: Wallach
  - first: Jennifer
    full: Jennifer Chayes
    id: jennifer-chayes
    last: Chayes
  - first: Christian
    full: Christian Borgs
    id: christian-borgs
    last: Borgs
  - first: Alexandra
    full: Alexandra Chouldechova
    id: alexandra-chouldechova
    last: Chouldechova
  - first: Sahin
    full: Sahin Geyik
    id: sahin-geyik
    last: Geyik
  - first: Krishnaram
    full: Krishnaram Kenthapadi
    id: krishnaram-kenthapadi
    last: Kenthapadi
  - first: Anna
    full: Anna Rumshisky
    id: anna-rumshisky
    last: Rumshisky
  - first: Adam
    full: Adam Kalai
    id: adam-kalai
    last: Kalai
  author_string: Alexey Romanov, Maria De-Arteaga, Hanna Wallach, Jennifer Chayes,
    Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, Anna
    Rumshisky, Adam Kalai
  bibkey: romanov-etal-2019-whats
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)'
  doi: 10.18653/v1/N19-1424
  month: June
  page_first: '4187'
  page_last: '4195'
  pages: "4187\u20134195"
  paper_id: '424'
  parent_volume_id: N19-1
  pdf: https://www.aclweb.org/anthology/N19-1424.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-1424.jpg
  title: "What\u2019s in a Name? Reducing Bias in Bios without Access to Protected\
    \ Attributes"
  title_html: "What\u2019s in a Name? <span class=\"acl-fixed-case\">R</span>educing\
    \ Bias in Bios without Access to Protected Attributes"
  url: https://www.aclweb.org/anthology/N19-1424
  year: '2019'
N19-2000:
  address: Minneapolis, Minnesota
  author:
  - first: Anastassia
    full: Anastassia Loukina
    id: anastassia-loukina
    last: Loukina
  - first: Michelle
    full: Michelle Morales
    id: michelle-morales
    last: Morales
  - first: Rohit
    full: Rohit Kumar
    id: rohit-kumar
    last: Kumar
  author_string: Anastassia Loukina, Michelle Morales, Rohit Kumar
  bibkey: naacl-2019-2019-north
  bibtype: proceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  month: June
  paper_id: '0'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2000.jpg
  title: 'Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  title_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  url: https://www.aclweb.org/anthology/N19-2000
  year: '2019'
N19-2001:
  abstract: Input method editor (IME) converts sequential alphabet key inputs to words
    in a target language. It is an indispensable service for billions of Asian users.
    Although the neural-based language model is extensively studied and shows promising
    results in sequence-to-sequence tasks, applying a neural-based language model
    to IME was not considered feasible due to high latency when converting words on
    user devices. In this work, we articulate the bottleneck of neural IME decoding
    to be the heavy softmax computation over a large vocabulary. We propose an approach
    that incrementally builds a subset vocabulary from the word lattice. Our approach
    always computes the probability with a selected subset vocabulary. When the selected
    vocabulary is updated, the stale probabilities in previous steps are fixed by
    recomputing the missing logits. The experiments on Japanese IME benchmark shows
    an over 50x speedup for the softmax computations comparing to the baseline, reaching
    real-time speed even on commodity CPU without losing conversion accuracy. The
    approach is potentially applicable to other incremental sequence-to-sequence decoding
    tasks such as real-time continuous speech recognition.
  address: Minneapolis, Minnesota
  author:
  - first: Jiali
    full: Jiali Yao
    id: jiali-yao
    last: Yao
  - first: Raphael
    full: Raphael Shu
    id: raphael-shu
    last: Shu
  - first: Xinjian
    full: Xinjian Li
    id: xinjian-li
    last: Li
  - first: Katsutoshi
    full: Katsutoshi Ohtsuki
    id: katsutoshi-ohtsuki
    last: Ohtsuki
  - first: Hideki
    full: Hideki Nakayama
    id: hideki-nakayama
    last: Nakayama
  author_string: Jiali Yao, Raphael Shu, Xinjian Li, Katsutoshi Ohtsuki, Hideki Nakayama
  bibkey: yao-etal-2019-enabling
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2001
  month: June
  page_first: '1'
  page_last: '8'
  pages: "1\u20138"
  paper_id: '1'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2001.jpg
  title: Enabling Real-time Neural IME with Incremental Vocabulary Selection
  title_html: Enabling Real-time Neural <span class="acl-fixed-case">IME</span> with
    Incremental Vocabulary Selection
  url: https://www.aclweb.org/anthology/N19-2001
  year: '2019'
N19-2002:
  abstract: In this paper, we introduce an approach for leveraging available data
    across multiple locales sharing the same language to 1) improve domain classification
    model accuracy in Spoken Language Understanding and user experience even if new
    locales do not have sufficient data and 2) reduce the cost of scaling the domain
    classifier to a large number of locales. We propose a locale-agnostic universal
    domain classification model based on selective multi-task learning that learns
    a joint representation of an utterance over locales with different sets of domains
    and allows locales to share knowledge selectively depending on the domains. The
    experimental results demonstrate the effectiveness of our approach on domain classification
    task in the scenario of multiple locales with imbalanced data and disparate domain
    sets. The proposed approach outperforms other baselines models especially when
    classifying locale-specific domains and also low-resourced domains.
  address: Minneapolis, Minnesota
  author:
  - first: Jihwan
    full: Jihwan Lee
    id: jihwan-lee
    last: Lee
  - first: Ruhi
    full: Ruhi Sarikaya
    id: ruhi-sarikaya
    last: Sarikaya
  - first: Young-Bum
    full: Young-Bum Kim
    id: young-bum-kim
    last: Kim
  author_string: Jihwan Lee, Ruhi Sarikaya, Young-Bum Kim
  bibkey: lee-etal-2019-locale
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2002
  month: June
  page_first: '9'
  page_last: '15'
  pages: "9\u201315"
  paper_id: '2'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2002.jpg
  title: Locale-agnostic Universal Domain Classification Model in Spoken Language
    Understanding
  title_html: Locale-agnostic Universal Domain Classification Model in Spoken Language
    Understanding
  url: https://www.aclweb.org/anthology/N19-2002
  year: '2019'
N19-2003:
  abstract: Executable semantic parsing is the task of converting natural language
    utterances into logical forms that can be directly used as queries to get a response.
    We build a transfer learning framework for executable semantic parsing. We show
    that the framework is effective for Question Answering (Q&A) as well as for Spoken
    Language Understanding (SLU). We further investigate the case where a parser on
    a new domain can be learned by exploiting data on other domains, either via multi-task
    learning between the target domain and an auxiliary domain or via pre-training
    on the auxiliary domain and fine-tuning on the target domain. With either flavor
    of transfer learning, we are able to improve performance on most domains; we experiment
    with public data sets such as Overnight and NLmaps as well as with commercial
    SLU data. The experiments carried out on data sets that are different in nature
    show how executable semantic parsing can unify different areas of NLP such as
    Q&A and SLU.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-2003.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-2003.Presentation.pdf
  author:
  - first: Marco
    full: Marco Damonte
    id: marco-damonte
    last: Damonte
  - first: Rahul
    full: Rahul Goel
    id: rahul-goel
    last: Goel
  - first: Tagyoung
    full: Tagyoung Chung
    id: tagyoung-chung
    last: Chung
  author_string: Marco Damonte, Rahul Goel, Tagyoung Chung
  bibkey: damonte-etal-2019-practical
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2003
  month: June
  page_first: '16'
  page_last: '23'
  pages: "16\u201323"
  paper_id: '3'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2003.jpg
  title: Practical Semantic Parsing for Spoken Language Understanding
  title_html: Practical Semantic Parsing for Spoken Language Understanding
  url: https://www.aclweb.org/anthology/N19-2003
  year: '2019'
N19-2004:
  abstract: Data for human-human spoken dialogues for research and development are
    currently very limited in quantity, variety, and sources; such data are even scarcer
    in healthcare. In this work, we investigate fast prototyping of a dialogue comprehension
    system by leveraging on minimal nurse-to-patient conversations. We propose a framework
    inspired by nurse-initiated clinical symptom monitoring conversations to construct
    a simulated human-human dialogue dataset, embodying linguistic characteristics
    of spoken interactions like thinking aloud, self-contradiction, and topic drift.
    We then adopt an established bidirectional attention pointer network on this simulated
    dataset, achieving more than 80% F1 score on a held-out test set from real-world
    nurse-to-patient conversations. The ability to automatically comprehend conversations
    in the healthcare domain by exploiting only limited data has implications for
    improving clinical workflows through red flag symptom detection and triaging capabilities.
    We demonstrate the feasibility for efficient and effective extraction, retrieval
    and comprehension of symptom checking information discussed in multi-turn human-human
    spoken conversations.
  address: Minneapolis, Minnesota
  author:
  - first: Zhengyuan
    full: Zhengyuan Liu
    id: zhengyuan-liu
    last: Liu
  - first: Hazel
    full: Hazel Lim
    id: hazel-lim
    last: Lim
  - first: Nur Farah Ain
    full: Nur Farah Ain Suhaimi
    id: nur-farah-ain-suhaimi
    last: Suhaimi
  - first: Shao Chuen
    full: Shao Chuen Tong
    id: shao-chuen-tong
    last: Tong
  - first: Sharon
    full: Sharon Ong
    id: sharon-ong
    last: Ong
  - first: Angela
    full: Angela Ng
    id: angela-ng
    last: Ng
  - first: Sheldon
    full: Sheldon Lee
    id: sheldon-lee
    last: Lee
  - first: Michael R.
    full: Michael R. Macdonald
    id: michael-r-macdonald
    last: Macdonald
  - first: Savitha
    full: Savitha Ramasamy
    id: savitha-ramasamy
    last: Ramasamy
  - first: Pavitra
    full: Pavitra Krishnaswamy
    id: pavitra-krishnaswamy
    last: Krishnaswamy
  - first: Wai Leng
    full: Wai Leng Chow
    id: wai-leng-chow
    last: Chow
  - first: Nancy F.
    full: Nancy F. Chen
    id: nancy-chen
    last: Chen
  author_string: Zhengyuan Liu, Hazel Lim, Nur Farah Ain Suhaimi, Shao Chuen Tong,
    Sharon Ong, Angela Ng, Sheldon Lee, Michael R. Macdonald, Savitha Ramasamy, Pavitra
    Krishnaswamy, Wai Leng Chow, Nancy F. Chen
  bibkey: liu-etal-2019-fast
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2004
  month: June
  page_first: '24'
  page_last: '31'
  pages: "24\u201331"
  paper_id: '4'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2004.jpg
  title: Fast Prototyping a Dialogue Comprehension System for Nurse-Patient Conversations
    on Symptom Monitoring
  title_html: Fast Prototyping a Dialogue Comprehension System for Nurse-Patient Conversations
    on Symptom Monitoring
  url: https://www.aclweb.org/anthology/N19-2004
  year: '2019'
N19-2005:
  abstract: Visually rich documents (VRDs) are ubiquitous in daily business and life.
    Examples are purchase receipts, insurance policy documents, custom declaration
    forms and so on. In VRDs, visual and layout information is critical for document
    understanding, and texts in such documents cannot be serialized into the one-dimensional
    sequence without losing information. Classic information extraction models such
    as BiLSTM-CRF typically operate on text sequences and do not incorporate visual
    features. In this paper, we introduce a graph convolution based model to combine
    textual and visual information presented in VRDs. Graph embeddings are trained
    to summarize the context of a text segment in the document, and further combined
    with text embeddings for entity extraction. Extensive experiments have been conducted
    to show that our method outperforms BiLSTM-CRF baselines by significant margins,
    on two real-world datasets. Additionally, ablation studies are also performed
    to evaluate the effectiveness of each component of our model.
  address: Minneapolis, Minnesota
  author:
  - first: Xiaojing
    full: Xiaojing Liu
    id: xiaojing-liu
    last: Liu
  - first: Feiyu
    full: Feiyu Gao
    id: feiyu-gao
    last: Gao
  - first: Qiong
    full: Qiong Zhang
    id: qiong-zhang
    last: Zhang
  - first: Huasha
    full: Huasha Zhao
    id: huasha-zhao
    last: Zhao
  author_string: Xiaojing Liu, Feiyu Gao, Qiong Zhang, Huasha Zhao
  bibkey: liu-etal-2019-graph
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2005
  month: June
  page_first: '32'
  page_last: '39'
  pages: "32\u201339"
  paper_id: '5'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2005.jpg
  title: Graph Convolution for Multimodal Information Extraction from Visually Rich
    Documents
  title_html: Graph Convolution for Multimodal Information Extraction from Visually
    Rich Documents
  url: https://www.aclweb.org/anthology/N19-2005
  year: '2019'
N19-2006:
  abstract: "We consider the problem of diversifying automated reply suggestions for\
    \ a commercial instant-messaging (IM) system (Skype). Our conversation model is\
    \ a standard matching based information retrieval architecture, which consists\
    \ of two parallel encoders to project messages and replies into a common feature\
    \ representation. During inference, we select replies from a fixed response set\
    \ using nearest neighbors in the feature space. To diversify responses, we formulate\
    \ the model as a generative latent variable model with Conditional Variational\
    \ Auto-Encoder (M-CVAE). We propose a constrained-sampling approach to make the\
    \ variational inference in M-CVAE efficient for our production system. In offline\
    \ experiments, M-CVAE consistently increased diversity by \u223C30\u221240% without\
    \ significant impact on relevance. This translated to a \u223C5% gain in click-rate\
    \ in our online production system."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/361695513
    type: video
    url: https://vimeo.com/361695513
  author:
  - first: Budhaditya
    full: Budhaditya Deb
    id: budhaditya-deb
    last: Deb
  - first: Peter
    full: Peter Bailey
    id: peter-bailey
    last: Bailey
  - first: Milad
    full: Milad Shokouhi
    id: milad-shokouhi
    last: Shokouhi
  author_string: Budhaditya Deb, Peter Bailey, Milad Shokouhi
  bibkey: deb-etal-2019-diversifying
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2006
  month: June
  page_first: '40'
  page_last: '47'
  pages: "40\u201347"
  paper_id: '6'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2006.jpg
  title: Diversifying Reply Suggestions Using a Matching-Conditional Variational Autoencoder
  title_html: Diversifying Reply Suggestions Using a Matching-Conditional Variational
    Autoencoder
  url: https://www.aclweb.org/anthology/N19-2006
  year: '2019'
N19-2007:
  abstract: 'End-to-end neural models for goal-oriented conversational systems have
    become an increasingly active area of research, though results in real-world settings
    are few. We present real-world results for two issue types in the customer service
    domain. We train models on historical chat transcripts and test on live contacts
    using a human-in-the-loop research platform. Additionally, we incorporate customer
    profile features to assess their impact on model performance. We experiment with
    two approaches for response generation: (1) sequence-to-sequence generation and
    (2) template ranking. To test our models, a customer service agent handles live
    contacts and at each turn we present the top four model responses and allow the
    agent to select (and optionally edit) one of the suggestions or to type their
    own. We present results for turn acceptance rate, response coverage, and edit
    rate based on approximately 600 contacts, as well as qualitative analysis on patterns
    of turn rejection and edit behavior. Top-4 turn acceptance rate across all models
    ranges from 63%-80%. Our results suggest that these models are promising for an
    agent-support application.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/361700294
    type: video
    url: https://vimeo.com/361700294
  author:
  - first: Yichao
    full: Yichao Lu
    id: yichao-lu
    last: Lu
  - first: Manisha
    full: Manisha Srivastava
    id: manisha-srivastava
    last: Srivastava
  - first: Jared
    full: Jared Kramer
    id: jared-kramer
    last: Kramer
  - first: Heba
    full: Heba Elfardy
    id: heba-elfardy
    last: Elfardy
  - first: Andrea
    full: Andrea Kahn
    id: andrea-kahn
    last: Kahn
  - first: Song
    full: Song Wang
    id: song-wang
    last: Wang
  - first: Vikas
    full: Vikas Bhardwaj
    id: vikas-bhardwaj
    last: Bhardwaj
  author_string: Yichao Lu, Manisha Srivastava, Jared Kramer, Heba Elfardy, Andrea
    Kahn, Song Wang, Vikas Bhardwaj
  bibkey: lu-etal-2019-goal
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2007
  month: June
  page_first: '48'
  page_last: '55'
  pages: "48\u201355"
  paper_id: '7'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2007.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2007.jpg
  title: Goal-Oriented End-to-End Conversational Models with Profile Features in a
    Real-World Setting
  title_html: Goal-Oriented End-to-End Conversational Models with Profile Features
    in a Real-World Setting
  url: https://www.aclweb.org/anthology/N19-2007
  year: '2019'
N19-2008:
  abstract: "Consumers dissatisfied with the normal dispute resolution process provided\
    \ by an e-commerce company\u2019s customer service agents have the option of escalating\
    \ their complaints by filing grievances with a government authority. This paper\
    \ tackles the challenge of monitoring ongoing text chat dialogues to identify\
    \ cases where the customer expresses such an intent, providing triage and prioritization\
    \ for a separate pool of specialized agents specially trained to handle more complex\
    \ situations. We describe a hybrid model that tackles this challenge by integrating\
    \ recurrent neural networks with manually-engineered features. Experiments show\
    \ that both components are complementary and contribute to overall recall, outperforming\
    \ competitive baselines. A trial online deployment of our model demonstrates its\
    \ business value in improving customer service."
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/361707312
    type: video
    url: https://vimeo.com/361707312
  author:
  - first: Wei
    full: Wei Yang
    id: wei-yang
    last: Yang
  - first: Luchen
    full: Luchen Tan
    id: luchen-tan
    last: Tan
  - first: Chunwei
    full: Chunwei Lu
    id: chunwei-lu
    last: Lu
  - first: Anqi
    full: Anqi Cui
    id: anqi-cui
    last: Cui
  - first: Han
    full: Han Li
    id: han-li
    last: Li
  - first: Xi
    full: Xi Chen
    id: xi-chen
    last: Chen
  - first: Kun
    full: Kun Xiong
    id: kun-xiong
    last: Xiong
  - first: Muzi
    full: Muzi Wang
    id: muzi-wang
    last: Wang
  - first: Ming
    full: Ming Li
    id: ming-li
    last: Li
  - first: Jian
    full: Jian Pei
    id: jian-pei
    last: Pei
  - first: Jimmy
    full: Jimmy Lin
    id: jimmy-lin
    last: Lin
  author_string: Wei Yang, Luchen Tan, Chunwei Lu, Anqi Cui, Han Li, Xi Chen, Kun
    Xiong, Muzi Wang, Ming Li, Jian Pei, Jimmy Lin
  bibkey: yang-etal-2019-detecting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2008
  month: June
  page_first: '56'
  page_last: '63'
  pages: "56\u201363"
  paper_id: '8'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2008.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2008.jpg
  title: Detecting Customer Complaint Escalation with Recurrent Neural Networks and
    Manually-Engineered Features
  title_html: Detecting Customer Complaint Escalation with Recurrent Neural Networks
    and Manually-Engineered Features
  url: https://www.aclweb.org/anthology/N19-2008
  year: '2019'
N19-2009:
  abstract: Nowadays, more and more customers browse and purchase products in favor
    of using mobile E-Commerce Apps such as Taobao and Amazon. Since merchants are
    usually inclined to describe redundant and over-informative product titles to
    attract attentions from customers, it is important to concisely display short
    product titles on limited screen of mobile phones. To address this discrepancy,
    previous studies mainly consider textual information of long product titles and
    lacks of human-like view during training and evaluation process. In this paper,
    we propose a Multi-Modal Generative Adversarial Network (MM-GAN) for short product
    title generation in E-Commerce, which innovatively incorporates image information
    and attribute tags from product, as well as textual information from original
    long titles. MM-GAN poses short title generation as a reinforcement learning process,
    where the generated titles are evaluated by the discriminator in a human-like
    view. Extensive experiments on a large-scale E-Commerce dataset demonstrate that
    our algorithm outperforms other state-of-the-art methods. Moreover, we deploy
    our model into a real-world online E-Commerce environment and effectively boost
    the performance of click through rate and click conversion rate by 1.66% and 1.87%,
    respectively.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/361702996
    type: video
    url: https://vimeo.com/361702996
  author:
  - first: Jianguo
    full: Jianguo Zhang
    id: jianguo-zhang
    last: Zhang
  - first: Pengcheng
    full: Pengcheng Zou
    id: pengcheng-zou
    last: Zou
  - first: Zhao
    full: Zhao Li
    id: zhao-li
    last: Li
  - first: Yao
    full: Yao Wan
    id: yao-wan
    last: Wan
  - first: Xiuming
    full: Xiuming Pan
    id: xiuming-pan
    last: Pan
  - first: Yu
    full: Yu Gong
    id: yu-gong
    last: Gong
  - first: Philip S.
    full: Philip S. Yu
    id: philip-s-yu
    last: Yu
  author_string: Jianguo Zhang, Pengcheng Zou, Zhao Li, Yao Wan, Xiuming Pan, Yu Gong,
    Philip S. Yu
  bibkey: zhang-etal-2019-multi-modal
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2009
  month: June
  page_first: '64'
  page_last: '72'
  pages: "64\u201372"
  paper_id: '9'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2009.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2009.jpg
  title: Multi-Modal Generative Adversarial Network for Short Product Title Generation
    in Mobile E-Commerce
  title_html: Multi-Modal Generative Adversarial Network for Short Product Title Generation
    in Mobile E-Commerce
  url: https://www.aclweb.org/anthology/N19-2009
  year: '2019'
N19-2010:
  abstract: There have been many studies on neural headline generation models trained
    with a lot of (article, headline) pairs. However, there are few situations for
    putting such models into practical use in the real world since news articles typically
    already have corresponding headlines. In this paper, we describe a practical use
    case of neural headline generation in a news aggregator, where dozens of professional
    editors constantly select important news articles and manually create their headlines,
    which are much shorter than the original headlines. Specifically, we show how
    to deploy our model to an editing support tool and report the results of comparing
    the behavior of the editors before and after the release.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-2010.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-2010.Presentation.pdf
  - filename: https://vimeo.com/361705360
    type: video
    url: https://vimeo.com/361705360
  author:
  - first: Kazuma
    full: Kazuma Murao
    id: kazuma-murao
    last: Murao
  - first: Ken
    full: Ken Kobayashi
    id: ken-kobayashi
    last: Kobayashi
  - first: Hayato
    full: Hayato Kobayashi
    id: hayato-kobayashi
    last: Kobayashi
  - first: Taichi
    full: Taichi Yatsuka
    id: taichi-yatsuka
    last: Yatsuka
  - first: Takeshi
    full: Takeshi Masuyama
    id: takeshi-masuyama
    last: Masuyama
  - first: Tatsuru
    full: Tatsuru Higurashi
    id: tatsuru-higurashi
    last: Higurashi
  - first: Yoshimune
    full: Yoshimune Tabuchi
    id: yoshimune-tabuchi
    last: Tabuchi
  author_string: Kazuma Murao, Ken Kobayashi, Hayato Kobayashi, Taichi Yatsuka, Takeshi
    Masuyama, Tatsuru Higurashi, Yoshimune Tabuchi
  bibkey: murao-etal-2019-case
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2010
  month: June
  page_first: '73'
  page_last: '82'
  pages: "73\u201382"
  paper_id: '10'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2010.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2010.jpg
  title: A Case Study on Neural Headline Generation for Editing Support
  title_html: A Case Study on Neural Headline Generation for Editing Support
  url: https://www.aclweb.org/anthology/N19-2010
  year: '2019'
N19-2011:
  abstract: 'We explore the use of lexicons or gazettes in neural models for slot
    tagging in spoken language understanding. We develop models that encode lexicon
    information as neural features for use in a Long-short term memory neural network.
    Experiments are performed on data from 4 domains from an intelligent assistant
    under conditions that often occur in an industry setting, where there may be:
    1) large amounts of training data, 2) limited amounts of training data for new
    domains, and 3) cross domain training. Results show that the use of neural lexicon
    information leads to a significant improvement in slot tagging, with improvements
    in the F-score of up to 12%. Our findings have implications for how lexicons can
    be used to improve the performance of neural slot tagging models.'
  address: Minneapolis, Minnesota
  author:
  - first: Kyle
    full: Kyle Williams
    id: kyle-williams
    last: Williams
  author_string: Kyle Williams
  bibkey: williams-2019-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2011
  month: June
  page_first: '83'
  page_last: '89'
  pages: "83\u201389"
  paper_id: '11'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2011.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2011.jpg
  title: Neural Lexicons for Slot Tagging in Spoken Language Understanding
  title_html: Neural Lexicons for Slot Tagging in Spoken Language Understanding
  url: https://www.aclweb.org/anthology/N19-2011
  year: '2019'
N19-2012:
  abstract: We explore active learning (AL) for improving the accuracy of new domains
    in a natural language understanding (NLU) system. We propose an algorithm called
    Majority-CRF that uses an ensemble of classification models to guide the selection
    of relevant utterances, as well as a sequence labeling model to help prioritize
    informative examples. Experiments with three domains show that Majority-CRF achieves
    6.6%-9% relative error rate reduction compared to random sampling with the same
    annotation budget, and statistically significant improvements compared to other
    AL approaches. Additionally, case studies with human-in-the-loop AL on six new
    domains show 4.6%-9% improvement on an existing NLU system.
  address: Minneapolis, Minnesota
  author:
  - first: Stanislav
    full: Stanislav Peshterliev
    id: stanislav-peshterliev
    last: Peshterliev
  - first: John
    full: John Kearney
    id: john-kearney
    last: Kearney
  - first: Abhyuday
    full: Abhyuday Jagannatha
    id: abhyuday-jagannatha
    last: Jagannatha
  - first: Imre
    full: Imre Kiss
    id: imre-kiss
    last: Kiss
  - first: Spyros
    full: Spyros Matsoukas
    id: spyros-matsoukas
    last: Matsoukas
  author_string: Stanislav Peshterliev, John Kearney, Abhyuday Jagannatha, Imre Kiss,
    Spyros Matsoukas
  bibkey: peshterliev-etal-2019-active
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2012
  month: June
  page_first: '90'
  page_last: '96'
  pages: "90\u201396"
  paper_id: '12'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2012.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2012.jpg
  title: Active Learning for New Domains in Natural Language Understanding
  title_html: Active Learning for New Domains in Natural Language Understanding
  url: https://www.aclweb.org/anthology/N19-2012
  year: '2019'
N19-2013:
  abstract: "We present a novel approach to dialogue state tracking and referring\
    \ expression resolution tasks. Successful contextual understanding of multi-turn\
    \ spoken dialogues requires resolving referring expressions across turns and tracking\
    \ the entities relevant to the conversation across turns. Tracking conversational\
    \ state is particularly challenging in a multi-domain scenario when there exist\
    \ multiple spoken language understanding (SLU) sub-systems, and each SLU sub-system\
    \ operates on its domain-specific meaning representation. While previous approaches\
    \ have addressed the disparate schema issue by learning candidate transformations\
    \ of the meaning representation, in this paper, we instead model the reference\
    \ resolution as a dialogue context-aware user query reformulation task \u2013\
    \ the dialog state is serialized to a sequence of natural language tokens representing\
    \ the conversation. We develop our model for query reformulation using a pointer-generator\
    \ network and a novel multi-task learning setup. In our experiments, we show a\
    \ significant improvement in absolute F1 on an internal as well as a, soon to\
    \ be released, public benchmark respectively."
  address: Minneapolis, Minnesota
  author:
  - first: Pushpendre
    full: Pushpendre Rastogi
    id: pushpendre-rastogi
    last: Rastogi
  - first: Arpit
    full: Arpit Gupta
    id: arpit-gupta
    last: Gupta
  - first: Tongfei
    full: Tongfei Chen
    id: tongfei-chen
    last: Chen
  - first: Mathias
    full: Mathias Lambert
    id: mathias-lambert
    last: Lambert
  author_string: Pushpendre Rastogi, Arpit Gupta, Tongfei Chen, Mathias Lambert
  bibkey: rastogi-etal-2019-scaling
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2013
  month: June
  page_first: '97'
  page_last: '105'
  pages: "97\u2013105"
  paper_id: '13'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2013.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2013.jpg
  title: Scaling Multi-Domain Dialogue State Tracking via Query Reformulation
  title_html: Scaling Multi-Domain Dialogue State Tracking via Query Reformulation
  url: https://www.aclweb.org/anthology/N19-2013
  year: '2019'
N19-2014:
  abstract: 'There has been a significant investment in dialog systems (tools and
    runtime) for building conversational systems by major companies including Google,
    IBM, Microsoft, and Amazon. The question remains whether these tools are up to
    the task of building conversational, task-oriented dialog applications at the
    enterprise level. In our company, we are exploring and comparing several toolsets
    in an effort to determine their strengths and weaknesses in meeting our goals
    for dialog system development: accuracy, time to market, ease of replicating and
    extending applications, and efficiency and ease of use by developers. In this
    paper, we provide both quantitative and qualitative results in three main areas:
    natural language understanding, dialog, and text generation. While existing toolsets
    were all incomplete, we hope this paper will provide a roadmap of where they need
    to go to meet the goal of building effective dialog systems.'
  address: Minneapolis, Minnesota
  author:
  - first: Marie
    full: Marie Meteer
    id: marie-meteer
    last: Meteer
  - first: Meghan
    full: Meghan Hickey
    id: meghan-hickey
    last: Hickey
  - first: Carmi
    full: Carmi Rothberg
    id: carmi-rothberg
    last: Rothberg
  - first: David
    full: David Nahamoo
    id: david-nahamoo
    last: Nahamoo
  - first: Ellen
    full: Ellen Eide Kislal
    id: ellen-eide-kislal
    last: Eide Kislal
  author_string: Marie Meteer, Meghan Hickey, Carmi Rothberg, David Nahamoo, Ellen
    Eide Kislal
  bibkey: meteer-etal-2019-tools
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2014
  month: June
  page_first: '106'
  page_last: '113'
  pages: "106\u2013113"
  paper_id: '14'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2014.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2014.jpg
  title: Are the Tools up to the Task? an Evaluation of Commercial Dialog Tools in
    Developing Conversational Enterprise-grade Dialog Systems
  title_html: Are the Tools up to the Task? an Evaluation of Commercial Dialog Tools
    in Developing Conversational Enterprise-grade Dialog Systems
  url: https://www.aclweb.org/anthology/N19-2014
  year: '2019'
N19-2015:
  abstract: There are significant challenges involved in the design and implementation
    of a dialog-based tutoring system (DBT) ranging from domain engineering to natural
    language classification and eventually instantiating an adaptive, personalized
    dialog strategy. These issues are magnified when implementing such a system at
    scale and across domains. In this paper, we describe and reflect on the design,
    methods, decisions and assessments that led to the successful deployment of our
    AI driven DBT currently being used by several hundreds of college level students
    for practice and self-regulated study in diverse subjects like Sociology, Communications,
    and American Government.
  address: Minneapolis, Minnesota
  author:
  - first: Shazia
    full: Shazia Afzal
    id: shazia-afzal
    last: Afzal
  - first: Tejas
    full: Tejas Dhamecha
    id: tejas-dhamecha
    last: Dhamecha
  - first: Nirmal
    full: Nirmal Mukhi
    id: nirmal-mukhi
    last: Mukhi
  - first: Renuka
    full: Renuka Sindhgatta
    id: renuka-sindhgatta
    last: Sindhgatta
  - first: Smit
    full: Smit Marvaniya
    id: smit-marvaniya
    last: Marvaniya
  - first: Matthew
    full: Matthew Ventura
    id: matthew-ventura
    last: Ventura
  - first: Jessica
    full: Jessica Yarbro
    id: jessica-yarbro
    last: Yarbro
  author_string: Shazia Afzal, Tejas Dhamecha, Nirmal Mukhi, Renuka Sindhgatta, Smit
    Marvaniya, Matthew Ventura, Jessica Yarbro
  bibkey: afzal-etal-2019-development
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2015
  month: June
  page_first: '114'
  page_last: '121'
  pages: "114\u2013121"
  paper_id: '15'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2015.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2015.jpg
  title: Development and Deployment of a Large-Scale Dialog-based Intelligent Tutoring
    System
  title_html: Development and Deployment of a Large-Scale Dialog-based Intelligent
    Tutoring System
  url: https://www.aclweb.org/anthology/N19-2015
  year: '2019'
N19-2016:
  abstract: In this paper, we investigate the challenges of using reinforcement learning
    agents for question-answering over knowledge graphs for real-world applications.
    We examine the performance metrics used by state-of-the-art systems and determine
    that they are inadequate for such settings. More specifically, they do not evaluate
    the systems correctly for situations when there is no answer available and thus
    agents optimized for these metrics are poor at modeling confidence. We introduce
    a simple new performance metric for evaluating question-answering agents that
    is more representative of practical usage conditions, and optimize for this metric
    by extending the binary reward structure used in prior work to a ternary reward
    structure which also rewards an agent for not answering a question rather than
    giving an incorrect answer. We show that this can drastically improve the precision
    of answered questions while only not answering a limited number of previously
    correctly answered questions. Employing a supervised learning strategy using depth-first-search
    paths to bootstrap the reinforcement learning algorithm further improves performance.
  address: Minneapolis, Minnesota
  author:
  - first: "Fr\xE9deric"
    full: "Fr\xE9deric Godin"
    id: frederic-godin
    last: Godin
  - first: Anjishnu
    full: Anjishnu Kumar
    id: anjishnu-kumar
    last: Kumar
  - first: Arpit
    full: Arpit Mittal
    id: arpit-mittal
    last: Mittal
  author_string: "Fr\xE9deric Godin, Anjishnu Kumar, Arpit Mittal"
  bibkey: godin-etal-2019-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2016
  month: June
  page_first: '122'
  page_last: '129'
  pages: "122\u2013129"
  paper_id: '16'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2016.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2016.jpg
  title: 'Learning When Not to Answer: a Ternary Reward Structure for Reinforcement
    Learning Based Question Answering'
  title_html: 'Learning When Not to Answer: a Ternary Reward Structure for Reinforcement
    Learning Based Question Answering'
  url: https://www.aclweb.org/anthology/N19-2016
  year: '2019'
N19-2017:
  abstract: Software Requirement Specification documents provide natural language
    descriptions of the core functional requirements as a set of use-cases. Essentially,
    each use-case contains a set of actors and sequences of steps describing the interactions
    among them. Goals of use-case reviews and analyses include their correctness,
    completeness, detection of ambiguities, prototyping, verification, test case generation
    and traceability. Message Sequence Chart (MSC) have been proposed as a expressive,
    rigorous yet intuitive visual representation of use-cases. In this paper, we describe
    a linguistic knowledge-based approach to extract MSCs from use-cases. Compared
    to existing techniques, we extract richer constructs of the MSC notation such
    as timers, conditions and alt-boxes. We apply this tool to extract MSCs from several
    real-life software use-case descriptions and show that it performs better than
    the existing techniques. We also discuss the benefits and limitations of the extracted
    MSCs to meet the above goals.
  address: Minneapolis, Minnesota
  author:
  - first: Girish
    full: Girish Palshikar
    id: girish-palshikar
    last: Palshikar
  - first: Nitin
    full: Nitin Ramrakhiyani
    id: nitin-ramrakhiyani
    last: Ramrakhiyani
  - first: Sangameshwar
    full: Sangameshwar Patil
    id: sangameshwar-patil
    last: Patil
  - first: Sachin
    full: Sachin Pawar
    id: sachin-pawar
    last: Pawar
  - first: Swapnil
    full: Swapnil Hingmire
    id: swapnil-hingmire
    last: Hingmire
  - first: Vasudeva
    full: Vasudeva Varma
    id: vasudeva-varma
    last: Varma
  - first: Pushpak
    full: Pushpak Bhattacharyya
    id: pushpak-bhattacharyya
    last: Bhattacharyya
  author_string: Girish Palshikar, Nitin Ramrakhiyani, Sangameshwar Patil, Sachin
    Pawar, Swapnil Hingmire, Vasudeva Varma, Pushpak Bhattacharyya
  bibkey: palshikar-etal-2019-extraction
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2017
  month: June
  page_first: '130'
  page_last: '137'
  pages: "130\u2013137"
  paper_id: '17'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2017.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2017.jpg
  title: Extraction of Message Sequence Charts from Software Use-Case Descriptions
  title_html: Extraction of Message Sequence Charts from Software Use-Case Descriptions
  url: https://www.aclweb.org/anthology/N19-2017
  year: '2019'
N19-2018:
  abstract: 'A capable, automatic Question Answering (QA) system can provide more
    complete and accurate answers using a comprehensive knowledge base (KB). One important
    approach to constructing a comprehensive knowledge base is to extract information
    from Wikipedia infobox tables to populate an existing KB. Despite previous successes
    in the Infobox Extraction (IBE) problem (e.g., DBpedia), three major challenges
    remain: 1) Deterministic extraction patterns used in DBpedia are vulnerable to
    template changes; 2) Over-trusting Wikipedia anchor links can lead to entity disambiguation
    errors; 3) Heuristic-based extraction of unlinkable entities yields low precision,
    hurting both accuracy and completeness of the final KB. This paper presents a
    robust approach that tackles all three challenges. We build probabilistic models
    to predict relations between entity mentions directly from the infobox tables
    in HTML. The entity mentions are linked to identifiers in an existing KB if possible.
    The unlinkable ones are also parsed and preserved in the final output. Training
    data for both the relation extraction and the entity linking models are automatically
    generated using distant supervision. We demonstrate the empirical effectiveness
    of the proposed method in both precision and recall compared to a strong IBE baseline,
    DBpedia, with an absolute improvement of 41.3% in average F1. We also show that
    our extraction makes the final KB significantly more complete, improving the completeness
    score of list-value relation types by 61.4%.'
  address: Minneapolis, Minnesota
  author:
  - first: Boya
    full: Boya Peng
    id: boya-peng
    last: Peng
  - first: Yejin
    full: Yejin Huh
    id: yejin-huh
    last: Huh
  - first: Xiao
    full: Xiao Ling
    id: xiao-ling
    last: Ling
  - first: Michele
    full: Michele Banko
    id: michele-banko
    last: Banko
  author_string: Boya Peng, Yejin Huh, Xiao Ling, Michele Banko
  bibkey: peng-etal-2019-improving
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2018
  month: June
  page_first: '138'
  page_last: '148'
  pages: "138\u2013148"
  paper_id: '18'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2018.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2018.jpg
  title: Improving Knowledge Base Construction from Robust Infobox Extraction
  title_html: Improving Knowledge Base Construction from Robust Infobox Extraction
  url: https://www.aclweb.org/anthology/N19-2018
  year: '2019'
N19-2019:
  abstract: "In this paper we present a new method for intent recognition for complex\
    \ dialog management in low resource situations. Complex dialog management is required\
    \ because our target domain is real world mixed initiative food ordering between\
    \ agents and their customers, where individual customer utterances may contain\
    \ multiple intents and refer to food items with complex structure. For example,\
    \ a customer might say \u201CCan I get a deluxe burger with large fries and oh\
    \ put extra mayo on the burger would you?\u201D We approach this task as a multi-level\
    \ sequence labeling problem, with the constraint of limited real training data.\
    \ Both traditional methods like HMM, MEMM, or CRF and newer methods like DNN or\
    \ BiLSTM use only homogeneous feature sets. Newer methods perform better but also\
    \ require considerably more data. Previous research has done pseudo-data synthesis\
    \ to obtain the required amounts of training data. We propose to use a k-NN learner\
    \ with heterogeneous feature set. We used windowed word n-grams, POS tag n-grams\
    \ and pre-trained word embeddings as features. For the experiments we perform\
    \ a comparison between using pseudo-data and real world data. We also perform\
    \ semi-supervised self-training to obtain additional labeled data, in order to\
    \ better model real world scenarios. Instead of using massive pseudo-data, we\
    \ show that with only less than 1% of the data size, we can achieve better result\
    \ than any of the methods above by annotating real world data. We achieve labeled\
    \ bracketed F-scores of 75.46, 52.84 and 49.66 for the three levels of sequence\
    \ labeling where each level has a longer word span than its previous level. Overall\
    \ we achieve 60.71F. In comparison, two previous systems, MEMM and DNN-ELMO, achieved\
    \ 52.32 and 45.25 respectively."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-2019.Poster.pdf
    type: poster
    url: https://www.aclweb.org/anthology/attachments/N19-2019.Poster.pdf
  author:
  - first: Yue
    full: Yue Chen
    id: yue-chen
    last: Chen
  - first: John
    full: John Chen
    id: john-chen
    last: Chen
  author_string: Yue Chen, John Chen
  bibkey: chen-chen-2019-k
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2019
  month: June
  page_first: '149'
  page_last: '156'
  pages: "149\u2013156"
  paper_id: '19'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2019.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2019.jpg
  title: A k-Nearest Neighbor Approach towards Multi-level Sequence Labeling
  title_html: A k-Nearest Neighbor Approach towards Multi-level Sequence Labeling
  url: https://www.aclweb.org/anthology/N19-2019
  year: '2019'
N19-2020:
  abstract: Tracking user reported bugs requires considerable engineering effort in
    going through many repetitive reports and assigning them to the correct teams.
    This paper proposes a neural architecture that can jointly (1) detect if two bug
    reports are duplicates, and (2) aggregate them into latent topics. Leveraging
    the assumption that learning the topic of a bug is a sub-task for detecting duplicates,
    we design a loss function that can jointly perform both tasks but needs supervision
    for only duplicate classification, achieving topic clustering in an unsupervised
    fashion. We use a two-step attention module that uses self-attention for topic
    clustering and conditional attention for duplicate detection. We study the characteristics
    of two types of real world datasets that have been marked for duplicate bugs by
    engineers and by non-technical annotators. The results demonstrate that our model
    not only can outperform state-of-the-art methods for duplicate classification
    on both cases, but can also learn meaningful latent clusters without additional
    supervision.
  address: Minneapolis, Minnesota
  author:
  - first: Lahari
    full: Lahari Poddar
    id: lahari-poddar
    last: Poddar
  - first: Leonardo
    full: Leonardo Neves
    id: leonardo-neves
    last: Neves
  - first: William
    full: William Brendel
    id: william-brendel
    last: Brendel
  - first: Luis
    full: Luis Marujo
    id: luis-marujo
    last: Marujo
  - first: Sergey
    full: Sergey Tulyakov
    id: sergey-tulyakov
    last: Tulyakov
  - first: Pradeep
    full: Pradeep Karuturi
    id: pradeep-karuturi
    last: Karuturi
  author_string: Lahari Poddar, Leonardo Neves, William Brendel, Luis Marujo, Sergey
    Tulyakov, Pradeep Karuturi
  bibkey: poddar-etal-2019-train
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2020
  month: June
  page_first: '157'
  page_last: '165'
  pages: "157\u2013165"
  paper_id: '20'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2020.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2020.jpg
  title: 'Train One Get One Free: Partially Supervised Neural Network for Bug Report
    Duplicate Detection and Clustering'
  title_html: 'Train One Get One Free: Partially Supervised Neural Network for Bug
    Report Duplicate Detection and Clustering'
  url: https://www.aclweb.org/anthology/N19-2020
  year: '2019'
N19-2021:
  abstract: "This paper addresses the issue of generalization for Semantic Parsing\
    \ in an adversarial framework. Building models that are more robust to inter-document\
    \ variability is crucial for the integration of Semantic Parsing technologies\
    \ in real applications. The underlying question throughout this study is whether\
    \ adversarial learning can be used to train models on a higher level of abstraction\
    \ in order to increase their robustness to lexical and stylistic variations. We\
    \ propose to perform Semantic Parsing with a domain classification adversarial\
    \ task, covering various use-cases with or without explicit knowledge of the domain.\
    \ The strategy is first evaluated on a French corpus of encyclopedic documents,\
    \ annotated with FrameNet, in an information retrieval perspective. This corpus\
    \ constitutes a new public benchmark, gathering documents from various thematic\
    \ domains and various sources. We show that adversarial learning yields improved\
    \ results when using explicit domain classification as the adversarial task. We\
    \ also propose an unsupervised domain discovery approach that yields equivalent\
    \ improvements. The latter is also evaluated on a PropBank Semantic Role Labeling\
    \ task on the CoNLL-2005 benchmark and is shown to increase the model\u2019s generalization\
    \ capabilities on out-of-domain data."
  address: Minneapolis, Minnesota
  author:
  - first: Gabriel
    full: Gabriel Marzinotto
    id: gabriel-marzinotto
    last: Marzinotto
  - first: "G\xE9raldine"
    full: "G\xE9raldine Damnati"
    id: geraldine-damnati
    last: Damnati
  - first: "Fr\xE9d\xE9ric"
    full: "Fr\xE9d\xE9ric B\xE9chet"
    id: frederic-bechet
    last: "B\xE9chet"
  - first: "Beno\xEEt"
    full: "Beno\xEEt Favre"
    id: benoit-favre
    last: Favre
  author_string: "Gabriel Marzinotto, G\xE9raldine Damnati, Fr\xE9d\xE9ric B\xE9chet,\
    \ Beno\xEEt Favre"
  bibkey: marzinotto-etal-2019-robust
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2021
  month: June
  page_first: '166'
  page_last: '173'
  pages: "166\u2013173"
  paper_id: '21'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2021.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2021.jpg
  title: Robust Semantic Parsing with Adversarial Learning for Domain Generalization
  title_html: Robust Semantic Parsing with Adversarial Learning for Domain Generalization
  url: https://www.aclweb.org/anthology/N19-2021
  year: '2019'
N19-2022:
  abstract: Contract analysis can significantly ease the work for humans using AI
    techniques. This paper shows a problem of Element Tagging on Insurance Policy
    (ETIP). A novel Text-Of-Interest Convolutional Neural Network (TOI-CNN) is proposed
    for the ETIP solution. We introduce a TOI pooling layer to replace traditional
    pooling layer for processing the nested phrasal or clausal elements in insurance
    policies. The advantage of TOI pooling layer is that the nested elements from
    one sentence could share computation and context in the forward and backward passes.
    The computation of backpropagation through TOI pooling is also demonstrated in
    the paper. We have collected a large Chinese insurance contract dataset and labeled
    the critical elements of seven categories to test the performance of the proposed
    method. The results show the promising performance of our method in the ETIP problem.
  address: Minneapolis, Minnesota
  author:
  - first: Lin
    full: Lin Sun
    id: lin-sun
    last: Sun
  - first: Kai
    full: Kai Zhang
    id: kai-zhang
    last: Zhang
  - first: Fule
    full: Fule Ji
    id: fule-ji
    last: Ji
  - first: Zhenhua
    full: Zhenhua Yang
    id: zhenhua-yang
    last: Yang
  author_string: Lin Sun, Kai Zhang, Fule Ji, Zhenhua Yang
  bibkey: sun-etal-2019-toi
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2022
  month: June
  page_first: '174'
  page_last: '181'
  pages: "174\u2013181"
  paper_id: '22'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2022.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2022.jpg
  title: 'TOI-CNN: a Solution of Information Extraction on Chinese Insurance Policy'
  title_html: '<span class="acl-fixed-case">TOI</span>-<span class="acl-fixed-case">CNN</span>:
    a Solution of Information Extraction on <span class="acl-fixed-case">C</span>hinese
    Insurance Policy'
  url: https://www.aclweb.org/anthology/N19-2022
  year: '2019'
N19-2023:
  abstract: This work explores cross-lingual transfer learning (TL) for named entity
    recognition, focusing on bootstrapping Japanese from English. A deep neural network
    model is adopted and the best combination of weights to transfer is extensively
    investigated. Moreover, a novel approach is presented that overcomes linguistic
    differences between this language pair by romanizing a portion of the Japanese
    input. Experiments are conducted on external datasets, as well as internal large-scale
    real-world ones. Gains with TL are achieved for all evaluated cases. Finally,
    the influence on TL of the target dataset size and of the target tagset distribution
    is further investigated.
  address: Minneapolis, Minnesota
  author:
  - first: Andrew
    full: Andrew Johnson
    id: andrew-johnson
    last: Johnson
  - first: Penny
    full: Penny Karanasou
    id: penny-karanasou
    last: Karanasou
  - first: Judith
    full: Judith Gaspers
    id: judith-gaspers
    last: Gaspers
  - first: Dietrich
    full: Dietrich Klakow
    id: dietrich-klakow
    last: Klakow
  author_string: Andrew Johnson, Penny Karanasou, Judith Gaspers, Dietrich Klakow
  bibkey: johnson-etal-2019-cross
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2023
  month: June
  page_first: '182'
  page_last: '189'
  pages: "182\u2013189"
  paper_id: '23'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2023.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2023.jpg
  title: Cross-lingual Transfer Learning for Japanese Named Entity Recognition
  title_html: Cross-lingual Transfer Learning for <span class="acl-fixed-case">J</span>apanese
    Named Entity Recognition
  url: https://www.aclweb.org/anthology/N19-2023
  year: '2019'
N19-2024:
  abstract: Text normalization (TN) is an important step in conversational systems.
    It converts written text to its spoken form to facilitate speech recognition,
    natural language understanding and text-to-speech synthesis. Finite state transducers
    (FSTs) are commonly used to build grammars that handle text normalization. However,
    translating linguistic knowledge into grammars requires extensive effort. In this
    paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence
    (seq2seq) models. Previous research focuses on normalizing a word (or phrase)
    with the help of limited word-level context, while our approach directly normalizes
    full sentences. We find subword models with additional linguistic features yield
    the best performance (with a word error rate of 0.17%).
  address: Minneapolis, Minnesota
  author:
  - first: Courtney
    full: Courtney Mansfield
    id: courtney-mansfield
    last: Mansfield
  - first: Ming
    full: Ming Sun
    id: ming-sun
    last: Sun
  - first: Yuzong
    full: Yuzong Liu
    id: yuzong-liu
    last: Liu
  - first: Ankur
    full: Ankur Gandhe
    id: ankur-gandhe
    last: Gandhe
  - first: "Bj\xF6rn"
    full: "Bj\xF6rn Hoffmeister"
    id: bjorn-hoffmeister
    last: Hoffmeister
  author_string: "Courtney Mansfield, Ming Sun, Yuzong Liu, Ankur Gandhe, Bj\xF6rn\
    \ Hoffmeister"
  bibkey: mansfield-etal-2019-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2024
  month: June
  page_first: '190'
  page_last: '196'
  pages: "190\u2013196"
  paper_id: '24'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2024.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2024.jpg
  title: Neural Text Normalization with Subword Units
  title_html: Neural Text Normalization with Subword Units
  url: https://www.aclweb.org/anthology/N19-2024
  year: '2019'
N19-2025:
  abstract: "Named Entity Recognition (NER) has been mostly studied in the context\
    \ of written text. Specifically, NER is an important step in de-identification\
    \ (de-ID) of medical records, many of which are recorded conversations between\
    \ a patient and a doctor. In such recordings, audio spans with personal information\
    \ should be redacted, similar to the redaction of sensitive character spans in\
    \ de-ID for written text. The application of NER in the context of audio de-identification\
    \ has yet to be fully investigated. To this end, we define the task of audio de-ID,\
    \ in which audio spans with entity mentions should be detected. We then present\
    \ our pipeline for this task, which involves Automatic Speech Recognition (ASR),\
    \ NER on the transcript text, and text-to-audio alignment. Finally, we introduce\
    \ a novel metric for audio de-ID and a new evaluation benchmark consisting of\
    \ a large labeled segment of the Switchboard and Fisher audio datasets and detail\
    \ our pipeline\u2019s results on it."
  address: Minneapolis, Minnesota
  author:
  - first: Ido
    full: Ido Cohn
    id: ido-cohn
    last: Cohn
  - first: Itay
    full: Itay Laish
    id: itay-laish
    last: Laish
  - first: Genady
    full: Genady Beryozkin
    id: genady-beryozkin
    last: Beryozkin
  - first: Gang
    full: Gang Li
    id: gang-li
    last: Li
  - first: Izhak
    full: Izhak Shafran
    id: izhak-shafran
    last: Shafran
  - first: Idan
    full: Idan Szpektor
    id: idan-szpektor
    last: Szpektor
  - first: Tzvika
    full: Tzvika Hartman
    id: tzvika-hartman
    last: Hartman
  - first: Avinatan
    full: Avinatan Hassidim
    id: avinatan-hassidim
    last: Hassidim
  - first: Yossi
    full: Yossi Matias
    id: yossi-matias
    last: Matias
  author_string: Ido Cohn, Itay Laish, Genady Beryozkin, Gang Li, Izhak Shafran, Idan
    Szpektor, Tzvika Hartman, Avinatan Hassidim, Yossi Matias
  bibkey: cohn-etal-2019-audio
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2025
  month: June
  page_first: '197'
  page_last: '204'
  pages: "197\u2013204"
  paper_id: '25'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2025.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2025.jpg
  title: Audio De-identification - a New Entity Recognition Task
  title_html: Audio De-identification - a New Entity Recognition Task
  url: https://www.aclweb.org/anthology/N19-2025
  year: '2019'
N19-2026:
  abstract: Neural text-to-speech synthesis (NTTS) models have shown significant progress
    in generating high-quality speech, however they require a large quantity of training
    data. This makes creating models for multiple styles expensive and time-consuming.
    In this paper different styles of speech are analysed based on prosodic variations,
    from this a model is proposed to synthesise speech in the style of a newscaster,
    with just a few hours of supplementary data. We pose the problem of synthesising
    in a target style using limited data as that of creating a bi-style model that
    can synthesise both neutral-style and newscaster-style speech via a one-hot vector
    which factorises the two styles. We also propose conditioning the model on contextual
    word embeddings, and extensively evaluate it against neutral NTTS, and neutral
    concatenative-based synthesis. This model closes the gap in perceived style-appropriateness
    between natural recordings for newscaster-style of speech, and neutral speech
    synthesis by approximately two-thirds.
  address: Minneapolis, Minnesota
  author:
  - first: Nishant
    full: Nishant Prateek
    id: nishant-prateek
    last: Prateek
  - first: Mateusz
    full: "Mateusz \u0141ajszczak"
    id: mateusz-lajszczak
    last: "\u0141ajszczak"
  - first: Roberto
    full: Roberto Barra-Chicote
    id: roberto-barra-chicote
    last: Barra-Chicote
  - first: Thomas
    full: Thomas Drugman
    id: thomas-drugman
    last: Drugman
  - first: Jaime
    full: Jaime Lorenzo-Trueba
    id: jaime-lorenzo-trueba
    last: Lorenzo-Trueba
  - first: Thomas
    full: Thomas Merritt
    id: thomas-merritt
    last: Merritt
  - first: Srikanth
    full: Srikanth Ronanki
    id: srikanth-ronanki
    last: Ronanki
  - first: Trevor
    full: Trevor Wood
    id: trevor-wood
    last: Wood
  author_string: "Nishant Prateek, Mateusz \u0141ajszczak, Roberto Barra-Chicote,\
    \ Thomas Drugman, Jaime Lorenzo-Trueba, Thomas Merritt, Srikanth Ronanki, Trevor\
    \ Wood"
  bibkey: prateek-etal-2019-news
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2026
  month: June
  page_first: '205'
  page_last: '213'
  pages: "205\u2013213"
  paper_id: '26'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2026.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2026.jpg
  title: 'In Other News: a Bi-style Text-to-speech Model for Synthesizing Newscaster
    Voice with Limited Data'
  title_html: 'In Other News: a Bi-style Text-to-speech Model for Synthesizing Newscaster
    Voice with Limited Data'
  url: https://www.aclweb.org/anthology/N19-2026
  year: '2019'
N19-2027:
  abstract: "Neural approaches to Natural Language Generation (NLG) have been promising\
    \ for goal-oriented dialogue. One of the challenges of productionizing these approaches,\
    \ however, is the ability to control response quality, and ensure that generated\
    \ responses are acceptable. We propose the use of a generate, filter, and rank\
    \ framework, in which candidate responses are first filtered to eliminate unacceptable\
    \ responses, and then ranked to select the best response. While acceptability\
    \ includes grammatical correctness and semantic correctness, we focus only on\
    \ grammaticality classification in this paper, and show that existing datasets\
    \ for grammatical error correction don\u2019t correctly capture the distribution\
    \ of errors that data-driven generators are likely to make. We release a grammatical\
    \ classification and semantic correctness classification dataset for the weather\
    \ domain that consists of responses generated by 3 data-driven NLG systems. We\
    \ then explore two supervised learning approaches (CNNs and GBDTs) for classifying\
    \ grammaticality. Our experiments show that grammaticality classification is very\
    \ sensitive to the distribution of errors in the data, and that these distributions\
    \ vary significantly with both the source of the response as well as the domain.\
    \ We show that it\u2019s possible to achieve high precision with reasonable recall\
    \ on our dataset."
  address: Minneapolis, Minnesota
  author:
  - first: Ashwini
    full: Ashwini Challa
    id: ashwini-challa
    last: Challa
  - first: Kartikeya
    full: Kartikeya Upasani
    id: kartikeya-upasani
    last: Upasani
  - first: Anusha
    full: Anusha Balakrishnan
    id: anusha-balakrishnan
    last: Balakrishnan
  - first: Rajen
    full: Rajen Subba
    id: rajen-subba
    last: Subba
  author_string: Ashwini Challa, Kartikeya Upasani, Anusha Balakrishnan, Rajen Subba
  bibkey: challa-etal-2019-generate
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2027
  month: June
  page_first: '214'
  page_last: '225'
  pages: "214\u2013225"
  paper_id: '27'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2027.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2027.jpg
  title: 'Generate, Filter, and Rank: Grammaticality Classification for Production-Ready
    NLG Systems'
  title_html: 'Generate, Filter, and Rank: Grammaticality Classification for Production-Ready
    <span class="acl-fixed-case">NLG</span> Systems'
  url: https://www.aclweb.org/anthology/N19-2027
  year: '2019'
N19-2028:
  abstract: The article dwell time (i.e., expected time that users spend on an article)
    is among the most important factors showing the article engagement. It is of great
    interest to predict the dwell time of an article before its release. This allows
    digital newspapers to make informed decisions and publish more engaging articles.
    In this paper, we propose a novel content-based approach based on a deep neural
    network architecture for predicting article dwell times. The proposed model extracts
    emotion, event and entity features from an article, learns interactions among
    them, and combines the interactions with the word-based features of the article
    to learn a model for predicting the dwell time. The experimental results on a
    real dataset from a major newspaper show that the proposed model outperforms other
    state-of-the-art baselines.
  address: Minneapolis, Minnesota
  author:
  - first: Heidar
    full: Heidar Davoudi
    id: heidar-davoudi
    last: Davoudi
  - first: Aijun
    full: Aijun An
    id: aijun-an
    last: An
  - first: Gordon
    full: Gordon Edall
    id: gordon-edall
    last: Edall
  author_string: Heidar Davoudi, Aijun An, Gordon Edall
  bibkey: davoudi-etal-2019-content
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Industry Papers)'
  doi: 10.18653/v1/N19-2028
  month: June
  page_first: '226'
  page_last: '233'
  pages: "226\u2013233"
  paper_id: '28'
  parent_volume_id: N19-2
  pdf: https://www.aclweb.org/anthology/N19-2028.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-2028.jpg
  title: Content-based Dwell Time Engagement Prediction Model for News Articles
  title_html: Content-based Dwell Time Engagement Prediction Model for News Articles
  url: https://www.aclweb.org/anthology/N19-2028
  year: '2019'
N19-3000:
  address: Minneapolis, Minnesota
  author:
  - first: Sudipta
    full: Sudipta Kar
    id: sudipta-kar
    last: Kar
  - first: Farah
    full: Farah Nadeem
    id: farah-nadeem
    last: Nadeem
  - first: Laura
    full: Laura Burdick
    id: laura-burdick
    last: Burdick
  - first: Greg
    full: Greg Durrett
    id: greg-durrett
    last: Durrett
  - first: Na-Rae
    full: Na-Rae Han
    id: na-rae-han
    last: Han
  author_string: Sudipta Kar, Farah Nadeem, Laura Burdick, Greg Durrett, Na-Rae Han
  bibkey: naacl-2019-2019-north-american
  bibtype: proceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  month: June
  paper_id: '0'
  parent_volume_id: N19-3
  pdf: https://www.aclweb.org/anthology/N19-3000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-3000.jpg
  title: 'Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics: Student Research Workshop'
  title_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  url: https://www.aclweb.org/anthology/N19-3000
  year: '2019'
N19-3001:
  abstract: It has become commonplace for people to share their opinions about all
    kinds of products by posting reviews online. It has also become commonplace for
    potential customers to do research about the quality and limitations of these
    products by posting questions online. We test the extent to which reviews are
    useful in question-answering by combining two Amazon datasets and focusing our
    attention on yes/no questions. A manual analysis of 400 cases reveals that the
    reviews directly contain the answer to the question just over a third of the time.
    Preliminary reading comprehension experiments with this dataset prove inconclusive,
    with accuracy in the range 50-66%.
  address: Minneapolis, Minnesota
  author:
  - first: Daria
    full: Daria Dzendzik
    id: daria-dzendzik
    last: Dzendzik
  - first: Carl
    full: Carl Vogel
    id: carl-vogel
    last: Vogel
  - first: Jennifer
    full: Jennifer Foster
    id: jennifer-foster
    last: Foster
  author_string: Daria Dzendzik, Carl Vogel, Jennifer Foster
  bibkey: dzendzik-etal-2019-dish
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N19-3001
  month: June
  page_first: '1'
  page_last: '6'
  pages: "1\u20136"
  paper_id: '1'
  parent_volume_id: N19-3
  pdf: https://www.aclweb.org/anthology/N19-3001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-3001.jpg
  title: "Is It Dish Washer Safe? Automatically Answering \u201CYes/No\u201D Questions\
    \ Using Customer Reviews"
  title_html: "Is It Dish Washer Safe? Automatically Answering \u201CYes/No\u201D\
    \ Questions Using Customer Reviews"
  url: https://www.aclweb.org/anthology/N19-3001
  year: '2019'
N19-3002:
  abstract: "Many text corpora exhibit socially problematic biases, which can be propagated\
    \ or amplified in the models trained on such data. For example, doctor cooccurs\
    \ more frequently with male pronouns than female pronouns. In this study we (i)\
    \ propose a metric to measure gender bias; (ii) measure bias in a text corpus\
    \ and the text generated from a recurrent neural network language model trained\
    \ on the text corpus; (iii) propose a regularization loss term for the language\
    \ model that minimizes the projection of encoder-trained embeddings onto an embedding\
    \ subspace that encodes gender; (iv) finally, evaluate efficacy of our proposed\
    \ method on reducing gender bias. We find this regularization method to be effective\
    \ in reducing gender bias up to an optimal weight assigned to the loss term, beyond\
    \ which the model becomes unstable as the perplexity increases. We replicate this\
    \ study on three training corpora\u2014Penn Treebank, WikiText-2, and CNN/Daily\
    \ Mail\u2014resulting in similar conclusions."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-3002.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-3002.Presentation.pdf
  - filename: N19-3002.Note.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N19-3002.Note.pdf
  - filename: https://vimeo.com/347400639
    type: video
    url: https://vimeo.com/347400639
  author:
  - first: Shikha
    full: Shikha Bordia
    id: shikha-bordia
    last: Bordia
  - first: Samuel R.
    full: Samuel R. Bowman
    id: samuel-bowman
    last: Bowman
  author_string: Shikha Bordia, Samuel R. Bowman
  bibkey: bordia-bowman-2019-identifying
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N19-3002
  month: June
  page_first: '7'
  page_last: '15'
  pages: "7\u201315"
  paper_id: '2'
  parent_volume_id: N19-3
  pdf: https://www.aclweb.org/anthology/N19-3002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-3002.jpg
  title: Identifying and Reducing Gender Bias in Word-Level Language Models
  title_html: Identifying and Reducing Gender Bias in Word-Level Language Models
  url: https://www.aclweb.org/anthology/N19-3002
  year: '2019'
N19-3003:
  abstract: It has been established that the performance of speech recognition systems
    depends on multiple factors including the lexical content, speaker identity and
    dialect. Here we use three English datasets of acted emotion to demonstrate that
    emotional content also impacts the performance of commercial systems. On two of
    the corpora, emotion is a bigger contributor to recognition errors than speaker
    identity and on two, neutral speech is recognized considerably better than emotional
    speech. We further evaluate the commercial systems on spontaneous interactions
    that contain portions of emotional speech. We propose and validate on the acted
    datasets, a method that allows us to evaluate the overall impact of emotion on
    recognition even when manual transcripts are not available. Using this method,
    we show that emotion in natural spontaneous dialogue is a less prominent but still
    significant factor in recognition accuracy.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/355749887
    type: video
    url: https://vimeo.com/355749887
  author:
  - first: Rushab
    full: Rushab Munot
    id: rushab-munot
    last: Munot
  - first: Ani
    full: Ani Nenkova
    id: ani-nenkova
    last: Nenkova
  author_string: Rushab Munot, Ani Nenkova
  bibkey: munot-nenkova-2019-emotion
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N19-3003
  month: June
  page_first: '16'
  page_last: '21'
  pages: "16\u201321"
  paper_id: '3'
  parent_volume_id: N19-3
  pdf: https://www.aclweb.org/anthology/N19-3003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-3003.jpg
  title: Emotion Impacts Speech Recognition Performance
  title_html: Emotion Impacts Speech Recognition Performance
  url: https://www.aclweb.org/anthology/N19-3003
  year: '2019'
N19-3004:
  abstract: "When developing topic classifiers for real-world applications, we begin\
    \ by defining a set of meaningful topic labels. Ideally, an intelligent classifier\
    \ can understand these labels right away and start classifying documents. Indeed,\
    \ a human can confidently tell if an article is about science, politics, sports,\
    \ or none of the above, after knowing just the class labels. We study the problem\
    \ of training an initial topic classifier using only class labels. We investigate\
    \ existing techniques for solving this problem and propose a simple but effective\
    \ approach. Experiments on a variety of topic classification data sets show that\
    \ learning from class labels can save significant initial labeling effort, essentially\
    \ providing a \u201Dfree\u201D warm start to the topic classifier."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-3004.Presentation.pptx
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-3004.Presentation.pptx
  - filename: https://vimeo.com/353479338
    type: video
    url: https://vimeo.com/353479338
  author:
  - first: Jiatong
    full: Jiatong Li
    id: jiatong-li
    last: Li
  - first: Kai
    full: Kai Zheng
    id: kai-zheng
    last: Zheng
  - first: Hua
    full: Hua Xu
    id: hua-xu
    last: Xu
  - first: Qiaozhu
    full: Qiaozhu Mei
    id: qiaozhu-mei
    last: Mei
  - first: Yue
    full: Yue Wang
    id: yue-wang
    last: Wang
  author_string: Jiatong Li, Kai Zheng, Hua Xu, Qiaozhu Mei, Yue Wang
  bibkey: li-etal-2019-strength
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N19-3004
  month: June
  page_first: '22'
  page_last: '28'
  pages: "22\u201328"
  paper_id: '4'
  parent_volume_id: N19-3
  pdf: https://www.aclweb.org/anthology/N19-3004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-3004.jpg
  title: 'The Strength of the Weakest Supervision: Topic Classification Using Class
    Labels'
  title_html: 'The Strength of the Weakest Supervision: Topic Classification Using
    Class Labels'
  url: https://www.aclweb.org/anthology/N19-3004
  year: '2019'
N19-3005:
  abstract: In this paper, we address the problem of effectively self-training neural
    networks in a low-resource setting. Self-training is frequently used to automatically
    increase the amount of training data. However, in a low-resource scenario, it
    is less effective due to unreliable annotations created using self-labeling of
    unlabeled data. We propose to combine self-training with noise handling on the
    self-labeled data. Directly estimating noise on the combined clean training set
    and self-labeled data can lead to corruption of the clean data and hence, performs
    worse. Thus, we propose the Clean and Noisy Label Neural Network which trains
    on clean and noisy self-labeled data simultaneously by explicitly modelling clean
    and noisy labels separately. In our experiments on Chunking and NER, this approach
    performs more robustly than the baselines. Complementary to this explicit approach,
    noise can also be handled implicitly with the help of an auxiliary learning task.
    To such a complementary approach, our method is more beneficial than other baseline
    methods and together provides the best performance overall.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/360579467
    type: video
    url: https://vimeo.com/360579467
  author:
  - first: Debjit
    full: Debjit Paul
    id: debjit-paul
    last: Paul
  - first: Mittul
    full: Mittul Singh
    id: mittul-singh
    last: Singh
  - first: Michael A.
    full: Michael A. Hedderich
    id: michael-a-hedderich
    last: Hedderich
  - first: Dietrich
    full: Dietrich Klakow
    id: dietrich-klakow
    last: Klakow
  author_string: Debjit Paul, Mittul Singh, Michael A. Hedderich, Dietrich Klakow
  bibkey: paul-etal-2019-handling
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N19-3005
  month: June
  page_first: '29'
  page_last: '34'
  pages: "29\u201334"
  paper_id: '5'
  parent_volume_id: N19-3
  pdf: https://www.aclweb.org/anthology/N19-3005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-3005.jpg
  title: Handling Noisy Labels for Robustly Learning from Self-Training Data for Low-Resource
    Sequence Labeling
  title_html: Handling Noisy Labels for Robustly Learning from Self-Training Data
    for Low-Resource Sequence Labeling
  url: https://www.aclweb.org/anthology/N19-3005
  year: '2019'
N19-3006:
  abstract: Detecting opinion expression is a potential and essential task in opinion
    mining that can be extended to advanced tasks. In this paper, we considered opinion
    expression detection as a sequence labeling task and exploited different deep
    contextualized embedders into the state-of-the-art architecture, composed of bidirectional
    long short-term memory (BiLSTM) and conditional random field (CRF). Our experimental
    results show that using different word embeddings can cause contrasting results,
    and the model can achieve remarkable scores with deep contextualized embeddings.
    Especially, using BERT embedder can significantly exceed using ELMo embedder.
  address: Minneapolis, Minnesota
  author:
  - first: Wen-Bin
    full: Wen-Bin Han
    id: wen-bin-han
    last: Han
  - first: Noriko
    full: Noriko Kando
    id: noriko-kando
    last: Kando
  author_string: Wen-Bin Han, Noriko Kando
  bibkey: han-kando-2019-opinion
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N19-3006
  month: June
  page_first: '35'
  page_last: '42'
  pages: "35\u201342"
  paper_id: '6'
  parent_volume_id: N19-3
  pdf: https://www.aclweb.org/anthology/N19-3006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-3006.jpg
  title: Opinion Mining with Deep Contextualized Embeddings
  title_html: Opinion Mining with Deep Contextualized Embeddings
  url: https://www.aclweb.org/anthology/N19-3006
  year: '2019'
N19-3007:
  abstract: This paper focuses on a traditional relation extraction task in the context
    of limited annotated data and a narrow knowledge domain. We explore this task
    with a clinical corpus consisting of 200 breast cancer follow-up treatment letters
    in which 16 distinct types of relations are annotated. We experiment with an approach
    to extracting typed relations called window-bounded co-occurrence (WBC), which
    uses an adjustable context window around entity mentions of a relevant type, and
    compare its performance with a more typical intra-sentential co-occurrence baseline.
    We further introduce a new bag-of-concepts (BoC) approach to feature engineering
    based on the state-of-the-art word embeddings and word synonyms. We demonstrate
    the competitiveness of BoC by comparing with methods of higher complexity, and
    explore its effectiveness on this small dataset.
  address: Minneapolis, Minnesota
  author:
  - first: Jiyu
    full: Jiyu Chen
    id: jiyu-chen
    last: Chen
  - first: Karin
    full: Karin Verspoor
    id: karin-verspoor
    last: Verspoor
  - first: Zenan
    full: Zenan Zhai
    id: zenan-zhai
    last: Zhai
  author_string: Jiyu Chen, Karin Verspoor, Zenan Zhai
  bibkey: chen-etal-2019-bag
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N19-3007
  month: June
  page_first: '43'
  page_last: '52'
  pages: "43\u201352"
  paper_id: '7'
  parent_volume_id: N19-3
  pdf: https://www.aclweb.org/anthology/N19-3007.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-3007.jpg
  title: A Bag-of-concepts Model Improves Relation Extraction in a Narrow Knowledge
    Domain with Limited Data
  title_html: A Bag-of-concepts Model Improves Relation Extraction in a Narrow Knowledge
    Domain with Limited Data
  url: https://www.aclweb.org/anthology/N19-3007
  year: '2019'
N19-3008:
  abstract: GANs have been shown to perform exceedingly well on tasks pertaining to
    image generation and style transfer. In the field of language modelling, word
    embeddings such as GLoVe and word2vec are state-of-the-art methods for applying
    neural network models on textual data. Attempts have been made to utilize GANs
    with word embeddings for text generation. This study presents an approach to text
    generation using Skip-Thought sentence embeddings with GANs based on gradient
    penalty functions and f-measures. The proposed architecture aims to reproduce
    writing style in the generated text by modelling the way of expression at a sentence
    level across all the works of an author. Extensive experiments were run in different
    embedding settings on a variety of tasks including conditional text generation
    and language generation. The model outperforms baseline text generation networks
    across several automated evaluation metrics like BLEU-n, METEOR and ROUGE. Further,
    wide applicability and effectiveness in real life tasks are demonstrated through
    human judgement scores.
  address: Minneapolis, Minnesota
  author:
  - first: Afroz
    full: Afroz Ahamad
    id: afroz-ahamad
    last: Ahamad
  author_string: Afroz Ahamad
  bibkey: ahamad-2019-generating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N19-3008
  month: June
  page_first: '53'
  page_last: '60'
  pages: "53\u201360"
  paper_id: '8'
  parent_volume_id: N19-3
  pdf: https://www.aclweb.org/anthology/N19-3008.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-3008.jpg
  title: Generating Text through Adversarial Training Using Skip-Thought Vectors
  title_html: Generating Text through Adversarial Training Using Skip-Thought Vectors
  url: https://www.aclweb.org/anthology/N19-3008
  year: '2019'
N19-3009:
  abstract: This paper presents a new approach to generating English text from Abstract
    Meaning Representation (AMR). In contrast to the neural and statistical MT approaches
    used in other AMR generation systems, this one is largely rule-based, supplemented
    only by a language model and simple statistical linearization models, allowing
    for more control over the output. We also address the difficulties of automatically
    evaluating AMR generation systems and the problems with BLEU for this task. We
    compare automatic metrics to human evaluations and show that while METEOR and
    TER arguably reflect human judgments better than BLEU, further research into suitable
    evaluation metrics is needed.
  address: Minneapolis, Minnesota
  author:
  - first: Emma
    full: Emma Manning
    id: emma-manning
    last: Manning
  author_string: Emma Manning
  bibkey: manning-2019-partially
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N19-3009
  month: June
  page_first: '61'
  page_last: '70'
  pages: "61\u201370"
  paper_id: '9'
  parent_volume_id: N19-3
  pdf: https://www.aclweb.org/anthology/N19-3009.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-3009.jpg
  title: A Partially Rule-Based Approach to AMR Generation
  title_html: A Partially Rule-Based Approach to <span class="acl-fixed-case">AMR</span>
    Generation
  url: https://www.aclweb.org/anthology/N19-3009
  year: '2019'
N19-3010:
  abstract: Semantics and pragmatics are two complimentary and intertwined aspects
    of meaning in language. The former is concerned with the literal (context-free)
    meaning of words and sentences, the latter focuses on the intended meaning, one
    that is context-dependent. While NLP research has focused in the past mostly on
    semantics, the goal of this thesis is to develop computational models that leverage
    this pragmatic knowledge in language that is crucial to performing many NLP tasks
    correctly. In this proposal, we begin by reviewing the current progress in this
    thesis, namely, on the tasks of definiteness prediction and adverbial presupposition
    triggering. Then we discuss the proposed research for the remainder of the thesis
    which builds on this progress towards the goal of building better and more pragmatically-aware
    natural language generation and understanding systems.
  address: Minneapolis, Minnesota
  author:
  - first: Jad
    full: Jad Kabbara
    id: jad-kabbara
    last: Kabbara
  author_string: Jad Kabbara
  bibkey: kabbara-2019-computational
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N19-3010
  month: June
  page_first: '71'
  page_last: '76'
  pages: "71\u201376"
  paper_id: '10'
  parent_volume_id: N19-3
  pdf: https://www.aclweb.org/anthology/N19-3010.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-3010.jpg
  title: Computational Investigations of Pragmatic Effects in Natural Language
  title_html: Computational Investigations of Pragmatic Effects in Natural Language
  url: https://www.aclweb.org/anthology/N19-3010
  year: '2019'
N19-3011:
  abstract: 'Event Detection has been one of the research areas in Text Mining that
    has attracted attention during this decade due to the widespread availability
    of social media data specifically twitter data. Twitter has become a major source
    for information about real-world events because of the use of hashtags and the
    small word limit of Twitter that ensures concise presentation of events. Previous
    works on event detection from tweets are either applicable to detect localized
    events or breaking news only or miss out on many important events. This paper
    presents the problems associated with event detection from tweets and a tweet-segmentation
    based system for event detection called SEDTWik, an extension to a previous work,
    that is able to detect newsworthy events occurring at different locations of the
    world from a wide range of categories. The main idea is to split each tweet and
    hash-tag into segments, extract bursty segments, cluster them, and summarize them.
    We evaluated our results on the well-known Events2012 corpus and achieved state-of-the-art
    results. Keywords: Event detection, Twitter, Social Media, Microblogging, Tweet
    segmentation, Text Mining, Wikipedia, Hashtag.'
  address: Minneapolis, Minnesota
  author:
  - first: Keval
    full: Keval Morabia
    id: keval-morabia
    last: Morabia
  - first: Neti Lalita
    full: Neti Lalita Bhanu Murthy
    id: neti-lalita-bhanu-murthy
    last: Bhanu Murthy
  - first: Aruna
    full: Aruna Malapati
    id: aruna-malapati
    last: Malapati
  - first: Surender
    full: Surender Samant
    id: surender-samant
    last: Samant
  author_string: Keval Morabia, Neti Lalita Bhanu Murthy, Aruna Malapati, Surender
    Samant
  bibkey: morabia-etal-2019-sedtwik
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N19-3011
  month: June
  page_first: '77'
  page_last: '85'
  pages: "77\u201385"
  paper_id: '11'
  parent_volume_id: N19-3
  pdf: https://www.aclweb.org/anthology/N19-3011.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-3011.jpg
  title: 'SEDTWik: Segmentation-based Event Detection from Tweets Using Wikipedia'
  title_html: '<span class="acl-fixed-case">SEDTW</span>ik: Segmentation-based Event
    Detection from Tweets Using <span class="acl-fixed-case">W</span>ikipedia'
  url: https://www.aclweb.org/anthology/N19-3011
  year: '2019'
N19-3012:
  abstract: Multimodal machine translation is an attractive application of neural
    machine translation (NMT). It helps computers to deeply understand visual objects
    and their relations with natural languages. However, multimodal NMT systems suffer
    from a shortage of available training data, resulting in poor performance for
    translating rare words. In NMT, pretrained word embeddings have been shown to
    improve NMT of low-resource domains, and a search-based approach is proposed to
    address the rare word problem. In this study, we effectively combine these two
    approaches in the context of multimodal NMT and explore how we can take full advantage
    of pretrained word embeddings to better translate rare words. We report overall
    performance improvements of 1.24 METEOR and 2.49 BLEU and achieve an improvement
    of 7.67 F-score for rare word translation.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-3012.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-3012.Presentation.pdf
  - filename: https://vimeo.com/355800547
    type: video
    url: https://vimeo.com/355800547
  author:
  - first: Tosho
    full: Tosho Hirasawa
    id: tosho-hirasawa
    last: Hirasawa
  - first: Hayahide
    full: Hayahide Yamagishi
    id: hayahide-yamagishi
    last: Yamagishi
  - first: Yukio
    full: Yukio Matsumura
    id: yukio-matsumura
    last: Matsumura
  - first: Mamoru
    full: Mamoru Komachi
    id: mamoru-komachi
    last: Komachi
  author_string: Tosho Hirasawa, Hayahide Yamagishi, Yukio Matsumura, Mamoru Komachi
  bibkey: hirasawa-etal-2019-multimodal
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N19-3012
  month: June
  page_first: '86'
  page_last: '91'
  pages: "86\u201391"
  paper_id: '12'
  parent_volume_id: N19-3
  pdf: https://www.aclweb.org/anthology/N19-3012.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-3012.jpg
  title: Multimodal Machine Translation with Embedding Prediction
  title_html: Multimodal Machine Translation with Embedding Prediction
  url: https://www.aclweb.org/anthology/N19-3012
  year: '2019'
N19-3013:
  abstract: Automated extraction methods are widely available for vowels, but automated
    methods for coding rhoticity have lagged far behind. R-fulness versus r-lessness
    (in words like park, store, etc.) is a classic and frequently cited variable,
    but it is still commonly coded by human analysts rather than automated methods.
    Human-coding requires extensive resources and lacks replicability, making it difficult
    to compare large datasets across research groups. Can reliable automated methods
    be developed to aid in coding rhoticity? In this study, we use Neural Networks/Deep
    Learning, training our model on 208 Boston-area speakers.
  address: Minneapolis, Minnesota
  author:
  - first: Sarah
    full: Sarah Gupta
    id: sarah-gupta
    last: Gupta
  - first: Anthony
    full: Anthony DiPadova
    id: anthony-dipadova
    last: DiPadova
  author_string: Sarah Gupta, Anthony DiPadova
  bibkey: gupta-dipadova-2019-deep
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N19-3013
  month: June
  page_first: '92'
  page_last: '96'
  pages: "92\u201396"
  paper_id: '13'
  parent_volume_id: N19-3
  pdf: https://www.aclweb.org/anthology/N19-3013.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-3013.jpg
  title: 'Deep Learning and Sociophonetics: Automatic Coding of Rhoticity Using Neural
    Networks'
  title_html: 'Deep Learning and Sociophonetics: Automatic Coding of Rhoticity Using
    Neural Networks'
  url: https://www.aclweb.org/anthology/N19-3013
  year: '2019'
N19-3014:
  abstract: "One of the main challenges in Spoken Language Understanding (SLU) is\
    \ dealing with \u2018open-vocabulary\u2019 slots. Recently, SLU models based on\
    \ neural network were proposed, but it is still difficult to recognize the slots\
    \ of unknown words or \u2018open-vocabulary\u2019 slots because of the high cost\
    \ of creating a manually tagged SLU dataset. This paper proposes data noising,\
    \ which reflects the characteristics of the \u2018open-vocabulary\u2019 slots,\
    \ for data augmentation. We applied it to an attention based bi-directional recurrent\
    \ neural network (Liu and Lane, 2016) and experimented with three datasets: Airline\
    \ Travel Information System (ATIS), Snips, and MIT-Restaurant. We achieved performance\
    \ improvements of up to 0.57% and 3.25 in intent prediction (accuracy) and slot\
    \ filling (f1-score), respectively. Our method is advantageous because it does\
    \ not require additional memory and it can be applied simultaneously with the\
    \ training process of the model."
  address: Minneapolis, Minnesota
  author:
  - first: Hwa-Yeon
    full: Hwa-Yeon Kim
    id: hwa-yeon-kim
    last: Kim
  - first: Yoon-Hyung
    full: Yoon-Hyung Roh
    id: yoon-hyung-roh
    last: Roh
  - first: Young-Kil
    full: Young-Kil Kim
    id: young-gil-kim
    last: Kim
  author_string: Hwa-Yeon Kim, Yoon-Hyung Roh, Young-Kil Kim
  bibkey: kim-etal-2019-data
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N19-3014
  month: June
  page_first: '97'
  page_last: '102'
  pages: "97\u2013102"
  paper_id: '14'
  parent_volume_id: N19-3
  pdf: https://www.aclweb.org/anthology/N19-3014.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-3014.jpg
  title: Data Augmentation by Data Noising for Open-vocabulary Slots in Spoken Language
    Understanding
  title_html: Data Augmentation by Data Noising for Open-vocabulary Slots in Spoken
    Language Understanding
  url: https://www.aclweb.org/anthology/N19-3014
  year: '2019'
N19-3015:
  abstract: This study examines the role of three influential theories of language
    processing, viz., Surprisal Theory, Uniform Information Density (UID) hypothesis
    and Dependency Locality Theory (DLT), in predicting disfluencies in speech production.
    To this end, we incorporate features based on lexical surprisal, word duration
    and DLT integration and storage costs into logistic regression classifiers aimed
    to predict disfluencies in the Switchboard corpus of English conversational speech.
    We find that disfluencies occur in the face of upcoming difficulties and speakers
    tend to handle this by lessening cognitive load before disfluencies occur. Further,
    we see that reparandums behave differently from disfluent fillers possibly due
    to the lessening of the cognitive load also happening in the word choice of the
    reparandum, i.e., in the disfluency itself. While the UID hypothesis does not
    seem to play a significant role in disfluency prediction, lexical surprisal and
    DLT costs do give promising results in explaining language production. Further,
    we also find that as a means to lessen cognitive load for upcoming difficulties
    speakers take more time on words preceding disfluencies, making duration a key
    element in understanding disfluencies.
  address: Minneapolis, Minnesota
  author:
  - first: Samvit
    full: Samvit Dammalapati
    id: samvit-dammalapati
    last: Dammalapati
  - first: Rajakrishnan
    full: Rajakrishnan Rajkumar
    id: rajakrishnan-rajkumar
    last: Rajkumar
  - first: Sumeet
    full: Sumeet Agarwal
    id: sumeet-agarwal
    last: Agarwal
  author_string: Samvit Dammalapati, Rajakrishnan Rajkumar, Sumeet Agarwal
  bibkey: dammalapati-etal-2019-expectation
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N19-3015
  month: June
  page_first: '103'
  page_last: '109'
  pages: "103\u2013109"
  paper_id: '15'
  parent_volume_id: N19-3
  pdf: https://www.aclweb.org/anthology/N19-3015.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-3015.jpg
  title: Expectation and Locality Effects in the Prediction of Disfluent Fillers and
    Repairs in English Speech
  title_html: Expectation and Locality Effects in the Prediction of Disfluent Fillers
    and Repairs in <span class="acl-fixed-case">E</span>nglish Speech
  url: https://www.aclweb.org/anthology/N19-3015
  year: '2019'
N19-3016:
  abstract: In this paper we study how different ways of combining character and word-level
    representations affect the quality of both final word and sentence representations.
    We provide strong empirical evidence that modeling characters improves the learned
    representations at the word and sentence levels, and that doing so is particularly
    useful when representing less frequent words. We further show that a feature-wise
    sigmoid gating mechanism is a robust method for creating representations that
    encode semantic similarity, as it performed reasonably well in several word similarity
    datasets. Finally, our findings suggest that properly capturing semantic similarity
    at the word level does not consistently yield improved performance in downstream
    sentence-level tasks.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-3016.Poster.pdf
    type: poster
    url: https://www.aclweb.org/anthology/attachments/N19-3016.Poster.pdf
  author:
  - first: Jorge
    full: Jorge Balazs
    id: jorge-balazs
    last: Balazs
  - first: Yutaka
    full: Yutaka Matsuo
    id: yutaka-matsuo
    last: Matsuo
  author_string: Jorge Balazs, Yutaka Matsuo
  bibkey: balazs-matsuo-2019-gating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N19-3016
  month: June
  page_first: '110'
  page_last: '124'
  pages: "110\u2013124"
  paper_id: '16'
  parent_volume_id: N19-3
  pdf: https://www.aclweb.org/anthology/N19-3016.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-3016.jpg
  title: 'Gating Mechanisms for Combining Character and Word-level Word Representations:
    an Empirical Study'
  title_html: 'Gating Mechanisms for Combining Character and Word-level Word Representations:
    an Empirical Study'
  url: https://www.aclweb.org/anthology/N19-3016
  year: '2019'
N19-3017:
  abstract: Pregroup calculus has been used for the representation of free word order
    languages (Sanskrit and Hungarian), using a construction called precyclicity.
    However, restricted word order alternation has not been handled before. This paper
    aims at introducing and formally expressing three methods of representing word
    order alternation in the pregroup representation of any language. This paper describes
    the word order alternation patterns of Hindi, and creates a basic pregroup representation
    for the language. In doing so, the shortcoming of correct reductions for ungrammatical
    sentences due to the current apparatus is highlighted, and the aforementioned
    methods are invoked for a grammatically accurate representation of restricted
    word order alternation. The replicability of these methods is explained in the
    representation of adverbs and prepositional phrases in English.
  address: Minneapolis, Minnesota
  author:
  - first: Alok
    full: Alok Debnath
    id: alok-debnath
    last: Debnath
  - first: Manish
    full: Manish Shrivastava
    id: manish-shrivastava
    last: Shrivastava
  author_string: Alok Debnath, Manish Shrivastava
  bibkey: debnath-shrivastava-2019-pregroup
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N19-3017
  month: June
  page_first: '125'
  page_last: '135'
  pages: "125\u2013135"
  paper_id: '17'
  parent_volume_id: N19-3
  pdf: https://www.aclweb.org/anthology/N19-3017.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-3017.jpg
  title: A Pregroup Representation of Word Order Alternation Using Hindi Syntax
  title_html: A Pregroup Representation of Word Order Alternation Using <span class="acl-fixed-case">H</span>indi
    Syntax
  url: https://www.aclweb.org/anthology/N19-3017
  year: '2019'
N19-3018:
  abstract: 'The #MeToo movement is an ongoing prevalent phenomenon on social media
    aiming to demonstrate the frequency and widespread of sexual harassment by providing
    a platform to speak narrate personal experiences of such harassment. The aggregation
    and analysis of such disclosures pave the way to development of technology-based
    prevention of sexual harassment. We contend that the lack of specificity in generic
    sentence classification models may not be the best way to tackle text subtleties
    that intrinsically prevail in a classification task as complex as identifying
    disclosures of sexual harassment. We propose the Disclosure Language Model, a
    three part ULMFiT architecture, consisting of a Language model, a Medium-Specific
    (Twitter) model and a Task-Specific classifier to tackle this problem and create
    a manually annotated real-world dataset to test our technique on this, to show
    that using a Discourse Language Model often yields better classification performance
    over (i) Generic deep learning based sentence classification models (ii) existing
    models that rely on handcrafted stylistic features. An extensive comparison with
    state-of-the-art generic and specific models along with a detailed error analysis
    presents the case for our proposed methodology.'
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/359711283
    type: video
    url: https://vimeo.com/359711283
  author:
  - first: Arijit
    full: Arijit Ghosh Chowdhury
    id: arijit-ghosh-chowdhury
    last: Ghosh Chowdhury
  - first: Ramit
    full: Ramit Sawhney
    id: ramit-sawhney
    last: Sawhney
  - first: Puneet
    full: Puneet Mathur
    id: puneet-mathur
    last: Mathur
  - first: Debanjan
    full: Debanjan Mahata
    id: debanjan-mahata
    last: Mahata
  - first: Rajiv
    full: Rajiv Ratn Shah
    id: rajiv-ratn-shah
    last: Ratn Shah
  author_string: Arijit Ghosh Chowdhury, Ramit Sawhney, Puneet Mathur, Debanjan Mahata,
    Rajiv Ratn Shah
  bibkey: ghosh-chowdhury-etal-2019-speak
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N19-3018
  month: June
  page_first: '136'
  page_last: '146'
  pages: "136\u2013146"
  paper_id: '18'
  parent_volume_id: N19-3
  pdf: https://www.aclweb.org/anthology/N19-3018.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-3018.jpg
  title: Speak up, Fight Back! Detection of Social Media Disclosures of Sexual Harassment
  title_html: Speak up, Fight Back! Detection of Social Media Disclosures of Sexual
    Harassment
  url: https://www.aclweb.org/anthology/N19-3018
  year: '2019'
N19-3019:
  abstract: Suicide is a leading cause of death among youth and the use of social
    media to detect suicidal ideation is an active line of research. While it has
    been established that these users share a common set of properties, the current
    state-of-the-art approaches utilize only text-based (stylistic and semantic) cues.
    We contend that the use of information from networks in the form of condensed
    social graph embeddings and author profiling using features from historical data
    can be combined with an existing set of features to improve the performance. To
    that end, we experiment on a manually annotated dataset of tweets created using
    a three-phase strategy and propose SNAP-BATNET, a deep learning based model to
    extract text-based features and a novel Feature Stacking approach to combine other
    community-based information such as historical author profiling and graph embeddings
    that outperform the current state-of-the-art. We conduct a comprehensive quantitative
    analysis with baselines, both generic and specific, that presents the case for
    SNAP-BATNET, along with an error analysis that highlights the limitations and
    challenges faced paving the way to the future of AI-based suicide ideation detection.
  address: Minneapolis, Minnesota
  attachment:
  - filename: https://vimeo.com/359731391
    type: video
    url: https://vimeo.com/359731391
  author:
  - first: Rohan
    full: Rohan Mishra
    id: rohan-mishra
    last: Mishra
  - first: Pradyumn
    full: Pradyumn Prakhar Sinha
    id: pradyumn-prakhar-sinha
    last: Prakhar Sinha
  - first: Ramit
    full: Ramit Sawhney
    id: ramit-sawhney
    last: Sawhney
  - first: Debanjan
    full: Debanjan Mahata
    id: debanjan-mahata
    last: Mahata
  - first: Puneet
    full: Puneet Mathur
    id: puneet-mathur
    last: Mathur
  - first: Rajiv
    full: Rajiv Ratn Shah
    id: rajiv-ratn-shah
    last: Ratn Shah
  author_string: Rohan Mishra, Pradyumn Prakhar Sinha, Ramit Sawhney, Debanjan Mahata,
    Puneet Mathur, Rajiv Ratn Shah
  bibkey: mishra-etal-2019-snap
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N19-3019
  month: June
  page_first: '147'
  page_last: '156'
  pages: "147\u2013156"
  paper_id: '19'
  parent_volume_id: N19-3
  pdf: https://www.aclweb.org/anthology/N19-3019.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-3019.jpg
  title: 'SNAP-BATNET: Cascading Author Profiling and Social Network Graphs for Suicide
    Ideation Detection on Social Media'
  title_html: '<span class="acl-fixed-case">SNAP</span>-<span class="acl-fixed-case">BATNET</span>:
    Cascading Author Profiling and Social Network Graphs for Suicide Ideation Detection
    on Social Media'
  url: https://www.aclweb.org/anthology/N19-3019
  year: '2019'
N19-4000:
  address: Minneapolis, Minnesota
  author:
  - first: Waleed
    full: Waleed Ammar
    id: waleed-ammar
    last: Ammar
  - first: Annie
    full: Annie Louis
    id: annie-louis
    last: Louis
  - first: Nasrin
    full: Nasrin Mostafazadeh
    id: nasrin-mostafazadeh
    last: Mostafazadeh
  author_string: Waleed Ammar, Annie Louis, Nasrin Mostafazadeh
  bibkey: naacl-2019-2019-north-american-chapter
  bibtype: proceedings
  booktitle: Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics (Demonstrations)
  month: June
  paper_id: '0'
  parent_volume_id: N19-4
  pdf: https://www.aclweb.org/anthology/N19-4000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-4000.jpg
  title: Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics (Demonstrations)
  title_html: Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics (Demonstrations)
  url: https://www.aclweb.org/anthology/N19-4000
  year: '2019'
N19-4001:
  abstract: We present Abbreviation Explorer, a system that supports interactive exploration
    of abbreviations that are challenging for Unsupervised Abbreviation Disambiguation
    (UAD). Abbreviation Explorer helps to identify long-forms that are easily confused,
    and to pinpoint likely causes such as limitations of normalization, language switching,
    or inconsistent typing. It can also support determining which long-forms would
    benefit from additional input text for unsupervised abbreviation disambiguation.
    The system provides options for creating corrective rules that merge redundant
    long-forms with identical meaning. The identified rules can be easily applied
    to the already existing vector spaces used by UAD to improve disambiguation performance,
    while also avoiding the cost of retraining.
  address: Minneapolis, Minnesota
  author:
  - first: Manuel R.
    full: Manuel R. Ciosici
    id: manuel-r-ciosici
    last: Ciosici
  - first: Ira
    full: Ira Assent
    id: ira-assent
    last: Assent
  author_string: Manuel R. Ciosici, Ira Assent
  bibkey: ciosici-assent-2019-abbreviation
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics (Demonstrations)
  booktitle_html: Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics (Demonstrations)
  doi: 10.18653/v1/N19-4001
  month: June
  page_first: '1'
  page_last: '5'
  pages: "1\u20135"
  paper_id: '1'
  parent_volume_id: N19-4
  pdf: https://www.aclweb.org/anthology/N19-4001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-4001.jpg
  title: Abbreviation Explorer - an interactive system for pre-evaluation of Unsupervised
    Abbreviation Disambiguation
  title_html: Abbreviation Explorer - an interactive system for pre-evaluation of
    Unsupervised Abbreviation Disambiguation
  url: https://www.aclweb.org/anthology/N19-4001
  year: '2019'
N19-4002:
  abstract: This demo paper describes ADIDA, a web-based system for automatic dialect
    identification for Arabic text. The system distinguishes among the dialects of
    25 Arab cities (from Rabat to Muscat) in addition to Modern Standard Arabic. The
    results are presented with either a point map or a heat map visualizing the automatic
    identification probabilities over a geographical map of the Arab World.
  address: Minneapolis, Minnesota
  author:
  - first: Ossama
    full: Ossama Obeid
    id: ossama-obeid
    last: Obeid
  - first: Mohammad
    full: Mohammad Salameh
    id: mohammad-salameh
    last: Salameh
  - first: Houda
    full: Houda Bouamor
    id: houda-bouamor
    last: Bouamor
  - first: Nizar
    full: Nizar Habash
    id: nizar-habash
    last: Habash
  author_string: Ossama Obeid, Mohammad Salameh, Houda Bouamor, Nizar Habash
  bibkey: obeid-etal-2019-adida
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics (Demonstrations)
  booktitle_html: Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics (Demonstrations)
  doi: 10.18653/v1/N19-4002
  month: June
  page_first: '6'
  page_last: '11'
  pages: "6\u201311"
  paper_id: '2'
  parent_volume_id: N19-4
  pdf: https://www.aclweb.org/anthology/N19-4002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-4002.jpg
  title: 'ADIDA: Automatic Dialect Identification for Arabic'
  title_html: '<span class="acl-fixed-case">ADIDA</span>: Automatic Dialect Identification
    for <span class="acl-fixed-case">A</span>rabic'
  url: https://www.aclweb.org/anthology/N19-4002
  year: '2019'
N19-4003:
  abstract: Many of the most pressing current research problems (e.g., public health,
    food security, or climate change) require multi-disciplinary collaborations. In
    order to facilitate this process, we propose a system that incorporates multi-domain
    extractions of causal interactions into a single searchable knowledge graph. Our
    system enables users to search iteratively over direct and indirect connections
    in this knowledge graph, and collaboratively build causal models in real time.
    To enable the aggregation of causal information from multiple languages, we extend
    an open-domain machine reader to Portuguese. The new Portuguese reader extracts
    over 600 thousand causal statements from 120 thousand Portuguese publications
    with a precision of 62%, which demonstrates the value of mining multilingual scientific
    information.
  address: Minneapolis, Minnesota
  author:
  - first: George C. G.
    full: George C. G. Barbosa
    id: george-c-g-barbosa
    last: Barbosa
  - first: Zechy
    full: Zechy Wong
    id: zechy-wong
    last: Wong
  - first: Gus
    full: Gus Hahn-Powell
    id: gus-hahn-powell
    last: Hahn-Powell
  - first: Dane
    full: Dane Bell
    id: dane-bell
    last: Bell
  - first: Rebecca
    full: Rebecca Sharp
    id: rebecca-sharp
    last: Sharp
  - first: Marco A.
    full: "Marco A. Valenzuela-Esc\xE1rcega"
    id: marco-a-valenzuela-escarcega
    last: "Valenzuela-Esc\xE1rcega"
  - first: Mihai
    full: Mihai Surdeanu
    id: mihai-surdeanu
    last: Surdeanu
  author_string: "George C. G. Barbosa, Zechy Wong, Gus Hahn-Powell, Dane Bell, Rebecca\
    \ Sharp, Marco A. Valenzuela-Esc\xE1rcega, Mihai Surdeanu"
  bibkey: barbosa-etal-2019-enabling
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics (Demonstrations)
  booktitle_html: Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics (Demonstrations)
  doi: 10.18653/v1/N19-4003
  month: June
  page_first: '12'
  page_last: '17'
  pages: "12\u201317"
  paper_id: '3'
  parent_volume_id: N19-4
  pdf: https://www.aclweb.org/anthology/N19-4003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-4003.jpg
  title: Enabling Search and Collaborative Assembly of Causal Interactions Extracted
    from Multilingual and Multi-domain Free Text
  title_html: Enabling Search and Collaborative Assembly of Causal Interactions Extracted
    from Multilingual and Multi-domain Free Text
  url: https://www.aclweb.org/anthology/N19-4003
  year: '2019'
N19-4004:
  abstract: "Nowadays, we are surrounded by more and more online news articles. Tens\
    \ or hundreds of news articles need to be read if we wish to explore a hot news\
    \ event or topic. So it is of vital importance to automatically synthesize a batch\
    \ of news articles related to the event or topic into a new synthesis article\
    \ (or overview article) for reader\u2019s convenience. It is so challenging to\
    \ make news synthesis fully automatic that there is no successful solution by\
    \ now. In this paper, we put forward a novel Interactive News Synthesis system\
    \ (i.e. INS), which can help generate news overview articles automatically or\
    \ by interacting with users. More importantly, INS can serve as a tool for editors\
    \ to help them finish their jobs. In our experiments, INS performs well on both\
    \ topic representation and synthesis article generation. A user study also demonstrates\
    \ the usefulness and users\u2019 satisfaction with the INS tool. A demo video\
    \ is available at https://youtu.be/7ItteKW3GEk."
  address: Minneapolis, Minnesota
  author:
  - first: Hui
    full: Hui Liu
    id: hui-liu
    last: Liu
  - first: Wentao
    full: Wentao Qin
    id: wentao-qin
    last: Qin
  - first: Xiaojun
    full: Xiaojun Wan
    id: xiaojun-wan
    last: Wan
  author_string: Hui Liu, Wentao Qin, Xiaojun Wan
  bibkey: liu-etal-2019-ins
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics (Demonstrations)
  booktitle_html: Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics (Demonstrations)
  doi: 10.18653/v1/N19-4004
  month: June
  page_first: '18'
  page_last: '23'
  pages: "18\u201323"
  paper_id: '4'
  parent_volume_id: N19-4
  pdf: https://www.aclweb.org/anthology/N19-4004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-4004.jpg
  title: 'INS: An Interactive Chinese News Synthesis System'
  title_html: '<span class="acl-fixed-case">INS</span>: An Interactive <span class="acl-fixed-case">C</span>hinese
    News Synthesis System'
  url: https://www.aclweb.org/anthology/N19-4004
  year: '2019'
N19-4005:
  abstract: "We present a method for learning bilingual word embeddings in order to\
    \ support second language (L2) learners in finding recurring phrases and example\
    \ sentences that match mixed-code queries (e.g., \u201C\u63A5 \u53D7 sentence\u201D\
    ) composed of words in both target language and native language (L1). In our approach,\
    \ mixed-code queries are transformed into target language queries aimed at maximizing\
    \ the probability of retrieving relevant target language phrases and sentences.\
    \ The method involves converting a given parallel corpus into mixed-code data,\
    \ generating word embeddings from mixed-code data, and expanding queries in target\
    \ languages based on bilingual word embeddings. We present a prototype search\
    \ engine, x.Linggle, that applies the method to a linguistic search engine for\
    \ a parallel corpus. Preliminary evaluation on a list of common word-translation\
    \ shows that the method performs reasonablly well."
  address: Minneapolis, Minnesota
  author:
  - first: Chia-Fang
    full: Chia-Fang Ho
    id: chia-fang-ho
    last: Ho
  - first: Jason
    full: Jason Chang
    id: jason-s-chang
    last: Chang
  - first: Jhih-Jie
    full: Jhih-Jie Chen
    id: jhih-jie-chen
    last: Chen
  - first: Chingyu
    full: Chingyu Yang
    id: chingyu-yang
    last: Yang
  author_string: Chia-Fang Ho, Jason Chang, Jhih-Jie Chen, Chingyu Yang
  bibkey: ho-etal-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics (Demonstrations)
  booktitle_html: Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics (Demonstrations)
  doi: 10.18653/v1/N19-4005
  month: June
  page_first: '24'
  page_last: '28'
  pages: "24\u201328"
  paper_id: '5'
  parent_volume_id: N19-4
  pdf: https://www.aclweb.org/anthology/N19-4005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-4005.jpg
  title: Learning to Respond to Mixed-code Queries using Bilingual Word Embeddings
  title_html: Learning to Respond to Mixed-code Queries using Bilingual Word Embeddings
  url: https://www.aclweb.org/anthology/N19-4005
  year: '2019'
N19-4006:
  abstract: Evaluating translation models is a trade-off between effort and detail.
    On the one end of the spectrum there are automatic count-based methods such as
    BLEU, on the other end linguistic evaluations by humans, which arguably are more
    informative but also require a disproportionately high effort. To narrow the spectrum,
    we propose a general approach on how to automatically expose systematic differences
    between human and machine translations to human experts. Inspired by adversarial
    settings, we train a neural text classifier to distinguish human from machine
    translations. A classifier that performs and generalizes well after training should
    recognize systematic differences between the two classes, which we uncover with
    neural explainability methods. Our proof-of-concept implementation, DiaMaT, is
    open source. Applied to a dataset translated by a state-of-the-art neural Transformer
    model, DiaMaT achieves a classification accuracy of 75% and exposes meaningful
    differences between humans and the Transformer, amidst the current discussion
    about human parity.
  address: Minneapolis, Minnesota
  author:
  - first: Robert
    full: Robert Schwarzenberg
    id: robert-schwarzenberg
    last: Schwarzenberg
  - first: David
    full: David Harbecke
    id: david-harbecke
    last: Harbecke
  - first: Vivien
    full: Vivien Macketanz
    id: vivien-macketanz
    last: Macketanz
  - first: Eleftherios
    full: Eleftherios Avramidis
    id: eleftherios-avramidis
    last: Avramidis
  - first: Sebastian
    full: "Sebastian M\xF6ller"
    id: sebastian-moller
    last: "M\xF6ller"
  author_string: "Robert Schwarzenberg, David Harbecke, Vivien Macketanz, Eleftherios\
    \ Avramidis, Sebastian M\xF6ller"
  bibkey: schwarzenberg-etal-2019-train
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics (Demonstrations)
  booktitle_html: Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics (Demonstrations)
  doi: 10.18653/v1/N19-4006
  month: June
  page_first: '29'
  page_last: '34'
  pages: "29\u201334"
  paper_id: '6'
  parent_volume_id: N19-4
  pdf: https://www.aclweb.org/anthology/N19-4006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-4006.jpg
  title: 'Train, Sort, Explain: Learning to Diagnose Translation Models'
  title_html: 'Train, Sort, Explain: Learning to Diagnose Translation Models'
  url: https://www.aclweb.org/anthology/N19-4006
  year: '2019'
N19-4007:
  abstract: 'In this paper, we describe compare-mt, a tool for holistic analysis and
    comparison of the results of systems for language generation tasks such as machine
    translation. The main goal of the tool is to give the user a high-level and coherent
    view of the salient differences between systems that can then be used to guide
    further analysis or system improvement. It implements a number of tools to do
    so, such as analysis of accuracy of generation of particular types of words, bucketed
    histograms of sentence accuracies or counts based on salient characteristics,
    and extraction of characteristic n-grams for each system. It also has a number
    of advanced features such as use of linguistic labels, source side data, or comparison
    of log likelihoods for probabilistic models, and also aims to be easily extensible
    by users to new types of analysis. compare-mt is a pure-Python open source package,
    that has already proven useful to generate analyses that have been used in our
    published papers. Demo Video: https://youtu.be/NyJEQT7t2CA'
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-4007.Supplementary.pdf
    type: supplementary
    url: https://www.aclweb.org/anthology/attachments/N19-4007.Supplementary.pdf
  author:
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  - first: Zi-Yi
    full: Zi-Yi Dou
    id: zi-yi-dou
    last: Dou
  - first: Junjie
    full: Junjie Hu
    id: junjie-hu
    last: Hu
  - first: Paul
    full: Paul Michel
    id: paul-michel
    last: Michel
  - first: Danish
    full: Danish Pruthi
    id: danish-pruthi
    last: Pruthi
  - first: Xinyi
    full: Xinyi Wang
    id: xinyi-wang
    last: Wang
  author_string: Graham Neubig, Zi-Yi Dou, Junjie Hu, Paul Michel, Danish Pruthi,
    Xinyi Wang
  bibkey: neubig-etal-2019-compare
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics (Demonstrations)
  booktitle_html: Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics (Demonstrations)
  doi: 10.18653/v1/N19-4007
  month: June
  page_first: '35'
  page_last: '41'
  pages: "35\u201341"
  paper_id: '7'
  parent_volume_id: N19-4
  pdf: https://www.aclweb.org/anthology/N19-4007.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-4007.jpg
  title: 'compare-mt: A Tool for Holistic Comparison of Language Generation Systems'
  title_html: 'compare-mt: A Tool for Holistic Comparison of Language Generation Systems'
  url: https://www.aclweb.org/anthology/N19-4007
  year: '2019'
N19-4008:
  abstract: 'Building causal models of complicated phenomena such as food insecurity
    is currently a slow and labor-intensive manual process. In this paper, we introduce
    an approach that builds executable probabilistic models from raw, free text. The
    proposed approach is implemented through three systems: Eidos, INDRA, and Delphi.
    Eidos is an open-domain machine reading system designed to extract causal relations
    from natural language. It is rule-based, allowing for rapid domain transfer, customizability,
    and interpretability. INDRA aggregates multiple sources of causal information
    and performs assembly to create a coherent knowledge base and assess its reliability.
    This assembled knowledge serves as the starting point for modeling. Delphi is
    a modeling framework that assembles quantified causal fragments and their contexts
    into executable probabilistic models that respect the semantics of the original
    text, and can be used to support decision making.'
  address: Minneapolis, Minnesota
  author:
  - first: Rebecca
    full: Rebecca Sharp
    id: rebecca-sharp
    last: Sharp
  - first: Adarsh
    full: Adarsh Pyarelal
    id: adarsh-pyarelal
    last: Pyarelal
  - first: Benjamin
    full: Benjamin Gyori
    id: benjamin-gyori
    last: Gyori
  - first: Keith
    full: Keith Alcock
    id: keith-alcock
    last: Alcock
  - first: Egoitz
    full: Egoitz Laparra
    id: egoitz-laparra
    last: Laparra
  - first: Marco A.
    full: "Marco A. Valenzuela-Esc\xE1rcega"
    id: marco-a-valenzuela-escarcega
    last: "Valenzuela-Esc\xE1rcega"
  - first: Ajay
    full: Ajay Nagesh
    id: ajay-nagesh
    last: Nagesh
  - first: Vikas
    full: Vikas Yadav
    id: vikas-yadav
    last: Yadav
  - first: John
    full: John Bachman
    id: john-bachman
    last: Bachman
  - first: Zheng
    full: Zheng Tang
    id: zheng-tang
    last: Tang
  - first: Heather
    full: Heather Lent
    id: heather-lent
    last: Lent
  - first: Fan
    full: Fan Luo
    id: fan-luo
    last: Luo
  - first: Mithun
    full: Mithun Paul
    id: mithun-paul
    last: Paul
  - first: Steven
    full: Steven Bethard
    id: steven-bethard
    last: Bethard
  - first: Kobus
    full: Kobus Barnard
    id: kobus-barnard
    last: Barnard
  - first: Clayton
    full: Clayton Morrison
    id: clayton-morrison
    last: Morrison
  - first: Mihai
    full: Mihai Surdeanu
    id: mihai-surdeanu
    last: Surdeanu
  author_string: "Rebecca Sharp, Adarsh Pyarelal, Benjamin Gyori, Keith Alcock, Egoitz\
    \ Laparra, Marco A. Valenzuela-Esc\xE1rcega, Ajay Nagesh, Vikas Yadav, John Bachman,\
    \ Zheng Tang, Heather Lent, Fan Luo, Mithun Paul, Steven Bethard, Kobus Barnard,\
    \ Clayton Morrison, Mihai Surdeanu"
  bibkey: sharp-etal-2019-eidos
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics (Demonstrations)
  booktitle_html: Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics (Demonstrations)
  doi: 10.18653/v1/N19-4008
  month: June
  page_first: '42'
  page_last: '47'
  pages: "42\u201347"
  paper_id: '8'
  parent_volume_id: N19-4
  pdf: https://www.aclweb.org/anthology/N19-4008.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-4008.jpg
  title: 'Eidos, INDRA, & Delphi: From Free Text to Executable Causal Models'
  title_html: 'Eidos, <span class="acl-fixed-case">INDRA</span>, &amp; Delphi: From
    Free Text to Executable Causal Models'
  url: https://www.aclweb.org/anthology/N19-4008
  year: '2019'
N19-4009:
  abstract: fairseq is an open-source sequence modeling toolkit that allows researchers
    and developers to train custom models for translation, summarization, language
    modeling, and other text generation tasks. The toolkit is based on PyTorch and
    supports distributed training across multiple GPUs and machines. We also support
    fast mixed-precision training and inference on modern GPUs. A demo video can be
    found at https://www.youtube.com/watch?v=OtgDdWtHvto
  address: Minneapolis, Minnesota
  author:
  - first: Myle
    full: Myle Ott
    id: myle-ott
    last: Ott
  - first: Sergey
    full: Sergey Edunov
    id: sergey-edunov
    last: Edunov
  - first: Alexei
    full: Alexei Baevski
    id: alexei-baevski
    last: Baevski
  - first: Angela
    full: Angela Fan
    id: angela-fan
    last: Fan
  - first: Sam
    full: Sam Gross
    id: sam-gross
    last: Gross
  - first: Nathan
    full: Nathan Ng
    id: nathan-ng
    last: Ng
  - first: David
    full: David Grangier
    id: david-grangier
    last: Grangier
  - first: Michael
    full: Michael Auli
    id: michael-auli
    last: Auli
  author_string: Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan
    Ng, David Grangier, Michael Auli
  bibkey: ott-etal-2019-fairseq
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics (Demonstrations)
  booktitle_html: Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics (Demonstrations)
  doi: 10.18653/v1/N19-4009
  month: June
  page_first: '48'
  page_last: '53'
  pages: "48\u201353"
  paper_id: '9'
  parent_volume_id: N19-4
  pdf: https://www.aclweb.org/anthology/N19-4009.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-4009.jpg
  title: 'fairseq: A Fast, Extensible Toolkit for Sequence Modeling'
  title_html: 'fairseq: A Fast, Extensible Toolkit for Sequence Modeling'
  url: https://www.aclweb.org/anthology/N19-4009
  year: '2019'
N19-4010:
  abstract: "We present FLAIR, an NLP framework designed to facilitate training and\
    \ distribution of state-of-the-art sequence labeling, text classification and\
    \ language models. The core idea of the framework is to present a simple, unified\
    \ interface for conceptually very different types of word and document embeddings.\
    \ This effectively hides all embedding-specific engineering complexity and allows\
    \ researchers to \u201Cmix and match\u201D various embeddings with little effort.\
    \ The framework also implements standard model training and hyperparameter selection\
    \ routines, as well as a data fetching module that can download publicly available\
    \ NLP datasets and convert them into data structures for quick set up of experiments.\
    \ Finally, FLAIR also ships with a \u201Cmodel zoo\u201D of pre-trained models\
    \ to allow researchers to use state-of-the-art NLP models in their applications.\
    \ This paper gives an overview of the framework and its functionality. The framework\
    \ is available on GitHub at https://github.com/zalandoresearch/flair ."
  address: Minneapolis, Minnesota
  author:
  - first: Alan
    full: Alan Akbik
    id: alan-akbik
    last: Akbik
  - first: Tanja
    full: Tanja Bergmann
    id: tanja-bergmann
    last: Bergmann
  - first: Duncan
    full: Duncan Blythe
    id: duncan-blythe
    last: Blythe
  - first: Kashif
    full: Kashif Rasul
    id: kashif-rasul
    last: Rasul
  - first: Stefan
    full: Stefan Schweter
    id: stefan-schweter
    last: Schweter
  - first: Roland
    full: Roland Vollgraf
    id: roland-vollgraf
    last: Vollgraf
  author_string: Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter,
    Roland Vollgraf
  bibkey: akbik-etal-2019-flair
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics (Demonstrations)
  booktitle_html: Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics (Demonstrations)
  doi: 10.18653/v1/N19-4010
  month: June
  page_first: '54'
  page_last: '59'
  pages: "54\u201359"
  paper_id: '10'
  parent_volume_id: N19-4
  pdf: https://www.aclweb.org/anthology/N19-4010.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-4010.jpg
  title: 'FLAIR: An Easy-to-Use Framework for State-of-the-Art NLP'
  title_html: '<span class="acl-fixed-case">FLAIR</span>: An Easy-to-Use Framework
    for State-of-the-Art <span class="acl-fixed-case">NLP</span>'
  url: https://www.aclweb.org/anthology/N19-4010
  year: '2019'
N19-4011:
  abstract: Open-domain dialog systems (i.e. chatbots) are difficult to evaluate.
    The current best practice for analyzing and comparing these dialog systems is
    the use of human judgments. However, the lack of standardization in evaluation
    procedures, and the fact that model parameters and code are rarely published hinder
    systematic human evaluation experiments. We introduce a unified framework for
    human evaluation of chatbots that augments existing tools and provides a web-based
    hub for researchers to share and compare their dialog systems. Researchers can
    submit their trained models to the ChatEval web interface and obtain comparisons
    with baselines and prior work. The evaluation code is open-source to ensure standardization
    and transparency. In addition, we introduce open-source baseline models and evaluation
    datasets. ChatEval can be found at https://chateval.org.
  address: Minneapolis, Minnesota
  author:
  - first: "Jo\xE3o"
    full: "Jo\xE3o Sedoc"
    id: joao-sedoc
    last: Sedoc
  - first: Daphne
    full: Daphne Ippolito
    id: daphne-ippolito
    last: Ippolito
  - first: Arun
    full: Arun Kirubarajan
    id: arun-kirubarajan
    last: Kirubarajan
  - first: Jai
    full: Jai Thirani
    id: jai-thirani
    last: Thirani
  - first: Lyle
    full: Lyle Ungar
    id: lyle-ungar
    last: Ungar
  - first: Chris
    full: Chris Callison-Burch
    id: chris-callison-burch
    last: Callison-Burch
  author_string: "Jo\xE3o Sedoc, Daphne Ippolito, Arun Kirubarajan, Jai Thirani, Lyle\
    \ Ungar, Chris Callison-Burch"
  bibkey: sedoc-etal-2019-chateval
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics (Demonstrations)
  booktitle_html: Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics (Demonstrations)
  doi: 10.18653/v1/N19-4011
  month: June
  page_first: '60'
  page_last: '65'
  pages: "60\u201365"
  paper_id: '11'
  parent_volume_id: N19-4
  pdf: https://www.aclweb.org/anthology/N19-4011.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-4011.jpg
  title: 'ChatEval: A Tool for Chatbot Evaluation'
  title_html: '<span class="acl-fixed-case">C</span>hat<span class="acl-fixed-case">E</span>val:
    A Tool for Chatbot Evaluation'
  url: https://www.aclweb.org/anthology/N19-4011
  year: '2019'
N19-4012:
  abstract: Neural abstractive text summarization (NATS) has received a lot of attention
    in the past few years from both industry and academia. In this paper, we introduce
    an open-source toolkit, namely LeafNATS, for training and evaluation of different
    sequence-to-sequence based models for the NATS task, and for deploying the pre-trained
    models to real-world applications. The toolkit is modularized and extensible in
    addition to maintaining competitive performance in the NATS task. A live news
    blogging system has also been implemented to demonstrate how these models can
    aid blog/news editors by providing them suggestions of headlines and summaries
    of their articles.
  address: Minneapolis, Minnesota
  author:
  - first: Tian
    full: Tian Shi
    id: tian-shi
    last: Shi
  - first: Ping
    full: Ping Wang
    id: ping-wang
    last: Wang
  - first: Chandan K.
    full: Chandan K. Reddy
    id: chandan-k-reddy
    last: Reddy
  author_string: Tian Shi, Ping Wang, Chandan K. Reddy
  bibkey: shi-etal-2019-leafnats
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics (Demonstrations)
  booktitle_html: Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics (Demonstrations)
  doi: 10.18653/v1/N19-4012
  month: June
  page_first: '66'
  page_last: '71'
  pages: "66\u201371"
  paper_id: '12'
  parent_volume_id: N19-4
  pdf: https://www.aclweb.org/anthology/N19-4012.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-4012.jpg
  title: 'LeafNATS: An Open-Source Toolkit and Live Demo System for Neural Abstractive
    Text Summarization'
  title_html: '<span class="acl-fixed-case">L</span>eaf<span class="acl-fixed-case">NATS</span>:
    An Open-Source Toolkit and Live Demo System for Neural Abstractive Text Summarization'
  url: https://www.aclweb.org/anthology/N19-4012
  year: '2019'
N19-4013:
  abstract: We demonstrate an end-to-end question answering system that integrates
    BERT with the open-source Anserini information retrieval toolkit. In contrast
    to most question answering and reading comprehension models today, which operate
    over small amounts of input text, our system integrates best practices from IR
    with a BERT-based reader to identify answers from a large corpus of Wikipedia
    articles in an end-to-end fashion. We report large improvements over previous
    results on a standard benchmark test collection, showing that fine-tuning pretrained
    BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans.
  address: Minneapolis, Minnesota
  author:
  - first: Wei
    full: Wei Yang
    id: wei-yang
    last: Yang
  - first: Yuqing
    full: Yuqing Xie
    id: yuqing-xie
    last: Xie
  - first: Aileen
    full: Aileen Lin
    id: aileen-lin
    last: Lin
  - first: Xingyu
    full: Xingyu Li
    id: xingyu-li
    last: Li
  - first: Luchen
    full: Luchen Tan
    id: luchen-tan
    last: Tan
  - first: Kun
    full: Kun Xiong
    id: kun-xiong
    last: Xiong
  - first: Ming
    full: Ming Li
    id: ming-li
    last: Li
  - first: Jimmy
    full: Jimmy Lin
    id: jimmy-lin
    last: Lin
  author_string: Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong,
    Ming Li, Jimmy Lin
  bibkey: yang-etal-2019-end-end
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics (Demonstrations)
  booktitle_html: Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics (Demonstrations)
  doi: 10.18653/v1/N19-4013
  month: June
  page_first: '72'
  page_last: '77'
  pages: "72\u201377"
  paper_id: '13'
  parent_volume_id: N19-4
  pdf: https://www.aclweb.org/anthology/N19-4013.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-4013.jpg
  title: End-to-End Open-Domain Question Answering with BERTserini
  title_html: End-to-End Open-Domain Question Answering with <span class="acl-fixed-case">BERT</span>serini
  url: https://www.aclweb.org/anthology/N19-4013
  year: '2019'
N19-4014:
  abstract: 'We present FAKTA which is a unified framework that integrates various
    components of a fact-checking process: document retrieval from media sources with
    various types of reliability, stance detection of documents with respect to given
    claims, evidence extraction, and linguistic analysis. FAKTA predicts the factuality
    of given claims and provides evidence at the document and sentence level to explain
    its predictions.'
  address: Minneapolis, Minnesota
  author:
  - first: Moin
    full: Moin Nadeem
    id: moin-nadeem
    last: Nadeem
  - first: Wei
    full: Wei Fang
    id: wei-fang
    last: Fang
  - first: Brian
    full: Brian Xu
    id: brian-xu
    last: Xu
  - first: Mitra
    full: Mitra Mohtarami
    id: mitra-mohtarami
    last: Mohtarami
  - first: James
    full: James Glass
    id: james-glass
    last: Glass
  author_string: Moin Nadeem, Wei Fang, Brian Xu, Mitra Mohtarami, James Glass
  bibkey: nadeem-etal-2019-fakta
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics (Demonstrations)
  booktitle_html: Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics (Demonstrations)
  doi: 10.18653/v1/N19-4014
  month: June
  page_first: '78'
  page_last: '83'
  pages: "78\u201383"
  paper_id: '14'
  parent_volume_id: N19-4
  pdf: https://www.aclweb.org/anthology/N19-4014.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-4014.jpg
  title: 'FAKTA: An Automatic End-to-End Fact Checking System'
  title_html: '<span class="acl-fixed-case">FAKTA</span>: An Automatic End-to-End
    Fact Checking System'
  url: https://www.aclweb.org/anthology/N19-4014
  year: '2019'
N19-4015:
  abstract: In this paper, we introduce iComposer, an interactive web-based songwriting
    system designed to assist human creators by greatly simplifying music production.
    iComposer automatically creates melodies to accompany any given text. It also
    enables users to generate a set of lyrics given arbitrary melodies. iComposer
    is based on three sequence-to-sequence models, which are used to predict melody,
    rhythm, and lyrics, respectively. Songs generated by iComposer are compared with
    human-composed and randomly-generated ones in a subjective test, the experimental
    results of which demonstrate the capability of the proposed system to write pleasing
    melodies and meaningful lyrics at a level similar to that of humans.
  address: Minneapolis, Minnesota
  author:
  - first: Hsin-Pei
    full: Hsin-Pei Lee
    id: hsin-pei-lee
    last: Lee
  - first: Jhih-Sheng
    full: Jhih-Sheng Fang
    id: jhih-sheng-fang
    last: Fang
  - first: Wei-Yun
    full: Wei-Yun Ma
    id: wei-yun-ma
    last: Ma
  author_string: Hsin-Pei Lee, Jhih-Sheng Fang, Wei-Yun Ma
  bibkey: lee-etal-2019-icomposer
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics (Demonstrations)
  booktitle_html: Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics (Demonstrations)
  doi: 10.18653/v1/N19-4015
  month: June
  page_first: '84'
  page_last: '88'
  pages: "84\u201388"
  paper_id: '15'
  parent_volume_id: N19-4
  pdf: https://www.aclweb.org/anthology/N19-4015.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-4015.jpg
  title: 'iComposer: An Automatic Songwriting System for Chinese Popular Music'
  title_html: 'i<span class="acl-fixed-case">C</span>omposer: An Automatic Songwriting
    System for <span class="acl-fixed-case">C</span>hinese Popular Music'
  url: https://www.aclweb.org/anthology/N19-4015
  year: '2019'
N19-4016:
  abstract: Story composition is a challenging problem for machines and even for humans.
    We present a neural narrative generation system that interacts with humans to
    generate stories. Our system has different levels of human interaction, which
    enables us to understand at what stage of story-writing human collaboration is
    most productive, both to improving story quality and human engagement in the writing
    process. We compare different varieties of interaction in story-writing, story-planning,
    and diversity controls under time constraints, and show that increased types of
    human collaboration at both planning and writing stages results in a 10-50% improvement
    in story quality as compared to less interactive baselines. We also show an accompanying
    increase in user engagement and satisfaction with stories as compared to our own
    less interactive systems and to previous turn-taking approaches to interaction.
    Finally, we find that humans tasked with collaboratively improving a particular
    characteristic of a story are in fact able to do so, which has implications for
    future uses of human-in-the-loop systems.
  address: Minneapolis, Minnesota
  author:
  - first: Seraphina
    full: Seraphina Goldfarb-Tarrant
    id: seraphina-goldfarb-tarrant
    last: Goldfarb-Tarrant
  - first: Haining
    full: Haining Feng
    id: haining-feng
    last: Feng
  - first: Nanyun
    full: Nanyun Peng
    id: nanyun-peng
    last: Peng
  author_string: Seraphina Goldfarb-Tarrant, Haining Feng, Nanyun Peng
  bibkey: goldfarb-tarrant-etal-2019-plan
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics (Demonstrations)
  booktitle_html: Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics (Demonstrations)
  doi: 10.18653/v1/N19-4016
  month: June
  page_first: '89'
  page_last: '97'
  pages: "89\u201397"
  paper_id: '16'
  parent_volume_id: N19-4
  pdf: https://www.aclweb.org/anthology/N19-4016.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-4016.jpg
  title: 'Plan, Write, and Revise: an Interactive System for Open-Domain Story Generation'
  title_html: 'Plan, Write, and Revise: an Interactive System for Open-Domain Story
    Generation'
  url: https://www.aclweb.org/anthology/N19-4016
  year: '2019'
N19-4017:
  abstract: Expert finding is the task of ranking persons for a predefined topic or
    search query. Finding experts for a specified area is an important task and has
    attracted much attention in the information retrieval community. Most approaches
    for this task are evaluated in a supervised fashion, which depend on predefined
    topics of interest as well as gold standard expert rankings. Famous representatives
    of such datasets are enriched versions of DBLP provided by the ArnetMiner projet
    or the W3C Corpus of TREC. However, manually ranking experts can be considered
    highly subjective and detailed rankings are hardly distinguishable. Evaluating
    these datasets does not necessarily guarantee a good or bad performance of the
    system. Particularly for dynamic systems, where topics are not predefined but
    formulated as a search query, we believe a more informative approach is to perform
    user studies for directly comparing different methods in the same view. In order
    to accomplish this in a user-friendly way, we present the LT Expert Finder web-application,
    which is equipped with various query-based expert finding methods that can be
    easily extended, a detailed expert profile view, detailed evidence in form of
    relevant documents and statistics, and an evaluation component that allows the
    qualitative comparison between different rankings.
  address: Minneapolis, Minnesota
  author:
  - first: Tim
    full: Tim Fischer
    id: tim-fischer
    last: Fischer
  - first: Steffen
    full: Steffen Remus
    id: steffen-remus
    last: Remus
  - first: Chris
    full: Chris Biemann
    id: chris-biemann
    last: Biemann
  author_string: Tim Fischer, Steffen Remus, Chris Biemann
  bibkey: fischer-etal-2019-lt
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics (Demonstrations)
  booktitle_html: Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics (Demonstrations)
  doi: 10.18653/v1/N19-4017
  month: June
  page_first: '98'
  page_last: '104'
  pages: "98\u2013104"
  paper_id: '17'
  parent_volume_id: N19-4
  pdf: https://www.aclweb.org/anthology/N19-4017.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-4017.jpg
  title: 'LT Expertfinder: An Evaluation Framework for Expert Finding Methods'
  title_html: '<span class="acl-fixed-case">LT</span> Expertfinder: An Evaluation
    Framework for Expert Finding Methods'
  url: https://www.aclweb.org/anthology/N19-4017
  year: '2019'
N19-4018:
  abstract: We present SkillBot that takes the first step to enable end users to teach
    new skills in personal assistants (PA). Unlike existing PA products that need
    software developers to build new skills via IDE tools, an end user can use SkillBot
    to build new skills just by naturally demonstrating the task on device screen.
    SkillBot automatically develops a natural language understanding (NLU) engine
    and implements the action without the need of coding. On both benchmark and in-house
    datasets, we validate the competitive performance of SkillBot automatically built
    NLU. We also observe that it only takes a few minutes for an end user to build
    a new skill using SkillBot.
  address: Minneapolis, Minnesota
  author:
  - first: Yilin
    full: Yilin Shen
    id: yilin-shen
    last: Shen
  - first: Avik
    full: Avik Ray
    id: avik-ray
    last: Ray
  - first: Hongxia
    full: Hongxia Jin
    id: hongxia-jin
    last: Jin
  - first: Sandeep
    full: Sandeep Nama
    id: sandeep-nama
    last: Nama
  author_string: Yilin Shen, Avik Ray, Hongxia Jin, Sandeep Nama
  bibkey: shen-etal-2019-skillbot
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics (Demonstrations)
  booktitle_html: Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics (Demonstrations)
  doi: 10.18653/v1/N19-4018
  month: June
  page_first: '105'
  page_last: '109'
  pages: "105\u2013109"
  paper_id: '18'
  parent_volume_id: N19-4
  pdf: https://www.aclweb.org/anthology/N19-4018.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-4018.jpg
  title: 'SkillBot: Towards Automatic Skill Development via User Demonstration'
  title_html: '<span class="acl-fixed-case">S</span>kill<span class="acl-fixed-case">B</span>ot:
    Towards Automatic Skill Development via User Demonstration'
  url: https://www.aclweb.org/anthology/N19-4018
  year: '2019'
N19-4019:
  abstract: "This paper demonstrates a state-of-the-art end-to-end multilingual (English,\
    \ Russian, and Ukrainian) knowledge extraction system that can perform entity\
    \ discovery and linking, relation extraction, event extraction, and coreference.\
    \ It extracts and aggregates knowledge elements across multiple languages and\
    \ documents as well as provides visualizations of the results along three dimensions:\
    \ temporal (as displayed in an event timeline), spatial (as displayed in an event\
    \ heatmap), and relational (as displayed in entity-relation networks). For our\
    \ system to further support users\u2019 analyses of causal sequences of events\
    \ in complex situations, we also integrate a wide range of human moral value measures,\
    \ independently derived from region-based survey, into the event heatmap. This\
    \ system is publicly available as a docker container and a live demo."
  address: Minneapolis, Minnesota
  author:
  - first: Manling
    full: Manling Li
    id: manling-li
    last: Li
  - first: Ying
    full: Ying Lin
    id: ying-lin
    last: Lin
  - first: Joseph
    full: Joseph Hoover
    id: joseph-hoover
    last: Hoover
  - first: Spencer
    full: Spencer Whitehead
    id: spencer-whitehead
    last: Whitehead
  - first: Clare
    full: Clare Voss
    id: clare-voss
    last: Voss
  - first: Morteza
    full: Morteza Dehghani
    id: morteza-dehghani
    last: Dehghani
  - first: Heng
    full: Heng Ji
    id: heng-ji
    last: Ji
  author_string: Manling Li, Ying Lin, Joseph Hoover, Spencer Whitehead, Clare Voss,
    Morteza Dehghani, Heng Ji
  bibkey: li-etal-2019-multilingual
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics (Demonstrations)
  booktitle_html: Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics (Demonstrations)
  doi: 10.18653/v1/N19-4019
  month: June
  page_first: '110'
  page_last: '115'
  pages: "110\u2013115"
  paper_id: '19'
  parent_volume_id: N19-4
  pdf: https://www.aclweb.org/anthology/N19-4019.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-4019.jpg
  title: Multilingual Entity, Relation, Event and Human Value Extraction
  title_html: Multilingual Entity, Relation, Event and Human Value Extraction
  url: https://www.aclweb.org/anthology/N19-4019
  year: '2019'
N19-4020:
  abstract: Legal litigation planning can benefit from statistics collected from past
    decisions made by judges. Information on the typical duration for a submitted
    motion, for example, can give valuable clues for developing a successful strategy.
    Such information is encoded in semi-structured documents called dockets. In order
    to extract and aggregate this information, we deployed various information extraction
    and machine learning techniques. The aggregated data can be queried in real time
    within the Westlaw Edge search engine. In addition to a keyword search for judges,
    lawyers, law firms, parties and courts, we also implemented a question answering
    interface that offers targeted questions in order to get to the respective answers
    quicker.
  address: Minneapolis, Minnesota
  author:
  - first: Thomas
    full: Thomas Vacek
    id: thomas-vacek
    last: Vacek
  - first: Dezhao
    full: Dezhao Song
    id: dezhao-song
    last: Song
  - first: Hugo
    full: Hugo Molina-Salgado
    id: hugo-molina-salgado
    last: Molina-Salgado
  - first: Ronald
    full: Ronald Teo
    id: ronald-teo
    last: Teo
  - first: Conner
    full: Conner Cowling
    id: conner-cowling
    last: Cowling
  - first: Frank
    full: Frank Schilder
    id: frank-schilder
    last: Schilder
  author_string: Thomas Vacek, Dezhao Song, Hugo Molina-Salgado, Ronald Teo, Conner
    Cowling, Frank Schilder
  bibkey: vacek-etal-2019-litigation
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics (Demonstrations)
  booktitle_html: Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics (Demonstrations)
  doi: 10.18653/v1/N19-4020
  month: June
  page_first: '116'
  page_last: '121'
  pages: "116\u2013121"
  paper_id: '20'
  parent_volume_id: N19-4
  pdf: https://www.aclweb.org/anthology/N19-4020.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-4020.jpg
  title: 'Litigation Analytics: Extracting and querying motions and orders from US
    federal courts'
  title_html: 'Litigation Analytics: Extracting and querying motions and orders from
    <span class="acl-fixed-case">US</span> federal courts'
  url: https://www.aclweb.org/anthology/N19-4020
  year: '2019'
N19-4021:
  abstract: In this paper, we introduce a morphologically-aware electronic dictionary
    for St. Lawrence Island Yupik, an endangered language of the Bering Strait region.
    Implemented using HTML, Javascript, and CSS, the dictionary is set in an uncluttered
    interface and permits users to search in Yupik or in English for Yupik root words
    and Yupik derivational suffixes. For each matching result, our electronic dictionary
    presents the user with the corresponding entry from the Badten (2008) Yupik-English
    paper dictionary. Because Yupik is a polysynthetic language, handling of multimorphemic
    word forms is critical. If a user searches for an inflected Yupik word form, we
    perform a morphological analysis and return entries for the root word and for
    any derivational suffixes present in the word. This electronic dictionary should
    serve not only as a valuable resource for all students and speakers of Yupik,
    but also for field linguists working towards documentation and conservation of
    the language.
  address: Minneapolis, Minnesota
  author:
  - first: Benjamin
    full: Benjamin Hunt
    id: benjamin-hunt
    last: Hunt
  - first: Emily
    full: Emily Chen
    id: emily-chen
    last: Chen
  - first: Sylvia L.R.
    full: Sylvia L.R. Schreiner
    id: sylvia-l-r-schreiner
    last: Schreiner
  - first: Lane
    full: Lane Schwartz
    id: lane-schwartz
    last: Schwartz
  author_string: Benjamin Hunt, Emily Chen, Sylvia L.R. Schreiner, Lane Schwartz
  bibkey: hunt-etal-2019-community
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics (Demonstrations)
  booktitle_html: Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics (Demonstrations)
  doi: 10.18653/v1/N19-4021
  month: June
  page_first: '122'
  page_last: '126'
  pages: "122\u2013126"
  paper_id: '21'
  parent_volume_id: N19-4
  pdf: https://www.aclweb.org/anthology/N19-4021.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-4021.jpg
  title: 'Community lexical access for an endangered polysynthetic language: An electronic
    dictionary for St. Lawrence Island Yupik'
  title_html: 'Community lexical access for an endangered polysynthetic language:
    An electronic dictionary for St. Lawrence Island Yupik'
  url: https://www.aclweb.org/anthology/N19-4021
  year: '2019'
N19-4022:
  abstract: We present a web-based system that facilitates the exploration of complex
    morphological patterns found in morphologically very rich languages. The need
    for better understanding of such patterns is urgent for linguistics and important
    for cross-linguistically applicable natural language processing. In this paper
    we give an overview of the system architecture and describe a sample case study
    on Abui [abz], a Trans-New Guinea language spoken in Indonesia.
  address: Minneapolis, Minnesota
  author:
  - first: Haley
    full: Haley Lepp
    id: haley-lepp
    last: Lepp
  - first: Olga
    full: Olga Zamaraeva
    id: olga-zamaraeva
    last: Zamaraeva
  - first: Emily M.
    full: Emily M. Bender
    id: emily-m-bender
    last: Bender
  author_string: Haley Lepp, Olga Zamaraeva, Emily M. Bender
  bibkey: lepp-etal-2019-visualizing
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics (Demonstrations)
  booktitle_html: Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics (Demonstrations)
  doi: 10.18653/v1/N19-4022
  month: June
  page_first: '127'
  page_last: '131'
  pages: "127\u2013131"
  paper_id: '22'
  parent_volume_id: N19-4
  pdf: https://www.aclweb.org/anthology/N19-4022.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-4022.jpg
  title: Visualizing Inferred Morphotactic Systems
  title_html: Visualizing Inferred Morphotactic Systems
  url: https://www.aclweb.org/anthology/N19-4022
  year: '2019'
N19-4023:
  abstract: 'This paper presents a research platform that supports spoken dialogue
    interaction with multiple robots. The demonstration showcases our crafted MultiBot
    testing scenario in which users can verbally issue search, navigate, and follow
    instructions to two robotic teammates: a simulated ground robot and an aerial
    robot. This flexible language and robotic platform takes advantage of existing
    tools for speech recognition and dialogue management that are compatible with
    new domains, and implements an inter-agent communication protocol (tactical behavior
    specification), where verbal instructions are encoded for tasks assigned to the
    appropriate robot.'
  address: Minneapolis, Minnesota
  author:
  - first: Matthew
    full: Matthew Marge
    id: matthew-marge
    last: Marge
  - first: Stephen
    full: Stephen Nogar
    id: stephen-nogar
    last: Nogar
  - first: Cory J.
    full: Cory J. Hayes
    id: cory-hayes
    last: Hayes
  - first: Stephanie M.
    full: Stephanie M. Lukin
    id: stephanie-lukin
    last: Lukin
  - first: Jesse
    full: Jesse Bloecker
    id: jesse-bloecker
    last: Bloecker
  - first: Eric
    full: Eric Holder
    id: eric-holder
    last: Holder
  - first: Clare
    full: Clare Voss
    id: clare-voss
    last: Voss
  author_string: Matthew Marge, Stephen Nogar, Cory J. Hayes, Stephanie M. Lukin,
    Jesse Bloecker, Eric Holder, Clare Voss
  bibkey: marge-etal-2019-research
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics (Demonstrations)
  booktitle_html: Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics (Demonstrations)
  doi: 10.18653/v1/N19-4023
  month: June
  page_first: '132'
  page_last: '137'
  pages: "132\u2013137"
  paper_id: '23'
  parent_volume_id: N19-4
  pdf: https://www.aclweb.org/anthology/N19-4023.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-4023.jpg
  title: A Research Platform for Multi-Robot Dialogue with Humans
  title_html: A <span class="acl-fixed-case">R</span>esearch <span class="acl-fixed-case">P</span>latform
    for <span class="acl-fixed-case">M</span>ulti-<span class="acl-fixed-case">R</span>obot
    <span class="acl-fixed-case">D</span>ialogue with <span class="acl-fixed-case">H</span>umans
  url: https://www.aclweb.org/anthology/N19-4023
  year: '2019'
N19-4024:
  abstract: 'In this paper we introduce Chat-crowd, an interactive environment for
    visual layout composition via conversational interactions. Chat-crowd supports
    multiple agents with two conversational roles: agents who play the role of a designer
    are in charge of placing objects in an editable canvas according to instructions
    or commands issued by agents with a director role. The system can be integrated
    with crowdsourcing platforms for both synchronous and asynchronous data collection
    and is equipped with comprehensive quality controls on the performance of both
    types of agents. We expect that this system will be useful to build multimodal
    goal-oriented dialog tasks that require spatial and geometric reasoning.'
  address: Minneapolis, Minnesota
  author:
  - first: Paola
    full: Paola Cascante-Bonilla
    id: paola-cascante-bonilla
    last: Cascante-Bonilla
  - first: Xuwang
    full: Xuwang Yin
    id: xuwang-yin
    last: Yin
  - first: Vicente
    full: Vicente Ordonez
    id: vicente-ordonez
    last: Ordonez
  - first: Song
    full: Song Feng
    id: song-feng
    last: Feng
  author_string: Paola Cascante-Bonilla, Xuwang Yin, Vicente Ordonez, Song Feng
  bibkey: cascante-bonilla-etal-2019-chat
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics (Demonstrations)
  booktitle_html: Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics (Demonstrations)
  doi: 10.18653/v1/N19-4024
  month: June
  page_first: '138'
  page_last: '142'
  pages: "138\u2013142"
  paper_id: '24'
  parent_volume_id: N19-4
  pdf: https://www.aclweb.org/anthology/N19-4024.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-4024.jpg
  title: 'Chat-crowd: A Dialog-based Platform for Visual Layout Composition'
  title_html: 'Chat-crowd: A Dialog-based Platform for Visual Layout Composition'
  url: https://www.aclweb.org/anthology/N19-4024
  year: '2019'
N19-5000:
  address: Minneapolis, Minnesota
  author:
  - first: Anoop
    full: Anoop Sarkar
    id: anoop-sarkar
    last: Sarkar
  - first: Michael
    full: Michael Strube
    id: michael-strube
    last: Strube
  author_string: Anoop Sarkar, Michael Strube
  bibkey: naacl-2019-2019-north-american-chapter-association
  bibtype: proceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Tutorials'
  month: June
  paper_id: '0'
  parent_volume_id: N19-5
  pdf: https://www.aclweb.org/anthology/N19-5000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-5000.jpg
  title: 'Proceedings of the 2019 Conference of the North American Chapter of the
    Association for Computational Linguistics: Tutorials'
  title_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Tutorials'
  url: https://www.aclweb.org/anthology/N19-5000
  year: '2019'
N19-5001:
  abstract: Adversarial learning is a game-theoretic learning paradigm, which has
    achieved huge successes in the field of Computer Vision recently. Adversarial
    learning is also a general framework that enables a variety of learning models,
    including the popular Generative Adversarial Networks (GANs). Due to the discrete
    nature of language, designing adversarial learning models is still challenging
    for NLP problems. In this tutorial, we provide a gentle introduction to the foundation
    of deep adversarial learning, as well as some practical problem formulations and
    solutions in NLP. We describe recent advances in deep adversarial learning for
    NLP, with a special focus on generation, adversarial examples & rules, and dialogue.
    We provide an overview of the research area, categorize different types of adversarial
    learning models, and discuss pros and cons, aiming at providing some practical
    perspectives on the future of adversarial learning for solving real-world NLP
    problems.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-5001.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-5001.Presentation.pdf
  - filename: https://vimeo.com/359555654
    type: video
    url: https://vimeo.com/359555654
  author:
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  - first: Sameer
    full: Sameer Singh
    id: sameer-singh
    last: Singh
  - first: Jiwei
    full: Jiwei Li
    id: jiwei-li
    last: Li
  author_string: William Yang Wang, Sameer Singh, Jiwei Li
  bibkey: wang-etal-2019-deep
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Tutorials'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Tutorials'
  doi: 10.18653/v1/N19-5001
  month: June
  page_first: '1'
  page_last: '5'
  pages: "1\u20135"
  paper_id: '1'
  parent_volume_id: N19-5
  pdf: https://www.aclweb.org/anthology/N19-5001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-5001.jpg
  title: Deep Adversarial Learning for NLP
  title_html: Deep Adversarial Learning for <span class="acl-fixed-case">NLP</span>
  url: https://www.aclweb.org/anthology/N19-5001
  year: '2019'
N19-5002:
  abstract: This tutorial discusses cutting-edge research on NLI, including recent
    advance on dataset development, cutting-edge deep learning models, and highlights
    from recent research on using NLI to understand capabilities and limits of deep
    learning models for language understanding and reasoning.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-5002.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-5002.Presentation.pdf
  - filename: https://vimeo.com/347430433
    type: video
    url: https://vimeo.com/347430433
  author:
  - first: Samuel
    full: Samuel Bowman
    id: samuel-bowman
    last: Bowman
  - first: Xiaodan
    full: Xiaodan Zhu
    id: xiaodan-zhu
    last: Zhu
  author_string: Samuel Bowman, Xiaodan Zhu
  bibkey: bowman-zhu-2019-deep
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Tutorials'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Tutorials'
  doi: 10.18653/v1/N19-5002
  month: June
  page_first: '6'
  page_last: '8'
  pages: "6\u20138"
  paper_id: '2'
  parent_volume_id: N19-5
  pdf: https://www.aclweb.org/anthology/N19-5002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-5002.jpg
  title: Deep Learning for Natural Language Inference
  title_html: Deep Learning for Natural Language Inference
  url: https://www.aclweb.org/anthology/N19-5002
  year: '2019'
N19-5003:
  abstract: "This tutorial is designed to help researchers answer the following sorts\
    \ of questions: - Are people happier on the weekend? - What was 1861\u2019s word\
    \ of the year? - Are Democrats and Republicans more different than ever? - When\
    \ did \u201Cgay\u201D stop meaning \u201Chappy\u201D? - Are gender stereotypes\
    \ getting weaker, stronger, or just different? - Who is a linguistic leader? -\
    \ How can we get internet users to be more polite and objective? Such questions\
    \ are fundamental to the social sciences and humanities, and scholars in these\
    \ disciplines are increasingly turning to computational techniques for answers.\
    \ Meanwhile, the ACL community is increasingly engaged with data that varies across\
    \ time, and with the social insights that can be offered by analyzing temporal\
    \ patterns and trends. The purpose of this tutorial is to facilitate this convergence\
    \ in two main ways: 1. By synthesizing recent computational techniques for handling\
    \ and modeling temporal data, such as dynamic word embeddings, the tutorial will\
    \ provide a starting point for future computational research. It will also identify\
    \ useful tools for social scientists and digital humanities scholars. 2. The tutorial\
    \ will provide an overview of techniques and datasets from the quantitative social\
    \ sciences and the digital humanities, which are not well-known in the computational\
    \ linguistics community. These techniques include vector autoregressive models,\
    \ multiple comparisons corrections for hypothesis testing, and causal inference.\
    \ Datasets include historical newspaper archives and corpora of contemporary political\
    \ speech."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-5003.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-5003.Presentation.pdf
  - filename: https://vimeo.com/347475879
    type: video
    url: https://vimeo.com/347475879
  author:
  - first: Jacob
    full: Jacob Eisenstein
    id: jacob-eisenstein
    last: Eisenstein
  author_string: Jacob Eisenstein
  bibkey: eisenstein-2019-measuring
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Tutorials'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Tutorials'
  doi: 10.18653/v1/N19-5003
  month: June
  page_first: '9'
  page_last: '14'
  pages: "9\u201314"
  paper_id: '3'
  parent_volume_id: N19-5
  pdf: https://www.aclweb.org/anthology/N19-5003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-5003.jpg
  title: Measuring and Modeling Language Change
  title_html: Measuring and Modeling Language Change
  url: https://www.aclweb.org/anthology/N19-5003
  year: '2019'
N19-5004:
  abstract: The classic supervised machine learning paradigm is based on learning
    in isolation, a single predictive model for a task using a single dataset. This
    approach requires a large number of training examples and performs best for well-defined
    and narrow tasks. Transfer learning refers to a set of methods that extend this
    approach by leveraging data from additional domains or tasks to train a model
    with better generalization properties. Over the last two years, the field of Natural
    Language Processing (NLP) has witnessed the emergence of several transfer learning
    methods and architectures which significantly improved upon the state-of-the-art
    on a wide range of NLP tasks. These improvements together with the wide availability
    and ease of integration of these methods are reminiscent of the factors that led
    to the success of pretrained word embeddings and ImageNet pretraining in computer
    vision, and indicate that these methods will likely become a common tool in the
    NLP landscape as well as an important research direction. We will present an overview
    of modern transfer learning methods in NLP, how models are pre-trained, what information
    the representations they learn capture, and review examples and case studies on
    how these models can be integrated and adapted in downstream NLP tasks.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-5004.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-5004.Presentation.pdf
  - filename: https://vimeo.com/359399507
    type: video
    url: https://vimeo.com/359399507
  author:
  - first: Sebastian
    full: Sebastian Ruder
    id: sebastian-ruder
    last: Ruder
  - first: Matthew E.
    full: Matthew E. Peters
    id: matthew-e-peters
    last: Peters
  - first: Swabha
    full: Swabha Swayamdipta
    id: swabha-swayamdipta
    last: Swayamdipta
  - first: Thomas
    full: Thomas Wolf
    id: thomas-wolf
    last: Wolf
  author_string: Sebastian Ruder, Matthew E. Peters, Swabha Swayamdipta, Thomas Wolf
  bibkey: ruder-etal-2019-transfer
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Tutorials'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Tutorials'
  doi: 10.18653/v1/N19-5004
  month: June
  page_first: '15'
  page_last: '18'
  pages: "15\u201318"
  paper_id: '4'
  parent_volume_id: N19-5
  pdf: https://www.aclweb.org/anthology/N19-5004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-5004.jpg
  title: Transfer Learning in Natural Language Processing
  title_html: Transfer Learning in Natural Language Processing
  url: https://www.aclweb.org/anthology/N19-5004
  year: '2019'
N19-5005:
  abstract: "The goal of this tutorial is to bring the fields of computational linguistics\
    \ and computational cognitive science closer: we will introduce different stages\
    \ of language acquisition and their parallel problems in NLP. As an example, one\
    \ of the early challenges children face is mapping the meaning of word labels\
    \ (such as \u201Ccat\u201D) to their referents (the furry animal in the living\
    \ room). Word learning is similar to the word alignment problem in machine translation.\
    \ We explain the current computational models of language acquisition, their limitations,\
    \ and how the insights from these models can be incorporated into NLP applications.\
    \ Moreover, we discuss how we can take advantage of the cognitive science of language\
    \ in computational linguistics: for example, by designing cognitively-motivated\
    \ evaluations task or buildings language-learning inductive biases into our models."
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-5005.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-5005.Presentation.pdf
  - filename: https://vimeo.com/347451419
    type: video
    url: https://vimeo.com/347451419
  author:
  - first: Aida
    full: Aida Nematzadeh
    id: aida-nematzadeh
    last: Nematzadeh
  - first: Richard
    full: Richard Futrell
    id: richard-futrell
    last: Futrell
  - first: Roger
    full: Roger Levy
    id: roger-levy
    last: Levy
  author_string: Aida Nematzadeh, Richard Futrell, Roger Levy
  bibkey: nematzadeh-etal-2019-language
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Tutorials'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Tutorials'
  doi: 10.18653/v1/N19-5005
  month: June
  page_first: '19'
  page_last: '21'
  pages: "19\u201321"
  paper_id: '5'
  parent_volume_id: N19-5
  pdf: https://www.aclweb.org/anthology/N19-5005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-5005.jpg
  title: Language Learning and Processing in People and Machines
  title_html: Language Learning and Processing in People and Machines
  url: https://www.aclweb.org/anthology/N19-5005
  year: '2019'
N19-5006:
  abstract: Rapid growth in adoption of electronic health records (EHRs) has led to
    an unprecedented expansion in the availability of large longitudinal datasets.
    Large initiatives such as the Electronic Medical Records and Genomics (eMERGE)
    Network, the Patient-Centered Outcomes Research Network (PCORNet), and the Observational
    Health Data Science and Informatics (OHDSI) consortium, have been established
    and have reported successful applications of secondary use of EHRs in clinical
    research and practice. In these applications, natural language processing (NLP)
    technologies have played a crucial role as much of detailed patient information
    in EHRs is embedded in narrative clinical documents. Meanwhile, a number of clinical
    NLP systems, such as MedLEE, MetaMap/MetaMap Lite, cTAKES, and MedTagger have
    been developed and utilized to extract useful information from diverse types of
    clinical text, such as clinical notes, radiology reports, and pathology reports.
    Success stories in applying these tools have been reported widely. Despite the
    demonstrated success of NLP in the clinical domain, methodologies and tools developed
    for the clinical NLP are still underknown and underutilized by students and experts
    in the general NLP domain, mainly due to the limited exposure to EHR data. Through
    this tutorial, we would like to introduce NLP methodologies and tools developed
    in the clinical domain, and showcase the real-world NLP applications in clinical
    research and practice at Mayo Clinic (the No. 1 national hospital ranked by the
    U.S. News & World Report) and the University of Minnesota (the No. 41 best global
    universities ranked by the U.S. News & World Report). We will review NLP techniques
    in solving clinical problems and facilitating clinical research, the state-of-the
    art clinical NLP tools, and share collaboration experience with clinicians, as
    well as publicly available EHR data and medical resources, and finally conclude
    the tutorial with vast opportunities and challenges of clinical NLP. The tutorial
    will provide an overview of clinical backgrounds, and does not presume knowledge
    in medicine or health care. The goal of this tutorial is to encourage NLP researchers
    in the general domain (as opposed to the specialized clinical domain) to contribute
    to this burgeoning area. In this tutorial, we will first present an overview of
    clinical NLP. We will then dive into two subareas of clinical NLP in clinical
    research, including big data infrastructure for large-scale clinical NLP and advances
    of NLP in clinical research, and two subareas in clinical practice, including
    clinical information extraction and patient cohort retrieval using EHRs. Around
    70% of the tutorial will review clinical problems, cutting-edge methodologies,
    and real-world clinical NLP tools while another 30% introduce use cases at Mayo
    Clinic and the University of Minnesota. Finally, we will conclude the tutorial
    with challenges and opportunities in this rapidly developing domain.
  address: Minneapolis, Minnesota
  attachment:
  - filename: N19-5006.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/N19-5006.Presentation.pdf
  - filename: N19-5006.Poster.pdf
    type: poster
    url: https://www.aclweb.org/anthology/attachments/N19-5006.Poster.pdf
  - filename: https://vimeo.com/347507919
    type: video
    url: https://vimeo.com/347507919
  author:
  - first: Yanshan
    full: Yanshan Wang
    id: yanshan-wang
    last: Wang
  - first: Ahmad
    full: Ahmad Tafti
    id: ahmad-tafti
    last: Tafti
  - first: Sunghwan
    full: Sunghwan Sohn
    id: sunghwan-sohn
    last: Sohn
  - first: Rui
    full: Rui Zhang
    id: rui-zhang
    last: Zhang
  author_string: Yanshan Wang, Ahmad Tafti, Sunghwan Sohn, Rui Zhang
  bibkey: wang-etal-2019-applications
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Tutorials'
  booktitle_html: 'Proceedings of the 2019 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Tutorials'
  doi: 10.18653/v1/N19-5006
  month: June
  page_first: '22'
  page_last: '25'
  pages: "22\u201325"
  paper_id: '6'
  parent_volume_id: N19-5
  pdf: https://www.aclweb.org/anthology/N19-5006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N19-5006.jpg
  title: Applications of Natural Language Processing in Clinical Research and Practice
  title_html: Applications of Natural Language Processing in Clinical Research and
    Practice
  url: https://www.aclweb.org/anthology/N19-5006
  year: '2019'
