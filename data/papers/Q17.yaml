Q17-1000:
  bibkey: tacl-2017-transactions
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  paper_id: '0'
  parent_volume_id: Q17-1
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1000.jpg
  title: Transactions of the Association for Computational Linguistics, Volume 5
  title_html: Transactions of the Association for Computational Linguistics, Volume
    5
  year: '2017'
Q17-1001:
  abstract: "Probabilistic topic models are important tools for indexing, summarizing,\
    \ and analyzing large document collections by their themes. However, promoting\
    \ end-user understanding of topics remains an open research problem. We compare\
    \ labels generated by users given four topic visualization techniques\u2014word\
    \ lists, word lists with bars, word clouds, and network graphs\u2014against each\
    \ other and against automatically generated labels. Our basis of comparison is\
    \ participant ratings of how well labels describe documents from the topic. Our\
    \ study has two phases: a labeling phase where participants label visualized topics\
    \ and a validation phase where different participants select which labels best\
    \ describe the topics\u2019 documents. Although all visualizations produce similar\
    \ quality labels, simple visualizations such as word lists allow participants\
    \ to quickly understand topics, while complex visualizations take longer but expose\
    \ multi-word expressions that simpler visualizations obscure. Automatic labels\
    \ lag behind user-created labels, but our dataset of manually labeled topics highlights\
    \ linguistic patterns (e.g., hypernyms, phrases) that can be used to improve automatic\
    \ topic labeling algorithms."
  attachment:
  - filename: https://vimeo.com/234957075
    type: video
    url: https://vimeo.com/234957075
  author:
  - first: Alison
    full: Alison Smith
    id: alison-smith
    last: Smith
  - first: Tak Yeon
    full: Tak Yeon Lee
    id: tak-yeon-lee
    last: Lee
  - first: Forough
    full: Forough Poursabzi-Sangdeh
    id: forough-poursabzi-sangdeh
    last: Poursabzi-Sangdeh
  - first: Jordan
    full: Jordan Boyd-Graber
    id: jordan-boyd-graber
    last: Boyd-Graber
  - first: Niklas
    full: Niklas Elmqvist
    id: niklas-elmqvist
    last: Elmqvist
  - first: Leah
    full: Leah Findlater
    id: leah-findlater
    last: Findlater
  author_string: Alison Smith, Tak Yeon Lee, Forough Poursabzi-Sangdeh, Jordan Boyd-Graber,
    Niklas Elmqvist, Leah Findlater
  bibkey: smith-etal-2017-evaluating
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00042
  page_first: '1'
  page_last: '16'
  pages: "1\u201316"
  paper_id: '1'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1001.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1001.jpg
  title: Evaluating Visual Representations for Topic Understanding and Their Effects
    on Manually Generated Topic Labels
  title_html: Evaluating Visual Representations for Topic Understanding and Their
    Effects on Manually Generated Topic Labels
  url: https://www.aclweb.org/anthology/Q17-1001
  year: '2017'
Q17-1002:
  abstract: Important advances have recently been made using computational semantic
    models to decode brain activity patterns associated with concepts; however, this
    work has almost exclusively focused on concrete nouns. How well these models extend
    to decoding abstract nouns is largely unknown. We address this question by applying
    state-of-the-art computational models to decode functional Magnetic Resonance
    Imaging (fMRI) activity patterns, elicited by participants reading and imagining
    a diverse set of both concrete and abstract nouns. One of the models we use is
    linguistic, exploiting the recent word2vec skipgram approach trained on Wikipedia.
    The second is visually grounded, using deep convolutional neural networks trained
    on Google Images. Dual coding theory considers concrete concepts to be encoded
    in the brain both linguistically and visually, and abstract concepts only linguistically.
    Splitting the fMRI data according to human concreteness ratings, we indeed observe
    that both models significantly decode the most concrete nouns; however, accuracy
    is significantly greater using the text-based models for the most abstract nouns.
    More generally this confirms that current computational models are sufficiently
    advanced to assist in investigating the representational structure of abstract
    concepts in the brain.
  attachment:
  - filename: https://vimeo.com/234954554
    type: video
    url: https://vimeo.com/234954554
  author:
  - first: Andrew J.
    full: Andrew J. Anderson
    id: andrew-j-anderson
    last: Anderson
  - first: Douwe
    full: Douwe Kiela
    id: douwe-kiela
    last: Kiela
  - first: Stephen
    full: Stephen Clark
    id: stephen-clark
    last: Clark
  - first: Massimo
    full: Massimo Poesio
    id: massimo-poesio
    last: Poesio
  author_string: Andrew J. Anderson, Douwe Kiela, Stephen Clark, Massimo Poesio
  bibkey: anderson-etal-2017-visually
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00043
  page_first: '17'
  page_last: '30'
  pages: "17\u201330"
  paper_id: '2'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1002.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1002.jpg
  title: Visually Grounded and Textual Semantic Models Differentially Decode Brain
    Activity Associated with Concrete and Abstract Nouns
  title_html: Visually Grounded and Textual Semantic Models Differentially Decode
    Brain Activity Associated with Concrete and Abstract Nouns
  url: https://www.aclweb.org/anthology/Q17-1002
  year: '2017'
Q17-1003:
  abstract: Recent research in psycholinguistics has provided increasing evidence
    that humans predict upcoming content. Prediction also affects perception and might
    be a key to robustness in human language processing. In this paper, we investigate
    the factors that affect human prediction by building a computational model that
    can predict upcoming discourse referents based on linguistic knowledge alone vs.
    linguistic knowledge jointly with common-sense knowledge in the form of scripts.
    We find that script knowledge significantly improves model estimates of human
    predictions. In a second study, we test the highly controversial hypothesis that
    predictability influences referring expression type but do not find evidence for
    such an effect.
  attachment:
  - filename: https://vimeo.com/234958123
    type: video
    url: https://vimeo.com/234958123
  author:
  - first: Ashutosh
    full: Ashutosh Modi
    id: ashutosh-modi
    last: Modi
  - first: Ivan
    full: Ivan Titov
    id: ivan-titov
    last: Titov
  - first: Vera
    full: Vera Demberg
    id: vera-demberg
    last: Demberg
  - first: Asad
    full: Asad Sayeed
    id: asad-sayeed
    last: Sayeed
  - first: Manfred
    full: Manfred Pinkal
    id: manfred-pinkal
    last: Pinkal
  author_string: Ashutosh Modi, Ivan Titov, Vera Demberg, Asad Sayeed, Manfred Pinkal
  bibkey: modi-etal-2017-modeling
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00044
  page_first: '31'
  page_last: '44'
  pages: "31\u201344"
  paper_id: '3'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1003.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1003.jpg
  title: 'Modeling Semantic Expectation: Using Script Knowledge for Referent Prediction'
  title_html: 'Modeling Semantic Expectation: Using Script Knowledge for Referent
    Prediction'
  url: https://www.aclweb.org/anthology/Q17-1003
  year: '2017'
Q17-1004:
  abstract: Transition-based models can be fast and accurate for constituent parsing.
    Compared with chart-based models, they leverage richer features by extracting
    history information from a parser stack, which consists of a sequence of non-local
    constituents. On the other hand, during incremental parsing, constituent information
    on the right hand side of the current word is not utilized, which is a relative
    weakness of shift-reduce parsing. To address this limitation, we leverage a fast
    neural model to extract lookahead features. In particular, we build a bidirectional
    LSTM model, which leverages full sentence information to predict the hierarchy
    of constituents that each word starts and ends. The results are then passed to
    a strong transition-based constituent parser as lookahead features. The resulting
    parser gives 1.3% absolute improvement in WSJ and 2.3% in CTB compared to the
    baseline, giving the highest reported accuracies for fully-supervised parsing.
  author:
  - first: Jiangming
    full: Jiangming Liu
    id: jiangming-liu
    last: Liu
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  author_string: Jiangming Liu, Yue Zhang
  bibkey: liu-zhang-2017-shift
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00045
  page_first: '45'
  page_last: '58'
  pages: "45\u201358"
  paper_id: '4'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1004.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1004.jpg
  title: Shift-Reduce Constituent Parsing with Neural Lookahead Features
  title_html: Shift-Reduce Constituent Parsing with Neural Lookahead Features
  url: https://www.aclweb.org/anthology/Q17-1004
  year: '2017'
Q17-1005:
  abstract: Decoding of phrase-based translation models in the general case is known
    to be NP-complete, by a reduction from the traveling salesman problem (Knight,
    1999). In practice, phrase-based systems often impose a hard distortion limit
    that limits the movement of phrases during translation. However, the impact on
    complexity after imposing such a constraint is not well studied. In this paper,
    we describe a dynamic programming algorithm for phrase-based decoding with a fixed
    distortion limit. The runtime of the algorithm is O(nd!lhd+1) where n is the sentence
    length, d is the distortion limit, l is a bound on the number of phrases starting
    at any position in the sentence, and h is related to the maximum number of target
    language translations for any source word. The algorithm makes use of a novel
    representation that gives a new perspective on decoding of phrase-based models.
  attachment:
  - filename: https://vimeo.com/234951458
    type: video
    url: https://vimeo.com/234951458
  - filename: Q17-1005.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/Q17-1005.Presentation.pdf
  author:
  - first: Yin-Wen
    full: Yin-Wen Chang
    id: yin-wen-chang
    last: Chang
  - first: Michael
    full: Michael Collins
    id: michael-collins
    last: Collins
  author_string: Yin-Wen Chang, Michael Collins
  bibkey: chang-collins-2017-polynomial
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00046
  page_first: '59'
  page_last: '71'
  pages: "59\u201371"
  paper_id: '5'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1005.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1005.jpg
  title: A Polynomial-Time Dynamic Programming Algorithm for Phrase-Based Decoding
    with a Fixed Distortion Limit
  title_html: A Polynomial-Time Dynamic Programming Algorithm for Phrase-Based Decoding
    with a Fixed Distortion Limit
  url: https://www.aclweb.org/anthology/Q17-1005
  year: '2017'
Q17-1006:
  abstract: "We present a probabilistic model of phonotactics, the set of well-formed\
    \ phoneme sequences in a language. Unlike most computational models of phonotactics\
    \ (Hayes and Wilson, 2008; Goldsmith and Riggle, 2012), we take a fully generative\
    \ approach, modeling a process where forms are built up out of subparts by phonologically-informed\
    \ structure building operations. We learn an inventory of subparts by applying\
    \ stochastic memoization (Johnson et al., 2007; Goodman et al., 2008) to a generative\
    \ process for phonemes structured as an and-or graph, based on concepts of feature\
    \ hierarchy from generative phonology (Clements, 1985; Dresher, 2009). Subparts\
    \ are combined in a way that allows tier-based feature interactions. We evaluate\
    \ our models\u2019 ability to capture phonotactic distributions in the lexicons\
    \ of 14 languages drawn from the WOLEX corpus (Graff, 2012). Our full model robustly\
    \ assigns higher probabilities to held-out forms than a sophisticated N-gram model\
    \ for all languages. We also present novel analyses that probe model behavior\
    \ in more detail."
  attachment:
  - filename: https://vimeo.com/234952732
    type: video
    url: https://vimeo.com/234952732
  author:
  - first: Richard
    full: Richard Futrell
    id: richard-futrell
    last: Futrell
  - first: Adam
    full: Adam Albright
    id: adam-albright
    last: Albright
  - first: Peter
    full: Peter Graff
    id: peter-graff
    last: Graff
  - first: Timothy J.
    full: "Timothy J. O\u2019Donnell"
    id: timothy-odonnell
    last: "O\u2019Donnell"
  author_string: "Richard Futrell, Adam Albright, Peter Graff, Timothy J. O\u2019\
    Donnell"
  bibkey: futrell-etal-2017-generative
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00047
  page_first: '73'
  page_last: '86'
  pages: "73\u201386"
  paper_id: '6'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1006.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1006.jpg
  title: A Generative Model of Phonotactics
  title_html: A Generative Model of Phonotactics
  url: https://www.aclweb.org/anthology/Q17-1006
  year: '2017'
Q17-1007:
  abstract: In neural machine translation (NMT), generation of a target word depends
    on both source and target contexts. We find that source contexts have a direct
    impact on the adequacy of a translation while target contexts affect the fluency.
    Intuitively, generation of a content word should rely more on the source context
    and generation of a functional word should rely more on the target context. Due
    to the lack of effective control over the influence from source and target contexts,
    conventional NMT tends to yield fluent but inadequate translations. To address
    this problem, we propose context gates which dynamically control the ratios at
    which source and target contexts contribute to the generation of target words.
    In this way, we can enhance both the adequacy and fluency of NMT with more careful
    control of the information flow from contexts. Experiments show that our approach
    significantly improves upon a standard attention-based NMT system by +2.3 BLEU
    points.
  attachment:
  - filename: https://vimeo.com/234951517
    type: video
    url: https://vimeo.com/234951517
  author:
  - first: Zhaopeng
    full: Zhaopeng Tu
    id: zhaopeng-tu
    last: Tu
  - first: Yang
    full: Yang Liu
    id: yang-liu-ict
    last: Liu
  - first: Zhengdong
    full: Zhengdong Lu
    id: zhengdong-lu
    last: Lu
  - first: Xiaohua
    full: Xiaohua Liu
    id: xiaohua-liu
    last: Liu
  - first: Hang
    full: Hang Li
    id: hang-li
    last: Li
  author_string: Zhaopeng Tu, Yang Liu, Zhengdong Lu, Xiaohua Liu, Hang Li
  bibkey: tu-etal-2017-context
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00048
  page_first: '87'
  page_last: '99'
  pages: "87\u201399"
  paper_id: '7'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1007.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1007.jpg
  title: Context Gates for Neural Machine Translation
  title_html: Context Gates for Neural Machine Translation
  url: https://www.aclweb.org/anthology/Q17-1007
  year: '2017'
Q17-1008:
  abstract: Past work in relation extraction has focused on binary relations in single
    sentences. Recent NLP inroads in high-value domains have sparked interest in the
    more general setting of extracting n-ary relations that span multiple sentences.
    In this paper, we explore a general relation extraction framework based on graph
    long short-term memory networks (graph LSTMs) that can be easily extended to cross-sentence
    n-ary relation extraction. The graph formulation provides a unified way of exploring
    different LSTM approaches and incorporating various intra-sentential and inter-sentential
    dependencies, such as sequential, syntactic, and discourse relations. A robust
    contextual representation is learned for the entities, which serves as input to
    the relation classifier. This simplifies handling of relations with arbitrary
    arity, and enables multi-task learning with related relations. We evaluate this
    framework in two important precision medicine settings, demonstrating its effectiveness
    with both conventional supervised learning and distant supervision. Cross-sentence
    extraction produced larger knowledge bases. and multi-task learning significantly
    improved extraction accuracy. A thorough analysis of various LSTM approaches yielded
    useful insight the impact of linguistic analysis on extraction accuracy.
  attachment:
  - filename: https://vimeo.com/234956645
    type: video
    url: https://vimeo.com/234956645
  author:
  - first: Nanyun
    full: Nanyun Peng
    id: nanyun-peng
    last: Peng
  - first: Hoifung
    full: Hoifung Poon
    id: hoifung-poon
    last: Poon
  - first: Chris
    full: Chris Quirk
    id: chris-quirk
    last: Quirk
  - first: Kristina
    full: Kristina Toutanova
    id: kristina-toutanova
    last: Toutanova
  - first: Wen-tau
    full: Wen-tau Yih
    id: wen-tau-yih
    last: Yih
  author_string: Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, Wen-tau
    Yih
  bibkey: peng-etal-2017-cross
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00049
  page_first: '101'
  page_last: '115'
  pages: "101\u2013115"
  paper_id: '8'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1008.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1008.jpg
  title: Cross-Sentence N-ary Relation Extraction with Graph LSTMs
  title_html: Cross-Sentence N-ary Relation Extraction with Graph <span class="acl-fixed-case">LSTM</span>s
  url: https://www.aclweb.org/anthology/Q17-1008
  year: '2017'
Q17-1009:
  abstract: "This paper explores extending shallow semantic parsing beyond lexical-unit\
    \ triggers, using causal relations as a test case. Semantic parsing becomes difficult\
    \ in the face of the wide variety of linguistic realizations that causation can\
    \ take on. We therefore base our approach on the concept of constructions from\
    \ the linguistic paradigm known as Construction Grammar (CxG). In CxG, a construction\
    \ is a form/function pairing that can rely on arbitrary linguistic and semantic\
    \ features. Rather than codifying all aspects of each construction\u2019s form,\
    \ as some attempts to employ CxG in NLP have done, we propose methods that offload\
    \ that problem to machine learning. We describe two supervised approaches for\
    \ tagging causal constructions and their arguments. Both approaches combine automatically\
    \ induced pattern-matching rules with statistical classifiers that learn the subtler\
    \ parameters of the constructions. Our results show that these approaches are\
    \ promising: they significantly outperform na\xEFve baselines for both construction\
    \ recognition and cause and effect head matches."
  author:
  - first: Jesse
    full: Jesse Dunietz
    id: jesse-dunietz
    last: Dunietz
  - first: Lori
    full: Lori Levin
    id: lori-levin
    last: Levin
  - first: Jaime
    full: Jaime Carbonell
    id: jaime-g-carbonell
    last: Carbonell
  author_string: Jesse Dunietz, Lori Levin, Jaime Carbonell
  bibkey: dunietz-etal-2017-automatically
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00050
  page_first: '117'
  page_last: '133'
  pages: "117\u2013133"
  paper_id: '9'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1009.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1009.jpg
  title: Automatically Tagging Constructions of Causation and Their Slot-Fillers
  title_html: Automatically Tagging Constructions of Causation and Their Slot-Fillers
  url: https://www.aclweb.org/anthology/Q17-1009
  year: '2017'
Q17-1010:
  abstract: Continuous word representations, trained on large unlabeled corpora are
    useful for many natural language processing tasks. Popular models that learn such
    representations ignore the morphology of words, by assigning a distinct vector
    to each word. This is a limitation, especially for languages with large vocabularies
    and many rare words. In this paper, we propose a new approach based on the skipgram
    model, where each word is represented as a bag of character n-grams. A vector
    representation is associated to each character n-gram; words being represented
    as the sum of these representations. Our method is fast, allowing to train models
    on large corpora quickly and allows us to compute word representations for words
    that did not appear in the training data. We evaluate our word representations
    on nine different languages, both on word similarity and analogy tasks. By comparing
    to recently proposed morphological word representations, we show that our vectors
    achieve state-of-the-art performance on these tasks.
  attachment:
  - filename: https://vimeo.com/234958672
    type: video
    url: https://vimeo.com/234958672
  author:
  - first: Piotr
    full: Piotr Bojanowski
    id: piotr-bojanowski
    last: Bojanowski
  - first: Edouard
    full: Edouard Grave
    id: edouard-grave
    last: Grave
  - first: Armand
    full: Armand Joulin
    id: armand-joulin
    last: Joulin
  - first: Tomas
    full: Tomas Mikolov
    id: tomas-mikolov
    last: Mikolov
  author_string: Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov
  bibkey: bojanowski-etal-2017-enriching
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00051
  page_first: '135'
  page_last: '146'
  pages: "135\u2013146"
  paper_id: '10'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1010.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1010.jpg
  title: Enriching Word Vectors with Subword Information
  title_html: Enriching Word Vectors with Subword Information
  url: https://www.aclweb.org/anthology/Q17-1010
  year: '2017'
Q17-1011:
  abstract: "We show how to predict the basic word-order facts of a novel language\
    \ given only a corpus of part-of-speech (POS) sequences. We predict how often\
    \ direct objects follow their verbs, how often adjectives follow their nouns,\
    \ and in general the directionalities of all dependency relations. Such typological\
    \ properties could be helpful in grammar induction. While such a problem is usually\
    \ regarded as unsupervised learning, our innovation is to treat it as supervised\
    \ learning, using a large collection of realistic synthetic languages as training\
    \ data. The supervised learner must identify surface features of a language\u2019\
    s POS sequence (hand-engineered or neural features) that correlate with the language\u2019\
    s deeper structure (latent trees). In the experiment, we show: 1) Given a small\
    \ set of real languages, it helps to add many synthetic languages to the training\
    \ data. 2) Our system is robust even when the POS sequences include noise. 3)\
    \ Our system on this task outperforms a grammar induction baseline by a large\
    \ margin."
  attachment:
  - filename: https://vimeo.com/234953702
    type: video
    url: https://vimeo.com/234953702
  author:
  - first: Dingquan
    full: Dingquan Wang
    id: dingquan-wang
    last: Wang
  - first: Jason
    full: Jason Eisner
    id: jason-eisner
    last: Eisner
  author_string: Dingquan Wang, Jason Eisner
  bibkey: wang-eisner-2017-fine
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00052
  page_first: '147'
  page_last: '161'
  pages: "147\u2013161"
  paper_id: '11'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1011.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1011.jpg
  title: 'Fine-Grained Prediction of Syntactic Typology: Discovering Latent Structure
    with Supervised Learning'
  title_html: 'Fine-Grained Prediction of Syntactic Typology: Discovering Latent Structure
    with Supervised Learning'
  url: https://www.aclweb.org/anthology/Q17-1011
  year: '2017'
Q17-1012:
  abstract: Sequential LSTMs have been extended to model tree structures, giving competitive
    results for a number of tasks. Existing methods model constituent trees by bottom-up
    combinations of constituent nodes, making direct use of input word information
    only for leaf nodes. This is different from sequential LSTMs, which contain references
    to input words for each node. In this paper, we propose a method for automatic
    head-lexicalization for tree-structure LSTMs, propagating head words from leaf
    nodes to every constituent node. In addition, enabled by head lexicalization,
    we build a tree LSTM in the top-down direction, which corresponds to bidirectional
    sequential LSTMs in structure. Experiments show that both extensions give better
    representations of tree structures. Our final model gives the best results on
    the Stanford Sentiment Treebank and highly competitive results on the TREC question
    type classification task.
  attachment:
  - filename: https://vimeo.com/234954994
    type: video
    url: https://vimeo.com/234954994
  author:
  - first: Zhiyang
    full: Zhiyang Teng
    id: zhiyang-teng
    last: Teng
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  author_string: Zhiyang Teng, Yue Zhang
  bibkey: teng-zhang-2017-head
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00053
  page_first: '163'
  page_last: '177'
  pages: "163\u2013177"
  paper_id: '12'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1012.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1012.jpg
  title: Head-Lexicalized Bidirectional Tree LSTMs
  title_html: Head-Lexicalized Bidirectional Tree <span class="acl-fixed-case">LSTM</span>s
  url: https://www.aclweb.org/anthology/Q17-1012
  year: '2017'
Q17-1013:
  abstract: "This paper presents a novel hybrid generative/discriminative model of\
    \ word segmentation based on nonparametric Bayesian methods. Unlike ordinary discriminative\
    \ word segmentation which relies only on labeled data, our semi-supervised model\
    \ also leverages a huge amounts of unlabeled text to automatically learn new \u201C\
    words\u201D, and further constrains them by using a labeled data to segment non-standard\
    \ texts such as those found in social networking services. Specifically, our hybrid\
    \ model combines a discriminative classifier (CRF; Lafferty et al. (2001) and\
    \ unsupervised word segmentation (NPYLM; Mochihashi et al. (2009)), with a transparent\
    \ exchange of information between these two model structures within the semi-supervised\
    \ framework (JESS-CM; Suzuki and Isozaki (2008)). We confirmed that it can appropriately\
    \ segment non-standard texts like those in Twitter and Weibo and has nearly state-of-the-art\
    \ accuracy on standard datasets in Japanese, Chinese, and Thai."
  attachment:
  - filename: https://vimeo.com/238235035
    type: video
    url: https://vimeo.com/238235035
  author:
  - first: Ryo
    full: Ryo Fujii
    id: ryo-fujii
    last: Fujii
  - first: Ryo
    full: Ryo Domoto
    id: ryo-domoto
    last: Domoto
  - first: Daichi
    full: Daichi Mochihashi
    id: daichi-mochihashi
    last: Mochihashi
  author_string: Ryo Fujii, Ryo Domoto, Daichi Mochihashi
  bibkey: fujii-etal-2017-nonparametric
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00054
  page_first: '179'
  page_last: '189'
  pages: "179\u2013189"
  paper_id: '13'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1013.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1013.jpg
  title: Nonparametric Bayesian Semi-supervised Word Segmentation
  title_html: Nonparametric <span class="acl-fixed-case">B</span>ayesian Semi-supervised
    Word Segmentation
  url: https://www.aclweb.org/anthology/Q17-1013
  year: '2017'
Q17-1014:
  abstract: "Much of scientific progress stems from previously published findings,\
    \ but searching through the vast sea of scientific publications is difficult.\
    \ We often rely on metrics of scholarly authority to find the prominent authors\
    \ but these authority indices do not differentiate authority based on research\
    \ topics. We present Latent Topical-Authority Indexing (LTAI) for jointly modeling\
    \ the topics, citations, and topical authority in a corpus of academic papers.\
    \ Compared to previous models, LTAI differs in two main aspects. First, it explicitly\
    \ models the generative process of the citations, rather than treating the citations\
    \ as given. Second, it models each author\u2019s influence on citations of a paper\
    \ based on the topics of the cited papers, as well as the citing papers. We fit\
    \ LTAI into four academic corpora: CORA, Arxiv Physics, PNAS, and Citeseer. We\
    \ compare the performance of LTAI against various baselines, starting with the\
    \ latent Dirichlet allocation, to the more advanced models including author-link\
    \ topic model and dynamic author citation topic model. The results show that LTAI\
    \ achieves improved accuracy over other similar models when predicting words,\
    \ citations and authors of publications."
  attachment:
  - filename: https://vimeo.com/238233899
    type: video
    url: https://vimeo.com/238233899
  author:
  - first: Jooyeon
    full: Jooyeon Kim
    id: jooyeon-kim
    last: Kim
  - first: Dongwoo
    full: Dongwoo Kim
    id: dongwoo-kim
    last: Kim
  - first: Alice
    full: Alice Oh
    id: alice-oh
    last: Oh
  author_string: Jooyeon Kim, Dongwoo Kim, Alice Oh
  bibkey: kim-etal-2017-joint
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00055
  page_first: '191'
  page_last: '204'
  pages: "191\u2013204"
  paper_id: '14'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1014.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1014.jpg
  title: Joint Modeling of Topics, Citations, and Topical Authority in Academic Corpora
  title_html: Joint Modeling of Topics, Citations, and Topical Authority in Academic
    Corpora
  url: https://www.aclweb.org/anthology/Q17-1014
  year: '2017'
Q17-1015:
  abstract: 'Translation quality estimation is a task of growing importance in NLP,
    due to its potential to reduce post-editing human effort in disruptive ways. However,
    this potential is currently limited by the relatively low accuracy of existing
    systems. In this paper, we achieve remarkable improvements by exploiting synergies
    between the related tasks of word-level quality estimation and automatic post-editing.
    First, we stack a new, carefully engineered, neural model into a rich feature-based
    word-level quality estimation system. Then, we use the output of an automatic
    post-editing system as an extra feature, obtaining striking results on WMT16:
    a word-level FMULT1 score of 57.47% (an absolute gain of +7.95% over the current
    state of the art), and a Pearson correlation score of 65.56% for sentence-level
    HTER prediction (an absolute gain of +13.36%).'
  attachment:
  - filename: https://vimeo.com/234955039
    type: video
    url: https://vimeo.com/234955039
  author:
  - first: "Andr\xE9 F. T."
    full: "Andr\xE9 F. T. Martins"
    id: andre-f-t-martins
    last: Martins
  - first: Marcin
    full: Marcin Junczys-Dowmunt
    id: marcin-junczys-dowmunt
    last: Junczys-Dowmunt
  - first: Fabio N.
    full: Fabio N. Kepler
    id: fabio-kepler
    last: Kepler
  - first: "Ram\xF3n"
    full: "Ram\xF3n Astudillo"
    id: ramon-astudillo1
    last: Astudillo
  - first: Chris
    full: Chris Hokamp
    id: chris-hokamp
    last: Hokamp
  - first: Roman
    full: Roman Grundkiewicz
    id: roman-grundkiewicz
    last: Grundkiewicz
  author_string: "Andr\xE9 F. T. Martins, Marcin Junczys-Dowmunt, Fabio N. Kepler,\
    \ Ram\xF3n Astudillo, Chris Hokamp, Roman Grundkiewicz"
  bibkey: martins-etal-2017-pushing
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00056
  page_first: '205'
  page_last: '218'
  pages: "205\u2013218"
  paper_id: '15'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1015.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1015.jpg
  title: Pushing the Limits of Translation Quality Estimation
  title_html: Pushing the Limits of Translation Quality Estimation
  url: https://www.aclweb.org/anthology/Q17-1015
  year: '2017'
Q17-1016:
  abstract: "Debate and deliberation play essential roles in politics and government,\
    \ but most models presume that debates are won mainly via superior style or agenda\
    \ control. Ideally, however, debates would be won on the merits, as a function\
    \ of which side has the stronger arguments. We propose a predictive model of debate\
    \ that estimates the effects of linguistic features and the latent persuasive\
    \ strengths of different topics, as well as the interactions between the two.\
    \ Using a dataset of 118 Oxford-style debates, our model\u2019s combination of\
    \ content (as latent topics) and style (as linguistic features) allows us to predict\
    \ audience-adjudicated winners with 74% accuracy, significantly outperforming\
    \ linguistic features alone (66%). Our model finds that winning sides employ stronger\
    \ arguments, and allows us to identify the linguistic features associated with\
    \ strong or weak arguments."
  attachment:
  - filename: https://vimeo.com/234953410
    type: video
    url: https://vimeo.com/234953410
  author:
  - first: Lu
    full: Lu Wang
    id: lu-wang
    last: Wang
  - first: Nick
    full: Nick Beauchamp
    id: nick-beauchamp
    last: Beauchamp
  - first: Sarah
    full: Sarah Shugars
    id: sarah-shugars
    last: Shugars
  - first: Kechen
    full: Kechen Qin
    id: kechen-qin
    last: Qin
  author_string: Lu Wang, Nick Beauchamp, Sarah Shugars, Kechen Qin
  bibkey: wang-etal-2017-winning
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00057
  page_first: '219'
  page_last: '232'
  pages: "219\u2013232"
  paper_id: '16'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1016.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1016.jpg
  title: 'Winning on the Merits: The Joint Effects of Content and Style on Debate
    Outcomes'
  title_html: 'Winning on the Merits: The Joint Effects of Content and Style on Debate
    Outcomes'
  url: https://www.aclweb.org/anthology/Q17-1016
  year: '2017'
Q17-1017:
  abstract: "Our goal is to construct a domain-targeted, high precision knowledge\
    \ base (KB), containing general (subject,predicate,object) statements about the\
    \ world, in support of a downstream question-answering (QA) application. Despite\
    \ recent advances in information extraction (IE) techniques, no suitable resource\
    \ for our task already exists; existing resources are either too noisy, too named-entity\
    \ centric, or too incomplete, and typically have not been constructed with a clear\
    \ scope or purpose. To address these, we have created a domain-targeted, high\
    \ precision knowledge extraction pipeline, leveraging Open IE, crowdsourcing,\
    \ and a novel canonical schema learning algorithm (called CASI), that produces\
    \ high precision knowledge targeted to a particular domain - in our case, elementary\
    \ science. To measure the KB\u2019s coverage of the target domain\u2019s knowledge\
    \ (its \u201Ccomprehensiveness\u201D with respect to science) we measure recall\
    \ with respect to an independent corpus of domain text, and show that our pipeline\
    \ produces output with over 80% precision and 23% recall with respect to that\
    \ target, a substantially higher coverage of tuple-expressible science knowledge\
    \ than other comparable resources. We have made the KB publicly available."
  author:
  - first: Bhavana
    full: Bhavana Dalvi Mishra
    id: bhavana-dalvi
    last: Dalvi Mishra
  - first: Niket
    full: Niket Tandon
    id: niket-tandon
    last: Tandon
  - first: Peter
    full: Peter Clark
    id: peter-clark
    last: Clark
  author_string: Bhavana Dalvi Mishra, Niket Tandon, Peter Clark
  bibkey: dalvi-mishra-etal-2017-domain
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00058
  page_first: '233'
  page_last: '246'
  pages: "233\u2013246"
  paper_id: '17'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1017.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1017.jpg
  title: Domain-Targeted, High Precision Knowledge Extraction
  title_html: Domain-Targeted, High Precision Knowledge Extraction
  url: https://www.aclweb.org/anthology/Q17-1017
  year: '2017'
Q17-1018:
  abstract: In this paper we propose and carefully evaluate a sequence labeling framework
    which solely utilizes sparse indicator features derived from dense distributed
    word representations. The proposed model obtains (near) state-of-the art performance
    for both part-of-speech tagging and named entity recognition for a variety of
    languages. Our model relies only on a few thousand sparse coding-derived features,
    without applying any modification of the word representations employed for the
    different tasks. The proposed model has favorable generalization properties as
    it retains over 89.8% of its average POS tagging accuracy when trained at 1.2%
    of the total available training data, i.e. 150 sentences per language.
  attachment:
  - filename: https://vimeo.com/234952125
    type: video
    url: https://vimeo.com/234952125
  - filename: Q17-1018.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/Q17-1018.Presentation.pdf
  author:
  - first: "G\xE1bor"
    full: "G\xE1bor Berend"
    id: gabor-berend
    last: Berend
  author_string: "G\xE1bor Berend"
  bibkey: berend-2017-sparse
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00059
  page_first: '247'
  page_last: '261'
  pages: "247\u2013261"
  paper_id: '18'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1018.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1018.jpg
  title: Sparse Coding of Neural Word Embeddings for Multilingual Sequence Labeling
  title_html: Sparse Coding of Neural Word Embeddings for Multilingual Sequence Labeling
  url: https://www.aclweb.org/anthology/Q17-1018
  year: '2017'
Q17-1019:
  abstract: "Pruning hypotheses during dynamic programming is commonly used to speed\
    \ up inference in settings such as parsing. Unlike prior work, we train a pruning\
    \ policy under an objective that measures end-to-end performance: we search for\
    \ a fast and accurate policy. This poses a difficult machine learning problem,\
    \ which we tackle with the lols algorithm. lols training must continually compute\
    \ the effects of changing pruning decisions: we show how to make this efficient\
    \ in the constituency parsing setting, via dynamic programming and change propagation\
    \ algorithms. We find that optimizing end-to-end performance in this way leads\
    \ to a better Pareto frontier\u2014i.e., parsers which are more accurate for a\
    \ given runtime."
  attachment:
  - filename: https://vimeo.com/234953773
    type: video
    url: https://vimeo.com/234953773
  author:
  - first: Tim
    full: Tim Vieira
    id: tim-vieira
    last: Vieira
  - first: Jason
    full: Jason Eisner
    id: jason-eisner
    last: Eisner
  author_string: Tim Vieira, Jason Eisner
  bibkey: vieira-eisner-2017-learning
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00060
  page_first: '263'
  page_last: '278'
  pages: "263\u2013278"
  paper_id: '19'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1019.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1019.jpg
  title: 'Learning to Prune: Exploring the Frontier of Fast and Accurate Parsing'
  title_html: 'Learning to Prune: Exploring the Frontier of Fast and Accurate Parsing'
  url: https://www.aclweb.org/anthology/Q17-1019
  year: '2017'
Q17-1020:
  abstract: 'We describe a simple but effective method for cross-lingual syntactic
    transfer of dependency parsers, in the scenario where a large amount of translation
    data is not available. This method makes use of three steps: 1) a method for deriving
    cross-lingual word clusters, which can then be used in a multilingual parser;
    2) a method for transferring lexical information from a target language to source
    language treebanks; 3) a method for integrating these steps with the density-driven
    annotation projection method of Rasooli and Collins (2015). Experiments show improvements
    over the state-of-the-art in several languages used in previous work, in a setting
    where the only source of translation data is the Bible, a considerably smaller
    corpus than the Europarl corpus used in previous work. Results using the Europarl
    corpus as a source of translation data show additional improvements over the results
    of Rasooli and Collins (2015). We conclude with results on 38 datasets from the
    Universal Dependencies corpora.'
  attachment:
  - filename: https://vimeo.com/276419865
    type: video
    url: https://vimeo.com/276419865
  author:
  - first: Mohammad Sadegh
    full: Mohammad Sadegh Rasooli
    id: mohammad-sadegh-rasooli
    last: Rasooli
  - first: Michael
    full: Michael Collins
    id: michael-collins
    last: Collins
  author_string: Mohammad Sadegh Rasooli, Michael Collins
  bibkey: rasooli-collins-2017-cross
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00061
  page_first: '279'
  page_last: '293'
  pages: "279\u2013293"
  paper_id: '20'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1020.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1020.jpg
  title: Cross-Lingual Syntactic Transfer with Limited Resources
  title_html: Cross-Lingual Syntactic Transfer with Limited Resources
  url: https://www.aclweb.org/anthology/Q17-1020
  year: '2017'
Q17-1021:
  abstract: "Variation in language is ubiquitous, particularly in newer forms of writing\
    \ such as social media. Fortunately, variation is not random; it is often linked\
    \ to social properties of the author. In this paper, we show how to exploit social\
    \ networks to make sentiment analysis more robust to social language variation.\
    \ The key idea is linguistic homophily: the tendency of socially linked individuals\
    \ to use language in similar ways. We formalize this idea in a novel attention-based\
    \ neural network architecture, in which attention is divided among several basis\
    \ models, depending on the author\u2019s position in the social network. This\
    \ has the effect of smoothing the classification function across the social network,\
    \ and makes it possible to induce personalized classifiers even for authors for\
    \ whom there is no labeled data or demographic metadata. This model significantly\
    \ improves the accuracies of sentiment analysis on Twitter and on review data."
  attachment:
  - filename: https://vimeo.com/234952612
    type: video
    url: https://vimeo.com/234952612
  author:
  - first: Yi
    full: Yi Yang
    id: yi-yang
    last: Yang
  - first: Jacob
    full: Jacob Eisenstein
    id: jacob-eisenstein
    last: Eisenstein
  author_string: Yi Yang, Jacob Eisenstein
  bibkey: yang-eisenstein-2017-overcoming
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00062
  page_first: '295'
  page_last: '307'
  pages: "295\u2013307"
  paper_id: '21'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1021.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1021.jpg
  title: Overcoming Language Variation in Sentiment Analysis with Social Attention
  title_html: Overcoming Language Variation in Sentiment Analysis with Social Attention
  url: https://www.aclweb.org/anthology/Q17-1021
  year: '2017'
Q17-1022:
  abstract: We present Attract-Repel, an algorithm for improving the semantic quality
    of word vectors by injecting constraints extracted from lexical resources. Attract-Repel
    facilitates the use of constraints from mono- and cross-lingual resources, yielding
    semantically specialized cross-lingual vector spaces. Our evaluation shows that
    the method can make use of existing cross-lingual lexicons to construct high-quality
    vector spaces for a plethora of different languages, facilitating semantic transfer
    from high- to lower-resource ones. The effectiveness of our approach is demonstrated
    with state-of-the-art results on semantic similarity datasets in six languages.
    We next show that Attract-Repel-specialized vectors boost performance in the downstream
    task of dialogue state tracking (DST) across multiple languages. Finally, we show
    that cross-lingual vector spaces produced by our algorithm facilitate the training
    of multilingual DST models, which brings further performance improvements.
  author:
  - first: Nikola
    full: "Nikola Mrk\u0161i\u0107"
    id: nikola-mrksic
    last: "Mrk\u0161i\u0107"
  - first: Ivan
    full: "Ivan Vuli\u0107"
    id: ivan-vulic
    last: "Vuli\u0107"
  - first: Diarmuid
    full: "Diarmuid \xD3 S\xE9aghdha"
    id: diarmuid-o-seaghdha
    last: "\xD3 S\xE9aghdha"
  - first: Ira
    full: Ira Leviant
    id: ira-leviant
    last: Leviant
  - first: Roi
    full: Roi Reichart
    id: roi-reichart
    last: Reichart
  - first: Milica
    full: "Milica Ga\u0161i\u0107"
    id: milica-gasic
    last: "Ga\u0161i\u0107"
  - first: Anna
    full: Anna Korhonen
    id: anna-korhonen
    last: Korhonen
  - first: Steve
    full: Steve Young
    id: steve-young
    last: Young
  author_string: "Nikola Mrk\u0161i\u0107, Ivan Vuli\u0107, Diarmuid \xD3 S\xE9aghdha,\
    \ Ira Leviant, Roi Reichart, Milica Ga\u0161i\u0107, Anna Korhonen, Steve Young"
  bibkey: mrksic-etal-2017-semantic
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00063
  page_first: '309'
  page_last: '324'
  pages: "309\u2013324"
  paper_id: '22'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1022.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1022.jpg
  title: Semantic Specialization of Distributional Word Vector Spaces using Monolingual
    and Cross-Lingual Constraints
  title_html: Semantic Specialization of Distributional Word Vector Spaces using Monolingual
    and Cross-Lingual Constraints
  url: https://www.aclweb.org/anthology/Q17-1022
  year: '2017'
Q17-1023:
  abstract: 'We present a model of pragmatic referring expression interpretation in
    a grounded communication task (identifying colors from descriptions) that draws
    upon predictions from two recurrent neural network classifiers, a speaker and
    a listener, unified by a recursive pragmatic reasoning framework. Experiments
    show that this combined pragmatic model interprets color descriptions more accurately
    than the classifiers from which it is built, and that much of this improvement
    results from combining the speaker and listener perspectives. We observe that
    pragmatic reasoning helps primarily in the hardest cases: when the model must
    distinguish very similar colors, or when few utterances adequately express the
    target color. Our findings make use of a newly-collected corpus of human utterances
    in color reference games, which exhibit a variety of pragmatic behaviors. We also
    show that the embedded speaker model reproduces many of these pragmatic behaviors.'
  attachment:
  - filename: https://vimeo.com/238230459
    type: video
    url: https://vimeo.com/238230459
  author:
  - first: Will
    full: Will Monroe
    id: will-monroe
    last: Monroe
  - first: Robert X.D.
    full: Robert X.D. Hawkins
    id: robert-x-d-hawkins
    last: Hawkins
  - first: Noah D.
    full: Noah D. Goodman
    id: noah-goodman
    last: Goodman
  - first: Christopher
    full: Christopher Potts
    id: christopher-potts
    last: Potts
  author_string: Will Monroe, Robert X.D. Hawkins, Noah D. Goodman, Christopher Potts
  bibkey: monroe-etal-2017-colors
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00064
  page_first: '325'
  page_last: '338'
  pages: "325\u2013338"
  paper_id: '23'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1023.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1023.jpg
  title: 'Colors in Context: A Pragmatic Neural Model for Grounded Language Understanding'
  title_html: 'Colors in Context: A Pragmatic Neural Model for Grounded Language Understanding'
  url: https://www.aclweb.org/anthology/Q17-1023
  year: '2017'
Q17-1024:
  abstract: "We propose a simple solution to use a single Neural Machine Translation\
    \ (NMT) model to translate between multiple languages. Our solution requires no\
    \ changes to the model architecture from a standard NMT system but instead introduces\
    \ an artificial token at the beginning of the input sentence to specify the required\
    \ target language. Using a shared wordpiece vocabulary, our approach enables Multilingual\
    \ NMT systems using a single model. On the WMT\u201914 benchmarks, a single multilingual\
    \ model achieves comparable performance for English\u2192French and surpasses\
    \ state-of-theart results for English\u2192German. Similarly, a single multilingual\
    \ model surpasses state-of-the-art results for French\u2192English and German\u2192\
    English on WMT\u201914 and WMT\u201915 benchmarks, respectively. On production\
    \ corpora, multilingual models of up to twelve language pairs allow for better\
    \ translation of many individual pairs. Our models can also learn to perform implicit\
    \ bridging between language pairs never seen explicitly during training, showing\
    \ that transfer learning and zero-shot translation is possible for neural translation.\
    \ Finally, we show analyses that hints at a universal interlingua representation\
    \ in our models and also show some interesting examples when mixing languages."
  attachment:
  - filename: https://vimeo.com/238233299
    type: video
    url: https://vimeo.com/238233299
  author:
  - first: Melvin
    full: Melvin Johnson
    id: melvin-johnson
    last: Johnson
  - first: Mike
    full: Mike Schuster
    id: mike-schuster
    last: Schuster
  - first: Quoc V.
    full: Quoc V. Le
    id: quoc-le
    last: Le
  - first: Maxim
    full: Maxim Krikun
    id: maxim-krikun
    last: Krikun
  - first: Yonghui
    full: Yonghui Wu
    id: yonghui-wu
    last: Wu
  - first: Zhifeng
    full: Zhifeng Chen
    id: zhifeng-chen
    last: Chen
  - first: Nikhil
    full: Nikhil Thorat
    id: nikhil-thorat
    last: Thorat
  - first: Fernanda
    full: "Fernanda Vi\xE9gas"
    id: fernanda-viegas
    last: "Vi\xE9gas"
  - first: Martin
    full: Martin Wattenberg
    id: martin-wattenberg
    last: Wattenberg
  - first: Greg
    full: Greg Corrado
    id: greg-corrado
    last: Corrado
  - first: Macduff
    full: Macduff Hughes
    id: macduff-hughes
    last: Hughes
  - first: Jeffrey
    full: Jeffrey Dean
    id: jeffrey-dean
    last: Dean
  author_string: "Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui\
    \ Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi\xE9gas, Martin Wattenberg, Greg\
    \ Corrado, Macduff Hughes, Jeffrey Dean"
  bibkey: johnson-etal-2017-googles
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00065
  page_first: '339'
  page_last: '351'
  pages: "339\u2013351"
  paper_id: '24'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1024.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1024.jpg
  title: "Google\u2019s Multilingual Neural Machine Translation System: Enabling Zero-Shot\
    \ Translation"
  title_html: "<span class=\"acl-fixed-case\">G</span>oogle\u2019s Multilingual Neural\
    \ Machine Translation System: Enabling Zero-Shot Translation"
  url: https://www.aclweb.org/anthology/Q17-1024
  year: '2017'
Q17-1025:
  abstract: 'This paper focuses on unsupervised modeling of morphological families,
    collectively comprising a forest over the language vocabulary. This formulation
    enables us to capture edge-wise properties reflecting single-step morphological
    derivations, along with global distributional properties of the entire forest.
    These global properties constrain the size of the affix set and encourage formation
    of tight morphological families. The resulting objective is solved using Integer
    Linear Programming (ILP) paired with contrastive estimation. We train the model
    by alternating between optimizing the local log-linear model and the global ILP
    objective. We evaluate our system on three tasks: root detection, clustering of
    morphological families, and segmentation. Our experiments demonstrate that our
    model yields consistent gains in all three tasks compared with the best published
    results.'
  attachment:
  - filename: https://vimeo.com/234952859
    type: video
    url: https://vimeo.com/234952859
  author:
  - first: Jiaming
    full: Jiaming Luo
    id: jiaming-luo
    last: Luo
  - first: Karthik
    full: Karthik Narasimhan
    id: karthik-narasimhan
    last: Narasimhan
  - first: Regina
    full: Regina Barzilay
    id: regina-barzilay
    last: Barzilay
  author_string: Jiaming Luo, Karthik Narasimhan, Regina Barzilay
  bibkey: luo-etal-2017-unsupervised
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00066
  page_first: '353'
  page_last: '364'
  pages: "353\u2013364"
  paper_id: '25'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1025.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1025.jpg
  title: Unsupervised Learning of Morphological Forests
  title_html: Unsupervised Learning of Morphological Forests
  url: https://www.aclweb.org/anthology/Q17-1025
  year: '2017'
Q17-1026:
  abstract: "Most existing machine translation systems operate at the level of words,\
    \ relying on explicit segmentation to extract tokens. We introduce a neural machine\
    \ translation (NMT) model that maps a source character sequence to a target character\
    \ sequence without any segmentation. We employ a character-level convolutional\
    \ network with max-pooling at the encoder to reduce the length of source representation,\
    \ allowing the model to be trained at a speed comparable to subword-level models\
    \ while capturing local regularities. Our character-to-character model outperforms\
    \ a recently proposed baseline with a subword-level encoder on WMT\u201915 DE-EN\
    \ and CS-EN, and gives comparable performance on FI-EN and RU-EN. We then demonstrate\
    \ that it is possible to share a single character-level encoder across multiple\
    \ languages by training a model on a many-to-one translation task. In this multilingual\
    \ setting, the character-level encoder significantly outperforms the subword-level\
    \ encoder on all the language pairs. We observe that on CS-EN, FI-EN and RU-EN,\
    \ the quality of the multilingual character-level translation even surpasses the\
    \ models specifically trained on that language pair alone, both in terms of the\
    \ BLEU score and human judgment."
  attachment:
  - filename: https://vimeo.com/234955246
    type: video
    url: https://vimeo.com/234955246
  author:
  - first: Jason
    full: Jason Lee
    id: jason-lee
    last: Lee
  - first: Kyunghyun
    full: Kyunghyun Cho
    id: kyunghyun-cho
    last: Cho
  - first: Thomas
    full: Thomas Hofmann
    id: thomas-hofmann
    last: Hofmann
  author_string: Jason Lee, Kyunghyun Cho, Thomas Hofmann
  bibkey: lee-etal-2017-fully
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00067
  page_first: '365'
  page_last: '378'
  pages: "365\u2013378"
  paper_id: '26'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1026.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1026.jpg
  title: Fully Character-Level Neural Machine Translation without Explicit Segmentation
  title_html: Fully Character-Level Neural Machine Translation without Explicit Segmentation
  url: https://www.aclweb.org/anthology/Q17-1026
  year: '2017'
Q17-1027:
  abstract: 'Humans have the capacity to draw common-sense inferences from natural
    language: various things that are likely but not certain to hold based on established
    discourse, and are rarely stated explicitly. We propose an evaluation of automated
    common-sense inference based on an extension of recognizing textual entailment:
    predicting ordinal human responses on the subjective likelihood of an inference
    holding in a given context. We describe a framework for extracting common-sense
    knowledge from corpora, which is then used to construct a dataset for this ordinal
    entailment task. We train a neural sequence-to-sequence model on this dataset,
    which we use to score and generate possible inferences. Further, we annotate subsets
    of previously established datasets via our ordinal annotation protocol in order
    to then analyze the distinctions between these and what we have constructed.'
  author:
  - first: Sheng
    full: Sheng Zhang
    id: sheng-zhang
    last: Zhang
  - first: Rachel
    full: Rachel Rudinger
    id: rachel-rudinger
    last: Rudinger
  - first: Kevin
    full: Kevin Duh
    id: kevin-duh
    last: Duh
  - first: Benjamin
    full: Benjamin Van Durme
    id: benjamin-van-durme
    last: Van Durme
  author_string: Sheng Zhang, Rachel Rudinger, Kevin Duh, Benjamin Van Durme
  bibkey: zhang-etal-2017-ordinal
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00068
  page_first: '379'
  page_last: '395'
  pages: "379\u2013395"
  paper_id: '27'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1027.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1027.jpg
  title: Ordinal Common-sense Inference
  title_html: Ordinal Common-sense Inference
  url: https://www.aclweb.org/anthology/Q17-1027
  year: '2017'
Q17-1028:
  abstract: We describe a neural network model that jointly learns distributed representations
    of texts and knowledge base (KB) entities. Given a text in the KB, we train our
    proposed model to predict entities that are relevant to the text. Our model is
    designed to be generic with the ability to address various NLP tasks with ease.
    We train the model using a large corpus of texts and their entity annotations
    extracted from Wikipedia. We evaluated the model on three important NLP tasks
    (i.e., sentence textual similarity, entity linking, and factoid question answering)
    involving both unsupervised and supervised settings. As a result, we achieved
    state-of-the-art results on all three of these tasks. Our code and trained models
    are publicly available for further academic research.
  author:
  - first: Ikuya
    full: Ikuya Yamada
    id: ikuya-yamada
    last: Yamada
  - first: Hiroyuki
    full: Hiroyuki Shindo
    id: hiroyuki-shindo
    last: Shindo
  - first: Hideaki
    full: Hideaki Takeda
    id: hideaki-takeda
    last: Takeda
  - first: Yoshiyasu
    full: Yoshiyasu Takefuji
    id: yoshiyasu-takefuji
    last: Takefuji
  author_string: Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, Yoshiyasu Takefuji
  bibkey: yamada-etal-2017-learning
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00069
  page_first: '397'
  page_last: '411'
  pages: "397\u2013411"
  paper_id: '28'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1028.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1028.jpg
  title: Learning Distributed Representations of Texts and Entities from Knowledge
    Base
  title_html: Learning Distributed Representations of Texts and Entities from Knowledge
    Base
  url: https://www.aclweb.org/anthology/Q17-1028
  year: '2017'
Q17-1029:
  abstract: Both bottom-up and top-down strategies have been used for neural transition-based
    constituent parsing. The parsing strategies differ in terms of the order in which
    they recognize productions in the derivation tree, where bottom-up strategies
    and top-down strategies take post-order and pre-order traversal over trees, respectively.
    Bottom-up parsers benefit from rich features from readily built partial parses,
    but lack lookahead guidance in the parsing process; top-down parsers benefit from
    non-local guidance for local decisions, but rely on a strong encoder over the
    input to predict a constituent hierarchy before its construction. To mitigate
    both issues, we propose a novel parsing system based on in-order traversal over
    syntactic trees, designing a set of transition actions to find a compromise between
    bottom-up constituent information and top-down lookahead information. Based on
    stack-LSTM, our psycholinguistically motivated constituent parsing system achieves
    91.8 F1 on the WSJ benchmark. Furthermore, the system achieves 93.6 F1 with supervised
    reranking and 94.2 F1 with semi-supervised reranking, which are the best results
    on the WSJ benchmark.
  author:
  - first: Jiangming
    full: Jiangming Liu
    id: jiangming-liu
    last: Liu
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  author_string: Jiangming Liu, Yue Zhang
  bibkey: liu-zhang-2017-order
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00070
  page_first: '413'
  page_last: '424'
  pages: "413\u2013424"
  paper_id: '29'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1029.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1029.jpg
  title: In-Order Transition-based Constituent Parsing
  title_html: In-Order Transition-based Constituent Parsing
  url: https://www.aclweb.org/anthology/Q17-1029
  year: '2017'
Q17-1030:
  abstract: We introduce a method for measuring the correspondence between low-level
    speech features and human perception, using a cognitive model of speech perception
    implemented directly on speech recordings. We evaluate two speaker normalization
    techniques using this method and find that in both cases, speech features that
    are normalized across speakers predict human data better than unnormalized speech
    features, consistent with previous research. Results further reveal differences
    across normalization methods in how well each predicts human data. This work provides
    a new framework for evaluating low-level representations of speech on their match
    to human perception, and lays the groundwork for creating more ecologically valid
    models of speech perception.
  author:
  - first: Caitlin
    full: Caitlin Richter
    id: caitlin-richter
    last: Richter
  - first: Naomi H.
    full: Naomi H. Feldman
    id: naomi-feldman
    last: Feldman
  - first: Harini
    full: Harini Salgado
    id: harini-salgado
    last: Salgado
  - first: Aren
    full: Aren Jansen
    id: aren-jansen
    last: Jansen
  author_string: Caitlin Richter, Naomi H. Feldman, Harini Salgado, Aren Jansen
  bibkey: richter-etal-2017-evaluating
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00071
  page_first: '425'
  page_last: '440'
  pages: "425\u2013440"
  paper_id: '30'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1030.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1030.jpg
  title: Evaluating Low-Level Speech Features Against Human Perceptual Data
  title_html: Evaluating Low-Level Speech Features Against Human Perceptual Data
  url: https://www.aclweb.org/anthology/Q17-1030
  year: '2017'
Q17-1031:
  abstract: General treebank analyses are graph structured, but parsers are typically
    restricted to tree structures for efficiency and modeling reasons. We propose
    a new representation and algorithm for a class of graph structures that is flexible
    enough to cover almost all treebank structures, while still admitting efficient
    learning and inference. In particular, we consider directed, acyclic, one-endpoint-crossing
    graph structures, which cover most long-distance dislocation, shared argumentation,
    and similar tree-violating linguistic phenomena. We describe how to convert phrase
    structure parses, including traces, to our new representation in a reversible
    manner. Our dynamic program uniquely decomposes structures, is sound and complete,
    and covers 97.3% of the Penn English Treebank. We also implement a proof-of-concept
    parser that recovers a range of null elements and trace types.
  attachment:
  - filename: https://vimeo.com/238235203
    type: video
    url: https://vimeo.com/238235203
  author:
  - first: Jonathan K.
    full: Jonathan K. Kummerfeld
    id: jonathan-k-kummerfeld
    last: Kummerfeld
  - first: Dan
    full: Dan Klein
    id: dan-klein
    last: Klein
  author_string: Jonathan K. Kummerfeld, Dan Klein
  bibkey: kummerfeld-klein-2017-parsing
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00072
  page_first: '441'
  page_last: '454'
  pages: "441\u2013454"
  paper_id: '31'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1031.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1031.jpg
  title: 'Parsing with Traces: An O(n4) Algorithm and a Structural Representation'
  title_html: 'Parsing with Traces: An O(n4) Algorithm and a Structural Representation'
  url: https://www.aclweb.org/anthology/Q17-1031
  year: '2017'
Q17-1032:
  abstract: We present a new model for acquiring comprehensive multiword lexicons
    from large corpora based on competition among n-gram candidates. In contrast to
    the standard approach of simple ranking by association measure, in our model n-grams
    are arranged in a lattice structure based on subsumption and overlap relationships,
    with nodes inhibiting other nodes in their vicinity when they are selected as
    a lexical item. We show how the configuration of such a lattice can be optimized
    tractably, and demonstrate using annotations of sampled n-grams that our method
    consistently outperforms alternatives by at least 0.05 F-score across several
    corpora and languages.
  attachment:
  - filename: https://vimeo.com/277673914
    type: video
    url: https://vimeo.com/277673914
  author:
  - first: Julian
    full: Julian Brooke
    id: julian-brooke
    last: Brooke
  - first: Jan
    full: "Jan \u0160najder"
    id: jan-snajder
    last: "\u0160najder"
  - first: Timothy
    full: Timothy Baldwin
    id: timothy-baldwin
    last: Baldwin
  author_string: "Julian Brooke, Jan \u0160najder, Timothy Baldwin"
  bibkey: brooke-etal-2017-unsupervised
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00073
  page_first: '455'
  page_last: '470'
  pages: "455\u2013470"
  paper_id: '32'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1032.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1032.jpg
  title: Unsupervised Acquisition of Comprehensive Multiword Lexicons using Competition
    in an n-gram Lattice
  title_html: Unsupervised Acquisition of Comprehensive Multiword Lexicons using Competition
    in an n-gram Lattice
  url: https://www.aclweb.org/anthology/Q17-1032
  year: '2017'
Q17-1033:
  abstract: 'With the ever growing amount of textual data from a large variety of
    languages, domains, and genres, it has become standard to evaluate NLP algorithms
    on multiple datasets in order to ensure a consistent performance across heterogeneous
    setups. However, such multiple comparisons pose significant challenges to traditional
    statistical analysis methods in NLP and can lead to erroneous conclusions. In
    this paper we propose a Replicability Analysis framework for a statistically sound
    analysis of multiple comparisons between algorithms for NLP tasks. We discuss
    the theoretical advantages of this framework over the current, statistically unjustified,
    practice in the NLP literature, and demonstrate its empirical value across four
    applications: multi-domain dependency parsing, multilingual POS tagging, cross-domain
    sentiment classification and word similarity prediction.'
  attachment:
  - filename: https://vimeo.com/285803652
    type: video
    url: https://vimeo.com/285803652
  author:
  - first: Rotem
    full: Rotem Dror
    id: rotem-dror
    last: Dror
  - first: Gili
    full: Gili Baumer
    id: gili-baumer
    last: Baumer
  - first: Marina
    full: Marina Bogomolov
    id: marina-bogomolov
    last: Bogomolov
  - first: Roi
    full: Roi Reichart
    id: roi-reichart
    last: Reichart
  author_string: Rotem Dror, Gili Baumer, Marina Bogomolov, Roi Reichart
  bibkey: dror-etal-2017-replicability
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00074
  page_first: '471'
  page_last: '486'
  pages: "471\u2013486"
  paper_id: '33'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1033.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1033.jpg
  title: 'Replicability Analysis for Natural Language Processing: Testing Significance
    with Multiple Datasets'
  title_html: 'Replicability Analysis for Natural Language Processing: Testing Significance
    with Multiple Datasets'
  url: https://www.aclweb.org/anthology/Q17-1033
  year: '2017'
Q17-1034:
  abstract: "We present a new framework to induce an in-domain phrase table from in-domain\
    \ monolingual data that can be used to adapt a general-domain statistical machine\
    \ translation system to the targeted domain. Our method first compiles sets of\
    \ phrases in source and target languages separately and generates candidate phrase\
    \ pairs by taking the Cartesian product of the two phrase sets. It then computes\
    \ inexpensive features for each candidate phrase pair and filters them using a\
    \ supervised classifier in order to induce an in-domain phrase table. We experimented\
    \ on the language pair English\u2013French, both translation directions, in two\
    \ domains and obtained consistently better results than a strong baseline system\
    \ that uses an in-domain bilingual lexicon. We also conducted an error analysis\
    \ that showed the induced phrase tables proposed useful translations, especially\
    \ for words and phrases unseen in the parallel data used to train the general-domain\
    \ baseline system."
  author:
  - first: Benjamin
    full: Benjamin Marie
    id: benjamin-marie
    last: Marie
  - first: Atsushi
    full: Atsushi Fujita
    id: atsushi-fujita
    last: Fujita
  author_string: Benjamin Marie, Atsushi Fujita
  bibkey: marie-fujita-2017-phrase
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00075
  page_first: '487'
  page_last: '500'
  pages: "487\u2013500"
  paper_id: '34'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1034.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1034.jpg
  title: Phrase Table Induction Using In-Domain Monolingual Data for Domain Adaptation
    in Statistical Machine Translation
  title_html: Phrase Table Induction Using In-Domain Monolingual Data for Domain Adaptation
    in Statistical Machine Translation
  url: https://www.aclweb.org/anthology/Q17-1034
  year: '2017'
Q17-1035:
  abstract: Current word alignment models do not distinguish between different types
    of alignment links. In this paper, we provide a new probabilistic model for word
    alignment where word alignments are associated with linguistically motivated alignment
    types. We propose a novel task of joint prediction of word alignment and alignment
    types and propose novel semi-supervised learning algorithms for this task. We
    also solve a sub-task of predicting the alignment type given an aligned word pair.
    In our experimental results, the generative models we introduce to model alignment
    types significantly outperform the models without alignment types.
  author:
  - first: Anahita
    full: Anahita Mansouri Bigvand
    id: anahita-mansouri-bigvand
    last: Mansouri Bigvand
  - first: Te
    full: Te Bu
    id: te-bu
    last: Bu
  - first: Anoop
    full: Anoop Sarkar
    id: anoop-sarkar
    last: Sarkar
  author_string: Anahita Mansouri Bigvand, Te Bu, Anoop Sarkar
  bibkey: mansouri-bigvand-etal-2017-joint
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00076
  page_first: '501'
  page_last: '514'
  pages: "501\u2013514"
  paper_id: '35'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1035.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1035.jpg
  title: Joint Prediction of Word Alignment with Alignment Types
  title_html: Joint Prediction of Word Alignment with Alignment Types
  url: https://www.aclweb.org/anthology/Q17-1035
  year: '2017'
Q17-1036:
  abstract: We introduce a neural method for transfer learning between two (source
    and target) classification tasks or aspects over the same domain. Rather than
    training on target labels, we use a few keywords pertaining to source and target
    aspects indicating sentence relevance instead of document class labels. Documents
    are encoded by learning to embed and softly select relevant sentences in an aspect-dependent
    manner. A shared classifier is trained on the source encoded documents and labels,
    and applied to target encoded documents. We ensure transfer through aspect-adversarial
    training so that encoded documents are, as sets, aspect-invariant. Experimental
    results demonstrate that our approach outperforms different baselines and model
    variants on two datasets, yielding an improvement of 27% on a pathology dataset
    and 5% on a review dataset.
  attachment:
  - filename: https://vimeo.com/276406923
    type: video
    url: https://vimeo.com/276406923
  author:
  - first: Yuan
    full: Yuan Zhang
    id: yuan-zhang
    last: Zhang
  - first: Regina
    full: Regina Barzilay
    id: regina-barzilay
    last: Barzilay
  - first: Tommi
    full: Tommi Jaakkola
    id: tommi-jaakkola
    last: Jaakkola
  author_string: Yuan Zhang, Regina Barzilay, Tommi Jaakkola
  bibkey: zhang-etal-2017-aspect
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00077
  page_first: '515'
  page_last: '528'
  pages: "515\u2013528"
  paper_id: '36'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1036.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1036.jpg
  title: Aspect-augmented Adversarial Networks for Domain Adaptation
  title_html: Aspect-augmented Adversarial Networks for Domain Adaptation
  url: https://www.aclweb.org/anthology/Q17-1036
  year: '2017'
Q17-1037:
  abstract: While generative models such as Latent Dirichlet Allocation (LDA) have
    proven fruitful in topic modeling, they often require detailed assumptions and
    careful specification of hyperparameters. Such model complexity issues only compound
    when trying to generalize generative models to incorporate human input. We introduce
    Correlation Explanation (CorEx), an alternative approach to topic modeling that
    does not assume an underlying generative model, and instead learns maximally informative
    topics through an information-theoretic framework. This framework naturally generalizes
    to hierarchical and semi-supervised extensions with no additional modeling assumptions.
    In particular, word-level domain knowledge can be flexibly incorporated within
    CorEx through anchor words, allowing topic separability and representation to
    be promoted with minimal human intervention. Across a variety of datasets, metrics,
    and experiments, we demonstrate that CorEx produces topics that are comparable
    in quality to those produced by unsupervised and semi-supervised variants of LDA.
  attachment:
  - filename: https://vimeo.com/276403824
    type: video
    url: https://vimeo.com/276403824
  author:
  - first: Ryan J.
    full: Ryan J. Gallagher
    id: ryan-j-gallagher
    last: Gallagher
  - first: Kyle
    full: Kyle Reing
    id: kyle-reing
    last: Reing
  - first: David
    full: David Kale
    id: david-kale
    last: Kale
  - first: Greg
    full: Greg Ver Steeg
    id: greg-ver-steeg
    last: Ver Steeg
  author_string: Ryan J. Gallagher, Kyle Reing, David Kale, Greg Ver Steeg
  bibkey: gallagher-etal-2017-anchored
  bibtype: article
  booktitle: Transactions of the Association for Computational Linguistics, Volume
    5
  doi: 10.1162/tacl_a_00078
  page_first: '529'
  page_last: '542'
  pages: "529\u2013542"
  paper_id: '37'
  parent_volume_id: Q17-1
  pdf: https://www.aclweb.org/anthology/Q17-1037.pdf
  thumbnail: https://www.aclweb.org/anthology/thumb/Q17-1037.jpg
  title: 'Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge'
  title_html: 'Anchored Correlation Explanation: Topic Modeling with Minimal Domain
    Knowledge'
  url: https://www.aclweb.org/anthology/Q17-1037
  year: '2017'
