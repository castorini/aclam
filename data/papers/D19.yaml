D19-1000:
  address: Hong Kong, China
  author:
  - first: Kentaro
    full: Kentaro Inui
    id: kentaro-inui
    last: Inui
  - first: Jing
    full: Jing Jiang
    id: jing-jiang
    last: Jiang
  - first: Vincent
    full: Vincent Ng
    id: vincent-ng
    last: Ng
  - first: Xiaojun
    full: Xiaojun Wan
    id: xiaojun-wan
    last: Wan
  author_string: Kentaro Inui, Jing Jiang, Vincent Ng, Xiaojun Wan
  bibkey: emnlp-2019-2019
  bibtype: proceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  month: November
  paper_id: '0'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1000.jpg
  title: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  title_html: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  url: https://www.aclweb.org/anthology/D19-1000
  year: '2019'
D19-1001:
  abstract: Neural sequence generation is typically performed token-by-token and left-to-right.
    Whenever a token is generated only previously produced tokens are taken into consideration.
    In contrast, for problems such as sequence classification, bidirectional attention,
    which takes both past and future tokens into consideration, has been shown to
    perform much better. We propose to make the sequence generation process bidirectional
    by employing special placeholder tokens. Treated as a node in a fully connected
    graph, a placeholder token can take past and future tokens into consideration
    when generating the actual output token. We verify the effectiveness of our approach
    experimentally on two conversational tasks where the proposed bidirectional model
    outperforms competitive baselines by a large margin.
  address: Hong Kong, China
  attachment:
  - filename: D19-1001.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1001.Attachment.pdf
  author:
  - first: Carolin
    full: Carolin Lawrence
    id: carolin-lawrence
    last: Lawrence
  - first: Bhushan
    full: Bhushan Kotnis
    id: bhushan-kotnis
    last: Kotnis
  - first: Mathias
    full: Mathias Niepert
    id: mathias-niepert
    last: Niepert
  author_string: Carolin Lawrence, Bhushan Kotnis, Mathias Niepert
  bibkey: lawrence-etal-2019-attending
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1001
  month: November
  page_first: '1'
  page_last: '10'
  pages: "1\u201310"
  paper_id: '1'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1001.jpg
  title: Attending to Future Tokens for Bidirectional Sequence Generation
  title_html: Attending to Future Tokens for Bidirectional Sequence Generation
  url: https://www.aclweb.org/anthology/D19-1001
  year: '2019'
D19-1002:
  abstract: "Attention mechanisms play a central role in NLP systems, especially within\
    \ recurrent neural network (RNN) models. Recently, there has been increasing interest\
    \ in whether or not the intermediate representations offered by these modules\
    \ may be used to explain the reasoning for a model\u2019s prediction, and consequently\
    \ reach insights regarding the model\u2019s decision-making process. A recent\
    \ paper claims that \u2018Attention is not Explanation\u2019 (Jain and Wallace,\
    \ 2019). We challenge many of the assumptions underlying this work, arguing that\
    \ such a claim depends on one\u2019s definition of explanation, and that testing\
    \ it needs to take into account all elements of the model. We propose four alternative\
    \ tests to determine when/whether attention can be used as explanation: a simple\
    \ uniform-weights baseline; a variance calibration based on multiple random seed\
    \ runs; a diagnostic framework using frozen weights from pretrained models; and\
    \ an end-to-end adversarial attention training protocol. Each allows for meaningful\
    \ interpretation of attention mechanisms in RNN models. We show that even when\
    \ reliable adversarial distributions can be found, they don\u2019t perform well\
    \ on the simple diagnostic, indicating that prior work does not disprove the usefulness\
    \ of attention mechanisms for explainability."
  address: Hong Kong, China
  attachment:
  - filename: D19-1002.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1002.Attachment.zip
  author:
  - first: Sarah
    full: Sarah Wiegreffe
    id: sarah-wiegreffe
    last: Wiegreffe
  - first: Yuval
    full: Yuval Pinter
    id: yuval-pinter
    last: Pinter
  author_string: Sarah Wiegreffe, Yuval Pinter
  bibkey: wiegreffe-pinter-2019-attention
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1002
  month: November
  page_first: '11'
  page_last: '20'
  pages: "11\u201320"
  paper_id: '2'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1002.jpg
  title: Attention is not not Explanation
  title_html: Attention is not not Explanation
  url: https://www.aclweb.org/anthology/D19-1002
  year: '2019'
D19-1003:
  abstract: Active learning (AL) is a widely-used training strategy for maximizing
    predictive performance subject to a fixed annotation budget. In AL, one iteratively
    selects training examples for annotation, often those for which the current model
    is most uncertain (by some measure). The hope is that active sampling leads to
    better performance than would be achieved under independent and identically distributed
    (i.i.d.) random samples. While AL has shown promise in retrospective evaluations,
    these studies often ignore practical obstacles to its use. In this paper, we show
    that while AL may provide benefits when used with specific models and for particular
    domains, the benefits of current approaches do not generalize reliably across
    models and tasks. This is problematic because in practice, one does not have the
    opportunity to explore and compare alternative AL strategies. Moreover, AL couples
    the training dataset with the model used to guide its acquisition. We find that
    subsequently training a successor model with an actively-acquired dataset does
    not consistently outperform training on i.i.d. sampled data. Our findings raise
    the question of whether the downsides inherent to AL are worth the modest and
    inconsistent performance gains it tends to afford.
  address: Hong Kong, China
  attachment:
  - filename: D19-1003.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1003.Attachment.zip
  author:
  - first: David
    full: David Lowell
    id: david-lowell
    last: Lowell
  - first: Zachary C.
    full: Zachary C. Lipton
    id: zachary-c-lipton
    last: Lipton
  - first: Byron C.
    full: Byron C. Wallace
    id: byron-c-wallace
    last: Wallace
  author_string: David Lowell, Zachary C. Lipton, Byron C. Wallace
  bibkey: lowell-etal-2019-practical
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1003
  month: November
  page_first: '21'
  page_last: '30'
  pages: "21\u201330"
  paper_id: '3'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1003.jpg
  title: Practical Obstacles to Deploying Active Learning
  title_html: Practical Obstacles to Deploying Active Learning
  url: https://www.aclweb.org/anthology/D19-1003
  year: '2019'
D19-1004:
  abstract: Deep learning systems thrive on abundance of labeled training data but
    such data is not always available, calling for alternative methods of supervision.
    One such method is expectation regularization (XR) (Mann and McCallum, 2007),
    where models are trained based on expected label proportions. We propose a novel
    application of the XR framework for transfer learning between related tasks, where
    knowing the labels of task A provides an estimation of the label proportion of
    task B. We then use a model trained for A to label a large corpus, and use this
    corpus with an XR loss to train a model for task B. To make the XR framework applicable
    to large-scale deep-learning setups, we propose a stochastic batched approximation
    procedure. We demonstrate the approach on the task of Aspect-based Sentiment classification,
    where we effectively use a sentence-level sentiment predictor to train accurate
    aspect-based predictor. The method improves upon fully supervised neural system
    trained on aspect-level data, and is also cumulative with LM-based pretraining,
    as we demonstrate by improving a BERT-based Aspect-based Sentiment model.
  address: Hong Kong, China
  author:
  - first: Matan
    full: Matan Ben Noach
    id: matan-ben-noach
    last: Ben Noach
  - first: Yoav
    full: Yoav Goldberg
    id: yoav-goldberg
    last: Goldberg
  author_string: Matan Ben Noach, Yoav Goldberg
  bibkey: ben-noach-goldberg-2019-transfer
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1004
  month: November
  page_first: '31'
  page_last: '42'
  pages: "31\u201342"
  paper_id: '4'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1004.jpg
  title: Transfer Learning Between Related Tasks Using Expected Label Proportions
  title_html: Transfer Learning Between Related Tasks Using Expected Label Proportions
  url: https://www.aclweb.org/anthology/D19-1004
  year: '2019'
D19-1005:
  abstract: "Contextual word representations, typically trained on unstructured, unlabeled\
    \ text, do not contain any explicit grounding to real world entities and are often\
    \ unable to remember facts about those entities. We propose a general method to\
    \ embed multiple knowledge bases (KBs) into large scale models, and thereby enhance\
    \ their representations with structured, human-curated knowledge. For each KB,\
    \ we first use an integrated entity linker to retrieve relevant entity embeddings,\
    \ then update contextual word representations via a form of word-to-entity attention.\
    \ In contrast to previous approaches, the entity linkers and self-supervised language\
    \ modeling objective are jointly trained end-to-end in a multitask setting that\
    \ combines a small amount of entity linking supervision with a large amount of\
    \ raw text. After integrating WordNet and a subset of Wikipedia into BERT, the\
    \ knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability\
    \ to recall facts as measured in a probing task and downstream performance on\
    \ relationship extraction, entity typing, and word sense disambiguation. KnowBert\u2019\
    s runtime is comparable to BERT\u2019s and it scales to large KBs."
  address: Hong Kong, China
  attachment:
  - filename: D19-1005.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1005.Attachment.pdf
  author:
  - first: Matthew E.
    full: Matthew E. Peters
    id: matthew-e-peters
    last: Peters
  - first: Mark
    full: Mark Neumann
    id: mark-neumann
    last: Neumann
  - first: Robert
    full: Robert Logan
    id: robert-logan
    last: Logan
  - first: Roy
    full: Roy Schwartz
    id: roy-schwartz
    last: Schwartz
  - first: Vidur
    full: Vidur Joshi
    id: vidur-joshi
    last: Joshi
  - first: Sameer
    full: Sameer Singh
    id: sameer-singh
    last: Singh
  - first: Noah A.
    full: Noah A. Smith
    id: noah-a-smith
    last: Smith
  author_string: Matthew E. Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur
    Joshi, Sameer Singh, Noah A. Smith
  bibkey: peters-etal-2019-knowledge
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1005
  month: November
  page_first: '43'
  page_last: '54'
  pages: "43\u201354"
  paper_id: '5'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1005.jpg
  title: Knowledge Enhanced Contextual Word Representations
  title_html: Knowledge Enhanced Contextual Word Representations
  url: https://www.aclweb.org/anthology/D19-1005
  year: '2019'
D19-1006:
  abstract: "Replacing static word embeddings with contextualized word representations\
    \ has yielded significant improvements on many NLP tasks. However, just how contextual\
    \ are the contextualized representations produced by models such as ELMo and BERT?\
    \ Are there infinitely many context-specific representations for each word, or\
    \ are words essentially assigned one of a finite number of word-sense representations?\
    \ For one, we find that the contextualized representations of all words are not\
    \ isotropic in any layer of the contextualizing model. While representations of\
    \ the same word in different contexts still have a greater cosine similarity than\
    \ those of two different words, this self-similarity is much lower in upper layers.\
    \ This suggests that upper layers of contextualizing models produce more context-specific\
    \ representations, much like how upper layers of LSTMs produce more task-specific\
    \ representations. In all layers of ELMo, BERT, and GPT-2, on average, less than\
    \ 5% of the variance in a word\u2019s contextualized representations can be explained\
    \ by a static embedding for that word, providing some justification for the success\
    \ of contextualized representations."
  address: Hong Kong, China
  author:
  - first: Kawin
    full: Kawin Ethayarajh
    id: kawin-ethayarajh
    last: Ethayarajh
  author_string: Kawin Ethayarajh
  bibkey: ethayarajh-2019-contextual
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1006
  month: November
  page_first: '55'
  page_last: '65'
  pages: "55\u201365"
  paper_id: '6'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1006.jpg
  title: How Contextual are Contextualized Word Representations? Comparing the Geometry
    of BERT, ELMo, and GPT-2 Embeddings
  title_html: How Contextual are Contextualized Word Representations? Comparing the
    Geometry of <span class="acl-fixed-case">BERT</span>, <span class="acl-fixed-case">ELM</span>o,
    and <span class="acl-fixed-case">GPT</span>-2 Embeddings
  url: https://www.aclweb.org/anthology/D19-1006
  year: '2019'
D19-1007:
  abstract: Word embeddings are increasingly used for the automatic detection of semantic
    change; yet, a robust evaluation and systematic comparison of the choices involved
    has been lacking. We propose a new evaluation framework for semantic change detection
    and find that (i) using the whole time series is preferable over only comparing
    between the first and last time points; (ii) independently trained and aligned
    embeddings perform better than continuously trained embeddings for long time periods;
    and (iii) that the reference point for comparison matters. We also present an
    analysis of the changes detected on a large Twitter dataset spanning 5.5 years.
  address: Hong Kong, China
  author:
  - first: Philippa
    full: Philippa Shoemark
    id: philippa-shoemark
    last: Shoemark
  - first: Farhana Ferdousi
    full: Farhana Ferdousi Liza
    id: farhana-ferdousi-liza
    last: Liza
  - first: Dong
    full: Dong Nguyen
    id: dong-nguyen
    last: Nguyen
  - first: Scott
    full: Scott Hale
    id: scott-hale
    last: Hale
  - first: Barbara
    full: Barbara McGillivray
    id: barbara-mcgillivray
    last: McGillivray
  author_string: Philippa Shoemark, Farhana Ferdousi Liza, Dong Nguyen, Scott Hale,
    Barbara McGillivray
  bibkey: shoemark-etal-2019-room
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1007
  month: November
  page_first: '66'
  page_last: '76'
  pages: "66\u201376"
  paper_id: '7'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1007.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1007.jpg
  title: 'Room to Glo: A Systematic Comparison of Semantic Change Detection Approaches
    with Word Embeddings'
  title_html: 'Room to <span class="acl-fixed-case">G</span>lo: A Systematic Comparison
    of Semantic Change Detection Approaches with Word Embeddings'
  url: https://www.aclweb.org/anthology/D19-1007
  year: '2019'
D19-1008:
  abstract: Similarity measures based purely on word embeddings are comfortably competing
    with much more sophisticated deep learning and expert-engineered systems on unsupervised
    semantic textual similarity (STS) tasks. In contrast to commonly used geometric
    approaches, we treat a single word embedding as e.g. 300 observations from a scalar
    random variable. Using this paradigm, we first illustrate that similarities derived
    from elementary pooling operations and classic correlation coefficients yield
    excellent results on standard STS benchmarks, outperforming many recently proposed
    methods while being much faster and trivial to implement. Next, we demonstrate
    how to avoid pooling operations altogether and compare sets of word embeddings
    directly via correlation operators between reproducing kernel Hilbert spaces.
    Just like cosine similarity is used to compare individual word vectors, we introduce
    a novel application of the centered kernel alignment (CKA) as a natural generalisation
    of squared cosine similarity for sets of word vectors. Likewise, CKA is very easy
    to implement and enjoys very strong empirical results.
  address: Hong Kong, China
  attachment:
  - filename: D19-1008.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1008.Attachment.zip
  author:
  - first: Vitalii
    full: Vitalii Zhelezniak
    id: vitalii-zhelezniak
    last: Zhelezniak
  - first: April
    full: April Shen
    id: april-shen
    last: Shen
  - first: Daniel
    full: Daniel Busbridge
    id: daniel-busbridge
    last: Busbridge
  - first: Aleksandar
    full: Aleksandar Savkov
    id: aleksandar-savkov
    last: Savkov
  - first: Nils
    full: Nils Hammerla
    id: nils-hammerla
    last: Hammerla
  author_string: Vitalii Zhelezniak, April Shen, Daniel Busbridge, Aleksandar Savkov,
    Nils Hammerla
  bibkey: zhelezniak-etal-2019-correlations
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1008
  month: November
  page_first: '77'
  page_last: '87'
  pages: "77\u201387"
  paper_id: '8'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1008.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1008.jpg
  title: Correlations between Word Vector Sets
  title_html: Correlations between Word Vector Sets
  url: https://www.aclweb.org/anthology/D19-1008
  year: '2019'
D19-1009:
  abstract: 'Game-theoretic models, thanks to their intrinsic ability to exploit contextual
    information, have shown to be particularly suited for the Word Sense Disambiguation
    task. They represent ambiguous words as the players of a non cooperative game
    and their senses as the strategies that the players can select in order to play
    the games. The interaction among the players is modeled with a weighted graph
    and the payoff as an embedding similarity function, that the players try to maximize.
    The impact of the word and sense embedding representations in the framework has
    been tested and analyzed extensively: experiments on standard benchmarks show
    state-of-art performances and different tests hint at the usefulness of using
    disambiguation to obtain contextualized word representations.'
  address: Hong Kong, China
  author:
  - first: Rocco
    full: Rocco Tripodi
    id: rocco-tripodi
    last: Tripodi
  - first: Roberto
    full: Roberto Navigli
    id: roberto-navigli
    last: Navigli
  author_string: Rocco Tripodi, Roberto Navigli
  bibkey: tripodi-navigli-2019-game
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1009
  month: November
  page_first: '88'
  page_last: '99'
  pages: "88\u201399"
  paper_id: '9'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1009.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1009.jpg
  title: 'Game Theory Meets Embeddings: a Unified Framework for Word Sense Disambiguation'
  title_html: 'Game Theory Meets Embeddings: a Unified Framework for Word Sense Disambiguation'
  url: https://www.aclweb.org/anthology/D19-1009
  year: '2019'
D19-1010:
  abstract: Dialog policy decides what and how a task-oriented dialog system will
    respond, and plays a vital role in delivering effective conversations. Many studies
    apply Reinforcement Learning to learn a dialog policy with the reward function
    which requires elaborate design and pre-specified user goals. With the growing
    needs to handle complex goals across multiple domains, such manually designed
    reward functions are not affordable to deal with the complexity of real-world
    tasks. To this end, we propose Guided Dialog Policy Learning, a novel algorithm
    based on Adversarial Inverse Reinforcement Learning for joint reward estimation
    and policy optimization in multi-domain task-oriented dialog. The proposed approach
    estimates the reward signal and infers the user goal in the dialog sessions. The
    reward estimator evaluates the state-action pairs so that it can guide the dialog
    policy at each dialog turn. Extensive experiments on a multi-domain dialog dataset
    show that the dialog policy guided by the learned reward function achieves remarkably
    higher task success than state-of-the-art baselines.
  address: Hong Kong, China
  attachment:
  - filename: D19-1010.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1010.Attachment.zip
  author:
  - first: Ryuichi
    full: Ryuichi Takanobu
    id: ryuichi-takanobu
    last: Takanobu
  - first: Hanlin
    full: Hanlin Zhu
    id: hanlin-zhu
    last: Zhu
  - first: Minlie
    full: Minlie Huang
    id: minlie-huang
    last: Huang
  author_string: Ryuichi Takanobu, Hanlin Zhu, Minlie Huang
  bibkey: takanobu-etal-2019-guided
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1010
  month: November
  page_first: '100'
  page_last: '110'
  pages: "100\u2013110"
  paper_id: '10'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1010.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1010.jpg
  title: 'Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented
    Dialog'
  title_html: 'Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented
    Dialog'
  url: https://www.aclweb.org/anthology/D19-1010
  year: '2019'
D19-1011:
  abstract: Multi-turn retrieval-based conversation is an important task for building
    intelligent dialogue systems. Existing works mainly focus on matching candidate
    responses with every context utterance on multiple levels of granularity, which
    ignore the side effect of using excessive context information. Context utterances
    provide abundant information for extracting more matching features, but it also
    brings noise signals and unnecessary information. In this paper, we will analyze
    the side effect of using too many context utterances and propose a multi-hop selector
    network (MSN) to alleviate the problem. Specifically, MSN firstly utilizes a multi-hop
    selector to select the relevant utterances as context. Then, the model matches
    the filtered context with the candidate response and obtains a matching score.
    Experimental results show that MSN outperforms some state-of-the-art methods on
    three public multi-turn dialogue datasets.
  address: Hong Kong, China
  author:
  - first: Chunyuan
    full: Chunyuan Yuan
    id: chunyuan-yuan
    last: Yuan
  - first: Wei
    full: Wei Zhou
    id: wei-zhou
    last: Zhou
  - first: Mingming
    full: Mingming Li
    id: mingming-li
    last: Li
  - first: Shangwen
    full: Shangwen Lv
    id: shangwen-lv
    last: Lv
  - first: Fuqing
    full: Fuqing Zhu
    id: fuqing-zhu
    last: Zhu
  - first: Jizhong
    full: Jizhong Han
    id: jizhong-han
    last: Han
  - first: Songlin
    full: Songlin Hu
    id: songlin-hu
    last: Hu
  author_string: Chunyuan Yuan, Wei Zhou, Mingming Li, Shangwen Lv, Fuqing Zhu, Jizhong
    Han, Songlin Hu
  bibkey: yuan-etal-2019-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1011
  month: November
  page_first: '111'
  page_last: '120'
  pages: "111\u2013120"
  paper_id: '11'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1011.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1011.jpg
  title: Multi-hop Selector Network for Multi-turn Response Selection in Retrieval-based
    Chatbots
  title_html: Multi-hop Selector Network for Multi-turn Response Selection in Retrieval-based
    Chatbots
  url: https://www.aclweb.org/anthology/D19-1011
  year: '2019'
D19-1012:
  abstract: 'Previous research on empathetic dialogue systems has mostly focused on
    generating responses given certain emotions. However, being empathetic not only
    requires the ability of generating emotional responses, but more importantly,
    requires the understanding of user emotions and replying appropriately. In this
    paper, we propose a novel end-to-end approach for modeling empathy in dialogue
    systems: Mixture of Empathetic Listeners (MoEL). Our model first captures the
    user emotions and outputs an emotion distribution. Based on this, MoEL will softly
    combine the output states of the appropriate Listener(s), which are each optimized
    to react to certain emotions, and generate an empathetic response. Human evaluations
    on EMPATHETIC-DIALOGUES dataset confirm that MoEL outperforms multitask training
    baseline in terms of empathy, relevance, and fluency. Furthermore, the case study
    on generated responses of different Listeners shows high interpretability of our
    model.'
  address: Hong Kong, China
  author:
  - first: Zhaojiang
    full: Zhaojiang Lin
    id: zhaojiang-lin
    last: Lin
  - first: Andrea
    full: Andrea Madotto
    id: andrea-madotto
    last: Madotto
  - first: Jamin
    full: Jamin Shin
    id: jamin-shin
    last: Shin
  - first: Peng
    full: Peng Xu
    id: peng-xu
    last: Xu
  - first: Pascale
    full: Pascale Fung
    id: pascale-fung
    last: Fung
  author_string: Zhaojiang Lin, Andrea Madotto, Jamin Shin, Peng Xu, Pascale Fung
  bibkey: lin-etal-2019-moel
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1012
  month: November
  page_first: '121'
  page_last: '132'
  pages: "121\u2013132"
  paper_id: '12'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1012.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1012.jpg
  title: 'MoEL: Mixture of Empathetic Listeners'
  title_html: '<span class="acl-fixed-case">M</span>o<span class="acl-fixed-case">EL</span>:
    Mixture of Empathetic Listeners'
  url: https://www.aclweb.org/anthology/D19-1012
  year: '2019'
D19-1013:
  abstract: Querying the knowledge base (KB) has long been a challenge in the end-to-end
    task-oriented dialogue system. Previous sequence-to-sequence (Seq2Seq) dialogue
    generation work treats the KB query as an attention over the entire KB, without
    the guarantee that the generated entities are consistent with each other. In this
    paper, we propose a novel framework which queries the KB in two steps to improve
    the consistency of generated entities. In the first step, inspired by the observation
    that a response can usually be supported by a single KB row, we introduce a KB
    retrieval component which explicitly returns the most relevant KB row given a
    dialogue history. The retrieval result is further used to filter the irrelevant
    entities in a Seq2Seq response generation model to improve the consistency among
    the output entities. In the second step, we further perform the attention mechanism
    to address the most correlated KB column. Two methods are proposed to make the
    training feasible without labeled retrieval data, which include distant supervision
    and Gumbel-Softmax technique. Experiments on two publicly available task oriented
    dialog datasets show the effectiveness of our model by outperforming the baseline
    systems and producing entity-consistent responses.
  address: Hong Kong, China
  author:
  - first: Libo
    full: Libo Qin
    id: libo-qin
    last: Qin
  - first: Yijia
    full: Yijia Liu
    id: yijia-liu
    last: Liu
  - first: Wanxiang
    full: Wanxiang Che
    id: wanxiang-che
    last: Che
  - first: Haoyang
    full: Haoyang Wen
    id: haoyang-wen
    last: Wen
  - first: Yangming
    full: Yangming Li
    id: yangming-li
    last: Li
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  author_string: Libo Qin, Yijia Liu, Wanxiang Che, Haoyang Wen, Yangming Li, Ting
    Liu
  bibkey: qin-etal-2019-entity
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1013
  month: November
  page_first: '133'
  page_last: '142'
  pages: "133\u2013142"
  paper_id: '13'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1013.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1013.jpg
  title: Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever
  title_html: Entity-Consistent End-to-end Task-Oriented Dialogue System with <span
    class="acl-fixed-case">KB</span> Retriever
  url: https://www.aclweb.org/anthology/D19-1013
  year: '2019'
D19-1014:
  abstract: Reinforcement learning (RL) is an effective approach to learn an optimal
    dialog policy for task-oriented visual dialog systems. A common practice is to
    apply RL on a neural sequence-to-sequence(seq2seq) framework with the action space
    being the output vocabulary in the decoder. However, it is difficult to design
    a reward function that can achieve a balance between learning an effective policy
    and generating a natural dialog response. This paper proposes a novel framework
    that alternatively trains a RL policy for image guessing and a supervised seq2seq
    model to improve dialog generation quality. We evaluate our framework on the GuessWhich
    task and the framework achieves the state-of-the-art performance in both task
    completion and dialog quality.
  address: Hong Kong, China
  attachment:
  - filename: D19-1014.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1014.Attachment.zip
  author:
  - first: Mingyang
    full: Mingyang Zhou
    id: mingyang-zhou
    last: Zhou
  - first: Josh
    full: Josh Arnold
    id: josh-arnold
    last: Arnold
  - first: Zhou
    full: Zhou Yu
    id: zhou-yu
    last: Yu
  author_string: Mingyang Zhou, Josh Arnold, Zhou Yu
  bibkey: zhou-etal-2019-building
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1014
  month: November
  page_first: '143'
  page_last: '153'
  pages: "143\u2013153"
  paper_id: '14'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1014.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1014.jpg
  title: Building Task-Oriented Visual Dialog Systems Through Alternative Optimization
    Between Dialog Policy and Language Generation
  title_html: Building Task-Oriented Visual Dialog Systems Through Alternative Optimization
    Between Dialog Policy and Language Generation
  url: https://www.aclweb.org/anthology/D19-1014
  year: '2019'
D19-1015:
  abstract: Emotion recognition in conversation (ERC) has received much attention,
    lately, from researchers due to its potential widespread applications in diverse
    areas, such as health-care, education, and human resources. In this paper, we
    present Dialogue Graph Convolutional Network (DialogueGCN), a graph neural network
    based approach to ERC. We leverage self and inter-speaker dependency of the interlocutors
    to model conversational context for emotion recognition. Through the graph network,
    DialogueGCN addresses context propagation issues present in the current RNN-based
    methods. We empirically show that this method alleviates such issues, while outperforming
    the current state of the art on a number of benchmark emotion classification datasets.
  address: Hong Kong, China
  author:
  - first: Deepanway
    full: Deepanway Ghosal
    id: deepanway-ghosal
    last: Ghosal
  - first: Navonil
    full: Navonil Majumder
    id: navonil-majumder
    last: Majumder
  - first: Soujanya
    full: Soujanya Poria
    id: soujanya-poria
    last: Poria
  - first: Niyati
    full: Niyati Chhaya
    id: niyati-chhaya
    last: Chhaya
  - first: Alexander
    full: Alexander Gelbukh
    id: alexander-gelbukh
    last: Gelbukh
  author_string: Deepanway Ghosal, Navonil Majumder, Soujanya Poria, Niyati Chhaya,
    Alexander Gelbukh
  bibkey: ghosal-etal-2019-dialoguegcn
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1015
  month: November
  page_first: '154'
  page_last: '164'
  pages: "154\u2013164"
  paper_id: '15'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1015.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1015.jpg
  title: 'DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition
    in Conversation'
  title_html: '<span class="acl-fixed-case">D</span>ialogue<span class="acl-fixed-case">GCN</span>:
    A Graph Convolutional Neural Network for Emotion Recognition in Conversation'
  url: https://www.aclweb.org/anthology/D19-1015
  year: '2019'
D19-1016:
  abstract: Messages in human conversations inherently convey emotions. The task of
    detecting emotions in textual conversations leads to a wide range of applications
    such as opinion mining in social networks. However, enabling machines to analyze
    emotions in conversations is challenging, partly because humans often rely on
    the context and commonsense knowledge to express emotions. In this paper, we address
    these challenges by proposing a Knowledge-Enriched Transformer (KET), where contextual
    utterances are interpreted using hierarchical self-attention and external commonsense
    knowledge is dynamically leveraged using a context-aware affective graph attention
    mechanism. Experiments on multiple textual conversation datasets demonstrate that
    both context and commonsense knowledge are consistently beneficial to the emotion
    detection performance. In addition, the experimental results show that our KET
    model outperforms the state-of-the-art models on most of the tested datasets in
    F1 score.
  address: Hong Kong, China
  author:
  - first: Peixiang
    full: Peixiang Zhong
    id: peixiang-zhong
    last: Zhong
  - first: Di
    full: Di Wang
    id: di-wang
    last: Wang
  - first: Chunyan
    full: Chunyan Miao
    id: chunyan-miao
    last: Miao
  author_string: Peixiang Zhong, Di Wang, Chunyan Miao
  bibkey: zhong-etal-2019-knowledge
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1016
  month: November
  page_first: '165'
  page_last: '176'
  pages: "165\u2013176"
  paper_id: '16'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1016.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1016.jpg
  title: Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations
  title_html: Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations
  url: https://www.aclweb.org/anthology/D19-1016
  year: '2019'
D19-1017:
  abstract: Multiple emotions with different intensities are often evoked by events
    described in documents. Oftentimes, such event information is hidden and needs
    to be discovered from texts. Unveiling the hidden event information can help to
    understand how the emotions are evoked and provide explainable results. However,
    existing studies often ignore the latent event information. In this paper, we
    proposed a novel interpretable relevant emotion ranking model with the event information
    incorporated into a deep learning architecture using the event-driven attentions.
    Moreover, corpus-level event embeddings and document-level event distributions
    are introduced respectively to consider the global events in corpus and the document-specific
    events simultaneously. Experimental results on three real-world corpora show that
    the proposed approach performs remarkably better than the state-of-the-art emotion
    detection approaches and multi-label approaches. Moreover, interpretable results
    can be obtained to shed light on the events which trigger certain emotions.
  address: Hong Kong, China
  author:
  - first: Yang
    full: Yang Yang
    id: yang-yang
    last: Yang
  - first: Deyu
    full: Deyu Zhou
    id: deyu-zhou
    last: Zhou
  - first: Yulan
    full: Yulan He
    id: yulan-he
    last: He
  - first: Meng
    full: Meng Zhang
    id: meng-zhang
    last: Zhang
  author_string: Yang Yang, Deyu Zhou, Yulan He, Meng Zhang
  bibkey: yang-etal-2019-interpretable
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1017
  month: November
  page_first: '177'
  page_last: '187'
  pages: "177\u2013187"
  paper_id: '17'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1017.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1017.jpg
  title: Interpretable Relevant Emotion Ranking with Event-Driven Attention
  title_html: Interpretable Relevant Emotion Ranking with Event-Driven Attention
  url: https://www.aclweb.org/anthology/D19-1017
  year: '2019'
D19-1018:
  abstract: "Several recent works have considered the problem of generating reviews\
    \ (or \u2018tips\u2019) as a form of explanation as to why a recommendation might\
    \ match a customer\u2019s interests. While promising, we demonstrate that existing\
    \ approaches struggle (in terms of both quality and content) to generate justifications\
    \ that are relevant to users\u2019 decision-making process. We seek to introduce\
    \ new datasets and methods to address the recommendation justification task. In\
    \ terms of data, we first propose an \u2018extractive\u2019 approach to identify\
    \ review segments which justify users\u2019 intentions; this approach is then\
    \ used to distantly label massive review corpora and construct large-scale personalized\
    \ recommendation justification datasets. In terms of generation, we are able to\
    \ design two personalized generation models with this data: (1) a reference-based\
    \ Seq2Seq model with aspect-planning which can generate justifications covering\
    \ different aspects, and (2) an aspect-conditional masked language model which\
    \ can generate diverse justifications based on templates extracted from justification\
    \ histories. We conduct experiments on two real-world datasets which show that\
    \ our model is capable of generating convincing and diverse justifications."
  address: Hong Kong, China
  author:
  - first: Jianmo
    full: Jianmo Ni
    id: jianmo-ni
    last: Ni
  - first: Jiacheng
    full: Jiacheng Li
    id: jiacheng-li
    last: Li
  - first: Julian
    full: Julian McAuley
    id: julian-mcauley
    last: McAuley
  author_string: Jianmo Ni, Jiacheng Li, Julian McAuley
  bibkey: ni-etal-2019-justifying
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1018
  month: November
  page_first: '188'
  page_last: '197'
  pages: "188\u2013197"
  paper_id: '18'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1018.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1018.jpg
  title: Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained
    Aspects
  title_html: Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained
    Aspects
  url: https://www.aclweb.org/anthology/D19-1018
  year: '2019'
D19-1019:
  abstract: Customers ask questions and customer service staffs answer their questions,
    which is the basic service model via multi-turn customer service (CS) dialogues
    on E-commerce platforms. Existing studies fail to provide comprehensive service
    satisfaction analysis, namely satisfaction polarity classification (e.g., well
    satisfied, met and unsatisfied) and sentimental utterance identification (e.g.,
    positive, neutral and negative). In this paper, we conduct a pilot study on the
    task of service satisfaction analysis (SSA) based on multi-turn CS dialogues.
    We propose an extensible Context-Assisted Multiple Instance Learning (CAMIL) model
    to predict the sentiments of all the customer utterances and then aggregate those
    sentiments into service satisfaction polarity. After that, we propose a novel
    Context Clue Matching Mechanism (CCMM) to enhance the representations of all customer
    utterances with their matched context clues, i.e., sentiment and reasoning clues.
    We construct two CS dialogue datasets from a top E-commerce platform. Extensive
    experimental results are presented and contrasted against a few previous models
    to demonstrate the efficacy of our model.
  address: Hong Kong, China
  author:
  - first: Kaisong
    full: Kaisong Song
    id: kaisong-song
    last: Song
  - first: Lidong
    full: Lidong Bing
    id: lidong-bing
    last: Bing
  - first: Wei
    full: Wei Gao
    id: wei-gao
    last: Gao
  - first: Jun
    full: Jun Lin
    id: jun-lin
    last: Lin
  - first: Lujun
    full: Lujun Zhao
    id: lujun-zhao
    last: Zhao
  - first: Jiancheng
    full: Jiancheng Wang
    id: jiancheng-wang
    last: Wang
  - first: Changlong
    full: Changlong Sun
    id: changlong-sun
    last: Sun
  - first: Xiaozhong
    full: Xiaozhong Liu
    id: xiaozhong-liu
    last: Liu
  - first: Qiong
    full: Qiong Zhang
    id: qiong-zhang
    last: Zhang
  author_string: Kaisong Song, Lidong Bing, Wei Gao, Jun Lin, Lujun Zhao, Jiancheng
    Wang, Changlong Sun, Xiaozhong Liu, Qiong Zhang
  bibkey: song-etal-2019-using
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1019
  month: November
  page_first: '198'
  page_last: '207'
  pages: "198\u2013207"
  paper_id: '19'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1019.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1019.jpg
  title: Using Customer Service Dialogues for Satisfaction Analysis with Context-Assisted
    Multiple Instance Learning
  title_html: Using Customer Service Dialogues for Satisfaction Analysis with Context-Assisted
    Multiple Instance Learning
  url: https://www.aclweb.org/anthology/D19-1019
  year: '2019'
D19-1020:
  abstract: Medical relation extraction discovers relations between entity mentions
    in text, such as research articles. For this task, dependency syntax has been
    recognized as a crucial source of features. Yet in the medical domain, 1-best
    parse trees suffer from relatively low accuracies, diminishing their usefulness.
    We investigate a method to alleviate this problem by utilizing dependency forests.
    Forests contain more than one possible decisions and therefore have higher recall
    but more noise compared with 1-best outputs. A graph neural network is used to
    represent the forests, automatically distinguishing the useful syntactic information
    from parsing noise. Results on two benchmarks show that our method outperforms
    the standard tree-based methods, giving the state-of-the-art results in the literature.
  address: Hong Kong, China
  author:
  - first: Linfeng
    full: Linfeng Song
    id: linfeng-song
    last: Song
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  - first: Daniel
    full: Daniel Gildea
    id: daniel-gildea
    last: Gildea
  - first: Mo
    full: Mo Yu
    id: mo-yu
    last: Yu
  - first: Zhiguo
    full: Zhiguo Wang
    id: zhiguo-wang
    last: Wang
  - first: Jinsong
    full: Jinsong Su
    id: jinsong-su
    last: Su
  author_string: Linfeng Song, Yue Zhang, Daniel Gildea, Mo Yu, Zhiguo Wang, Jinsong
    Su
  bibkey: song-etal-2019-leveraging
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1020
  month: November
  page_first: '208'
  page_last: '218'
  pages: "208\u2013218"
  paper_id: '20'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1020.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1020.jpg
  title: Leveraging Dependency Forest for Neural Medical Relation Extraction
  title_html: Leveraging Dependency Forest for Neural Medical Relation Extraction
  url: https://www.aclweb.org/anthology/D19-1020
  year: '2019'
D19-1021:
  abstract: Open relation extraction (OpenRE) aims to extract relational facts from
    the open-domain corpus. To this end, it discovers relation patterns between named
    entities and then clusters those semantically equivalent patterns into a united
    relation cluster. Most OpenRE methods typically confine themselves to unsupervised
    paradigms, without taking advantage of existing relational facts in knowledge
    bases (KBs) and their high-quality labeled instances. To address this issue, we
    propose Relational Siamese Networks (RSNs) to learn similarity metrics of relations
    from labeled data of pre-defined relations, and then transfer the relational knowledge
    to identify novel relations in unlabeled data. Experiment results on two real-world
    datasets show that our framework can achieve significant improvements as compared
    with other state-of-the-art methods. Our code is available at https://github.com/thunlp/RSN.
  address: Hong Kong, China
  author:
  - first: Ruidong
    full: Ruidong Wu
    id: ruidong-wu
    last: Wu
  - first: Yuan
    full: Yuan Yao
    id: yuan-yao
    last: Yao
  - first: Xu
    full: Xu Han
    id: xu-han
    last: Han
  - first: Ruobing
    full: Ruobing Xie
    id: ruobing-xie
    last: Xie
  - first: Zhiyuan
    full: Zhiyuan Liu
    id: zhiyuan-liu
    last: Liu
  - first: Fen
    full: Fen Lin
    id: fen-lin
    last: Lin
  - first: Leyu
    full: Leyu Lin
    id: leyu-lin
    last: Lin
  - first: Maosong
    full: Maosong Sun
    id: maosong-sun
    last: Sun
  author_string: Ruidong Wu, Yuan Yao, Xu Han, Ruobing Xie, Zhiyuan Liu, Fen Lin,
    Leyu Lin, Maosong Sun
  bibkey: wu-etal-2019-open
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1021
  month: November
  page_first: '219'
  page_last: '228'
  pages: "219\u2013228"
  paper_id: '21'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1021.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1021.jpg
  title: 'Open Relation Extraction: Relational Knowledge Transfer from Supervised
    Data to Unsupervised Data'
  title_html: 'Open Relation Extraction: Relational Knowledge Transfer from Supervised
    Data to Unsupervised Data'
  url: https://www.aclweb.org/anthology/D19-1021
  year: '2019'
D19-1022:
  abstract: While attention mechanisms have been proven to be effective in many NLP
    tasks, majority of them are data-driven. We propose a novel knowledge-attention
    encoder which incorporates prior knowledge from external lexical resources into
    deep neural networks for relation extraction task. Furthermore, we present three
    effective ways of integrating knowledge-attention with self-attention to maximize
    the utilization of both knowledge and data. The proposed relation extraction system
    is end-to-end and fully attention-based. Experiment results show that the proposed
    knowledge-attention mechanism has complementary strengths with self-attention,
    and our integrated models outperform existing CNN, RNN, and self-attention based
    models. State-of-the-art performance is achieved on TACRED, a complex and large-scale
    relation extraction dataset.
  address: Hong Kong, China
  attachment:
  - filename: D19-1022.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1022.Attachment.pdf
  author:
  - first: Pengfei
    full: Pengfei Li
    id: pengfei-li
    last: Li
  - first: Kezhi
    full: Kezhi Mao
    id: kezhi-mao
    last: Mao
  - first: Xuefeng
    full: Xuefeng Yang
    id: xuefeng-yang
    last: Yang
  - first: Qi
    full: Qi Li
    id: qi-li
    last: Li
  author_string: Pengfei Li, Kezhi Mao, Xuefeng Yang, Qi Li
  bibkey: li-etal-2019-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1022
  month: November
  page_first: '229'
  page_last: '239'
  pages: "229\u2013239"
  paper_id: '22'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1022.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1022.jpg
  title: Improving Relation Extraction with Knowledge-attention
  title_html: Improving Relation Extraction with Knowledge-attention
  url: https://www.aclweb.org/anthology/D19-1022
  year: '2019'
D19-1023:
  abstract: Entity alignment is a viable means for integrating heterogeneous knowledge
    among different knowledge graphs (KGs). Recent developments in the field often
    take an embedding-based approach to model the structural information of KGs so
    that entity alignment can be easily performed in the embedding space. However,
    most existing works do not explicitly utilize useful relation representations
    to assist in entity alignment, which, as we will show in the paper, is a simple
    yet effective way for improving entity alignment. This paper presents a novel
    joint learning framework for entity alignment. At the core of our approach is
    a Graph Convolutional Network (GCN) based framework for learning both entity and
    relation representations. Rather than relying on pre-aligned relation seeds to
    learn relation representations, we first approximate them using entity embeddings
    learned by the GCN. We then incorporate the relation approximation into entities
    to iteratively learn better representations for both. Experiments performed on
    three real-world cross-lingual datasets show that our approach substantially outperforms
    state-of-the-art entity alignment methods.
  address: Hong Kong, China
  attachment:
  - filename: D19-1023.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1023.Attachment.zip
  author:
  - first: Yuting
    full: Yuting Wu
    id: yuting-wu
    last: Wu
  - first: Xiao
    full: Xiao Liu
    id: xiao-liu
    last: Liu
  - first: Yansong
    full: Yansong Feng
    id: yansong-feng
    last: Feng
  - first: Zheng
    full: Zheng Wang
    id: zheng-wang
    last: Wang
  - first: Dongyan
    full: Dongyan Zhao
    id: dongyan-zhao
    last: Zhao
  author_string: Yuting Wu, Xiao Liu, Yansong Feng, Zheng Wang, Dongyan Zhao
  bibkey: wu-etal-2019-jointly
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1023
  month: November
  page_first: '240'
  page_last: '249'
  pages: "240\u2013249"
  paper_id: '23'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1023.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1023.jpg
  title: Jointly Learning Entity and Relation Representations for Entity Alignment
  title_html: Jointly Learning Entity and Relation Representations for Entity Alignment
  url: https://www.aclweb.org/anthology/D19-1023
  year: '2019'
D19-1024:
  abstract: For large-scale knowledge graphs (KGs), recent research has been focusing
    on the large proportion of infrequent relations which have been ignored by previous
    studies. For example few-shot learning paradigm for relations has been investigated.
    In this work, we further advocate that handling uncommon entities is inevitable
    when dealing with infrequent relations. Therefore, we propose a meta-learning
    framework that aims at handling infrequent relations with few-shot learning and
    uncommon entities by using textual descriptions. We design a novel model to better
    extract key information from textual descriptions. Besides, we also develop a
    novel generative model in our framework to enhance the performance by generating
    extra triplets during the training stage. Experiments are conducted on two datasets
    from real-world KGs, and the results show that our framework outperforms previous
    methods when dealing with infrequent relations and their accompanying uncommon
    entities.
  address: Hong Kong, China
  attachment:
  - filename: D19-1024.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1024.Attachment.zip
  author:
  - first: Zihao
    full: Zihao Wang
    id: zihao-wang
    last: Wang
  - first: Kwunping
    full: Kwunping Lai
    id: kwunping-lai
    last: Lai
  - first: Piji
    full: Piji Li
    id: piji-li
    last: Li
  - first: Lidong
    full: Lidong Bing
    id: lidong-bing
    last: Bing
  - first: Wai
    full: Wai Lam
    id: wai-lam
    last: Lam
  author_string: Zihao Wang, Kwunping Lai, Piji Li, Lidong Bing, Wai Lam
  bibkey: wang-etal-2019-tackling
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1024
  month: November
  page_first: '250'
  page_last: '260'
  pages: "250\u2013260"
  paper_id: '24'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1024.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1024.jpg
  title: Tackling Long-Tailed Relations and Uncommon Entities in Knowledge Graph Completion
  title_html: Tackling Long-Tailed Relations and Uncommon Entities in Knowledge Graph
    Completion
  url: https://www.aclweb.org/anthology/D19-1024
  year: '2019'
D19-1025:
  abstract: 'Name tagging in low-resource languages or domains suffers from inadequate
    training data. Existing work heavily relies on additional information, while leaving
    those noisy annotations unexplored that extensively exist on the web. In this
    paper, we propose a novel neural model for name tagging solely based on weakly
    labeled (WL) data, so that it can be applied in any low-resource settings. To
    take the best advantage of all WL sentences, we split them into high-quality and
    noisy portions for two modules, respectively: (1) a classification module focusing
    on the large portion of noisy data can efficiently and robustly pretrain the tag
    classifier by capturing textual context semantics; and (2) a costly sequence labeling
    module focusing on high-quality data utilizes Partial-CRFs with non-entity sampling
    to achieve global optimum. Two modules are combined via shared parameters. Extensive
    experiments involving five low-resource languages and fine-grained food domain
    demonstrate our superior performance (6% and 7.8% F1 gains on average) as well
    as efficiency.'
  address: Hong Kong, China
  author:
  - first: Yixin
    full: Yixin Cao
    id: yixin-cao
    last: Cao
  - first: Zikun
    full: Zikun Hu
    id: zikun-hu
    last: Hu
  - first: Tat-seng
    full: Tat-seng Chua
    id: tat-seng-chua1
    last: Chua
  - first: Zhiyuan
    full: Zhiyuan Liu
    id: zhiyuan-liu
    last: Liu
  - first: Heng
    full: Heng Ji
    id: heng-ji
    last: Ji
  author_string: Yixin Cao, Zikun Hu, Tat-seng Chua, Zhiyuan Liu, Heng Ji
  bibkey: cao-etal-2019-low
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1025
  month: November
  page_first: '261'
  page_last: '270'
  pages: "261\u2013270"
  paper_id: '25'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1025.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1025.jpg
  title: Low-Resource Name Tagging Learned with Weakly Labeled Data
  title_html: Low-Resource Name Tagging Learned with Weakly Labeled Data
  url: https://www.aclweb.org/anthology/D19-1025
  year: '2019'
D19-1026:
  abstract: "Despite of the recent success of collective entity linking (EL) methods,\
    \ these \u201Cglobal\u201D inference methods may yield sub-optimal results when\
    \ the \u201Call-mention coherence\u201D assumption breaks, and often suffer from\
    \ high computational cost at the inference stage, due to the complex search space.\
    \ In this paper, we propose a simple yet effective solution, called Dynamic Context\
    \ Augmentation (DCA), for collective EL, which requires only one pass through\
    \ the mentions in a document. DCA sequentially accumulates context information\
    \ to make efficient, collective inference, and can cope with different local EL\
    \ models as a plug-and-enhance module. We explore both supervised and reinforcement\
    \ learning strategies for learning the DCA model. Extensive experiments show the\
    \ effectiveness of our model with different learning settings, base models, decision\
    \ orders and attention mechanisms."
  address: Hong Kong, China
  attachment:
  - filename: D19-1026.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1026.Attachment.zip
  author:
  - first: Xiyuan
    full: Xiyuan Yang
    id: xiyuan-yang
    last: Yang
  - first: Xiaotao
    full: Xiaotao Gu
    id: xiaotao-gu
    last: Gu
  - first: Sheng
    full: Sheng Lin
    id: sheng-lin
    last: Lin
  - first: Siliang
    full: Siliang Tang
    id: siliang-tang
    last: Tang
  - first: Yueting
    full: Yueting Zhuang
    id: yueting-zhuang
    last: Zhuang
  - first: Fei
    full: Fei Wu
    id: fei-wu
    last: Wu
  - first: Zhigang
    full: Zhigang Chen
    id: zhigang-chen
    last: Chen
  - first: Guoping
    full: Guoping Hu
    id: guoping-hu
    last: Hu
  - first: Xiang
    full: Xiang Ren
    id: xiang-ren
    last: Ren
  author_string: Xiyuan Yang, Xiaotao Gu, Sheng Lin, Siliang Tang, Yueting Zhuang,
    Fei Wu, Zhigang Chen, Guoping Hu, Xiang Ren
  bibkey: yang-etal-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1026
  month: November
  page_first: '271'
  page_last: '281'
  pages: "271\u2013281"
  paper_id: '26'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1026.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1026.jpg
  title: Learning Dynamic Context Augmentation for Global Entity Linking
  title_html: Learning Dynamic Context Augmentation for Global Entity Linking
  url: https://www.aclweb.org/anthology/D19-1026
  year: '2019'
D19-1027:
  abstract: To extract the structured representations of open-domain events, Bayesian
    graphical models have made some progress. However, these approaches typically
    assume that all words in a document are generated from a single event. While this
    may be true for short text such as tweets, such an assumption does not generally
    hold for long text such as news articles. Moreover, Bayesian graphical models
    often rely on Gibbs sampling for parameter inference which may take long time
    to converge. To address these limitations, we propose an event extraction model
    based on Generative Adversarial Nets, called Adversarial-neural Event Model (AEM).
    AEM models an event with a Dirichlet prior and uses a generator network to capture
    the patterns underlying latent events. A discriminator is used to distinguish
    documents reconstructed from the latent events and the original documents. A byproduct
    of the discriminator is that the features generated by the learned discriminator
    network allow the visualization of the extracted events. Our model has been evaluated
    on two Twitter datasets and a news article dataset. Experimental results show
    that our model outperforms the baseline approaches on all the datasets, with more
    significant improvements observed on the news article dataset where an increase
    of 15% is observed in F-measure.
  address: Hong Kong, China
  author:
  - first: Rui
    full: Rui Wang
    id: rui-wang
    last: Wang
  - first: Deyu
    full: Deyu Zhou
    id: deyu-zhou
    last: Zhou
  - first: Yulan
    full: Yulan He
    id: yulan-he
    last: He
  author_string: Rui Wang, Deyu Zhou, Yulan He
  bibkey: wang-etal-2019-open
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1027
  month: November
  page_first: '282'
  page_last: '291'
  pages: "282\u2013291"
  paper_id: '27'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1027.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1027.jpg
  title: Open Event Extraction from Online Text using a Generative Adversarial Network
  title_html: Open Event Extraction from Online Text using a Generative Adversarial
    Network
  url: https://www.aclweb.org/anthology/D19-1027
  year: '2019'
D19-1028:
  abstract: 'Bootstrapping for Entity Set Expansion (ESE) aims at iteratively acquiring
    new instances of a specific target category. Traditional bootstrapping methods
    often suffer from two problems: 1) delayed feedback, i.e., the pattern evaluation
    relies on both its direct extraction quality and extraction quality in later iterations.
    2) sparse supervision, i.e., only few seed entities are used as the supervision.
    To address the above two problems, we propose a novel bootstrapping method combining
    the Monte Carlo Tree Search (MCTS) algorithm with a deep similarity network, which
    can efficiently estimate delayed feedback for pattern evaluation and adaptively
    score entities given sparse supervision signals. Experimental results confirm
    the effectiveness of the proposed method.'
  address: Hong Kong, China
  author:
  - first: Lingyong
    full: Lingyong Yan
    id: lingyong-yan
    last: Yan
  - first: Xianpei
    full: Xianpei Han
    id: xianpei-han
    last: Han
  - first: Le
    full: Le Sun
    id: le-sun
    last: Sun
  - first: Ben
    full: Ben He
    id: ben-he
    last: He
  author_string: Lingyong Yan, Xianpei Han, Le Sun, Ben He
  bibkey: yan-etal-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1028
  month: November
  page_first: '292'
  page_last: '301'
  pages: "292\u2013301"
  paper_id: '28'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1028.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1028.jpg
  title: Learning to Bootstrap for Entity Set Expansion
  title_html: Learning to Bootstrap for Entity Set Expansion
  url: https://www.aclweb.org/anthology/D19-1028
  year: '2019'
D19-1029:
  abstract: Condition is essential in scientific statement. Without the conditions
    (e.g., equipment, environment) that were precisely specified, facts (e.g., observations)
    in the statements may no longer be valid. Existing ScienceIE methods, which aim
    at extracting factual tuples from scientific text, do not consider the conditions.
    In this work, we propose a new sequence labeling framework (as well as a new tag
    schema) to jointly extract the fact and condition tuples from statement sentences.
    The framework has (1) a multi-output module to generate one or multiple tuples
    and (2) a multi-input module to feed in multiple types of signals as sequences.
    It improves F1 score relatively by 4.2% on BioNLP2013 and by 6.2% on a new bio-text
    dataset for tuple extraction.
  address: Hong Kong, China
  author:
  - first: Tianwen
    full: Tianwen Jiang
    id: tianwen-jiang
    last: Jiang
  - first: Tong
    full: Tong Zhao
    id: tong-zhao
    last: Zhao
  - first: Bing
    full: Bing Qin
    id: bing-qin
    last: Qin
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  - first: Nitesh
    full: Nitesh Chawla
    id: nitesh-chawla
    last: Chawla
  - first: Meng
    full: Meng Jiang
    id: meng-jiang
    last: Jiang
  author_string: Tianwen Jiang, Tong Zhao, Bing Qin, Ting Liu, Nitesh Chawla, Meng
    Jiang
  bibkey: jiang-etal-2019-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1029
  month: November
  page_first: '302'
  page_last: '312'
  pages: "302\u2013312"
  paper_id: '29'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1029.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1029.jpg
  title: Multi-Input Multi-Output Sequence Labeling for Joint Extraction of Fact and
    Condition Tuples from Scientific Text
  title_html: Multi-Input Multi-Output Sequence Labeling for Joint Extraction of Fact
    and Condition Tuples from Scientific Text
  url: https://www.aclweb.org/anthology/D19-1029
  year: '2019'
D19-1030:
  abstract: 'The identification of complex semantic structures such as events and
    entity relations, already a challenging Information Extraction task, is doubly
    difficult from sources written in under-resourced and under-annotated languages.
    We investigate the suitability of cross-lingual structure transfer techniques
    for these tasks. We exploit relation- and event-relevant language-universal features,
    leveraging both symbolic (including part-of-speech and dependency path) and distributional
    (including type representation and contextualized representation) information.
    By representing all entity mentions, event triggers, and contexts into this complex
    and structured multilingual common space, using graph convolutional networks,
    we can train a relation or event extractor from source language annotations and
    apply it to the target language. Extensive experiments on cross-lingual relation
    and event transfer among English, Chinese, and Arabic demonstrate that our approach
    achieves performance comparable to state-of-the-art supervised models trained
    on up to 3,000 manually annotated mentions: up to 62.6% F-score for Relation Extraction,
    and 63.1% F-score for Event Argument Role Labeling. The event argument role labeling
    model transferred from English to Chinese achieves similar performance as the
    model trained from Chinese. We thus find that language-universal symbolic and
    distributional representations are complementary for cross-lingual structure transfer.'
  address: Hong Kong, China
  author:
  - first: Ananya
    full: Ananya Subburathinam
    id: ananya-subburathinam
    last: Subburathinam
  - first: Di
    full: Di Lu
    id: di-lu
    last: Lu
  - first: Heng
    full: Heng Ji
    id: heng-ji
    last: Ji
  - first: Jonathan
    full: Jonathan May
    id: jonathan-may
    last: May
  - first: Shih-Fu
    full: Shih-Fu Chang
    id: shih-fu-chang
    last: Chang
  - first: Avirup
    full: Avirup Sil
    id: avirup-sil
    last: Sil
  - first: Clare
    full: Clare Voss
    id: clare-voss
    last: Voss
  author_string: Ananya Subburathinam, Di Lu, Heng Ji, Jonathan May, Shih-Fu Chang,
    Avirup Sil, Clare Voss
  bibkey: subburathinam-etal-2019-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1030
  month: November
  page_first: '313'
  page_last: '325'
  pages: "313\u2013325"
  paper_id: '30'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1030.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1030.jpg
  title: Cross-lingual Structure Transfer for Relation and Event Extraction
  title_html: Cross-lingual Structure Transfer for Relation and Event Extraction
  url: https://www.aclweb.org/anthology/D19-1030
  year: '2019'
D19-1031:
  abstract: Distant supervision for relation extraction enables one to effectively
    acquire structured relations out of very large text corpora with less human efforts.
    Nevertheless, most of the prior-art models for such tasks assume that the given
    text can be noisy, but their corresponding labels are clean. Such unrealistic
    assumption is contradictory with the fact that the given labels are often noisy
    as well, thus leading to significant performance degradation of those models on
    real-world data. To cope with this challenge, we propose a novel label-denoising
    framework that combines neural network with probabilistic modelling, which naturally
    takes into account the noisy labels during learning. We empirically demonstrate
    that our approach significantly improves the current art in uncovering the ground-truth
    relation labels.
  address: Hong Kong, China
  author:
  - first: Junfan
    full: Junfan Chen
    id: junfan-chen
    last: Chen
  - first: Richong
    full: Richong Zhang
    id: richong-zhang
    last: Zhang
  - first: Yongyi
    full: Yongyi Mao
    id: yongyi-mao
    last: Mao
  - first: Hongyu
    full: Hongyu Guo
    id: hongyu-guo
    last: Guo
  - first: Jie
    full: Jie Xu
    id: jie-xu
    last: Xu
  author_string: Junfan Chen, Richong Zhang, Yongyi Mao, Hongyu Guo, Jie Xu
  bibkey: chen-etal-2019-uncover
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1031
  month: November
  page_first: '326'
  page_last: '336'
  pages: "326\u2013336"
  paper_id: '31'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1031.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1031.jpg
  title: 'Uncover the Ground-Truth Relations in Distant Supervision: A Neural Expectation-Maximization
    Framework'
  title_html: 'Uncover the Ground-Truth Relations in Distant Supervision: A Neural
    Expectation-Maximization Framework'
  url: https://www.aclweb.org/anthology/D19-1031
  year: '2019'
D19-1032:
  abstract: Most existing event extraction (EE) methods merely extract event arguments
    within the sentence scope. However, such sentence-level EE methods struggle to
    handle soaring amounts of documents from emerging applications, such as finance,
    legislation, health, etc., where event arguments always scatter across different
    sentences, and even multiple such event mentions frequently co-exist in the same
    document. To address these challenges, we propose a novel end-to-end model, Doc2EDAG,
    which can generate an entity-based directed acyclic graph to fulfill the document-level
    EE (DEE) effectively. Moreover, we reformalize a DEE task with the no-trigger-words
    design to ease the document-level event labeling. To demonstrate the effectiveness
    of Doc2EDAG, we build a large-scale real-world dataset consisting of Chinese financial
    announcements with the challenges mentioned above. Extensive experiments with
    comprehensive analyses illustrate the superiority of Doc2EDAG over state-of-the-art
    methods. Data and codes can be found at https://github.com/dolphin-zs/Doc2EDAG.
  address: Hong Kong, China
  attachment:
  - filename: D19-1032.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1032.Attachment.zip
  author:
  - first: Shun
    full: Shun Zheng
    id: shun-zheng
    last: Zheng
  - first: Wei
    full: Wei Cao
    id: wei-cao
    last: Cao
  - first: Wei
    full: Wei Xu
    id: wei-xu
    last: Xu
  - first: Jiang
    full: Jiang Bian
    id: jiang-bian
    last: Bian
  author_string: Shun Zheng, Wei Cao, Wei Xu, Jiang Bian
  bibkey: zheng-etal-2019-doc2edag
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1032
  month: November
  page_first: '337'
  page_last: '346'
  pages: "337\u2013346"
  paper_id: '32'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1032.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1032.jpg
  title: 'Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event
    Extraction'
  title_html: '<span class="acl-fixed-case">D</span>oc2<span class="acl-fixed-case">EDAG</span>:
    An End-to-End Document-level Framework for <span class="acl-fixed-case">C</span>hinese
    Financial Event Extraction'
  url: https://www.aclweb.org/anthology/D19-1032
  year: '2019'
D19-1033:
  abstract: Event detection (ED) aims to locate trigger words in raw text and then
    classify them into correct event types. In this task, neural net- work based models
    became mainstream in re- cent years. However, two problems arise when it comes
    to languages without natural delim- iters, such as Chinese. First, word-based
    mod- els severely suffer from the problem of word- trigger mismatch, limiting
    the performance of the methods. In addition, even if trigger words could be accurately
    located, the ambi- guity of polysemy of triggers could still af- fect the trigger
    classification stage. To ad- dress the two issues simultaneously, we pro- pose
    the Trigger-aware Lattice Neural Net- work (TLNN). (1) The framework dynami- cally
    incorporates word and character informa- tion so that the trigger-word mismatch
    issue can be avoided. (2) Moreover, for polysemous characters and words, we model
    all senses of them with the help of an external linguistic knowledge base, so
    as to alleviate the prob- lem of ambiguous triggers. Experiments on two benchmark
    datasets show that our model could effectively tackle the two issues and outperforms
    previous state-of-the-art methods significantly, giving the best results. The
    source code of this paper can be obtained from https://github.com/thunlp/TLNN.
  address: Hong Kong, China
  author:
  - first: Ning
    full: Ning Ding
    id: ning-ding
    last: Ding
  - first: Ziran
    full: Ziran Li
    id: ziran-li
    last: Li
  - first: Zhiyuan
    full: Zhiyuan Liu
    id: zhiyuan-liu
    last: Liu
  - first: Haitao
    full: Haitao Zheng
    id: haitao-zheng
    last: Zheng
  - first: Zibo
    full: Zibo Lin
    id: zibo-lin
    last: Lin
  author_string: Ning Ding, Ziran Li, Zhiyuan Liu, Haitao Zheng, Zibo Lin
  bibkey: ding-etal-2019-event
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1033
  month: November
  page_first: '347'
  page_last: '356'
  pages: "347\u2013356"
  paper_id: '33'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1033.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1033.jpg
  title: Event Detection with Trigger-Aware Lattice Neural Network
  title_html: Event Detection with Trigger-Aware Lattice Neural Network
  url: https://www.aclweb.org/anthology/D19-1033
  year: '2019'
D19-1034:
  abstract: In natural language processing, it is common that many entities contain
    other entities inside them. Most existing works on named entity recognition (NER)
    only deal with flat entities but ignore nested ones. We propose a boundary-aware
    neural model for nested NER which leverages entity boundaries to predict entity
    categorical labels. Our model can locate entities precisely by detecting boundaries
    using sequence labeling models. Based on the detected boundaries, our model utilizes
    the boundary-relevant regions to predict entity categorical labels, which can
    decrease computation cost and relieve error propagation problem in layered sequence
    labeling model. We introduce multitask learning to capture the dependencies of
    entity boundaries and their categorical labels, which helps to improve the performance
    of identifying entities. We conduct our experiments on GENIA dataset and the experimental
    results demonstrate that our model outperforms other state-of-the-art methods.
  address: Hong Kong, China
  attachment:
  - filename: D19-1034.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1034.Attachment.zip
  author:
  - first: Changmeng
    full: Changmeng Zheng
    id: changmeng-zheng
    last: Zheng
  - first: Yi
    full: Yi Cai
    id: yi-cai
    last: Cai
  - first: Jingyun
    full: Jingyun Xu
    id: jingyun-xu
    last: Xu
  - first: Ho-fung
    full: Ho-fung Leung
    id: ho-fung-leung
    last: Leung
  - first: Guandong
    full: Guandong Xu
    id: guandong-xu
    last: Xu
  author_string: Changmeng Zheng, Yi Cai, Jingyun Xu, Ho-fung Leung, Guandong Xu
  bibkey: zheng-etal-2019-boundary
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1034
  month: November
  page_first: '357'
  page_last: '366'
  pages: "357\u2013366"
  paper_id: '34'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1034.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1034.jpg
  title: A Boundary-aware Neural Model for Nested Named Entity Recognition
  title_html: A Boundary-aware Neural Model for Nested Named Entity Recognition
  url: https://www.aclweb.org/anthology/D19-1034
  year: '2019'
D19-1035:
  abstract: "The multiple relation extraction task tries to extract all relational\
    \ facts from a sentence. Existing works didn\u2019t consider the extraction order\
    \ of relational facts in a sentence. In this paper we argue that the extraction\
    \ order is important in this task. To take the extraction order into consideration,\
    \ we apply the reinforcement learning into a sequence-to-sequence model. The proposed\
    \ model could generate relational facts freely. Widely conducted experiments on\
    \ two public datasets demonstrate the efficacy of the proposed method."
  address: Hong Kong, China
  author:
  - first: Xiangrong
    full: Xiangrong Zeng
    id: xiangrong-zeng
    last: Zeng
  - first: Shizhu
    full: Shizhu He
    id: shizhu-he
    last: He
  - first: Daojian
    full: Daojian Zeng
    id: daojian-zeng
    last: Zeng
  - first: Kang
    full: Kang Liu
    id: kang-liu
    last: Liu
  - first: Shengping
    full: Shengping Liu
    id: shengping-liu
    last: Liu
  - first: Jun
    full: Jun Zhao
    id: jun-zhao
    last: Zhao
  author_string: Xiangrong Zeng, Shizhu He, Daojian Zeng, Kang Liu, Shengping Liu,
    Jun Zhao
  bibkey: zeng-etal-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1035
  month: November
  page_first: '367'
  page_last: '377'
  pages: "367\u2013377"
  paper_id: '35'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1035.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1035.jpg
  title: Learning the Extraction Order of Multiple Relational Facts in a Sentence
    with Reinforcement Learning
  title_html: Learning the Extraction Order of Multiple Relational Facts in a Sentence
    with Reinforcement Learning
  url: https://www.aclweb.org/anthology/D19-1035
  year: '2019'
D19-1036:
  abstract: Open Information Extraction (OpenIE) methods are effective at extracting
    (noun phrase, relation phrase, noun phrase) triples from text, e.g., (Barack Obama,
    took birth in, Honolulu). Organization of such triples in the form of a graph
    with noun phrases (NPs) as nodes and relation phrases (RPs) as edges results in
    the construction of Open Knowledge Graphs (OpenKGs). In order to use such OpenKGs
    in downstream tasks, it is often desirable to learn embeddings of the NPs and
    RPs present in the graph. Even though several Knowledge Graph (KG) embedding methods
    have been recently proposed, all of those methods have targeted Ontological KGs,
    as opposed to OpenKGs. Straightforward application of existing Ontological KG
    embedding methods to OpenKGs is challenging, as unlike Ontological KGs, OpenKGs
    are not canonicalized, i.e., a real-world entity may be represented using multiple
    nodes in the OpenKG, with each node corresponding to a different NP referring
    to the entity. For example, nodes with labels Barack Obama, Obama, and President
    Obama may refer to the same real-world entity Barack Obama. Even though canonicalization
    of OpenKGs has received some attention lately, output of such methods has not
    been used to improve OpenKG embed- dings. We fill this gap in the paper and propose
    Canonicalization-infused Representations (CaRe) for OpenKGs. Through extensive
    experiments, we observe that CaRe enables existing models to adapt to the challenges
    in OpenKGs and achieve substantial improvements for the link prediction task.
  address: Hong Kong, China
  author:
  - first: Swapnil
    full: Swapnil Gupta
    id: swapnil-gupta
    last: Gupta
  - first: Sreyash
    full: Sreyash Kenkre
    id: sreyash-kenkre
    last: Kenkre
  - first: Partha
    full: Partha Talukdar
    id: partha-talukdar
    last: Talukdar
  author_string: Swapnil Gupta, Sreyash Kenkre, Partha Talukdar
  bibkey: gupta-etal-2019-care
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1036
  month: November
  page_first: '378'
  page_last: '388'
  pages: "378\u2013388"
  paper_id: '36'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1036.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1036.jpg
  title: 'CaRe: Open Knowledge Graph Embeddings'
  title_html: '<span class="acl-fixed-case">C</span>a<span class="acl-fixed-case">R</span>e:
    Open Knowledge Graph Embeddings'
  url: https://www.aclweb.org/anthology/D19-1036
  year: '2019'
D19-1037:
  abstract: Distance supervision is widely used in relation extraction tasks, particularly
    when large-scale manual annotations are virtually impossible to conduct. Although
    Distantly Supervised Relation Extraction (DSRE) benefits from automatic labelling,
    it suffers from serious mislabelling issues, i.e. some or all of the instances
    for an entity pair (head and tail entities) do not express the labelled relation.
    In this paper, we propose a novel model that employs a collaborative curriculum
    learning framework to reduce the effects of mislabelled data. Specifically, we
    firstly propose an internal self-attention mechanism between the convolution operations
    in convolutional neural networks (CNNs) to learn a better sentence representation
    from the noisy inputs. Then we define two sentence selection models as two relation
    extractors in order to collaboratively learn and regularise each other under a
    curriculum scheme to alleviate noisy effects, where the curriculum could be constructed
    by conflicts or small loss. Finally, experiments are conducted on a widely-used
    public dataset and the results indicate that the proposed model significantly
    outperforms baselines including the state-of-the-art in terms of P@N and PR curve
    metrics, thus evidencing its capability of reducing noisy effects for DSRE.
  address: Hong Kong, China
  author:
  - first: Yuyun
    full: Yuyun Huang
    id: yuyun-huang
    last: Huang
  - first: Jinhua
    full: Jinhua Du
    id: jinhua-du
    last: Du
  author_string: Yuyun Huang, Jinhua Du
  bibkey: huang-du-2019-self
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1037
  month: November
  page_first: '389'
  page_last: '398'
  pages: "389\u2013398"
  paper_id: '37'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1037.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1037.jpg
  title: Self-Attention Enhanced CNNs and Collaborative Curriculum Learning for Distantly
    Supervised Relation Extraction
  title_html: Self-Attention Enhanced <span class="acl-fixed-case">CNN</span>s and
    Collaborative Curriculum Learning for Distantly Supervised Relation Extraction
  url: https://www.aclweb.org/anthology/D19-1037
  year: '2019'
D19-1038:
  abstract: Relation extraction (RE) seeks to detect and classify semantic relationships
    between entities, which provides useful information for many NLP applications.
    Since the state-of-the-art RE models require large amounts of manually annotated
    data and language-specific resources to achieve high accuracy, it is very challenging
    to transfer an RE model of a resource-rich language to a resource-poor language.
    In this paper, we propose a new approach for cross-lingual RE model transfer based
    on bilingual word embedding mapping. It projects word embeddings from a target
    language to a source language, so that a well-trained source-language neural network
    RE model can be directly applied to the target language. Experiment results show
    that the proposed approach achieves very good performance for a number of target
    languages on both in-house and open datasets, using a small bilingual dictionary
    with only 1K word pairs.
  address: Hong Kong, China
  author:
  - first: Jian
    full: Jian Ni
    id: jian-ni
    last: Ni
  - first: Radu
    full: Radu Florian
    id: radu-florian
    last: Florian
  author_string: Jian Ni, Radu Florian
  bibkey: ni-florian-2019-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1038
  month: November
  page_first: '399'
  page_last: '409'
  pages: "399\u2013409"
  paper_id: '38'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1038.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1038.jpg
  title: Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding
    Mapping
  title_html: Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding
    Mapping
  url: https://www.aclweb.org/anthology/D19-1038
  year: '2019'
D19-1039:
  abstract: Distant supervision (DS) has been widely used to automatically construct
    (noisy) labeled data for relation extraction (RE). Given two entities, distant
    supervision exploits sentences that directly mention them for predicting their
    semantic relation. We refer to this strategy as 1-hop DS, which unfortunately
    may not work well for long-tail entities with few supporting sentences. In this
    paper, we introduce a new strategy named 2-hop DS to enhance distantly supervised
    RE, based on the observation that there exist a large number of relational tables
    on the Web which contain entity pairs that share common relations. We refer to
    such entity pairs as anchors for each other, and collect all sentences that mention
    the anchor entity pairs of a given target entity pair to help relation prediction.
    We develop a new neural RE method REDS2 in the multi-instance learning paradigm,
    which adopts a hierarchical model structure to fuse information respectively from
    1-hop DS and 2-hop DS. Extensive experimental results on a benchmark dataset show
    that REDS2 can consistently outperform various baselines across different settings
    by a substantial margin.
  address: Hong Kong, China
  author:
  - first: Xiang
    full: Xiang Deng
    id: xiang-deng
    last: Deng
  - first: Huan
    full: Huan Sun
    id: huan-sun
    last: Sun
  author_string: Xiang Deng, Huan Sun
  bibkey: deng-sun-2019-leveraging
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1039
  month: November
  page_first: '410'
  page_last: '420'
  pages: "410\u2013420"
  paper_id: '39'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1039.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1039.jpg
  title: Leveraging 2-hop Distant Supervision from Table Entity Pairs for Relation
    Extraction
  title_html: Leveraging 2-hop Distant Supervision from Table Entity Pairs for Relation
    Extraction
  url: https://www.aclweb.org/anthology/D19-1039
  year: '2019'
D19-1040:
  abstract: 'Rich entity representations are useful for a wide class of problems involving
    entities. Despite their importance, there is no standardized benchmark that evaluates
    the overall quality of entity representations. In this work, we propose EntEval:
    a test suite of diverse tasks that require nontrivial understanding of entities
    including entity typing, entity similarity, entity relation prediction, and entity
    disambiguation. In addition, we develop training techniques for learning better
    entity representations by using natural hyperlink annotations in Wikipedia. We
    identify effective objectives for incorporating the contextual information in
    hyperlinks into state-of-the-art pretrained language models (Peters et al., 2018)
    and show that they improve strong baselines on multiple EntEval tasks.'
  address: Hong Kong, China
  author:
  - first: Mingda
    full: Mingda Chen
    id: mingda-chen
    last: Chen
  - first: Zewei
    full: Zewei Chu
    id: zewei-chu
    last: Chu
  - first: Yang
    full: Yang Chen
    id: yang-chen
    last: Chen
  - first: Karl
    full: Karl Stratos
    id: karl-stratos
    last: Stratos
  - first: Kevin
    full: Kevin Gimpel
    id: kevin-gimpel
    last: Gimpel
  author_string: Mingda Chen, Zewei Chu, Yang Chen, Karl Stratos, Kevin Gimpel
  bibkey: chen-etal-2019-enteval
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1040
  month: November
  page_first: '421'
  page_last: '433'
  pages: "421\u2013433"
  paper_id: '40'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1040.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1040.jpg
  title: 'EntEval: A Holistic Evaluation Benchmark for Entity Representations'
  title_html: '<span class="acl-fixed-case">E</span>nt<span class="acl-fixed-case">E</span>val:
    A Holistic Evaluation Benchmark for Entity Representations'
  url: https://www.aclweb.org/anthology/D19-1040
  year: '2019'
D19-1041:
  abstract: We propose a joint event and temporal relation extraction model with shared
    representation learning and structured prediction. The proposed method has two
    advantages over existing work. First, it improves event representation by allowing
    the event and relation modules to share the same contextualized embeddings and
    neural representation learner. Second, it avoids error propagation in the conventional
    pipeline systems by leveraging structured inference and learning methods to assign
    both the event labels and the temporal relation labels jointly. Experiments show
    that the proposed method can improve both event extraction and temporal relation
    extraction over state-of-the-art systems, with the end-to-end F1 improved by 10%
    and 6.8% on two benchmark datasets respectively.
  address: Hong Kong, China
  attachment:
  - filename: D19-1041.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1041.Attachment.pdf
  author:
  - first: Rujun
    full: Rujun Han
    id: rujun-han
    last: Han
  - first: Qiang
    full: Qiang Ning
    id: qiang-ning
    last: Ning
  - first: Nanyun
    full: Nanyun Peng
    id: nanyun-peng
    last: Peng
  author_string: Rujun Han, Qiang Ning, Nanyun Peng
  bibkey: han-etal-2019-joint
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1041
  month: November
  page_first: '434'
  page_last: '444'
  pages: "434\u2013444"
  paper_id: '41'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1041.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1041.jpg
  title: Joint Event and Temporal Relation Extraction with Shared Representations
    and Structured Prediction
  title_html: Joint Event and Temporal Relation Extraction with Shared Representations
    and Structured Prediction
  url: https://www.aclweb.org/anthology/D19-1041
  year: '2019'
D19-1042:
  abstract: While existing hierarchical text classification (HTC) methods attempt
    to capture label hierarchies for model training, they either make local decisions
    regarding each label or completely ignore the hierarchy information during inference.
    To solve the mismatch between training and inference as well as modeling label
    dependencies in a more principled way, we formulate HTC as a Markov decision process
    and propose to learn a Label Assignment Policy via deep reinforcement learning
    to determine where to place an object and when to stop the assignment process.
    The proposed method, HiLAP, explores the hierarchy during both training and inference
    time in a consistent manner and makes inter-dependent decisions. As a general
    framework, HiLAP can incorporate different neural encoders as base models for
    end-to-end training. Experiments on five public datasets and four base models
    show that HiLAP yields an average improvement of 33.4% in Macro-F1 over flat classifiers
    and outperforms state-of-the-art HTC methods by a large margin. Data and code
    can be found at https://github.com/morningmoni/HiLAP.
  address: Hong Kong, China
  attachment:
  - filename: D19-1042.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1042.Attachment.zip
  author:
  - first: Yuning
    full: Yuning Mao
    id: yuning-mao
    last: Mao
  - first: Jingjing
    full: Jingjing Tian
    id: jingjing-tian
    last: Tian
  - first: Jiawei
    full: Jiawei Han
    id: jiawei-han
    last: Han
  - first: Xiang
    full: Xiang Ren
    id: xiang-ren
    last: Ren
  author_string: Yuning Mao, Jingjing Tian, Jiawei Han, Xiang Ren
  bibkey: mao-etal-2019-hierarchical
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1042
  month: November
  page_first: '445'
  page_last: '455'
  pages: "445\u2013455"
  paper_id: '42'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1042.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1042.jpg
  title: Hierarchical Text Classification with Reinforced Label Assignment
  title_html: Hierarchical Text Classification with Reinforced Label Assignment
  url: https://www.aclweb.org/anthology/D19-1042
  year: '2019'
D19-1043:
  abstract: "As an essential component of natural language processing, text classification\
    \ relies on deep learning in recent years. Various neural networks are designed\
    \ for text classification on the basis of word embedding. However, polysemy is\
    \ a fundamental feature of the natural language, which brings challenges to text\
    \ classification. One polysemic word contains more than one sense, while the word\
    \ embedding procedure conflates different senses of a polysemic word into a single\
    \ vector. Extracting the distinct representation for the specific sense could\
    \ thus lead to fine-grained models with strong generalization ability. It has\
    \ been demonstrated that multiple senses of a word actually reside in linear superposition\
    \ within the word embedding so that specific senses can be extracted from the\
    \ original word embedding. Therefore, we propose to use capsule networks to construct\
    \ the vectorized representation of semantics and utilize hyperplanes to decompose\
    \ each capsule to acquire the specific senses. A novel dynamic routing mechanism\
    \ named \u2018routing-on-hyperplane\u2019 will select the proper sense for the\
    \ downstream classification task. Our model is evaluated on 6 different datasets,\
    \ and the experimental results show that our model is capable of extracting more\
    \ discriminative semantic features and yields a significant performance gain compared\
    \ to other baseline methods."
  address: Hong Kong, China
  author:
  - first: Chunning
    full: Chunning Du
    id: chunning-du
    last: Du
  - first: Haifeng
    full: Haifeng Sun
    id: haifeng-sun
    last: Sun
  - first: Jingyu
    full: Jingyu Wang
    id: jingyu-wang
    last: Wang
  - first: Qi
    full: Qi Qi
    id: qi-qi
    last: Qi
  - first: Jianxin
    full: Jianxin Liao
    id: jianxin-liao
    last: Liao
  - first: Chun
    full: Chun Wang
    id: chun-wang
    last: Wang
  - first: Bing
    full: Bing Ma
    id: bing-ma
    last: Ma
  author_string: Chunning Du, Haifeng Sun, Jingyu Wang, Qi Qi, Jianxin Liao, Chun
    Wang, Bing Ma
  bibkey: du-etal-2019-investigating
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1043
  month: November
  page_first: '456'
  page_last: '465'
  pages: "456\u2013465"
  paper_id: '43'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1043.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1043.jpg
  title: Investigating Capsule Network and Semantic Feature on Hyperplanes for Text
    Classification
  title_html: Investigating Capsule Network and Semantic Feature on Hyperplanes for
    Text Classification
  url: https://www.aclweb.org/anthology/D19-1043
  year: '2019'
D19-1044:
  abstract: Multi-label text classification (MLTC) aims to tag most relevant labels
    for the given document. In this paper, we propose a Label-Specific Attention Network
    (LSAN) to learn a label-specific document representation. LSAN takes advantage
    of label semantic information to determine the semantic connection between labels
    and document for constructing label-specific document representation. Meanwhile,
    the self-attention mechanism is adopted to identify the label-specific document
    representation from document content information. In order to seamlessly integrate
    the above two parts, an adaptive fusion strategy is proposed, which can effectively
    output the comprehensive label-specific document representation to build multi-label
    text classifier. Extensive experimental results demonstrate that LSAN consistently
    outperforms the state-of-the-art methods on four different datasets, especially
    on the prediction of low-frequency labels. The code and hyper-parameter settings
    are released to facilitate other researchers.
  address: Hong Kong, China
  author:
  - first: Lin
    full: Lin Xiao
    id: lin-xiao
    last: Xiao
  - first: Xin
    full: Xin Huang
    id: xin-huang
    last: Huang
  - first: Boli
    full: Boli Chen
    id: boli-chen
    last: Chen
  - first: Liping
    full: Liping Jing
    id: liping-jing
    last: Jing
  author_string: Lin Xiao, Xin Huang, Boli Chen, Liping Jing
  bibkey: xiao-etal-2019-label
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1044
  month: November
  page_first: '466'
  page_last: '475'
  pages: "466\u2013475"
  paper_id: '44'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1044.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1044.jpg
  title: Label-Specific Document Representation for Multi-Label Text Classification
  title_html: Label-Specific Document Representation for Multi-Label Text Classification
  url: https://www.aclweb.org/anthology/D19-1044
  year: '2019'
D19-1045:
  abstract: "Most of the current effective methods for text classification tasks are\
    \ based on large-scale labeled data and a great number of parameters, but when\
    \ the supervised training data are few and difficult to be collected, these models\
    \ are not available. In this work, we propose a hierarchical attention prototypical\
    \ networks (HAPN) for few-shot text classification. We design the feature level,\
    \ word level, and instance level multi cross attention for our model to enhance\
    \ the expressive ability of semantic space, so it can highlight or weaken the\
    \ importance of the features, words, and instances separately. We verify the effectiveness\
    \ of our model on two standard benchmark few-shot text classification datasets\u2014\
    FewRel and CSID, and achieve the state-of-the-art performance. The visualization\
    \ of hierarchical attention layers illustrates that our model can capture more\
    \ important features, words, and instances. In addition, our attention mechanism\
    \ increases support set augmentability and accelerates convergence speed in the\
    \ training stage."
  address: Hong Kong, China
  author:
  - first: Shengli
    full: Shengli Sun
    id: shengli-sun
    last: Sun
  - first: Qingfeng
    full: Qingfeng Sun
    id: qingfeng-sun
    last: Sun
  - first: Kevin
    full: Kevin Zhou
    id: kevin-zhou
    last: Zhou
  - first: Tengchao
    full: Tengchao Lv
    id: tengchao-lv
    last: Lv
  author_string: Shengli Sun, Qingfeng Sun, Kevin Zhou, Tengchao Lv
  bibkey: sun-etal-2019-hierarchical
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1045
  month: November
  page_first: '476'
  page_last: '485'
  pages: "476\u2013485"
  paper_id: '45'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1045.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1045.jpg
  title: Hierarchical Attention Prototypical Networks for Few-Shot Text Classification
  title_html: Hierarchical Attention Prototypical Networks for Few-Shot Text Classification
  url: https://www.aclweb.org/anthology/D19-1045
  year: '2019'
D19-1046:
  abstract: Feature importance is commonly used to explain machine predictions. While
    feature importance can be derived from a machine learning model with a variety
    of methods, the consistency of feature importance via different methods remains
    understudied. In this work, we systematically compare feature importance from
    built-in mechanisms in a model such as attention values and post-hoc methods that
    approximate model behavior such as LIME. Using text classification as a testbed,
    we find that 1) no matter which method we use, important features from traditional
    models such as SVM and XGBoost are more similar with each other, than with deep
    learning models; 2) post-hoc methods tend to generate more similar important features
    for two models than built-in methods. We further demonstrate how such similarity
    varies across instances. Notably, important features do not always resemble each
    other better when two models agree on the predicted label than when they disagree.
  address: Hong Kong, China
  attachment:
  - filename: D19-1046.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1046.Attachment.pdf
  author:
  - first: Vivian
    full: Vivian Lai
    id: vivian-lai
    last: Lai
  - first: Zheng
    full: Zheng Cai
    id: zheng-cai
    last: Cai
  - first: Chenhao
    full: Chenhao Tan
    id: chenhao-tan
    last: Tan
  author_string: Vivian Lai, Zheng Cai, Chenhao Tan
  bibkey: lai-etal-2019-many
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1046
  month: November
  page_first: '486'
  page_last: '495'
  pages: "486\u2013495"
  paper_id: '46'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1046.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1046.jpg
  title: 'Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature
    Importance in Text Classification'
  title_html: 'Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature
    Importance in Text Classification'
  url: https://www.aclweb.org/anthology/D19-1046
  year: '2019'
D19-1047:
  abstract: For text classification, traditional local feature driven models learn
    long dependency by deeply stacking or hybrid modeling. This paper proposes a novel
    Encoder1-Encoder2 architecture, where global information is incorporated into
    the procedure of local feature extraction from scratch. In particular, Encoder1
    serves as a global information provider, while Encoder2 performs as a local feature
    extractor and is directly fed into the classifier. Meanwhile, two modes are also
    designed for their interaction. Thanks to the awareness of global information,
    our method is able to learn better instance specific local features and thus avoids
    complicated upper operations. Experiments conducted on eight benchmark datasets
    demonstrate that our proposed architecture promotes local feature driven models
    by a substantial margin and outperforms the previous best models in the fully-supervised
    setting.
  address: Hong Kong, China
  author:
  - first: Guocheng
    full: Guocheng Niu
    id: guocheng-niu
    last: Niu
  - first: Hengru
    full: Hengru Xu
    id: hengru-xu
    last: Xu
  - first: Bolei
    full: Bolei He
    id: bolei-he
    last: He
  - first: Xinyan
    full: Xinyan Xiao
    id: xinyan-xiao
    last: Xiao
  - first: Hua
    full: Hua Wu
    id: hua-wu
    last: Wu
  - first: Sheng
    full: Sheng Gao
    id: sheng-gao
    last: Gao
  author_string: Guocheng Niu, Hengru Xu, Bolei He, Xinyan Xiao, Hua Wu, Sheng Gao
  bibkey: niu-etal-2019-enhancing
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1047
  month: November
  page_first: '496'
  page_last: '506'
  pages: "496\u2013506"
  paper_id: '47'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1047.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1047.jpg
  title: Enhancing Local Feature Extraction with Global Representation for Neural
    Text Classification
  title_html: Enhancing Local Feature Extraction with Global Representation for Neural
    Text Classification
  url: https://www.aclweb.org/anthology/D19-1047
  year: '2019'
D19-1048:
  abstract: Generative classifiers offer potential advantages over their discriminative
    counterparts, namely in the areas of data efficiency, robustness to data shift
    and adversarial examples, and zero-shot learning (Ng and Jordan,2002; Yogatama
    et al., 2017; Lewis and Fan,2019). In this paper, we improve generative text classifiers
    by introducing discrete latent variables into the generative story, and explore
    several graphical model configurations. We parameterize the distributions using
    standard neural architectures used in conditional language modeling and perform
    learning by directly maximizing the log marginal likelihood via gradient-based
    optimization, which avoids the need to do expectation-maximization. We empirically
    characterize the performance of our models on six text classification datasets.
    The choice of where to include the latent variable has a significant impact on
    performance, with the strongest results obtained when using the latent variable
    as an auxiliary conditioning variable in the generation of the textual input.
    This model consistently outperforms both the generative and discriminative classifiers
    in small-data settings. We analyze our model by finding that the latent variable
    captures interpretable properties of the data, even with very small training sets.
  address: Hong Kong, China
  attachment:
  - filename: D19-1048.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1048.Attachment.pdf
  author:
  - first: Xiaoan
    full: Xiaoan Ding
    id: xiaoan-ding
    last: Ding
  - first: Kevin
    full: Kevin Gimpel
    id: kevin-gimpel
    last: Gimpel
  author_string: Xiaoan Ding, Kevin Gimpel
  bibkey: ding-gimpel-2019-latent
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1048
  month: November
  page_first: '507'
  page_last: '517'
  pages: "507\u2013517"
  paper_id: '48'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1048.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1048.jpg
  title: Latent-Variable Generative Models for Data-Efficient Text Classification
  title_html: Latent-Variable Generative Models for Data-Efficient Text Classification
  url: https://www.aclweb.org/anthology/D19-1048
  year: '2019'
D19-1049:
  abstract: "Finding the right reviewers to assess the quality of conference submissions\
    \ is a time consuming process for conference organizers. Given the importance\
    \ of this step, various automated reviewer-paper matching solutions have been\
    \ proposed to alleviate the burden. Prior approaches including bag-of-words model\
    \ and probabilistic topic model are less effective to deal with the vocabulary\
    \ mismatch and partial topic overlap between the submission and reviewer. Our\
    \ approach, the common topic model, jointly models the topics common to the submission\
    \ and the reviewer\u2019s profile while relying on abstract topic vectors. Experiments\
    \ and insightful evaluations on two datasets demonstrate that the proposed method\
    \ achieves consistent improvements compared to the state-of-the-art."
  address: Hong Kong, China
  author:
  - first: Omer
    full: Omer Anjum
    id: omer-anjum
    last: Anjum
  - first: Hongyu
    full: Hongyu Gong
    id: hongyu-gong
    last: Gong
  - first: Suma
    full: Suma Bhat
    id: suma-bhat
    last: Bhat
  - first: Wen-Mei
    full: Wen-Mei Hwu
    id: wen-mei-hwu
    last: Hwu
  - first: JinJun
    full: JinJun Xiong
    id: jinjun-xiong
    last: Xiong
  author_string: Omer Anjum, Hongyu Gong, Suma Bhat, Wen-Mei Hwu, JinJun Xiong
  bibkey: anjum-etal-2019-pare
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1049
  month: November
  page_first: '518'
  page_last: '528'
  pages: "518\u2013528"
  paper_id: '49'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1049.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1049.jpg
  title: 'PaRe: A Paper-Reviewer Matching Approach Using a Common Topic Space'
  title_html: '<span class="acl-fixed-case">P</span>a<span class="acl-fixed-case">R</span>e:
    A Paper-Reviewer Matching Approach Using a Common Topic Space'
  url: https://www.aclweb.org/anthology/D19-1049
  year: '2019'
D19-1050:
  abstract: What information from an act of sentence understanding is robustly represented
    in the human brain? We investigate this question by comparing sentence encoding
    models on a brain decoding task, where the sentence that an experimental participant
    has seen must be predicted from the fMRI signal evoked by the sentence. We take
    a pre-trained BERT architecture as a baseline sentence encoding model and fine-tune
    it on a variety of natural language understanding (NLU) tasks, asking which lead
    to improvements in brain-decoding performance. We find that none of the sentence
    encoding tasks tested yield significant increases in brain decoding performance.
    Through further task ablations and representational analyses, we find that tasks
    which produce syntax-light representations yield significant improvements in brain
    decoding performance. Our results constrain the space of NLU models that could
    best account for human neural representations of language, but also suggest limits
    on the possibility of decoding fine-grained syntactic information from fMRI human
    neuroimaging.
  address: Hong Kong, China
  attachment:
  - filename: D19-1050.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1050.Attachment.zip
  author:
  - first: Jon
    full: Jon Gauthier
    id: jon-gauthier
    last: Gauthier
  - first: Roger
    full: Roger Levy
    id: roger-levy
    last: Levy
  author_string: Jon Gauthier, Roger Levy
  bibkey: gauthier-levy-2019-linking
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1050
  month: November
  page_first: '529'
  page_last: '539'
  pages: "529\u2013539"
  paper_id: '50'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1050.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1050.jpg
  title: Linking artificial and human neural representations of language
  title_html: Linking artificial and human neural representations of language
  url: https://www.aclweb.org/anthology/D19-1050
  year: '2019'
D19-1051:
  abstract: 'Text summarization aims at compressing long documents into a shorter
    form that conveys the most important parts of the original document. Despite increased
    interest in the community and notable research effort, progress on benchmark datasets
    has stagnated. We critically evaluate key ingredients of the current research
    setup: datasets, evaluation metrics, and models, and highlight three primary shortcomings:
    1) automatically collected datasets leave the task underconstrained and may contain
    noise detrimental to training and evaluation, 2) current evaluation protocol is
    weakly correlated with human judgment and does not account for important characteristics
    such as factual correctness, 3) models overfit to layout biases of current datasets
    and offer limited diversity in their outputs.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1051.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1051.Attachment.zip
  author:
  - first: Wojciech
    full: Wojciech Kryscinski
    id: wojciech-kryscinski
    last: Kryscinski
  - first: Nitish Shirish
    full: Nitish Shirish Keskar
    id: nitish-shirish-keskar
    last: Keskar
  - first: Bryan
    full: Bryan McCann
    id: bryan-mccann
    last: McCann
  - first: Caiming
    full: Caiming Xiong
    id: caiming-xiong
    last: Xiong
  - first: Richard
    full: Richard Socher
    id: richard-socher
    last: Socher
  author_string: Wojciech Kryscinski, Nitish Shirish Keskar, Bryan McCann, Caiming
    Xiong, Richard Socher
  bibkey: kryscinski-etal-2019-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1051
  month: November
  page_first: '540'
  page_last: '551'
  pages: "540\u2013551"
  paper_id: '51'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1051.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1051.jpg
  title: 'Neural Text Summarization: A Critical Evaluation'
  title_html: 'Neural Text Summarization: A Critical Evaluation'
  url: https://www.aclweb.org/anthology/D19-1051
  year: '2019'
D19-1052:
  abstract: Traditionally, most data-to-text applications have been designed using
    a modular pipeline architecture, in which non-linguistic input data is converted
    into natural language through several intermediate transformations. By contrast,
    recent neural models for data-to-text generation have been proposed as end-to-end
    approaches, where the non-linguistic input is rendered in natural language with
    much less explicit intermediate representations in between. This study introduces
    a systematic comparison between neural pipeline and end-to-end data-to-text approaches
    for the generation of text from RDF triples. Both architectures were implemented
    making use of the encoder-decoder Gated-Recurrent Units (GRU) and Transformer,
    two state-of-the art deep learning methods. Automatic and human evaluations together
    with a qualitative analysis suggest that having explicit intermediate steps in
    the generation process results in better texts than the ones generated by end-to-end
    approaches. Moreover, the pipeline models generalize better to unseen inputs.
    Data and code are publicly available.
  address: Hong Kong, China
  attachment:
  - filename: D19-1052.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1052.Attachment.pdf
  author:
  - first: Thiago
    full: Thiago Castro Ferreira
    id: thiago-castro-ferreira
    last: Castro Ferreira
  - first: Chris
    full: Chris van der Lee
    id: chris-van-der-lee
    last: van der Lee
  - first: Emiel
    full: Emiel van Miltenburg
    id: emiel-van-miltenburg
    last: van Miltenburg
  - first: Emiel
    full: Emiel Krahmer
    id: emiel-krahmer
    last: Krahmer
  author_string: Thiago Castro Ferreira, Chris van der Lee, Emiel van Miltenburg,
    Emiel Krahmer
  bibkey: castro-ferreira-etal-2019-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1052
  month: November
  page_first: '552'
  page_last: '562'
  pages: "552\u2013562"
  paper_id: '52'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1052.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1052.jpg
  title: 'Neural data-to-text generation: A comparison between pipeline and end-to-end
    architectures'
  title_html: 'Neural data-to-text generation: A comparison between pipeline and end-to-end
    architectures'
  url: https://www.aclweb.org/anthology/D19-1052
  year: '2019'
D19-1053:
  abstract: A robust evaluation metric has a profound impact on the development of
    text generation systems. A desirable metric compares system output against references
    based on their semantics rather than surface forms. In this paper we investigate
    strategies to encode system and reference texts to devise a metric that shows
    a high correlation with human judgment of text quality. We validate our new metric,
    namely MoverScore, on a number of text generation tasks including summarization,
    machine translation, image captioning, and data-to-text generation, where the
    outputs are produced by a variety of neural and non-neural systems. Our findings
    suggest that metrics combining contextualized representations with a distance
    measure perform the best. Such metrics also demonstrate strong generalization
    capability across tasks. For ease-of-use we make our metrics available as web
    service.
  address: Hong Kong, China
  attachment:
  - filename: D19-1053.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1053.Attachment.pdf
  author:
  - first: Wei
    full: Wei Zhao
    id: wei-zhao
    last: Zhao
  - first: Maxime
    full: Maxime Peyrard
    id: maxime-peyrard
    last: Peyrard
  - first: Fei
    full: Fei Liu
    id: fei-liu-utdallas
    last: Liu
  - first: Yang
    full: Yang Gao
    id: yang-gao
    last: Gao
  - first: Christian M.
    full: Christian M. Meyer
    id: christian-m-meyer
    last: Meyer
  - first: Steffen
    full: Steffen Eger
    id: steffen-eger
    last: Eger
  author_string: Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer,
    Steffen Eger
  bibkey: zhao-etal-2019-moverscore
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1053
  month: November
  page_first: '563'
  page_last: '578'
  pages: "563\u2013578"
  paper_id: '53'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1053.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1053.jpg
  title: 'MoverScore: Text Generation Evaluating with Contextualized Embeddings and
    Earth Mover Distance'
  title_html: '<span class="acl-fixed-case">M</span>over<span class="acl-fixed-case">S</span>core:
    Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance'
  url: https://www.aclweb.org/anthology/D19-1053
  year: '2019'
D19-1054:
  abstract: 'Many text generation tasks naturally contain two steps: content selection
    and surface realization. Current neural encoder-decoder models conflate both steps
    into a black-box architecture. As a result, the content to be described in the
    text cannot be explicitly controlled. This paper tackles this problem by decoupling
    content selection from the decoder. The decoupled content selection is human interpretable,
    whose value can be manually manipulated to control the content of generated text.
    The model can be trained end-to-end without human annotations by maximizing a
    lower bound of the marginal likelihood. We further propose an effective way to
    trade-off between performance and controllability with a single adjustable hyperparameter.
    In both data-to-text and headline generation tasks, our model achieves promising
    results, paving the way for controllable content selection in text generation.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1054.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1054.Attachment.pdf
  author:
  - first: Xiaoyu
    full: Xiaoyu Shen
    id: xiaoyu-shen
    last: Shen
  - first: Jun
    full: Jun Suzuki
    id: jun-suzuki
    last: Suzuki
  - first: Kentaro
    full: Kentaro Inui
    id: kentaro-inui
    last: Inui
  - first: Hui
    full: Hui Su
    id: hui-su
    last: Su
  - first: Dietrich
    full: Dietrich Klakow
    id: dietrich-klakow
    last: Klakow
  - first: Satoshi
    full: Satoshi Sekine
    id: satoshi-sekine
    last: Sekine
  author_string: Xiaoyu Shen, Jun Suzuki, Kentaro Inui, Hui Su, Dietrich Klakow, Satoshi
    Sekine
  bibkey: shen-etal-2019-select
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1054
  month: November
  page_first: '579'
  page_last: '590'
  pages: "579\u2013590"
  paper_id: '54'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1054.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1054.jpg
  title: 'Select and Attend: Towards Controllable Content Selection in Text Generation'
  title_html: 'Select and Attend: Towards Controllable Content Selection in Text Generation'
  url: https://www.aclweb.org/anthology/D19-1054
  year: '2019'
D19-1055:
  abstract: 'Building effective text generation systems requires three critical components:
    content selection, text planning, and surface realization, and traditionally they
    are tackled as separate problems. Recent all-in-one style neural generation models
    have made impressive progress, yet they often produce outputs that are incoherent
    and unfaithful to the input. To address these issues, we present an end-to-end
    trained two-step generation model, where a sentence-level content planner first
    decides on the keyphrases to cover as well as a desired language style, followed
    by a surface realization decoder that generates relevant and coherent text. For
    experiments, we consider three tasks from domains with diverse topics and varying
    language styles: persuasive argument construction from Reddit, paragraph generation
    for normal and simple versions of Wikipedia, and abstract generation for scientific
    articles. Automatic evaluation shows that our system can significantly outperform
    competitive comparisons. Human judges further rate our system generated text as
    more fluent and correct, compared to the generations by its variants that do not
    consider language style.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1055.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1055.Attachment.pdf
  author:
  - first: Xinyu
    full: Xinyu Hua
    id: xinyu-hua
    last: Hua
  - first: Lu
    full: Lu Wang
    id: lu-wang
    last: Wang
  author_string: Xinyu Hua, Lu Wang
  bibkey: hua-wang-2019-sentence
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1055
  month: November
  page_first: '591'
  page_last: '602'
  pages: "591\u2013602"
  paper_id: '55'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1055.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1055.jpg
  title: Sentence-Level Content Planning and Style Specification for Neural Text Generation
  title_html: Sentence-Level Content Planning and Style Specification for Neural Text
    Generation
  url: https://www.aclweb.org/anthology/D19-1055
  year: '2019'
D19-1056:
  abstract: We propose a Cross-lingual Encoder-Decoder model that simultaneously translates
    and generates sentences with Semantic Role Labeling annotations in a resource-poor
    target language. Unlike annotation projection techniques, our model does not need
    parallel data during inference time. Our approach can be applied in monolingual,
    multilingual and cross-lingual settings and is able to produce dependency-based
    and span-based SRL annotations. We benchmark the labeling performance of our model
    in different monolingual and multilingual settings using well-known SRL datasets.
    We then train our model in a cross-lingual setting to generate new SRL labeled
    data. Finally, we measure the effectiveness of our method by using the generated
    data to augment the training basis for resource-poor languages and perform manual
    evaluation to show that it produces high-quality sentences and assigns accurate
    semantic role annotations. Our proposed architecture offers a flexible method
    for leveraging SRL data in multiple languages.
  address: Hong Kong, China
  attachment:
  - filename: D19-1056.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1056.Attachment.zip
  author:
  - first: Angel
    full: Angel Daza
    id: angel-daza
    last: Daza
  - first: Anette
    full: Anette Frank
    id: anette-frank
    last: Frank
  author_string: Angel Daza, Anette Frank
  bibkey: daza-frank-2019-translate
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1056
  month: November
  page_first: '603'
  page_last: '615'
  pages: "603\u2013615"
  paper_id: '56'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1056.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1056.jpg
  title: Translate and Label! An Encoder-Decoder Approach for Cross-lingual Semantic
    Role Labeling
  title_html: Translate and Label! An Encoder-Decoder Approach for Cross-lingual Semantic
    Role Labeling
  url: https://www.aclweb.org/anthology/D19-1056
  year: '2019'
D19-1057:
  abstract: As a fundamental NLP task, semantic role labeling (SRL) aims to discover
    the semantic roles for each predicate within one sentence. This paper investigates
    how to incorporate syntactic knowledge into the SRL task effectively. We present
    different approaches of en- coding the syntactic information derived from dependency
    trees of different quality and representations; we propose a syntax-enhanced self-attention
    model and compare it with other two strong baseline methods; and we con- duct
    experiments with newly published deep contextualized word representations as well.
    The experiment results demonstrate that with proper incorporation of the high
    quality syntactic information, our model achieves a new state-of-the-art performance
    for the Chinese SRL task on the CoNLL-2009 dataset.
  address: Hong Kong, China
  author:
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  - first: Rui
    full: Rui Wang
    id: rui-wang
    last: Wang
  - first: Luo
    full: Luo Si
    id: luo-si
    last: Si
  author_string: Yue Zhang, Rui Wang, Luo Si
  bibkey: zhang-etal-2019-syntax
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1057
  month: November
  page_first: '616'
  page_last: '626'
  pages: "616\u2013626"
  paper_id: '57'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1057.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1057.jpg
  title: Syntax-Enhanced Self-Attention-Based Semantic Role Labeling
  title_html: Syntax-Enhanced Self-Attention-Based Semantic Role Labeling
  url: https://www.aclweb.org/anthology/D19-1057
  year: '2019'
D19-1058:
  abstract: We present VerbAtlas, a new, hand-crafted lexical-semantic resource whose
    goal is to bring together all verbal synsets from WordNet into semantically-coherent
    frames. The frames define a common, prototypical argument structure while at the
    same time providing new concept-specific information. In contrast to PropBank,
    which defines enumerative semantic roles, VerbAtlas comes with an explicit, cross-frame
    set of semantic roles linked to selectional preferences expressed in terms of
    WordNet synsets, and is the first resource enriched with semantic information
    about implicit, shadow, and default arguments. We demonstrate the effectiveness
    of VerbAtlas in the task of dependency-based Semantic Role Labeling and show how
    its integration into a high-performance system leads to improvements on both the
    in-domain and out-of-domain test sets of CoNLL-2009. VerbAtlas is available at
    http://verbatlas.org.
  address: Hong Kong, China
  author:
  - first: Andrea
    full: Andrea Di Fabio
    id: andrea-di-fabio
    last: Di Fabio
  - first: Simone
    full: Simone Conia
    id: simone-conia
    last: Conia
  - first: Roberto
    full: Roberto Navigli
    id: roberto-navigli
    last: Navigli
  author_string: Andrea Di Fabio, Simone Conia, Roberto Navigli
  bibkey: di-fabio-etal-2019-verbatlas
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1058
  month: November
  page_first: '627'
  page_last: '637'
  pages: "627\u2013637"
  paper_id: '58'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1058.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1058.jpg
  title: 'VerbAtlas: a Novel Large-Scale Verbal Semantic Resource and Its Application
    to Semantic Role Labeling'
  title_html: '<span class="acl-fixed-case">V</span>erb<span class="acl-fixed-case">A</span>tlas:
    a Novel Large-Scale Verbal Semantic Resource and Its Application to Semantic Role
    Labeling'
  url: https://www.aclweb.org/anthology/D19-1058
  year: '2019'
D19-1059:
  abstract: "We propose a simple and robust non-parameterized approach for building\
    \ sentence representations. Inspired by the Gram-Schmidt Process in geometric\
    \ theory, we build an orthogonal basis of the subspace spanned by a word and its\
    \ surrounding context in a sentence. We model the semantic meaning of a word in\
    \ a sentence based on two aspects. One is its relatedness to the word vector subspace\
    \ already spanned by its contextual words. The other is the word\u2019s novel\
    \ semantic meaning which shall be introduced as a new basis vector perpendicular\
    \ to this existing subspace. Following this motivation, we develop an innovative\
    \ method based on orthogonal basis to combine pre-trained word embeddings into\
    \ sentence representations. This approach requires zero parameters, along with\
    \ efficient inference performance. We evaluate our approach on 11 downstream NLP\
    \ tasks. Our model shows superior performance compared with non-parameterized\
    \ alternatives and it is competitive to other approaches relying on either large\
    \ amounts of labelled data or prolonged training time."
  address: Hong Kong, China
  attachment:
  - filename: D19-1059.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1059.Attachment.pdf
  author:
  - first: Ziyi
    full: Ziyi Yang
    id: ziyi-yang
    last: Yang
  - first: Chenguang
    full: Chenguang Zhu
    id: chenguang-zhu
    last: Zhu
  - first: Weizhu
    full: Weizhu Chen
    id: weizhu-chen
    last: Chen
  author_string: Ziyi Yang, Chenguang Zhu, Weizhu Chen
  bibkey: yang-etal-2019-parameter
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1059
  month: November
  page_first: '638'
  page_last: '648'
  pages: "638\u2013648"
  paper_id: '59'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1059.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1059.jpg
  title: Parameter-free Sentence Embedding via Orthogonal Basis
  title_html: Parameter-free Sentence Embedding via Orthogonal Basis
  url: https://www.aclweb.org/anthology/D19-1059
  year: '2019'
D19-1060:
  abstract: Prior work on pretrained sentence embeddings and benchmarks focus on the
    capabilities of stand-alone sentences. We propose DiscoEval, a test suite of tasks
    to evaluate whether sentence representations include broader context information.
    We also propose a variety of training objectives that makes use of natural annotations
    from Wikipedia to build sentence encoders capable of modeling discourse. We benchmark
    sentence encoders pretrained with our proposed training objectives, as well as
    other popular pretrained sentence encoders on DiscoEval and other sentence evaluation
    tasks. Empirically, we show that these training objectives help to encode different
    aspects of information in document structures. Moreover, BERT and ELMo demonstrate
    strong performances over DiscoEval with individual hidden layers showing different
    characteristics.
  address: Hong Kong, China
  attachment:
  - filename: D19-1060.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1060.Attachment.zip
  author:
  - first: Mingda
    full: Mingda Chen
    id: mingda-chen
    last: Chen
  - first: Zewei
    full: Zewei Chu
    id: zewei-chu
    last: Chu
  - first: Kevin
    full: Kevin Gimpel
    id: kevin-gimpel
    last: Gimpel
  author_string: Mingda Chen, Zewei Chu, Kevin Gimpel
  bibkey: chen-etal-2019-evaluation
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1060
  month: November
  page_first: '649'
  page_last: '662'
  pages: "649\u2013662"
  paper_id: '60'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1060.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1060.jpg
  title: Evaluation Benchmarks and Learning Criteria for Discourse-Aware Sentence
    Representations
  title_html: Evaluation Benchmarks and Learning Criteria for Discourse-Aware Sentence
    Representations
  url: https://www.aclweb.org/anthology/D19-1060
  year: '2019'
D19-1061:
  abstract: This paper describes a new dataset and experiments to determine whether
    authors of tweets possess the objects they tweet about. We work with 5,000 tweets
    and show that both humans and neural networks benefit from images in addition
    to text. We also introduce a simple yet effective strategy to incorporate visual
    information into any neural network beyond weights from pretrained networks. Specifically,
    we consider the tags identified in an image as an additional textual input, and
    leverage pretrained word embeddings as usually done with regular text. Experimental
    results show this novel strategy is beneficial.
  address: Hong Kong, China
  author:
  - first: Dhivya
    full: Dhivya Chinnappa
    id: dhivya-chinnappa
    last: Chinnappa
  - first: Srikala
    full: Srikala Murugan
    id: srikala-murugan
    last: Murugan
  - first: Eduardo
    full: Eduardo Blanco
    id: eduardo-blanco
    last: Blanco
  author_string: Dhivya Chinnappa, Srikala Murugan, Eduardo Blanco
  bibkey: chinnappa-etal-2019-extracting
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1061
  month: November
  page_first: '663'
  page_last: '672'
  pages: "663\u2013672"
  paper_id: '61'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1061.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1061.jpg
  title: 'Extracting Possessions from Social Media: Images Complement Language'
  title_html: 'Extracting Possessions from Social Media: Images Complement Language'
  url: https://www.aclweb.org/anthology/D19-1061
  year: '2019'
D19-1062:
  abstract: We introduce a large-scale crowdsourced text adventure game as a research
    platform for studying grounded dialogue. In it, agents can perceive, emote, and
    act whilst conducting dialogue with other agents. Models and humans can both act
    as characters within the game. We describe the results of training state-of-the-art
    generative and retrieval models in this setting. We show that in addition to using
    past dialogue, these models are able to effectively use the state of the underlying
    world to condition their predictions. In particular, we show that grounding on
    the details of the local environment, including location descriptions, and the
    objects (and their affordances) and characters (and their previous actions) present
    within it allows better predictions of agent behavior and dialogue. We analyze
    the ingredients necessary for successful grounding in this setting, and how each
    of these factors relate to agents that can talk and act successfully.
  address: Hong Kong, China
  attachment:
  - filename: D19-1062.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1062.Attachment.zip
  author:
  - first: Jack
    full: Jack Urbanek
    id: jack-urbanek
    last: Urbanek
  - first: Angela
    full: Angela Fan
    id: angela-fan
    last: Fan
  - first: Siddharth
    full: Siddharth Karamcheti
    id: siddharth-karamcheti
    last: Karamcheti
  - first: Saachi
    full: Saachi Jain
    id: saachi-jain
    last: Jain
  - first: Samuel
    full: Samuel Humeau
    id: samuel-humeau
    last: Humeau
  - first: Emily
    full: Emily Dinan
    id: emily-dinan
    last: Dinan
  - first: Tim
    full: "Tim Rockt\xE4schel"
    id: tim-rocktaschel
    last: "Rockt\xE4schel"
  - first: Douwe
    full: Douwe Kiela
    id: douwe-kiela
    last: Kiela
  - first: Arthur
    full: Arthur Szlam
    id: arthur-szlam
    last: Szlam
  - first: Jason
    full: Jason Weston
    id: jason-weston
    last: Weston
  author_string: "Jack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel\
    \ Humeau, Emily Dinan, Tim Rockt\xE4schel, Douwe Kiela, Arthur Szlam, Jason Weston"
  bibkey: urbanek-etal-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1062
  month: November
  page_first: '673'
  page_last: '683'
  pages: "673\u2013683"
  paper_id: '62'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1062.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1062.jpg
  title: Learning to Speak and Act in a Fantasy Text Adventure Game
  title_html: Learning to Speak and Act in a Fantasy Text Adventure Game
  url: https://www.aclweb.org/anthology/D19-1062
  year: '2019'
D19-1063:
  abstract: "Mobile agents that can leverage help from humans can potentially accomplish\
    \ more complex tasks than they could entirely on their own. We develop \u201C\
    Help, Anna!\u201D (HANNA), an interactive photo-realistic simulator in which an\
    \ agent fulfills object-finding tasks by requesting and interpreting natural language-and-vision\
    \ assistance. An agent solving tasks in a HANNA environment can leverage simulated\
    \ human assistants, called ANNA (Automatic Natural Navigation Assistants), which,\
    \ upon request, provide natural language and visual instructions to direct the\
    \ agent towards the goals. To address the HANNA problem, we develop a memory-augmented\
    \ neural agent that hierarchically models multiple levels of decision-making,\
    \ and an imitation learning algorithm that teaches the agent to avoid repeating\
    \ past mistakes while simultaneously predicting its own chances of making future\
    \ progress. Empirically, our approach is able to ask for help more effectively\
    \ than competitive baselines and, thus, attains higher task success rate on both\
    \ previously seen and previously unseen environments."
  address: Hong Kong, China
  attachment:
  - filename: D19-1063.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1063.Attachment.zip
  author:
  - first: Khanh
    full: Khanh Nguyen
    id: khanh-nguyen
    last: Nguyen
  - first: Hal
    full: "Hal Daum\xE9 III"
    id: hal-daume-iii
    last: "Daum\xE9 III"
  author_string: "Khanh Nguyen, Hal Daum\xE9 III"
  bibkey: nguyen-daume-iii-2019-help
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1063
  month: November
  page_first: '684'
  page_last: '695'
  pages: "684\u2013695"
  paper_id: '63'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1063.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1063.jpg
  title: Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective
    Curiosity-Encouraging Imitation Learning
  title_html: Help, Anna! Visual Navigation with Natural Multimodal Assistance via
    Retrospective Curiosity-Encouraging Imitation Learning
  url: https://www.aclweb.org/anthology/D19-1063
  year: '2019'
D19-1064:
  abstract: "Language grounding is an active field aiming at enriching textual representations\
    \ with visual information. Generally, textual and visual elements are embedded\
    \ in the same representation space, which implicitly assumes a one-to-one correspondence\
    \ between modalities. This hypothesis does not hold when representing words, and\
    \ becomes problematic when used to learn sentence representations \u2014 the focus\
    \ of this paper \u2014 as a visual scene can be described by a wide variety of\
    \ sentences. To overcome this limitation, we propose to transfer visual information\
    \ to textual representations by learning an intermediate representation space:\
    \ the grounded space. We further propose two new complementary objectives ensuring\
    \ that (1) sentences associated with the same visual content are close in the\
    \ grounded space and (2) similarities between related elements are preserved across\
    \ modalities. We show that this model outperforms the previous state-of-the-art\
    \ on classification and semantic relatedness tasks."
  address: Hong Kong, China
  author:
  - first: Patrick
    full: Patrick Bordes
    id: patrick-bordes
    last: Bordes
  - first: Eloi
    full: Eloi Zablocki
    id: eloi-zablocki
    last: Zablocki
  - first: Laure
    full: Laure Soulier
    id: laure-soulier
    last: Soulier
  - first: Benjamin
    full: Benjamin Piwowarski
    id: benjamin-piwowarski
    last: Piwowarski
  - first: Patrick
    full: Patrick Gallinari
    id: patrick-gallinari
    last: Gallinari
  author_string: Patrick Bordes, Eloi Zablocki, Laure Soulier, Benjamin Piwowarski,
    Patrick Gallinari
  bibkey: bordes-etal-2019-incorporating
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1064
  month: November
  page_first: '696'
  page_last: '707'
  pages: "696\u2013707"
  paper_id: '64'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1064.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1064.jpg
  title: Incorporating Visual Semantics into Sentence Representations within a Grounded
    Space
  title_html: Incorporating Visual Semantics into Sentence Representations within
    a Grounded Space
  url: https://www.aclweb.org/anthology/D19-1064
  year: '2019'
D19-1065:
  abstract: "We introduce the new Birds-to-Words dataset of 41k sentences describing\
    \ fine-grained differences between photographs of birds. The language collected\
    \ is highly detailed, while remaining understandable to the everyday observer\
    \ (e.g., \u201Cheart-shaped face,\u201D \u201Csquat body\u201D). Paragraph-length\
    \ descriptions naturally adapt to varying levels of taxonomic and visual distance\u2014\
    drawn from a novel stratified sampling approach\u2014with the appropriate level\
    \ of detail. We propose a new model called Neural Naturalist that uses a joint\
    \ image encoding and comparative module to generate comparative language, and\
    \ evaluate the results with humans who must use the descriptions to distinguish\
    \ real images. Our results indicate promising potential for neural models to explain\
    \ differences in visual embedding space using natural language, as well as a concrete\
    \ path for machine learning to aid citizen scientists in their effort to preserve\
    \ biodiversity."
  address: Hong Kong, China
  attachment:
  - filename: D19-1065.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1065.Attachment.zip
  author:
  - first: Maxwell
    full: Maxwell Forbes
    id: maxwell-forbes
    last: Forbes
  - first: Christine
    full: Christine Kaeser-Chen
    id: christine-kaeser-chen
    last: Kaeser-Chen
  - first: Piyush
    full: Piyush Sharma
    id: piyush-sharma
    last: Sharma
  - first: Serge
    full: Serge Belongie
    id: serge-belongie
    last: Belongie
  author_string: Maxwell Forbes, Christine Kaeser-Chen, Piyush Sharma, Serge Belongie
  bibkey: forbes-etal-2019-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1065
  month: November
  page_first: '708'
  page_last: '717'
  pages: "708\u2013717"
  paper_id: '65'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1065.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1065.jpg
  title: 'Neural Naturalist: Generating Fine-Grained Image Comparisons'
  title_html: 'Neural Naturalist: Generating Fine-Grained Image Comparisons'
  url: https://www.aclweb.org/anthology/D19-1065
  year: '2019'
D19-1066:
  abstract: "The Entity Linking (EL) task identifies entity mentions in a text corpus\
    \ and associates them with an unambiguous identifier in a Knowledge Base. While\
    \ much work has been done on the topic, we first present the results of a survey\
    \ that reveal a lack of consensus in the community regarding what forms of mentions\
    \ in a text and what forms of links the EL task should consider. We argue that\
    \ no one definition of the Entity Linking task fits all, and rather propose a\
    \ fine-grained categorization of different types of entity mentions and links.\
    \ We then re-annotate three EL benchmark datasets \u2013 ACE2004, KORE50, and\
    \ VoxEL \u2013 with respect to these categories. We propose a fuzzy recall metric\
    \ to address the lack of consensus and conclude with fine-grained evaluation results\
    \ comparing a selection of online EL systems."
  address: Hong Kong, China
  author:
  - first: Henry
    full: "Henry Rosales-M\xE9ndez"
    id: henry-rosales-mendez
    last: "Rosales-M\xE9ndez"
  - first: Aidan
    full: Aidan Hogan
    id: aidan-hogan
    last: Hogan
  - first: Barbara
    full: Barbara Poblete
    id: barbara-poblete
    last: Poblete
  author_string: "Henry Rosales-M\xE9ndez, Aidan Hogan, Barbara Poblete"
  bibkey: rosales-mendez-etal-2019-fine
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1066
  month: November
  page_first: '718'
  page_last: '727'
  pages: "718\u2013727"
  paper_id: '66'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1066.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1066.jpg
  title: Fine-Grained Evaluation for Entity Linking
  title_html: Fine-Grained Evaluation for Entity Linking
  url: https://www.aclweb.org/anthology/D19-1066
  year: '2019'
D19-1067:
  abstract: We propose a novel supervised open information extraction (Open IE) framework
    that leverages an ensemble of unsupervised Open IE systems and a small amount
    of labeled data to improve system performance. It uses the outputs of multiple
    unsupervised Open IE systems plus a diverse set of lexical and syntactic information
    such as word embedding, part-of-speech embedding, syntactic role embedding and
    dependency structure as its input features and produces a sequence of word labels
    indicating whether the word belongs to a relation, the arguments of the relation
    or irrelevant. Comparing with existing supervised Open IE systems, our approach
    leverages the knowledge in existing unsupervised Open IE systems to overcome the
    problem of insufficient training data. By employing multiple unsupervised Open
    IE systems, our system learns to combine the strength and avoid the weakness in
    each individual Open IE system. We have conducted experiments on multiple labeled
    benchmark data sets. Our evaluation results have demonstrated the superiority
    of the proposed method over existing supervised and unsupervised models by a significant
    margin.
  address: Hong Kong, China
  author:
  - first: Arpita
    full: Arpita Roy
    id: arpita-roy
    last: Roy
  - first: Youngja
    full: Youngja Park
    id: youngja-park
    last: Park
  - first: Taesung
    full: Taesung Lee
    id: taesung-lee
    last: Lee
  - first: Shimei
    full: Shimei Pan
    id: shimei-pan
    last: Pan
  author_string: Arpita Roy, Youngja Park, Taesung Lee, Shimei Pan
  bibkey: roy-etal-2019-supervising
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1067
  month: November
  page_first: '728'
  page_last: '737'
  pages: "728\u2013737"
  paper_id: '67'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1067.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1067.jpg
  title: Supervising Unsupervised Open Information Extraction Models
  title_html: Supervising Unsupervised Open Information Extraction Models
  url: https://www.aclweb.org/anthology/D19-1067
  year: '2019'
D19-1068:
  abstract: The scarcity in annotated data poses a great challenge for event detection
    (ED). Cross-lingual ED aims to tackle this challenge by transferring knowledge
    between different languages to boost performance. However, previous cross-lingual
    methods for ED demonstrated a heavy dependency on parallel resources, which might
    limit their applicability. In this paper, we propose a new method for cross-lingual
    ED, demonstrating a minimal dependency on parallel resources. Specifically, to
    construct a lexical mapping between different languages, we devise a context-dependent
    translation method; to treat the word order difference problem, we propose a shared
    syntactic order event detector for multilingual co-training. The efficiency of
    our method is studied through extensive experiments on two standard datasets.
    Empirical results indicate that our method is effective in 1) performing cross-lingual
    transfer concerning different directions and 2) tackling the extremely annotation-poor
    scenario.
  address: Hong Kong, China
  author:
  - first: Jian
    full: Jian Liu
    id: jian-liu
    last: Liu
  - first: Yubo
    full: Yubo Chen
    id: yubo-chen
    last: Chen
  - first: Kang
    full: Kang Liu
    id: kang-liu
    last: Liu
  - first: Jun
    full: Jun Zhao
    id: jun-zhao
    last: Zhao
  author_string: Jian Liu, Yubo Chen, Kang Liu, Jun Zhao
  bibkey: liu-etal-2019-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1068
  month: November
  page_first: '738'
  page_last: '748'
  pages: "738\u2013748"
  paper_id: '68'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1068.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1068.jpg
  title: Neural Cross-Lingual Event Detection with Minimal Parallel Resources
  title_html: Neural Cross-Lingual Event Detection with Minimal Parallel Resources
  url: https://www.aclweb.org/anthology/D19-1068
  year: '2019'
D19-1069:
  abstract: KnowledgeNet is a benchmark dataset for the task of automatically populating
    a knowledge base (Wikidata) with facts expressed in natural language text on the
    web. KnowledgeNet provides text exhaustively annotated with facts, thus enabling
    the holistic end-to-end evaluation of knowledge base population systems as a whole,
    unlike previous benchmarks that are more suitable for the evaluation of individual
    subcomponents (e.g., entity linking, relation extraction). We discuss five baseline
    approaches, where the best approach achieves an F1 score of 0.50, significantly
    outperforming a traditional approach by 79% (0.28). However, our best baseline
    is far from reaching human performance (0.82), indicating our dataset is challenging.
    The KnowledgeNet dataset and baselines are available at https://github.com/diffbot/knowledge-net
  address: Hong Kong, China
  author:
  - first: Filipe
    full: Filipe Mesquita
    id: filipe-mesquita
    last: Mesquita
  - first: Matteo
    full: Matteo Cannaviccio
    id: matteo-cannaviccio
    last: Cannaviccio
  - first: Jordan
    full: Jordan Schmidek
    id: jordan-schmidek
    last: Schmidek
  - first: Paramita
    full: Paramita Mirza
    id: paramita-mirza
    last: Mirza
  - first: Denilson
    full: Denilson Barbosa
    id: denilson-barbosa
    last: Barbosa
  author_string: Filipe Mesquita, Matteo Cannaviccio, Jordan Schmidek, Paramita Mirza,
    Denilson Barbosa
  bibkey: mesquita-etal-2019-knowledgenet
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1069
  month: November
  page_first: '749'
  page_last: '758'
  pages: "749\u2013758"
  paper_id: '69'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1069.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1069.jpg
  title: 'KnowledgeNet: A Benchmark Dataset for Knowledge Base Population'
  title_html: '<span class="acl-fixed-case">K</span>nowledge<span class="acl-fixed-case">N</span>et:
    A Benchmark Dataset for Knowledge Base Population'
  url: https://www.aclweb.org/anthology/D19-1069
  year: '2019'
D19-1070:
  abstract: "Tracking entities in procedural language requires understanding the transformations\
    \ arising from actions on entities as well as those entities\u2019 interactions.\
    \ While self-attention-based pre-trained language encoders like GPT and BERT have\
    \ been successfully applied across a range of natural language understanding tasks,\
    \ their ability to handle the nuances of procedural texts is still unknown. In\
    \ this paper, we explore the use of pre-trained transformer networks for entity\
    \ tracking tasks in procedural text. First, we test standard lightweight approaches\
    \ for prediction with pre-trained transformers, and find that these approaches\
    \ underperforms even simple baselines. We show that much stronger results can\
    \ be attained by restructuring the input to guide the model to focus on a particular\
    \ entity. Second, we assess the degree to which the transformer networks capture\
    \ the process dynamics, investigating such factors as merged entities and oblique\
    \ entity references. On two different tasks, ingredient detection in recipes and\
    \ QA over scientific processes, we achieve state-of-the-art results, but our models\
    \ still largely attend to shallow context clues and do not form complex representations\
    \ of intermediate process state."
  address: Hong Kong, China
  author:
  - first: Aditya
    full: Aditya Gupta
    id: aditya-gupta
    last: Gupta
  - first: Greg
    full: Greg Durrett
    id: greg-durrett
    last: Durrett
  author_string: Aditya Gupta, Greg Durrett
  bibkey: gupta-durrett-2019-effective
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1070
  month: November
  page_first: '759'
  page_last: '769'
  pages: "759\u2013769"
  paper_id: '70'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1070.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1070.jpg
  title: Effective Use of Transformer Networks for Entity Tracking
  title_html: Effective Use of Transformer Networks for Entity Tracking
  url: https://www.aclweb.org/anthology/D19-1070
  year: '2019'
D19-1071:
  abstract: Pre-training has proven to be effective in unsupervised machine translation
    due to its ability to model deep context information in cross-lingual scenarios.
    However, the cross-lingual information obtained from shared BPE spaces is inexplicit
    and limited. In this paper, we propose a novel cross-lingual pre-training method
    for unsupervised machine translation by incorporating explicit cross-lingual training
    signals. Specifically, we first calculate cross-lingual n-gram embeddings and
    infer an n-gram translation table from them. With those n-gram translation pairs,
    we propose a new pre-training model called Cross-lingual Masked Language Model
    (CMLM), which randomly chooses source n-grams in the input text stream and predicts
    their translation candidates at each time step. Experiments show that our method
    can incorporate beneficial cross-lingual information into pre-trained models.
    Taking pre-trained CMLM models as the encoder and decoder, we significantly improve
    the performance of unsupervised machine translation.
  address: Hong Kong, China
  author:
  - first: Shuo
    full: Shuo Ren
    id: shuo-ren
    last: Ren
  - first: Yu
    full: Yu Wu
    id: yu-wu
    last: Wu
  - first: Shujie
    full: Shujie Liu
    id: shujie-liu
    last: Liu
  - first: Ming
    full: Ming Zhou
    id: ming-zhou
    last: Zhou
  - first: Shuai
    full: Shuai Ma
    id: shuai-ma
    last: Ma
  author_string: Shuo Ren, Yu Wu, Shujie Liu, Ming Zhou, Shuai Ma
  bibkey: ren-etal-2019-explicit
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1071
  month: November
  page_first: '770'
  page_last: '779'
  pages: "770\u2013779"
  paper_id: '71'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1071.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1071.jpg
  title: Explicit Cross-lingual Pre-training for Unsupervised Machine Translation
  title_html: Explicit Cross-lingual Pre-training for Unsupervised Machine Translation
  url: https://www.aclweb.org/anthology/D19-1071
  year: '2019'
D19-1072:
  abstract: Learning target side syntactic structure has been shown to improve Neural
    Machine Translation (NMT). However, incorporating syntax through latent variables
    introduces additional complexity in inference, as the models need to marginalize
    over the latent syntactic structures. To avoid this, models often resort to greedy
    search which only allows them to explore a limited portion of the latent space.
    In this work, we introduce a new latent variable model, LaSyn, that captures the
    co-dependence between syntax and semantics, while allowing for effective and efficient
    inference over the latent space. LaSyn decouples direct dependence between successive
    latent variables, which allows its decoder to exhaustively search through the
    latent syntactic choices, while keeping decoding speed proportional to the size
    of the latent variable vocabulary. We implement LaSyn by modifying a transformer-based
    NMT system and design a neural expectation maximization algorithm that we regularize
    with part-of-speech information as the latent sequences. Evaluations on four different
    MT tasks show that incorporating target side syntax with LaSyn improves both translation
    quality, and also provides an opportunity to improve diversity.
  address: Hong Kong, China
  attachment:
  - filename: D19-1072.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1072.Attachment.pdf
  author:
  - first: Xuewen
    full: Xuewen Yang
    id: xuewen-yang
    last: Yang
  - first: Yingru
    full: Yingru Liu
    id: yingru-liu
    last: Liu
  - first: Dongliang
    full: Dongliang Xie
    id: dongliang-xie
    last: Xie
  - first: Xin
    full: Xin Wang
    id: xin-wang
    last: Wang
  - first: Niranjan
    full: Niranjan Balasubramanian
    id: niranjan-balasubramanian
    last: Balasubramanian
  author_string: Xuewen Yang, Yingru Liu, Dongliang Xie, Xin Wang, Niranjan Balasubramanian
  bibkey: yang-etal-2019-latent
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1072
  month: November
  page_first: '780'
  page_last: '790'
  pages: "780\u2013790"
  paper_id: '72'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1072.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1072.jpg
  title: Latent Part-of-Speech Sequences for Neural Machine Translation
  title_html: Latent Part-of-Speech Sequences for Neural Machine Translation
  url: https://www.aclweb.org/anthology/D19-1072
  year: '2019'
D19-1073:
  abstract: While back-translation is simple and effective in exploiting abundant
    monolingual corpora to improve low-resource neural machine translation (NMT),
    the synthetic bilingual corpora generated by NMT models trained on limited authentic
    bilingual data are inevitably noisy. In this work, we propose to quantify the
    confidence of NMT model predictions based on model uncertainty. With word- and
    sentence-level confidence measures based on uncertainty, it is possible for back-translation
    to better cope with noise in synthetic bilingual corpora. Experiments on Chinese-English
    and English-German translation tasks show that uncertainty-based confidence estimation
    significantly improves the performance of back-translation.
  address: Hong Kong, China
  author:
  - first: Shuo
    full: Shuo Wang
    id: shuo-wang
    last: Wang
  - first: Yang
    full: Yang Liu
    id: yang-liu
    last: Liu
  - first: Chao
    full: Chao Wang
    id: chao-wang
    last: Wang
  - first: Huanbo
    full: Huanbo Luan
    id: huanbo-luan
    last: Luan
  - first: Maosong
    full: Maosong Sun
    id: maosong-sun
    last: Sun
  author_string: Shuo Wang, Yang Liu, Chao Wang, Huanbo Luan, Maosong Sun
  bibkey: wang-etal-2019-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1073
  month: November
  page_first: '791'
  page_last: '802'
  pages: "791\u2013802"
  paper_id: '73'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1073.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1073.jpg
  title: Improving Back-Translation with Uncertainty-based Confidence Estimation
  title_html: Improving Back-Translation with Uncertainty-based Confidence Estimation
  url: https://www.aclweb.org/anthology/D19-1073
  year: '2019'
D19-1074:
  abstract: 'In this study, we first investigate a novel capsule network with dynamic
    routing for linear time Neural Machine Translation (NMT), referred as CapsNMT.
    CapsNMT uses an aggregation mechanism to map the source sentence into a matrix
    with pre-determined size, and then applys a deep LSTM network to decode the target
    sequence from the source representation. Unlike the previous work (CITATION) to
    store the source sentence with a passive and bottom-up way, the dynamic routing
    policy encodes the source sentence with an iterative process to decide the credit
    attribution between nodes from lower and higher layers. CapsNMT has two core properties:
    it runs in time that is linear in the length of the sequences and provides a more
    flexible way to aggregate the part-whole information of the source sentence. On
    WMT14 English-German task and a larger WMT14 English-French task, CapsNMT achieves
    comparable results with the Transformer system. To the best of our knowledge,
    this is the first work that capsule networks have been empirically investigated
    for sequence to sequence problems.'
  address: Hong Kong, China
  author:
  - first: Mingxuan
    full: Mingxuan Wang
    id: mingxuan-wang
    last: Wang
  author_string: Mingxuan Wang
  bibkey: wang-2019-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1074
  month: November
  page_first: '803'
  page_last: '812'
  pages: "803\u2013812"
  paper_id: '74'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1074.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1074.jpg
  title: Towards Linear Time Neural Machine Translation with Capsule Networks
  title_html: Towards Linear Time Neural Machine Translation with Capsule Networks
  url: https://www.aclweb.org/anthology/D19-1074
  year: '2019'
D19-1075:
  abstract: Entity alignment aims to find entities in different knowledge graphs (KGs)
    that refer to the same real-world object. An effective solution for cross-lingual
    entity alignment is crucial for many cross-lingual AI and NLP applications. Recently
    many embedding-based approaches were proposed for cross-lingual entity alignment.
    However, almost all of them are based on TransE or its variants, which have been
    demonstrated by many studies to be unsuitable for encoding multi-mapping relations
    such as 1-N, N-1 and N-N relations, thus these methods obtain low alignment precision.
    To solve this issue, we propose a new embedding-based framework. Through defining
    dot product-based functions over embeddings, our model can better capture the
    semantics of both 1-1 and multi-mapping relations. We calibrate embeddings of
    different KGs via a small set of pre-aligned seeds. We also propose a weighted
    negative sampling strategy to generate valuable negative samples during training
    and we regard prediction as a bidirectional problem in the end. Experimental results
    (especially with the metric Hits@1) on real-world multilingual datasets show that
    our approach significantly outperforms many other embedding-based approaches with
    state-of-the-art performance.
  address: Hong Kong, China
  author:
  - first: Xiaofei
    full: Xiaofei Shi
    id: xiaofei-shi
    last: Shi
  - first: Yanghua
    full: Yanghua Xiao
    id: yanghua-xiao
    last: Xiao
  author_string: Xiaofei Shi, Yanghua Xiao
  bibkey: shi-xiao-2019-modeling
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1075
  month: November
  page_first: '813'
  page_last: '822'
  pages: "813\u2013822"
  paper_id: '75'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1075.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1075.jpg
  title: Modeling Multi-mapping Relations for Precise Cross-lingual Entity Alignment
  title_html: Modeling Multi-mapping Relations for Precise Cross-lingual Entity Alignment
  url: https://www.aclweb.org/anthology/D19-1075
  year: '2019'
D19-1076:
  abstract: Enabling cross-lingual NLP tasks by leveraging multilingual word embedding
    has recently attracted much attention. An important motivation is to support lower
    resourced languages, however, most efforts focus on demonstrating the effectiveness
    of the techniques using embeddings derived from similar languages to English with
    large parallel content. In this study, we first describe the general requirements
    for the success of these techniques and then present a noise tolerant piecewise
    linear technique to learn a non-linear mapping between two monolingual word embedding
    vector spaces. We evaluate our approach on inferring bilingual dictionaries. We
    show that our technique outperforms the state-of-the-art in lower resourced settings
    with an average of 3.7% improvement of precision @10 across 14 mostly low resourced
    languages.
  address: Hong Kong, China
  author:
  - first: Masud
    full: Masud Moshtaghi
    id: masud-moshtaghi
    last: Moshtaghi
  author_string: Masud Moshtaghi
  bibkey: moshtaghi-2019-supervised
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1076
  month: November
  page_first: '823'
  page_last: '832'
  pages: "823\u2013832"
  paper_id: '76'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1076.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1076.jpg
  title: Supervised and Nonlinear Alignment of Two Embedding Spaces for Dictionary
    Induction in Low Resourced Languages
  title_html: Supervised and Nonlinear Alignment of Two Embedding Spaces for Dictionary
    Induction in Low Resourced Languages
  url: https://www.aclweb.org/anthology/D19-1076
  year: '2019'
D19-1077:
  abstract: 'Pretrained contextual representation models (Peters et al., 2018; Devlin
    et al., 2018) have pushed forward the state-of-the-art on many NLP tasks. A new
    release of BERT (Devlin, 2018) includes a model simultaneously pretrained on 104
    languages with impressive performance for zero-shot cross-lingual transfer on
    a natural language inference task. This paper explores the broader cross-lingual
    potential of mBERT (multilingual) as a zero shot language transfer model on 5
    NLP tasks covering a total of 39 languages from various language families: NLI,
    document classification, NER, POS tagging, and dependency parsing. We compare
    mBERT with the best-published methods for zero-shot cross-lingual transfer and
    find mBERT competitive on each task. Additionally, we investigate the most effective
    strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes
    away from language specific features, and measure factors that influence cross-lingual
    transfer.'
  address: Hong Kong, China
  author:
  - first: Shijie
    full: Shijie Wu
    id: shijie-wu
    last: Wu
  - first: Mark
    full: Mark Dredze
    id: mark-dredze
    last: Dredze
  author_string: Shijie Wu, Mark Dredze
  bibkey: wu-dredze-2019-beto
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1077
  month: November
  page_first: '833'
  page_last: '844'
  pages: "833\u2013844"
  paper_id: '77'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1077.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1077.jpg
  title: 'Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT'
  title_html: 'Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of <span
    class="acl-fixed-case">BERT</span>'
  url: https://www.aclweb.org/anthology/D19-1077
  year: '2019'
D19-1078:
  abstract: Previous studies on the domain adaptation for neural machine translation
    (NMT) mainly focus on the one-pass transferring out-of-domain translation knowledge
    to in-domain NMT model. In this paper, we argue that such a strategy fails to
    fully extract the domain-shared translation knowledge, and repeatedly utilizing
    corpora of different domains can lead to better distillation of domain-shared
    translation knowledge. To this end, we propose an iterative dual domain adaptation
    framework for NMT. Specifically, we first pretrain in-domain and out-of-domain
    NMT models using their own training corpora respectively, and then iteratively
    perform bidirectional translation knowledge transfer (from in-domain to out-of-domain
    and then vice versa) based on knowledge distillation until the in-domain NMT model
    convergences. Furthermore, we extend the proposed framework to the scenario of
    multiple out-of-domain training corpora, where the above-mentioned transfer is
    performed sequentially between the in-domain and each out-of-domain NMT models
    in the ascending order of their domain similarities. Empirical results on Chinese-English
    and English-German translation tasks demonstrate the effectiveness of our framework.
  address: Hong Kong, China
  author:
  - first: Jiali
    full: Jiali Zeng
    id: jiali-zeng
    last: Zeng
  - first: Yang
    full: Yang Liu
    id: yang-liu
    last: Liu
  - first: Jinsong
    full: Jinsong Su
    id: jinsong-su
    last: Su
  - first: Yubing
    full: Yubing Ge
    id: yubing-ge
    last: Ge
  - first: Yaojie
    full: Yaojie Lu
    id: yaojie-lu
    last: Lu
  - first: Yongjing
    full: Yongjing Yin
    id: yongjing-yin
    last: Yin
  - first: Jiebo
    full: Jiebo Luo
    id: jiebo-luo
    last: Luo
  author_string: Jiali Zeng, Yang Liu, Jinsong Su, Yubing Ge, Yaojie Lu, Yongjing
    Yin, Jiebo Luo
  bibkey: zeng-etal-2019-iterative
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1078
  month: November
  page_first: '845'
  page_last: '855'
  pages: "845\u2013855"
  paper_id: '78'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1078.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1078.jpg
  title: Iterative Dual Domain Adaptation for Neural Machine Translation
  title_html: Iterative Dual Domain Adaptation for Neural Machine Translation
  url: https://www.aclweb.org/anthology/D19-1078
  year: '2019'
D19-1079:
  abstract: Conventional Neural Machine Translation (NMT) models benefit from the
    training with an additional agent, e.g., dual learning, and bidirectional decoding
    with one agent decod- ing from left to right and the other decoding in the opposite
    direction. In this paper, we extend the training framework to the multi-agent
    sce- nario by introducing diverse agents in an in- teractive updating process.
    At training time, each agent learns advanced knowledge from others, and they work
    together to improve translation quality. Experimental results on NIST Chinese-English,
    IWSLT 2014 German- English, WMT 2014 English-German and large-scale Chinese-English
    translation tasks indicate that our approach achieves absolute improvements over
    the strong baseline sys- tems and shows competitive performance on all tasks.
  address: Hong Kong, China
  author:
  - first: Tianchi
    full: Tianchi Bi
    id: tianchi-bi
    last: Bi
  - first: Hao
    full: Hao Xiong
    id: hao-xiong
    last: Xiong
  - first: Zhongjun
    full: Zhongjun He
    id: zhongjun-he
    last: He
  - first: Hua
    full: Hua Wu
    id: hua-wu
    last: Wu
  - first: Haifeng
    full: Haifeng Wang
    id: haifeng-wang
    last: Wang
  author_string: Tianchi Bi, Hao Xiong, Zhongjun He, Hua Wu, Haifeng Wang
  bibkey: bi-etal-2019-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1079
  month: November
  page_first: '856'
  page_last: '865'
  pages: "856\u2013865"
  paper_id: '79'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1079.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1079.jpg
  title: Multi-agent Learning for Neural Machine Translation
  title_html: Multi-agent Learning for Neural Machine Translation
  url: https://www.aclweb.org/anthology/D19-1079
  year: '2019'
D19-1080:
  abstract: 'We present effective pre-training strategies for neural machine translation
    (NMT) using parallel corpora involving a pivot language, i.e., source-pivot and
    pivot-target, leading to a significant improvement in source-target translation.
    We propose three methods to increase the relation among source, pivot, and target
    languages in the pre-training: 1) step-wise training of a single model for different
    language pairs, 2) additional adapter component to smoothly connect pre-trained
    encoder and decoder, and 3) cross-lingual encoder training via autoencoding of
    the pivot language. Our methods greatly outperform multilingual models up to +2.6%
    BLEU in WMT 2019 French-German and German-Czech tasks. We show that our improvements
    are valid also in zero-shot/zero-resource scenarios.'
  address: Hong Kong, China
  author:
  - first: Yunsu
    full: Yunsu Kim
    id: yunsu-kim
    last: Kim
  - first: Petre
    full: Petre Petrov
    id: petre-petrov
    last: Petrov
  - first: Pavel
    full: Pavel Petrushkov
    id: pavel-petrushkov
    last: Petrushkov
  - first: Shahram
    full: Shahram Khadivi
    id: shahram-khadivi
    last: Khadivi
  - first: Hermann
    full: Hermann Ney
    id: hermann-ney
    last: Ney
  author_string: Yunsu Kim, Petre Petrov, Pavel Petrushkov, Shahram Khadivi, Hermann
    Ney
  bibkey: kim-etal-2019-pivot
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1080
  month: November
  page_first: '866'
  page_last: '876'
  pages: "866\u2013876"
  paper_id: '80'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1080.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1080.jpg
  title: Pivot-based Transfer Learning for Neural Machine Translation between Non-English
    Languages
  title_html: Pivot-based Transfer Learning for Neural Machine Translation between
    Non-<span class="acl-fixed-case">E</span>nglish Languages
  url: https://www.aclweb.org/anthology/D19-1080
  year: '2019'
D19-1081:
  abstract: 'Modern sentence-level NMT systems often produce plausible translations
    of isolated sentences. However, when put in context, these translations may end
    up being inconsistent with each other. We propose a monolingual DocRepair model
    to correct inconsistencies between sentence-level translations. DocRepair performs
    automatic post-editing on a sequence of sentence-level translations, refining
    translations of sentences in context of each other. For training, the DocRepair
    model requires only monolingual document-level data in the target language. It
    is trained as a monolingual sequence-to-sequence model that maps inconsistent
    groups of sentences into consistent ones. The consistent groups come from the
    original training data; the inconsistent groups are obtained by sampling round-trip
    translations for each isolated sentence. We show that this approach successfully
    imitates inconsistencies we aim to fix: using contrastive evaluation, we show
    large improvements in the translation of several contextual phenomena in an English-Russian
    translation task, as well as improvements in the BLEU score. We also conduct a
    human evaluation and show a strong preference of the annotators to corrected translations
    over the baseline ones. Moreover, we analyze which discourse phenomena are hard
    to capture using monolingual data only.'
  address: Hong Kong, China
  author:
  - first: Elena
    full: Elena Voita
    id: elena-voita
    last: Voita
  - first: Rico
    full: Rico Sennrich
    id: rico-sennrich
    last: Sennrich
  - first: Ivan
    full: Ivan Titov
    id: ivan-titov
    last: Titov
  author_string: Elena Voita, Rico Sennrich, Ivan Titov
  bibkey: voita-etal-2019-context
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1081
  month: November
  page_first: '877'
  page_last: '886'
  pages: "877\u2013886"
  paper_id: '81'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1081.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1081.jpg
  title: Context-Aware Monolingual Repair for Neural Machine Translation
  title_html: Context-Aware Monolingual Repair for Neural Machine Translation
  url: https://www.aclweb.org/anthology/D19-1081
  year: '2019'
D19-1082:
  abstract: "Current state-of-the-art neural machine translation (NMT) uses a deep\
    \ multi-head self-attention network with no explicit phrase information. However,\
    \ prior work on statistical machine translation has shown that extending the basic\
    \ translation unit from words to phrases has produced substantial improvements,\
    \ suggesting the possibility of improving NMT performance from explicit modeling\
    \ of phrases. In this work, we present multi-granularity self-attention (Mg-Sa):\
    \ a neural network that combines multi-head self-attention and phrase modeling.\
    \ Specifically, we train several attention heads to attend to phrases in either\
    \ n-gram or syntactic formalisms. Moreover, we exploit interactions among phrases\
    \ to enhance the strength of structure modeling \u2013 a commonly-cited weakness\
    \ of self-attention. Experimental results on WMT14 English-to-German and NIST\
    \ Chinese-to-English translation tasks show the proposed approach consistently\
    \ improves performance. Targeted linguistic analysis reveal that Mg-Sa indeed\
    \ captures useful phrase information at various levels of granularities."
  address: Hong Kong, China
  author:
  - first: Jie
    full: Jie Hao
    id: jie-hao
    last: Hao
  - first: Xing
    full: Xing Wang
    id: xing-wang
    last: Wang
  - first: Shuming
    full: Shuming Shi
    id: shuming-shi
    last: Shi
  - first: Jinfeng
    full: Jinfeng Zhang
    id: jinfeng-zhang
    last: Zhang
  - first: Zhaopeng
    full: Zhaopeng Tu
    id: zhaopeng-tu
    last: Tu
  author_string: Jie Hao, Xing Wang, Shuming Shi, Jinfeng Zhang, Zhaopeng Tu
  bibkey: hao-etal-2019-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1082
  month: November
  page_first: '887'
  page_last: '897'
  pages: "887\u2013897"
  paper_id: '82'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1082.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1082.jpg
  title: Multi-Granularity Self-Attention for Neural Machine Translation
  title_html: Multi-Granularity Self-Attention for Neural Machine Translation
  url: https://www.aclweb.org/anthology/D19-1082
  year: '2019'
D19-1083:
  abstract: The general trend in NLP is towards increasing model capacity and performance
    via deeper neural networks. However, simply stacking more layers of the popular
    Transformer architecture for machine translation results in poor convergence and
    high computational overhead. Our empirical analysis suggests that convergence
    is poor due to gradient vanishing caused by the interaction between residual connection
    and layer normalization. We propose depth-scaled initialization (DS-Init), which
    decreases parameter variance at the initialization stage, and reduces output variance
    of residual connections so as to ease gradient back-propagation through normalization
    layers. To address computational cost, we propose a merged attention sublayer
    (MAtt) which combines a simplified average-based self-attention sublayer and the
    encoder-decoder attention sublayer on the decoder side. Results on WMT and IWSLT
    translation tasks with five translation directions show that deep Transformers
    with DS-Init and MAtt can substantially outperform their base counterpart in terms
    of BLEU (+1.1 BLEU on average for 12-layer models), while matching the decoding
    speed of the baseline model thanks to the efficiency improvements of MAtt. Source
    code for reproduction will be released soon.
  address: Hong Kong, China
  author:
  - first: Biao
    full: Biao Zhang
    id: biao-zhang
    last: Zhang
  - first: Ivan
    full: Ivan Titov
    id: ivan-titov
    last: Titov
  - first: Rico
    full: Rico Sennrich
    id: rico-sennrich
    last: Sennrich
  author_string: Biao Zhang, Ivan Titov, Rico Sennrich
  bibkey: zhang-etal-2019-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1083
  month: November
  page_first: '898'
  page_last: '909'
  pages: "898\u2013909"
  paper_id: '83'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1083.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1083.jpg
  title: Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention
  title_html: Improving Deep Transformer with Depth-Scaled Initialization and Merged
    Attention
  url: https://www.aclweb.org/anthology/D19-1083
  year: '2019'
D19-1084:
  abstract: "We introduce a novel discriminative word alignment model, which we integrate\
    \ into a Transformer-based machine translation model. In experiments based on\
    \ a small number of labeled examples (\u223C1.7K\u20135K sentences) we evaluate\
    \ its performance intrinsically on both English-Chinese and English-Arabic alignment,\
    \ where we achieve major improvements over unsupervised baselines (11\u201327\
    \ F1). We evaluate the model extrinsically on data projection for Chinese NER,\
    \ showing that our alignments lead to higher performance when used to project\
    \ NER tags from English to Chinese. Finally, we perform an ablation analysis and\
    \ an annotation experiment that jointly support the utility and feasibility of\
    \ future manual alignment elicitation."
  address: Hong Kong, China
  author:
  - first: Elias
    full: Elias Stengel-Eskin
    id: elias-stengel-eskin
    last: Stengel-Eskin
  - first: Tzu-ray
    full: Tzu-ray Su
    id: tzu-ray-su1
    last: Su
  - first: Matt
    full: Matt Post
    id: matt-post
    last: Post
  - first: Benjamin
    full: Benjamin Van Durme
    id: benjamin-van-durme
    last: Van Durme
  author_string: Elias Stengel-Eskin, Tzu-ray Su, Matt Post, Benjamin Van Durme
  bibkey: stengel-eskin-etal-2019-discriminative
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1084
  month: November
  page_first: '910'
  page_last: '920'
  pages: "910\u2013920"
  paper_id: '84'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1084.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1084.jpg
  title: A Discriminative Neural Model for Cross-Lingual Word Alignment
  title_html: A Discriminative Neural Model for Cross-Lingual Word Alignment
  url: https://www.aclweb.org/anthology/D19-1084
  year: '2019'
D19-1085:
  abstract: Zero pronouns (ZPs) are frequently omitted in pro-drop languages, but
    should be recalled in non-pro-drop languages. This discourse phenomenon poses
    a significant challenge for machine translation (MT) when translating texts from
    pro-drop to non-pro-drop languages. In this paper, we propose a unified and discourse-aware
    ZP translation approach for neural MT models. Specifically, we jointly learn to
    predict and translate ZPs in an end-to-end manner, allowing both components to
    interact with each other. In addition, we employ hierarchical neural networks
    to exploit discourse-level context, which is beneficial for ZP prediction and
    thus translation. Experimental results on both Chinese-English and Japanese-English
    data show that our approach significantly and accumulatively improves both translation
    performance and ZP prediction accuracy over not only baseline but also previous
    works using external ZP prediction models. Extensive analyses confirm that the
    performance improvement comes from the alleviation of different kinds of errors
    especially caused by subjective ZPs.
  address: Hong Kong, China
  author:
  - first: Longyue
    full: Longyue Wang
    id: longyue-wang
    last: Wang
  - first: Zhaopeng
    full: Zhaopeng Tu
    id: zhaopeng-tu
    last: Tu
  - first: Xing
    full: Xing Wang
    id: xing-wang
    last: Wang
  - first: Shuming
    full: Shuming Shi
    id: shuming-shi
    last: Shi
  author_string: Longyue Wang, Zhaopeng Tu, Xing Wang, Shuming Shi
  bibkey: wang-etal-2019-one
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1085
  month: November
  page_first: '921'
  page_last: '930'
  pages: "921\u2013930"
  paper_id: '85'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1085.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1085.jpg
  title: 'One Model to Learn Both: Zero Pronoun Prediction and Translation'
  title_html: 'One Model to Learn Both: Zero Pronoun Prediction and Translation'
  url: https://www.aclweb.org/anthology/D19-1085
  year: '2019'
D19-1086:
  abstract: Previous studies have shown that neural machine translation (NMT) models
    can benefit from explicitly modeling translated () and untranslated () source
    contents as recurrent states (CITATION). However, this less interpretable recurrent
    process hinders its power to model the dynamic updating of and contents during
    decoding. In this paper, we propose to model the dynamic principles by explicitly
    separating source words into groups of translated and untranslated contents through
    parts-to-wholes assignment. The assignment is learned through a novel variant
    of routing-by-agreement mechanism (CITATION), namely Guided Dynamic Routing, where
    the translating status at each decoding step guides the routing process to assign
    each source word to its associated group (i.e., translated or untranslated content)
    represented by a capsule, enabling translation to be made from holistic context.
    Experiments show that our approach achieves substantial improvements over both
    Rnmt and Transformer by producing more adequate translations. Extensive analysis
    demonstrates that our method is highly interpretable, which is able to recognize
    the translated and untranslated contents as expected.
  address: Hong Kong, China
  attachment:
  - filename: D19-1086.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1086.Attachment.pdf
  author:
  - first: Zaixiang
    full: Zaixiang Zheng
    id: zaixiang-zheng
    last: Zheng
  - first: Shujian
    full: Shujian Huang
    id: shujian-huang
    last: Huang
  - first: Zhaopeng
    full: Zhaopeng Tu
    id: zhaopeng-tu
    last: Tu
  - first: Xin-Yu
    full: Xin-Yu Dai
    id: xinyu-dai
    last: Dai
  - first: Jiajun
    full: Jiajun Chen
    id: jiajun-chen
    last: Chen
  author_string: Zaixiang Zheng, Shujian Huang, Zhaopeng Tu, Xin-Yu Dai, Jiajun Chen
  bibkey: zheng-etal-2019-dynamic
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1086
  month: November
  page_first: '931'
  page_last: '941'
  pages: "931\u2013941"
  paper_id: '86'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1086.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1086.jpg
  title: Dynamic Past and Future for Neural Machine Translation
  title_html: Dynamic Past and Future for Neural Machine Translation
  url: https://www.aclweb.org/anthology/D19-1086
  year: '2019'
D19-1087:
  abstract: While achieving great fluency, current machine translation (MT) techniques
    are bottle-necked by adequacy issues. To have a closer study of these issues and
    accelerate model development, we propose automatic detecting adequacy errors in
    MT hypothesis for MT model evaluation. To do that, we annotate missing and wrong
    translations, the two most prevalent issues for current neural machine translation
    model, in 15000 Chinese-English translation pairs. We build a supervised alignment
    model for translation error detection (AlignDet) based on a simple Alignment Triangle
    strategy to set the benchmark for automatic error detection task. We also discuss
    the difficulties of this task and the benefits of this task for existing evaluation
    metrics.
  address: Hong Kong, China
  attachment:
  - filename: D19-1087.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1087.Attachment.pdf
  author:
  - first: Wenqiang
    full: Wenqiang Lei
    id: wenqiang-lei
    last: Lei
  - first: Weiwen
    full: Weiwen Xu
    id: weiwen-xu
    last: Xu
  - first: Ai Ti
    full: Ai Ti Aw
    id: aiti-aw
    last: Aw
  - first: Yuanxin
    full: Yuanxin Xiang
    id: yuanxin-xiang
    last: Xiang
  - first: Tat Seng
    full: Tat Seng Chua
    id: tat-seng-chua
    last: Chua
  author_string: Wenqiang Lei, Weiwen Xu, Ai Ti Aw, Yuanxin Xiang, Tat Seng Chua
  bibkey: lei-etal-2019-revisit
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1087
  month: November
  page_first: '942'
  page_last: '952'
  pages: "942\u2013952"
  paper_id: '87'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1087.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1087.jpg
  title: "Revisit Automatic Error Detection for Wrong and Missing Translation \u2013\
    \ A Supervised Approach"
  title_html: "Revisit Automatic Error Detection for Wrong and Missing Translation\
    \ \u2013 A Supervised Approach"
  url: https://www.aclweb.org/anthology/D19-1087
  year: '2019'
D19-1088:
  abstract: Although neural machine translation (NMT) has advanced the state-of-the-art
    on various language pairs, the interpretability of NMT remains unsatisfactory.
    In this work, we propose to address this gap by focusing on understanding the
    input-output behavior of NMT models. Specifically, we measure the word importance
    by attributing the NMT output to every input word through a gradient-based method.
    We validate the approach on a couple of perturbation operations, language pairs,
    and model architectures, demonstrating its superiority on identifying input words
    with higher influence on translation performance. Encouragingly, the calculated
    importance can serve as indicators of input words that are under-translated by
    NMT models. Furthermore, our analysis reveals that words of certain syntactic
    categories have higher importance while the categories vary across language pairs,
    which can inspire better design principles of NMT architectures for multi-lingual
    translation.
  address: Hong Kong, China
  attachment:
  - filename: D19-1088.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1088.Attachment.pdf
  author:
  - first: Shilin
    full: Shilin He
    id: shilin-he
    last: He
  - first: Zhaopeng
    full: Zhaopeng Tu
    id: zhaopeng-tu
    last: Tu
  - first: Xing
    full: Xing Wang
    id: xing-wang
    last: Wang
  - first: Longyue
    full: Longyue Wang
    id: longyue-wang
    last: Wang
  - first: Michael
    full: Michael Lyu
    id: michael-lyu
    last: Lyu
  - first: Shuming
    full: Shuming Shi
    id: shuming-shi
    last: Shi
  author_string: Shilin He, Zhaopeng Tu, Xing Wang, Longyue Wang, Michael Lyu, Shuming
    Shi
  bibkey: he-etal-2019-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1088
  month: November
  page_first: '953'
  page_last: '962'
  pages: "953\u2013962"
  paper_id: '88'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1088.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1088.jpg
  title: Towards Understanding Neural Machine Translation with Word Importance
  title_html: Towards Understanding Neural Machine Translation with Word Importance
  url: https://www.aclweb.org/anthology/D19-1088
  year: '2019'
D19-1089:
  abstract: 'Multilingual neural machine translation (NMT), which translates multiple
    languages using a single model, is of great practical importance due to its advantages
    in simplifying the training process, reducing online maintenance costs, and enhancing
    low-resource and zero-shot translation. Given there are thousands of languages
    in the world and some of them are very different, it is extremely burdensome to
    handle them all in a single model or use a separate model for each language pair.
    Therefore, given a fixed resource budget, e.g., the number of models, how to determine
    which languages should be supported by one model is critical to multilingual NMT,
    which, unfortunately, has been ignored by previous work. In this work, we develop
    a framework that clusters languages into different groups and trains one multilingual
    model for each cluster. We study two methods for language clustering: (1) using
    prior knowledge, where we cluster languages according to language family, and
    (2) using language embedding, in which we represent each language by an embedding
    vector and cluster them in the embedding space. In particular, we obtain the embedding
    vectors of all the languages by training a universal neural machine translation
    model. Our experiments on 23 languages show that the first clustering method is
    simple and easy to understand but leading to suboptimal translation accuracy,
    while the second method sufficiently captures the relationship among languages
    well and improves the translation accuracy for almost all the languages over baseline
    methods.'
  address: Hong Kong, China
  author:
  - first: Xu
    full: Xu Tan
    id: xu-tan
    last: Tan
  - first: Jiale
    full: Jiale Chen
    id: jiale-chen
    last: Chen
  - first: Di
    full: Di He
    id: di-he
    last: He
  - first: Yingce
    full: Yingce Xia
    id: yingce-xia
    last: Xia
  - first: Tao
    full: Tao Qin
    id: tao-qin
    last: Qin
  - first: Tie-Yan
    full: Tie-Yan Liu
    id: tie-yan-liu
    last: Liu
  author_string: Xu Tan, Jiale Chen, Di He, Yingce Xia, Tao Qin, Tie-Yan Liu
  bibkey: tan-etal-2019-multilingual
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1089
  month: November
  page_first: '963'
  page_last: '973'
  pages: "963\u2013973"
  paper_id: '89'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1089.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1089.jpg
  title: Multilingual Neural Machine Translation with Language Clustering
  title_html: Multilingual Neural Machine Translation with Language Clustering
  url: https://www.aclweb.org/anthology/D19-1089
  year: '2019'
D19-1090:
  abstract: "Human translators routinely have to translate rare inflections of words\
    \ \u2013 due to the Zipfian distribution of words in a language. When translating\
    \ from Spanish, a good translator would have no problem identifying the proper\
    \ translation of a statistically rare inflection such as habl\xE1ramos. Note the\
    \ lexeme itself, hablar, is relatively common. In this work, we investigate whether\
    \ state-of-the-art bilingual lexicon inducers are capable of learning this kind\
    \ of generalization. We introduce 40 morphologically complete dictionaries in\
    \ 10 languages and evaluate three of the best performing models on the task of\
    \ translation of less frequent morphological forms. We demonstrate that the performance\
    \ of state-of-the-art models drops considerably when evaluated on infrequent morphological\
    \ inflections and then show that adding a simple morphological constraint at training\
    \ time improves the performance, proving that the bilingual lexicon inducers can\
    \ benefit from better encoding of morphology."
  address: Hong Kong, China
  attachment:
  - filename: D19-1090.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1090.Attachment.zip
  author:
  - first: Paula
    full: Paula Czarnowska
    id: paula-czarnowska
    last: Czarnowska
  - first: Sebastian
    full: Sebastian Ruder
    id: sebastian-ruder
    last: Ruder
  - first: Edouard
    full: Edouard Grave
    id: edouard-grave
    last: Grave
  - first: Ryan
    full: Ryan Cotterell
    id: ryan-cotterell
    last: Cotterell
  - first: Ann
    full: Ann Copestake
    id: ann-copestake
    last: Copestake
  author_string: Paula Czarnowska, Sebastian Ruder, Edouard Grave, Ryan Cotterell,
    Ann Copestake
  bibkey: czarnowska-etal-2019-dont
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1090
  month: November
  page_first: '974'
  page_last: '983'
  pages: "974\u2013983"
  paper_id: '90'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1090.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1090.jpg
  title: "Don\u2019t Forget the Long Tail! A Comprehensive Analysis of Morphological\
    \ Generalization in Bilingual Lexicon Induction"
  title_html: "Don\u2019t Forget the Long Tail! A Comprehensive Analysis of Morphological\
    \ Generalization in Bilingual Lexicon Induction"
  url: https://www.aclweb.org/anthology/D19-1090
  year: '2019'
D19-1091:
  abstract: 'Recent years have seen exceptional strides in the task of automatic morphological
    inflection generation. However, for a long tail of languages the necessary resources
    are hard to come by, and state-of-the-art neural methods that work well under
    higher resource settings perform poorly in the face of a paucity of data. In response,
    we propose a battery of improvements that greatly improve performance under such
    low-resource conditions. First, we present a novel two-step attention architecture
    for the inflection decoder. In addition, we investigate the effects of cross-lingual
    transfer from single and multiple languages, as well as monolingual data hallucination.
    The macro-averaged accuracy of our models outperforms the state-of-the-art by
    15 percentage points. Also, we identify the crucial factors for success with cross-lingual
    transfer for morphological inflection: typological similarity and a common representation
    across languages.'
  address: Hong Kong, China
  author:
  - first: Antonios
    full: Antonios Anastasopoulos
    id: antonios-anastasopoulos
    last: Anastasopoulos
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  author_string: Antonios Anastasopoulos, Graham Neubig
  bibkey: anastasopoulos-neubig-2019-pushing
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1091
  month: November
  page_first: '984'
  page_last: '996'
  pages: "984\u2013996"
  paper_id: '91'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1091.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1091.jpg
  title: Pushing the Limits of Low-Resource Morphological Inflection
  title_html: Pushing the Limits of Low-Resource Morphological Inflection
  url: https://www.aclweb.org/anthology/D19-1091
  year: '2019'
D19-1092:
  abstract: Treebank translation is a promising method for cross-lingual transfer
    of syntactic dependency knowledge. The basic idea is to map dependency arcs from
    a source treebank to its target translation according to word alignments. This
    method, however, can suffer from imperfect alignment between source and target
    words. To address this problem, we investigate syntactic transfer by code mixing,
    translating only confident words in a source treebank. Cross-lingual word embeddings
    are leveraged for transferring syntactic knowledge to the target from the resulting
    code-mixed treebank. Experiments on University Dependency Treebanks show that
    code-mixed treebanks are more effective than translated treebanks, giving highly
    competitive performances among cross-lingual parsing methods.
  address: Hong Kong, China
  author:
  - first: Meishan
    full: Meishan Zhang
    id: meishan-zhang
    last: Zhang
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  - first: Guohong
    full: Guohong Fu
    id: guohong-fu
    last: Fu
  author_string: Meishan Zhang, Yue Zhang, Guohong Fu
  bibkey: zhang-etal-2019-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1092
  month: November
  page_first: '997'
  page_last: '1006'
  pages: "997\u20131006"
  paper_id: '92'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1092.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1092.jpg
  title: Cross-Lingual Dependency Parsing Using Code-Mixed TreeBank
  title_html: Cross-Lingual Dependency Parsing Using Code-Mixed <span class="acl-fixed-case">T</span>ree<span
    class="acl-fixed-case">B</span>ank
  url: https://www.aclweb.org/anthology/D19-1092
  year: '2019'
D19-1093:
  abstract: Transition-based top-down parsing with pointer networks has achieved state-of-the-art
    results in multiple parsing tasks, while having a linear time complexity. However,
    the decoder of these parsers has a sequential structure, which does not yield
    the most appropriate inductive bias for deriving tree structures. In this paper,
    we propose hierarchical pointer network parsers, and apply them to dependency
    and sentence-level discourse parsing tasks. Our results on standard benchmark
    datasets demonstrate the effectiveness of our approach, outperforming existing
    methods and setting a new state-of-the-art.
  address: Hong Kong, China
  attachment:
  - filename: D19-1093.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1093.Attachment.pdf
  author:
  - first: Linlin
    full: Linlin Liu
    id: linlin-liu
    last: Liu
  - first: Xiang
    full: Xiang Lin
    id: xiang-lin
    last: Lin
  - first: Shafiq
    full: Shafiq Joty
    id: shafiq-joty
    last: Joty
  - first: Simeng
    full: Simeng Han
    id: simeng-han
    last: Han
  - first: Lidong
    full: Lidong Bing
    id: lidong-bing
    last: Bing
  author_string: Linlin Liu, Xiang Lin, Shafiq Joty, Simeng Han, Lidong Bing
  bibkey: liu-etal-2019-hierarchical
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1093
  month: November
  page_first: '1007'
  page_last: '1017'
  pages: "1007\u20131017"
  paper_id: '93'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1093.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1093.jpg
  title: Hierarchical Pointer Net Parsing
  title_html: Hierarchical Pointer Net Parsing
  url: https://www.aclweb.org/anthology/D19-1093
  year: '2019'
D19-1094:
  abstract: The successful application of neural networks to a variety of NLP tasks
    has provided strong impetus to develop end-to-end models for semantic role labeling
    which forego the need for extensive feature engineering. Recent approaches rely
    on high-quality annotations which are costly to obtain, and mostly unavailable
    in low resource scenarios (e.g., rare languages or domains). Our work aims to
    reduce the annotation effort involved via semi-supervised learning. We propose
    an end-to-end SRL model and demonstrate it can effectively leverage unlabeled
    data under the cross-view training modeling paradigm. Our LSTM-based semantic
    role labeler is jointly trained with a sentence learner, which performs POS tagging,
    dependency parsing, and predicate identification which we argue are critical to
    learning directly from unlabeled data without recourse to external pre-processing
    tools. Experimental results on the CoNLL-2009 benchmark dataset show that our
    model outperforms the state of the art in English, and consistently improves performance
    in other languages, including Chinese, German, and Spanish.
  address: Hong Kong, China
  author:
  - first: Rui
    full: Rui Cai
    id: rui-cai
    last: Cai
  - first: Mirella
    full: Mirella Lapata
    id: mirella-lapata
    last: Lapata
  author_string: Rui Cai, Mirella Lapata
  bibkey: cai-lapata-2019-semi
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1094
  month: November
  page_first: '1018'
  page_last: '1027'
  pages: "1018\u20131027"
  paper_id: '94'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1094.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1094.jpg
  title: Semi-Supervised Semantic Role Labeling with Cross-View Training
  title_html: Semi-Supervised Semantic Role Labeling with Cross-View Training
  url: https://www.aclweb.org/anthology/D19-1094
  year: '2019'
D19-1095:
  abstract: Previous work on cross-lingual sequence labeling tasks either requires
    parallel data or bridges the two languages through word-by-word matching. Such
    requirements and assumptions are infeasible for most languages, especially for
    languages with large linguistic distances, e.g., English and Chinese. In this
    work, we propose a Multilingual Language Model with deep semantic Alignment (MLMA)
    to generate language-independent representations for cross-lingual sequence labeling.
    Our methods require only monolingual corpora with no bilingual resources at all
    and take advantage of deep contextualized representations. Experimental results
    show that our approach achieves new state-of-the-art NER and POS performance across
    European languages, and is also effective on distant language pairs such as English
    and Chinese.
  address: Hong Kong, China
  author:
  - first: Zuyi
    full: Zuyi Bao
    id: zuyi-bao
    last: Bao
  - first: Rui
    full: Rui Huang
    id: rui-huang
    last: Huang
  - first: Chen
    full: Chen Li
    id: chen-li
    last: Li
  - first: Kenny
    full: Kenny Zhu
    id: kenny-zhu
    last: Zhu
  author_string: Zuyi Bao, Rui Huang, Chen Li, Kenny Zhu
  bibkey: bao-etal-2019-low
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1095
  month: November
  page_first: '1028'
  page_last: '1039'
  pages: "1028\u20131039"
  paper_id: '95'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1095.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1095.jpg
  title: Low-Resource Sequence Labeling via Unsupervised Multilingual Contextualized
    Representations
  title_html: Low-Resource Sequence Labeling via Unsupervised Multilingual Contextualized
    Representations
  url: https://www.aclweb.org/anthology/D19-1095
  year: '2019'
D19-1096:
  abstract: Recurrent neural networks (RNN) used for Chinese named entity recognition
    (NER) that sequentially track character and word information have achieved great
    success. However, the characteristic of chain structure and the lack of global
    semantics determine that RNN-based models are vulnerable to word ambiguities.
    In this work, we try to alleviate this problem by introducing a lexicon-based
    graph neural network with global semantics, in which lexicon knowledge is used
    to connect characters to capture the local composition, while a global relay node
    can capture global sentence semantics and long-range dependency. Based on the
    multiple graph-based interactions among characters, potential words, and the whole-sentence
    semantics, word ambiguities can be effectively tackled. Experiments on four NER
    datasets show that the proposed model achieves significant improvements against
    other baseline models.
  address: Hong Kong, China
  author:
  - first: Tao
    full: Tao Gui
    id: tao-gui
    last: Gui
  - first: Yicheng
    full: Yicheng Zou
    id: yicheng-zou
    last: Zou
  - first: Qi
    full: Qi Zhang
    id: qi-zhang
    last: Zhang
  - first: Minlong
    full: Minlong Peng
    id: minlong-peng
    last: Peng
  - first: Jinlan
    full: Jinlan Fu
    id: jinlan-fu
    last: Fu
  - first: Zhongyu
    full: Zhongyu Wei
    id: zhongyu-wei
    last: Wei
  - first: Xuanjing
    full: Xuanjing Huang
    id: xuan-jing-huang
    last: Huang
  author_string: Tao Gui, Yicheng Zou, Qi Zhang, Minlong Peng, Jinlan Fu, Zhongyu
    Wei, Xuanjing Huang
  bibkey: gui-etal-2019-lexicon
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1096
  month: November
  page_first: '1040'
  page_last: '1050'
  pages: "1040\u20131050"
  paper_id: '96'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1096.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1096.jpg
  title: A Lexicon-Based Graph Neural Network for Chinese NER
  title_html: A Lexicon-Based Graph Neural Network for <span class="acl-fixed-case">C</span>hinese
    <span class="acl-fixed-case">NER</span>
  url: https://www.aclweb.org/anthology/D19-1096
  year: '2019'
D19-1097:
  abstract: Spoken Language Understanding (SLU) mainly involves two tasks, intent
    detection and slot filling, which are generally modeled jointly in existing works.
    However, most existing models fail to fully utilize cooccurrence relations between
    slots and intents, which restricts their potential performance. To address this
    issue, in this paper we propose a novel Collaborative Memory Network (CM-Net)
    based on the well-designed block, named CM-block. The CM-block firstly captures
    slot-specific and intent-specific features from memories in a collaborative manner,
    and then uses these enriched features to enhance local context representations,
    based on which the sequential information flow leads to more specific (slot and
    intent) global utterance representations. Through stacking multiple CM-blocks,
    our CM-Net is able to alternately perform information exchange among specific
    memories, local contexts and the global utterance, and thus incrementally enriches
    each other. We evaluate the CM-Net on two standard benchmarks (ATIS and SNIPS)
    and a self-collected corpus (CAIS). Experimental results show that the CM-Net
    achieves the state-of-the-art results on the ATIS and SNIPS in most of criteria,
    and significantly outperforms the baseline models on the CAIS. Additionally, we
    make the CAIS dataset publicly available for the research community.
  address: Hong Kong, China
  attachment:
  - filename: D19-1097.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1097.Attachment.pdf
  author:
  - first: Yijin
    full: Yijin Liu
    id: yijin-liu
    last: Liu
  - first: Fandong
    full: Fandong Meng
    id: fandong-meng
    last: Meng
  - first: Jinchao
    full: Jinchao Zhang
    id: jinchao-zhang
    last: Zhang
  - first: Jie
    full: Jie Zhou
    id: jie-zhou
    last: Zhou
  - first: Yufeng
    full: Yufeng Chen
    id: yufeng-chen
    last: Chen
  - first: Jinan
    full: Jinan Xu
    id: jinan-xu
    last: Xu
  author_string: Yijin Liu, Fandong Meng, Jinchao Zhang, Jie Zhou, Yufeng Chen, Jinan
    Xu
  bibkey: liu-etal-2019-cm
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1097
  month: November
  page_first: '1051'
  page_last: '1060'
  pages: "1051\u20131060"
  paper_id: '97'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1097.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1097.jpg
  title: 'CM-Net: A Novel Collaborative Memory Network for Spoken Language Understanding'
  title_html: '<span class="acl-fixed-case">CM</span>-Net: A Novel Collaborative Memory
    Network for Spoken Language Understanding'
  url: https://www.aclweb.org/anthology/D19-1097
  year: '2019'
D19-1098:
  abstract: "Pre-training Transformer from large-scale raw texts and fine-tuning on\
    \ the desired task have achieved state-of-the-art results on diverse NLP tasks.\
    \ However, it is unclear what the learned attention captures. The attention computed\
    \ by attention heads seems not to match human intuitions about hierarchical structures.\
    \ This paper proposes Tree Transformer, which adds an extra constraint to attention\
    \ heads of the bidirectional Transformer encoder in order to encourage the attention\
    \ heads to follow tree structures. The tree structures can be automatically induced\
    \ from raw texts by our proposed \u201CConstituent Attention\u201D module, which\
    \ is simply implemented by self-attention between two adjacent words. With the\
    \ same training procedure identical to BERT, the experiments demonstrate the effectiveness\
    \ of Tree Transformer in terms of inducing tree structures, better language modeling,\
    \ and further learning more explainable attention scores."
  address: Hong Kong, China
  attachment:
  - filename: D19-1098.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1098.Attachment.pdf
  author:
  - first: Yaushian
    full: Yaushian Wang
    id: yaushian-wang
    last: Wang
  - first: Hung-Yi
    full: Hung-Yi Lee
    id: hung-yi-lee
    last: Lee
  - first: Yun-Nung
    full: Yun-Nung Chen
    id: yun-nung-chen
    last: Chen
  author_string: Yaushian Wang, Hung-Yi Lee, Yun-Nung Chen
  bibkey: wang-etal-2019-tree
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1098
  month: November
  page_first: '1061'
  page_last: '1070'
  pages: "1061\u20131070"
  paper_id: '98'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1098.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1098.jpg
  title: 'Tree Transformer: Integrating Tree Structures into Self-Attention'
  title_html: 'Tree Transformer: Integrating Tree Structures into Self-Attention'
  url: https://www.aclweb.org/anthology/D19-1098
  year: '2019'
D19-1099:
  abstract: Modern state-of-the-art Semantic Role Labeling (SRL) methods rely on expressive
    sentence encoders (e.g., multi-layer LSTMs) but tend to model only local (if any)
    interactions between individual argument labeling decisions. This contrasts with
    earlier work and also with the intuition that the labels of individual arguments
    are strongly interdependent. We model interactions between argument labeling decisions
    through iterative refinement. Starting with an output produced by a factorized
    model, we iteratively refine it using a refinement network. Instead of modeling
    arbitrary interactions among roles and words, we encode prior knowledge about
    the SRL problem by designing a restricted network architecture capturing non-local
    interactions. This modeling choice prevents overfitting and results in an effective
    model, outperforming strong factorized baseline models on all 7 CoNLL-2009 languages,
    and achieving state-of-the-art results on 5 of them, including English.
  address: Hong Kong, China
  attachment:
  - filename: D19-1099.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1099.Attachment.pdf
  author:
  - first: Chunchuan
    full: Chunchuan Lyu
    id: chunchuan-lyu
    last: Lyu
  - first: Shay B.
    full: Shay B. Cohen
    id: shay-b-cohen
    last: Cohen
  - first: Ivan
    full: Ivan Titov
    id: ivan-titov
    last: Titov
  author_string: Chunchuan Lyu, Shay B. Cohen, Ivan Titov
  bibkey: lyu-etal-2019-semantic
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1099
  month: November
  page_first: '1071'
  page_last: '1082'
  pages: "1071\u20131082"
  paper_id: '99'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1099.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1099.jpg
  title: Semantic Role Labeling with Iterative Structure Refinement
  title_html: Semantic Role Labeling with Iterative Structure Refinement
  url: https://www.aclweb.org/anthology/D19-1099
  year: '2019'
D19-1100:
  abstract: 'Although over 100 languages are supported by strong off-the-shelf machine
    translation systems, only a subset of them possess large annotated corpora for
    named entity recognition. Motivated by this fact, we leverage machine translation
    to improve annotation-projection approaches to cross-lingual named entity recognition.
    We propose a system that improves over prior entity-projection methods by: (a)
    leveraging machine translation systems twice: first for translating sentences
    and subsequently for translating entities; (b) matching entities based on orthographic
    and phonetic similarity; and (c) identifying matches based on distributional statistics
    derived from the dataset. Our approach improves upon current state-of-the-art
    methods for cross-lingual named entity recognition on 5 diverse languages by an
    average of 4.1 points. Further, our method achieves state-of-the-art F_1 scores
    for Armenian, outperforming even a monolingual model trained on Armenian source
    data.'
  address: Hong Kong, China
  author:
  - first: Alankar
    full: Alankar Jain
    id: alankar-jain
    last: Jain
  - first: Bhargavi
    full: Bhargavi Paranjape
    id: bhargavi-paranjape
    last: Paranjape
  - first: Zachary C.
    full: Zachary C. Lipton
    id: zachary-c-lipton
    last: Lipton
  author_string: Alankar Jain, Bhargavi Paranjape, Zachary C. Lipton
  bibkey: jain-etal-2019-entity
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1100
  month: November
  page_first: '1083'
  page_last: '1092'
  pages: "1083\u20131092"
  paper_id: '100'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1100.pdf
  publisher: Association for Computational Linguistics
  revision:
  - id: '1'
    url: https://www.aclweb.org/anthology/D19-1100v1.pdf
    value: D19-1100v1
  - explanation: Added missing reference section.
    id: '2'
    url: https://www.aclweb.org/anthology/D19-1100v2.pdf
    value: D19-1100v2
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1100.jpg
  title: Entity Projection via Machine Translation for Cross-Lingual NER
  title_html: Entity Projection via Machine Translation for Cross-Lingual <span class="acl-fixed-case">NER</span>
  url: https://www.aclweb.org/anthology/D19-1100
  year: '2019'
D19-1101:
  abstract: Current methods for sequence tagging, a core task in NLP, are data hungry,
    which motivates the use of crowdsourcing as a cheap way to obtain labelled data.
    However, annotators are often unreliable and current aggregation methods cannot
    capture common types of span annotation error. To address this, we propose a Bayesian
    method for aggregating sequence tags that reduces errors by modelling sequential
    dependencies between the annotations as well as the ground-truth labels. By taking
    a Bayesian approach, we account for uncertainty in the model due to both annotator
    errors and the lack of data for modelling annotators who complete few tasks. We
    evaluate our model on crowdsourced data for named entity recognition, information
    extraction and argument mining, showing that our sequential model outperforms
    the previous state of the art, and that Bayesian approaches outperform non-Bayesian
    alternatives. We also find that our approach can reduce crowdsourcing costs through
    more effective active learning, as it better captures uncertainty in the sequence
    labels when there are few annotations.
  address: Hong Kong, China
  author:
  - first: Edwin D.
    full: Edwin D. Simpson
    id: edwin-d-simpson
    last: Simpson
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Edwin D. Simpson, Iryna Gurevych
  bibkey: simpson-gurevych-2019-bayesian
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1101
  month: November
  page_first: '1093'
  page_last: '1104'
  pages: "1093\u20131104"
  paper_id: '101'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1101.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1101.jpg
  title: A Bayesian Approach for Sequence Tagging with Crowds
  title_html: A <span class="acl-fixed-case">B</span>ayesian Approach for Sequence
    Tagging with Crowds
  url: https://www.aclweb.org/anthology/D19-1101
  year: '2019'
D19-1102:
  abstract: "Parsers are available for only a handful of the world\u2019s languages,\
    \ since they require lots of training data. How far can we get with just a small\
    \ amount of training data? We systematically compare a set of simple strategies\
    \ for improving low-resource parsers: data augmentation, which has not been tested\
    \ before; cross-lingual training; and transliteration. Experimenting on three\
    \ typologically diverse low-resource languages\u2014North S\xE1mi, Galician, and\
    \ Kazah\u2014We find that (1) when only the low-resource treebank is available,\
    \ data augmentation is very helpful; (2) when a related high-resource treebank\
    \ is available, cross-lingual training is helpful and complements data augmentation;\
    \ and (3) when the high-resource treebank uses a different writing system, transliteration\
    \ into a shared orthographic spaces is also very helpful."
  address: Hong Kong, China
  attachment:
  - filename: D19-1102.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1102.Attachment.pdf
  author:
  - first: Clara
    full: Clara Vania
    id: clara-vania
    last: Vania
  - first: Yova
    full: Yova Kementchedjhieva
    id: yova-kementchedjhieva
    last: Kementchedjhieva
  - first: Anders
    full: "Anders S\xF8gaard"
    id: anders-sogaard
    last: "S\xF8gaard"
  - first: Adam
    full: Adam Lopez
    id: adam-lopez
    last: Lopez
  author_string: "Clara Vania, Yova Kementchedjhieva, Anders S\xF8gaard, Adam Lopez"
  bibkey: vania-etal-2019-systematic
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1102
  month: November
  page_first: '1105'
  page_last: '1116'
  pages: "1105\u20131116"
  paper_id: '102'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1102.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1102.jpg
  title: A systematic comparison of methods for low-resource dependency parsing on
    genuinely low-resource languages
  title_html: A systematic comparison of methods for low-resource dependency parsing
    on genuinely low-resource languages
  url: https://www.aclweb.org/anthology/D19-1102
  year: '2019'
D19-1103:
  abstract: Prior work on cross-lingual dependency parsing often focuses on capturing
    the commonalities between source and target languages and overlook the potential
    to leverage the linguistic properties of the target languages to facilitate the
    transfer. In this paper, we show that weak supervisions of linguistic knowledge
    for the target languages can improve a cross-lingual graph-based dependency parser
    substantially. Specifically, we explore several types of corpus linguistic statistics
    and compile them into corpus-statistics constraints to facilitate the inference
    procedure. We propose new algorithms that adapt two techniques, Lagrangian relaxation
    and posterior regularization, to conduct inference with corpus-statistics constraints.
    Experiments show that the Lagrangian relaxation and posterior regularization techniques
    improve the performances on 15 and 17 out of 19 target languages, respectively.
    The improvements are especially large for the target languages that have different
    word order features from the source language.
  address: Hong Kong, China
  attachment:
  - filename: D19-1103.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1103.Attachment.pdf
  author:
  - first: Tao
    full: Tao Meng
    id: tao-meng
    last: Meng
  - first: Nanyun
    full: Nanyun Peng
    id: nanyun-peng
    last: Peng
  - first: Kai-Wei
    full: Kai-Wei Chang
    id: kai-wei-chang
    last: Chang
  author_string: Tao Meng, Nanyun Peng, Kai-Wei Chang
  bibkey: meng-etal-2019-target
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1103
  month: November
  page_first: '1117'
  page_last: '1128'
  pages: "1117\u20131128"
  paper_id: '103'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1103.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1103.jpg
  title: Target Language-Aware Constrained Inference for Cross-lingual Dependency
    Parsing
  title_html: Target Language-Aware Constrained Inference for Cross-lingual Dependency
    Parsing
  url: https://www.aclweb.org/anthology/D19-1103
  year: '2019'
D19-1104:
  abstract: "Computing devices have recently become capable of interacting with their\
    \ end users via natural language. However, they can only operate within a limited\
    \ \u201Csupported\u201D domain of discourse and fail drastically when faced with\
    \ an out-of-domain utterance, mainly due to the limitations of their semantic\
    \ parser. In this paper, we propose a semantic parser that generalizes to out-of-domain\
    \ examples by learning a general strategy for parsing an unseen utterance through\
    \ adapting the logical forms of seen utterances, instead of learning to generate\
    \ a logical form from scratch. Our parser maintains a memory consisting of a representative\
    \ subset of the seen utterances paired with their logical forms. Given an unseen\
    \ utterance, our parser works by looking up a similar utterance from the memory\
    \ and adapting its logical form until it fits the unseen utterance. Moreover,\
    \ we present a data generation strategy for constructing utterance-logical form\
    \ pairs from different domains. Our results show an improvement of up to 68.8%\
    \ on one-shot parsing under two different evaluation settings compared to the\
    \ baselines."
  address: Hong Kong, China
  attachment:
  - filename: D19-1104.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1104.Attachment.zip
  author:
  - first: Zhichu
    full: Zhichu Lu
    id: zhichu-lu
    last: Lu
  - first: Forough
    full: Forough Arabshahi
    id: forough-arabshahi
    last: Arabshahi
  - first: Igor
    full: Igor Labutov
    id: igor-labutov
    last: Labutov
  - first: Tom
    full: Tom Mitchell
    id: tom-mitchell
    last: Mitchell
  author_string: Zhichu Lu, Forough Arabshahi, Igor Labutov, Tom Mitchell
  bibkey: lu-etal-2019-look
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1104
  month: November
  page_first: '1129'
  page_last: '1139'
  pages: "1129\u20131139"
  paper_id: '104'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1104.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1104.jpg
  title: 'Look-up and Adapt: A One-shot Semantic Parser'
  title_html: 'Look-up and Adapt: A One-shot Semantic Parser'
  url: https://www.aclweb.org/anthology/D19-1104
  year: '2019'
D19-1105:
  abstract: "The segmentation problem is one of the fundamental challenges associated\
    \ with name entity recognition (NER) tasks that aim to reduce the boundary error\
    \ when detecting a sequence of entity words. A considerable number of advanced\
    \ approaches have been proposed and most of them exhibit performance deterioration\
    \ when entities become longer. Inspired by previous work in which a multi-task\
    \ strategy is used to solve segmentation problems, we design a similarity based\
    \ auxiliary classifier (SAC), which can distinguish entity words from non-entity\
    \ words. Unlike conventional classifiers, SAC uses vectors to indicate tags. Therefore,\
    \ SAC can calculate the similarities between words and tags, and then compute\
    \ a weighted sum of the tag vectors, which can be considered a useful feature\
    \ for NER tasks. Empirical results are used to verify the rationality of the SAC\
    \ structure and demonstrate the SAC model\u2019s potential in performance improvement\
    \ against our baseline approaches."
  address: Hong Kong, China
  author:
  - first: Shiyuan
    full: Shiyuan Xiao
    id: shiyuan-xiao
    last: Xiao
  - first: Yuanxin
    full: Yuanxin Ouyang
    id: yuanxin-ouyang
    last: Ouyang
  - first: Wenge
    full: Wenge Rong
    id: wenge-rong
    last: Rong
  - first: Jianxin
    full: Jianxin Yang
    id: jianxin-yang
    last: Yang
  - first: Zhang
    full: Zhang Xiong
    id: zhang-xiong
    last: Xiong
  author_string: Shiyuan Xiao, Yuanxin Ouyang, Wenge Rong, Jianxin Yang, Zhang Xiong
  bibkey: xiao-etal-2019-similarity
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1105
  month: November
  page_first: '1140'
  page_last: '1149'
  pages: "1140\u20131149"
  paper_id: '105'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1105.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1105.jpg
  title: Similarity Based Auxiliary Classifier for Named Entity Recognition
  title_html: Similarity Based Auxiliary Classifier for Named Entity Recognition
  url: https://www.aclweb.org/anthology/D19-1105
  year: '2019'
D19-1106:
  abstract: This paper describes a method of variable beam size inference for Recurrent
    Neural Network Grammar (rnng) by drawing inspiration from sequential Monte-Carlo
    methods such as particle filtering. The paper studies the relevance of such methods
    for speeding up the computations of direct generative parsing for rnng. But it
    also studies the potential cognitive interpretation of the underlying representations
    built by the search method (beam activity) through analysis of neuro-imaging signal.
  address: Hong Kong, China
  attachment:
  - filename: D19-1106.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1106.Attachment.zip
  author:
  - first: Benoit
    full: "Benoit Crabb\xE9"
    id: benoit-crabbe
    last: "Crabb\xE9"
  - first: Murielle
    full: Murielle Fabre
    id: murielle-fabre
    last: Fabre
  - first: Christophe
    full: Christophe Pallier
    id: christophe-pallier
    last: Pallier
  author_string: "Benoit Crabb\xE9, Murielle Fabre, Christophe Pallier"
  bibkey: crabbe-etal-2019-variable
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1106
  month: November
  page_first: '1150'
  page_last: '1160'
  pages: "1150\u20131160"
  paper_id: '106'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1106.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1106.jpg
  title: Variable beam search for generative neural parsing and its relevance for
    the analysis of neuro-imaging signal
  title_html: Variable beam search for generative neural parsing and its relevance
    for the analysis of neuro-imaging signal
  url: https://www.aclweb.org/anthology/D19-1106
  year: '2019'
D19-1107:
  abstract: Crowdsourcing has been the prevalent paradigm for creating natural language
    understanding datasets in recent years. A common crowdsourcing practice is to
    recruit a small number of high-quality workers, and have them massively generate
    examples. Having only a few workers generate the majority of examples raises concerns
    about data diversity, especially when workers freely generate sentences. In this
    paper, we perform a series of experiments showing these concerns are evident in
    three recent NLP datasets. We show that model performance improves when training
    with annotator identifiers as features, and that models are able to recognize
    the most productive annotators. Moreover, we show that often models do not generalize
    well to examples from annotators that did not contribute to the training set.
    Our findings suggest that annotator bias should be monitored during dataset creation,
    and that test set annotators should be disjoint from training set annotators.
  address: Hong Kong, China
  author:
  - first: Mor
    full: Mor Geva
    id: mor-geva
    last: Geva
  - first: Yoav
    full: Yoav Goldberg
    id: yoav-goldberg
    last: Goldberg
  - first: Jonathan
    full: Jonathan Berant
    id: jonathan-berant
    last: Berant
  author_string: Mor Geva, Yoav Goldberg, Jonathan Berant
  bibkey: geva-etal-2019-modeling
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1107
  month: November
  page_first: '1161'
  page_last: '1166'
  pages: "1161\u20131166"
  paper_id: '107'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1107.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1107.jpg
  title: Are We Modeling the Task or the Annotator? An Investigation of Annotator
    Bias in Natural Language Understanding Datasets
  title_html: Are We Modeling the Task or the Annotator? An Investigation of Annotator
    Bias in Natural Language Understanding Datasets
  url: https://www.aclweb.org/anthology/D19-1107
  year: '2019'
D19-1108:
  abstract: We design a generic framework for learning a robust text classification
    model that achieves high accuracy under different selection budgets (a.k.a selection
    rates) at test-time. We take a different approach from existing methods and learn
    to dynamically filter a large fraction of unimportant words by a low-complexity
    selector such that any high-complexity state-of-art classifier only needs to process
    a small fraction of text, relevant for the target task. To this end, we propose
    a data aggregation method to train the classifier, allowing it to achieve competitive
    performance on fractured sentences. On four benchmark text classification tasks,
    we demonstrate that the framework gains consistent speedup with little degradation
    in accuracy on various selection budgets.
  address: Hong Kong, China
  attachment:
  - filename: D19-1108.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1108.Attachment.pdf
  author:
  - first: Md Rizwan
    full: Md Rizwan Parvez
    id: md-rizwan-parvez
    last: Parvez
  - first: Tolga
    full: Tolga Bolukbasi
    id: tolga-bolukbasi
    last: Bolukbasi
  - first: Kai-Wei
    full: Kai-Wei Chang
    id: kai-wei-chang
    last: Chang
  - first: Venkatesh
    full: Venkatesh Saligrama
    id: venkatesh-saligrama
    last: Saligrama
  author_string: Md Rizwan Parvez, Tolga Bolukbasi, Kai-Wei Chang, Venkatesh Saligrama
  bibkey: parvez-etal-2019-robust
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1108
  month: November
  page_first: '1167'
  page_last: '1172'
  pages: "1167\u20131172"
  paper_id: '108'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1108.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1108.jpg
  title: Robust Text Classifier on Test-Time Budgets
  title_html: Robust Text Classifier on Test-Time Budgets
  url: https://www.aclweb.org/anthology/D19-1108
  year: '2019'
D19-1109:
  abstract: "Inferring commonsense knowledge is a key challenge in machine learning.\
    \ Due to the sparsity of training data, previous work has shown that supervised\
    \ methods for commonsense knowledge mining underperform when evaluated on novel\
    \ data. In this work, we develop a method for generating commonsense knowledge\
    \ using a large, pre-trained bidirectional language model. By transforming relational\
    \ triples into masked sentences, we can use this model to rank a triple\u2019\
    s validity by the estimated pointwise mutual information between the two entities.\
    \ Since we do not update the weights of the bidirectional model, our approach\
    \ is not biased by the coverage of any one commonsense knowledge base. Though\
    \ we do worse on a held-out test set than models explicitly trained on a corresponding\
    \ training set, our approach outperforms these methods when mining commonsense\
    \ knowledge from new sources, suggesting that our unsupervised technique generalizes\
    \ better than current supervised approaches."
  address: Hong Kong, China
  attachment:
  - filename: D19-1109.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1109.Attachment.zip
  author:
  - first: Joe
    full: Joe Davison
    id: joe-davison
    last: Davison
  - first: Joshua
    full: Joshua Feldman
    id: joshua-feldman
    last: Feldman
  - first: Alexander
    full: Alexander Rush
    id: alexander-m-rush
    last: Rush
  author_string: Joe Davison, Joshua Feldman, Alexander Rush
  bibkey: davison-etal-2019-commonsense
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1109
  month: November
  page_first: '1173'
  page_last: '1178'
  pages: "1173\u20131178"
  paper_id: '109'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1109.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1109.jpg
  title: Commonsense Knowledge Mining from Pretrained Models
  title_html: Commonsense Knowledge Mining from Pretrained Models
  url: https://www.aclweb.org/anthology/D19-1109
  year: '2019'
D19-1110:
  abstract: "Neural models for NLP typically use large numbers of parameters to reach\
    \ state-of-the-art performance, which can lead to excessive memory usage and increased\
    \ runtime. We present a structure learning method for learning sparse, parameter-efficient\
    \ NLP models. Our method applies group lasso to rational RNNs (Peng et al., 2018),\
    \ a family of models that is closely connected to weighted finite-state automata\
    \ (WFSAs). We take advantage of rational RNNs\u2019 natural grouping of the weights,\
    \ so the group lasso penalty directly removes WFSA states, substantially reducing\
    \ the number of parameters in the model. Our experiments on a number of sentiment\
    \ analysis datasets, using both GloVe and BERT embeddings, show that our approach\
    \ learns neural structures which have fewer parameters without sacrificing performance\
    \ relative to parameter-rich baselines. Our method also highlights the interpretable\
    \ properties of rational RNNs. We show that sparsifying such models makes them\
    \ easier to visualize, and we present models that rely exclusively on as few as\
    \ three WFSAs after pruning more than 90% of the weights. We publicly release\
    \ our code."
  address: Hong Kong, China
  attachment:
  - filename: D19-1110.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1110.Attachment.pdf
  author:
  - first: Jesse
    full: Jesse Dodge
    id: jesse-dodge
    last: Dodge
  - first: Roy
    full: Roy Schwartz
    id: roy-schwartz
    last: Schwartz
  - first: Hao
    full: Hao Peng
    id: hao-peng
    last: Peng
  - first: Noah A.
    full: Noah A. Smith
    id: noah-a-smith
    last: Smith
  author_string: Jesse Dodge, Roy Schwartz, Hao Peng, Noah A. Smith
  bibkey: dodge-etal-2019-rnn
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1110
  month: November
  page_first: '1179'
  page_last: '1184'
  pages: "1179\u20131184"
  paper_id: '110'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1110.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1110.jpg
  title: RNN Architecture Learning with Sparse Regularization
  title_html: <span class="acl-fixed-case">RNN</span> Architecture Learning with Sparse
    Regularization
  url: https://www.aclweb.org/anthology/D19-1110
  year: '2019'
D19-1111:
  abstract: 'Word embeddings are useful for a wide variety of tasks, but they lack
    interpretability. By rotating word spaces, interpretable dimensions can be identified
    while preserving the information contained in the embeddings without any loss.
    In this work, we investigate three methods for making word spaces interpretable
    by rotation: Densifier (Rothe et al., 2016), linear SVMs and DensRay, a new method
    we propose. In contrast to Densifier, DensRay can be computed in closed form,
    is hyperparameter-free and thus more robust than Densifier. We evaluate the three
    methods on lexicon induction and set-based word analogy. In addition we provide
    qualitative insights as to how interpretable word spaces can be used for removing
    gender bias from embeddings.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1111.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1111.Attachment.pdf
  author:
  - first: Philipp
    full: Philipp Dufter
    id: philipp-dufter
    last: Dufter
  - first: Hinrich
    full: "Hinrich Sch\xFCtze"
    id: hinrich-schutze
    last: "Sch\xFCtze"
  author_string: "Philipp Dufter, Hinrich Sch\xFCtze"
  bibkey: dufter-schutze-2019-analytical
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1111
  month: November
  page_first: '1185'
  page_last: '1191'
  pages: "1185\u20131191"
  paper_id: '111'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1111.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1111.jpg
  title: Analytical Methods for Interpretable Ultradense Word Embeddings
  title_html: Analytical Methods for Interpretable Ultradense Word Embeddings
  url: https://www.aclweb.org/anthology/D19-1111
  year: '2019'
D19-1112:
  abstract: Learning general representations of text is a fundamental problem for
    many natural language understanding (NLU) tasks. Previously, researchers have
    proposed to use language model pre-training and multi-task learning to learn robust
    representations. However, these methods can achieve sub-optimal performance in
    low-resource scenarios. Inspired by the recent success of optimization-based meta-learning
    algorithms, in this paper, we explore the model-agnostic meta-learning algorithm
    (MAML) and its variants for low-resource NLU tasks. We validate our methods on
    the GLUE benchmark and show that our proposed models can outperform several strong
    baselines. We further empirically demonstrate that the learned representations
    can be adapted to new tasks efficiently and effectively.
  address: Hong Kong, China
  attachment:
  - filename: D19-1112.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1112.Attachment.pdf
  author:
  - first: Zi-Yi
    full: Zi-Yi Dou
    id: zi-yi-dou
    last: Dou
  - first: Keyi
    full: Keyi Yu
    id: keyi-yu
    last: Yu
  - first: Antonios
    full: Antonios Anastasopoulos
    id: antonios-anastasopoulos
    last: Anastasopoulos
  author_string: Zi-Yi Dou, Keyi Yu, Antonios Anastasopoulos
  bibkey: dou-etal-2019-investigating
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1112
  month: November
  page_first: '1192'
  page_last: '1197'
  pages: "1192\u20131197"
  paper_id: '112'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1112.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1112.jpg
  title: Investigating Meta-Learning Algorithms for Low-Resource Natural Language
    Understanding Tasks
  title_html: Investigating Meta-Learning Algorithms for Low-Resource Natural Language
    Understanding Tasks
  url: https://www.aclweb.org/anthology/D19-1112
  year: '2019'
D19-1113:
  abstract: Contextualized word embeddings, such as ELMo, provide meaningful representations
    for words and their contexts. They have been shown to have a great impact on downstream
    applications. However, we observe that the contextualized embeddings of a word
    might change drastically when its contexts are paraphrased. As these embeddings
    are over-sensitive to the context, the downstream model may make different predictions
    when the input sentence is paraphrased. To address this issue, we propose a post-processing
    approach to retrofit the embedding with paraphrases. Our method learns an orthogonal
    transformation on the input space of the contextualized word embedding model,
    which seeks to minimize the variance of word representations on paraphrased contexts.
    Experiments show that the proposed method significantly improves ELMo on various
    sentence classification and inference tasks.
  address: Hong Kong, China
  author:
  - first: Weijia
    full: Weijia Shi
    id: weijia-shi
    last: Shi
  - first: Muhao
    full: Muhao Chen
    id: muhao-chen
    last: Chen
  - first: Pei
    full: Pei Zhou
    id: pei-zhou
    last: Zhou
  - first: Kai-Wei
    full: Kai-Wei Chang
    id: kai-wei-chang
    last: Chang
  author_string: Weijia Shi, Muhao Chen, Pei Zhou, Kai-Wei Chang
  bibkey: shi-etal-2019-retrofitting
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1113
  month: November
  page_first: '1198'
  page_last: '1203'
  pages: "1198\u20131203"
  paper_id: '113'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1113.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1113.jpg
  title: Retrofitting Contextualized Word Embeddings with Paraphrases
  title_html: Retrofitting Contextualized Word Embeddings with Paraphrases
  url: https://www.aclweb.org/anthology/D19-1113
  year: '2019'
D19-1114:
  abstract: Semantic similarity modeling is central to many NLP problems such as natural
    language inference and question answering. Syntactic structures interact closely
    with semantics in learning compositional representations and alleviating long-range
    dependency issues. How-ever, such structure priors have not been well exploited
    in previous work for semantic mod-eling. To examine their effectiveness, we start
    with the Pairwise Word Interaction Model, one of the best models according to
    a recent reproducibility study, then introduce components for modeling context
    and structure using multi-layer BiLSTMs and TreeLSTMs. In addition, we introduce
    residual connections to the deep convolutional neural network component of the
    model. Extensive evaluations on eight benchmark datasets show that incorporating
    structural information contributes to consistent improvements over strong baselines.
  address: Hong Kong, China
  author:
  - first: Linqing
    full: Linqing Liu
    id: linqing-liu
    last: Liu
  - first: Wei
    full: Wei Yang
    id: wei-yang
    last: Yang
  - first: Jinfeng
    full: Jinfeng Rao
    id: jinfeng-rao
    last: Rao
  - first: Raphael
    full: Raphael Tang
    id: raphael-tang
    last: Tang
  - first: Jimmy
    full: Jimmy Lin
    id: jimmy-lin
    last: Lin
  author_string: Linqing Liu, Wei Yang, Jinfeng Rao, Raphael Tang, Jimmy Lin
  bibkey: liu-etal-2019-incorporating
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1114
  month: November
  page_first: '1204'
  page_last: '1209'
  pages: "1204\u20131209"
  paper_id: '114'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1114.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1114.jpg
  title: Incorporating Contextual and Syntactic Structures Improves Semantic Similarity
    Modeling
  title_html: Incorporating Contextual and Syntactic Structures Improves Semantic
    Similarity Modeling
  url: https://www.aclweb.org/anthology/D19-1114
  year: '2019'
D19-1115:
  abstract: Whereas traditional cryptography encrypts a secret message into an unintelligible
    form, steganography conceals that communication is taking place by encoding a
    secret message into a cover signal. Language is a particularly pragmatic cover
    signal due to its benign occurrence and independence from any one medium. Traditionally,
    linguistic steganography systems encode secret messages in existing text via synonym
    substitution or word order rearrangements. Advances in neural language models
    enable previously impractical generation-based techniques. We propose a steganography
    technique based on arithmetic coding with large-scale neural language models.
    We find that our approach can generate realistic looking cover sentences as evaluated
    by humans, while at the same time preserving security by matching the cover message
    distribution with the language model distribution.
  address: Hong Kong, China
  attachment:
  - filename: D19-1115.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1115.Attachment.zip
  author:
  - first: Zachary
    full: Zachary Ziegler
    id: zachary-ziegler
    last: Ziegler
  - first: Yuntian
    full: Yuntian Deng
    id: yuntian-deng
    last: Deng
  - first: Alexander
    full: Alexander Rush
    id: alexander-m-rush
    last: Rush
  author_string: Zachary Ziegler, Yuntian Deng, Alexander Rush
  bibkey: ziegler-etal-2019-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1115
  month: November
  page_first: '1210'
  page_last: '1215'
  pages: "1210\u20131215"
  paper_id: '115'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1115.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1115.jpg
  title: Neural Linguistic Steganography
  title_html: Neural Linguistic Steganography
  url: https://www.aclweb.org/anthology/D19-1115
  year: '2019'
D19-1116:
  abstract: ROUGE is widely used to automatically evaluate summarization systems.
    However, ROUGE measures semantic overlap between a system summary and a human
    reference on word-string level, much at odds with the contemporary treatment of
    semantic meaning. Here we present a suite of experiments on using distributed
    representations for evaluating summarizers, both in reference-based and in reference-free
    setting. Our experimental results show that the max value over each dimension
    of the summary ELMo word embeddings is a good representation that results in high
    correlation with human ratings. Averaging the cosine similarity of all encoders
    we tested yields high correlation with manual scores in reference-free setting.
    The distributed representations outperform ROUGE in recent corpora for abstractive
    news summarization but are less good on test data used in past evaluations.
  address: Hong Kong, China
  attachment:
  - filename: D19-1116.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1116.Attachment.pdf
  author:
  - first: Simeng
    full: Simeng Sun
    id: simeng-sun
    last: Sun
  - first: Ani
    full: Ani Nenkova
    id: ani-nenkova
    last: Nenkova
  author_string: Simeng Sun, Ani Nenkova
  bibkey: sun-nenkova-2019-feasibility
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1116
  month: November
  page_first: '1216'
  page_last: '1221'
  pages: "1216\u20131221"
  paper_id: '116'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1116.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1116.jpg
  title: The Feasibility of Embedding Based Automatic Evaluation for Single Document
    Summarization
  title_html: The Feasibility of Embedding Based Automatic Evaluation for Single Document
    Summarization
  url: https://www.aclweb.org/anthology/D19-1116
  year: '2019'
D19-1117:
  abstract: Attention plays a key role in the improvement of sequence-to-sequence-based
    document summarization models. To obtain a powerful attention helping with reproducing
    the most salient information and avoiding repetitions, we augment the vanilla
    attention model from both local and global aspects. We propose attention refinement
    unit paired with local variance loss to impose supervision on the attention model
    at each decoding step, and we also propose a global variance loss to optimize
    the attention distributions of all decoding steps from the global perspective.
    The performances on CNN/Daily Mail dataset verify the effectiveness of our methods.
  address: Hong Kong, China
  author:
  - first: Min
    full: Min Gui
    id: min-gui
    last: Gui
  - first: Junfeng
    full: Junfeng Tian
    id: junfeng-tian
    last: Tian
  - first: Rui
    full: Rui Wang
    id: rui-wang
    last: Wang
  - first: Zhenglu
    full: Zhenglu Yang
    id: zhenglu-yang
    last: Yang
  author_string: Min Gui, Junfeng Tian, Rui Wang, Zhenglu Yang
  bibkey: gui-etal-2019-attention
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1117
  month: November
  page_first: '1222'
  page_last: '1228'
  pages: "1222\u20131228"
  paper_id: '117'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1117.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1117.jpg
  title: Attention Optimization for Abstractive Document Summarization
  title_html: Attention Optimization for Abstractive Document Summarization
  url: https://www.aclweb.org/anthology/D19-1117
  year: '2019'
D19-1118:
  abstract: Unresolved coreference is a bottleneck for relation extraction, and high-quality
    coreference resolvers may produce an output that makes it a lot easier to extract
    knowledge triples. We show how to improve coreference resolvers by forwarding
    their input to a relation extraction system and reward the resolvers for producing
    triples that are found in knowledge bases. Since relation extraction systems can
    rely on different forms of supervision and be biased in different ways, we obtain
    the best performance, improving over the state of the art, using multi-task reinforcement
    learning.
  address: Hong Kong, China
  author:
  - first: Rahul
    full: Rahul Aralikatte
    id: rahul-aralikatte
    last: Aralikatte
  - first: Heather
    full: Heather Lent
    id: heather-lent
    last: Lent
  - first: Ana Valeria
    full: Ana Valeria Gonzalez
    id: ana-valeria-gonzalez
    last: Gonzalez
  - first: Daniel
    full: Daniel Herschcovich
    id: daniel-herschcovich
    last: Herschcovich
  - first: Chen
    full: Chen Qiu
    id: chen-qiu
    last: Qiu
  - first: Anders
    full: Anders Sandholm
    id: anders-sandholm
    last: Sandholm
  - first: Michael
    full: Michael Ringaard
    id: michael-ringaard
    last: Ringaard
  - first: Anders
    full: "Anders S\xF8gaard"
    id: anders-sogaard
    last: "S\xF8gaard"
  author_string: "Rahul Aralikatte, Heather Lent, Ana Valeria Gonzalez, Daniel Herschcovich,\
    \ Chen Qiu, Anders Sandholm, Michael Ringaard, Anders S\xF8gaard"
  bibkey: aralikatte-etal-2019-rewarding
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1118
  month: November
  page_first: '1229'
  page_last: '1235'
  pages: "1229\u20131235"
  paper_id: '118'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1118.pdf
  publisher: Association for Computational Linguistics
  revision:
  - id: '1'
    url: https://www.aclweb.org/anthology/D19-1118v1.pdf
    value: D19-1118v1
  - explanation: Corrected an error in Figure 2 that incorrectly portrayed the data
      collection process for training the reward models.
    id: '2'
    url: https://www.aclweb.org/anthology/D19-1118v2.pdf
    value: D19-1118v2
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1118.jpg
  title: Rewarding Coreference Resolvers for Being Consistent with World Knowledge
  title_html: Rewarding Coreference Resolvers for Being Consistent with World Knowledge
  url: https://www.aclweb.org/anthology/D19-1118
  year: '2019'
D19-1119:
  abstract: The incorporation of pseudo data in the training of grammatical error
    correction models has been one of the main factors in improving the performance
    of such models. However, consensus is lacking on experimental configurations,
    namely, choosing how the pseudo data should be generated or used. In this study,
    these choices are investigated through extensive experiments, and state-of-the-art
    performance is achieved on the CoNLL-2014 test set (F0.5=65.0) and the official
    test set of the BEA-2019 shared task (F0.5=70.2) without making any modifications
    to the model architecture.
  address: Hong Kong, China
  attachment:
  - filename: D19-1119.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1119.Attachment.pdf
  author:
  - first: Shun
    full: Shun Kiyono
    id: shun-kiyono
    last: Kiyono
  - first: Jun
    full: Jun Suzuki
    id: jun-suzuki
    last: Suzuki
  - first: Masato
    full: Masato Mita
    id: masato-mita
    last: Mita
  - first: Tomoya
    full: Tomoya Mizumoto
    id: tomoya-mizumoto
    last: Mizumoto
  - first: Kentaro
    full: Kentaro Inui
    id: kentaro-inui
    last: Inui
  author_string: Shun Kiyono, Jun Suzuki, Masato Mita, Tomoya Mizumoto, Kentaro Inui
  bibkey: kiyono-etal-2019-empirical
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1119
  month: November
  page_first: '1236'
  page_last: '1242'
  pages: "1236\u20131242"
  paper_id: '119'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1119.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1119.jpg
  title: An Empirical Study of Incorporating Pseudo Data into Grammatical Error Correction
  title_html: An Empirical Study of Incorporating Pseudo Data into Grammatical Error
    Correction
  url: https://www.aclweb.org/anthology/D19-1119
  year: '2019'
D19-1120:
  abstract: "Multilingual topic models (MTMs) learn topics on documents in multiple\
    \ languages. Past models align topics across languages by implicitly assuming\
    \ the documents in different languages are highly comparable, often a false assumption.\
    \ We introduce a new model that does not rely on this assumption, particularly\
    \ useful in important low-resource language scenarios. Our MTM learns weighted\
    \ topic links and connects cross-lingual topics only when the dominant words defining\
    \ them are similar, outperforming LDA and previous MTMs in classification tasks\
    \ using documents\u2019 topic posteriors as features. It also learns coherent\
    \ topics on documents with low comparability."
  address: Hong Kong, China
  attachment:
  - filename: D19-1120.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1120.Attachment.pdf
  author:
  - first: Weiwei
    full: Weiwei Yang
    id: weiwei-yang
    last: Yang
  - first: Jordan
    full: Jordan Boyd-Graber
    id: jordan-boyd-graber
    last: Boyd-Graber
  - first: Philip
    full: Philip Resnik
    id: philip-resnik
    last: Resnik
  author_string: Weiwei Yang, Jordan Boyd-Graber, Philip Resnik
  bibkey: yang-etal-2019-multilingual
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1120
  month: November
  page_first: '1243'
  page_last: '1248'
  pages: "1243\u20131248"
  paper_id: '120'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1120.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1120.jpg
  title: A Multilingual Topic Model for Learning Weighted Topic Links Across Corpora
    with Low Comparability
  title_html: A Multilingual Topic Model for Learning Weighted Topic Links Across
    Corpora with Low Comparability
  url: https://www.aclweb.org/anthology/D19-1120
  year: '2019'
D19-1121:
  abstract: "Socio-economic conditions are difficult to measure. For example, the\
    \ U.S. Bureau of Labor Statistics needs to conduct large-scale household surveys\
    \ regularly to track the unemployment rate, an indicator widely used by economists\
    \ and policymakers. We argue that events reported in streaming news can be used\
    \ as \u201Cmicro-sensors\u201D for measuring socio-economic conditions. Similar\
    \ to collecting surveys and then counting answers, it is possible to measure a\
    \ socio-economic indicator by counting related events. In this paper, we propose\
    \ Event-Centric Indicator Measure (ECIM), a novel approach to measure socio-economic\
    \ indicators with events. We empirically demonstrate strong correlation between\
    \ ECIM values to several representative indicators in socio-economic research."
  address: Hong Kong, China
  author:
  - first: Bonan
    full: Bonan Min
    id: bonan-min
    last: Min
  - first: Xiaoxi
    full: Xiaoxi Zhao
    id: xiaoxi-zhao
    last: Zhao
  author_string: Bonan Min, Xiaoxi Zhao
  bibkey: min-zhao-2019-measure
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1121
  month: November
  page_first: '1249'
  page_last: '1254'
  pages: "1249\u20131254"
  paper_id: '121'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1121.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1121.jpg
  title: 'Measure Country-Level Socio-Economic Indicators with Streaming News: An
    Empirical Study'
  title_html: 'Measure Country-Level Socio-Economic Indicators with Streaming News:
    An Empirical Study'
  url: https://www.aclweb.org/anthology/D19-1121
  year: '2019'
D19-1122:
  abstract: "We introduce a new dataset consisting of natural language interactions\
    \ annotated with medical family histories, obtained during interactions with a\
    \ genetic counselor and through crowdsourcing, following a questionnaire created\
    \ by experts in the domain. We describe the data collection process and the annotations\
    \ performed by medical professionals, including illness and personal attributes\
    \ (name, age, gender, family relationships) for the patient and their family members.\
    \ An initial system that performs argument identification and relation extraction\
    \ shows promising results \u2013 average F-score of 0.87 on complex sentences\
    \ on the targeted relations."
  address: Hong Kong, China
  author:
  - first: Mahmoud
    full: Mahmoud Azab
    id: mahmoud-azab
    last: Azab
  - first: Stephane
    full: Stephane Dadian
    id: stephane-dadian
    last: Dadian
  - first: Vivi
    full: Vivi Nastase
    id: vivi-nastase
    last: Nastase
  - first: Larry
    full: Larry An
    id: larry-an
    last: An
  - first: Rada
    full: Rada Mihalcea
    id: rada-mihalcea
    last: Mihalcea
  author_string: Mahmoud Azab, Stephane Dadian, Vivi Nastase, Larry An, Rada Mihalcea
  bibkey: azab-etal-2019-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1122
  month: November
  page_first: '1255'
  page_last: '1260'
  pages: "1255\u20131260"
  paper_id: '122'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1122.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1122.jpg
  title: 'Towards Extracting Medical Family History from Natural Language Interactions:
    A New Dataset and Baselines'
  title_html: 'Towards Extracting Medical Family History from Natural Language Interactions:
    A New Dataset and Baselines'
  url: https://www.aclweb.org/anthology/D19-1122
  year: '2019'
D19-1123:
  abstract: In task-oriented dialogues, Natural Language Generation (NLG) is the final
    yet crucial step to produce user-facing system utterances. The result of NLG is
    directly related to the perceived quality and usability of a dialogue system.
    While most existing systems provide semantically correct responses given goals
    to present, they struggle to match the variation and fluency in the human language.
    In this paper, we propose a novel multi-task learning framework, NLG-LM, for natural
    language generation. In addition to generating high-quality responses conveying
    the required information, it also explicitly targets for naturalness in generated
    responses via an unconditioned language model. This can significantly improve
    the learning of style and variation in human language. Empirical results show
    that this multi-task learning framework outperforms previous models across multiple
    datasets. For example, it improves the previous best BLEU score on the E2E-NLG
    dataset by 2.2%, and on the Laptop dataset by 6.1%.
  address: Hong Kong, China
  attachment:
  - filename: D19-1123.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1123.Attachment.pdf
  author:
  - first: Chenguang
    full: Chenguang Zhu
    id: chenguang-zhu
    last: Zhu
  - first: Michael
    full: Michael Zeng
    id: michael-zeng
    last: Zeng
  - first: Xuedong
    full: Xuedong Huang
    id: xuedong-huang
    last: Huang
  author_string: Chenguang Zhu, Michael Zeng, Xuedong Huang
  bibkey: zhu-etal-2019-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1123
  month: November
  page_first: '1261'
  page_last: '1266'
  pages: "1261\u20131266"
  paper_id: '123'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1123.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1123.jpg
  title: Multi-task Learning for Natural Language Generation in Task-Oriented Dialogue
  title_html: Multi-task Learning for Natural Language Generation in Task-Oriented
    Dialogue
  url: https://www.aclweb.org/anthology/D19-1123
  year: '2019'
D19-1124:
  abstract: Variational encoder-decoders have achieved well-recognized performance
    in the dialogue generation task. Existing works simply assume the Gaussian priors
    of the latent variable, which are incapable of representing complex latent variables
    effectively. To address the issues, we propose to use the Dirichlet distribution
    with flexible structures to characterize the latent variables in place of the
    traditional Gaussian distribution, called Dirichlet Latent Variable Hierarchical
    Recurrent Encoder-Decoder model (Dir-VHRED). Based on which, we further find that
    there is redundancy among the dimensions of latent variable, and the lengths and
    sentence patterns of the responses can be strongly correlated to each dimension
    of the latent variable. Therefore, controllable responses can be generated through
    specifying the value of each dimension of the latent variable. Experimental results
    on benchmarks show that our proposed Dir-VHRED yields substantial improvements
    on negative log-likelihood, word-embedding-based and human evaluations.
  address: Hong Kong, China
  attachment:
  - filename: D19-1124.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1124.Attachment.zip
  author:
  - first: Min
    full: Min Zeng
    id: min-zeng
    last: Zeng
  - first: Yisen
    full: Yisen Wang
    id: yisen-wang
    last: Wang
  - first: Yuan
    full: Yuan Luo
    id: yuan-luo
    last: Luo
  author_string: Min Zeng, Yisen Wang, Yuan Luo
  bibkey: zeng-etal-2019-dirichlet
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1124
  month: November
  page_first: '1267'
  page_last: '1272'
  pages: "1267\u20131272"
  paper_id: '124'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1124.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1124.jpg
  title: Dirichlet Latent Variable Hierarchical Recurrent Encoder-Decoder in Dialogue
    Generation
  title_html: <span class="acl-fixed-case">D</span>irichlet Latent Variable Hierarchical
    Recurrent Encoder-Decoder in Dialogue Generation
  url: https://www.aclweb.org/anthology/D19-1124
  year: '2019'
D19-1125:
  abstract: Dialogue systems benefit greatly from optimizing on detailed annotations,
    such as transcribed utterances, internal dialogue state representations and dialogue
    act labels. However, collecting these annotations is expensive and time-consuming,
    holding back development in the area of dialogue modelling. In this paper, we
    investigate semi-supervised learning methods that are able to reduce the amount
    of required intermediate labelling. We find that by leveraging un-annotated data
    instead, the amount of turn-level annotations of dialogue state can be significantly
    reduced when building a neural dialogue system. Our analysis on the MultiWOZ corpus,
    covering a range of domains and topics, finds that annotations can be reduced
    by up to 30% while maintaining equivalent system performance. We also describe
    and evaluate the first end-to-end dialogue model created for the MultiWOZ corpus.
  address: Hong Kong, China
  author:
  - first: Bo-Hsiang
    full: Bo-Hsiang Tseng
    id: bo-hsiang-tseng
    last: Tseng
  - first: Marek
    full: Marek Rei
    id: marek-rei
    last: Rei
  - first: "Pawe\u0142"
    full: "Pawe\u0142 Budzianowski"
    id: pawel-budzianowski
    last: Budzianowski
  - first: Richard
    full: Richard Turner
    id: richard-turner
    last: Turner
  - first: Bill
    full: Bill Byrne
    id: bill-byrne
    last: Byrne
  - first: Anna
    full: Anna Korhonen
    id: anna-korhonen
    last: Korhonen
  author_string: "Bo-Hsiang Tseng, Marek Rei, Pawe\u0142 Budzianowski, Richard Turner,\
    \ Bill Byrne, Anna Korhonen"
  bibkey: tseng-etal-2019-semi
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1125
  month: November
  page_first: '1273'
  page_last: '1278'
  pages: "1273\u20131278"
  paper_id: '125'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1125.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1125.jpg
  title: Semi-Supervised Bootstrapping of Dialogue State Trackers for Task-Oriented
    Modelling
  title_html: Semi-Supervised Bootstrapping of Dialogue State Trackers for Task-Oriented
    Modelling
  url: https://www.aclweb.org/anthology/D19-1125
  year: '2019'
D19-1126:
  abstract: "Semantic slot filling is one of the major tasks in spoken language understanding\
    \ (SLU). After a slot filling model is trained on precollected data, it is crucial\
    \ to continually improve the model after deployment to learn users\u2019 new expressions.\
    \ As the data amount grows, it becomes infeasible to either store such huge data\
    \ and repeatedly retrain the model on all data or fine tune the model only on\
    \ new data without forgetting old expressions. In this paper, we introduce a novel\
    \ progressive slot filling model, ProgModel. ProgModel consists of a novel context\
    \ gate that transfers previously learned knowledge to a small size expanded component;\
    \ and meanwhile enables this new component to be fast trained to learn from new\
    \ data. As such, ProgModel learns the new knowledge by only using new data at\
    \ each time and meanwhile preserves the previously learned expressions. Our experiments\
    \ show that ProgModel needs much less training time and smaller model size to\
    \ outperform various model fine tuning competitors by up to 4.24% and 3.03% on\
    \ two benchmark datasets."
  address: Hong Kong, China
  author:
  - first: Yilin
    full: Yilin Shen
    id: yilin-shen
    last: Shen
  - first: Xiangyu
    full: Xiangyu Zeng
    id: xiangyu-zeng
    last: Zeng
  - first: Hongxia
    full: Hongxia Jin
    id: hongxia-jin
    last: Jin
  author_string: Yilin Shen, Xiangyu Zeng, Hongxia Jin
  bibkey: shen-etal-2019-progressive
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1126
  month: November
  page_first: '1279'
  page_last: '1284'
  pages: "1279\u20131284"
  paper_id: '126'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1126.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1126.jpg
  title: A Progressive Model to Enable Continual Learning for Semantic Slot Filling
  title_html: A Progressive Model to Enable Continual Learning for Semantic Slot Filling
  url: https://www.aclweb.org/anthology/D19-1126
  year: '2019'
D19-1127:
  abstract: Natural Language Understanding (NLU) is a core component of dialog systems.
    It typically involves two tasks - Intent Classification (IC) and Slot Labeling
    (SL), which are then followed by a dialogue management (DM) component. Such NLU
    systems cater to utterances in isolation, thus pushing the problem of context
    management to DM. However, contextual information is critical to the correct prediction
    of intents in a conversation. Prior work on contextual NLU has been limited in
    terms of the types of contextual signals used and the understanding of their impact
    on the model. In this work, we propose a context-aware self-attentive NLU (CASA-NLU)
    model that uses multiple signals over a variable context window, such as previous
    intents, slots, dialog acts and utterances, in addition to the current user utterance.
    CASA-NLU outperforms a recurrent contextual NLU baseline on two conversational
    datasets, yielding a gain of up to 7% on the IC task. Moreover, a non-contextual
    variant of CASA-NLU achieves state-of-the-art performance on standard public datasets
    - SNIPS and ATIS.
  address: Hong Kong, China
  attachment:
  - filename: D19-1127.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1127.Attachment.pdf
  author:
  - first: Arshit
    full: Arshit Gupta
    id: arshit-gupta
    last: Gupta
  - first: Peng
    full: Peng Zhang
    id: peng-zhang
    last: Zhang
  - first: Garima
    full: Garima Lalwani
    id: garima-lalwani
    last: Lalwani
  - first: Mona
    full: Mona Diab
    id: mona-diab
    last: Diab
  author_string: Arshit Gupta, Peng Zhang, Garima Lalwani, Mona Diab
  bibkey: gupta-etal-2019-casa
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1127
  month: November
  page_first: '1285'
  page_last: '1290'
  pages: "1285\u20131290"
  paper_id: '127'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1127.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1127.jpg
  title: 'CASA-NLU: Context-Aware Self-Attentive Natural Language Understanding for
    Task-Oriented Chatbots'
  title_html: '<span class="acl-fixed-case">CASA</span>-<span class="acl-fixed-case">NLU</span>:
    Context-Aware Self-Attentive Natural Language Understanding for Task-Oriented
    Chatbots'
  url: https://www.aclweb.org/anthology/D19-1127
  year: '2019'
D19-1128:
  abstract: We study how to sample negative examples to automatically construct a
    training set for effective model learning in retrieval-based dialogue systems.
    Following an idea of dynamically adapting negative examples to matching models
    in learning, we consider four strategies including minimum sampling, maximum sampling,
    semi-hard sampling, and decay-hard sampling. Empirical studies on two benchmarks
    with three matching models indicate that compared with the widely used random
    sampling strategy, although the first two strategies lead to performance drop,
    the latter two ones can bring consistent improvement to the performance of all
    the models on both benchmarks.
  address: Hong Kong, China
  author:
  - first: Jia
    full: Jia Li
    id: jia-li
    last: Li
  - first: Chongyang
    full: Chongyang Tao
    id: chongyang-tao
    last: Tao
  - first: Wei
    full: Wei Wu
    id: wei-wu
    last: Wu
  - first: Yansong
    full: Yansong Feng
    id: yansong-feng
    last: Feng
  - first: Dongyan
    full: Dongyan Zhao
    id: dongyan-zhao
    last: Zhao
  - first: Rui
    full: Rui Yan
    id: rui-yan
    last: Yan
  author_string: Jia Li, Chongyang Tao, Wei Wu, Yansong Feng, Dongyan Zhao, Rui Yan
  bibkey: li-etal-2019-sampling
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1128
  month: November
  page_first: '1291'
  page_last: '1296'
  pages: "1291\u20131296"
  paper_id: '128'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1128.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1128.jpg
  title: Sampling Matters! An Empirical Study of Negative Sampling Strategies for
    Learning of Matching Models in Retrieval-based Dialogue Systems
  title_html: Sampling Matters! An Empirical Study of Negative Sampling Strategies
    for Learning of Matching Models in Retrieval-based Dialogue Systems
  url: https://www.aclweb.org/anthology/D19-1128
  year: '2019'
D19-1129:
  abstract: Despite the surging demands for multilingual task-oriented dialog systems
    (e.g., Alexa, Google Home), there has been less research done in multilingual
    or cross-lingual scenarios. Hence, we propose a zero-shot adaptation of task-oriented
    dialogue system to low-resource languages. To tackle this challenge, we first
    use a set of very few parallel word pairs to refine the aligned cross-lingual
    word-level representations. We then employ a latent variable model to cope with
    the variance of similar sentences across different languages, which is induced
    by imperfect cross-lingual alignments and inherent differences in languages. Finally,
    the experimental results show that even though we utilize much less external resources,
    our model achieves better adaptation performance for natural language understanding
    task (i.e., the intent detection and slot filling) compared to the current state-of-the-art
    model in the zero-shot scenario.
  address: Hong Kong, China
  author:
  - first: Zihan
    full: Zihan Liu
    id: zihan-liu
    last: Liu
  - first: Jamin
    full: Jamin Shin
    id: jamin-shin
    last: Shin
  - first: Yan
    full: Yan Xu
    id: yan-xu
    last: Xu
  - first: Genta Indra
    full: Genta Indra Winata
    id: genta-indra-winata
    last: Winata
  - first: Peng
    full: Peng Xu
    id: peng-xu
    last: Xu
  - first: Andrea
    full: Andrea Madotto
    id: andrea-madotto
    last: Madotto
  - first: Pascale
    full: Pascale Fung
    id: pascale-fung
    last: Fung
  author_string: Zihan Liu, Jamin Shin, Yan Xu, Genta Indra Winata, Peng Xu, Andrea
    Madotto, Pascale Fung
  bibkey: liu-etal-2019-zero
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1129
  month: November
  page_first: '1297'
  page_last: '1303'
  pages: "1297\u20131303"
  paper_id: '129'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1129.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1129.jpg
  title: Zero-shot Cross-lingual Dialogue Systems with Transferable Latent Variables
  title_html: Zero-shot Cross-lingual Dialogue Systems with Transferable Latent Variables
  url: https://www.aclweb.org/anthology/D19-1129
  year: '2019'
D19-1130:
  abstract: "Dialogue management (DM) plays a key role in the quality of the interaction\
    \ with the user in a task-oriented dialogue system. In most existing approaches,\
    \ the agent predicts only one DM policy action per turn. This significantly limits\
    \ the expressive power of the conversational agent and introduces unwanted turns\
    \ of interactions that may challenge users\u2019 patience. Longer conversations\
    \ also lead to more errors and the system needs to be more robust to handle them.\
    \ In this paper, we compare the performance of several models on the task of predicting\
    \ multiple acts for each turn. A novel policy model is proposed based on a recurrent\
    \ cell called gated Continue-Act-Slots (gCAS) that overcomes the limitations of\
    \ the existing models. Experimental results show that gCAS outperforms other approaches.\
    \ The datasets and code are available at https://leishu02.github.io/."
  address: Hong Kong, China
  author:
  - first: Lei
    full: Lei Shu
    id: lei-shu
    last: Shu
  - first: Hu
    full: Hu Xu
    id: hu-xu
    last: Xu
  - first: Bing
    full: Bing Liu
    id: bing-liu
    last: Liu
  - first: Piero
    full: Piero Molino
    id: piero-molino
    last: Molino
  author_string: Lei Shu, Hu Xu, Bing Liu, Piero Molino
  bibkey: shu-etal-2019-modeling
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1130
  month: November
  page_first: '1304'
  page_last: '1310'
  pages: "1304\u20131310"
  paper_id: '130'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1130.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1130.jpg
  title: Modeling Multi-Action Policy for Task-Oriented Dialogues
  title_html: Modeling Multi-Action Policy for Task-Oriented Dialogues
  url: https://www.aclweb.org/anthology/D19-1130
  year: '2019'
D19-1131:
  abstract: "Task-oriented dialog systems need to know when a query falls outside\
    \ their range of supported intents, but current text classification corpora only\
    \ define label sets that cover every example. We introduce a new dataset that\
    \ includes queries that are out-of-scope\u2014i.e., queries that do not fall into\
    \ any of the system\u2019s supported intents. This poses a new challenge because\
    \ models cannot assume that every query at inference time belongs to a system-supported\
    \ intent class. Our dataset also covers 150 intent classes over 10 domains, capturing\
    \ the breadth that a production task-oriented agent must handle. We evaluate a\
    \ range of benchmark classifiers on our dataset along with several different out-of-scope\
    \ identification schemes. We find that while the classifiers perform well on in-scope\
    \ intent classification, they struggle to identify out-of-scope queries. Our dataset\
    \ and evaluation fill an important gap in the field, offering a way of more rigorously\
    \ and realistically benchmarking text classification in task-driven dialog systems."
  address: Hong Kong, China
  attachment:
  - filename: D19-1131.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1131.Attachment.zip
  author:
  - first: Stefan
    full: Stefan Larson
    id: stefan-larson
    last: Larson
  - first: Anish
    full: Anish Mahendran
    id: anish-mahendran
    last: Mahendran
  - first: Joseph J.
    full: Joseph J. Peper
    id: joseph-j-peper
    last: Peper
  - first: Christopher
    full: Christopher Clarke
    id: christopher-clarke
    last: Clarke
  - first: Andrew
    full: Andrew Lee
    id: andrew-lee
    last: Lee
  - first: Parker
    full: Parker Hill
    id: parker-hill
    last: Hill
  - first: Jonathan K.
    full: Jonathan K. Kummerfeld
    id: jonathan-k-kummerfeld
    last: Kummerfeld
  - first: Kevin
    full: Kevin Leach
    id: kevin-leach
    last: Leach
  - first: Michael A.
    full: Michael A. Laurenzano
    id: michael-a-laurenzano
    last: Laurenzano
  - first: Lingjia
    full: Lingjia Tang
    id: lingjia-tang
    last: Tang
  - first: Jason
    full: Jason Mars
    id: jason-mars
    last: Mars
  author_string: Stefan Larson, Anish Mahendran, Joseph J. Peper, Christopher Clarke,
    Andrew Lee, Parker Hill, Jonathan K. Kummerfeld, Kevin Leach, Michael A. Laurenzano,
    Lingjia Tang, Jason Mars
  bibkey: larson-etal-2019-evaluation
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1131
  month: November
  page_first: '1311'
  page_last: '1316'
  pages: "1311\u20131316"
  paper_id: '131'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1131.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1131.jpg
  title: An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction
  title_html: An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction
  url: https://www.aclweb.org/anthology/D19-1131
  year: '2019'
D19-1132:
  abstract: "Automatic data augmentation (AutoAugment) (Cubuk et al., 2019) searches\
    \ for optimal perturbation policies via a controller trained using performance\
    \ rewards of a sampled policy on the target task, hence reducing data-level model\
    \ bias. While being a powerful algorithm, their work has focused on computer vision\
    \ tasks, where it is comparatively easy to apply imperceptible perturbations without\
    \ changing an image\u2019s semantic meaning. In our work, we adapt AutoAugment\
    \ to automatically discover effective perturbation policies for natural language\
    \ processing (NLP) tasks such as dialogue generation. We start with a pool of\
    \ atomic operations that apply subtle semantic-preserving perturbations to the\
    \ source inputs of a dialogue task (e.g., different POS-tag types of stopword\
    \ dropout, grammatical errors, and paraphrasing). Next, we allow the controller\
    \ to learn more complex augmentation policies by searching over the space of the\
    \ various combinations of these atomic operations. Moreover, we also explore conditioning\
    \ the controller on the source inputs of the target task, since certain strategies\
    \ may not apply to inputs that do not contain that strategy\u2019s required linguistic\
    \ features. Empirically, we demonstrate that both our input-agnostic and input-aware\
    \ controllers discover useful data augmentation policies, and achieve significant\
    \ improvements over the previous state-of-the-art, including trained on manually-designed\
    \ policies."
  address: Hong Kong, China
  author:
  - first: Tong
    full: Tong Niu
    id: tong-niu
    last: Niu
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  author_string: Tong Niu, Mohit Bansal
  bibkey: niu-bansal-2019-automatically
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1132
  month: November
  page_first: '1317'
  page_last: '1323'
  pages: "1317\u20131323"
  paper_id: '132'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1132.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1132.jpg
  title: Automatically Learning Data Augmentation Policies for Dialogue Tasks
  title_html: Automatically Learning Data Augmentation Policies for Dialogue Tasks
  url: https://www.aclweb.org/anthology/D19-1132
  year: '2019'
D19-1133:
  abstract: The preprocessing pipelines in Natural Language Processing usually involve
    a step of removing sentences consisted of illegal characters. The definition of
    illegal characters and the specific removal strategy depend on the task, language,
    domain, etc, which often lead to tiresome and repetitive scripting of rules. In
    this paper, we introduce a simple statistical method, uniblock, to overcome this
    problem. For each sentence, uniblock generates a fixed-size feature vector using
    Unicode block information of the characters. A Gaussian mixture model is then
    estimated on some clean corpus using variational inference. The learned model
    can then be used to score sentences and filter corpus. We present experimental
    results on Sentiment Analysis, Language Modeling and Machine Translation, and
    show the simplicity and effectiveness of our method.
  address: Hong Kong, China
  author:
  - first: Yingbo
    full: Yingbo Gao
    id: yingbo-gao
    last: Gao
  - first: Weiyue
    full: Weiyue Wang
    id: weiyue-wang
    last: Wang
  - first: Hermann
    full: Hermann Ney
    id: hermann-ney
    last: Ney
  author_string: Yingbo Gao, Weiyue Wang, Hermann Ney
  bibkey: gao-etal-2019-uniblock
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1133
  month: November
  page_first: '1324'
  page_last: '1329'
  pages: "1324\u20131329"
  paper_id: '133'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1133.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1133.jpg
  title: 'uniblock: Scoring and Filtering Corpus with Unicode Block Information'
  title_html: 'uniblock: Scoring and Filtering Corpus with <span class="acl-fixed-case">U</span>nicode
    Block Information'
  url: https://www.aclweb.org/anthology/D19-1133
  year: '2019'
D19-1134:
  abstract: Current multilingual word translation methods are focused on jointly learning
    mappings from each language to a shared space. The actual translation, however,
    is still performed as an isolated bilingual task. In this study we propose a multilingual
    translation procedure that uses all the learned mappings to translate a word from
    one language to another. For each source word, we first search for the most relevant
    auxiliary languages. We then use the translations to these languages to form an
    improved representation of the source word. Finally, this representation is used
    for the actual translation to the target language. Experiments on a standard multilingual
    word translation benchmark demonstrate that our model outperforms state of the
    art results.
  address: Hong Kong, China
  author:
  - first: Hagai
    full: Hagai Taitelbaum
    id: hagai-taitelbaum
    last: Taitelbaum
  - first: Gal
    full: Gal Chechik
    id: gal-chechik
    last: Chechik
  - first: Jacob
    full: Jacob Goldberger
    id: jacob-goldberger
    last: Goldberger
  author_string: Hagai Taitelbaum, Gal Chechik, Jacob Goldberger
  bibkey: taitelbaum-etal-2019-multilingual
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1134
  month: November
  page_first: '1330'
  page_last: '1335'
  pages: "1330\u20131335"
  paper_id: '134'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1134.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1134.jpg
  title: Multilingual word translation using auxiliary languages
  title_html: Multilingual word translation using auxiliary languages
  url: https://www.aclweb.org/anthology/D19-1134
  year: '2019'
D19-1135:
  abstract: "Recent studies have shown that a hybrid of self-attention networks (SANs)\
    \ and recurrent neural networks RNNs outperforms both individual architectures,\
    \ while not much is known about why the hybrid models work. With the belief that\
    \ modeling hierarchical structure is an essential complementary between SANs and\
    \ RNNs, we propose to further enhance the strength of hybrid models with an advanced\
    \ variant of RNNs \u2013 Ordered Neurons LSTM (ON-LSTM), which introduces a syntax-oriented\
    \ inductive bias to perform tree-like composition. Experimental results on the\
    \ benchmark machine translation task show that the proposed approach outperforms\
    \ both individual architectures and a standard hybrid model. Further analyses\
    \ on targeted linguistic evaluation and logical inference tasks demonstrate that\
    \ the proposed approach indeed benefits from a better modeling of hierarchical\
    \ structure."
  address: Hong Kong, China
  attachment:
  - filename: D19-1135.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1135.Attachment.pdf
  author:
  - first: Jie
    full: Jie Hao
    id: jie-hao
    last: Hao
  - first: Xing
    full: Xing Wang
    id: xing-wang
    last: Wang
  - first: Shuming
    full: Shuming Shi
    id: shuming-shi
    last: Shi
  - first: Jinfeng
    full: Jinfeng Zhang
    id: jinfeng-zhang
    last: Zhang
  - first: Zhaopeng
    full: Zhaopeng Tu
    id: zhaopeng-tu
    last: Tu
  author_string: Jie Hao, Xing Wang, Shuming Shi, Jinfeng Zhang, Zhaopeng Tu
  bibkey: hao-etal-2019-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1135
  month: November
  page_first: '1336'
  page_last: '1341'
  pages: "1336\u20131341"
  paper_id: '135'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1135.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1135.jpg
  title: Towards Better Modeling Hierarchical Structure for Self-Attention with Ordered
    Neurons
  title_html: Towards Better Modeling Hierarchical Structure for Self-Attention with
    Ordered Neurons
  url: https://www.aclweb.org/anthology/D19-1135
  year: '2019'
D19-1136:
  abstract: "We introduce Vecalign, a novel bilingual sentence alignment method which\
    \ is linear in time and space with respect to the number of sentences being aligned\
    \ and which requires only bilingual sentence embeddings. On a standard German\u2013\
    French test set, Vecalign outperforms the previous state-of-the-art method (which\
    \ has quadratic time complexity and requires a machine translation system) by\
    \ 5 F1 points. It substantially outperforms the popular Hunalign toolkit at recovering\
    \ Bible verse alignments in medium- to low-resource language pairs, and it improves\
    \ downstream MT quality by 1.7 and 1.6 BLEU in Sinhala-English and Nepali-English,\
    \ respectively, compared to the Hunalign-based Paracrawl pipeline."
  address: Hong Kong, China
  author:
  - first: Brian
    full: Brian Thompson
    id: brian-thompson
    last: Thompson
  - first: Philipp
    full: Philipp Koehn
    id: philipp-koehn
    last: Koehn
  author_string: Brian Thompson, Philipp Koehn
  bibkey: thompson-koehn-2019-vecalign
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1136
  month: November
  page_first: '1342'
  page_last: '1348'
  pages: "1342\u20131348"
  paper_id: '136'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1136.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1136.jpg
  title: 'Vecalign: Improved Sentence Alignment in Linear Time and Space'
  title_html: '<span class="acl-fixed-case">V</span>ecalign: Improved Sentence Alignment
    in Linear Time and Space'
  url: https://www.aclweb.org/anthology/D19-1136
  year: '2019'
D19-1137:
  abstract: 'Simultaneous translation is widely useful but remains challenging. Previous
    work falls into two main categories: (a) fixed-latency policies such as Ma et
    al. (2019) and (b) adaptive policies such as Gu et al. (2017). The former are
    simple and effective, but have to aggressively predict future content due to diverging
    source-target word order; the latter do not anticipate, but suffer from unstable
    and inefficient training. To combine the merits of both approaches, we propose
    a simple supervised-learning framework to learn an adaptive policy from oracle
    READ/WRITE sequences generated from parallel text. At each step, such an oracle
    sequence chooses to WRITE the next target word if the available source sentence
    context provides enough information to do so, otherwise READ the next source word.
    Experiments on German<=>English show that our method, without retraining the underlying
    NMT model, can learn flexible policies with better BLEU scores and similar latencies
    compared to previous work.'
  address: Hong Kong, China
  author:
  - first: Baigong
    full: Baigong Zheng
    id: baigong-zheng
    last: Zheng
  - first: Renjie
    full: Renjie Zheng
    id: renjie-zheng
    last: Zheng
  - first: Mingbo
    full: Mingbo Ma
    id: mingbo-ma
    last: Ma
  - first: Liang
    full: Liang Huang
    id: liang-huang
    last: Huang
  author_string: Baigong Zheng, Renjie Zheng, Mingbo Ma, Liang Huang
  bibkey: zheng-etal-2019-simpler
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1137
  month: November
  page_first: '1349'
  page_last: '1354'
  pages: "1349\u20131354"
  paper_id: '137'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1137.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1137.jpg
  title: Simpler and Faster Learning of Adaptive Policies for Simultaneous Translation
  title_html: Simpler and Faster Learning of Adaptive Policies for Simultaneous Translation
  url: https://www.aclweb.org/anthology/D19-1137
  year: '2019'
D19-1138:
  abstract: "Contextual word embeddings (e.g. GPT, BERT, ELMo, etc.) have demonstrated\
    \ state-of-the-art performance on various NLP tasks. Recent work with the multilingual\
    \ version of BERT has shown that the model performs surprisingly well in cross-lingual\
    \ settings, even when only labeled English data is used to finetune the model.\
    \ We improve upon multilingual BERT\u2019s zero-resource cross-lingual performance\
    \ via adversarial learning. We report the magnitude of the improvement on the\
    \ multilingual MLDoc text classification and CoNLL 2002/2003 named entity recognition\
    \ tasks. Furthermore, we show that language-adversarial training encourages BERT\
    \ to align the embeddings of English documents and their translations, which may\
    \ be the cause of the observed performance gains."
  address: Hong Kong, China
  author:
  - first: Phillip
    full: Phillip Keung
    id: phillip-keung
    last: Keung
  - first: Yichao
    full: Yichao Lu
    id: yichao-lu
    last: Lu
  - first: Vikas
    full: Vikas Bhardwaj
    id: vikas-bhardwaj
    last: Bhardwaj
  author_string: Phillip Keung, Yichao Lu, Vikas Bhardwaj
  bibkey: keung-etal-2019-adversarial
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1138
  month: November
  page_first: '1355'
  page_last: '1360'
  pages: "1355\u20131360"
  paper_id: '138'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1138.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1138.jpg
  title: Adversarial Learning with Contextual Embeddings for Zero-resource Cross-lingual
    Classification and NER
  title_html: Adversarial Learning with Contextual Embeddings for Zero-resource Cross-lingual
    Classification and <span class="acl-fixed-case">NER</span>
  url: https://www.aclweb.org/anthology/D19-1138
  year: '2019'
D19-1139:
  abstract: "In the Transformer network architecture, positional embeddings are used\
    \ to encode order dependencies into the input representation. However, this input\
    \ representation only involves static order dependencies based on discrete numerical\
    \ information, that is, are independent of word content. To address this issue,\
    \ this work proposes a recurrent positional embedding approach based on word vector.\
    \ In this approach, these recurrent positional embeddings are learned by a recurrent\
    \ neural network, encoding word content-based order dependencies into the input\
    \ representation. They are then integrated into the existing multi-head self-attention\
    \ model as independent heads or part of each head. The experimental results revealed\
    \ that the proposed approach improved translation performance over that of the\
    \ state-of-the-art Transformer baseline in WMT\u201914 English-to-German and NIST\
    \ Chinese-to-English translation tasks."
  address: Hong Kong, China
  author:
  - first: Kehai
    full: Kehai Chen
    id: kehai-chen
    last: Chen
  - first: Rui
    full: Rui Wang
    id: rui-wang
    last: Wang
  - first: Masao
    full: Masao Utiyama
    id: masao-utiyama
    last: Utiyama
  - first: Eiichiro
    full: Eiichiro Sumita
    id: eiichiro-sumita
    last: Sumita
  author_string: Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita
  bibkey: chen-etal-2019-recurrent
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1139
  month: November
  page_first: '1361'
  page_last: '1367'
  pages: "1361\u20131367"
  paper_id: '139'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1139.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1139.jpg
  title: Recurrent Positional Embedding for Neural Machine Translation
  title_html: Recurrent Positional Embedding for Neural Machine Translation
  url: https://www.aclweb.org/anthology/D19-1139
  year: '2019'
D19-1140:
  abstract: "We propose a neural machine translation (NMT) approach that, instead\
    \ of pursuing adequacy and fluency (\u201Chuman-oriented\u201D quality criteria),\
    \ aims to generate translations that are best suited as input to a natural language\
    \ processing component designed for a specific downstream task (a \u201Cmachine-oriented\u201D\
    \ criterion). Towards this objective, we present a reinforcement learning technique\
    \ based on a new candidate sampling strategy, which exploits the results obtained\
    \ on the downstream task as weak feedback. Experiments in sentiment classification\
    \ of Twitter data in German and Italian show that feeding an English classifier\
    \ with \u201Cmachine-oriented\u201D translations significantly improves its performance.\
    \ Classification results outperform those obtained with translations produced\
    \ by general-purpose NMT models as well as by an approach based on reinforcement\
    \ learning. Moreover, our results on both languages approximate the classification\
    \ accuracy computed on gold standard English tweets."
  address: Hong Kong, China
  author:
  - first: Amirhossein
    full: Amirhossein Tebbifakhr
    id: amirhossein-tebbifakhr
    last: Tebbifakhr
  - first: Luisa
    full: Luisa Bentivogli
    id: luisa-bentivogli
    last: Bentivogli
  - first: Matteo
    full: Matteo Negri
    id: matteo-negri
    last: Negri
  - first: Marco
    full: Marco Turchi
    id: marco-turchi
    last: Turchi
  author_string: Amirhossein Tebbifakhr, Luisa Bentivogli, Matteo Negri, Marco Turchi
  bibkey: tebbifakhr-etal-2019-machine
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1140
  month: November
  page_first: '1368'
  page_last: '1374'
  pages: "1368\u20131374"
  paper_id: '140'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1140.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1140.jpg
  title: 'Machine Translation for Machines: the Sentiment Classification Use Case'
  title_html: 'Machine Translation for Machines: the Sentiment Classification Use
    Case'
  url: https://www.aclweb.org/anthology/D19-1140
  year: '2019'
D19-1141:
  abstract: Byte-Pair Encoding (BPE) is an unsupervised sub-word tokenization technique,
    commonly used in neural machine translation and other NLP tasks. Its effectiveness
    makes it a de facto standard, but the reasons for this are not well understood.
    We link BPE to the broader family of dictionary-based compression algorithms and
    compare it with other members of this family. Our experiments across datasets,
    language pairs, translation models, and vocabulary size show that - given a fixed
    vocabulary size budget - the fewer tokens an algorithm needs to cover the test
    set, the better the translation (as measured by BLEU).
  address: Hong Kong, China
  author:
  - first: Matthias
    full: "Matthias Gall\xE9"
    id: matthias-galle
    last: "Gall\xE9"
  author_string: "Matthias Gall\xE9"
  bibkey: galle-2019-investigating
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1141
  month: November
  page_first: '1375'
  page_last: '1381'
  pages: "1375\u20131381"
  paper_id: '141'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1141.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1141.jpg
  title: 'Investigating the Effectiveness of BPE: The Power of Shorter Sequences'
  title_html: 'Investigating the Effectiveness of <span class="acl-fixed-case">BPE</span>:
    The Power of Shorter Sequences'
  url: https://www.aclweb.org/anthology/D19-1141
  year: '2019'
D19-1142:
  abstract: Bilingual lexicons are valuable resources used by professional human translators.
    While these resources can be easily incorporated in statistical machine translation,
    it is unclear how to best do so in the neural framework. In this work, we present
    the HABLex dataset, designed to test methods for bilingual lexicon integration
    into neural machine translation. Our data consists of human generated alignments
    of words and phrases in machine translation test sets in three language pairs
    (Russian-English, Chinese-English, and Korean-English), resulting in clean bilingual
    lexicons which are well matched to the reference. We also present two simple baselines
    - constrained decoding and continued training - and an improvement to continued
    training to address overfitting.
  address: Hong Kong, China
  author:
  - first: Brian
    full: Brian Thompson
    id: brian-thompson
    last: Thompson
  - first: Rebecca
    full: Rebecca Knowles
    id: rebecca-knowles
    last: Knowles
  - first: Xuan
    full: Xuan Zhang
    id: xuan-zhang
    last: Zhang
  - first: Huda
    full: Huda Khayrallah
    id: huda-khayrallah
    last: Khayrallah
  - first: Kevin
    full: Kevin Duh
    id: kevin-duh
    last: Duh
  - first: Philipp
    full: Philipp Koehn
    id: philipp-koehn
    last: Koehn
  author_string: Brian Thompson, Rebecca Knowles, Xuan Zhang, Huda Khayrallah, Kevin
    Duh, Philipp Koehn
  bibkey: thompson-etal-2019-hablex
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1142
  month: November
  page_first: '1382'
  page_last: '1387'
  pages: "1382\u20131387"
  paper_id: '142'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1142.pdf
  publisher: Association for Computational Linguistics
  revision:
  - id: '1'
    url: https://www.aclweb.org/anthology/D19-1142v1.pdf
    value: D19-1142v1
  - explanation: This revision corrects data size entries in Table 2 (where the original
      submission had incorrect values for Chinese and had switched development and
      test sizes for Russian and Korean) and corrects an error regarding discontiguous
      alignments in Section 3.3 (some, but not all, discontiguous alignments were
      removed from the data).
    id: '2'
    url: https://www.aclweb.org/anthology/D19-1142v2.pdf
    value: D19-1142v2
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1142.jpg
  title: 'HABLex: Human Annotated Bilingual Lexicons for Experiments in Machine Translation'
  title_html: '<span class="acl-fixed-case">HABL</span>ex: Human Annotated Bilingual
    Lexicons for Experiments in Machine Translation'
  url: https://www.aclweb.org/anthology/D19-1142
  year: '2019'
D19-1143:
  abstract: Despite impressive empirical successes of neural machine translation (NMT)
    on standard benchmarks, limited parallel data impedes the application of NMT models
    to many language pairs. Data augmentation methods such as back-translation make
    it possible to use monolingual data to help alleviate these issues, but back-translation
    itself fails in extreme low-resource scenarios, especially for syntactically divergent
    languages. In this paper, we propose a simple yet effective solution, whereby
    target-language sentences are re-ordered to match the order of the source and
    used as an additional source of training-time supervision. Experiments with simulated
    low-resource Japanese-to-English, and real low-resource Uyghur-to-English scenarios
    find significant improvements over other semi-supervised alternatives.
  address: Hong Kong, China
  attachment:
  - filename: D19-1143.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1143.Attachment.zip
  author:
  - first: Chunting
    full: Chunting Zhou
    id: chunting-zhou
    last: Zhou
  - first: Xuezhe
    full: Xuezhe Ma
    id: xuezhe-ma
    last: Ma
  - first: Junjie
    full: Junjie Hu
    id: junjie-hu
    last: Hu
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  author_string: Chunting Zhou, Xuezhe Ma, Junjie Hu, Graham Neubig
  bibkey: zhou-etal-2019-handling
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1143
  month: November
  page_first: '1388'
  page_last: '1394'
  pages: "1388\u20131394"
  paper_id: '143'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1143.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1143.jpg
  title: Handling Syntactic Divergence in Low-resource Machine Translation
  title_html: Handling Syntactic Divergence in Low-resource Machine Translation
  url: https://www.aclweb.org/anthology/D19-1143
  year: '2019'
D19-1144:
  abstract: Beam search is universally used in (full-sentence) machine translation
    but its application to simultaneous translation remains highly non-trivial, where
    output words are committed on the fly. In particular, the recently proposed wait-k
    policy (Ma et al., 2018) is a simple and effective method that (after an initial
    wait) commits one output word on receiving each input word, making beam search
    seemingly inapplicable. To address this challenge, we propose a new speculative
    beam search algorithm that hallucinates several steps into the future in order
    to reach a more accurate decision by implicitly benefiting from a target language
    model. This idea makes beam search applicable for the first time to the generation
    of a single word in each step. Experiments over diverse language pairs show large
    improvement compared to previous work.
  address: Hong Kong, China
  attachment:
  - filename: D19-1144.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1144.Attachment.pdf
  author:
  - first: Renjie
    full: Renjie Zheng
    id: renjie-zheng
    last: Zheng
  - first: Mingbo
    full: Mingbo Ma
    id: mingbo-ma
    last: Ma
  - first: Baigong
    full: Baigong Zheng
    id: baigong-zheng
    last: Zheng
  - first: Liang
    full: Liang Huang
    id: liang-huang
    last: Huang
  author_string: Renjie Zheng, Mingbo Ma, Baigong Zheng, Liang Huang
  bibkey: zheng-etal-2019-speculative
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1144
  month: November
  page_first: '1395'
  page_last: '1402'
  pages: "1395\u20131402"
  paper_id: '144'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1144.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1144.jpg
  title: Speculative Beam Search for Simultaneous Translation
  title_html: Speculative Beam Search for Simultaneous Translation
  url: https://www.aclweb.org/anthology/D19-1144
  year: '2019'
D19-1145:
  abstract: Although self-attention networks (SANs) have advanced the state-of-the-art
    on various NLP tasks, one criticism of SANs is their ability of encoding positions
    of input words (Shaw et al., 2018). In this work, we propose to augment SANs with
    structural position representations to model the latent structure of the input
    sentence, which is complementary to the standard sequential positional representations.
    Specifically, we use dependency tree to represent the grammatical structure of
    a sentence, and propose two strategies to encode the positional relationships
    among words in the dependency tree. Experimental results on NIST Chinese-to-English
    and WMT14 English-to-German translation tasks show that the proposed approach
    consistently boosts performance over both the absolute and relative sequential
    position representations.
  address: Hong Kong, China
  author:
  - first: Xing
    full: Xing Wang
    id: xing-wang
    last: Wang
  - first: Zhaopeng
    full: Zhaopeng Tu
    id: zhaopeng-tu
    last: Tu
  - first: Longyue
    full: Longyue Wang
    id: longyue-wang
    last: Wang
  - first: Shuming
    full: Shuming Shi
    id: shuming-shi
    last: Shi
  author_string: Xing Wang, Zhaopeng Tu, Longyue Wang, Shuming Shi
  bibkey: wang-etal-2019-self
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1145
  month: November
  page_first: '1403'
  page_last: '1409'
  pages: "1403\u20131409"
  paper_id: '145'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1145.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1145.jpg
  title: Self-Attention with Structural Position Representations
  title_html: Self-Attention with Structural Position Representations
  url: https://www.aclweb.org/anthology/D19-1145
  year: '2019'
D19-1146:
  abstract: "This paper highlights the impressive utility of multi-parallel corpora\
    \ for transfer learning in a one-to-many low-resource neural machine translation\
    \ (NMT) setting. We report on a systematic comparison of multistage fine-tuning\
    \ configurations, consisting of (1) pre-training on an external large (209k\u2013\
    440k) parallel corpus for English and a helping target language, (2) mixed pre-training\
    \ or fine-tuning on a mixture of the external and low-resource (18k) target parallel\
    \ corpora, and (3) pure fine-tuning on the target parallel corpora. Our experiments\
    \ confirm that multi-parallel corpora are extremely useful despite their scarcity\
    \ and content-wise redundancy thus exhibiting the true power of multilingualism.\
    \ Even when the helping target language is not one of the target languages of\
    \ our concern, our multistage fine-tuning can give 3\u20139 BLEU score gains over\
    \ a simple one-to-one model."
  address: Hong Kong, China
  author:
  - first: Raj
    full: Raj Dabre
    id: raj-dabre
    last: Dabre
  - first: Atsushi
    full: Atsushi Fujita
    id: atsushi-fujita
    last: Fujita
  - first: Chenhui
    full: Chenhui Chu
    id: chenhui-chu
    last: Chu
  author_string: Raj Dabre, Atsushi Fujita, Chenhui Chu
  bibkey: dabre-etal-2019-exploiting
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1146
  month: November
  page_first: '1410'
  page_last: '1416'
  pages: "1410\u20131416"
  paper_id: '146'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1146.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1146.jpg
  title: Exploiting Multilingualism through Multistage Fine-Tuning for Low-Resource
    Neural Machine Translation
  title_html: Exploiting Multilingualism through Multistage Fine-Tuning for Low-Resource
    Neural Machine Translation
  url: https://www.aclweb.org/anthology/D19-1146
  year: '2019'
D19-1147:
  abstract: The recent success of neural machine translation models relies on the
    availability of high quality, in-domain data. Domain adaptation is required when
    domain-specific data is scarce or nonexistent. Previous unsupervised domain adaptation
    strategies include training the model with in-domain copied monolingual or back-translated
    data. However, these methods use generic representations for text regardless of
    domain shift, which makes it infeasible for translation models to control outputs
    conditional on a specific domain. In this work, we propose an approach that adapts
    models with domain-aware feature embeddings, which are learned via an auxiliary
    language modeling task. Our approach allows the model to assign domain-specific
    representations to words and output sentences in the desired domain. Our empirical
    results demonstrate the effectiveness of the proposed strategy, achieving consistent
    improvements in multiple experimental settings. In addition, we show that combining
    our method with back translation can further improve the performance of the model.
  address: Hong Kong, China
  author:
  - first: Zi-Yi
    full: Zi-Yi Dou
    id: zi-yi-dou
    last: Dou
  - first: Junjie
    full: Junjie Hu
    id: junjie-hu
    last: Hu
  - first: Antonios
    full: Antonios Anastasopoulos
    id: antonios-anastasopoulos
    last: Anastasopoulos
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  author_string: Zi-Yi Dou, Junjie Hu, Antonios Anastasopoulos, Graham Neubig
  bibkey: dou-etal-2019-unsupervised
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1147
  month: November
  page_first: '1417'
  page_last: '1422'
  pages: "1417\u20131422"
  paper_id: '147'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1147.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1147.jpg
  title: Unsupervised Domain Adaptation for Neural Machine Translation with Domain-Aware
    Feature Embeddings
  title_html: Unsupervised Domain Adaptation for Neural Machine Translation with Domain-Aware
    Feature Embeddings
  url: https://www.aclweb.org/anthology/D19-1147
  year: '2019'
D19-1148:
  abstract: Grammar induction aims to discover syntactic structures from unannotated
    sentences. In this paper, we propose a framework in which the learning process
    of the grammar model of one language is influenced by knowledge from the model
    of another language. Unlike previous work on multilingual grammar induction, our
    approach does not rely on any external resource, such as parallel corpora, word
    alignments or linguistic phylogenetic trees. We propose three regularization methods
    that encourage similarity between model parameters, dependency edge scores, and
    parse trees respectively. We deploy our methods on a state-of-the-art unsupervised
    discriminative parser and evaluate it on both transfer grammar induction and bilingual
    grammar induction. Empirical results on multiple languages show that our methods
    outperform strong baselines.
  address: Hong Kong, China
  author:
  - first: Yong
    full: Yong Jiang
    id: yong-jiang
    last: Jiang
  - first: Wenjuan
    full: Wenjuan Han
    id: wenjuan-han
    last: Han
  - first: Kewei
    full: Kewei Tu
    id: kewei-tu
    last: Tu
  author_string: Yong Jiang, Wenjuan Han, Kewei Tu
  bibkey: jiang-etal-2019-regularization
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1148
  month: November
  page_first: '1423'
  page_last: '1428'
  pages: "1423\u20131428"
  paper_id: '148'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1148.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1148.jpg
  title: A Regularization-based Framework for Bilingual Grammar Induction
  title_html: A Regularization-based Framework for Bilingual Grammar Induction
  url: https://www.aclweb.org/anthology/D19-1148
  year: '2019'
D19-1149:
  abstract: Neural machine translation (NMT) has achieved new state-of-the-art performance
    in translating ambiguous words. However, it is still unclear which component dominates
    the process of disambiguation. In this paper, we explore the ability of NMT encoders
    and decoders to disambiguate word senses by evaluating hidden states and investigating
    the distributions of self-attention. We train a classifier to predict whether
    a translation is correct given the representation of an ambiguous noun. We find
    that encoder hidden states outperform word embeddings significantly which indicates
    that encoders adequately encode relevant information for disambiguation into hidden
    states. In contrast to encoders, the effect of decoder is different in models
    with different architectures. Moreover, the attention weights and attention entropy
    show that self-attention can detect ambiguous nouns and distribute more attention
    to the context.
  address: Hong Kong, China
  attachment:
  - filename: D19-1149.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1149.Attachment.zip
  author:
  - first: Gongbo
    full: Gongbo Tang
    id: gongbo-tang
    last: Tang
  - first: Rico
    full: Rico Sennrich
    id: rico-sennrich
    last: Sennrich
  - first: Joakim
    full: Joakim Nivre
    id: joakim-nivre
    last: Nivre
  author_string: Gongbo Tang, Rico Sennrich, Joakim Nivre
  bibkey: tang-etal-2019-encoders
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1149
  month: November
  page_first: '1429'
  page_last: '1435'
  pages: "1429\u20131435"
  paper_id: '149'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1149.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1149.jpg
  title: Encoders Help You Disambiguate Word Senses in Neural Machine Translation
  title_html: Encoders Help You Disambiguate Word Senses in Neural Machine Translation
  url: https://www.aclweb.org/anthology/D19-1149
  year: '2019'
D19-1150:
  abstract: Korean morphological analysis has been considered as a sequence of morpheme
    processing and POS tagging. Thus, a pipeline model of the tasks has been adopted
    widely by previous studies. However, the model has a problem that it cannot utilize
    interactions among the tasks. This paper formulates Korean morphological analysis
    as a combination of the tasks and presents a tied sequence-to-sequence multi-task
    model for training the two tasks simultaneously without any explicit regularization.
    The experiments prove the proposed model achieves the state-of-the-art performance.
  address: Hong Kong, China
  author:
  - first: Hyun-Je
    full: Hyun-Je Song
    id: hyun-je-song
    last: Song
  - first: Seong-Bae
    full: Seong-Bae Park
    id: seong-bae-park
    last: Park
  author_string: Hyun-Je Song, Seong-Bae Park
  bibkey: song-park-2019-korean
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1150
  month: November
  page_first: '1436'
  page_last: '1441'
  pages: "1436\u20131441"
  paper_id: '150'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1150.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1150.jpg
  title: Korean Morphological Analysis with Tied Sequence-to-Sequence Multi-Task Model
  title_html: <span class="acl-fixed-case">K</span>orean Morphological Analysis with
    Tied Sequence-to-Sequence Multi-Task Model
  url: https://www.aclweb.org/anthology/D19-1150
  year: '2019'
D19-1151:
  abstract: 'Diacritic restoration has gained importance with the growing need for
    machines to understand written texts. The task is typically modeled as a sequence
    labeling problem and currently Bidirectional Long Short Term Memory (BiLSTM) models
    provide state-of-the-art results. Recently, Bai et al. (2018) show the advantages
    of Temporal Convolutional Neural Networks (TCN) over Recurrent Neural Networks
    (RNN) for sequence modeling in terms of performance and computational resources.
    As diacritic restoration benefits from both previous as well as subsequent timesteps,
    we further apply and evaluate a variant of TCN, Acausal TCN (A-TCN), which incorporates
    context from both directions (previous and future) rather than strictly incorporating
    previous context as in the case of TCN. A-TCN yields significant improvement over
    TCN for diacritization in three different languages: Arabic, Yoruba, and Vietnamese.
    Furthermore, A-TCN and BiLSTM have comparable performance, making A-TCN an efficient
    alternative over BiLSTM since convolutions can be trained in parallel. A-TCN is
    significantly faster than BiLSTM at inference time (270% 334% improvement in the
    amount of text diacritized per minute).'
  address: Hong Kong, China
  author:
  - first: Sawsan
    full: Sawsan Alqahtani
    id: sawsan-alqahtani
    last: Alqahtani
  - first: Ajay
    full: Ajay Mishra
    id: ajay-mishra
    last: Mishra
  - first: Mona
    full: Mona Diab
    id: mona-diab
    last: Diab
  author_string: Sawsan Alqahtani, Ajay Mishra, Mona Diab
  bibkey: alqahtani-etal-2019-efficient
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1151
  month: November
  page_first: '1442'
  page_last: '1448'
  pages: "1442\u20131448"
  paper_id: '151'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1151.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1151.jpg
  title: Efficient Convolutional Neural Networks for Diacritic Restoration
  title_html: Efficient Convolutional Neural Networks for Diacritic Restoration
  url: https://www.aclweb.org/anthology/D19-1151
  year: '2019'
D19-1152:
  abstract: "Prior work on training generative Visual Dialog models with reinforcement\
    \ learning ((Das et al., ICCV 2017) has explored a Q-Bot-A-Bot image-guessing\
    \ game and shown that this \u2018self-talk\u2019 approach can lead to improved\
    \ performance at the downstream dialog-conditioned image-guessing task. However,\
    \ this improvement saturates and starts degrading after a few rounds of interaction,\
    \ and does not lead to a better Visual Dialog model. We find that this is due\
    \ in part to repeated interactions between Q-Bot and A-BOT during self-talk, which\
    \ are not informative with respect to the image. To improve this, we devise a\
    \ simple auxiliary objective that incentivizes Q-Bot to ask diverse questions,\
    \ thus reducing repetitions and in turn enabling A-Bot to explore a larger state\
    \ space during RL i.e. be exposed to more visual concepts to talk about, and varied\
    \ questions to answer. We evaluate our approach via a host of automatic metrics\
    \ and human studies, and demonstrate that it leads to better dialog, i.e. dialog\
    \ that is more diverse (i.e. less repetitive), consistent (i.e. has fewer conflicting\
    \ exchanges), fluent (i.e., more human-like), and detailed, while still being\
    \ comparably image-relevant as prior work and ablations."
  address: Hong Kong, China
  attachment:
  - filename: D19-1152.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1152.Attachment.pdf
  author:
  - first: Vishvak
    full: Vishvak Murahari
    id: vishvak-murahari
    last: Murahari
  - first: Prithvijit
    full: Prithvijit Chattopadhyay
    id: prithvijit-chattopadhyay
    last: Chattopadhyay
  - first: Dhruv
    full: Dhruv Batra
    id: dhruv-batra
    last: Batra
  - first: Devi
    full: Devi Parikh
    id: devi-parikh
    last: Parikh
  - first: Abhishek
    full: Abhishek Das
    id: abhishek-das
    last: Das
  author_string: Vishvak Murahari, Prithvijit Chattopadhyay, Dhruv Batra, Devi Parikh,
    Abhishek Das
  bibkey: murahari-etal-2019-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1152
  month: November
  page_first: '1449'
  page_last: '1454'
  pages: "1449\u20131454"
  paper_id: '152'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1152.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1152.jpg
  title: Improving Generative Visual Dialog by Answering Diverse Questions
  title_html: Improving Generative Visual Dialog by Answering Diverse Questions
  url: https://www.aclweb.org/anthology/D19-1152
  year: '2019'
D19-1153:
  abstract: A typical cross-lingual transfer learning approach boosting model performance
    on a language is to pre-train the model on all available supervised data from
    another language. However, in large-scale systems this leads to high training
    times and computational requirements. In addition, characteristic differences
    between the source and target languages raise a natural question of whether source
    data selection can improve the knowledge transfer. In this paper, we address this
    question and propose a simple but effective language model based source-language
    data selection method for cross-lingual transfer learning in large-scale spoken
    language understanding. The experimental results show that with data selection
    i) source data and hence training speed is reduced significantly and ii) model
    performance is improved.
  address: Hong Kong, China
  author:
  - first: Quynh
    full: Quynh Do
    id: quynh-do
    last: Do
  - first: Judith
    full: Judith Gaspers
    id: judith-gaspers
    last: Gaspers
  author_string: Quynh Do, Judith Gaspers
  bibkey: do-gaspers-2019-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1153
  month: November
  page_first: '1455'
  page_last: '1460'
  pages: "1455\u20131460"
  paper_id: '153'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1153.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1153.jpg
  title: Cross-lingual Transfer Learning with Data Selection for Large-Scale Spoken
    Language Understanding
  title_html: Cross-lingual Transfer Learning with Data Selection for Large-Scale
    Spoken Language Understanding
  url: https://www.aclweb.org/anthology/D19-1153
  year: '2019'
D19-1154:
  abstract: With the aim of promoting and understanding the multilingual version of
    image search, we leverage visual object detection and propose a model with diverse
    multi-head attention to learn grounded multilingual multimodal representations.
    Specifically, our model attends to different types of textual semantics in two
    languages and visual objects for fine-grained alignments between sentences and
    images. We introduce a new objective function which explicitly encourages attention
    diversity to learn an improved visual-semantic embedding space. We evaluate our
    model in the German-Image and English-Image matching tasks on the Multi30K dataset,
    and in the Semantic Textual Similarity task with the English descriptions of visual
    content. Results show that our model yields a significant performance gain over
    other methods in all of the three tasks.
  address: Hong Kong, China
  author:
  - first: Po-Yao
    full: Po-Yao Huang
    id: po-yao-huang
    last: Huang
  - first: Xiaojun
    full: Xiaojun Chang
    id: xiaojun-chang
    last: Chang
  - first: Alexander
    full: Alexander Hauptmann
    id: alexander-g-hauptmann
    last: Hauptmann
  author_string: Po-Yao Huang, Xiaojun Chang, Alexander Hauptmann
  bibkey: huang-etal-2019-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1154
  month: November
  page_first: '1461'
  page_last: '1467'
  pages: "1461\u20131467"
  paper_id: '154'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1154.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1154.jpg
  title: Multi-Head Attention with Diversity for Learning Grounded Multilingual Multimodal
    Representations
  title_html: Multi-Head Attention with Diversity for Learning Grounded Multilingual
    Multimodal Representations
  url: https://www.aclweb.org/anthology/D19-1154
  year: '2019'
D19-1155:
  abstract: Object detection plays an important role in current solutions to vision
    and language tasks like image captioning and visual question answering. However,
    popular models like Faster R-CNN rely on a costly process of annotating ground-truths
    for both the bounding boxes and their corresponding semantic labels, making it
    less amenable as a primitive task for transfer learning. In this paper, we examine
    the effect of decoupling box proposal and featurization for down-stream tasks.
    The key insight is that this allows us to leverage a large amount of labeled annotations
    that were previously unavailable for standard object detection benchmarks. Empirically,
    we demonstrate that this leads to effective transfer learning and improved image
    captioning and visual question answering models, as measured on publicly-available
    benchmarks.
  address: Hong Kong, China
  attachment:
  - filename: D19-1155.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1155.Attachment.zip
  author:
  - first: Soravit
    full: Soravit Changpinyo
    id: soravit-changpinyo
    last: Changpinyo
  - first: Bo
    full: Bo Pang
    id: bo-pang
    last: Pang
  - first: Piyush
    full: Piyush Sharma
    id: piyush-sharma
    last: Sharma
  - first: Radu
    full: Radu Soricut
    id: radu-soricut
    last: Soricut
  author_string: Soravit Changpinyo, Bo Pang, Piyush Sharma, Radu Soricut
  bibkey: changpinyo-etal-2019-decoupled
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1155
  month: November
  page_first: '1468'
  page_last: '1474'
  pages: "1468\u20131474"
  paper_id: '155'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1155.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1155.jpg
  title: Decoupled Box Proposal and Featurization with Ultrafine-Grained Semantic
    Labels Improve Image Captioning and Visual Question Answering
  title_html: Decoupled Box Proposal and Featurization with Ultrafine-Grained Semantic
    Labels Improve Image Captioning and Visual Question Answering
  url: https://www.aclweb.org/anthology/D19-1155
  year: '2019'
D19-1156:
  abstract: "Popular metrics used for evaluating image captioning systems, such as\
    \ BLEU and CIDEr, provide a single score to gauge the system\u2019s overall effectiveness.\
    \ This score is often not informative enough to indicate what specific errors\
    \ are made by a given system. In this study, we present a fine-grained evaluation\
    \ method REO for automatically measuring the performance of image captioning systems.\
    \ REO assesses the quality of captions from three perspectives: 1) Relevance to\
    \ the ground truth, 2) Extraness of the content that is irrelevant to the ground\
    \ truth, and 3) Omission of the elements in the images and human references. Experiments\
    \ on three benchmark datasets demonstrate that our method achieves a higher consistency\
    \ with human judgments and provides more intuitive evaluation results than alternative\
    \ metrics."
  address: Hong Kong, China
  author:
  - first: Ming
    full: Ming Jiang
    id: ming-jiang
    last: Jiang
  - first: Junjie
    full: Junjie Hu
    id: junjie-hu
    last: Hu
  - first: Qiuyuan
    full: Qiuyuan Huang
    id: qiuyuan-huang
    last: Huang
  - first: Lei
    full: Lei Zhang
    id: lei-zhang
    last: Zhang
  - first: Jana
    full: Jana Diesner
    id: jana-diesner
    last: Diesner
  - first: Jianfeng
    full: Jianfeng Gao
    id: jianfeng-gao
    last: Gao
  author_string: Ming Jiang, Junjie Hu, Qiuyuan Huang, Lei Zhang, Jana Diesner, Jianfeng
    Gao
  bibkey: jiang-etal-2019-reo
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1156
  month: November
  page_first: '1475'
  page_last: '1480'
  pages: "1475\u20131480"
  paper_id: '156'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1156.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1156.jpg
  title: 'REO-Relevance, Extraness, Omission: A Fine-grained Evaluation for Image
    Captioning'
  title_html: '<span class="acl-fixed-case">REO</span>-Relevance, Extraness, Omission:
    A Fine-grained Evaluation for Image Captioning'
  url: https://www.aclweb.org/anthology/D19-1156
  year: '2019'
D19-1157:
  abstract: We propose weakly supervised language localization networks (WSLLN) to
    detect events in long, untrimmed videos given language queries. To learn the correspondence
    between visual segments and texts, most previous methods require temporal coordinates
    (start and end times) of events for training, which leads to high costs of annotation.
    WSLLN relieves the annotation burden by training with only video-sentence pairs
    without accessing to temporal locations of events. With a simple end-to-end structure,
    WSLLN measures segment-text consistency and conducts segment selection (conditioned
    on the text) simultaneously. Results from both are merged and optimized as a video-sentence
    matching problem. Experiments on ActivityNet Captions and DiDeMo demonstrate that
    WSLLN achieves state-of-the-art performance.
  address: Hong Kong, China
  author:
  - first: Mingfei
    full: Mingfei Gao
    id: mingfei-gao
    last: Gao
  - first: Larry
    full: Larry Davis
    id: larry-davis
    last: Davis
  - first: Richard
    full: Richard Socher
    id: richard-socher
    last: Socher
  - first: Caiming
    full: Caiming Xiong
    id: caiming-xiong
    last: Xiong
  author_string: Mingfei Gao, Larry Davis, Richard Socher, Caiming Xiong
  bibkey: gao-etal-2019-wslln
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1157
  month: November
  page_first: '1481'
  page_last: '1487'
  pages: "1481\u20131487"
  paper_id: '157'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1157.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1157.jpg
  title: WSLLN:Weakly Supervised Natural Language Localization Networks
  title_html: <span class="acl-fixed-case">WSLLN</span>:Weakly Supervised Natural
    Language Localization Networks
  url: https://www.aclweb.org/anthology/D19-1157
  year: '2019'
D19-1158:
  abstract: "Grounding is crucial for natural language understanding. An important\
    \ subtask is to understand modified color expressions, such as \u201Clight blue\u201D\
    . We present a model of color modifiers that, compared with previous additive\
    \ models in RGB space, learns more complex transformations. In addition, we present\
    \ a model that operates in the HSV color space. We show that certain adjectives\
    \ are better modeled in that space. To account for all modifiers, we train a hard\
    \ ensemble model that selects a color space depending on the modifier-color pair.\
    \ Experimental results show significant and consistent improvements compared to\
    \ the state-of-the-art baseline model."
  address: Hong Kong, China
  author:
  - first: Xudong
    full: Xudong Han
    id: xudong-han
    last: Han
  - first: Philip
    full: Philip Schulz
    id: philip-schulz
    last: Schulz
  - first: Trevor
    full: Trevor Cohn
    id: trevor-cohn
    last: Cohn
  author_string: Xudong Han, Philip Schulz, Trevor Cohn
  bibkey: han-etal-2019-grounding
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1158
  month: November
  page_first: '1488'
  page_last: '1493'
  pages: "1488\u20131493"
  paper_id: '158'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1158.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1158.jpg
  title: 'Grounding learning of modifier dynamics: An application to color naming'
  title_html: 'Grounding learning of modifier dynamics: An application to color naming'
  url: https://www.aclweb.org/anthology/D19-1158
  year: '2019'
D19-1159:
  abstract: Core to the vision-and-language navigation (VLN) challenge is building
    robust instruction representations and action decoding schemes, which can generalize
    well to previously unseen instructions and environments. In this paper, we report
    two simple but highly effective methods to address these challenges and lead to
    a new state-of-the-art performance. First, we adapt large-scale pretrained language
    models to learn text representations that generalize better to previously unseen
    instructions. Second, we propose a stochastic sampling scheme to reduce the considerable
    gap between the expert actions in training and sampled actions in test, so that
    the agent can learn to correct its own mistakes during long sequential action
    decoding. Combining the two techniques, we achieve a new state of the art on the
    Room-to-Room benchmark with 6% absolute gain over the previous best result (47%
    -> 53%) on the Success Rate weighted by Path Length metric.
  address: Hong Kong, China
  attachment:
  - filename: D19-1159.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1159.Attachment.pdf
  author:
  - first: Xiujun
    full: Xiujun Li
    id: xiujun-li
    last: Li
  - first: Chunyuan
    full: Chunyuan Li
    id: chunyuan-li
    last: Li
  - first: Qiaolin
    full: Qiaolin Xia
    id: qiaolin-xia
    last: Xia
  - first: Yonatan
    full: Yonatan Bisk
    id: yonatan-bisk
    last: Bisk
  - first: Asli
    full: Asli Celikyilmaz
    id: asli-celikyilmaz
    last: Celikyilmaz
  - first: Jianfeng
    full: Jianfeng Gao
    id: jianfeng-gao
    last: Gao
  - first: Noah A.
    full: Noah A. Smith
    id: noah-a-smith
    last: Smith
  - first: Yejin
    full: Yejin Choi
    id: yejin-choi
    last: Choi
  author_string: Xiujun Li, Chunyuan Li, Qiaolin Xia, Yonatan Bisk, Asli Celikyilmaz,
    Jianfeng Gao, Noah A. Smith, Yejin Choi
  bibkey: li-etal-2019-robust
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1159
  month: November
  page_first: '1494'
  page_last: '1499'
  pages: "1494\u20131499"
  paper_id: '159'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1159.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1159.jpg
  title: Robust Navigation with Language Pretraining and Stochastic Sampling
  title_html: Robust Navigation with Language Pretraining and Stochastic Sampling
  url: https://www.aclweb.org/anthology/D19-1159
  year: '2019'
D19-1160:
  abstract: We explore whether it is possible to leverage eye-tracking data in an
    RNN dependency parser (for English) when such information is only available during
    training - i.e. no aggregated or token-level gaze features are used at inference
    time. To do so, we train a multitask learning model that parses sentences as sequence
    labeling and leverages gaze features as auxiliary tasks. Our method also learns
    to train from disjoint datasets, i.e. it can be used to test whether already collected
    gaze features are useful to improve the performance on new non-gazed annotated
    treebanks. Accuracy gains are modest but positive, showing the feasibility of
    the approach. It can serve as a first step towards architectures that can better
    leverage eye-tracking data or other complementary information available only for
    training sentences, possibly leading to improvements in syntactic parsing.
  address: Hong Kong, China
  attachment:
  - filename: D19-1160.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1160.Attachment.pdf
  author:
  - first: Michalina
    full: Michalina Strzyz
    id: michalina-strzyz
    last: Strzyz
  - first: David
    full: David Vilares
    id: david-vilares
    last: Vilares
  - first: Carlos
    full: "Carlos G\xF3mez-Rodr\xEDguez"
    id: carlos-gomez-rodriguez
    last: "G\xF3mez-Rodr\xEDguez"
  author_string: "Michalina Strzyz, David Vilares, Carlos G\xF3mez-Rodr\xEDguez"
  bibkey: strzyz-etal-2019-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1160
  month: November
  page_first: '1500'
  page_last: '1506'
  pages: "1500\u20131506"
  paper_id: '160'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1160.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1160.jpg
  title: Towards Making a Dependency Parser See
  title_html: Towards Making a Dependency Parser See
  url: https://www.aclweb.org/anthology/D19-1160
  year: '2019'
D19-1161:
  abstract: "Understanding text often requires identifying meaningful constituent\
    \ spans such as noun phrases and verb phrases. In this work, we show that we can\
    \ effectively recover these types of labels using the learned phrase vectors from\
    \ deep inside-outside recursive autoencoders (DIORA). Specifically, we cluster\
    \ span representations to induce span labels. Additionally, we improve the model\u2019\
    s labeling accuracy by integrating latent code learning into the training procedure.\
    \ We evaluate this approach empirically through unsupervised labeled constituency\
    \ parsing. Our method outperforms ELMo and BERT on two versions of the Wall Street\
    \ Journal (WSJ) dataset and is competitive to prior work that requires additional\
    \ human annotations, improving over a previous state-of-the-art system that depends\
    \ on ground-truth part-of-speech tags by 5 absolute F1 points (19% relative error\
    \ reduction)."
  address: Hong Kong, China
  attachment:
  - filename: D19-1161.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1161.Attachment.pdf
  author:
  - first: Andrew
    full: Andrew Drozdov
    id: andrew-drozdov
    last: Drozdov
  - first: Patrick
    full: Patrick Verga
    id: patrick-verga
    last: Verga
  - first: Yi-Pei
    full: Yi-Pei Chen
    id: yi-pei-chen
    last: Chen
  - first: Mohit
    full: Mohit Iyyer
    id: mohit-iyyer
    last: Iyyer
  - first: Andrew
    full: Andrew McCallum
    id: andrew-mccallum
    last: McCallum
  author_string: Andrew Drozdov, Patrick Verga, Yi-Pei Chen, Mohit Iyyer, Andrew McCallum
  bibkey: drozdov-etal-2019-unsupervised
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1161
  month: November
  page_first: '1507'
  page_last: '1512'
  pages: "1507\u20131512"
  paper_id: '161'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1161.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1161.jpg
  title: Unsupervised Labeled Parsing with Deep Inside-Outside Recursive Autoencoders
  title_html: Unsupervised Labeled Parsing with Deep Inside-Outside Recursive Autoencoders
  url: https://www.aclweb.org/anthology/D19-1161
  year: '2019'
D19-1162:
  abstract: Dependency parsing of conversational input can play an important role
    in language understanding for dialog systems by identifying the relationships
    between entities extracted from user utterances. Additionally, effective dependency
    parsing can elucidate differences in language structure and usage for discourse
    analysis of human-human versus human-machine dialogs. However, models trained
    on datasets based on news articles and web data do not perform well on spoken
    human-machine dialog, and currently available annotation schemes do not adapt
    well to dialog data. Therefore, we propose the Spoken Conversation Universal Dependencies
    (SCUD) annotation scheme that extends the Universal Dependencies (UD) (Nivre et
    al., 2016) guidelines to spoken human-machine dialogs. We also provide ConvBank,
    a conversation dataset between humans and an open-domain conversational dialog
    system with SCUD annotation. Finally, to demonstrate the utility of the dataset,
    we train a dependency parser on the ConvBank dataset. We demonstrate that by pre-training
    a dependency parser on a set of larger public datasets and fine-tuning on ConvBank
    data, we achieved the best result, 85.05% unlabeled and 77.82% labeled attachment
    accuracy.
  address: Hong Kong, China
  author:
  - first: Sam
    full: Sam Davidson
    id: sam-davidson
    last: Davidson
  - first: Dian
    full: Dian Yu
    id: dian-yu
    last: Yu
  - first: Zhou
    full: Zhou Yu
    id: zhou-yu
    last: Yu
  author_string: Sam Davidson, Dian Yu, Zhou Yu
  bibkey: davidson-etal-2019-dependency
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1162
  month: November
  page_first: '1513'
  page_last: '1519'
  pages: "1513\u20131519"
  paper_id: '162'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1162.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1162.jpg
  title: Dependency Parsing for Spoken Dialog Systems
  title_html: Dependency Parsing for Spoken Dialog Systems
  url: https://www.aclweb.org/anthology/D19-1162
  year: '2019'
D19-1163:
  abstract: 'We propose a semantic parser for parsing compositional utterances into
    Task Oriented Parse (TOP), a tree representation that has intents and slots as
    labels of nesting tree nodes. Our parser is span-based: it scores labels of the
    tree nodes covering each token span independently, but then decodes a valid tree
    globally. In contrast to previous sequence decoding approaches and other span-based
    parsers, we (1) improve the training speed by removing the need to run the decoder
    at training time; and (2) introduce edge scores, which model relations between
    parent and child labels, to mitigate the independence assumption between node
    labels and improve accuracy. Our best parser outperforms previous methods on the
    TOP dataset of mixed-domain task-oriented utterances in both accuracy and training
    speed.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1163.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1163.Attachment.zip
  author:
  - first: Panupong
    full: Panupong Pasupat
    id: panupong-pasupat
    last: Pasupat
  - first: Sonal
    full: Sonal Gupta
    id: sonal-gupta
    last: Gupta
  - first: Karishma
    full: Karishma Mandyam
    id: karishma-mandyam
    last: Mandyam
  - first: Rushin
    full: Rushin Shah
    id: rushin-shah
    last: Shah
  - first: Mike
    full: Mike Lewis
    id: mike-lewis
    last: Lewis
  - first: Luke
    full: Luke Zettlemoyer
    id: luke-zettlemoyer
    last: Zettlemoyer
  author_string: Panupong Pasupat, Sonal Gupta, Karishma Mandyam, Rushin Shah, Mike
    Lewis, Luke Zettlemoyer
  bibkey: pasupat-etal-2019-span
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1163
  month: November
  page_first: '1520'
  page_last: '1526'
  pages: "1520\u20131526"
  paper_id: '163'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1163.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1163.jpg
  title: Span-based Hierarchical Semantic Parsing for Task-Oriented Dialog
  title_html: Span-based Hierarchical Semantic Parsing for Task-Oriented Dialog
  url: https://www.aclweb.org/anthology/D19-1163
  year: '2019'
D19-1164:
  abstract: Context modeling is essential to generate coherent and consistent translation
    for Document-level Neural Machine Translations. The widely used method for document-level
    translation usually compresses the context information into a representation via
    hierarchical attention networks. However, this method neither considers the relationship
    between context words nor distinguishes the roles of context words. To address
    this problem, we propose a query-guided capsule networks to cluster context information
    into different perspectives from which the target translation may concern. Experiment
    results show that our method can significantly outperform strong baselines on
    multiple data sets of different domains.
  address: Hong Kong, China
  author:
  - first: Zhengxin
    full: Zhengxin Yang
    id: zhengxin-yang
    last: Yang
  - first: Jinchao
    full: Jinchao Zhang
    id: jinchao-zhang
    last: Zhang
  - first: Fandong
    full: Fandong Meng
    id: fandong-meng
    last: Meng
  - first: Shuhao
    full: Shuhao Gu
    id: shuhao-gu
    last: Gu
  - first: Yang
    full: Yang Feng
    id: yang-feng
    last: Feng
  - first: Jie
    full: Jie Zhou
    id: jie-zhou
    last: Zhou
  author_string: Zhengxin Yang, Jinchao Zhang, Fandong Meng, Shuhao Gu, Yang Feng,
    Jie Zhou
  bibkey: yang-etal-2019-enhancing
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1164
  month: November
  page_first: '1527'
  page_last: '1537'
  pages: "1527\u20131537"
  paper_id: '164'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1164.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1164.jpg
  title: Enhancing Context Modeling with a Query-Guided Capsule Network for Document-level
    Translation
  title_html: Enhancing Context Modeling with a Query-Guided Capsule Network for Document-level
    Translation
  url: https://www.aclweb.org/anthology/D19-1164
  year: '2019'
D19-1165:
  abstract: 'Fine-tuning pre-trained Neural Machine Translation (NMT) models is the
    dominant approach for adapting to new languages and domains. However, fine-tuning
    requires adapting and maintaining a separate model for each target task. We propose
    a simple yet efficient approach for adaptation in NMT. Our proposed approach consists
    of injecting tiny task specific adapter layers into a pre-trained model. These
    lightweight adapters, with just a small fraction of the original model size, adapt
    the model to multiple individual tasks simultaneously. We evaluate our approach
    on two tasks: (i) Domain Adaptation and (ii) Massively Multilingual NMT. Experiments
    on domain adaptation demonstrate that our proposed approach is on par with full
    fine-tuning on various domains, dataset sizes and model capacities. On a massively
    multilingual dataset of 103 languages, our adaptation approach bridges the gap
    between individual bilingual models and one massively multilingual model for most
    language pairs, paving the way towards universal machine translation.'
  address: Hong Kong, China
  author:
  - first: Ankur
    full: Ankur Bapna
    id: ankur-bapna
    last: Bapna
  - first: Orhan
    full: Orhan Firat
    id: orhan-firat
    last: Firat
  author_string: Ankur Bapna, Orhan Firat
  bibkey: bapna-firat-2019-simple
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1165
  month: November
  page_first: '1538'
  page_last: '1548'
  pages: "1538\u20131548"
  paper_id: '165'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1165.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1165.jpg
  title: Simple, Scalable Adaptation for Neural Machine Translation
  title_html: Simple, Scalable Adaptation for Neural Machine Translation
  url: https://www.aclweb.org/anthology/D19-1165
  year: '2019'
D19-1166:
  abstract: This work introduces a machine translation task where the output is aimed
    at audiences of different levels of target language proficiency. We collect a
    high quality dataset of news articles available in English and Spanish, written
    for diverse grade levels and propose a method to align segments across comparable
    bilingual articles. The resulting dataset makes it possible to train multi-task
    sequence to sequence models that can translate and simplify text jointly. We show
    that these multi-task models outperform pipeline approaches that translate and
    simplify text independently.
  address: Hong Kong, China
  attachment:
  - filename: D19-1166.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1166.Attachment.zip
  author:
  - first: Sweta
    full: Sweta Agrawal
    id: sweta-agrawal
    last: Agrawal
  - first: Marine
    full: Marine Carpuat
    id: marine-carpuat
    last: Carpuat
  author_string: Sweta Agrawal, Marine Carpuat
  bibkey: agrawal-carpuat-2019-controlling
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1166
  month: November
  page_first: '1549'
  page_last: '1564'
  pages: "1549\u20131564"
  paper_id: '166'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1166.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1166.jpg
  title: Controlling Text Complexity in Neural Machine Translation
  title_html: Controlling Text Complexity in Neural Machine Translation
  url: https://www.aclweb.org/anthology/D19-1166
  year: '2019'
D19-1167:
  abstract: 'Multilingual Neural Machine Translation (NMT) models have yielded large
    empirical success in transfer learning settings. However, these black-box representations
    are poorly understood, and their mode of transfer remains elusive. In this work,
    we attempt to understand massively multilingual NMT representations (with 103
    languages) using Singular Value Canonical Correlation Analysis (SVCCA), a representation
    similarity framework that allows us to compare representations across different
    languages, layers and models. Our analysis validates several empirical results
    and long-standing intuitions, and unveils new observations regarding how representations
    evolve in a multilingual translation model. We draw three major results from our
    analysis, with implications on cross-lingual transfer learning: (i) Encoder representations
    of different languages cluster based on linguistic similarity, (ii) Representations
    of a source language learned by the encoder are dependent on the target language,
    and vice-versa, and (iii) Representations of high resource and/or linguistically
    similar languages are more robust when fine-tuning on an arbitrary language pair,
    which is critical to determining how much cross-lingual transfer can be expected
    in a zero or few-shot setting. We further connect our findings with existing empirical
    observations in multilingual NMT and transfer learning.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1167.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1167.Attachment.zip
  author:
  - first: Sneha
    full: Sneha Kudugunta
    id: sneha-kudugunta
    last: Kudugunta
  - first: Ankur
    full: Ankur Bapna
    id: ankur-bapna
    last: Bapna
  - first: Isaac
    full: Isaac Caswell
    id: isaac-caswell
    last: Caswell
  - first: Orhan
    full: Orhan Firat
    id: orhan-firat
    last: Firat
  author_string: Sneha Kudugunta, Ankur Bapna, Isaac Caswell, Orhan Firat
  bibkey: kudugunta-etal-2019-investigating
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1167
  month: November
  page_first: '1565'
  page_last: '1575'
  pages: "1565\u20131575"
  paper_id: '167'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1167.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1167.jpg
  title: Investigating Multilingual NMT Representations at Scale
  title_html: Investigating Multilingual <span class="acl-fixed-case">NMT</span> Representations
    at Scale
  url: https://www.aclweb.org/anthology/D19-1167
  year: '2019'
D19-1168:
  abstract: Document-level machine translation (MT) remains challenging due to the
    difficulty in efficiently using document context for translation. In this paper,
    we propose a hierarchical model to learn the global context for document-level
    neural machine translation (NMT). This is done through a sentence encoder to capture
    intra-sentence dependencies and a document encoder to model document-level inter-sentence
    consistency and coherence. With this hierarchical architecture, we feedback the
    extracted global document context to each word in a top-down fashion to distinguish
    different translations of a word according to its specific surrounding context.
    In addition, since large-scale in-domain document-level parallel corpora are usually
    unavailable, we use a two-step training strategy to take advantage of a large-scale
    corpus with out-of-domain parallel sentence pairs and a small-scale corpus with
    in-domain parallel document pairs to achieve the domain adaptability. Experimental
    results on several benchmark corpora show that our proposed model can significantly
    improve document-level translation performance over several strong NMT baselines.
  address: Hong Kong, China
  author:
  - first: Xin
    full: Xin Tan
    id: xin-tan
    last: Tan
  - first: Longyin
    full: Longyin Zhang
    id: longyin-zhang
    last: Zhang
  - first: Deyi
    full: Deyi Xiong
    id: deyi-xiong
    last: Xiong
  - first: Guodong
    full: Guodong Zhou
    id: guodong-zhou
    last: Zhou
  author_string: Xin Tan, Longyin Zhang, Deyi Xiong, Guodong Zhou
  bibkey: tan-etal-2019-hierarchical
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1168
  month: November
  page_first: '1576'
  page_last: '1585'
  pages: "1576\u20131585"
  paper_id: '168'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1168.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1168.jpg
  title: Hierarchical Modeling of Global Context for Document-Level Neural Machine
    Translation
  title_html: Hierarchical Modeling of Global Context for Document-Level Neural Machine
    Translation
  url: https://www.aclweb.org/anthology/D19-1168
  year: '2019'
D19-1169:
  abstract: 'Though the community has made great progress on Machine Reading Comprehension
    (MRC) task, most of the previous works are solving English-based MRC problems,
    and there are few efforts on other languages mainly due to the lack of large-scale
    training data.In this paper, we propose Cross-Lingual Machine Reading Comprehension
    (CLMRC) task for the languages other than English. Firstly, we present several
    back-translation approaches for CLMRC task which is straightforward to adopt.
    However, to exactly align the answer into source language is difficult and could
    introduce additional noise. In this context, we propose a novel model called Dual
    BERT, which takes advantage of the large-scale training data provided by rich-resource
    language (such as English) and learn the semantic relations between the passage
    and question in bilingual context, and then utilize the learned knowledge to improve
    reading comprehension performance of low-resource language. We conduct experiments
    on two Chinese machine reading comprehension datasets CMRC 2018 and DRCD. The
    results show consistent and significant improvements over various state-of-the-art
    systems by a large margin, which demonstrate the potentials in CLMRC task. Resources
    available: https://github.com/ymcui/Cross-Lingual-MRC'
  address: Hong Kong, China
  author:
  - first: Yiming
    full: Yiming Cui
    id: yiming-cui
    last: Cui
  - first: Wanxiang
    full: Wanxiang Che
    id: wanxiang-che
    last: Che
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  - first: Bing
    full: Bing Qin
    id: bing-qin
    last: Qin
  - first: Shijin
    full: Shijin Wang
    id: shijin-wang
    last: Wang
  - first: Guoping
    full: Guoping Hu
    id: guoping-hu
    last: Hu
  author_string: Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, Guoping
    Hu
  bibkey: cui-etal-2019-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1169
  month: November
  page_first: '1586'
  page_last: '1595'
  pages: "1586\u20131595"
  paper_id: '169'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1169.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1169.jpg
  title: Cross-Lingual Machine Reading Comprehension
  title_html: Cross-Lingual Machine Reading Comprehension
  url: https://www.aclweb.org/anthology/D19-1169
  year: '2019'
D19-1170:
  abstract: Rapid progress has been made in the field of reading comprehension and
    question answering, where several systems have achieved human parity in some simplified
    settings. However, the performance of these models degrades significantly when
    they are applied to more realistic scenarios, such as answers involve various
    types, multiple text strings are correct answers, or discrete reasoning abilities
    are required. In this paper, we introduce the Multi-Type Multi-Span Network (MTMSN),
    a neural reading comprehension model that combines a multi-type answer predictor
    designed to support various answer types (e.g., span, count, negation, and arithmetic
    expression) with a multi-span extraction method for dynamically producing one
    or multiple text spans. In addition, an arithmetic expression reranking mechanism
    is proposed to rank expression candidates for further confirming the prediction.
    Experiments show that our model achieves 79.9 F1 on the DROP hidden test set,
    creating new state-of-the-art results. Source code (https://github.com/huminghao16/MTMSN)
    is released to facilitate future work.
  address: Hong Kong, China
  attachment:
  - filename: D19-1170.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1170.Attachment.zip
  author:
  - first: Minghao
    full: Minghao Hu
    id: minghao-hu
    last: Hu
  - first: Yuxing
    full: Yuxing Peng
    id: yuxing-peng
    last: Peng
  - first: Zhen
    full: Zhen Huang
    id: zhen-huang
    last: Huang
  - first: Dongsheng
    full: Dongsheng Li
    id: dongsheng-li
    last: Li
  author_string: Minghao Hu, Yuxing Peng, Zhen Huang, Dongsheng Li
  bibkey: hu-etal-2019-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1170
  month: November
  page_first: '1596'
  page_last: '1606'
  pages: "1596\u20131606"
  paper_id: '170'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1170.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1170.jpg
  title: A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete
    Reasoning
  title_html: A Multi-Type Multi-Span Network for Reading Comprehension that Requires
    Discrete Reasoning
  url: https://www.aclweb.org/anthology/D19-1170
  year: '2019'
D19-1171:
  abstract: "Supervised training of neural models to duplicate question detection\
    \ in community Question Answering (CQA) requires large amounts of labeled question\
    \ pairs, which can be costly to obtain. To minimize this cost, recent works thus\
    \ often used alternative methods, e.g., adversarial domain adaptation. In this\
    \ work, we propose two novel methods\u2014weak supervision using the title and\
    \ body of a question, and the automatic generation of duplicate questions\u2014\
    and show that both can achieve improved performances even though they do not require\
    \ any labeled data. We provide a comparison of popular training strategies and\
    \ show that our proposed approaches are more effective in many cases because they\
    \ can utilize larger amounts of data from the CQA forums. Finally, we show that\
    \ weak supervision with question title and body information is also an effective\
    \ method to train CQA answer selection models without direct answer supervision."
  address: Hong Kong, China
  attachment:
  - filename: D19-1171.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1171.Attachment.zip
  author:
  - first: Andreas
    full: "Andreas R\xFCckl\xE9"
    id: andreas-ruckle
    last: "R\xFCckl\xE9"
  - first: Nafise Sadat
    full: Nafise Sadat Moosavi
    id: nafise-sadat-moosavi
    last: Moosavi
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: "Andreas R\xFCckl\xE9, Nafise Sadat Moosavi, Iryna Gurevych"
  bibkey: ruckle-etal-2019-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1171
  month: November
  page_first: '1607'
  page_last: '1617'
  pages: "1607\u20131617"
  paper_id: '171'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1171.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1171.jpg
  title: Neural Duplicate Question Detection without Labeled Training Data
  title_html: Neural Duplicate Question Detection without Labeled Training Data
  url: https://www.aclweb.org/anthology/D19-1171
  year: '2019'
D19-1172:
  abstract: 'The ability to ask clarification questions is essential for knowledge-based
    question answering (KBQA) systems, especially for handling ambiguous phenomena.
    Despite its importance, clarification has not been well explored in current KBQA
    systems. Further progress requires supervised resources for training and evaluation,
    and powerful models for clarification-related text understanding and generation.
    In this paper, we construct a new clarification dataset, CLAQUA, with nearly 40K
    open-domain examples. The dataset supports three serial tasks: given a question,
    identify whether clarification is needed; if yes, generate a clarification question;
    then predict answers base on external user feedback. We provide representative
    baselines for these tasks and further introduce a coarse-to-fine model for clarification
    question generation. Experiments show that the proposed model achieves better
    performance than strong baselines. The further analysis demonstrates that our
    dataset brings new challenges and there still remain several unsolved problems,
    like reasonable automatic evaluation metrics for clarification question generation
    and powerful models for handling entity sparsity.'
  address: Hong Kong, China
  author:
  - first: Jingjing
    full: Jingjing Xu
    id: jingjing-xu
    last: Xu
  - first: Yuechen
    full: Yuechen Wang
    id: yuechen-wang
    last: Wang
  - first: Duyu
    full: Duyu Tang
    id: duyu-tang
    last: Tang
  - first: Nan
    full: Nan Duan
    id: nan-duan
    last: Duan
  - first: Pengcheng
    full: Pengcheng Yang
    id: pengcheng-yang
    last: Yang
  - first: Qi
    full: Qi Zeng
    id: qi-zeng
    last: Zeng
  - first: Ming
    full: Ming Zhou
    id: ming-zhou
    last: Zhou
  - first: Xu
    full: Xu Sun
    id: xu-sun
    last: Sun
  author_string: Jingjing Xu, Yuechen Wang, Duyu Tang, Nan Duan, Pengcheng Yang, Qi
    Zeng, Ming Zhou, Xu Sun
  bibkey: xu-etal-2019-asking
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1172
  month: November
  page_first: '1618'
  page_last: '1629'
  pages: "1618\u20131629"
  paper_id: '172'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1172.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1172.jpg
  title: Asking Clarification Questions in Knowledge-Based Question Answering
  title_html: Asking Clarification Questions in Knowledge-Based Question Answering
  url: https://www.aclweb.org/anthology/D19-1172
  year: '2019'
D19-1173:
  abstract: We address the problem of Duplicate Question Detection (DQD) in low-resource
    domain-specific Community Question Answering forums. Our multi-view framework
    MV-DASE combines an ensemble of sentence encoders via Generalized Canonical Correlation
    Analysis, using unlabeled data only. In our experiments, the ensemble includes
    generic and domain-specific averaged word embeddings, domain-finetuned BERT and
    the Universal Sentence Encoder. We evaluate MV-DASE on the CQADupStack corpus
    and on additional low-resource Stack Exchange forums. Combining the strengths
    of different encoders, we significantly outperform BM25, all single-view systems
    as well as a recent supervised domain-adversarial DQD method.
  address: Hong Kong, China
  attachment:
  - filename: D19-1173.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1173.Attachment.zip
  author:
  - first: Nina
    full: Nina Poerner
    id: nina-poerner
    last: Poerner
  - first: Hinrich
    full: "Hinrich Sch\xFCtze"
    id: hinrich-schutze
    last: "Sch\xFCtze"
  author_string: "Nina Poerner, Hinrich Sch\xFCtze"
  bibkey: poerner-schutze-2019-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1173
  month: November
  page_first: '1630'
  page_last: '1641'
  pages: "1630\u20131641"
  paper_id: '173'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1173.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1173.jpg
  title: Multi-View Domain Adapted Sentence Embeddings for Low-Resource Unsupervised
    Duplicate Question Detection
  title_html: Multi-View Domain Adapted Sentence Embeddings for Low-Resource Unsupervised
    Duplicate Question Detection
  url: https://www.aclweb.org/anthology/D19-1173
  year: '2019'
D19-1174:
  abstract: Sexism, an injustice that subjects women and girls to enormous suffering,
    manifests in blatant as well as subtle ways. In the wake of growing documentation
    of experiences of sexism on the web, the automatic categorization of accounts
    of sexism has the potential to assist social scientists and policy makers in utilizing
    such data to study and counter sexism better. The existing work on sexism classification,
    which is different from sexism detection, has certain limitations in terms of
    the categories of sexism used and/or whether they can co-occur. To the best of
    our knowledge, this is the first work on the multi-label classification of sexism
    of any kind(s), and we contribute the largest dataset for sexism categorization.
    We develop a neural solution for this multi-label classification that can combine
    sentence representations obtained using models such as BERT with distributional
    and linguistic word embeddings using a flexible, hierarchical architecture involving
    recurrent components and optional convolutional ones. Further, we leverage unlabeled
    accounts of sexism to infuse domain-specific elements into our framework. The
    best proposed method outperforms several deep learning as well as traditional
    machine learning baselines by an appreciable margin.
  address: Hong Kong, China
  attachment:
  - filename: D19-1174.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1174.Attachment.pdf
  author:
  - first: Pulkit
    full: Pulkit Parikh
    id: pulkit-parikh
    last: Parikh
  - first: Harika
    full: Harika Abburi
    id: harika-abburi
    last: Abburi
  - first: Pinkesh
    full: Pinkesh Badjatiya
    id: pinkesh-badjatiya
    last: Badjatiya
  - first: Radhika
    full: Radhika Krishnan
    id: radhika-krishnan
    last: Krishnan
  - first: Niyati
    full: Niyati Chhaya
    id: niyati-chhaya
    last: Chhaya
  - first: Manish
    full: Manish Gupta
    id: manish-gupta
    last: Gupta
  - first: Vasudeva
    full: Vasudeva Varma
    id: vasudeva-varma
    last: Varma
  author_string: Pulkit Parikh, Harika Abburi, Pinkesh Badjatiya, Radhika Krishnan,
    Niyati Chhaya, Manish Gupta, Vasudeva Varma
  bibkey: parikh-etal-2019-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1174
  month: November
  page_first: '1642'
  page_last: '1652'
  pages: "1642\u20131652"
  paper_id: '174'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1174.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1174.jpg
  title: Multi-label Categorization of Accounts of Sexism using a Neural Framework
  title_html: Multi-label Categorization of Accounts of Sexism using a Neural Framework
  url: https://www.aclweb.org/anthology/D19-1174
  year: '2019'
D19-1175:
  abstract: "The sequence of documents produced by any given author varies in style\
    \ and content, but some documents are more typical or representative of the source\
    \ than others. We quantify the extent to which a given short text is characteristic\
    \ of a specific person, using a dataset of tweets from fifteen celebrities. Such\
    \ analysis is useful for generating excerpts of high-volume Twitter profiles,\
    \ and understanding how representativeness relates to tweet popularity. We first\
    \ consider the related task of binary author detection (is x the author of text\
    \ T?), and report a test accuracy of 90.37% for the best of five approaches to\
    \ this problem. We then use these models to compute characterization scores among\
    \ all of an author\u2019s texts. A user study shows human evaluators agree with\
    \ our characterization model for all 15 celebrities in our dataset, each with\
    \ p-value < 0.05. We use these classifiers to show surprisingly strong correlations\
    \ between characterization scores and the popularity of the associated texts.\
    \ Indeed, we demonstrate a statistically significant correlation between this\
    \ score and tweet popularity (likes/replies/retweets) for 13 of the 15 celebrities\
    \ in our study."
  address: Hong Kong, China
  author:
  - first: Charuta
    full: Charuta Pethe
    id: charuta-pethe
    last: Pethe
  - first: Steve
    full: Steve Skiena
    id: steven-skiena
    last: Skiena
  author_string: Charuta Pethe, Steve Skiena
  bibkey: pethe-skiena-2019-trumpiest
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1175
  month: November
  page_first: '1653'
  page_last: '1663'
  pages: "1653\u20131663"
  paper_id: '175'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1175.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1175.jpg
  title: "The Trumpiest Trump? Identifying a Subject\u2019s Most Characteristic Tweets"
  title_html: "The Trumpiest Trump? Identifying a Subject\u2019s Most Characteristic\
    \ Tweets"
  url: https://www.aclweb.org/anthology/D19-1175
  year: '2019'
D19-1176:
  abstract: 'Microaggressions are subtle, often veiled, manifestations of human biases.
    These uncivil interactions can have a powerful negative impact on people by marginalizing
    minorities and disadvantaged groups. The linguistic subtlety of microaggressions
    in communication has made it difficult for researchers to analyze their exact
    nature, and to quantify and extract microaggressions automatically. Specifically,
    the lack of a corpus of real-world microaggressions and objective criteria for
    annotating them have prevented researchers from addressing these problems at scale.
    In this paper, we devise a general but nuanced, computationally operationalizable
    typology of microaggressions based on a small subset of data that we have. We
    then create two datasets: one with examples of diverse types of microaggressions
    recollected by their targets, and another with gender-based microaggressions in
    public conversations on social media. We introduce a new, more objective, criterion
    for annotation and an active-learning based procedure that increases the likelihood
    of surfacing posts containing microaggressions. Finally, we analyze the trends
    that emerge from these new datasets.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1176.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1176.Attachment.zip
  author:
  - first: Luke
    full: Luke Breitfeller
    id: luke-breitfeller
    last: Breitfeller
  - first: Emily
    full: Emily Ahn
    id: emily-ahn
    last: Ahn
  - first: David
    full: David Jurgens
    id: david-jurgens
    last: Jurgens
  - first: Yulia
    full: Yulia Tsvetkov
    id: yulia-tsvetkov
    last: Tsvetkov
  author_string: Luke Breitfeller, Emily Ahn, David Jurgens, Yulia Tsvetkov
  bibkey: breitfeller-etal-2019-finding
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1176
  month: November
  page_first: '1664'
  page_last: '1674'
  pages: "1664\u20131674"
  paper_id: '176'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1176.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1176.jpg
  title: 'Finding Microaggressions in the Wild: A Case for Locating Elusive Phenomena
    in Social Media Posts'
  title_html: 'Finding Microaggressions in the Wild: A Case for Locating Elusive Phenomena
    in Social Media Posts'
  url: https://www.aclweb.org/anthology/D19-1176
  year: '2019'
D19-1177:
  abstract: 'To automatically assess the helpfulness of a customer review online,
    conventional approaches generally acquire various linguistic and neural embedding
    features solely from the textual content of the review itself as the evidence.
    We, however, find out that a helpful review is largely concerned with the metadata
    (such as the name, the brand, the category, etc.) of its target product. It leaves
    us with a challenge of how to choose the correct key-value product metadata to
    help appraise the helpfulness of free-text reviews more precisely. To address
    this problem, we propose a novel framework composed of two mutual-benefit modules.
    Given a product, a selector (agent) learns from both the keys in the product metadata
    and one of its reviews to take an action that selects the correct value, and a
    successive predictor (network) makes the free-text review attend to this value
    to obtain better neural representations for helpfulness assessment. The predictor
    is directly optimized by SGD with the loss of helpfulness prediction, and the
    selector could be updated via policy gradient rewarded with the performance of
    the predictor. We use two real-world datasets from Amazon.com and Yelp.com, respectively,
    to compare the performance of our framework with other mainstream methods under
    two application scenarios: helpfulness identification and regression of customer
    reviews. Extensive results demonstrate that our framework can achieve state-of-the-art
    performance with substantial improvements.'
  address: Hong Kong, China
  author:
  - first: Miao
    full: Miao Fan
    id: miao-fan
    last: Fan
  - first: Chao
    full: Chao Feng
    id: chao-feng
    last: Feng
  - first: Mingming
    full: Mingming Sun
    id: mingming-sun
    last: Sun
  - first: Ping
    full: Ping Li
    id: ping-li
    last: Li
  author_string: Miao Fan, Chao Feng, Mingming Sun, Ping Li
  bibkey: fan-etal-2019-reinforced
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1177
  month: November
  page_first: '1675'
  page_last: '1683'
  pages: "1675\u20131683"
  paper_id: '177'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1177.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1177.jpg
  title: Reinforced Product Metadata Selection for Helpfulness Assessment of Customer
    Reviews
  title_html: Reinforced Product Metadata Selection for Helpfulness Assessment of
    Customer Reviews
  url: https://www.aclweb.org/anthology/D19-1177
  year: '2019'
D19-1178:
  abstract: "The evolution of social media users\u2019 behavior over time complicates\
    \ user-level comparison tasks such as verification, classification, clustering,\
    \ and ranking. As a result, naive approaches may fail to generalize to new users\
    \ or even to future observations of previously known users. In this paper, we\
    \ propose a novel procedure to learn a mapping from short episodes of user activity\
    \ on social media to a vector space in which the distance between points captures\
    \ the similarity of the corresponding users\u2019 invariant features. We fit the\
    \ model by optimizing a surrogate metric learning objective over a large corpus\
    \ of unlabeled social media content. Once learned, the mapping may be applied\
    \ to users not seen at training time and enables efficient comparisons of users\
    \ in the resulting vector space. We present a comprehensive evaluation to validate\
    \ the benefits of the proposed approach using data from Reddit, Twitter, and Wikipedia."
  address: Hong Kong, China
  author:
  - first: Nicholas
    full: Nicholas Andrews
    id: nicholas-andrews
    last: Andrews
  - first: Marcus
    full: Marcus Bishop
    id: marcus-bishop
    last: Bishop
  author_string: Nicholas Andrews, Marcus Bishop
  bibkey: andrews-bishop-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1178
  month: November
  page_first: '1684'
  page_last: '1695'
  pages: "1684\u20131695"
  paper_id: '178'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1178.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1178.jpg
  title: Learning Invariant Representations of Social Media Users
  title_html: Learning Invariant Representations of Social Media Users
  url: https://www.aclweb.org/anthology/D19-1178
  year: '2019'
D19-1179:
  abstract: "Stylistic variation in text needs to be studied with different aspects\
    \ including the writer\u2019s personal traits, interpersonal relations, rhetoric,\
    \ and more. Despite recent attempts on computational modeling of the variation,\
    \ the lack of parallel corpora of style language makes it difficult to systematically\
    \ control the stylistic change as well as evaluate such models. We release PASTEL,\
    \ the parallel and annotated stylistic language dataset, that contains ~41K parallel\
    \ sentences (8.3K parallel stories) annotated across different personas. Each\
    \ persona has different styles in conjunction: gender, age, country, political\
    \ view, education, ethnic, and time-of-writing. The dataset is collected from\
    \ human annotators with solid control of input denotation: not only preserving\
    \ original meaning between text, but promoting stylistic diversity to annotators.\
    \ We test the dataset on two interesting applications of style language, where\
    \ PASTEL helps design appropriate experiment and evaluation. First, in predicting\
    \ a target style (e.g., male or female in gender) given a text, multiple styles\
    \ of PASTEL make other external style variables controlled (or fixed), which is\
    \ a more accurate experimental design. Second, a simple supervised model with\
    \ our parallel text outperforms the unsupervised models using nonparallel text\
    \ in style transfer. Our dataset is publicly available."
  address: Hong Kong, China
  attachment:
  - filename: D19-1179.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1179.Attachment.pdf
  author:
  - first: Dongyeop
    full: Dongyeop Kang
    id: dongyeop-kang
    last: Kang
  - first: Varun
    full: Varun Gangal
    id: varun-gangal
    last: Gangal
  - first: Eduard
    full: Eduard Hovy
    id: eduard-hovy
    last: Hovy
  author_string: Dongyeop Kang, Varun Gangal, Eduard Hovy
  bibkey: kang-etal-2019-male
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1179
  month: November
  page_first: '1696'
  page_last: '1706'
  pages: "1696\u20131706"
  paper_id: '179'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1179.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1179.jpg
  title: '(Male, Bachelor) and (Female, Ph.D) have different connotations: Parallelly
    Annotated Stylistic Language Dataset with Multiple Personas'
  title_html: '(Male, Bachelor) and (Female, <span class="acl-fixed-case">P</span>h.<span
    class="acl-fixed-case">D</span>) have different connotations: Parallelly Annotated
    Stylistic Language Dataset with Multiple Personas'
  url: https://www.aclweb.org/anthology/D19-1179
  year: '2019'
D19-1180:
  abstract: 'According to screenwriting theory, turning points (e.g., change of plans,
    major setback, climax) are crucial narrative moments within a screenplay: they
    define the plot structure, determine its progression and segment the screenplay
    into thematic units (e.g., setup, complications, aftermath). We propose the task
    of turning point identification in movies as a means of analyzing their narrative
    structure. We argue that turning points and the segmentation they provide can
    facilitate processing long, complex narratives, such as screenplays, for summarization
    and question answering. We introduce a dataset consisting of screenplays and plot
    synopses annotated with turning points and present an end-to-end neural network
    model that identifies turning points in plot synopses and projects them onto scenes
    in screenplays. Our model outperforms strong baselines based on state-of-the-art
    sentence representations and the expected position of turning points.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1180.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1180.Attachment.zip
  author:
  - first: Pinelopi
    full: Pinelopi Papalampidi
    id: pinelopi-papalampidi
    last: Papalampidi
  - first: Frank
    full: Frank Keller
    id: frank-keller
    last: Keller
  - first: Mirella
    full: Mirella Lapata
    id: mirella-lapata
    last: Lapata
  author_string: Pinelopi Papalampidi, Frank Keller, Mirella Lapata
  bibkey: papalampidi-etal-2019-movie
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1180
  month: November
  page_first: '1707'
  page_last: '1717'
  pages: "1707\u20131717"
  paper_id: '180'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1180.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1180.jpg
  title: Movie Plot Analysis via Turning Point Identification
  title_html: Movie Plot Analysis via Turning Point Identification
  url: https://www.aclweb.org/anthology/D19-1180
  year: '2019'
D19-1181:
  abstract: "Despite detection of suicidal ideation on social media has made great\
    \ progress in recent years, people\u2019s implicitly and anti-real contrarily\
    \ expressed posts still remain as an obstacle, constraining the detectors to acquire\
    \ higher satisfactory performance. Enlightened by the hidden \u201Ctree holes\u201D\
    \ phenomenon on microblog, where people at suicide risk tend to disclose their\
    \ inner real feelings and thoughts to the microblog space whose authors have committed\
    \ suicide, we explore the use of tree holes to enhance microblog-based suicide\
    \ risk detection from the following two perspectives. (1) We build suicide-oriented\
    \ word embeddings based on tree hole contents to strength the sensibility of suicide-related\
    \ lexicons and context based on tree hole contents. (2) A two-layered attention\
    \ mechanism is deployed to grasp intermittently changing points from individual\u2019\
    s open blog streams, revealing one\u2019s inner emotional world more or less.\
    \ Our experimental results show that with suicide-oriented word embeddings and\
    \ attention, microblog-based suicide risk detection can achieve over 91% accuracy.\
    \ A large-scale well-labelled suicide data set is also reported in the paper."
  address: Hong Kong, China
  author:
  - first: Lei
    full: Lei Cao
    id: lei-cao
    last: Cao
  - first: Huijun
    full: Huijun Zhang
    id: huijun-zhang
    last: Zhang
  - first: Ling
    full: Ling Feng
    id: ling-feng
    last: Feng
  - first: Zihan
    full: Zihan Wei
    id: zihan-wei
    last: Wei
  - first: Xin
    full: Xin Wang
    id: xin-wang
    last: Wang
  - first: Ningyun
    full: Ningyun Li
    id: ningyun-li
    last: Li
  - first: Xiaohao
    full: Xiaohao He
    id: xiaohao-he
    last: He
  author_string: Lei Cao, Huijun Zhang, Ling Feng, Zihan Wei, Xin Wang, Ningyun Li,
    Xiaohao He
  bibkey: cao-etal-2019-latent
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1181
  month: November
  page_first: '1718'
  page_last: '1728'
  pages: "1718\u20131728"
  paper_id: '181'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1181.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1181.jpg
  title: Latent Suicide Risk Detection on Microblog via Suicide-Oriented Word Embeddings
    and Layered Attention
  title_html: Latent Suicide Risk Detection on Microblog via Suicide-Oriented Word
    Embeddings and Layered Attention
  url: https://www.aclweb.org/anthology/D19-1181
  year: '2019'
D19-1182:
  abstract: Many pledges are made in the course of an election campaign, forming important
    corpora for political analysis of campaign strategy and governmental accountability.
    At present, there are no publicly available annotated datasets of pledges, and
    most political analyses rely on manual annotations. In this paper we collate a
    novel dataset of manifestos from eleven Australian federal election cycles, with
    over 12,000 sentences annotated with specificity (e.g., rhetorical vs detailed
    pledge) on a fine-grained scale. We propose deep ordinal regression approaches
    for specificity prediction, under both supervised and semi-supervised settings,
    and provide empirical results demonstrating the effectiveness of the proposed
    techniques over several baseline approaches. We analyze the utility of pledge
    specificity modeling across a spectrum of policy issues in performing ideology
    prediction, and further provide qualitative analysis in terms of capturing party-specific
    issue salience across election cycles.
  address: Hong Kong, China
  author:
  - first: Shivashankar
    full: Shivashankar Subramanian
    id: shivashankar-subramanian
    last: Subramanian
  - first: Trevor
    full: Trevor Cohn
    id: trevor-cohn
    last: Cohn
  - first: Timothy
    full: Timothy Baldwin
    id: timothy-baldwin
    last: Baldwin
  author_string: Shivashankar Subramanian, Trevor Cohn, Timothy Baldwin
  bibkey: subramanian-etal-2019-deep
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1182
  month: November
  page_first: '1729'
  page_last: '1740'
  pages: "1729\u20131740"
  paper_id: '182'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1182.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1182.jpg
  title: Deep Ordinal Regression for Pledge Specificity Prediction
  title_html: Deep Ordinal Regression for Pledge Specificity Prediction
  url: https://www.aclweb.org/anthology/D19-1182
  year: '2019'
D19-1183:
  abstract: "Goal-oriented dialogue systems are now being widely adopted in industry\
    \ where it is of key importance to maintain a rapid prototyping cycle for new\
    \ products and domains. Data-driven dialogue system development has to be adapted\
    \ to meet this requirement \u2014 therefore, reducing the amount of data and annotations\
    \ necessary for training such systems is a central research problem. In this paper,\
    \ we present the Dialogue Knowledge Transfer Network (DiKTNet), a state-of-the-art\
    \ approach to goal-oriented dialogue generation which only uses a few example\
    \ dialogues (i.e. few-shot learning), none of which has to be annotated. We achieve\
    \ this by performing a 2-stage training. Firstly, we perform unsupervised dialogue\
    \ representation pre-training on a large source of goal-oriented dialogues in\
    \ multiple domains, the MetaLWOz corpus. Secondly, at the transfer stage, we train\
    \ DiKTNet using this representation together with 2 other textual knowledge sources\
    \ with different levels of generality: ELMo encoder and the main dataset\u2019\
    s source domains. Our main dataset is the Stanford Multi-Domain dialogue corpus.\
    \ We evaluate our model on it in terms of BLEU and Entity F1 scores, and show\
    \ that our approach significantly and consistently improves upon a series of baseline\
    \ models as well as over the previous state-of-the-art dialogue generation model,\
    \ ZSDG. The improvement upon the latter \u2014 up to 10% in Entity F1 and the\
    \ average of 3% in BLEU score \u2014 is achieved using only 10% equivalent of\
    \ ZSDG\u2019s in-domain training data."
  address: Hong Kong, China
  attachment:
  - filename: D19-1183.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1183.Attachment.pdf
  author:
  - first: Igor
    full: Igor Shalyminov
    id: igor-shalyminov
    last: Shalyminov
  - first: Sungjin
    full: Sungjin Lee
    id: sungjin-lee
    last: Lee
  - first: Arash
    full: Arash Eshghi
    id: arash-eshghi
    last: Eshghi
  - first: Oliver
    full: Oliver Lemon
    id: oliver-lemon
    last: Lemon
  author_string: Igor Shalyminov, Sungjin Lee, Arash Eshghi, Oliver Lemon
  bibkey: shalyminov-etal-2019-data
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1183
  month: November
  page_first: '1741'
  page_last: '1751'
  pages: "1741\u20131751"
  paper_id: '183'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1183.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1183.jpg
  title: Data-Efficient Goal-Oriented Conversation with Dialogue Knowledge Transfer
    Networks
  title_html: Data-Efficient Goal-Oriented Conversation with Dialogue Knowledge Transfer
    Networks
  url: https://www.aclweb.org/anthology/D19-1183
  year: '2019'
D19-1184:
  abstract: Neural models of dialog rely on generalized latent representations of
    language. This paper introduces a novel training procedure which explicitly learns
    multiple representations of language at several levels of granularity. The multi-granularity
    training algorithm modifies the mechanism by which negative candidate responses
    are sampled in order to control the granularity of learned latent representations.
    Strong performance gains are observed on the next utterance retrieval task using
    both the MultiWOZ dataset and the Ubuntu dialog corpus. Analysis significantly
    demonstrates that multiple granularities of representation are being learned,
    and that multi-granularity training facilitates better transfer to downstream
    tasks.
  address: Hong Kong, China
  author:
  - first: Shikib
    full: Shikib Mehri
    id: shikib-mehri
    last: Mehri
  - first: Maxine
    full: Maxine Eskenazi
    id: maxine-eskenazi
    last: Eskenazi
  author_string: Shikib Mehri, Maxine Eskenazi
  bibkey: mehri-eskenazi-2019-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1184
  month: November
  page_first: '1752'
  page_last: '1761'
  pages: "1752\u20131761"
  paper_id: '184'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1184.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1184.jpg
  title: Multi-Granularity Representations of Dialog
  title_html: Multi-Granularity Representations of Dialog
  url: https://www.aclweb.org/anthology/D19-1184
  year: '2019'
D19-1185:
  abstract: Identity fraud detection is of great importance in many real-world scenarios
    such as the financial industry. However, few studies addressed this problem before.
    In this paper, we focus on identity fraud detection in loan applications and propose
    to solve this problem with a novel interactive dialogue system which consists
    of two modules. One is the knowledge graph (KG) constructor organizing the personal
    information for each loan applicant. The other is structured dialogue management
    that can dynamically generate a series of questions based on the personal KG to
    ask the applicants and determine their identity states. We also present a heuristic
    user simulator based on problem analysis to evaluate our method. Experiments have
    shown that the trainable dialogue system can effectively detect fraudsters, and
    achieve higher recognition accuracy compared with rule-based systems. Furthermore,
    our learned dialogue strategies are interpretable and flexible, which can help
    promote real-world applications.
  address: Hong Kong, China
  attachment:
  - filename: D19-1185.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1185.Attachment.zip
  author:
  - first: Weikang
    full: Weikang Wang
    id: weikang-wang
    last: Wang
  - first: Jiajun
    full: Jiajun Zhang
    id: jiajun-zhang
    last: Zhang
  - first: Qian
    full: Qian Li
    id: qian-li
    last: Li
  - first: Chengqing
    full: Chengqing Zong
    id: chengqing-zong
    last: Zong
  - first: Zhifei
    full: Zhifei Li
    id: zhifei-li
    last: Li
  author_string: Weikang Wang, Jiajun Zhang, Qian Li, Chengqing Zong, Zhifei Li
  bibkey: wang-etal-2019-real
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1185
  month: November
  page_first: '1762'
  page_last: '1771'
  pages: "1762\u20131771"
  paper_id: '185'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1185.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1185.jpg
  title: Are You for Real? Detecting Identity Fraud via Dialogue Interactions
  title_html: Are You for Real? Detecting Identity Fraud via Dialogue Interactions
  url: https://www.aclweb.org/anthology/D19-1185
  year: '2019'
D19-1186:
  abstract: The neural encoder-decoder models have shown great promise in neural conversation
    generation. However, they cannot perceive and express the intention effectively,
    and hence often generate dull and generic responses. Unlike past work that has
    focused on diversifying the output at word-level or discourse-level with a flat
    model to alleviate this problem, we propose a hierarchical generation model to
    capture the different levels of diversity using the conditional variational autoencoders.
    Specifically, a hierarchical response generation (HRG) framework is proposed to
    capture the conversation intention in a natural and coherent way. It has two modules,
    namely, an expression reconstruction model to capture the hierarchical correlation
    between expression and intention, and an expression attention model to effectively
    combine the expressions with contents. Finally, the training procedure of HRG
    is improved by introducing reconstruction loss. Experiment results show that our
    model can generate the responses with more appropriate content and expression.
  address: Hong Kong, China
  author:
  - first: Bo
    full: Bo Zhang
    id: bo-zhang
    last: Zhang
  - first: Xiaoming
    full: Xiaoming Zhang
    id: xiaoming-zhang
    last: Zhang
  author_string: Bo Zhang, Xiaoming Zhang
  bibkey: zhang-zhang-2019-hierarchy
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1186
  month: November
  page_first: '1772'
  page_last: '1781'
  pages: "1772\u20131781"
  paper_id: '186'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1186.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1186.jpg
  title: Hierarchy Response Learning for Neural Conversation Generation
  title_html: Hierarchy Response Learning for Neural Conversation Generation
  url: https://www.aclweb.org/anthology/D19-1186
  year: '2019'
D19-1187:
  abstract: Two types of knowledge, triples from knowledge graphs and texts from documents,
    have been studied for knowledge aware open domain conversation generation, in
    which graph paths can narrow down vertex candidates for knowledge selection decision,
    and texts can provide rich information for response generation. Fusion of a knowledge
    graph and texts might yield mutually reinforcing advantages, but there is less
    study on that. To address this challenge, we propose a knowledge aware chatting
    machine with three components, an augmented knowledge graph with both triples
    and texts, knowledge selector, and knowledge aware response generator. For knowledge
    selection on the graph, we formulate it as a problem of multi-hop graph reasoning
    to effectively capture conversation flow, which is more explainable and flexible
    in comparison with previous works. To fully leverage long text information that
    differentiates our graph from others, we improve a state of the art reasoning
    algorithm with machine reading comprehension technology. We demonstrate the effectiveness
    of our system on two datasets in comparison with state-of-the-art models.
  address: Hong Kong, China
  author:
  - first: Zhibin
    full: Zhibin Liu
    id: zhibin-liu
    last: Liu
  - first: Zheng-Yu
    full: Zheng-Yu Niu
    id: zheng-yu-niu
    last: Niu
  - first: Hua
    full: Hua Wu
    id: hua-wu
    last: Wu
  - first: Haifeng
    full: Haifeng Wang
    id: haifeng-wang
    last: Wang
  author_string: Zhibin Liu, Zheng-Yu Niu, Hua Wu, Haifeng Wang
  bibkey: liu-etal-2019-knowledge
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1187
  month: November
  page_first: '1782'
  page_last: '1792'
  pages: "1782\u20131792"
  paper_id: '187'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1187.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1187.jpg
  title: Knowledge Aware Conversation Generation with Explainable Reasoning over Augmented
    Graphs
  title_html: Knowledge Aware Conversation Generation with Explainable Reasoning over
    Augmented Graphs
  url: https://www.aclweb.org/anthology/D19-1187
  year: '2019'
D19-1188:
  abstract: 'Neural conversation systems generate responses based on the sequence-to-sequence
    (SEQ2SEQ) paradigm. Typically, the model is equipped with a single set of learned
    parameters to generate responses for given input contexts. When confronting diverse
    conversations, its adaptability is rather limited and the model is hence prone
    to generate generic responses. In this work, we propose an Adaptive Neural Dialogue
    generation model, AdaND, which manages various conversations with conversation-specific
    parameterization. For each conversation, the model generates parameters of the
    encoder-decoder by referring to the input context. In particular, we propose two
    adaptive parameterization mechanisms: a context-aware and a topic-aware parameterization
    mechanism. The context-aware parameterization directly generates the parameters
    by capturing local semantics of the given context. The topic-aware parameterization
    enables parameter sharing among conversations with similar topics by first inferring
    the latent topics of the given context and then generating the parameters with
    respect to the distributional topics. Extensive experiments conducted on a large-scale
    real-world conversational dataset show that our model achieves superior performance
    in terms of both quantitative metrics and human evaluations.'
  address: Hong Kong, China
  author:
  - first: Hengyi
    full: Hengyi Cai
    id: hengyi-cai
    last: Cai
  - first: Hongshen
    full: Hongshen Chen
    id: hongshen-chen
    last: Chen
  - first: Cheng
    full: Cheng Zhang
    id: cheng-zhang
    last: Zhang
  - first: Yonghao
    full: Yonghao Song
    id: yonghao-song
    last: Song
  - first: Xiaofang
    full: Xiaofang Zhao
    id: xiaofang-zhao
    last: Zhao
  - first: Dawei
    full: Dawei Yin
    id: dawei-yin
    last: Yin
  author_string: Hengyi Cai, Hongshen Chen, Cheng Zhang, Yonghao Song, Xiaofang Zhao,
    Dawei Yin
  bibkey: cai-etal-2019-adaptive
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1188
  month: November
  page_first: '1793'
  page_last: '1802'
  pages: "1793\u20131802"
  paper_id: '188'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1188.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1188.jpg
  title: Adaptive Parameterization for Neural Dialogue Generation
  title_html: Adaptive Parameterization for Neural Dialogue Generation
  url: https://www.aclweb.org/anthology/D19-1188
  year: '2019'
D19-1189:
  abstract: "In this paper, we propose a novel end-to-end framework called KBRD, which\
    \ stands for Knowledge-Based Recommender Dialog System. It integrates the recommender\
    \ system and the dialog generation system. The dialog generation system can enhance\
    \ the performance of the recommendation system by introducing information about\
    \ users\u2019 preferences, and the recommender system can improve that of the\
    \ dialog generation system by providing recommendation-aware vocabulary bias.\
    \ Experimental results demonstrate that our proposed model has significant advantages\
    \ over the baselines in both the evaluation of dialog generation and recommendation.\
    \ A series of analyses show that the two systems can bring mutual benefits to\
    \ each other, and the introduced knowledge contributes to both their performances."
  address: Hong Kong, China
  author:
  - first: Qibin
    full: Qibin Chen
    id: qibin-chen
    last: Chen
  - first: Junyang
    full: Junyang Lin
    id: junyang-lin
    last: Lin
  - first: Yichang
    full: Yichang Zhang
    id: yichang-zhang
    last: Zhang
  - first: Ming
    full: Ming Ding
    id: ming-ding
    last: Ding
  - first: Yukuo
    full: Yukuo Cen
    id: yukuo-cen
    last: Cen
  - first: Hongxia
    full: Hongxia Yang
    id: hongxia-yang
    last: Yang
  - first: Jie
    full: Jie Tang
    id: jie-tang
    last: Tang
  author_string: Qibin Chen, Junyang Lin, Yichang Zhang, Ming Ding, Yukuo Cen, Hongxia
    Yang, Jie Tang
  bibkey: chen-etal-2019-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1189
  month: November
  page_first: '1803'
  page_last: '1813'
  pages: "1803\u20131813"
  paper_id: '189'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1189.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1189.jpg
  title: Towards Knowledge-Based Recommender Dialog System
  title_html: Towards Knowledge-Based Recommender Dialog System
  url: https://www.aclweb.org/anthology/D19-1189
  year: '2019'
D19-1190:
  abstract: Generating responses in a targeted style is a useful yet challenging task,
    especially in the absence of parallel data. With limited data, existing methods
    tend to generate responses that are either less stylized or less context-relevant.
    We propose StyleFusion, which bridges conversation modeling and non-parallel style
    transfer by sharing a structured latent space. This structure allows the system
    to generate stylized relevant responses by sampling in the neighborhood of the
    conversation model prediction, and continuously control the style level. We demonstrate
    this method using dialogues from Reddit data and two sets of sentences with distinct
    styles (arXiv and Sherlock Holmes novels). Automatic and human evaluation show
    that, without sacrificing appropriateness, the system generates responses of the
    targeted style and outperforms competitive baselines.
  address: Hong Kong, China
  author:
  - first: Xiang
    full: Xiang Gao
    id: xiang-gao
    last: Gao
  - first: Yizhe
    full: Yizhe Zhang
    id: yizhe-zhang
    last: Zhang
  - first: Sungjin
    full: Sungjin Lee
    id: sungjin-lee
    last: Lee
  - first: Michel
    full: Michel Galley
    id: michel-galley
    last: Galley
  - first: Chris
    full: Chris Brockett
    id: chris-brockett
    last: Brockett
  - first: Jianfeng
    full: Jianfeng Gao
    id: jianfeng-gao
    last: Gao
  - first: Bill
    full: Bill Dolan
    id: bill-dolan
    last: Dolan
  author_string: Xiang Gao, Yizhe Zhang, Sungjin Lee, Michel Galley, Chris Brockett,
    Jianfeng Gao, Bill Dolan
  bibkey: gao-etal-2019-structuring
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1190
  month: November
  page_first: '1814'
  page_last: '1823'
  pages: "1814\u20131823"
  paper_id: '190'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1190.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1190.jpg
  title: Structuring Latent Spaces for Stylized Response Generation
  title_html: Structuring Latent Spaces for Stylized Response Generation
  url: https://www.aclweb.org/anthology/D19-1190
  year: '2019'
D19-1191:
  abstract: "In multi-turn dialogue, utterances do not always take the full form of\
    \ sentences. These incomplete utterances will greatly reduce the performance of\
    \ open-domain dialogue systems. Restoring more incomplete utterances from context\
    \ could potentially help the systems generate more relevant responses. To facilitate\
    \ the study of incomplete utterance restoration for open-domain dialogue systems,\
    \ a large-scale multi-turn dataset Restoration-200K is collected and manually\
    \ labeled with the explicit relation between an utterance and its context. We\
    \ also propose a \u201Cpick-and-combine\u201D model to restore the incomplete\
    \ utterance from its context. Experimental results demonstrate that the annotated\
    \ dataset and the proposed approach significantly boost the response quality of\
    \ both single-turn and multi-turn dialogue systems."
  address: Hong Kong, China
  author:
  - first: Zhufeng
    full: Zhufeng Pan
    id: zhufeng-pan
    last: Pan
  - first: Kun
    full: Kun Bai
    id: kun-bai
    last: Bai
  - first: Yan
    full: Yan Wang
    id: yan-wang
    last: Wang
  - first: Lianqiang
    full: Lianqiang Zhou
    id: lianqiang-zhou
    last: Zhou
  - first: Xiaojiang
    full: Xiaojiang Liu
    id: xiaojiang-liu
    last: Liu
  author_string: Zhufeng Pan, Kun Bai, Yan Wang, Lianqiang Zhou, Xiaojiang Liu
  bibkey: pan-etal-2019-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1191
  month: November
  page_first: '1824'
  page_last: '1833'
  pages: "1824\u20131833"
  paper_id: '191'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1191.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1191.jpg
  title: Improving Open-Domain Dialogue Systems via Multi-Turn Incomplete Utterance
    Restoration
  title_html: Improving Open-Domain Dialogue Systems via Multi-Turn Incomplete Utterance
    Restoration
  url: https://www.aclweb.org/anthology/D19-1191
  year: '2019'
D19-1192:
  abstract: Context modeling has a pivotal role in open domain conversation. Existing
    works either use heuristic methods or jointly learn context modeling and response
    generation with an encoder-decoder framework. This paper proposes an explicit
    context rewriting method, which rewrites the last utterance by considering context
    history. We leverage pseudo-parallel data and elaborate a context rewriting network,
    which is built upon the CopyNet with the reinforcement learning method. The rewritten
    utterance is beneficial to candidate retrieval, explainable context modeling,
    as well as enabling to employ a single-turn framework to the multi-turn scenario.
    The empirical results show that our model outperforms baselines in terms of the
    rewriting quality, the multi-turn response generation, and the end-to-end retrieval-based
    chatbots.
  address: Hong Kong, China
  attachment:
  - filename: D19-1192.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1192.Attachment.zip
  author:
  - first: Kun
    full: Kun Zhou
    id: kun-zhou
    last: Zhou
  - first: Kai
    full: Kai Zhang
    id: kai-zhang
    last: Zhang
  - first: Yu
    full: Yu Wu
    id: yu-wu
    last: Wu
  - first: Shujie
    full: Shujie Liu
    id: shujie-liu
    last: Liu
  - first: Jingsong
    full: Jingsong Yu
    id: jingsong-yu
    last: Yu
  author_string: Kun Zhou, Kai Zhang, Yu Wu, Shujie Liu, Jingsong Yu
  bibkey: zhou-etal-2019-unsupervised
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1192
  month: November
  page_first: '1834'
  page_last: '1844'
  pages: "1834\u20131844"
  paper_id: '192'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1192.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1192.jpg
  title: Unsupervised Context Rewriting for Open Domain Conversation
  title_html: Unsupervised Context Rewriting for Open Domain Conversation
  url: https://www.aclweb.org/anthology/D19-1192
  year: '2019'
D19-1193:
  abstract: This paper proposes a dually interactive matching network (DIM) for presenting
    the personalities of dialogue agents in retrieval-based chatbots. This model develops
    from the interactive matching network (IMN) which models the matching degree between
    a context composed of multiple utterances and a response candidate. Compared with
    previous persona fusion approach which enhances the representation of a context
    by calculating its similarity with a given persona, the DIM model adopts a dual
    matching architecture, which performs interactive matching between responses and
    contexts and between responses and personas respectively for ranking response
    candidates. Experimental results on PERSONA-CHAT dataset show that the DIM model
    outperforms its baseline model, i.e., IMN with persona fusion, by a margin of
    14.5% and outperforms the present state-of-the-art model by a margin of 27.7%
    in terms of top-1 accuracy hits@1.
  address: Hong Kong, China
  author:
  - first: Jia-Chen
    full: Jia-Chen Gu
    id: jia-chen-gu
    last: Gu
  - first: Zhen-Hua
    full: Zhen-Hua Ling
    id: zhen-hua-ling
    last: Ling
  - first: Xiaodan
    full: Xiaodan Zhu
    id: xiaodan-zhu
    last: Zhu
  - first: Quan
    full: Quan Liu
    id: quan-liu
    last: Liu
  author_string: Jia-Chen Gu, Zhen-Hua Ling, Xiaodan Zhu, Quan Liu
  bibkey: gu-etal-2019-dually
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1193
  month: November
  page_first: '1845'
  page_last: '1854'
  pages: "1845\u20131854"
  paper_id: '193'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1193.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1193.jpg
  title: Dually Interactive Matching Network for Personalized Response Selection in
    Retrieval-Based Chatbots
  title_html: Dually Interactive Matching Network for Personalized Response Selection
    in Retrieval-Based Chatbots
  url: https://www.aclweb.org/anthology/D19-1193
  year: '2019'
D19-1194:
  abstract: 'Data-driven, knowledge-grounded neural conversation models are capable
    of generating more informative responses. However, these models have not yet demonstrated
    that they can zero-shot adapt to updated, unseen knowledge graphs. This paper
    proposes a new task about how to apply dynamic knowledge graphs in neural conversation
    model and presents a novel TV series conversation corpus (DyKgChat) for the task.
    Our new task and corpus aids in understanding the influence of dynamic knowledge
    graphs on responses generation. Also, we propose a preliminary model that selects
    an output from two networks at each time step: a sequence-to-sequence model (Seq2Seq)
    and a multi-hop reasoning model, in order to support dynamic knowledge graphs.
    To benchmark this new task and evaluate the capability of adaptation, we introduce
    several evaluation metrics and the experiments show that our proposed approach
    outperforms previous knowledge-grounded conversation models. The proposed corpus
    and model can motivate the future research directions.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1194.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1194.Attachment.pdf
  author:
  - first: Yi-Lin
    full: Yi-Lin Tuan
    id: yi-lin-tuan
    last: Tuan
  - first: Yun-Nung
    full: Yun-Nung Chen
    id: yun-nung-chen
    last: Chen
  - first: Hung-yi
    full: Hung-yi Lee
    id: hung-yi-lee1
    last: Lee
  author_string: Yi-Lin Tuan, Yun-Nung Chen, Hung-yi Lee
  bibkey: tuan-etal-2019-dykgchat
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1194
  month: November
  page_first: '1855'
  page_last: '1865'
  pages: "1855\u20131865"
  paper_id: '194'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1194.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1194.jpg
  title: 'DyKgChat: Benchmarking Dialogue Generation Grounding on Dynamic Knowledge
    Graphs'
  title_html: '<span class="acl-fixed-case">D</span>y<span class="acl-fixed-case">K</span>g<span
    class="acl-fixed-case">C</span>hat: Benchmarking Dialogue Generation Grounding
    on Dynamic Knowledge Graphs'
  url: https://www.aclweb.org/anthology/D19-1194
  year: '2019'
D19-1195:
  abstract: End-to-end sequence generation is a popular technique for developing open
    domain dialogue systems, though they suffer from the safe response problem. Researchers
    have attempted to tackle this problem by incorporating generative models with
    the returns of retrieval systems. Recently, a skeleton-then-response framework
    has been shown promising results for this task. Nevertheless, how to precisely
    extract a skeleton and how to effectively train a retrieval-guided response generator
    are still challenging. This paper presents a novel framework in which the skeleton
    extraction is made by an interpretable matching model and the following skeleton-guided
    response generation is accomplished by a separately trained generator. Extensive
    experiments demonstrate the effectiveness of our model designs.
  address: Hong Kong, China
  author:
  - first: Deng
    full: Deng Cai
    id: deng-cai
    last: Cai
  - first: Yan
    full: Yan Wang
    id: yan-wang
    last: Wang
  - first: Wei
    full: Wei Bi
    id: wei-bi
    last: Bi
  - first: Zhaopeng
    full: Zhaopeng Tu
    id: zhaopeng-tu
    last: Tu
  - first: Xiaojiang
    full: Xiaojiang Liu
    id: xiaojiang-liu
    last: Liu
  - first: Shuming
    full: Shuming Shi
    id: shuming-shi
    last: Shi
  author_string: Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, Shuming Shi
  bibkey: cai-etal-2019-retrieval
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1195
  month: November
  page_first: '1866'
  page_last: '1875'
  pages: "1866\u20131875"
  paper_id: '195'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1195.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1195.jpg
  title: Retrieval-guided Dialogue Response Generation via a Matching-to-Generation
    Framework
  title_html: Retrieval-guided Dialogue Response Generation via a Matching-to-Generation
    Framework
  url: https://www.aclweb.org/anthology/D19-1195
  year: '2019'
D19-1196:
  abstract: Existing approaches to dialogue state tracking rely on pre-defined ontologies
    consisting of a set of all possible slot types and values. Though such approaches
    exhibit promising performance on single-domain benchmarks, they suffer from computational
    complexity that increases proportionally to the number of pre-defined slots that
    need tracking. This issue becomes more severe when it comes to multi-domain dialogues
    which include larger numbers of slots. In this paper, we investigate how to approach
    DST using a generation framework without the pre-defined ontology list. Given
    each turn of user utterance and system response, we directly generate a sequence
    of belief states by applying a hierarchical encoder-decoder structure. In this
    way, the computational complexity of our model will be a constant regardless of
    the number of pre-defined slots. Experiments on both the multi-domain and the
    single domain dialogue state tracking dataset show that our model not only scales
    easily with the increasing number of pre-defined domains and slots but also reaches
    the state-of-the-art performance.
  address: Hong Kong, China
  author:
  - first: Liliang
    full: Liliang Ren
    id: liliang-ren
    last: Ren
  - first: Jianmo
    full: Jianmo Ni
    id: jianmo-ni
    last: Ni
  - first: Julian
    full: Julian McAuley
    id: julian-mcauley
    last: McAuley
  author_string: Liliang Ren, Jianmo Ni, Julian McAuley
  bibkey: ren-etal-2019-scalable
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1196
  month: November
  page_first: '1876'
  page_last: '1885'
  pages: "1876\u20131885"
  paper_id: '196'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1196.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1196.jpg
  title: Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation
  title_html: Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence
    Generation
  url: https://www.aclweb.org/anthology/D19-1196
  year: '2019'
D19-1197:
  abstract: We study open domain response generation with limited message-response
    pairs. The problem exists in real-world applications but is less explored by the
    existing work. Since the paired data now is no longer enough to train a neural
    generation model, we consider leveraging the large scale of unpaired data that
    are much easier to obtain, and propose response generation with both paired and
    unpaired data. The generation model is defined by an encoder-decoder architecture
    with templates as prior, where the templates are estimated from the unpaired data
    as a neural hidden semi-markov model. By this means, response generation learned
    from the small paired data can be aided by the semantic and syntactic knowledge
    in the large unpaired data. To balance the effect of the prior and the input message
    to response generation, we propose learning the whole generation model with an
    adversarial approach. Empirical studies on question response generation and sentiment
    response generation indicate that when only a few pairs are available, our model
    can significantly outperform several state-of-the-art response generation models
    in terms of both automatic and human evaluation.
  address: Hong Kong, China
  author:
  - first: Ze
    full: Ze Yang
    id: ze-yang
    last: Yang
  - first: Wei
    full: Wei Wu
    id: wei-wu
    last: Wu
  - first: Jian
    full: Jian Yang
    id: jian-yang
    last: Yang
  - first: Can
    full: Can Xu
    id: can-xu
    last: Xu
  - first: Zhoujun
    full: Zhoujun Li
    id: zhoujun-li
    last: Li
  author_string: Ze Yang, Wei Wu, Jian Yang, Can Xu, Zhoujun Li
  bibkey: yang-etal-2019-low
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1197
  month: November
  page_first: '1886'
  page_last: '1897'
  pages: "1886\u20131897"
  paper_id: '197'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1197.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1197.jpg
  title: Low-Resource Response Generation with Template Prior
  title_html: Low-Resource Response Generation with Template Prior
  url: https://www.aclweb.org/anthology/D19-1197
  year: '2019'
D19-1198:
  abstract: Neural conversation models such as encoder-decoder models are easy to
    generate bland and generic responses. Some researchers propose to use the conditional
    variational autoencoder (CVAE) which maximizes the lower bound on the conditional
    log-likelihood on a continuous latent variable. With different sampled latent
    variables, the model is expected to generate diverse responses. Although the CVAE-based
    models have shown tremendous potential, their improvement of generating high-quality
    responses is still unsatisfactory. In this paper, we introduce a discrete latent
    variable with an explicit semantic meaning to improve the CVAE on short-text conversation.
    A major advantage of our model is that we can exploit the semantic distance between
    the latent variables to maintain good diversity between the sampled latent variables.
    Accordingly, we propose a two-stage sampling approach to enable efficient diverse
    variable selection from a large latent space assumed in the short-text conversation
    task. Experimental results indicate that our model outperforms various kinds of
    generation models under both automatic and human evaluations and generates more
    diverse and informative responses.
  address: Hong Kong, China
  author:
  - first: Jun
    full: Jun Gao
    id: jun-gao
    last: Gao
  - first: Wei
    full: Wei Bi
    id: wei-bi
    last: Bi
  - first: Xiaojiang
    full: Xiaojiang Liu
    id: xiaojiang-liu
    last: Liu
  - first: Junhui
    full: Junhui Li
    id: junhui-li
    last: Li
  - first: Guodong
    full: Guodong Zhou
    id: guodong-zhou
    last: Zhou
  - first: Shuming
    full: Shuming Shi
    id: shuming-shi
    last: Shi
  author_string: Jun Gao, Wei Bi, Xiaojiang Liu, Junhui Li, Guodong Zhou, Shuming
    Shi
  bibkey: gao-etal-2019-discrete
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1198
  month: November
  page_first: '1898'
  page_last: '1908'
  pages: "1898\u20131908"
  paper_id: '198'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1198.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1198.jpg
  title: A Discrete CVAE for Response Generation on Short-Text Conversation
  title_html: A Discrete <span class="acl-fixed-case">CVAE</span> for Response Generation
    on Short-Text Conversation
  url: https://www.aclweb.org/anthology/D19-1198
  year: '2019'
D19-1199:
  abstract: Previous research on dialogue systems generally focuses on the conversation
    between two participants, yet multi-party conversations which involve more than
    two participants within one session bring up a more complicated but realistic
    scenario. In real multi- party conversations, we can observe who is speaking,
    but the addressee information is not always explicit. In this paper, we aim to
    tackle the challenge of identifying all the miss- ing addressees in a conversation
    session. To this end, we introduce a novel who-to-whom (W2W) model which models
    users and utterances in the session jointly in an interactive way. We conduct
    experiments on the benchmark Ubuntu Multi-Party Conversation Corpus and the experimental
    results demonstrate that our model outperforms baselines with consistent improvements.
  address: Hong Kong, China
  attachment:
  - filename: D19-1199.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1199.Attachment.pdf
  author:
  - first: Ran
    full: Ran Le
    id: ran-le
    last: Le
  - first: Wenpeng
    full: Wenpeng Hu
    id: wenpeng-hu
    last: Hu
  - first: Mingyue
    full: Mingyue Shang
    id: mingyue-shang
    last: Shang
  - first: Zhenjun
    full: Zhenjun You
    id: zhenjun-you
    last: You
  - first: Lidong
    full: Lidong Bing
    id: lidong-bing
    last: Bing
  - first: Dongyan
    full: Dongyan Zhao
    id: dongyan-zhao
    last: Zhao
  - first: Rui
    full: Rui Yan
    id: rui-yan
    last: Yan
  author_string: Ran Le, Wenpeng Hu, Mingyue Shang, Zhenjun You, Lidong Bing, Dongyan
    Zhao, Rui Yan
  bibkey: le-etal-2019-speaking
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1199
  month: November
  page_first: '1909'
  page_last: '1919'
  pages: "1909\u20131919"
  paper_id: '199'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1199.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1199.jpg
  title: Who Is Speaking to Whom? Learning to Identify Utterance Addressee in Multi-Party
    Conversations
  title_html: Who Is Speaking to Whom? Learning to Identify Utterance Addressee in
    Multi-Party Conversations
  url: https://www.aclweb.org/anthology/D19-1199
  year: '2019'
D19-1200:
  abstract: Neural sequence-to-sequence models for dialog systems suffer from the
    problem of favoring uninformative and non replier-specific responses due to lack
    of the global and relevant information guidance. The existing methods model the
    generation process by leveraging the neural variational network with simple Gaussian.
    However, the sampled information from latent space usually becomes useless due
    to the KL divergence vanishing issue, and the highly abstractive global variables
    easily dilute the personal features of replier, leading to a non replier-specific
    response. Therefore, a novel Semi-Supervised Stable Variational Network (SSVN)
    is proposed to address these issues. We use a unit hypersperical distribution,
    namely the von Mises-Fisher (vMF), as the latent space of a semi-supervised model,
    which can obtain the stable KL performance by setting a fixed variance and hence
    enhance the global information representation. Meanwhile, an unsupervised extractor
    is introduced to automatically distill the replier-tailored feature which is then
    injected into a supervised generator to encourage the replier-consistency. Experimental
    results on two large conversation datasets show that our model outperforms the
    competitive baseline models significantly, and can generate diverse and replier-specific
    responses.
  address: Hong Kong, China
  author:
  - first: Jinxin
    full: Jinxin Chang
    id: jinxin-chang
    last: Chang
  - first: Ruifang
    full: Ruifang He
    id: ruifang-he
    last: He
  - first: Longbiao
    full: Longbiao Wang
    id: longbiao-wang
    last: Wang
  - first: Xiangyu
    full: Xiangyu Zhao
    id: xiangyu-zhao
    last: Zhao
  - first: Ting
    full: Ting Yang
    id: ting-yang
    last: Yang
  - first: Ruifang
    full: Ruifang Wang
    id: ruifang-wang
    last: Wang
  author_string: Jinxin Chang, Ruifang He, Longbiao Wang, Xiangyu Zhao, Ting Yang,
    Ruifang Wang
  bibkey: chang-etal-2019-semi
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1200
  month: November
  page_first: '1920'
  page_last: '1930'
  pages: "1920\u20131930"
  paper_id: '200'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1200.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1200.jpg
  title: A Semi-Supervised Stable Variational Network for Promoting Replier-Consistency
    in Dialogue Generation
  title_html: A Semi-Supervised Stable Variational Network for Promoting Replier-Consistency
    in Dialogue Generation
  url: https://www.aclweb.org/anthology/D19-1200
  year: '2019'
D19-1201:
  abstract: Variational autoencoders (VAEs) and Wasserstein autoencoders (WAEs) have
    achieved noticeable progress in open-domain response generation. Through introducing
    latent variables in continuous space, these models are capable of capturing utterance-level
    semantics, e.g., topic, syntactic properties, and thus can generate informative
    and diversified responses. In this work, we improve the WAE for response generation.
    In addition to the utterance-level information, we also model user-level information
    in latent continue space. Specifically, we embed user-level and utterance-level
    information into two multimodal distributions, and combine these two multimodal
    distributions into a mixed distribution. This mixed distribution will be used
    as the prior distribution of WAE in our proposed model, named as PersonaWAE. Experimental
    results on a large-scale real-world dataset confirm the superiority of our model
    for generating informative and personalized responses, where both automatic and
    human evaluations outperform state-of-the-art models.
  address: Hong Kong, China
  author:
  - first: Zhangming
    full: Zhangming Chan
    id: zhangming-chan
    last: Chan
  - first: Juntao
    full: Juntao Li
    id: juntao-li
    last: Li
  - first: Xiaopeng
    full: Xiaopeng Yang
    id: xiaopeng-yang
    last: Yang
  - first: Xiuying
    full: Xiuying Chen
    id: xiuying-chen
    last: Chen
  - first: Wenpeng
    full: Wenpeng Hu
    id: wenpeng-hu
    last: Hu
  - first: Dongyan
    full: Dongyan Zhao
    id: dongyan-zhao
    last: Zhao
  - first: Rui
    full: Rui Yan
    id: rui-yan
    last: Yan
  author_string: Zhangming Chan, Juntao Li, Xiaopeng Yang, Xiuying Chen, Wenpeng Hu,
    Dongyan Zhao, Rui Yan
  bibkey: chan-etal-2019-modeling
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1201
  month: November
  page_first: '1931'
  page_last: '1940'
  pages: "1931\u20131940"
  paper_id: '201'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1201.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1201.jpg
  title: Modeling Personalization in Continuous Space for Response Generation via
    Augmented Wasserstein Autoencoders
  title_html: Modeling Personalization in Continuous Space for Response Generation
    via Augmented <span class="acl-fixed-case">W</span>asserstein Autoencoders
  url: https://www.aclweb.org/anthology/D19-1201
  year: '2019'
D19-1202:
  abstract: Generating appropriate conversation responses requires careful modeling
    of the utterances and speakers together. Some recent approaches to response generation
    model both the utterances and the speakers, but these approaches tend to generate
    responses that are overly tailored to the speakers. To overcome this limitation,
    we propose a new model with a stochastic variable designed to capture the speaker
    information and deliver it to the conversational context. An important part of
    this model is the network of speakers in which each speaker is connected to one
    or more conversational partner, and this network is then used to model the speakers
    better. To test whether our model generates more appropriate conversation responses,
    we build a new conversation corpus containing approximately 27,000 speakers and
    770,000 conversations. With this corpus, we run experiments of generating conversational
    responses and compare our model with other state-of-the-art models. By automatic
    evaluation metrics and human evaluation, we show that our model outperforms other
    models in generating appropriate responses. An additional advantage of our model
    is that it generates better responses for various new user scenarios, for example
    when one of the speakers is a known user in our corpus but the partner is a new
    user. For replicability, we make available all our code and data.
  address: Hong Kong, China
  attachment:
  - filename: D19-1202.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1202.Attachment.pdf
  - filename: D19-1202.Poster.pdf
    type: poster
    url: https://www.aclweb.org/anthology/attachments/D19-1202.Poster.pdf
  author:
  - first: JinYeong
    full: JinYeong Bak
    id: jinyeong-bak
    last: Bak
  - first: Alice
    full: Alice Oh
    id: alice-oh
    last: Oh
  author_string: JinYeong Bak, Alice Oh
  bibkey: bak-oh-2019-variational
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1202
  month: November
  page_first: '1941'
  page_last: '1950'
  pages: "1941\u20131950"
  paper_id: '202'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1202.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1202.jpg
  title: Variational Hierarchical User-based Conversation Model
  title_html: Variational Hierarchical User-based Conversation Model
  url: https://www.aclweb.org/anthology/D19-1202
  year: '2019'
D19-1203:
  abstract: "Traditional recommendation systems produce static rather than interactive\
    \ recommendations invariant to a user\u2019s specific requests, clarifications,\
    \ or current mood, and can suffer from the cold-start problem if their tastes\
    \ are unknown. These issues can be alleviated by treating recommendation as an\
    \ interactive dialogue task instead, where an expert recommender can sequentially\
    \ ask about someone\u2019s preferences, react to their requests, and recommend\
    \ more appropriate items. In this work, we collect a goal-driven recommendation\
    \ dialogue dataset (GoRecDial), which consists of 9,125 dialogue games and 81,260\
    \ conversation turns between pairs of human workers recommending movies to each\
    \ other. The task is specifically designed as a cooperative game between two players\
    \ working towards a quantifiable common goal. We leverage the dataset to develop\
    \ an end-to-end dialogue system that can simultaneously converse and recommend.\
    \ Models are first trained to imitate the behavior of human players without considering\
    \ the task goal itself (supervised training). We then finetune our models on simulated\
    \ bot-bot conversations between two paired pre-trained models (bot-play), in order\
    \ to achieve the dialogue goal. Our experiments show that models finetuned with\
    \ bot-play learn improved dialogue strategies, reach the dialogue goal more often\
    \ when paired with a human, and are rated as more consistent by humans compared\
    \ to models trained without bot-play. The dataset and code are publicly available\
    \ through the ParlAI framework."
  address: Hong Kong, China
  attachment:
  - filename: D19-1203.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1203.Attachment.pdf
  author:
  - first: Dongyeop
    full: Dongyeop Kang
    id: dongyeop-kang
    last: Kang
  - first: Anusha
    full: Anusha Balakrishnan
    id: anusha-balakrishnan
    last: Balakrishnan
  - first: Pararth
    full: Pararth Shah
    id: pararth-shah
    last: Shah
  - first: Paul
    full: Paul Crook
    id: paul-a-crook
    last: Crook
  - first: Y-Lan
    full: Y-Lan Boureau
    id: y-lan-boureau
    last: Boureau
  - first: Jason
    full: Jason Weston
    id: jason-weston
    last: Weston
  author_string: Dongyeop Kang, Anusha Balakrishnan, Pararth Shah, Paul Crook, Y-Lan
    Boureau, Jason Weston
  bibkey: kang-etal-2019-recommendation
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1203
  month: November
  page_first: '1951'
  page_last: '1961'
  pages: "1951\u20131961"
  paper_id: '203'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1203.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1203.jpg
  title: 'Recommendation as a Communication Game: Self-Supervised Bot-Play for Goal-oriented
    Dialogue'
  title_html: 'Recommendation as a Communication Game: Self-Supervised Bot-Play for
    Goal-oriented Dialogue'
  url: https://www.aclweb.org/anthology/D19-1203
  year: '2019'
D19-1204:
  abstract: 'We present CoSQL, a corpus for building cross-domain, general-purpose
    database (DB) querying dialogue systems. It consists of 30k+ turns plus 10k+ annotated
    SQL queries, obtained from a Wizard-of-Oz (WOZ) collection of 3k dialogues querying
    200 complex DBs spanning 138 domains. Each dialogue simulates a real-world DB
    query scenario with a crowd worker as a user exploring the DB and a SQL expert
    retrieving answers with SQL, clarifying ambiguous questions, or otherwise informing
    of unanswerable questions. When user questions are answerable by SQL, the expert
    describes the SQL and execution results to the user, hence maintaining a natural
    interaction flow. CoSQL introduces new challenges compared to existing task-oriented
    dialogue datasets: (1) the dialogue states are grounded in SQL, a domain-independent
    executable representation, instead of domain-specific slot value pairs, and (2)
    because testing is done on unseen databases, success requires generalizing to
    new domains. CoSQL includes three tasks: SQL-grounded dialogue state tracking,
    response generation from query results, and user dialogue act prediction. We evaluate
    a set of strong baselines for each task and show that CoSQL presents significant
    challenges for future research. The dataset, baselines, and leaderboard will be
    released at https://yale-lily.github.io/cosql.'
  address: Hong Kong, China
  author:
  - first: Tao
    full: Tao Yu
    id: tao-yu
    last: Yu
  - first: Rui
    full: Rui Zhang
    id: rui-zhang
    last: Zhang
  - first: Heyang
    full: Heyang Er
    id: heyang-er
    last: Er
  - first: Suyi
    full: Suyi Li
    id: suyi-li
    last: Li
  - first: Eric
    full: Eric Xue
    id: eric-xue
    last: Xue
  - first: Bo
    full: Bo Pang
    id: bo-pang
    last: Pang
  - first: Xi Victoria
    full: Xi Victoria Lin
    id: xi-victoria-lin
    last: Lin
  - first: Yi Chern
    full: Yi Chern Tan
    id: yi-chern-tan
    last: Tan
  - first: Tianze
    full: Tianze Shi
    id: tianze-shi
    last: Shi
  - first: Zihan
    full: Zihan Li
    id: zihan-li
    last: Li
  - first: Youxuan
    full: Youxuan Jiang
    id: youxuan-jiang
    last: Jiang
  - first: Michihiro
    full: Michihiro Yasunaga
    id: michihiro-yasunaga
    last: Yasunaga
  - first: Sungrok
    full: Sungrok Shim
    id: sungrok-shim
    last: Shim
  - first: Tao
    full: Tao Chen
    id: tao-chen
    last: Chen
  - first: Alexander
    full: Alexander Fabbri
    id: alexander-richard-fabbri
    last: Fabbri
  - first: Zifan
    full: Zifan Li
    id: zifan-li
    last: Li
  - first: Luyao
    full: Luyao Chen
    id: luyao-chen
    last: Chen
  - first: Yuwen
    full: Yuwen Zhang
    id: yuwen-zhang
    last: Zhang
  - first: Shreya
    full: Shreya Dixit
    id: shreya-dixit
    last: Dixit
  - first: Vincent
    full: Vincent Zhang
    id: vincent-zhang
    last: Zhang
  - first: Caiming
    full: Caiming Xiong
    id: caiming-xiong
    last: Xiong
  - first: Richard
    full: Richard Socher
    id: richard-socher
    last: Socher
  - first: Walter
    full: Walter Lasecki
    id: walter-lasecki
    last: Lasecki
  - first: Dragomir
    full: Dragomir Radev
    id: dragomir-radev
    last: Radev
  author_string: Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria
    Lin, Yi Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga, Sungrok
    Shim, Tao Chen, Alexander Fabbri, Zifan Li, Luyao Chen, Yuwen Zhang, Shreya Dixit,
    Vincent Zhang, Caiming Xiong, Richard Socher, Walter Lasecki, Dragomir Radev
  bibkey: yu-etal-2019-cosql
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1204
  month: November
  page_first: '1962'
  page_last: '1979'
  pages: "1962\u20131979"
  paper_id: '204'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1204.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1204.jpg
  title: 'CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural
    Language Interfaces to Databases'
  title_html: '<span class="acl-fixed-case">C</span>o<span class="acl-fixed-case">SQL</span>:
    A Conversational Text-to-<span class="acl-fixed-case">SQL</span> Challenge Towards
    Cross-Domain Natural Language Interfaces to Databases'
  url: https://www.aclweb.org/anthology/D19-1204
  year: '2019'
D19-1205:
  abstract: Dialogue Acts play an important role in conversation modeling. Research
    has shown the utility of dialogue acts for the response selection task, however,
    the underlying assumption is that the dialogue acts are readily available, which
    is impractical, as dialogue acts are rarely available for new conversations. This
    paper proposes an end-to-end multi-task model for conversation modeling, which
    is optimized for two tasks, dialogue act prediction and response selection, with
    the latter being the task of interest. It proposes a novel way of combining the
    predicted dialogue acts of context and response with the context (previous utterances)
    and response (follow-up utterance) in a crossway fashion, such that, it achieves
    at par performance for the response selection task compared to the model that
    uses actual dialogue acts. Through experiments on two well known datasets, we
    demonstrate that the multi-task model not only improves the accuracy of the dialogue
    act prediction task but also improves the MRR for the response selection task.
    Also, the cross-stitching of dialogue acts of context and response with the context
    and response is better than using either one of them individually.
  address: Hong Kong, China
  author:
  - first: Harshit
    full: Harshit Kumar
    id: harshit-kumar
    last: Kumar
  - first: Arvind
    full: Arvind Agarwal
    id: arvind-agarwal
    last: Agarwal
  - first: Sachindra
    full: Sachindra Joshi
    id: sachindra-joshi
    last: Joshi
  author_string: Harshit Kumar, Arvind Agarwal, Sachindra Joshi
  bibkey: kumar-etal-2019-practical
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1205
  month: November
  page_first: '1980'
  page_last: '1989'
  pages: "1980\u20131989"
  paper_id: '205'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1205.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1205.jpg
  title: A Practical Dialogue-Act-Driven Conversation Model for Multi-Turn Response
    Selection
  title_html: A Practical Dialogue-Act-Driven Conversation Model for Multi-Turn Response
    Selection
  url: https://www.aclweb.org/anthology/D19-1205
  year: '2019'
D19-1206:
  abstract: User simulators are essential for training reinforcement learning (RL)
    based dialog models. The performance of the simulator directly impacts the RL
    policy. However, building a good user simulator that models real user behaviors
    is challenging. We propose a method of standardizing user simulator building that
    can be used by the community to compare dialog system quality using the same set
    of user simulators fairly. We present implementations of six user simulators trained
    with different dialog planning and generation methods. We then calculate a set
    of automatic metrics to evaluate the quality of these simulators both directly
    and indirectly. We also ask human users to assess the simulators directly and
    indirectly by rating the simulated dialogs and interacting with the trained systems.
    This paper presents a comprehensive evaluation framework for user simulator study
    and provides a better understanding of the pros and cons of different user simulators,
    as well as their impacts on the trained systems.
  address: Hong Kong, China
  attachment:
  - filename: D19-1206.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1206.Attachment.pdf
  author:
  - first: Weiyan
    full: Weiyan Shi
    id: weiyan-shi
    last: Shi
  - first: Kun
    full: Kun Qian
    id: kun-qian
    last: Qian
  - first: Xuewei
    full: Xuewei Wang
    id: xuewei-wang
    last: Wang
  - first: Zhou
    full: Zhou Yu
    id: zhou-yu
    last: Yu
  author_string: Weiyan Shi, Kun Qian, Xuewei Wang, Zhou Yu
  bibkey: shi-etal-2019-build
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1206
  month: November
  page_first: '1990'
  page_last: '2000'
  pages: "1990\u20132000"
  paper_id: '206'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1206.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1206.jpg
  title: How to Build User Simulators to Train RL-based Dialog Systems
  title_html: How to Build User Simulators to Train <span class="acl-fixed-case">RL</span>-based
    Dialog Systems
  url: https://www.aclweb.org/anthology/D19-1206
  year: '2019'
D19-1207:
  abstract: This paper addresses the challenging task of video captioning which aims
    to generate descriptions for video data. Recently, the attention-based encoder-decoder
    structures have been widely used in video captioning. In existing literature,
    the attention weights are often built from the information of an individual modality,
    while, the association relationships between multiple modalities are neglected.
    Motivated by this, we propose a video captioning model with High-Order Cross-Modal
    Attention (HOCA) where the attention weights are calculated based on the high-order
    correlation tensor to capture the frame-level cross-modal interaction of different
    modalities sufficiently. Furthermore, we novelly introduce Low-Rank HOCA which
    adopts tensor decomposition to reduce the extremely large space requirement of
    HOCA, leading to a practical and efficient implementation in real-world applications.
    Experimental results on two benchmark datasets, MSVD and MSR-VTT, show that Low-rank
    HOCA establishes a new state-of-the-art.
  address: Hong Kong, China
  attachment:
  - filename: D19-1207.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1207.Attachment.zip
  author:
  - first: Tao
    full: Tao Jin
    id: tao-jin
    last: Jin
  - first: Siyu
    full: Siyu Huang
    id: siyu-huang
    last: Huang
  - first: Yingming
    full: Yingming Li
    id: yingming-li
    last: Li
  - first: Zhongfei
    full: Zhongfei Zhang
    id: zhongfei-zhang
    last: Zhang
  author_string: Tao Jin, Siyu Huang, Yingming Li, Zhongfei Zhang
  bibkey: jin-etal-2019-low
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1207
  month: November
  page_first: '2001'
  page_last: '2011'
  pages: "2001\u20132011"
  paper_id: '207'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1207.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1207.jpg
  title: 'Low-Rank HOCA: Efficient High-Order Cross-Modal Attention for Video Captioning'
  title_html: 'Low-Rank <span class="acl-fixed-case">HOCA</span>: Efficient High-Order
    Cross-Modal Attention for Video Captioning'
  url: https://www.aclweb.org/anthology/D19-1207
  year: '2019'
D19-1208:
  abstract: Constructing an organized dataset comprised of a large number of images
    and several captions for each image is a laborious task, which requires vast human
    effort. On the other hand, collecting a large number of images and sentences separately
    may be immensely easier. In this paper, we develop a novel data-efficient semi-supervised
    framework for training an image captioning model. We leverage massive unpaired
    image and caption data by learning to associate them. To this end, our proposed
    semi-supervised learning method assigns pseudo-labels to unpaired samples via
    Generative Adversarial Networks to learn the joint distribution of image and caption.
    To evaluate, we construct scarcely-paired COCO dataset, a modified version of
    MS COCO caption dataset. The empirical results show the effectiveness of our method
    compared to several strong baselines, especially when the amount of the paired
    samples are scarce.
  address: Hong Kong, China
  author:
  - first: Dong-Jin
    full: Dong-Jin Kim
    id: dong-jin-kim
    last: Kim
  - first: Jinsoo
    full: Jinsoo Choi
    id: jinsoo-choi
    last: Choi
  - first: Tae-Hyun
    full: Tae-Hyun Oh
    id: tae-hyun-oh
    last: Oh
  - first: In So
    full: In So Kweon
    id: in-so-kweon
    last: Kweon
  author_string: Dong-Jin Kim, Jinsoo Choi, Tae-Hyun Oh, In So Kweon
  bibkey: kim-etal-2019-image
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1208
  month: November
  page_first: '2012'
  page_last: '2023'
  pages: "2012\u20132023"
  paper_id: '208'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1208.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1208.jpg
  title: 'Image Captioning with Very Scarce Supervised Data: Adversarial Semi-Supervised
    Learning Approach'
  title_html: 'Image Captioning with Very Scarce Supervised Data: Adversarial Semi-Supervised
    Learning Approach'
  url: https://www.aclweb.org/anthology/D19-1208
  year: '2019'
D19-1209:
  abstract: Visual dialog (VisDial) is a task which requires a dialog agent to answer
    a series of questions grounded in an image. Unlike in visual question answering
    (VQA), the series of questions should be able to capture a temporal context from
    a dialog history and utilizes visually-grounded information. Visual reference
    resolution is a problem that addresses these challenges, requiring the agent to
    resolve ambiguous references in a given question and to find the references in
    a given image. In this paper, we propose Dual Attention Networks (DAN) for visual
    reference resolution in VisDial. DAN consists of two kinds of attention modules,
    REFER and FIND. Specifically, REFER module learns latent relationships between
    a given question and a dialog history by employing a multi-head attention mechanism.
    FIND module takes image features and reference-aware representations (i.e., the
    output of REFER module) as input, and performs visual grounding via bottom-up
    attention mechanism. We qualitatively and quantitatively evaluate our model on
    VisDial v1.0 and v0.9 datasets, showing that DAN outperforms the previous state-of-the-art
    model by a significant margin.
  address: Hong Kong, China
  author:
  - first: Gi-Cheon
    full: Gi-Cheon Kang
    id: gi-cheon-kang
    last: Kang
  - first: Jaeseo
    full: Jaeseo Lim
    id: jaeseo-lim
    last: Lim
  - first: Byoung-Tak
    full: Byoung-Tak Zhang
    id: byoung-tak-zhang
    last: Zhang
  author_string: Gi-Cheon Kang, Jaeseo Lim, Byoung-Tak Zhang
  bibkey: kang-etal-2019-dual
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1209
  month: November
  page_first: '2024'
  page_last: '2033'
  pages: "2024\u20132033"
  paper_id: '209'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1209.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1209.jpg
  title: Dual Attention Networks for Visual Reference Resolution in Visual Dialog
  title_html: Dual Attention Networks for Visual Reference Resolution in Visual Dialog
  url: https://www.aclweb.org/anthology/D19-1209
  year: '2019'
D19-1210:
  abstract: Images and text co-occur constantly on the web, but explicit links between
    images and sentences (or other intra-document textual units) are often not present.
    We present algorithms that discover image-sentence relationships without relying
    on explicit multimodal annotation in training. We experiment on seven datasets
    of varying difficulty, ranging from documents consisting of groups of images captioned
    post hoc by crowdworkers to naturally-occurring user-generated multimodal documents.
    We find that a structured training objective based on identifying whether collections
    of images and sentences co-occur in documents can suffice to predict links between
    specific sentences and specific images within the same document at test time.
  address: Hong Kong, China
  attachment:
  - filename: D19-1210.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1210.Attachment.zip
  author:
  - first: Jack
    full: Jack Hessel
    id: jack-hessel
    last: Hessel
  - first: Lillian
    full: Lillian Lee
    id: lillian-lee
    last: Lee
  - first: David
    full: David Mimno
    id: david-mimno
    last: Mimno
  author_string: Jack Hessel, Lillian Lee, David Mimno
  bibkey: hessel-etal-2019-unsupervised
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1210
  month: November
  page_first: '2034'
  page_last: '2045'
  pages: "2034\u20132045"
  paper_id: '210'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1210.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1210.jpg
  title: Unsupervised Discovery of Multimodal Links in Multi-image, Multi-sentence
    Documents
  title_html: Unsupervised Discovery of Multimodal Links in Multi-image, Multi-sentence
    Documents
  url: https://www.aclweb.org/anthology/D19-1210
  year: '2019'
D19-1211:
  abstract: Humor is a unique and creative communicative behavior often displayed
    during social interactions. It is produced in a multimodal manner, through the
    usage of words (text), gestures (visual) and prosodic cues (acoustic). Understanding
    humor from these three modalities falls within boundaries of multimodal language;
    a recent research trend in natural language processing that models natural language
    as it happens in face-to-face communication. Although humor detection is an established
    research area in NLP, in a multimodal context it has been understudied. This paper
    presents a diverse multimodal dataset, called UR-FUNNY, to open the door to understanding
    multimodal language used in expressing humor. The dataset and accompanying studies,
    present a framework in multimodal humor detection for the natural language processing
    community. UR-FUNNY is publicly available for research.
  address: Hong Kong, China
  author:
  - first: Md Kamrul
    full: Md Kamrul Hasan
    id: md-kamrul-hasan
    last: Hasan
  - first: Wasifur
    full: Wasifur Rahman
    id: wasifur-rahman
    last: Rahman
  - first: AmirAli
    full: AmirAli Bagher Zadeh
    id: amirali-bagher-zadeh
    last: Bagher Zadeh
  - first: Jianyuan
    full: Jianyuan Zhong
    id: jianyuan-zhong
    last: Zhong
  - first: Md Iftekhar
    full: Md Iftekhar Tanveer
    id: md-iftekhar-tanveer
    last: Tanveer
  - first: Louis-Philippe
    full: Louis-Philippe Morency
    id: louis-philippe-morency
    last: Morency
  - first: Mohammed (Ehsan)
    full: Mohammed (Ehsan) Hoque
    id: mohammed-ehsan-hoque
    last: Hoque
  author_string: Md Kamrul Hasan, Wasifur Rahman, AmirAli Bagher Zadeh, Jianyuan Zhong,
    Md Iftekhar Tanveer, Louis-Philippe Morency, Mohammed (Ehsan) Hoque
  bibkey: hasan-etal-2019-ur
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1211
  month: November
  page_first: '2046'
  page_last: '2056'
  pages: "2046\u20132056"
  paper_id: '211'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1211.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1211.jpg
  title: 'UR-FUNNY: A Multimodal Language Dataset for Understanding Humor'
  title_html: '<span class="acl-fixed-case">UR</span>-<span class="acl-fixed-case">FUNNY</span>:
    A Multimodal Language Dataset for Understanding Humor'
  url: https://www.aclweb.org/anthology/D19-1211
  year: '2019'
D19-1212:
  abstract: Multi-view learning algorithms are powerful representation learning tools,
    often exploited in the context of multimodal problems. However, for problems requiring
    inference at the token-level of a sequence (that is, a separate prediction must
    be made for every time step), it is often the case that single-view systems are
    used, or that more than one views are fused in a simple manner. We describe an
    incremental neural architecture paired with a novel training objective for incremental
    inference. The network operates on multi-view data. We demonstrate the effectiveness
    of our approach on the problem of predicting perpetrators in crime drama series,
    for which our model significantly outperforms previous work and strong baselines.
    Moreover, we introduce two tasks, crime case and speaker type tagging, that contribute
    to movie understanding and demonstrate the effectiveness of our model on them.
  address: Hong Kong, China
  author:
  - first: Nikos
    full: Nikos Papasarantopoulos
    id: nikos-papasarantopoulos
    last: Papasarantopoulos
  - first: Lea
    full: Lea Frermann
    id: lea-frermann
    last: Frermann
  - first: Mirella
    full: Mirella Lapata
    id: mirella-lapata
    last: Lapata
  - first: Shay B.
    full: Shay B. Cohen
    id: shay-b-cohen
    last: Cohen
  author_string: Nikos Papasarantopoulos, Lea Frermann, Mirella Lapata, Shay B. Cohen
  bibkey: papasarantopoulos-etal-2019-partners
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1212
  month: November
  page_first: '2057'
  page_last: '2067'
  pages: "2057\u20132067"
  paper_id: '212'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1212.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1212.jpg
  title: 'Partners in Crime: Multi-view Sequential Inference for Movie Understanding'
  title_html: 'Partners in Crime: Multi-view Sequential Inference for Movie Understanding'
  url: https://www.aclweb.org/anthology/D19-1212
  year: '2019'
D19-1213:
  abstract: In the current video captioning models, the video frames are collected
    in one network and the semantics are mixed into one feature, which not only increase
    the difficulty of the caption decoding, but also decrease the interpretability
    of the captioning models. To address these problems, we propose an Adaptive Semantic
    Guidance Network (ASGN), which instantiates the whole video semantics to different
    POS-aware semantics with the supervision of part of speech (POS) tag. In the encoding
    process, the POS tag activates the related neurons and parses the whole semantic
    information into corresponding encoded video representations. Furthermore, the
    potential of the model is stimulated by the POS-aware video features. In the decoding
    process, the related video features of noun and verb are used as the supervision
    to construct a new adaptive attention model which can decide whether to attend
    to the video feature or not. With the explicit improving of the interpretability
    of the network, the learning process is more transparent and the results are more
    predictable. Extensive experiments demonstrate the effectiveness of our model
    when compared with state-of-the-art models.
  address: Hong Kong, China
  author:
  - first: Xinyu
    full: Xinyu Xiao
    id: xinyu-xiao
    last: Xiao
  - first: Lingfeng
    full: Lingfeng Wang
    id: lingfeng-wang
    last: Wang
  - first: Bin
    full: Bin Fan
    id: bin-fan
    last: Fan
  - first: Shinming
    full: Shinming Xiang
    id: shinming-xiang
    last: Xiang
  - first: Chunhong
    full: Chunhong Pan
    id: chunhong-pan
    last: Pan
  author_string: Xinyu Xiao, Lingfeng Wang, Bin Fan, Shinming Xiang, Chunhong Pan
  bibkey: xiao-etal-2019-guiding
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1213
  month: November
  page_first: '2068'
  page_last: '2077'
  pages: "2068\u20132077"
  paper_id: '213'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1213.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1213.jpg
  title: 'Guiding the Flowing of Semantics: Interpretable Video Captioning via POS
    Tag'
  title_html: 'Guiding the Flowing of Semantics: Interpretable Video Captioning via
    <span class="acl-fixed-case">POS</span> Tag'
  url: https://www.aclweb.org/anthology/D19-1213
  year: '2019'
D19-1214:
  abstract: Intent detection and slot filling are two main tasks for building a spoken
    language understanding (SLU) system. The two tasks are closely tied and the slots
    often highly depend on the intent. In this paper, we propose a novel framework
    for SLU to better incorporate the intent information, which further guiding the
    slot filling. In our framework, we adopt a joint model with Stack-Propagation
    which can directly use the intent information as input for slot filling, thus
    to capture the intent semantic knowledge. In addition, to further alleviate the
    error propagation, we perform the token-level intent detection for the Stack-Propagation
    framework. Experiments on two publicly datasets show that our model achieves the
    state-of-the-art performance and outperforms other previous methods by a large
    margin. Finally, we use the Bidirectional Encoder Representation from Transformer
    (BERT) model in our framework, which further boost our performance in SLU task.
  address: Hong Kong, China
  author:
  - first: Libo
    full: Libo Qin
    id: libo-qin
    last: Qin
  - first: Wanxiang
    full: Wanxiang Che
    id: wanxiang-che
    last: Che
  - first: Yangming
    full: Yangming Li
    id: yangming-li
    last: Li
  - first: Haoyang
    full: Haoyang Wen
    id: haoyang-wen
    last: Wen
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  author_string: Libo Qin, Wanxiang Che, Yangming Li, Haoyang Wen, Ting Liu
  bibkey: qin-etal-2019-stack
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1214
  month: November
  page_first: '2078'
  page_last: '2087'
  pages: "2078\u20132087"
  paper_id: '214'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1214.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1214.jpg
  title: A Stack-Propagation Framework with Token-Level Intent Detection for Spoken
    Language Understanding
  title_html: A Stack-Propagation Framework with Token-Level Intent Detection for
    Spoken Language Understanding
  url: https://www.aclweb.org/anthology/D19-1214
  year: '2019'
D19-1215:
  abstract: 'A long-term goal of artificial intelligence is to have an agent execute
    commands communicated through natural language. In many cases the commands are
    grounded in a visual environment shared by the human who gives the command and
    the agent. Execution of the command then requires mapping the command into the
    physical visual space, after which the appropriate action can be taken. In this
    paper we consider the former. Or more specifically, we consider the problem in
    an autonomous driving setting, where a passenger requests an action that can be
    associated with an object found in a street scene. Our work presents the Talk2Car
    dataset, which is the first object referral dataset that contains commands written
    in natural language for self-driving cars. We provide a detailed comparison with
    related datasets such as ReferIt, RefCOCO, RefCOCO+, RefCOCOg, Cityscape-Ref and
    CLEVR-Ref. Additionally, we include a performance analysis using strong state-of-the-art
    models. The results show that the proposed object referral task is a challenging
    one for which the models show promising results but still require additional research
    in natural language processing, computer vision and the intersection of these
    fields. The dataset can be found on our website: http://macchina-ai.eu/'
  address: Hong Kong, China
  attachment:
  - filename: D19-1215.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1215.Attachment.pdf
  author:
  - first: Thierry
    full: Thierry Deruyttere
    id: thierry-deruyttere
    last: Deruyttere
  - first: Simon
    full: Simon Vandenhende
    id: simon-vandenhende
    last: Vandenhende
  - first: Dusan
    full: Dusan Grujicic
    id: dusan-grujicic
    last: Grujicic
  - first: Luc
    full: Luc Van Gool
    id: luc-van-gool
    last: Van Gool
  - first: Marie-Francine
    full: Marie-Francine Moens
    id: marie-francine-moens
    last: Moens
  author_string: Thierry Deruyttere, Simon Vandenhende, Dusan Grujicic, Luc Van Gool,
    Marie-Francine Moens
  bibkey: deruyttere-etal-2019-talk2car
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1215
  month: November
  page_first: '2088'
  page_last: '2098'
  pages: "2088\u20132098"
  paper_id: '215'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1215.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1215.jpg
  title: 'Talk2Car: Taking Control of Your Self-Driving Car'
  title_html: '<span class="acl-fixed-case">T</span>alk2<span class="acl-fixed-case">C</span>ar:
    Taking Control of Your Self-Driving Car'
  url: https://www.aclweb.org/anthology/D19-1215
  year: '2019'
D19-1216:
  abstract: The recent explosion of false claims in social media and on the Web in
    general has given rise to a lot of manual fact-checking initiatives. Unfortunately,
    the number of claims that need to be fact-checked is several orders of magnitude
    larger than what humans can handle manually. Thus, there has been a lot of research
    aiming at automating the process. Interestingly, previous work has largely ignored
    the growing number of claims about images. This is despite the fact that visual
    imagery is more influential than text and naturally appears alongside fake news.
    Here we aim at bridging this gap. In particular, we create a new dataset for this
    problem, and we explore a variety of features modeling the claim, the image, and
    the relationship between the claim and the image. The evaluation results show
    sizable improvements over the baseline. We release our dataset, hoping to enable
    further research on fact-checking claims about images.
  address: Hong Kong, China
  author:
  - first: Dimitrina
    full: Dimitrina Zlatkova
    id: dimitrina-zlatkova
    last: Zlatkova
  - first: Preslav
    full: Preslav Nakov
    id: preslav-nakov
    last: Nakov
  - first: Ivan
    full: Ivan Koychev
    id: ivan-koychev
    last: Koychev
  author_string: Dimitrina Zlatkova, Preslav Nakov, Ivan Koychev
  bibkey: zlatkova-etal-2019-fact
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1216
  month: November
  page_first: '2099'
  page_last: '2108'
  pages: "2099\u20132108"
  paper_id: '216'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1216.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1216.jpg
  title: 'Fact-Checking Meets Fauxtography: Verifying Claims About Images'
  title_html: 'Fact-Checking Meets Fauxtography: Verifying Claims About Images'
  url: https://www.aclweb.org/anthology/D19-1216
  year: '2019'
D19-1217:
  abstract: Video dialog is a new and challenging task, which requires the agent to
    answer questions combining video information with dialog history. And different
    from single-turn video question answering, the additional dialog history is important
    for video dialog, which often includes contextual information for the question.
    Existing visual dialog methods mainly use RNN to encode the dialog history as
    a single vector representation, which might be rough and straightforward. Some
    more advanced methods utilize hierarchical structure, attention and memory mechanisms,
    which still lack an explicit reasoning process. In this paper, we introduce a
    novel progressive inference mechanism for video dialog, which progressively updates
    query information based on dialog history and video content until the agent think
    the information is sufficient and unambiguous. In order to tackle the multi-modal
    fusion problem, we propose a cross-transformer module, which could learn more
    fine-grained and comprehensive interactions both inside and between the modalities.
    And besides answer generation, we also consider question generation, which is
    more challenging but significant for a complete video dialog system. We evaluate
    our method on two large-scale datasets, and the extensive experiments show the
    effectiveness of our method.
  address: Hong Kong, China
  author:
  - first: Weike
    full: Weike Jin
    id: weike-jin
    last: Jin
  - first: Zhou
    full: Zhou Zhao
    id: zhou-zhao
    last: Zhao
  - first: Mao
    full: Mao Gu
    id: mao-gu
    last: Gu
  - first: Jun
    full: Jun Xiao
    id: jun-xiao
    last: Xiao
  - first: Furu
    full: Furu Wei
    id: furu-wei
    last: Wei
  - first: Yueting
    full: Yueting Zhuang
    id: yueting-zhuang
    last: Zhuang
  author_string: Weike Jin, Zhou Zhao, Mao Gu, Jun Xiao, Furu Wei, Yueting Zhuang
  bibkey: jin-etal-2019-video
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1217
  month: November
  page_first: '2109'
  page_last: '2118'
  pages: "2109\u20132118"
  paper_id: '217'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1217.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1217.jpg
  title: Video Dialog via Progressive Inference and Cross-Transformer
  title_html: Video Dialog via Progressive Inference and Cross-Transformer
  url: https://www.aclweb.org/anthology/D19-1217
  year: '2019'
D19-1218:
  abstract: We study a collaborative scenario where a user not only instructs a system
    to complete tasks, but also acts alongside it. This allows the user to adapt to
    the system abilities by changing their language or deciding to simply accomplish
    some tasks themselves, and requires the system to effectively recover from errors
    as the user strategically assigns it new goals. We build a game environment to
    study this scenario, and learn to map user instructions to system actions. We
    introduce a learning approach focused on recovery from cascading errors between
    instructions, and modeling methods to explicitly reason about instructions with
    multiple goals. We evaluate with a new evaluation protocol using recorded interactions
    and online games with human users, and observe how users adapt to the system abilities.
  address: Hong Kong, China
  attachment:
  - filename: D19-1218.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1218.Attachment.pdf
  author:
  - first: Alane
    full: Alane Suhr
    id: alane-suhr
    last: Suhr
  - first: Claudia
    full: Claudia Yan
    id: claudia-yan
    last: Yan
  - first: Jack
    full: Jack Schluger
    id: jack-schluger
    last: Schluger
  - first: Stanley
    full: Stanley Yu
    id: stanley-yu
    last: Yu
  - first: Hadi
    full: Hadi Khader
    id: hadi-khader
    last: Khader
  - first: Marwa
    full: Marwa Mouallem
    id: marwa-mouallem
    last: Mouallem
  - first: Iris
    full: Iris Zhang
    id: iris-zhang
    last: Zhang
  - first: Yoav
    full: Yoav Artzi
    id: yoav-artzi
    last: Artzi
  author_string: Alane Suhr, Claudia Yan, Jack Schluger, Stanley Yu, Hadi Khader,
    Marwa Mouallem, Iris Zhang, Yoav Artzi
  bibkey: suhr-etal-2019-executing
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1218
  month: November
  page_first: '2119'
  page_last: '2130'
  pages: "2119\u20132130"
  paper_id: '218'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1218.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1218.jpg
  title: Executing Instructions in Situated Collaborative Interactions
  title_html: Executing Instructions in Situated Collaborative Interactions
  url: https://www.aclweb.org/anthology/D19-1218
  year: '2019'
D19-1219:
  abstract: "To advance models of multimodal context, we introduce a simple yet powerful\
    \ neural architecture for data that combines vision and natural language. The\
    \ \u201CBounding Boxes in Text Transformer\u201D (B2T2) also leverages referential\
    \ information binding words to portions of the image in a single unified architecture.\
    \ B2T2 is highly effective on the Visual Commonsense Reasoning benchmark, achieving\
    \ a new state-of-the-art with a 25% relative reduction in error rate compared\
    \ to published baselines and obtaining the best performance to date on the public\
    \ leaderboard (as of May 22, 2019). A detailed ablation analysis shows that the\
    \ early integration of the visual features into the text analysis is key to the\
    \ effectiveness of the new architecture. A reference implementation of our models\
    \ is provided."
  address: Hong Kong, China
  author:
  - first: Chris
    full: Chris Alberti
    id: chris-alberti
    last: Alberti
  - first: Jeffrey
    full: Jeffrey Ling
    id: jeffrey-ling
    last: Ling
  - first: Michael
    full: Michael Collins
    id: michael-collins
    last: Collins
  - first: David
    full: David Reitter
    id: david-reitter
    last: Reitter
  author_string: Chris Alberti, Jeffrey Ling, Michael Collins, David Reitter
  bibkey: alberti-etal-2019-fusion
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1219
  month: November
  page_first: '2131'
  page_last: '2140'
  pages: "2131\u20132140"
  paper_id: '219'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1219.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1219.jpg
  title: Fusion of Detected Objects in Text for Visual Question Answering
  title_html: Fusion of Detected Objects in Text for Visual Question Answering
  url: https://www.aclweb.org/anthology/D19-1219
  year: '2019'
D19-1220:
  abstract: "This paper presents a new metric called TIGEr for the automatic evaluation\
    \ of image captioning systems. Popular metrics, such as BLEU and CIDEr, are based\
    \ solely on text matching between reference captions and machine-generated captions,\
    \ potentially leading to biased evaluations because references may not fully cover\
    \ the image content and natural language is inherently ambiguous. Building upon\
    \ a machine-learned text-image grounding model, TIGEr allows to evaluate caption\
    \ quality not only based on how well a caption represents image content, but also\
    \ on how well machine-generated captions match human-generated captions. Our empirical\
    \ tests show that TIGEr has a higher consistency with human judgments than alternative\
    \ existing metrics. We also comprehensively assess the metric\u2019s effectiveness\
    \ in caption evaluation by measuring the correlation between human judgments and\
    \ metric scores."
  address: Hong Kong, China
  author:
  - first: Ming
    full: Ming Jiang
    id: ming-jiang
    last: Jiang
  - first: Qiuyuan
    full: Qiuyuan Huang
    id: qiuyuan-huang
    last: Huang
  - first: Lei
    full: Lei Zhang
    id: lei-zhang
    last: Zhang
  - first: Xin
    full: Xin Wang
    id: xin-wang
    last: Wang
  - first: Pengchuan
    full: Pengchuan Zhang
    id: pengchuan-zhang
    last: Zhang
  - first: Zhe
    full: Zhe Gan
    id: zhe-gan
    last: Gan
  - first: Jana
    full: Jana Diesner
    id: jana-diesner
    last: Diesner
  - first: Jianfeng
    full: Jianfeng Gao
    id: jianfeng-gao
    last: Gao
  author_string: Ming Jiang, Qiuyuan Huang, Lei Zhang, Xin Wang, Pengchuan Zhang,
    Zhe Gan, Jana Diesner, Jianfeng Gao
  bibkey: jiang-etal-2019-tiger
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1220
  month: November
  page_first: '2141'
  page_last: '2152'
  pages: "2141\u20132152"
  paper_id: '220'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1220.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1220.jpg
  title: 'TIGEr: Text-to-Image Grounding for Image Caption Evaluation'
  title_html: '<span class="acl-fixed-case">TIGE</span>r: Text-to-Image Grounding
    for Image Caption Evaluation'
  url: https://www.aclweb.org/anthology/D19-1220
  year: '2019'
D19-1221:
  abstract: "Adversarial examples highlight model vulnerabilities and are useful for\
    \ evaluation and interpretation. We define universal adversarial triggers: input-agnostic\
    \ sequences of tokens that trigger a model to produce a specific prediction when\
    \ concatenated to any input from a dataset. We propose a gradient-guided search\
    \ over tokens which finds short trigger sequences (e.g., one word for classification\
    \ and four words for language modeling) that successfully trigger the target prediction.\
    \ For example, triggers cause SNLI entailment accuracy to drop from 89.94% to\
    \ 0.55%, 72% of \u201Cwhy\u201D questions in SQuAD to be answered \u201Cto kill\
    \ american people\u201D, and the GPT-2 language model to spew racist output even\
    \ when conditioned on non-racial contexts. Furthermore, although the triggers\
    \ are optimized using white-box access to a specific model, they transfer to other\
    \ models for all tasks we consider. Finally, since triggers are input-agnostic,\
    \ they provide an analysis of global model behavior. For instance, they confirm\
    \ that SNLI models exploit dataset biases and help to diagnose heuristics learned\
    \ by reading comprehension models."
  address: Hong Kong, China
  attachment:
  - filename: D19-1221.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1221.Attachment.zip
  author:
  - first: Eric
    full: Eric Wallace
    id: eric-wallace
    last: Wallace
  - first: Shi
    full: Shi Feng
    id: shi-feng
    last: Feng
  - first: Nikhil
    full: Nikhil Kandpal
    id: nikhil-kandpal
    last: Kandpal
  - first: Matt
    full: Matt Gardner
    id: matt-gardner
    last: Gardner
  - first: Sameer
    full: Sameer Singh
    id: sameer-singh
    last: Singh
  author_string: Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, Sameer Singh
  bibkey: wallace-etal-2019-universal
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1221
  month: November
  page_first: '2153'
  page_last: '2162'
  pages: "2153\u20132162"
  paper_id: '221'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1221.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1221.jpg
  title: Universal Adversarial Triggers for Attacking and Analyzing NLP
  title_html: Universal Adversarial Triggers for Attacking and Analyzing <span class="acl-fixed-case">NLP</span>
  url: https://www.aclweb.org/anthology/D19-1221
  year: '2019'
D19-1222:
  abstract: "Performance drop due to domain-shift is an endemic problem for NLP models\
    \ in production. This problem creates an urge to continuously annotate evaluation\
    \ datasets to measure the expected drop in the model performance which can be\
    \ prohibitively expensive and slow. In this paper, we study the problem of predicting\
    \ the performance drop of modern NLP models under domain-shift, in the absence\
    \ of any target domain labels. We investigate three families of methods (\u210B\
    -divergence, reverse classification accuracy and confidence measures), show how\
    \ they can be used to predict the performance drop and study their robustness\
    \ to adversarial domain-shifts. Our results on sentiment classification and sequence\
    \ labelling show that our method is able to predict performance drops with an\
    \ error rate as low as 2.15% and 0.89% for sentiment analysis and POS tagging\
    \ respectively.-divergence, reverse classification accuracy and confidence measures),\
    \ show how they can be used to predict the performance drop and study their robustness\
    \ to adversarial domain-shifts. Our results on sentiment classification and sequence\
    \ labelling show that our method is able to predict performance drops with an\
    \ error rate as low as 2.15% and 0.89% for sentiment analysis and POS tagging\
    \ respectively."
  address: Hong Kong, China
  attachment:
  - filename: D19-1222.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1222.Attachment.pdf
  author:
  - first: Hady
    full: Hady Elsahar
    id: hady-elsahar
    last: Elsahar
  - first: Matthias
    full: "Matthias Gall\xE9"
    id: matthias-galle
    last: "Gall\xE9"
  author_string: "Hady Elsahar, Matthias Gall\xE9"
  bibkey: elsahar-galle-2019-annotate
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1222
  month: November
  page_first: '2163'
  page_last: '2173'
  pages: "2163\u20132173"
  paper_id: '222'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1222.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1222.jpg
  title: To Annotate or Not? Predicting Performance Drop under Domain Shift
  title_html: To Annotate or Not? Predicting Performance Drop under Domain Shift
  url: https://www.aclweb.org/anthology/D19-1222
  year: '2019'
D19-1223:
  abstract: "Attention mechanisms have become ubiquitous in NLP. Recent architectures,\
    \ notably the Transformer, learn powerful context-aware word representations through\
    \ layered, multi-headed attention. The multiple heads learn diverse types of word\
    \ relationships. However, with standard softmax attention, all attention heads\
    \ are dense, assigning a non-zero weight to all context words. In this work, we\
    \ introduce the adaptively sparse Transformer, wherein attention heads have flexible,\
    \ context-dependent sparsity patterns. This sparsity is accomplished by replacing\
    \ softmax with alpha-entmax: a differentiable generalization of softmax that allows\
    \ low-scoring words to receive precisely zero weight. Moreover, we derive a method\
    \ to automatically learn the alpha parameter \u2013 which controls the shape and\
    \ sparsity of alpha-entmax \u2013 allowing attention heads to choose between focused\
    \ or spread-out behavior. Our adaptively sparse Transformer improves interpretability\
    \ and head diversity when compared to softmax Transformers on machine translation\
    \ datasets. Findings of the quantitative and qualitative analysis of our approach\
    \ include that heads in different layers learn different sparsity preferences\
    \ and tend to be more diverse in their attention distributions than softmax Transformers.\
    \ Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover\
    \ different head specializations."
  address: Hong Kong, China
  attachment:
  - filename: D19-1223.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1223.Attachment.pdf
  author:
  - first: "Gon\xE7alo M."
    full: "Gon\xE7alo M. Correia"
    id: goncalo-m-correia
    last: Correia
  - first: Vlad
    full: Vlad Niculae
    id: vlad-niculae
    last: Niculae
  - first: "Andr\xE9 F. T."
    full: "Andr\xE9 F. T. Martins"
    id: andre-f-t-martins
    last: Martins
  author_string: "Gon\xE7alo M. Correia, Vlad Niculae, Andr\xE9 F. T. Martins"
  bibkey: correia-etal-2019-adaptively
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1223
  month: November
  page_first: '2174'
  page_last: '2184'
  pages: "2174\u20132184"
  paper_id: '223'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1223.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1223.jpg
  title: Adaptively Sparse Transformers
  title_html: Adaptively Sparse Transformers
  url: https://www.aclweb.org/anthology/D19-1223
  year: '2019'
D19-1224:
  abstract: 'Research in natural language processing proceeds, in part, by demonstrating
    that new models achieve superior performance (e.g., accuracy) on held-out test
    data, compared to previous results. In this paper, we demonstrate that test-set
    performance scores alone are insufficient for drawing accurate conclusions about
    which model performs best. We argue for reporting additional details, especially
    performance on validation data obtained during model development. We present a
    novel technique for doing so: expected validation performance of the best-found
    model as a function of computation budget (i.e., the number of hyperparameter
    search trials or the overall training time). Using our approach, we find multiple
    recent model comparisons where authors would have reached a different conclusion
    if they had used more (or less) computation. Our approach also allows us to estimate
    the amount of computation required to obtain a given accuracy; applying it to
    several recently published results yields massive variation across papers, from
    hours to weeks. We conclude with a set of best practices for reporting experimental
    results which allow for robust future comparisons, and provide code to allow researchers
    to use our technique.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1224.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1224.Attachment.zip
  author:
  - first: Jesse
    full: Jesse Dodge
    id: jesse-dodge
    last: Dodge
  - first: Suchin
    full: Suchin Gururangan
    id: suchin-gururangan
    last: Gururangan
  - first: Dallas
    full: Dallas Card
    id: dallas-card
    last: Card
  - first: Roy
    full: Roy Schwartz
    id: roy-schwartz
    last: Schwartz
  - first: Noah A.
    full: Noah A. Smith
    id: noah-a-smith
    last: Smith
  author_string: Jesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, Noah A.
    Smith
  bibkey: dodge-etal-2019-show
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1224
  month: November
  page_first: '2185'
  page_last: '2194'
  pages: "2185\u20132194"
  paper_id: '224'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1224.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1224.jpg
  title: 'Show Your Work: Improved Reporting of Experimental Results'
  title_html: 'Show Your Work: Improved Reporting of Experimental Results'
  url: https://www.aclweb.org/anthology/D19-1224
  year: '2019'
D19-1225:
  abstract: We propose a deep factorization model for typographic analysis that disentangles
    content from style. Specifically, a variational inference procedure factors each
    training glyph into the combination of a character-specific content embedding
    and a latent font-specific style variable. The underlying generative model combines
    these factors through an asymmetric transpose convolutional process to generate
    the image of the glyph itself. When trained on corpora of fonts, our model learns
    a manifold over font styles that can be used to analyze or reconstruct new, unseen
    fonts. On the task of reconstructing missing glyphs from an unknown font given
    only a small number of observations, our model outperforms both a strong nearest
    neighbors baseline and a state-of-the-art discriminative model from prior work.
  address: Hong Kong, China
  author:
  - first: Akshay
    full: Akshay Srivatsan
    id: akshay-srivatsan
    last: Srivatsan
  - first: Jonathan
    full: Jonathan Barron
    id: jonathan-barron
    last: Barron
  - first: Dan
    full: Dan Klein
    id: dan-klein
    last: Klein
  - first: Taylor
    full: Taylor Berg-Kirkpatrick
    id: taylor-berg-kirkpatrick
    last: Berg-Kirkpatrick
  author_string: Akshay Srivatsan, Jonathan Barron, Dan Klein, Taylor Berg-Kirkpatrick
  bibkey: srivatsan-etal-2019-deep
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1225
  month: November
  page_first: '2195'
  page_last: '2205'
  pages: "2195\u20132205"
  paper_id: '225'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1225.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1225.jpg
  title: A Deep Factorization of Style and Structure in Fonts
  title_html: A Deep Factorization of Style and Structure in Fonts
  url: https://www.aclweb.org/anthology/D19-1225
  year: '2019'
D19-1226:
  abstract: 'Semantic specialization integrates structured linguistic knowledge from
    external resources (such as lexical relations in WordNet) into pretrained distributional
    vectors in the form of constraints. However, this technique cannot be leveraged
    in many languages, because their structured external resources are typically incomplete
    or non-existent. To bridge this gap, we propose a novel method that transfers
    specialization from a resource-rich source language (English) to virtually any
    target language. Our specialization transfer comprises two crucial steps: 1) Inducing
    noisy constraints in the target language through automatic word translation; and
    2) Filtering the noisy constraints via a state-of-the-art relation prediction
    model trained on the source language constraints. This allows us to specialize
    any set of distributional vectors in the target language with the refined constraints.
    We prove the effectiveness of our method through intrinsic word similarity evaluation
    in 8 languages, and with 3 downstream tasks in 5 languages: lexical simplification,
    dialog state tracking, and semantic textual similarity. The gains over the previous
    state-of-art specialization methods are substantial and consistent across languages.
    Our results also suggest that the transfer method is effective even for lexically
    distant source-target language pairs. Finally, as a by-product, our method produces
    lists of WordNet-style lexical relations in resource-poor languages.'
  address: Hong Kong, China
  author:
  - first: Edoardo Maria
    full: Edoardo Maria Ponti
    id: edoardo-maria-ponti
    last: Ponti
  - first: Ivan
    full: "Ivan Vuli\u0107"
    id: ivan-vulic
    last: "Vuli\u0107"
  - first: Goran
    full: "Goran Glava\u0161"
    id: goran-glavas
    last: "Glava\u0161"
  - first: Roi
    full: Roi Reichart
    id: roi-reichart
    last: Reichart
  - first: Anna
    full: Anna Korhonen
    id: anna-korhonen
    last: Korhonen
  author_string: "Edoardo Maria Ponti, Ivan Vuli\u0107, Goran Glava\u0161, Roi Reichart,\
    \ Anna Korhonen"
  bibkey: ponti-etal-2019-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1226
  month: November
  page_first: '2206'
  page_last: '2217'
  pages: "2206\u20132217"
  paper_id: '226'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1226.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1226.jpg
  title: Cross-lingual Semantic Specialization via Lexical Relation Induction
  title_html: Cross-lingual Semantic Specialization via Lexical Relation Induction
  url: https://www.aclweb.org/anthology/D19-1226
  year: '2019'
D19-1227:
  abstract: Metaphors allow us to convey emotion by connecting physical experiences
    and abstract concepts. The results of previous research in linguistics and psychology
    suggest that metaphorical phrases tend to be more emotionally evocative than their
    literal counterparts. In this paper, we investigate the relationship between metaphor
    and emotion within a computational framework, by proposing the first joint model
    of these phenomena. We experiment with several multitask learning architectures
    for this purpose, involving both hard and soft parameter sharing. Our results
    demonstrate that metaphor identification and emotion prediction mutually benefit
    from joint learning and our models advance the state of the art in both of these
    tasks.
  address: Hong Kong, China
  author:
  - first: Verna
    full: Verna Dankers
    id: verna-dankers
    last: Dankers
  - first: Marek
    full: Marek Rei
    id: marek-rei
    last: Rei
  - first: Martha
    full: Martha Lewis
    id: martha-lewis
    last: Lewis
  - first: Ekaterina
    full: Ekaterina Shutova
    id: ekaterina-shutova
    last: Shutova
  author_string: Verna Dankers, Marek Rei, Martha Lewis, Ekaterina Shutova
  bibkey: dankers-etal-2019-modelling
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1227
  month: November
  page_first: '2218'
  page_last: '2229'
  pages: "2218\u20132229"
  paper_id: '227'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1227.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1227.jpg
  title: Modelling the interplay of metaphor and emotion through multitask learning
  title_html: Modelling the interplay of metaphor and emotion through multitask learning
  url: https://www.aclweb.org/anthology/D19-1227
  year: '2019'
D19-1228:
  abstract: "In natural language inference (NLI), contexts are considered veridical\
    \ if they allow us to infer that their underlying propositions make true claims\
    \ about the real world. We investigate whether a state-of-the-art natural language\
    \ inference model (BERT) learns to make correct inferences about veridicality\
    \ in verb-complement constructions. We introduce an NLI dataset for veridicality\
    \ evaluation consisting of 1,500 sentence pairs, covering 137 unique verbs. We\
    \ find that both human and model inferences generally follow theoretical patterns,\
    \ but exhibit a systematic bias towards assuming that verbs are veridical\u2013\
    a bias which is amplified in BERT. We further show that, encouragingly, BERT\u2019\
    s inferences are sensitive not only to the presence of individual verb types,\
    \ but also to the syntactic role of the verb, the form of the complement clause\
    \ (to- vs. that-complements), and negation."
  address: Hong Kong, China
  attachment:
  - filename: D19-1228.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1228.Attachment.zip
  author:
  - first: Alexis
    full: Alexis Ross
    id: alexis-ross
    last: Ross
  - first: Ellie
    full: Ellie Pavlick
    id: ellie-pavlick
    last: Pavlick
  author_string: Alexis Ross, Ellie Pavlick
  bibkey: ross-pavlick-2019-well
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1228
  month: November
  page_first: '2230'
  page_last: '2240'
  pages: "2230\u20132240"
  paper_id: '228'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1228.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1228.jpg
  title: How well do NLI models capture verb veridicality?
  title_html: How well do <span class="acl-fixed-case">NLI</span> models capture verb
    veridicality?
  url: https://www.aclweb.org/anthology/D19-1228
  year: '2019'
D19-1229:
  abstract: "There is an extensive history of scholarship into what constitutes a\
    \ \u201Cbasic\u201D color term, as well as a broadly attested acquisition sequence\
    \ of basic color terms across many languages, as articulated in the seminal work\
    \ of Berlin and Kay (1969). This paper employs a set of diverse measures on massively\
    \ cross-linguistic data to operationalize and critique the Berlin and Kay color\
    \ term hypotheses. Collectively, the 14 empirically-grounded computational linguistic\
    \ metrics we design\u2014as well as their aggregation\u2014correlate strongly\
    \ with both the Berlin and Kay basic/secondary color term partition (\u03B3 =\
    \ 0.96) and their hypothesized universal acquisition sequence. The measures and\
    \ result provide further empirical evidence from computational linguistics in\
    \ support of their claims, as well as additional nuance: they suggest treating\
    \ the partition as a spectrum instead of a dichotomy."
  address: Hong Kong, China
  attachment:
  - filename: D19-1229.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1229.Attachment.pdf
  author:
  - first: Arya D.
    full: Arya D. McCarthy
    id: arya-d-mccarthy
    last: McCarthy
  - first: Winston
    full: Winston Wu
    id: winston-wu
    last: Wu
  - first: Aaron
    full: Aaron Mueller
    id: aaron-mueller
    last: Mueller
  - first: William
    full: William Watson
    id: william-watson
    last: Watson
  - first: David
    full: David Yarowsky
    id: david-yarowsky
    last: Yarowsky
  author_string: Arya D. McCarthy, Winston Wu, Aaron Mueller, William Watson, David
    Yarowsky
  bibkey: mccarthy-etal-2019-modeling
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1229
  month: November
  page_first: '2241'
  page_last: '2250'
  pages: "2241\u20132250"
  paper_id: '229'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1229.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1229.jpg
  title: Modeling Color Terminology Across Thousands of Languages
  title_html: Modeling Color Terminology Across Thousands of Languages
  url: https://www.aclweb.org/anthology/D19-1229
  year: '2019'
D19-1230:
  abstract: "Negation is a universal but complicated linguistic phenomenon, which\
    \ has received considerable attention from the NLP community over the last decade,\
    \ since a negated statement often carries both an explicit negative focus and\
    \ implicit positive meanings. For the sake of understanding a negated statement,\
    \ it is critical to precisely detect the negative focus in context. However, how\
    \ to capture contextual information for negative focus detection is still an open\
    \ challenge. To well address this, we come up with an attention-based neural network\
    \ to model contextual information. In particular, we introduce a framework which\
    \ consists of a Bidirectional Long Short-Term Memory (BiLSTM) neural network and\
    \ a Conditional Random Fields (CRF) layer to effectively encode the order information\
    \ and the long-range context dependency in a sentence. Moreover, we design two\
    \ types of attention mechanisms, word-level contextual attention and topic-level\
    \ contextual attention, to take advantage of contextual information across sentences\
    \ from both the word perspective and the topic perspective, respectively. Experimental\
    \ results on the SEM\u201912 shared task corpus show that our approach achieves\
    \ the best performance on negative focus detection, yielding an absolute improvement\
    \ of 2.11% over the state-of-the-art. This demonstrates the great effectiveness\
    \ of the two types of contextual attention mechanisms."
  address: Hong Kong, China
  author:
  - first: Longxiang
    full: Longxiang Shen
    id: longxiang-shen
    last: Shen
  - first: Bowei
    full: Bowei Zou
    id: bowei-zou
    last: Zou
  - first: Yu
    full: Yu Hong
    id: yu-hong
    last: Hong
  - first: Guodong
    full: Guodong Zhou
    id: guodong-zhou
    last: Zhou
  - first: Qiaoming
    full: Qiaoming Zhu
    id: qiaoming-zhu
    last: Zhu
  - first: AiTi
    full: AiTi Aw
    id: aiti-aw
    last: Aw
  author_string: Longxiang Shen, Bowei Zou, Yu Hong, Guodong Zhou, Qiaoming Zhu, AiTi
    Aw
  bibkey: shen-etal-2019-negative
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1230
  month: November
  page_first: '2251'
  page_last: '2261'
  pages: "2251\u20132261"
  paper_id: '230'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1230.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1230.jpg
  title: Negative Focus Detection via Contextual Attention Mechanism
  title_html: Negative Focus Detection via Contextual Attention Mechanism
  url: https://www.aclweb.org/anthology/D19-1230
  year: '2019'
D19-1231:
  abstract: Recently, neural approaches to coherence modeling have achieved state-of-the-art
    results in several evaluation tasks. However, we show that most of these models
    often fail on harder tasks with more realistic application scenarios. In particular,
    the existing models underperform on tasks that require the model to be sensitive
    to local contexts such as candidate ranking in conversational dialogue and in
    machine translation. In this paper, we propose a unified coherence model that
    incorporates sentence grammar, inter-sentence coherence relations, and global
    coherence patterns into a common neural framework. With extensive experiments
    on local and global discrimination tasks, we demonstrate that our proposed model
    outperforms existing models by a good margin, and establish a new state-of-the-art.
  address: Hong Kong, China
  author:
  - first: Han Cheol
    full: Han Cheol Moon
    id: han-cheol-moon
    last: Moon
  - first: Tasnim
    full: Tasnim Mohiuddin
    id: muhammad-tasnim-mohiuddin
    last: Mohiuddin
  - first: Shafiq
    full: Shafiq Joty
    id: shafiq-joty
    last: Joty
  - first: Chi
    full: Chi Xu
    id: chi-xu
    last: Xu
  author_string: Han Cheol Moon, Tasnim Mohiuddin, Shafiq Joty, Chi Xu
  bibkey: moon-etal-2019-unified
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1231
  month: November
  page_first: '2262'
  page_last: '2272'
  pages: "2262\u20132272"
  paper_id: '231'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1231.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1231.jpg
  title: A Unified Neural Coherence Model
  title_html: A Unified Neural Coherence Model
  url: https://www.aclweb.org/anthology/D19-1231
  year: '2019'
D19-1232:
  abstract: We propose a novel topic-guided coherence modeling (TGCM) for sentence
    ordering. Our attention based pointer decoder directly utilize sentence vectors
    in a permutation-invariant manner, without being compressed into a single fixed-length
    vector as the paragraph representation. Thus, TGCM can improve global dependencies
    among sentences and preserve relatively informative paragraph-level semantics.
    Moreover, to predict the next sentence, we capture topic-enhanced sentence-pair
    interactions between the current predicted sentence and each next-sentence candidate.
    With the coherent topical context matching, we promote local dependencies that
    help identify the tight semantic connections for sentence ordering. The experimental
    results show that TGCM outperforms state-of-the-art models from various perspectives.
  address: Hong Kong, China
  author:
  - first: Byungkook
    full: Byungkook Oh
    id: byungkook-oh
    last: Oh
  - first: Seungmin
    full: Seungmin Seo
    id: seungmin-seo
    last: Seo
  - first: Cheolheon
    full: Cheolheon Shin
    id: cheolheon-shin
    last: Shin
  - first: Eunju
    full: Eunju Jo
    id: eunju-jo
    last: Jo
  - first: Kyong-Ho
    full: Kyong-Ho Lee
    id: kyong-ho-lee
    last: Lee
  author_string: Byungkook Oh, Seungmin Seo, Cheolheon Shin, Eunju Jo, Kyong-Ho Lee
  bibkey: oh-etal-2019-topic
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1232
  month: November
  page_first: '2273'
  page_last: '2283'
  pages: "2273\u20132283"
  paper_id: '232'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1232.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1232.jpg
  title: Topic-Guided Coherence Modeling for Sentence Ordering by Preserving Global
    and Local Information
  title_html: Topic-Guided Coherence Modeling for Sentence Ordering by Preserving
    Global and Local Information
  url: https://www.aclweb.org/anthology/D19-1232
  year: '2019'
D19-1233:
  abstract: "Rhetorical structure trees have been shown to be useful for several document-level\
    \ tasks including summarization and document classification. Previous approaches\
    \ to RST parsing have used discriminative models; however, these are less sample\
    \ efficient than generative models, and RST parsing datasets are typically small.\
    \ In this paper, we present the first generative model for RST parsing. Our model\
    \ is a document-level RNN grammar (RNNG) with a bottom-up traversal order. We\
    \ show that, for our parser\u2019s traversal order, previous beam search algorithms\
    \ for RNNGs have a left-branching bias which is ill-suited for RST parsing.We\
    \ develop a novel beam search algorithm that keeps track of both structure-and\
    \ word-generating actions without exhibit-ing this branching bias and results\
    \ in absolute improvements of 6.8 and 2.9 on unlabelled and labelled F1 over previous\
    \ algorithms. Overall, our generative model outperforms a discriminative model\
    \ with the same features by 2.6 F1points and achieves performance comparable to\
    \ the state-of-the-art, outperforming all published parsers from a recent replication\
    \ study that do not use additional training data"
  address: Hong Kong, China
  author:
  - first: Amandla
    full: Amandla Mabona
    id: amandla-mabona
    last: Mabona
  - first: Laura
    full: Laura Rimell
    id: laura-rimell
    last: Rimell
  - first: Stephen
    full: Stephen Clark
    id: stephen-clark
    last: Clark
  - first: Andreas
    full: Andreas Vlachos
    id: andreas-vlachos
    last: Vlachos
  author_string: Amandla Mabona, Laura Rimell, Stephen Clark, Andreas Vlachos
  bibkey: mabona-etal-2019-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1233
  month: November
  page_first: '2284'
  page_last: '2295'
  pages: "2284\u20132295"
  paper_id: '233'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1233.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1233.jpg
  title: Neural Generative Rhetorical Structure Parsing
  title_html: Neural Generative Rhetorical Structure Parsing
  url: https://www.aclweb.org/anthology/D19-1233
  year: '2019'
D19-1234:
  abstract: "This paper provides a detailed comparison of a data programming approach\
    \ with (i) off-the-shelf, state-of-the-art deep learning architectures that optimize\
    \ their representations (BERT) and (ii) handcrafted-feature approaches previously\
    \ used in the discourse analysis literature. We compare these approaches on the\
    \ task of learning discourse structure for multi-party dialogue. The data programming\
    \ paradigm offered by the Snorkel framework allows a user to label training data\
    \ using expert-composed heuristics, which are then transformed via the \u201C\
    generative step\u201D into probability distributions of the class labels given\
    \ the data. We show that on our task the generative model outperforms both deep\
    \ learning architectures as well as more traditional ML approaches when learning\
    \ discourse structure\u2014it even outperforms the combination of deep learning\
    \ methods and hand-crafted features. We also implement several strategies for\
    \ \u201Cdecoding\u201D our generative model output in order to improve our results.\
    \ We conclude that weak supervision methods hold great promise as a means for\
    \ creating and improving data sets for discourse structure."
  address: Hong Kong, China
  author:
  - first: Sonia
    full: Sonia Badene
    id: sonia-badene
    last: Badene
  - first: Kate
    full: Kate Thompson
    id: kate-thompson
    last: Thompson
  - first: Jean-Pierre
    full: "Jean-Pierre Lorr\xE9"
    id: jean-pierre-lorre
    last: "Lorr\xE9"
  - first: Nicholas
    full: Nicholas Asher
    id: nicholas-asher
    last: Asher
  author_string: "Sonia Badene, Kate Thompson, Jean-Pierre Lorr\xE9, Nicholas Asher"
  bibkey: badene-etal-2019-weak
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1234
  month: November
  page_first: '2296'
  page_last: '2305'
  pages: "2296\u20132305"
  paper_id: '234'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1234.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1234.jpg
  title: Weak Supervision for Learning Discourse Structure
  title_html: Weak Supervision for Learning Discourse Structure
  url: https://www.aclweb.org/anthology/D19-1234
  year: '2019'
D19-1235:
  abstract: Discourse parsing could not yet take full advantage of the neural NLP
    revolution, mostly due to the lack of annotated datasets. We propose a novel approach
    that uses distant supervision on an auxiliary task (sentiment classification),
    to generate abundant data for RST-style discourse structure prediction. Our approach
    combines a neural variant of multiple-instance learning, using document-level
    supervision, with an optimal CKY-style tree generation algorithm. In a series
    of experiments, we train a discourse parser (for only structure prediction) on
    our automatically generated dataset and compare it with parsers trained on human-annotated
    corpora (news domain RST-DT and Instructional domain). Results indicate that while
    our parser does not yet match the performance of a parser trained and tested on
    the same dataset (intra-domain), it does perform remarkably well on the much more
    difficult and arguably more useful task of inter-domain discourse structure prediction,
    where the parser is trained on one domain and tested/applied on another one.
  address: Hong Kong, China
  author:
  - first: Patrick
    full: Patrick Huber
    id: patrick-huber
    last: Huber
  - first: Giuseppe
    full: Giuseppe Carenini
    id: giuseppe-carenini
    last: Carenini
  author_string: Patrick Huber, Giuseppe Carenini
  bibkey: huber-carenini-2019-predicting
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1235
  month: November
  page_first: '2306'
  page_last: '2316'
  pages: "2306\u20132316"
  paper_id: '235'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1235.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1235.jpg
  title: Predicting Discourse Structure using Distant Supervision from Sentiment
  title_html: Predicting Discourse Structure using Distant Supervision from Sentiment
  url: https://www.aclweb.org/anthology/D19-1235
  year: '2019'
D19-1236:
  abstract: 'The review and selection process for scientific paper publication is
    essential for the quality of scholarly publications in a scientific field. The
    double-blind review system, which enforces author anonymity during the review
    period, is widely used by prestigious conferences and journals to ensure the integrity
    of this process. Although the notion of anonymity in the double-blind review has
    been questioned before, the availability of full text paper collections brings
    new opportunities for exploring the question: Is the double-blind review process
    really double-blind? We study this question on the ACL and EMNLP paper collections
    and present an analysis on how well deep learning techniques can infer the authors
    of a paper. Specifically, we explore Convolutional Neural Networks trained on
    various aspects of a paper, e.g., content, style features, and references, to
    understand the extent to which we can infer the authors of a paper and what aspects
    contribute the most. Our results show that the authors of a paper can be inferred
    with accuracy as high as 87% on ACL and 78% on EMNLP for the top 100 most prolific
    authors.'
  address: Hong Kong, China
  author:
  - first: Cornelia
    full: Cornelia Caragea
    id: cornelia-caragea
    last: Caragea
  - first: Ana
    full: Ana Uban
    id: ana-uban
    last: Uban
  - first: Liviu P.
    full: Liviu P. Dinu
    id: liviu-p-dinu
    last: Dinu
  author_string: Cornelia Caragea, Ana Uban, Liviu P. Dinu
  bibkey: caragea-etal-2019-myth
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1236
  month: November
  page_first: '2317'
  page_last: '2327'
  pages: "2317\u20132327"
  paper_id: '236'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1236.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1236.jpg
  title: 'The Myth of Double-Blind Review Revisited: ACL vs. EMNLP'
  title_html: 'The Myth of Double-Blind Review Revisited: <span class="acl-fixed-case">ACL</span>
    vs. <span class="acl-fixed-case">EMNLP</span>'
  url: https://www.aclweb.org/anthology/D19-1236
  year: '2019'
D19-1237:
  abstract: "The number of personal stories about sexual harassment shared online\
    \ has increased exponentially in recent years. This is in part inspired by the\
    \ #MeToo and #TimesUp movements. Safecity is an online forum for people who experienced\
    \ or witnessed sexual harassment to share their personal experiences. It has collected\
    \ >10,000 stories so far. Sexual harassment occurred in a variety of situations,\
    \ and categorization of the stories and extraction of their key elements will\
    \ provide great help for the related parties to understand and address sexual\
    \ harassment. In this study, we manually annotated those stories with labels in\
    \ the dimensions of location, time, and harassers\u2019 characteristics, and marked\
    \ the key elements related to these dimensions. Furthermore, we applied natural\
    \ language processing technologies with joint learning schemes to automatically\
    \ categorize these stories in those dimensions and extract key elements at the\
    \ same time. We also uncovered significant patterns from the categorized sexual\
    \ harassment stories. We believe our annotated data set, proposed algorithms,\
    \ and analysis will help people who have been harassed, authorities, researchers\
    \ and other related parties in various ways, such as automatically filling reports,\
    \ enlightening the public in order to prevent future harassment, and enabling\
    \ more effective, faster action to be taken."
  address: Hong Kong, China
  attachment:
  - filename: D19-1237.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1237.Attachment.zip
  author:
  - first: Yingchi
    full: Yingchi Liu
    id: yingchi-liu
    last: Liu
  - first: Quanzhi
    full: Quanzhi Li
    id: quanzhi-li
    last: Li
  - first: Marika
    full: Marika Cifor
    id: marika-cifor
    last: Cifor
  - first: Xiaozhong
    full: Xiaozhong Liu
    id: xiaozhong-liu
    last: Liu
  - first: Qiong
    full: Qiong Zhang
    id: qiong-zhang
    last: Zhang
  - first: Luo
    full: Luo Si
    id: luo-si
    last: Si
  author_string: Yingchi Liu, Quanzhi Li, Marika Cifor, Xiaozhong Liu, Qiong Zhang,
    Luo Si
  bibkey: liu-etal-2019-uncover
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1237
  month: November
  page_first: '2328'
  page_last: '2337'
  pages: "2328\u20132337"
  paper_id: '237'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1237.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1237.jpg
  title: Uncover Sexual Harassment Patterns from Personal Stories by Joint Key Element
    Extraction and Categorization
  title_html: Uncover Sexual Harassment Patterns from Personal Stories by Joint Key
    Element Extraction and Categorization
  url: https://www.aclweb.org/anthology/D19-1237
  year: '2019'
D19-1238:
  abstract: We propose a new framework to uncover the relationship between news events
    and real world phenomena. We present the Predictive Causal Graph (PCG) which allows
    to detect latent relationships between events mentioned in news streams. This
    graph is constructed by measuring how the occurrence of a word in the news influences
    the occurrence of another (set of) word(s) in the future. We show that PCG can
    be used to extract latent features from news streams, outperforming other graph-based
    methods in prediction error of 10 stock price time series for 12 months. We then
    extended PCG to be applicable for longer time windows by allowing time-varying
    factors, leading to stock price prediction error rates between 1.5% and 5% for
    about 4 years. We then manually validated PCG, finding that 67% of the causation
    semantic frame arguments present in the news corpus were directly connected in
    the PCG, the remaining being connected through a semantically relevant intermediate
    node.
  address: Hong Kong, China
  author:
  - first: Ananth
    full: Ananth Balashankar
    id: ananth-balashankar
    last: Balashankar
  - first: Sunandan
    full: Sunandan Chakraborty
    id: sunandan-chakraborty
    last: Chakraborty
  - first: Samuel
    full: Samuel Fraiberger
    id: samuel-fraiberger
    last: Fraiberger
  - first: Lakshminarayanan
    full: Lakshminarayanan Subramanian
    id: lakshminarayanan-subramanian
    last: Subramanian
  author_string: Ananth Balashankar, Sunandan Chakraborty, Samuel Fraiberger, Lakshminarayanan
    Subramanian
  bibkey: balashankar-etal-2019-identifying
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1238
  month: November
  page_first: '2338'
  page_last: '2348'
  pages: "2338\u20132348"
  paper_id: '238'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1238.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1238.jpg
  title: Identifying Predictive Causal Factors from News Streams
  title_html: Identifying Predictive Causal Factors from News Streams
  url: https://www.aclweb.org/anthology/D19-1238
  year: '2019'
D19-1239:
  abstract: Social media provides a timely yet challenging data source for adverse
    drug reaction (ADR) detection. Existing dictionary-based, semi-supervised learning
    approaches are intrinsically limited by the coverage and maintainability of laymen
    health vocabularies. In this paper, we introduce a data augmentation approach
    that leverages variational autoencoders to learn high-quality data distributions
    from a large unlabeled dataset, and subsequently, to automatically generate a
    large labeled training set from a small set of labeled samples. This allows for
    efficient social-media ADR detection with low training and re-training costs to
    adapt to the changes and emergence of informal medical laymen terms. An extensive
    evaluation performed on Twitter and Reddit data shows that our approach matches
    the performance of fully-supervised approaches while requiring only 25% of training
    data.
  address: Hong Kong, China
  author:
  - first: Sepideh
    full: Sepideh Mesbah
    id: sepideh-mesbah
    last: Mesbah
  - first: Jie
    full: Jie Yang
    id: jie-yang
    last: Yang
  - first: Robert-Jan
    full: Robert-Jan Sips
    id: robert-jan-sips
    last: Sips
  - first: Manuel
    full: Manuel Valle Torre
    id: manuel-valle-torre
    last: Valle Torre
  - first: Christoph
    full: Christoph Lofi
    id: christoph-lofi
    last: Lofi
  - first: Alessandro
    full: Alessandro Bozzon
    id: alessandro-bozzon
    last: Bozzon
  - first: Geert-Jan
    full: Geert-Jan Houben
    id: geert-jan-houben
    last: Houben
  author_string: Sepideh Mesbah, Jie Yang, Robert-Jan Sips, Manuel Valle Torre, Christoph
    Lofi, Alessandro Bozzon, Geert-Jan Houben
  bibkey: mesbah-etal-2019-training
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1239
  month: November
  page_first: '2349'
  page_last: '2359'
  pages: "2349\u20132359"
  paper_id: '239'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1239.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1239.jpg
  title: Training Data Augmentation for Detecting Adverse Drug Reactions in User-Generated
    Content
  title_html: Training Data Augmentation for Detecting Adverse Drug Reactions in User-Generated
    Content
  url: https://www.aclweb.org/anthology/D19-1239
  year: '2019'
D19-1240:
  abstract: "User-generated textual data is rich in content and has been used in many\
    \ user behavioral modeling tasks. However, it could also leak user private-attribute\
    \ information that they may not want to disclose such as age and location. User\u2019\
    s privacy concerns mandate data publishers to protect privacy. One effective way\
    \ is to anonymize the textual data. In this paper, we study the problem of textual\
    \ data anonymization and propose a novel Reinforcement Learning-based Text Anonymizor,\
    \ RLTA, which addresses the problem of private-attribute leakage while preserving\
    \ the utility of textual data. Our approach first extracts a latent representation\
    \ of the original text w.r.t. a given task, then leverages deep reinforcement\
    \ learning to automatically learn an optimal strategy for manipulating text representations\
    \ w.r.t. the received privacy and utility feedback. Experiments show the effectiveness\
    \ of this approach in terms of preserving both privacy and utility."
  address: Hong Kong, China
  author:
  - first: Ahmadreza
    full: Ahmadreza Mosallanezhad
    id: ahmadreza-mosallanezhad
    last: Mosallanezhad
  - first: Ghazaleh
    full: Ghazaleh Beigi
    id: ghazaleh-beigi
    last: Beigi
  - first: Huan
    full: Huan Liu
    id: huan-liu
    last: Liu
  author_string: Ahmadreza Mosallanezhad, Ghazaleh Beigi, Huan Liu
  bibkey: mosallanezhad-etal-2019-deep
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1240
  month: November
  page_first: '2360'
  page_last: '2369'
  pages: "2360\u20132369"
  paper_id: '240'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1240.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1240.jpg
  title: Deep Reinforcement Learning-based Text Anonymization against Private-Attribute
    Inference
  title_html: Deep Reinforcement Learning-based Text Anonymization against Private-Attribute
    Inference
  url: https://www.aclweb.org/anthology/D19-1240
  year: '2019'
D19-1241:
  abstract: Automatically solving math word problems is an interesting research topic
    that needs to bridge natural language descriptions and formal math equations.
    Previous studies introduced end-to-end neural network methods, but these approaches
    did not efficiently consider an important characteristic of the equation, i.e.,
    an abstract syntax tree. To address this problem, we propose a tree-structured
    decoding method that generates the abstract syntax tree of the equation in a top-down
    manner. In addition, our approach can automatically stop during decoding without
    a redundant stop token. The experimental results show that our method achieves
    single model state-of-the-art performance on Math23K, which is the largest dataset
    on this task.
  address: Hong Kong, China
  author:
  - first: Qianying
    full: Qianying Liu
    id: qianying-liu
    last: Liu
  - first: Wenyv
    full: Wenyv Guan
    id: wenyv-guan
    last: Guan
  - first: Sujian
    full: Sujian Li
    id: sujian-li
    last: Li
  - first: Daisuke
    full: Daisuke Kawahara
    id: daisuke-kawahara
    last: Kawahara
  author_string: Qianying Liu, Wenyv Guan, Sujian Li, Daisuke Kawahara
  bibkey: liu-etal-2019-tree
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1241
  month: November
  page_first: '2370'
  page_last: '2379'
  pages: "2370\u20132379"
  paper_id: '241'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1241.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1241.jpg
  title: Tree-structured Decoding for Solving Math Word Problems
  title_html: Tree-structured Decoding for Solving Math Word Problems
  url: https://www.aclweb.org/anthology/D19-1241
  year: '2019'
D19-1242:
  abstract: "We consider open-domain question answering (QA) where answers are drawn\
    \ from either a corpus, a knowledge base (KB), or a combination of both of these.\
    \ We focus on a setting in which a corpus is supplemented with a large but incomplete\
    \ KB, and on questions that require non-trivial (e.g., \u201Cmulti-hop\u201D)\
    \ reasoning. We describe PullNet, an integrated framework for (1) learning what\
    \ to retrieve and (2) reasoning with this heterogeneous information to find the\
    \ best answer. PullNet uses an iterative process to construct a question-specific\
    \ subgraph that contains information relevant to the question. In each iteration,\
    \ a graph convolutional network (graph CNN) is used to identify subgraph nodes\
    \ that should be expanded using retrieval (or \u201Cpull\u201D) operations on\
    \ the corpus and/or KB. After the subgraph is complete, another graph CNN is used\
    \ to extract the answer from the subgraph. This retrieve-and-reason process allows\
    \ us to answer multi-hop questions using large KBs and corpora. PullNet is weakly\
    \ supervised, requiring question-answer pairs but not gold inference paths. Experimentally\
    \ PullNet improves over the prior state-of-the art, and in the setting where a\
    \ corpus is used with incomplete KB these improvements are often dramatic. PullNet\
    \ is also often superior to prior systems in a KB-only setting or a text-only\
    \ setting."
  address: Hong Kong, China
  author:
  - first: Haitian
    full: Haitian Sun
    id: haitian-sun
    last: Sun
  - first: Tania
    full: Tania Bedrax-Weiss
    id: tania-bedrax-weiss
    last: Bedrax-Weiss
  - first: William
    full: William Cohen
    id: william-cohen
    last: Cohen
  author_string: Haitian Sun, Tania Bedrax-Weiss, William Cohen
  bibkey: sun-etal-2019-pullnet
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1242
  month: November
  page_first: '2380'
  page_last: '2390'
  pages: "2380\u20132390"
  paper_id: '242'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1242.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1242.jpg
  title: 'PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge
    Bases and Text'
  title_html: '<span class="acl-fixed-case">P</span>ull<span class="acl-fixed-case">N</span>et:
    Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and
    Text'
  url: https://www.aclweb.org/anthology/D19-1242
  year: '2019'
D19-1243:
  abstract: "Understanding narratives requires reading between the lines, which in\
    \ turn, requires interpreting the likely causes and effects of events, even when\
    \ they are not mentioned explicitly. In this paper, we introduce Cosmos QA, a\
    \ large-scale dataset of 35,600 problems that require commonsense-based reading\
    \ comprehension, formulated as multiple-choice questions. In stark contrast to\
    \ most existing reading comprehension datasets where the questions focus on factual\
    \ and literal understanding of the context paragraph, our dataset focuses on reading\
    \ between the lines over a diverse collection of people\u2019s everyday narratives,\
    \ asking such questions as \u201Cwhat might be the possible reason of ...?\",\
    \ or \u201Cwhat would have happened if ...\" that require reasoning beyond the\
    \ exact text spans in the context. To establish baseline performances on Cosmos\
    \ QA, we experiment with several state-of-the-art neural architectures for reading\
    \ comprehension, and also propose a new architecture that improves over the competitive\
    \ baselines. Experimental results demonstrate a significant gap between machine\
    \ (68.4%) and human performance (94%), pointing to avenues for future research\
    \ on commonsense machine comprehension. Dataset, code and leaderboard is publicly\
    \ available at https://wilburone.github.io/cosmos."
  address: Hong Kong, China
  attachment:
  - filename: D19-1243.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1243.Attachment.pdf
  author:
  - first: Lifu
    full: Lifu Huang
    id: lifu-huang
    last: Huang
  - first: Ronan
    full: Ronan Le Bras
    id: ronan-le-bras
    last: Le Bras
  - first: Chandra
    full: Chandra Bhagavatula
    id: chandra-bhagavatula
    last: Bhagavatula
  - first: Yejin
    full: Yejin Choi
    id: yejin-choi
    last: Choi
  author_string: Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi
  bibkey: huang-etal-2019-cosmos
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1243
  month: November
  page_first: '2391'
  page_last: '2401'
  pages: "2391\u20132401"
  paper_id: '243'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1243.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1243.jpg
  title: 'Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning'
  title_html: 'Cosmos <span class="acl-fixed-case">QA</span>: Machine Reading Comprehension
    with Contextual Commonsense Reasoning'
  url: https://www.aclweb.org/anthology/D19-1243
  year: '2019'
D19-1244:
  abstract: 'We propose a system that finds the strongest supporting evidence for
    a given answer to a question, using passage-based question-answering (QA) as a
    testbed. We train evidence agents to select the passage sentences that most convince
    a pretrained QA model of a given answer, if the QA model received those sentences
    instead of the full passage. Rather than finding evidence that convinces one model
    alone, we find that agents select evidence that generalizes; agent-chosen evidence
    increases the plausibility of the supported answer, as judged by other QA models
    and humans. Given its general nature, this approach improves QA in a robust manner:
    using agent-selected evidence (i) humans can correctly answer questions with only
    ~20% of the full passage and (ii) QA models can generalize to longer passages
    and harder questions.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1244.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1244.Attachment.zip
  author:
  - first: Ethan
    full: Ethan Perez
    id: ethan-perez
    last: Perez
  - first: Siddharth
    full: Siddharth Karamcheti
    id: siddharth-karamcheti
    last: Karamcheti
  - first: Rob
    full: Rob Fergus
    id: rob-fergus
    last: Fergus
  - first: Jason
    full: Jason Weston
    id: jason-weston
    last: Weston
  - first: Douwe
    full: Douwe Kiela
    id: douwe-kiela
    last: Kiela
  - first: Kyunghyun
    full: Kyunghyun Cho
    id: kyunghyun-cho
    last: Cho
  author_string: Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe
    Kiela, Kyunghyun Cho
  bibkey: perez-etal-2019-finding
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1244
  month: November
  page_first: '2402'
  page_last: '2411'
  pages: "2402\u20132411"
  paper_id: '244'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1244.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1244.jpg
  title: Finding Generalizable Evidence by Learning to Convince Q&A Models
  title_html: Finding Generalizable Evidence by Learning to Convince Q&amp;A Models
  url: https://www.aclweb.org/anthology/D19-1244
  year: '2019'
D19-1245:
  abstract: Open-domain question answering (OpenQA) aims to answer questions based
    on a number of unlabeled paragraphs. Existing approaches always follow the distantly
    supervised setup where some of the paragraphs are wrong-labeled (noisy), and mainly
    utilize the paragraph-question relevance to denoise. However, the paragraph-paragraph
    relevance, which may aggregate the evidence among relevant paragraphs, can also
    be utilized to discover more useful paragraphs. Moreover, current approaches mainly
    focus on the positive paragraphs which are known to contain the answer during
    training. This will affect the generalization ability of the model and make it
    be disturbed by the similar but irrelevant (distracting) paragraphs during testing.
    In this paper, we first introduce a ranking model leveraging the paragraph-question
    and the paragraph-paragraph relevance to compute a confidence score for each paragraph.
    Furthermore, based on the scores, we design a modified weighted sampling strategy
    for training to mitigate the influence of the noisy and distracting paragraphs.
    Experiments on three public datasets (Quasar-T, SearchQA and TriviaQA) show that
    our model advances the state of the art.
  address: Hong Kong, China
  author:
  - first: Yanfu
    full: Yanfu Xu
    id: yanfu-xu
    last: Xu
  - first: Zheng
    full: Zheng Lin
    id: zheng-lin
    last: Lin
  - first: Yuanxin
    full: Yuanxin Liu
    id: yuanxin-liu
    last: Liu
  - first: Rui
    full: Rui Liu
    id: rui-liu
    last: Liu
  - first: Weiping
    full: Weiping Wang
    id: weiping-wang
    last: Wang
  - first: Dan
    full: Dan Meng
    id: dan-meng
    last: Meng
  author_string: Yanfu Xu, Zheng Lin, Yuanxin Liu, Rui Liu, Weiping Wang, Dan Meng
  bibkey: xu-etal-2019-ranking
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1245
  month: November
  page_first: '2412'
  page_last: '2421'
  pages: "2412\u20132421"
  paper_id: '245'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1245.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1245.jpg
  title: Ranking and Sampling in Open-Domain Question Answering
  title_html: Ranking and Sampling in Open-Domain Question Answering
  url: https://www.aclweb.org/anthology/D19-1245
  year: '2019'
D19-1246:
  abstract: Bilinear diagonal models for knowledge graph embedding (KGE), such as
    DistMult and ComplEx, balance expressiveness and computational efficiency by representing
    relations as diagonal matrices. Although they perform well in predicting atomic
    relations, composite relations (relation paths) cannot be modeled naturally by
    the product of relation matrices, as the product of diagonal matrices is commutative
    and hence invariant with the order of relations. In this paper, we propose a new
    bilinear KGE model, called BlockHolE, based on block circulant matrices. In BlockHolE,
    relation matrices can be non-commutative, allowing composite relations to be modeled
    by matrix product. The model is parameterized in a way that covers a spectrum
    ranging from diagonal to full relation matrices. A fast computation technique
    can be developed on the basis of the duality of the Fourier transform of circulant
    matrices.
  address: Hong Kong, China
  author:
  - first: Katsuhiko
    full: Katsuhiko Hayashi
    id: katsuhiko-hayashi
    last: Hayashi
  - first: Masashi
    full: Masashi Shimbo
    id: masashi-shimbo
    last: Shimbo
  author_string: Katsuhiko Hayashi, Masashi Shimbo
  bibkey: hayashi-shimbo-2019-non
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1246
  month: November
  page_first: '2422'
  page_last: '2430'
  pages: "2422\u20132430"
  paper_id: '246'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1246.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1246.jpg
  title: A Non-commutative Bilinear Model for Answering Path Queries in Knowledge
    Graphs
  title_html: A Non-commutative Bilinear Model for Answering Path Queries in Knowledge
    Graphs
  url: https://www.aclweb.org/anthology/D19-1246
  year: '2019'
D19-1247:
  abstract: 'We tackle the task of question generation over knowledge bases. Conventional
    methods for this task neglect two crucial research issues: 1) the given predicate
    needs to be expressed; 2) the answer to the generated question needs to be definitive.
    In this paper, we strive toward the above two issues via incorporating diversified
    contexts and answer-aware loss. Specifically, we propose a neural encoder-decoder
    model with multi-level copy mechanisms to generate such questions. Furthermore,
    the answer aware loss is introduced to make generated questions corresponding
    to more definitive answers. Experiments demonstrate that our model achieves state-of-the-art
    performance. Meanwhile, such generated question is able to express the given predicate
    and correspond to a definitive answer.'
  address: Hong Kong, China
  author:
  - first: Cao
    full: Cao Liu
    id: cao-liu
    last: Liu
  - first: Kang
    full: Kang Liu
    id: kang-liu
    last: Liu
  - first: Shizhu
    full: Shizhu He
    id: shizhu-he
    last: He
  - first: Zaiqing
    full: Zaiqing Nie
    id: zaiqing-nie
    last: Nie
  - first: Jun
    full: Jun Zhao
    id: jun-zhao
    last: Zhao
  author_string: Cao Liu, Kang Liu, Shizhu He, Zaiqing Nie, Jun Zhao
  bibkey: liu-etal-2019-generating
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1247
  month: November
  page_first: '2431'
  page_last: '2441'
  pages: "2431\u20132441"
  paper_id: '247'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1247.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1247.jpg
  title: Generating Questions for Knowledge Bases via Incorporating Diversified Contexts
    and Answer-Aware Loss
  title_html: Generating Questions for Knowledge Bases via Incorporating Diversified
    Contexts and Answer-Aware Loss
  url: https://www.aclweb.org/anthology/D19-1247
  year: '2019'
D19-1248:
  abstract: 'We consider the problem of conversational question answering over a large-scale
    knowledge base. To handle huge entity vocabulary of a large-scale knowledge base,
    recent neural semantic parsing based approaches usually decompose the task into
    several subtasks and then solve them sequentially, which leads to following issues:
    1) errors in earlier subtasks will be propagated and negatively affect downstream
    ones; and 2) each subtask cannot naturally share supervision signals with others.
    To tackle these issues, we propose an innovative multi-task learning framework
    where a pointer-equipped semantic parsing model is designed to resolve coreference
    in conversations, and naturally empower joint learning with a novel type-aware
    entity detection model. The proposed framework thus enables shared supervisions
    and alleviates the effect of error propagation. Experiments on a large-scale conversational
    question answering dataset containing 1.6M question answering pairs over 12.8M
    entities show that the proposed framework improves overall F1 score from 67% to
    79% compared with previous state-of-the-art work.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1248.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1248.Attachment.pdf
  author:
  - first: Tao
    full: Tao Shen
    id: tao-shen
    last: Shen
  - first: Xiubo
    full: Xiubo Geng
    id: xiubo-geng
    last: Geng
  - first: Tao
    full: Tao Qin
    id: tao-qin
    last: Qin
  - first: Daya
    full: Daya Guo
    id: daya-guo
    last: Guo
  - first: Duyu
    full: Duyu Tang
    id: duyu-tang
    last: Tang
  - first: Nan
    full: Nan Duan
    id: nan-duan
    last: Duan
  - first: Guodong
    full: Guodong Long
    id: guodong-long
    last: Long
  - first: Daxin
    full: Daxin Jiang
    id: daxin-jiang
    last: Jiang
  author_string: Tao Shen, Xiubo Geng, Tao Qin, Daya Guo, Duyu Tang, Nan Duan, Guodong
    Long, Daxin Jiang
  bibkey: shen-etal-2019-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1248
  month: November
  page_first: '2442'
  page_last: '2451'
  pages: "2442\u20132451"
  paper_id: '248'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1248.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1248.jpg
  title: Multi-Task Learning for Conversational Question Answering over a Large-Scale
    Knowledge Base
  title_html: Multi-Task Learning for Conversational Question Answering over a Large-Scale
    Knowledge Base
  url: https://www.aclweb.org/anthology/D19-1248
  year: '2019'
D19-1249:
  abstract: This paper presents BiPaR, a bilingual parallel novel-style machine reading
    comprehension (MRC) dataset, developed to support multilingual and cross-lingual
    reading comprehension. The biggest difference between BiPaR and existing reading
    comprehension datasets is that each triple (Passage, Question, Answer) in BiPaR
    is written parallelly in two languages. We collect 3,667 bilingual parallel paragraphs
    from Chinese and English novels, from which we construct 14,668 parallel question-answer
    pairs via crowdsourced workers following a strict quality control procedure. We
    analyze BiPaR in depth and find that BiPaR offers good diversification in prefixes
    of questions, answer types and relationships between questions and passages. We
    also observe that answering questions of novels requires reading comprehension
    skills of coreference resolution, multi-sentence reasoning, and understanding
    of implicit causality, etc. With BiPaR, we build monolingual, multilingual, and
    cross-lingual MRC baseline models. Even for the relatively simple monolingual
    MRC on this dataset, experiments show that a strong BERT baseline is over 30 points
    behind human in terms of both EM and F1 score, indicating that BiPaR provides
    a challenging testbed for monolingual, multilingual and cross-lingual MRC on novels.
    The dataset is available at https://multinlp.github.io/BiPaR/.
  address: Hong Kong, China
  author:
  - first: Yimin
    full: Yimin Jing
    id: yimin-jing
    last: Jing
  - first: Deyi
    full: Deyi Xiong
    id: deyi-xiong
    last: Xiong
  - first: Zhen
    full: Zhen Yan
    id: zhen-yan
    last: Yan
  author_string: Yimin Jing, Deyi Xiong, Zhen Yan
  bibkey: jing-etal-2019-bipar
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1249
  month: November
  page_first: '2452'
  page_last: '2462'
  pages: "2452\u20132462"
  paper_id: '249'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1249.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1249.jpg
  title: 'BiPaR: A Bilingual Parallel Dataset for Multilingual and Cross-lingual Reading
    Comprehension on Novels'
  title_html: '<span class="acl-fixed-case">B</span>i<span class="acl-fixed-case">P</span>a<span
    class="acl-fixed-case">R</span>: A Bilingual Parallel Dataset for Multilingual
    and Cross-lingual Reading Comprehension on Novels'
  url: https://www.aclweb.org/anthology/D19-1249
  year: '2019'
D19-1250:
  abstract: "Recent progress in pretraining language models on large textual corpora\
    \ led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic\
    \ knowledge, these models may also be storing relational knowledge present in\
    \ the training data, and may be able to answer queries structured as \u201Cfill-in-the-blank\u201D\
    \ cloze statements. Language models have many advantages over structured knowledge\
    \ bases: they require no schema engineering, allow practitioners to query about\
    \ an open class of relations, are easy to extend to more data, and require no\
    \ human supervision to train. We present an in-depth analysis of the relational\
    \ knowledge already present (without fine-tuning) in a wide range of state-of-the-art\
    \ pretrained language models. We find that (i) without fine-tuning, BERT contains\
    \ relational knowledge competitive with traditional NLP methods that have some\
    \ access to oracle knowledge, (ii) BERT also does remarkably well on open-domain\
    \ question answering against a supervised baseline, and (iii) certain types of\
    \ factual knowledge are learned much more readily than others by standard language\
    \ model pretraining approaches. The surprisingly strong ability of these models\
    \ to recall factual knowledge without any fine-tuning demonstrates their potential\
    \ as unsupervised open-domain QA systems. The code to reproduce our analysis is\
    \ available at https://github.com/facebookresearch/LAMA."
  address: Hong Kong, China
  author:
  - first: Fabio
    full: Fabio Petroni
    id: fabio-petroni
    last: Petroni
  - first: Tim
    full: "Tim Rockt\xE4schel"
    id: tim-rocktaschel
    last: "Rockt\xE4schel"
  - first: Sebastian
    full: Sebastian Riedel
    id: sebastian-riedel
    last: Riedel
  - first: Patrick
    full: Patrick Lewis
    id: patrick-lewis
    last: Lewis
  - first: Anton
    full: Anton Bakhtin
    id: anton-bakhtin
    last: Bakhtin
  - first: Yuxiang
    full: Yuxiang Wu
    id: yuxiang-wu
    last: Wu
  - first: Alexander
    full: Alexander Miller
    id: alexander-miller
    last: Miller
  author_string: "Fabio Petroni, Tim Rockt\xE4schel, Sebastian Riedel, Patrick Lewis,\
    \ Anton Bakhtin, Yuxiang Wu, Alexander Miller"
  bibkey: petroni-etal-2019-language
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1250
  month: November
  page_first: '2463'
  page_last: '2473'
  pages: "2463\u20132473"
  paper_id: '250'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1250.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1250.jpg
  title: Language Models as Knowledge Bases?
  title_html: Language Models as Knowledge Bases?
  url: https://www.aclweb.org/anthology/D19-1250
  year: '2019'
D19-1251:
  abstract: "Numerical reasoning, such as addition, subtraction, sorting and counting\
    \ is a critical skill in human\u2019s reading comprehension, which has not been\
    \ well considered in existing machine reading comprehension (MRC) systems. To\
    \ address this issue, we propose a numerical MRC model named as NumNet, which\
    \ utilizes a numerically-aware graph neural network to consider the comparing\
    \ information and performs numerical reasoning over numbers in the question and\
    \ passage. Our system achieves an EM-score of 64.56% on the DROP dataset, outperforming\
    \ all existing machine reading comprehension models by considering the numerical\
    \ relations among numbers."
  address: Hong Kong, China
  attachment:
  - filename: D19-1251.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1251.Attachment.pdf
  author:
  - first: Qiu
    full: Qiu Ran
    id: qiu-ran
    last: Ran
  - first: Yankai
    full: Yankai Lin
    id: yankai-lin
    last: Lin
  - first: Peng
    full: Peng Li
    id: peng-li
    last: Li
  - first: Jie
    full: Jie Zhou
    id: jie-zhou
    last: Zhou
  - first: Zhiyuan
    full: Zhiyuan Liu
    id: zhiyuan-liu
    last: Liu
  author_string: Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, Zhiyuan Liu
  bibkey: ran-etal-2019-numnet
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1251
  month: November
  page_first: '2474'
  page_last: '2484'
  pages: "2474\u20132484"
  paper_id: '251'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1251.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1251.jpg
  title: 'NumNet: Machine Reading Comprehension with Numerical Reasoning'
  title_html: '<span class="acl-fixed-case">N</span>um<span class="acl-fixed-case">N</span>et:
    Machine Reading Comprehension with Numerical Reasoning'
  url: https://www.aclweb.org/anthology/D19-1251
  year: '2019'
D19-1252:
  abstract: 'We present Unicoder, a universal language encoder that is insensitive
    to different languages. Given an arbitrary NLP task, a model can be trained with
    Unicoder using training data in one language and directly applied to inputs of
    the same task in other languages. Comparing to similar efforts such as Multilingual
    BERT and XLM , three new cross-lingual pre-training tasks are proposed, including
    cross-lingual word recovery, cross-lingual paraphrase classification and cross-lingual
    masked language model. These tasks help Unicoder learn the mappings among different
    languages from more perspectives. We also find that doing fine-tuning on multiple
    languages together can bring further improvement. Experiments are performed on
    two tasks: cross-lingual natural language inference (XNLI) and cross-lingual question
    answering (XQA), where XLM is our baseline. On XNLI, 1.8% averaged accuracy improvement
    (on 15 languages) is obtained. On XQA, which is a new cross-lingual dataset built
    by us, 5.5% averaged accuracy improvement (on French and German) is obtained.'
  address: Hong Kong, China
  author:
  - first: Haoyang
    full: Haoyang Huang
    id: haoyang-huang
    last: Huang
  - first: Yaobo
    full: Yaobo Liang
    id: yaobo-liang
    last: Liang
  - first: Nan
    full: Nan Duan
    id: nan-duan
    last: Duan
  - first: Ming
    full: Ming Gong
    id: ming-gong
    last: Gong
  - first: Linjun
    full: Linjun Shou
    id: linjun-shou
    last: Shou
  - first: Daxin
    full: Daxin Jiang
    id: daxin-jiang
    last: Jiang
  - first: Ming
    full: Ming Zhou
    id: ming-zhou
    last: Zhou
  author_string: Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Linjun Shou, Daxin
    Jiang, Ming Zhou
  bibkey: huang-etal-2019-unicoder
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1252
  month: November
  page_first: '2485'
  page_last: '2494'
  pages: "2485\u20132494"
  paper_id: '252'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1252.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1252.jpg
  title: 'Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual
    Tasks'
  title_html: '<span class="acl-fixed-case">U</span>nicoder: A Universal Language
    Encoder by Pre-training with Multiple Cross-lingual Tasks'
  url: https://www.aclweb.org/anthology/D19-1252
  year: '2019'
D19-1253:
  abstract: "Text-based Question Generation (QG) aims at generating natural and relevant\
    \ questions that can be answered by a given answer in some context. Existing QG\
    \ models suffer from a \u201Csemantic drift\u201D problem, i.e., the semantics\
    \ of the model-generated question drifts away from the given context and answer.\
    \ In this paper, we first propose two semantics-enhanced rewards obtained from\
    \ downstream question paraphrasing and question answering tasks to regularize\
    \ the QG model to generate semantically valid questions. Second, since the traditional\
    \ evaluation metrics (e.g., BLEU) often fall short in evaluating the quality of\
    \ generated questions, we propose a QA-based evaluation method which measures\
    \ the QG model\u2019s ability to mimic human annotators in generating QA training\
    \ data. Experiments show that our method achieves the new state-of-the-art performance\
    \ w.r.t. traditional metrics, and also performs best on our QA-based evaluation\
    \ metrics. Further, we investigate how to use our QG model to augment QA datasets\
    \ and enable semi-supervised QA. We propose two ways to generate synthetic QA\
    \ pairs: generate new questions from existing articles or collect QA pairs from\
    \ new articles. We also propose two empirically effective strategies, a data filter\
    \ and mixing mini-batch training, to properly use the QG-generated data for QA.\
    \ Experiments show that our method improves over both BiDAF and BERT QA baselines,\
    \ even without introducing new articles."
  address: Hong Kong, China
  author:
  - first: Shiyue
    full: Shiyue Zhang
    id: shiyue-zhang
    last: Zhang
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  author_string: Shiyue Zhang, Mohit Bansal
  bibkey: zhang-bansal-2019-addressing
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1253
  month: November
  page_first: '2495'
  page_last: '2509'
  pages: "2495\u20132509"
  paper_id: '253'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1253.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1253.jpg
  title: Addressing Semantic Drift in Question Generation for Semi-Supervised Question
    Answering
  title_html: Addressing Semantic Drift in Question Generation for Semi-Supervised
    Question Answering
  url: https://www.aclweb.org/anthology/D19-1253
  year: '2019'
D19-1254:
  abstract: In this paper, we focus on unsupervised domain adaptation for Machine
    Reading Comprehension (MRC), where the source domain has a large amount of labeled
    data, while only unlabeled passages are available in the target domain. To this
    end, we propose an Adversarial Domain Adaptation framework (AdaMRC), where (i)
    pseudo questions are first generated for unlabeled passages in the target domain,
    and then () pseudo questions are first generated for unlabeled passages in the
    target domain, and then (ii) a domain classifier is incorporated into an MRC model
    to predict which domain a given passage-question pair comes from. The classifier
    and the passage-question encoder are jointly trained using adversarial learning
    to enforce domain-invariant representation learning. Comprehensive evaluations
    demonstrate that our approach () a domain classifier is incorporated into an MRC
    model to predict which domain a given passage-question pair comes from. The classifier
    and the passage-question encoder are jointly trained using adversarial learning
    to enforce domain-invariant representation learning. Comprehensive evaluations
    demonstrate that our approach (i) is generalizable to different MRC models and
    datasets, () is generalizable to different MRC models and datasets, (ii) can be
    combined with pre-trained large-scale language models (such as ELMo and BERT),
    and () can be combined with pre-trained large-scale language models (such as ELMo
    and BERT), and (iii) can be extended to semi-supervised learning.) can be extended
    to semi-supervised learning.
  address: Hong Kong, China
  author:
  - first: Huazheng
    full: Huazheng Wang
    id: huazheng-wang
    last: Wang
  - first: Zhe
    full: Zhe Gan
    id: zhe-gan
    last: Gan
  - first: Xiaodong
    full: Xiaodong Liu
    id: xiaodong-liu
    last: Liu
  - first: Jingjing
    full: Jingjing Liu
    id: jingjing-liu
    last: Liu
  - first: Jianfeng
    full: Jianfeng Gao
    id: jianfeng-gao
    last: Gao
  - first: Hongning
    full: Hongning Wang
    id: hongning-wang
    last: Wang
  author_string: Huazheng Wang, Zhe Gan, Xiaodong Liu, Jingjing Liu, Jianfeng Gao,
    Hongning Wang
  bibkey: wang-etal-2019-adversarial
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1254
  month: November
  page_first: '2510'
  page_last: '2520'
  pages: "2510\u20132520"
  paper_id: '254'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1254.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1254.jpg
  title: Adversarial Domain Adaptation for Machine Reading Comprehension
  title_html: Adversarial Domain Adaptation for Machine Reading Comprehension
  url: https://www.aclweb.org/anthology/D19-1254
  year: '2019'
D19-1255:
  abstract: 'Commonsense and background knowledge is required for a QA model to answer
    many nontrivial questions. Different from existing work on knowledge-aware QA,
    we focus on a more challenging task of leveraging external knowledge to generate
    answers in natural language for a given question with context. In this paper,
    we propose a new neural model, Knowledge-Enriched Answer Generator (KEAG), which
    is able to compose a natural answer by exploiting and aggregating evidence from
    all four information sources available: question, passage, vocabulary and knowledge.
    During the process of answer generation, KEAG adaptively determines when to utilize
    symbolic knowledge and which fact from the knowledge is useful. This allows the
    model to exploit external knowledge that is not explicitly stated in the given
    text, but that is relevant for generating an answer. The empirical study on public
    benchmark of answer generation demonstrates that KEAG improves answer quality
    over models without knowledge and existing knowledge-aware models, confirming
    its effectiveness in leveraging knowledge.'
  address: Hong Kong, China
  author:
  - first: Bin
    full: Bin Bi
    id: bin-bi
    last: Bi
  - first: Chen
    full: Chen Wu
    id: chen-wu
    last: Wu
  - first: Ming
    full: Ming Yan
    id: ming-yan
    last: Yan
  - first: Wei
    full: Wei Wang
    id: wei-wang
    last: Wang
  - first: Jiangnan
    full: Jiangnan Xia
    id: jiangnan-xia
    last: Xia
  - first: Chenliang
    full: Chenliang Li
    id: chenliang-li
    last: Li
  author_string: Bin Bi, Chen Wu, Ming Yan, Wei Wang, Jiangnan Xia, Chenliang Li
  bibkey: bi-etal-2019-incorporating
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1255
  month: November
  page_first: '2521'
  page_last: '2530'
  pages: "2521\u20132530"
  paper_id: '255'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1255.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1255.jpg
  title: Incorporating External Knowledge into Machine Reading for Generative Question
    Answering
  title_html: Incorporating External Knowledge into Machine Reading for Generative
    Question Answering
  url: https://www.aclweb.org/anthology/D19-1255
  year: '2019'
D19-1256:
  abstract: 'Answering multiple-choice questions in a setting in which no supporting
    documents are explicitly provided continues to stand as a core problem in natural
    language processing. The contribution of this article is two-fold. First, it describes
    a method which can be used to semantically rank documents extracted from Wikipedia
    or similar natural language corpora. Second, we propose a model employing the
    semantic ranking that holds the first place in two of the most popular leaderboards
    for answering multiple-choice questions: ARC Easy and Challenge. To achieve this,
    we introduce a self-attention based neural network that latently learns to rank
    documents by their importance related to a given question, whilst optimizing the
    objective of predicting the correct answer. These documents are considered relevant
    contexts for the underlying question. We have published the ranked documents so
    that they can be used off-the-shelf to improve downstream decision models.'
  address: Hong Kong, China
  author:
  - first: George Sebastian
    full: George Sebastian Pirtoaca
    id: george-sebastian-pirtoaca
    last: Pirtoaca
  - first: Traian
    full: Traian Rebedea
    id: traian-rebedea
    last: Rebedea
  - first: Stefan
    full: Stefan Ruseti
    id: stefan-ruseti
    last: Ruseti
  author_string: George Sebastian Pirtoaca, Traian Rebedea, Stefan Ruseti
  bibkey: pirtoaca-etal-2019-answering
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1256
  month: November
  page_first: '2531'
  page_last: '2540'
  pages: "2531\u20132540"
  paper_id: '256'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1256.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1256.jpg
  title: Answering questions by learning to rank - Learning to rank by answering questions
  title_html: Answering questions by learning to rank - Learning to rank by answering
    questions
  url: https://www.aclweb.org/anthology/D19-1256
  year: '2019'
D19-1257:
  abstract: In this work, we propose to use linguistic annotations as a basis for
    a Discourse-Aware Semantic Self-Attention encoder that we employ for reading comprehension
    on narrative texts. We extract relations between discourse units, events, and
    their arguments as well as coreferring mentions, using available annotation tools.
    Our empirical evaluation shows that the investigated structures improve the overall
    performance (up to +3.4 Rouge-L), especially intra-sentential and cross-sentential
    discourse relations, sentence-internal semantic role relations, and long-distance
    coreference relations. We show that dedicating self-attention heads to intra-sentential
    relations and relations connecting neighboring sentences is beneficial for finding
    answers to questions in longer contexts. Our findings encourage the use of discourse-semantic
    annotations to enhance the generalization capacity of self-attention models for
    reading comprehension.
  address: Hong Kong, China
  attachment:
  - filename: D19-1257.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1257.Attachment.pdf
  author:
  - first: Todor
    full: Todor Mihaylov
    id: todor-mihaylov
    last: Mihaylov
  - first: Anette
    full: Anette Frank
    id: anette-frank
    last: Frank
  author_string: Todor Mihaylov, Anette Frank
  bibkey: mihaylov-frank-2019-discourse
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1257
  month: November
  page_first: '2541'
  page_last: '2552'
  pages: "2541\u20132552"
  paper_id: '257'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1257.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1257.jpg
  title: Discourse-Aware Semantic Self-Attention for Narrative Reading Comprehension
  title_html: Discourse-Aware Semantic Self-Attention for Narrative Reading Comprehension
  url: https://www.aclweb.org/anthology/D19-1257
  year: '2019'
D19-1258:
  abstract: "Machine Reading at Scale (MRS) is a challenging task in which a system\
    \ is given an input query and is asked to produce a precise output by \u201Creading\u201D\
    \ information from a large knowledge base. The task has gained popularity with\
    \ its natural combination of information retrieval (IR) and machine comprehension\
    \ (MC). Advancements in representation learning have led to separated progress\
    \ in both IR and MC; however, very few studies have examined the relationship\
    \ and combined design of retrieval and comprehension at different levels of granularity,\
    \ for development of MRS systems. In this work, we give general guidelines on\
    \ system design for MRS by proposing a simple yet effective pipeline system with\
    \ special consideration on hierarchical semantic retrieval at both paragraph and\
    \ sentence level, and their potential effects on the downstream task. The system\
    \ is evaluated on both fact verification and open-domain multihop QA, achieving\
    \ state-of-the-art results on the leaderboard test sets of both FEVER and HOTPOTQA.\
    \ To further demonstrate the importance of semantic retrieval, we present ablation\
    \ and analysis studies to quantify the contribution of neural retrieval modules\
    \ at both paragraph-level and sentence-level, and illustrate that intermediate\
    \ semantic retrieval modules are vital for not only effectively filtering upstream\
    \ information and thus saving downstream computation, but also for shaping upstream\
    \ data distribution and providing better data for downstream modeling."
  address: Hong Kong, China
  author:
  - first: Yixin
    full: Yixin Nie
    id: yixin-nie
    last: Nie
  - first: Songhe
    full: Songhe Wang
    id: songhe-wang
    last: Wang
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  author_string: Yixin Nie, Songhe Wang, Mohit Bansal
  bibkey: nie-etal-2019-revealing
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1258
  month: November
  page_first: '2553'
  page_last: '2566'
  pages: "2553\u20132566"
  paper_id: '258'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1258.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1258.jpg
  title: Revealing the Importance of Semantic Retrieval for Machine Reading at Scale
  title_html: Revealing the Importance of Semantic Retrieval for Machine Reading at
    Scale
  url: https://www.aclweb.org/anthology/D19-1258
  year: '2019'
D19-1259:
  abstract: 'We introduce PubMedQA, a novel biomedical question answering (QA) dataset
    collected from PubMed abstracts. The task of PubMedQA is to answer research questions
    with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after
    coronary artery bypass grafting?) using the corresponding abstracts. PubMedQA
    has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially generated QA
    instances. Each PubMedQA instance is composed of (1) a question which is either
    an existing research article title or derived from one, (2) a context which is
    the corresponding abstract without its conclusion, (3) a long answer, which is
    the conclusion of the abstract and, presumably, answers the research question,
    and (4) a yes/no/maybe answer which summarizes the conclusion. PubMedQA is the
    first QA dataset where reasoning over biomedical research texts, especially their
    quantitative contents, is required to answer the questions. Our best performing
    model, multi-phase fine-tuning of BioBERT with long answer bag-of-word statistics
    as additional supervision, achieves 68.1% accuracy, compared to single human performance
    of 78.0% accuracy and majority-baseline of 55.2% accuracy, leaving much room for
    improvement. PubMedQA is publicly available at https://pubmedqa.github.io.'
  address: Hong Kong, China
  author:
  - first: Qiao
    full: Qiao Jin
    id: qiao-jin
    last: Jin
  - first: Bhuwan
    full: Bhuwan Dhingra
    id: bhuwan-dhingra
    last: Dhingra
  - first: Zhengping
    full: Zhengping Liu
    id: zhengping-liu
    last: Liu
  - first: William
    full: William Cohen
    id: william-cohen
    last: Cohen
  - first: Xinghua
    full: Xinghua Lu
    id: xinghua-lu
    last: Lu
  author_string: Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, Xinghua Lu
  bibkey: jin-etal-2019-pubmedqa
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1259
  month: November
  page_first: '2567'
  page_last: '2577'
  pages: "2567\u20132577"
  paper_id: '259'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1259.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1259.jpg
  title: 'PubMedQA: A Dataset for Biomedical Research Question Answering'
  title_html: '<span class="acl-fixed-case">P</span>ub<span class="acl-fixed-case">M</span>ed<span
    class="acl-fixed-case">QA</span>: A Dataset for Biomedical Research Question Answering'
  url: https://www.aclweb.org/anthology/D19-1259
  year: '2019'
D19-1260:
  abstract: "We propose an unsupervised strategy for the selection of justification\
    \ sentences for multi-hop question answering (QA) that (a) maximizes the relevance\
    \ of the selected sentences, (b) minimizes the overlap between the selected facts,\
    \ and (c) maximizes the coverage of both question and answer. This unsupervised\
    \ sentence selection can be coupled with any supervised QA model. We show that\
    \ the sentences selected by our method improve the performance of a state-of-the-art\
    \ supervised QA model on two multi-hop QA datasets: AI2\u2019s Reasoning Challenge\
    \ (ARC) and Multi-Sentence Reading Comprehension (MultiRC). We obtain new state-of-the-art\
    \ performance on both datasets among systems that do not use external resources\
    \ for training the QA system: 56.82% F1 on ARC (41.24% on Challenge and 64.49%\
    \ on Easy) and 26.1% EM0 on MultiRC. Our justification sentences have higher quality\
    \ than the justifications selected by a strong information retrieval baseline,\
    \ e.g., by 5.4% F1 in MultiRC. We also show that our unsupervised selection of\
    \ justification sentences is more stable across domains than a state-of-the-art\
    \ supervised sentence selection method."
  address: Hong Kong, China
  author:
  - first: Vikas
    full: Vikas Yadav
    id: vikas-yadav
    last: Yadav
  - first: Steven
    full: Steven Bethard
    id: steven-bethard
    last: Bethard
  - first: Mihai
    full: Mihai Surdeanu
    id: mihai-surdeanu
    last: Surdeanu
  author_string: Vikas Yadav, Steven Bethard, Mihai Surdeanu
  bibkey: yadav-etal-2019-quick
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1260
  month: November
  page_first: '2578'
  page_last: '2589'
  pages: "2578\u20132589"
  paper_id: '260'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1260.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1260.jpg
  title: 'Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences
    for Multi-hop Question Answering'
  title_html: 'Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences
    for Multi-hop Question Answering'
  url: https://www.aclweb.org/anthology/D19-1260
  year: '2019'
D19-1261:
  abstract: "It is challenging for current one-step retrieve-and-read question answering\
    \ (QA) systems to answer questions like \u201CWhich novel by the author of \u2018\
    Armada\u2019 will be adapted as a feature film by Steven Spielberg?\u201D because\
    \ the question seldom contains retrievable clues about the missing entity (here,\
    \ the author). Answering such a question requires multi-hop reasoning where one\
    \ must gather information about the missing entity (or facts) to proceed with\
    \ further reasoning. We present GoldEn (Gold Entity) Retriever, which iterates\
    \ between reading context and retrieving more supporting documents to answer open-domain\
    \ multi-hop questions. Instead of using opaque and computationally expensive neural\
    \ retrieval models, GoldEn Retriever generates natural language search queries\
    \ given the question and available context, and leverages off-the-shelf information\
    \ retrieval systems to query for missing entities. This allows GoldEn Retriever\
    \ to scale up efficiently for open-domain multi-hop reasoning while maintaining\
    \ interpretability. We evaluate GoldEn Retriever on the recently proposed open-domain\
    \ multi-hop QA dataset, HotpotQA, and demonstrate that it outperforms the best\
    \ previously published model despite not using pretrained language models such\
    \ as BERT."
  address: Hong Kong, China
  author:
  - first: Peng
    full: Peng Qi
    id: peng-qi
    last: Qi
  - first: Xiaowen
    full: Xiaowen Lin
    id: xiaowen-lin
    last: Lin
  - first: Leo
    full: Leo Mehr
    id: leo-mehr
    last: Mehr
  - first: Zijian
    full: Zijian Wang
    id: zijian-wang
    last: Wang
  - first: Christopher D.
    full: Christopher D. Manning
    id: christopher-d-manning
    last: Manning
  author_string: Peng Qi, Xiaowen Lin, Leo Mehr, Zijian Wang, Christopher D. Manning
  bibkey: qi-etal-2019-answering
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1261
  month: November
  page_first: '2590'
  page_last: '2602'
  pages: "2590\u20132602"
  paper_id: '261'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1261.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1261.jpg
  title: Answering Complex Open-domain Questions Through Iterative Query Generation
  title_html: Answering Complex Open-domain Questions Through Iterative Query Generation
  url: https://www.aclweb.org/anthology/D19-1261
  year: '2019'
D19-1262:
  abstract: Generating SQL codes from natural language questions (NL2SQL) is an emerging
    research area. Existing studies have mainly focused on clear scenarios where specified
    information is fully given to generate a SQL query. However, in developer forums
    such as Stack Overflow, questions cover more diverse tasks including table manipulation
    or performance issues, where a table is not specified. The SQL query posted in
    Stack Overflow, Pseudo-SQL (pSQL), does not usually contain table schemas and
    is not necessarily executable, is sufficient to guide developers. Here we describe
    a new NL2pSQL task to generate pSQL codes from natural language questions on under-specified
    database issues, NL2pSQL. In addition, we define two new metrics suitable for
    the proposed NL2pSQL task, Canonical-BLEU and SQL-BLEU, instead of the conventional
    BLEU. With a baseline model using sequence-to-sequence architecture integrated
    by denoising autoencoder, we confirm the validity of our task. Experiments show
    that the proposed NL2pSQL approach yields well-formed queries (up to 43% more
    than a standard Seq2Seq model). Our code and datasets will be publicly released.
  address: Hong Kong, China
  author:
  - first: Fuxiang
    full: Fuxiang Chen
    id: fuxiang-chen
    last: Chen
  - first: Seung-won
    full: Seung-won Hwang
    id: seung-won-hwang
    last: Hwang
  - first: Jaegul
    full: Jaegul Choo
    id: jaegul-choo
    last: Choo
  - first: Jung-Woo
    full: Jung-Woo Ha
    id: jung-woo-ha
    last: Ha
  - first: Sunghun
    full: Sunghun Kim
    id: sunghun-kim
    last: Kim
  author_string: Fuxiang Chen, Seung-won Hwang, Jaegul Choo, Jung-Woo Ha, Sunghun
    Kim
  bibkey: chen-etal-2019-nl2psql
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1262
  month: November
  page_first: '2603'
  page_last: '2613'
  pages: "2603\u20132613"
  paper_id: '262'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1262.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1262.jpg
  title: 'NL2pSQL: Generating Pseudo-SQL Queries from Under-Specified Natural Language
    Questions'
  title_html: '<span class="acl-fixed-case">NL</span>2p<span class="acl-fixed-case">SQL</span>:
    Generating Pseudo-<span class="acl-fixed-case">SQL</span> Queries from Under-Specified
    Natural Language Questions'
  url: https://www.aclweb.org/anthology/D19-1262
  year: '2019'
D19-1263:
  abstract: Formal query generation aims to generate correct executable queries for
    question answering over knowledge bases (KBs), given entity and relation linking
    results. Current approaches build universal paraphrasing or ranking models for
    the whole questions, which are likely to fail in generating queries for complex,
    long-tail questions. In this paper, we propose SubQG, a new query generation approach
    based on frequent query substructures, which helps rank the existing (but nonsignificant)
    query structures or build new query structures. Our experiments on two benchmark
    datasets show that our approach significantly outperforms the existing ones, especially
    for complex questions. Also, it achieves promising performance with limited training
    data and noisy entity/relation linking results.
  address: Hong Kong, China
  author:
  - first: Jiwei
    full: Jiwei Ding
    id: jiwei-ding
    last: Ding
  - first: Wei
    full: Wei Hu
    id: wei-hu
    last: Hu
  - first: Qixin
    full: Qixin Xu
    id: qixin-xu
    last: Xu
  - first: Yuzhong
    full: Yuzhong Qu
    id: yuzhong-qu
    last: Qu
  author_string: Jiwei Ding, Wei Hu, Qixin Xu, Yuzhong Qu
  bibkey: ding-etal-2019-leveraging
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1263
  month: November
  page_first: '2614'
  page_last: '2622'
  pages: "2614\u20132622"
  paper_id: '263'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1263.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1263.jpg
  title: Leveraging Frequent Query Substructures to Generate Formal Queries for Complex
    Question Answering
  title_html: Leveraging Frequent Query Substructures to Generate Formal Queries for
    Complex Question Answering
  url: https://www.aclweb.org/anthology/D19-1263
  year: '2019'
D19-1264:
  abstract: Knowledge Graph (KG) reasoning aims at finding reasoning paths for relations,
    in order to solve the problem of incompleteness in KG. Many previous path-based
    methods like PRA and DeepPath suffer from lacking memory components, or stuck
    in training. Therefore, their performances always rely on well-pretraining. In
    this paper, we present a deep reinforcement learning based model named by AttnPath,
    which incorporates LSTM and Graph Attention Mechanism as the memory components.
    We define two metrics, Mean Selection Rate (MSR) and Mean Replacement Rate (MRR),
    to quantitatively measure how difficult it is to learn the query relations, and
    take advantages of them to fine-tune the model under the framework of reinforcement
    learning. Meanwhile, a novel mechanism of reinforcement learning is proposed by
    forcing an agent to walk forward every step to avoid the agent stalling at the
    same entity node constantly. Based on this operation, the proposed model not only
    can get rid of the pretraining process, but also achieves state-of-the-art performance
    comparing with the other models. We test our model on FB15K-237 and NELL-995 datasets
    with different tasks. Extensive experiments show that our model is effective and
    competitive with many current state-of-the-art methods, and also performs well
    in practice.
  address: Hong Kong, China
  attachment:
  - filename: D19-1264.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1264.Attachment.zip
  author:
  - first: Heng
    full: Heng Wang
    id: heng-wang
    last: Wang
  - first: Shuangyin
    full: Shuangyin Li
    id: shuangyin-li
    last: Li
  - first: Rong
    full: Rong Pan
    id: rong-pan
    last: Pan
  - first: Mingzhi
    full: Mingzhi Mao
    id: mingzhi-mao
    last: Mao
  author_string: Heng Wang, Shuangyin Li, Rong Pan, Mingzhi Mao
  bibkey: wang-etal-2019-incorporating
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1264
  month: November
  page_first: '2623'
  page_last: '2631'
  pages: "2623\u20132631"
  paper_id: '264'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1264.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1264.jpg
  title: Incorporating Graph Attention Mechanism into Knowledge Graph Reasoning Based
    on Deep Reinforcement Learning
  title_html: Incorporating Graph Attention Mechanism into Knowledge Graph Reasoning
    Based on Deep Reinforcement Learning
  url: https://www.aclweb.org/anthology/D19-1264
  year: '2019'
D19-1265:
  abstract: News streams contain rich up-to-date information which can be used to
    update knowledge graphs (KGs). Most current text-based KG updating methods rely
    on elaborately designed information extraction (IE) systems and carefully crafted
    rules, which are often domain-specific and hard to maintain. Besides, such methods
    often hardly pay enough attention to the implicit information that lies underneath
    texts. In this paper, we propose a novel neural network method, GUpdater, to tackle
    these problems. GUpdater is built upon graph neural networks (GNNs) with a text-based
    attention mechanism to guide the updating message passing through the KG structures.
    Experiments on a real-world KG updating dataset show that our model can effectively
    broadcast the news information to the KG structures and perform necessary link-adding
    or link-deleting operations to ensure the KG up-to-date according to news snippets.
  address: Hong Kong, China
  author:
  - first: Jizhi
    full: Jizhi Tang
    id: jizhi-tang
    last: Tang
  - first: Yansong
    full: Yansong Feng
    id: yansong-feng
    last: Feng
  - first: Dongyan
    full: Dongyan Zhao
    id: dongyan-zhao
    last: Zhao
  author_string: Jizhi Tang, Yansong Feng, Dongyan Zhao
  bibkey: tang-etal-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1265
  month: November
  page_first: '2632'
  page_last: '2641'
  pages: "2632\u20132641"
  paper_id: '265'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1265.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1265.jpg
  title: Learning to Update Knowledge Graphs by Reading News
  title_html: Learning to Update Knowledge Graphs by Reading News
  url: https://www.aclweb.org/anthology/D19-1265
  year: '2019'
D19-1266:
  abstract: Knowledge graphs (KGs) often suffer from sparseness and incompleteness.
    Knowledge graph reasoning provides a feasible way to address such problems. Recent
    studies on knowledge graph reasoning have shown that reinforcement learning (RL)
    based methods can provide state-of-the-art performance. However, existing RL-based
    methods require numerous trials for path-finding and rely heavily on meticulous
    reward engineering to fit specific dataset, which is inefficient and laborious
    to apply to fast-evolving KGs. To this end, in this paper, we present DIVINE,
    a novel plug-and-play framework based on generative adversarial imitation learning
    for enhancing existing RL-based methods. DIVINE guides the path-finding process,
    and learns reasoning policies and reward functions self-adaptively through imitating
    the demonstrations automatically sampled from KGs. Experimental results on two
    benchmark datasets show that our framework improves the performance of existing
    RL-based methods while eliminating extra reward engineering.
  address: Hong Kong, China
  author:
  - first: Ruiping
    full: Ruiping Li
    id: ruiping-li
    last: Li
  - first: Xiang
    full: Xiang Cheng
    id: xiang-cheng
    last: Cheng
  author_string: Ruiping Li, Xiang Cheng
  bibkey: li-cheng-2019-divine
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1266
  month: November
  page_first: '2642'
  page_last: '2651'
  pages: "2642\u20132651"
  paper_id: '266'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1266.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1266.jpg
  title: 'DIVINE: A Generative Adversarial Imitation Learning Framework for Knowledge
    Graph Reasoning'
  title_html: '<span class="acl-fixed-case">DIVINE</span>: A Generative Adversarial
    Imitation Learning Framework for Knowledge Graph Reasoning'
  url: https://www.aclweb.org/anthology/D19-1266
  year: '2019'
D19-1267:
  abstract: Sentence matching is a key issue in natural language inference and paraphrase
    identification. Despite the recent progress on multi-layered neural network with
    cross sentence attention, one sentence learns attention to the intermediate representations
    of another sentence, which are propagated from preceding layers and therefore
    are uncertain and unstable for matching, particularly at the risk of error propagation.
    In this paper, we present an original semantics-oriented attention and deep fusion
    network (OSOA-DFN) for sentence matching. Unlike existing models, each attention
    layer of OSOA-DFN is oriented to the original semantic representation of another
    sentence, which captures the relevant information from a fixed matching target.
    The multiple attention layers allow one sentence to repeatedly read the important
    information of another sentence for better matching. We then additionally design
    deep fusion to propagate the attention information at each matching layer. At
    last, we introduce a self-attention mechanism to capture global context to enhance
    attention-aware representation within each sentence. Experiment results on three
    sentence matching benchmark datasets SNLI, SciTail and Quora show that OSOA-DFN
    has the ability to model sentence matching more precisely.
  address: Hong Kong, China
  author:
  - first: Mingtong
    full: Mingtong Liu
    id: mingtong-liu
    last: Liu
  - first: Yujie
    full: Yujie Zhang
    id: yujie-zhang
    last: Zhang
  - first: Jinan
    full: Jinan Xu
    id: jinan-xu
    last: Xu
  - first: Yufeng
    full: Yufeng Chen
    id: yufeng-chen
    last: Chen
  author_string: Mingtong Liu, Yujie Zhang, Jinan Xu, Yufeng Chen
  bibkey: liu-etal-2019-original
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1267
  month: November
  page_first: '2652'
  page_last: '2661'
  pages: "2652\u20132661"
  paper_id: '267'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1267.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1267.jpg
  title: Original Semantics-Oriented Attention and Deep Fusion Network for Sentence
    Matching
  title_html: Original Semantics-Oriented Attention and Deep Fusion Network for Sentence
    Matching
  url: https://www.aclweb.org/anthology/D19-1267
  year: '2019'
D19-1268:
  abstract: Incompleteness is a common problem for existing knowledge graphs (KGs),
    and the completion of KG which aims to predict links between entities is challenging.
    Most existing KG completion methods only consider the direct relation between
    nodes and ignore the relation paths which contain useful information for link
    prediction. Recently, a few methods take relation paths into consideration but
    pay less attention to the order of relations in paths which is important for reasoning.
    In addition, these path-based models always ignore nonlinear contributions of
    path features for link prediction. To solve these problems, we propose a novel
    KG completion method named OPTransE. Instead of embedding both entities of a relation
    into the same latent space as in previous methods, we project the head entity
    and the tail entity of each relation into different spaces to guarantee the order
    of relations in the path. Meanwhile, we adopt a pooling strategy to extract nonlinear
    and complex features of different paths to further improve the performance of
    link prediction. Experimental results on two benchmark datasets show that the
    proposed model OPTransE performs better than state-of-the-art methods.
  address: Hong Kong, China
  author:
  - first: Yao
    full: Yao Zhu
    id: yao-zhu
    last: Zhu
  - first: Hongzhi
    full: Hongzhi Liu
    id: hongzhi-liu
    last: Liu
  - first: Zhonghai
    full: Zhonghai Wu
    id: zhonghai-wu
    last: Wu
  - first: Yang
    full: Yang Song
    id: yang-song
    last: Song
  - first: Tao
    full: Tao Zhang
    id: tao-zhang
    last: Zhang
  author_string: Yao Zhu, Hongzhi Liu, Zhonghai Wu, Yang Song, Tao Zhang
  bibkey: zhu-etal-2019-representation
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1268
  month: November
  page_first: '2662'
  page_last: '2671'
  pages: "2662\u20132671"
  paper_id: '268'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1268.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1268.jpg
  title: Representation Learning with Ordered Relation Paths for Knowledge Graph Completion
  title_html: Representation Learning with Ordered Relation Paths for Knowledge Graph
    Completion
  url: https://www.aclweb.org/anthology/D19-1268
  year: '2019'
D19-1269:
  abstract: "In recent years, there has been a surge of interests in interpretable\
    \ graph reasoning methods. However, these models often suffer from limited performance\
    \ when working on sparse and incomplete graphs, due to the lack of evidential\
    \ paths that can reach target entities. Here we study open knowledge graph reasoning\u2014\
    a task that aims to reason for missing facts over a graph augmented by a background\
    \ text corpus. A key challenge of the task is to filter out \u201Cirrelevant\u201D\
    \ facts extracted from corpus, in order to maintain an effective search space\
    \ during path inference. We propose a novel reinforcement learning framework to\
    \ train two collaborative agents jointly, i.e., a multi-hop graph reasoner and\
    \ a fact extractor. The fact extraction agent generates fact triples from corpora\
    \ to enrich the graph on the fly; while the reasoning agent provides feedback\
    \ to the fact extractor and guides it towards promoting facts that are helpful\
    \ for the interpretable reasoning. Experiments on two public datasets demonstrate\
    \ the effectiveness of the proposed approach."
  address: Hong Kong, China
  attachment:
  - filename: D19-1269.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1269.Attachment.pdf
  author:
  - first: Cong
    full: Cong Fu
    id: cong-fu
    last: Fu
  - first: Tong
    full: Tong Chen
    id: tong-chen
    last: Chen
  - first: Meng
    full: Meng Qu
    id: meng-qu
    last: Qu
  - first: Woojeong
    full: Woojeong Jin
    id: woojeong-jin
    last: Jin
  - first: Xiang
    full: Xiang Ren
    id: xiang-ren
    last: Ren
  author_string: Cong Fu, Tong Chen, Meng Qu, Woojeong Jin, Xiang Ren
  bibkey: fu-etal-2019-collaborative
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1269
  month: November
  page_first: '2672'
  page_last: '2681'
  pages: "2672\u20132681"
  paper_id: '269'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1269.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1269.jpg
  title: Collaborative Policy Learning for Open Knowledge Graph Reasoning
  title_html: Collaborative Policy Learning for Open Knowledge Graph Reasoning
  url: https://www.aclweb.org/anthology/D19-1269
  year: '2019'
D19-1270:
  abstract: 'Understanding event and event-centered commonsense reasoning are crucial
    for natural language processing (NLP). Given an observed event, it is trivial
    for human to infer its intents and effects, while this type of If-Then reasoning
    still remains challenging for NLP systems. To facilitate this, a If-Then commonsense
    reasoning dataset Atomic is proposed, together with an RNN-based Seq2Seq model
    to conduct such reasoning. However, two fundamental problems still need to be
    addressed: first, the intents of an event may be multiple, while the generations
    of RNN-based Seq2Seq models are always semantically close; second, external knowledge
    of the event background may be necessary for understanding events and conducting
    the If-Then reasoning. To address these issues, we propose a novel context-aware
    variational autoencoder effectively learning event background information to guide
    the If-Then reasoning. Experimental results show that our approach improves the
    accuracy and diversity of inferences compared with state-of-the-art baseline methods.'
  address: Hong Kong, China
  author:
  - first: Li
    full: Li Du
    id: li-du
    last: Du
  - first: Xiao
    full: Xiao Ding
    id: xiao-ding
    last: Ding
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  - first: Zhongyang
    full: Zhongyang Li
    id: zhongyang-li
    last: Li
  author_string: Li Du, Xiao Ding, Ting Liu, Zhongyang Li
  bibkey: du-etal-2019-modeling
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1270
  month: November
  page_first: '2682'
  page_last: '2691'
  pages: "2682\u20132691"
  paper_id: '270'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1270.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1270.jpg
  title: Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware
    Variational Autoencoder
  title_html: Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware
    Variational Autoencoder
  url: https://www.aclweb.org/anthology/D19-1270
  year: '2019'
D19-1271:
  abstract: 'Natural language inference aims to predict whether a premise sentence
    can infer another hypothesis sentence. Existing methods typically have framed
    the reasoning problem as a semantic matching task. The both sentences are encoded
    and interacted symmetrically and in parallel. However, in the process of reasoning,
    the role of the two sentences is obviously different, and the sentence pairs for
    NLI are asymmetrical corpora. In this paper, we propose an asynchronous deep interaction
    network (ADIN) to complete the task. ADIN is a neural network structure stacked
    with multiple inference sub-layers, and each sub-layer consists of two local inference
    modules in an asymmetrical manner. Different from previous methods, this model
    deconstructs the reasoning process and implements the asynchronous and multi-step
    reasoning. Experiment results show that ADIN achieves competitive performance
    and outperforms strong baselines on three popular benchmarks: SNLI, MultiNLI,
    and SciTail.'
  address: Hong Kong, China
  author:
  - first: Di
    full: Di Liang
    id: di-liang
    last: Liang
  - first: Fubao
    full: Fubao Zhang
    id: fubao-zhang
    last: Zhang
  - first: Qi
    full: Qi Zhang
    id: qi-zhang
    last: Zhang
  - first: Xuanjing
    full: Xuanjing Huang
    id: xuan-jing-huang
    last: Huang
  author_string: Di Liang, Fubao Zhang, Qi Zhang, Xuanjing Huang
  bibkey: liang-etal-2019-asynchronous
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1271
  month: November
  page_first: '2692'
  page_last: '2700'
  pages: "2692\u20132700"
  paper_id: '271'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1271.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1271.jpg
  title: Asynchronous Deep Interaction Network for Natural Language Inference
  title_html: Asynchronous Deep Interaction Network for Natural Language Inference
  url: https://www.aclweb.org/anthology/D19-1271
  year: '2019'
D19-1272:
  abstract: "In this paper, we present a novel method for measurably adjusting the\
    \ semantics of text while preserving its sentiment and fluency, a task we call\
    \ semantic text exchange. This is useful for text data augmentation and the semantic\
    \ correction of text generated by chatbots and virtual assistants. We introduce\
    \ a pipeline called SMERTI that combines entity replacement, similarity masking,\
    \ and text infilling. We measure our pipeline\u2019s success by its Semantic Text\
    \ Exchange Score (STES): the ability to preserve the original text\u2019s sentiment\
    \ and fluency while adjusting semantic content. We propose to use masking (replacement)\
    \ rate threshold as an adjustable parameter to control the amount of semantic\
    \ change in the text. Our experiments demonstrate that SMERTI can outperform baseline\
    \ models on Yelp reviews, Amazon reviews, and news headlines."
  address: Hong Kong, China
  attachment:
  - filename: D19-1272.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1272.Attachment.zip
  author:
  - first: Steven Y.
    full: Steven Y. Feng
    id: steven-y-feng
    last: Feng
  - first: Aaron W.
    full: Aaron W. Li
    id: aaron-w-li
    last: Li
  - first: Jesse
    full: Jesse Hoey
    id: jesse-hoey
    last: Hoey
  author_string: Steven Y. Feng, Aaron W. Li, Jesse Hoey
  bibkey: feng-etal-2019-keep
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1272
  month: November
  page_first: '2701'
  page_last: '2711'
  pages: "2701\u20132711"
  paper_id: '272'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1272.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1272.jpg
  title: Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic Text
    Exchange
  title_html: Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic
    Text Exchange
  url: https://www.aclweb.org/anthology/D19-1272
  year: '2019'
D19-1273:
  abstract: "The news coverage of events often contains not one but multiple incompatible\
    \ accounts of what happened. We develop a query-based system that extracts compatible\
    \ sets of events (scenarios) from such data, formulated as one-class clustering.\
    \ Our system incrementally evaluates each event\u2019s compatibility with already\
    \ selected events, taking order into account. We use synthetic data consisting\
    \ of article mixtures for scalable training and evaluate our model on a new human-curated\
    \ dataset of scenarios about real-world news topics. Stronger neural network models\
    \ and harder synthetic training settings are both important to achieve high performance,\
    \ and our final scenario construction system substantially outperforms baselines\
    \ based on prior work."
  address: Hong Kong, China
  author:
  - first: Su
    full: Su Wang
    id: su-wang
    last: Wang
  - first: Greg
    full: Greg Durrett
    id: greg-durrett
    last: Durrett
  - first: Katrin
    full: Katrin Erk
    id: katrin-erk
    last: Erk
  author_string: Su Wang, Greg Durrett, Katrin Erk
  bibkey: wang-etal-2019-query
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1273
  month: November
  page_first: '2712'
  page_last: '2722'
  pages: "2712\u20132722"
  paper_id: '273'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1273.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1273.jpg
  title: Query-focused Scenario Construction
  title_html: Query-focused Scenario Construction
  url: https://www.aclweb.org/anthology/D19-1273
  year: '2019'
D19-1274:
  abstract: 'Entity alignment aims at integrating complementary knowledge graphs (KGs)
    from different sources or languages, which may benefit many knowledge-driven applications.
    It is challenging due to the heterogeneity of KGs and limited seed alignments.
    In this paper, we propose a semi-supervised entity alignment method by joint Knowledge
    Embedding model and Cross-Graph model (KECG). It can make better use of seed alignments
    to propagate over the entire graphs with KG-based constraints. Specifically, as
    for the knowledge embedding model, we utilize TransE to implicitly complete two
    KGs towards consistency and learn relational constraints between entities. As
    for the cross-graph model, we extend Graph Attention Network (GAT) with projection
    constraint to robustly encode graphs, and two KGs share the same GAT to transfer
    structural knowledge as well as to ignore unimportant neighbors for alignment
    via attention mechanism. Results on publicly available datasets as well as further
    analysis demonstrate the effectiveness of KECG. Our codes can be found in https:
    //github.com/THU-KEG/KECG.'
  address: Hong Kong, China
  author:
  - first: Chengjiang
    full: Chengjiang Li
    id: chengjiang-li
    last: Li
  - first: Yixin
    full: Yixin Cao
    id: yixin-cao
    last: Cao
  - first: Lei
    full: Lei Hou
    id: lei-hou
    last: Hou
  - first: Jiaxin
    full: Jiaxin Shi
    id: jiaxin-shi
    last: Shi
  - first: Juanzi
    full: Juanzi Li
    id: juanzi-li
    last: Li
  - first: Tat-Seng
    full: Tat-Seng Chua
    id: tat-seng-chua
    last: Chua
  author_string: Chengjiang Li, Yixin Cao, Lei Hou, Jiaxin Shi, Juanzi Li, Tat-Seng
    Chua
  bibkey: li-etal-2019-semi
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1274
  month: November
  page_first: '2723'
  page_last: '2732'
  pages: "2723\u20132732"
  paper_id: '274'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1274.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1274.jpg
  title: Semi-supervised Entity Alignment via Joint Knowledge Embedding Model and
    Cross-graph Model
  title_html: Semi-supervised Entity Alignment via Joint Knowledge Embedding Model
    and Cross-graph Model
  url: https://www.aclweb.org/anthology/D19-1274
  year: '2019'
D19-1275:
  abstract: "Probes, supervised models trained to predict properties (like parts-of-speech)\
    \ from representations (like ELMo), have achieved high accuracy on a range of\
    \ linguistic tasks. But does this mean that the representations encode linguistic\
    \ structure or just that the probe has learned the linguistic task? In this paper,\
    \ we propose control tasks, which associate word types with random outputs, to\
    \ complement linguistic tasks. By construction, these tasks can only be learned\
    \ by the probe itself. So a good probe, (one that reflects the representation),\
    \ should be selective, achieving high linguistic task accuracy and low control\
    \ task accuracy. The selectivity of a probe puts linguistic task accuracy in context\
    \ with the probe\u2019s capacity to memorize from word types. We construct control\
    \ tasks for English part-of-speech tagging and dependency edge prediction, and\
    \ show that popular probes on ELMo representations are not selective. We also\
    \ find that dropout, commonly used to control probe complexity, is ineffective\
    \ for improving selectivity of MLPs, but that other forms of regularization are\
    \ effective. Finally, we find that while probes on the first layer of ELMo yield\
    \ slightly better part-of-speech tagging accuracy than the second, probes on the\
    \ second layer are substantially more selective, which raises the question of\
    \ which layer better represents parts-of-speech."
  address: Hong Kong, China
  author:
  - first: John
    full: John Hewitt
    id: john-hewitt
    last: Hewitt
  - first: Percy
    full: Percy Liang
    id: percy-liang
    last: Liang
  author_string: John Hewitt, Percy Liang
  bibkey: hewitt-liang-2019-designing
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1275
  month: November
  page_first: '2733'
  page_last: '2743'
  pages: "2733\u20132743"
  paper_id: '275'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1275.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1275.jpg
  title: Designing and Interpreting Probes with Control Tasks
  title_html: Designing and Interpreting Probes with Control Tasks
  url: https://www.aclweb.org/anthology/D19-1275
  year: '2019'
D19-1276:
  abstract: 'Pre-trained word embeddings like ELMo and BERT contain rich syntactic
    and semantic information, resulting in state-of-the-art performance on various
    tasks. We propose a very fast variational information bottleneck (VIB) method
    to nonlinearly compress these embeddings, keeping only the information that helps
    a discriminative parser. We compress each word embedding to either a discrete
    tag or a continuous vector. In the discrete version, our automatically compressed
    tags form an alternative tag set: we show experimentally that our tags capture
    most of the information in traditional POS tag annotations, but our tag sequences
    can be parsed more accurately at the same level of tag granularity. In the continuous
    version, we show experimentally that moderately compressing the word embeddings
    by our method yields a more accurate parser in 8 of 9 languages, unlike simple
    dimensionality reduction.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1276.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1276.Attachment.pdf
  author:
  - first: Xiang Lisa
    full: Xiang Lisa Li
    id: xiang-lisa-li
    last: Li
  - first: Jason
    full: Jason Eisner
    id: jason-eisner
    last: Eisner
  author_string: Xiang Lisa Li, Jason Eisner
  bibkey: li-eisner-2019-specializing
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1276
  month: November
  page_first: '2744'
  page_last: '2754'
  pages: "2744\u20132754"
  paper_id: '276'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1276.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1276.jpg
  title: Specializing Word Embeddings (for Parsing) by Information Bottleneck
  title_html: Specializing Word Embeddings (for Parsing) by Information Bottleneck
  url: https://www.aclweb.org/anthology/D19-1276
  year: '2019'
D19-1277:
  abstract: 'Transition-based and graph-based dependency parsers have previously been
    shown to have complementary strengths and weaknesses: transition-based parsers
    exploit rich structural features but suffer from error propagation, while graph-based
    parsers benefit from global optimization but have restricted feature scope. In
    this paper, we show that, even though some details of the picture have changed
    after the switch to neural networks and continuous representations, the basic
    trade-off between rich features and global optimization remains essentially the
    same. Moreover, we show that deep contextualized word embeddings, which allow
    parsers to pack information about global sentence structure into local feature
    representations, benefit transition-based parsers more than graph-based parsers,
    making the two approaches virtually equivalent in terms of both accuracy and error
    profile. We argue that the reason is that these representations help prevent search
    errors and thereby allow transition-based parsers to better exploit their inherent
    strength of making accurate local decisions. We support this explanation by an
    error analysis of parsing experiments on 13 languages.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1277.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1277.Attachment.zip
  author:
  - first: Artur
    full: Artur Kulmizev
    id: artur-kulmizev
    last: Kulmizev
  - first: Miryam
    full: Miryam de Lhoneux
    id: miryam-de-lhoneux
    last: de Lhoneux
  - first: Johannes
    full: Johannes Gontrum
    id: johannes-gontrum
    last: Gontrum
  - first: Elena
    full: Elena Fano
    id: elena-fano
    last: Fano
  - first: Joakim
    full: Joakim Nivre
    id: joakim-nivre
    last: Nivre
  author_string: Artur Kulmizev, Miryam de Lhoneux, Johannes Gontrum, Elena Fano,
    Joakim Nivre
  bibkey: kulmizev-etal-2019-deep
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1277
  month: November
  page_first: '2755'
  page_last: '2768'
  pages: "2755\u20132768"
  paper_id: '277'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1277.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1277.jpg
  title: Deep Contextualized Word Embeddings in Transition-Based and Graph-Based Dependency
    Parsing - A Tale of Two Parsers Revisited
  title_html: Deep Contextualized Word Embeddings in Transition-Based and Graph-Based
    Dependency Parsing - A Tale of Two Parsers Revisited
  url: https://www.aclweb.org/anthology/D19-1277
  year: '2019'
D19-1278:
  abstract: "Semantic parses are directed acyclic graphs (DAGs), so semantic parsing\
    \ should be modeled as graph prediction. But predicting graphs presents difficult\
    \ technical challenges, so it is simpler and more common to predict the *linearized*\
    \ graphs found in semantic parsing datasets using well-understood sequence models.\
    \ The cost of this simplicity is that the predicted strings may not be well-formed\
    \ graphs. We present recurrent neural network DAG grammars, a graph-aware sequence\
    \ model that generates only well-formed graphs while sidestepping many difficulties\
    \ in graph prediction. We test our model on the Parallel Meaning Bank\u2014a multilingual\
    \ semantic graphbank. Our approach yields competitive results in English and establishes\
    \ the first results for German, Italian and Dutch."
  address: Hong Kong, China
  attachment:
  - filename: D19-1278.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1278.Attachment.zip
  author:
  - first: Federico
    full: Federico Fancellu
    id: federico-fancellu
    last: Fancellu
  - first: Sorcha
    full: Sorcha Gilroy
    id: sorcha-gilroy
    last: Gilroy
  - first: Adam
    full: Adam Lopez
    id: adam-lopez
    last: Lopez
  - first: Mirella
    full: Mirella Lapata
    id: mirella-lapata
    last: Lapata
  author_string: Federico Fancellu, Sorcha Gilroy, Adam Lopez, Mirella Lapata
  bibkey: fancellu-etal-2019-semantic
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1278
  month: November
  page_first: '2769'
  page_last: '2778'
  pages: "2769\u20132778"
  paper_id: '278'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1278.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1278.jpg
  title: Semantic graph parsing with recurrent neural network DAG grammars
  title_html: Semantic graph parsing with recurrent neural network <span class="acl-fixed-case">DAG</span>
    grammars
  url: https://www.aclweb.org/anthology/D19-1278
  year: '2019'
D19-1279:
  abstract: We present UDify, a multilingual multi-task model capable of accurately
    predicting universal part-of-speech, morphological features, lemmas, and dependency
    trees simultaneously for all 124 Universal Dependencies treebanks across 75 languages.
    By leveraging a multilingual BERT self-attention model pretrained on 104 languages,
    we found that fine-tuning it on all datasets concatenated together with simple
    softmax classifiers for each UD task can meet or exceed state-of-the-art UPOS,
    UFeats, Lemmas, (and especially) UAS, and LAS scores, without requiring any recurrent
    or language-specific components. We evaluate UDify for multilingual learning,
    showing that low-resource languages benefit the most from cross-linguistic annotations.
    We also evaluate for zero-shot learning, with results suggesting that multilingual
    training provides strong UD predictions even for languages that neither UDify
    nor BERT have ever been trained on.
  address: Hong Kong, China
  author:
  - first: Dan
    full: Dan Kondratyuk
    id: dan-kondratyuk
    last: Kondratyuk
  - first: Milan
    full: Milan Straka
    id: milan-straka
    last: Straka
  author_string: Dan Kondratyuk, Milan Straka
  bibkey: kondratyuk-straka-2019-75
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1279
  month: November
  page_first: '2779'
  page_last: '2795'
  pages: "2779\u20132795"
  paper_id: '279'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1279.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1279.jpg
  title: '75 Languages, 1 Model: Parsing Universal Dependencies Universally'
  title_html: '75 Languages, 1 Model: Parsing Universal Dependencies Universally'
  url: https://www.aclweb.org/anthology/D19-1279
  year: '2019'
D19-1280:
  abstract: 'Humans observe and interact with the world to acquire knowledge. However,
    most existing machine reading comprehension (MRC) tasks miss the interactive,
    information-seeking component of comprehension. Such tasks present models with
    static documents that contain all necessary information, usually concentrated
    in a single short substring. Thus, models can achieve strong performance through
    simple word- and phrase-based pattern matching. We address this problem by formulating
    a novel text-based question answering task: Question Answering with Interactive
    Text (QAit). In QAit, an agent must interact with a partially observable text-based
    environment to gather information required to answer questions. QAit poses questions
    about the existence, location, and attributes of objects found in the environment.
    The data is built using a text-based game generator that defines the underlying
    dynamics of interaction with the environment. We propose and evaluate a set of
    baseline models for the QAit task that includes deep reinforcement learning agents.
    Experiments show that the task presents a major challenge for machine reading
    systems, while humans solve it with relative ease.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1280.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1280.Attachment.zip
  author:
  - first: Xingdi
    full: Xingdi Yuan
    id: xingdi-yuan
    last: Yuan
  - first: Marc-Alexandre
    full: "Marc-Alexandre C\xF4t\xE9"
    id: marc-alexandre-cote
    last: "C\xF4t\xE9"
  - first: Jie
    full: Jie Fu
    id: jie-fu
    last: Fu
  - first: Zhouhan
    full: Zhouhan Lin
    id: zhouhan-lin
    last: Lin
  - first: Chris
    full: Chris Pal
    id: christopher-pal
    last: Pal
  - first: Yoshua
    full: Yoshua Bengio
    id: yoshua-bengio
    last: Bengio
  - first: Adam
    full: Adam Trischler
    id: adam-trischler
    last: Trischler
  author_string: "Xingdi Yuan, Marc-Alexandre C\xF4t\xE9, Jie Fu, Zhouhan Lin, Chris\
    \ Pal, Yoshua Bengio, Adam Trischler"
  bibkey: yuan-etal-2019-interactive
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1280
  month: November
  page_first: '2796'
  page_last: '2813'
  pages: "2796\u20132813"
  paper_id: '280'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1280.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1280.jpg
  title: Interactive Language Learning by Question Answering
  title_html: Interactive Language Learning by Question Answering
  url: https://www.aclweb.org/anthology/D19-1280
  year: '2019'
D19-1281:
  abstract: "Multi-hop textual question answering requires combining information from\
    \ multiple sentences. We focus on a natural setting where, unlike typical reading\
    \ comprehension, only partial information is provided with each question. The\
    \ model must retrieve and use additional knowledge to correctly answer the question.\
    \ To tackle this challenge, we develop a novel approach that explicitly identifies\
    \ the knowledge gap between a key span in the provided knowledge and the answer\
    \ choices. The model, GapQA, learns to fill this gap by determining the relationship\
    \ between the span and an answer choice, based on retrieved knowledge targeting\
    \ this gap. We propose jointly training a model to simultaneously fill this knowledge\
    \ gap and compose it with the provided partial knowledge. On the OpenBookQA dataset,\
    \ given partial knowledge, explicitly identifying what\u2019s missing substantially\
    \ outperforms previous approaches."
  address: Hong Kong, China
  attachment:
  - filename: D19-1281.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1281.Attachment.pdf
  author:
  - first: Tushar
    full: Tushar Khot
    id: tushar-khot
    last: Khot
  - first: Ashish
    full: Ashish Sabharwal
    id: ashish-sabharwal
    last: Sabharwal
  - first: Peter
    full: Peter Clark
    id: peter-clark
    last: Clark
  author_string: Tushar Khot, Ashish Sabharwal, Peter Clark
  bibkey: khot-etal-2019-whats
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1281
  month: November
  page_first: '2814'
  page_last: '2828'
  pages: "2814\u20132828"
  paper_id: '281'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1281.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1281.jpg
  title: "What\u2019s Missing: A Knowledge Gap Guided Approach for Multi-hop Question\
    \ Answering"
  title_html: "What\u2019s Missing: A Knowledge Gap Guided Approach for Multi-hop\
    \ Question Answering"
  url: https://www.aclweb.org/anthology/D19-1281
  year: '2019'
D19-1282:
  abstract: Commonsense reasoning aims to empower machines with the human ability
    to make presumptions about ordinary situations in our daily life. In this paper,
    we propose a textual inference framework for answering commonsense questions,
    which effectively utilizes external, structured commonsense knowledge graphs to
    perform explainable inferences. The framework first grounds a question-answer
    pair from the semantic space to the knowledge-based symbolic space as a schema
    graph, a related sub-graph of external knowledge graphs. It represents schema
    graphs with a novel knowledge-aware graph network module named KagNet, and finally
    scores answers with graph representations. Our model is based on graph convolutional
    networks and LSTMs, with a hierarchical path-based attention mechanism. The intermediate
    attention scores make it transparent and interpretable, which thus produce trustworthy
    inferences. Using ConceptNet as the only external resource for Bert-based models,
    we achieved state-of-the-art performance on the CommonsenseQA, a large-scale dataset
    for commonsense reasoning.
  address: Hong Kong, China
  author:
  - first: Bill Yuchen
    full: Bill Yuchen Lin
    id: bill-yuchen-lin
    last: Lin
  - first: Xinyue
    full: Xinyue Chen
    id: xinyue-chen
    last: Chen
  - first: Jamin
    full: Jamin Chen
    id: jamin-chen
    last: Chen
  - first: Xiang
    full: Xiang Ren
    id: xiang-ren
    last: Ren
  author_string: Bill Yuchen Lin, Xinyue Chen, Jamin Chen, Xiang Ren
  bibkey: lin-etal-2019-kagnet
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1282
  month: November
  page_first: '2829'
  page_last: '2839'
  pages: "2829\u20132839"
  paper_id: '282'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1282.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1282.jpg
  title: 'KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning'
  title_html: '<span class="acl-fixed-case">K</span>ag<span class="acl-fixed-case">N</span>et:
    Knowledge-Aware Graph Networks for Commonsense Reasoning'
  url: https://www.aclweb.org/anthology/D19-1282
  year: '2019'
D19-1283:
  abstract: This paper studies the problem of supporting question answering in a new
    language with limited training resources. As an extreme scenario, when no such
    resource exists, one can (1) transfer labels from another language, and (2) generate
    labels from unlabeled data, using translator and automatic labeling function respectively.
    However, these approaches inevitably introduce noises to the training data, due
    to translation or generation errors, which require a judicious use of data with
    varying confidence. To address this challenge, we propose a weakly-supervised
    framework that quantifies such noises from automatically generated labels, to
    deemphasize or fix noisy data in training. On reading comprehension task, we demonstrate
    the effectiveness of our model on low-resource languages with varying similarity
    to English, namely, Korean and French.
  address: Hong Kong, China
  author:
  - first: Kyungjae
    full: Kyungjae Lee
    id: kyungjae-lee
    last: Lee
  - first: Sunghyun
    full: Sunghyun Park
    id: sunghyun-park
    last: Park
  - first: Hojae
    full: Hojae Han
    id: hojae-han
    last: Han
  - first: Jinyoung
    full: Jinyoung Yeo
    id: jinyoung-yeo
    last: Yeo
  - first: Seung-won
    full: Seung-won Hwang
    id: seung-won-hwang
    last: Hwang
  - first: Juho
    full: Juho Lee
    id: juho-lee
    last: Lee
  author_string: Kyungjae Lee, Sunghyun Park, Hojae Han, Jinyoung Yeo, Seung-won Hwang,
    Juho Lee
  bibkey: lee-etal-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1283
  month: November
  page_first: '2840'
  page_last: '2850'
  pages: "2840\u20132850"
  paper_id: '283'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1283.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1283.jpg
  title: Learning with Limited Data for Multilingual Reading Comprehension
  title_html: Learning with Limited Data for Multilingual Reading Comprehension
  url: https://www.aclweb.org/anthology/D19-1283
  year: '2019'
D19-1284:
  abstract: "Many question answering (QA) tasks only provide weak supervision for\
    \ how the answer should be computed. For example, TriviaQA answers are entities\
    \ that can be mentioned multiple times in supporting documents, while DROP answers\
    \ can be computed by deriving many different equations from numbers in the reference\
    \ text. In this paper, we show it is possible to convert such tasks into discrete\
    \ latent variable learning problems with a precomputed, task-specific set of possible\
    \ solutions (e.g. different mentions or equations) that contains one correct option.\
    \ We then develop a hard EM learning scheme that computes gradients relative to\
    \ the most likely solution at each update. Despite its simplicity, we show that\
    \ this approach significantly outperforms previous methods on six QA tasks, including\
    \ absolute gains of 2\u201310%, and achieves the state-of-the-art on five of them.\
    \ Using hard updates instead of maximizing marginal likelihood is key to these\
    \ results as it encourages the model to find the one correct answer, which we\
    \ show through detailed qualitative analysis."
  address: Hong Kong, China
  author:
  - first: Sewon
    full: Sewon Min
    id: sewon-min
    last: Min
  - first: Danqi
    full: Danqi Chen
    id: danqi-chen
    last: Chen
  - first: Hannaneh
    full: Hannaneh Hajishirzi
    id: hannaneh-hajishirzi
    last: Hajishirzi
  - first: Luke
    full: Luke Zettlemoyer
    id: luke-zettlemoyer
    last: Zettlemoyer
  author_string: Sewon Min, Danqi Chen, Hannaneh Hajishirzi, Luke Zettlemoyer
  bibkey: min-etal-2019-discrete
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1284
  month: November
  page_first: '2851'
  page_last: '2864'
  pages: "2851\u20132864"
  paper_id: '284'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1284.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1284.jpg
  title: A Discrete Hard EM Approach for Weakly Supervised Question Answering
  title_html: A Discrete Hard <span class="acl-fixed-case">EM</span> Approach for
    Weakly Supervised Question Answering
  url: https://www.aclweb.org/anthology/D19-1284
  year: '2019'
D19-1285:
  abstract: "This work aims at modeling how the meaning of gradable adjectives of\
    \ size (\u2018big\u2019, \u2018small\u2019) can be learned from visually-grounded\
    \ contexts. Inspired by cognitive and linguistic evidence showing that the use\
    \ of these expressions relies on setting a threshold that is dependent on a specific\
    \ context, we investigate the ability of multi-modal models in assessing whether\
    \ an object is \u2018big\u2019 or \u2018small\u2019 in a given visual scene. In\
    \ contrast with the standard computational approach that simplistically treats\
    \ gradable adjectives as \u2018fixed\u2019 attributes, we pose the problem as\
    \ relational: to be successful, a model has to consider the full visual context.\
    \ By means of four main tasks, we show that state-of-the-art models (but not a\
    \ relatively strong baseline) can learn the function subtending the meaning of\
    \ size adjectives, though their performance is found to decrease while moving\
    \ from simple to more complex tasks. Crucially, models fail in developing abstract\
    \ representations of gradable adjectives that can be used compositionally."
  address: Hong Kong, China
  author:
  - first: Sandro
    full: Sandro Pezzelle
    id: sandro-pezzelle
    last: Pezzelle
  - first: Raquel
    full: "Raquel Fern\xE1ndez"
    id: raquel-fernandez
    last: "Fern\xE1ndez"
  author_string: "Sandro Pezzelle, Raquel Fern\xE1ndez"
  bibkey: pezzelle-fernandez-2019-red
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1285
  month: November
  page_first: '2865'
  page_last: '2876'
  pages: "2865\u20132876"
  paper_id: '285'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1285.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1285.jpg
  title: 'Is the Red Square Big? MALeViC: Modeling Adjectives Leveraging Visual Contexts'
  title_html: 'Is the Red Square Big? <span class="acl-fixed-case">MAL</span>e<span
    class="acl-fixed-case">V</span>i<span class="acl-fixed-case">C</span>: Modeling
    Adjectives Leveraging Visual Contexts'
  url: https://www.aclweb.org/anthology/D19-1285
  year: '2019'
D19-1286:
  abstract: "Though state-of-the-art sentence representation models can perform tasks\
    \ requiring significant knowledge of grammar, it is an open question how best\
    \ to evaluate their grammatical knowledge. We explore five experimental methods\
    \ inspired by prior work evaluating pretrained sentence representation models.\
    \ We use a single linguistic phenomenon, negative polarity item (NPI) licensing,\
    \ as a case study for our experiments. NPIs like any are grammatical only if they\
    \ appear in a licensing environment like negation (Sue doesn\u2019t have any cats\
    \ vs. *Sue has any cats). This phenomenon is challenging because of the variety\
    \ of NPI licensing environments that exist. We introduce an artificially generated\
    \ dataset that manipulates key features of NPI licensing for the experiments.\
    \ We find that BERT has significant knowledge of these features, but its success\
    \ varies widely across different experimental methods. We conclude that a variety\
    \ of methods is necessary to reveal all relevant aspects of a model\u2019s grammatical\
    \ knowledge in a given domain."
  address: Hong Kong, China
  attachment:
  - filename: D19-1286.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1286.Attachment.pdf
  author:
  - first: Alex
    full: Alex Warstadt
    id: alex-warstadt
    last: Warstadt
  - first: Yu
    full: Yu Cao
    id: yu-cao
    last: Cao
  - first: Ioana
    full: Ioana Grosu
    id: ioana-grosu
    last: Grosu
  - first: Wei
    full: Wei Peng
    id: wei-peng
    last: Peng
  - first: Hagen
    full: Hagen Blix
    id: hagen-blix
    last: Blix
  - first: Yining
    full: Yining Nie
    id: yining-nie
    last: Nie
  - first: Anna
    full: Anna Alsop
    id: anna-alsop
    last: Alsop
  - first: Shikha
    full: Shikha Bordia
    id: shikha-bordia
    last: Bordia
  - first: Haokun
    full: Haokun Liu
    id: haokun-liu
    last: Liu
  - first: Alicia
    full: Alicia Parrish
    id: alicia-parrish
    last: Parrish
  - first: Sheng-Fu
    full: Sheng-Fu Wang
    id: sheng-fu-wang
    last: Wang
  - first: Jason
    full: Jason Phang
    id: jason-phang
    last: Phang
  - first: Anhad
    full: Anhad Mohananey
    id: anhad-mohananey
    last: Mohananey
  - first: Phu Mon
    full: Phu Mon Htut
    id: phu-mon-htut
    last: Htut
  - first: Paloma
    full: Paloma Jeretic
    id: paloma-jeretic
    last: Jeretic
  - first: Samuel R.
    full: Samuel R. Bowman
    id: samuel-bowman
    last: Bowman
  author_string: Alex Warstadt, Yu Cao, Ioana Grosu, Wei Peng, Hagen Blix, Yining
    Nie, Anna Alsop, Shikha Bordia, Haokun Liu, Alicia Parrish, Sheng-Fu Wang, Jason
    Phang, Anhad Mohananey, Phu Mon Htut, Paloma Jeretic, Samuel R. Bowman
  bibkey: warstadt-etal-2019-investigating
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1286
  month: November
  page_first: '2877'
  page_last: '2887'
  pages: "2877\u20132887"
  paper_id: '286'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1286.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1286.jpg
  title: "Investigating BERT\u2019s Knowledge of Language: Five Analysis Methods with\
    \ NPIs"
  title_html: "Investigating <span class=\"acl-fixed-case\">BERT</span>\u2019s Knowledge\
    \ of Language: Five Analysis Methods with <span class=\"acl-fixed-case\">NPI</span>s"
  url: https://www.aclweb.org/anthology/D19-1286
  year: '2019'
D19-1287:
  abstract: "Neural language models have achieved state-of-the-art performances on\
    \ many NLP tasks, and recently have been shown to learn a number of hierarchically-sensitive\
    \ syntactic dependencies between individual words. However, equally important\
    \ for language processing is the ability to combine words into phrasal constituents,\
    \ and use constituent-level features to drive downstream expectations. Here we\
    \ investigate neural models\u2019 ability to represent constituent-level features,\
    \ using coordinated noun phrases as a case study. We assess whether different\
    \ neural language models trained on English and French represent phrase-level\
    \ number and gender features, and use those features to drive downstream expectations.\
    \ Our results suggest that models use a linear combination of NP constituent number\
    \ to drive CoordNP/verb number agreement. This behavior is highly regular and\
    \ even sensitive to local syntactic context, however it differs crucially from\
    \ observed human behavior. Models have less success with gender agreement. Models\
    \ trained on large corpora perform best, and there is no obvious advantage for\
    \ models trained using explicit syntactic supervision."
  address: Hong Kong, China
  author:
  - first: Aixiu
    full: Aixiu An
    id: aixiu-an
    last: An
  - first: Peng
    full: Peng Qian
    id: peng-qian
    last: Qian
  - first: Ethan
    full: Ethan Wilcox
    id: ethan-wilcox
    last: Wilcox
  - first: Roger
    full: Roger Levy
    id: roger-levy
    last: Levy
  author_string: Aixiu An, Peng Qian, Ethan Wilcox, Roger Levy
  bibkey: an-etal-2019-representation
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1287
  month: November
  page_first: '2888'
  page_last: '2899'
  pages: "2888\u20132899"
  paper_id: '287'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1287.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1287.jpg
  title: 'Representation of Constituents in Neural Language Models: Coordination Phrase
    as a Case Study'
  title_html: 'Representation of Constituents in Neural Language Models: Coordination
    Phrase as a Case Study'
  url: https://www.aclweb.org/anthology/D19-1287
  year: '2019'
D19-1288:
  abstract: "Can we construct a neural language model which is inductively biased\
    \ towards learning human language? Motivated by this question, we aim at constructing\
    \ an informative prior for held-out languages on the task of character-level,\
    \ open-vocabulary language modelling. We obtain this prior as the posterior over\
    \ network weights conditioned on the data from a sample of training languages,\
    \ which is approximated through Laplace\u2019s method. Based on a large and diverse\
    \ sample of languages, the use of our prior outperforms baseline models with an\
    \ uninformative prior in both zero-shot and few-shot settings, showing that the\
    \ prior is imbued with universal linguistic knowledge. Moreover, we harness broad\
    \ language-specific information available for most languages of the world, i.e.,\
    \ features from typological databases, as distant supervision for held-out languages.\
    \ We explore several language modelling conditioning techniques, including concatenation\
    \ and meta-networks for parameter generation. They appear beneficial in the few-shot\
    \ setting, but ineffective in the zero-shot setting. Since the paucity of even\
    \ plain digital text affects the majority of the world\u2019s languages, we hope\
    \ that these insights will broaden the scope of applications for language technology."
  address: Hong Kong, China
  attachment:
  - filename: D19-1288.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1288.Attachment.pdf
  author:
  - first: Edoardo Maria
    full: Edoardo Maria Ponti
    id: edoardo-maria-ponti
    last: Ponti
  - first: Ivan
    full: "Ivan Vuli\u0107"
    id: ivan-vulic
    last: "Vuli\u0107"
  - first: Ryan
    full: Ryan Cotterell
    id: ryan-cotterell
    last: Cotterell
  - first: Roi
    full: Roi Reichart
    id: roi-reichart
    last: Reichart
  - first: Anna
    full: Anna Korhonen
    id: anna-korhonen
    last: Korhonen
  author_string: "Edoardo Maria Ponti, Ivan Vuli\u0107, Ryan Cotterell, Roi Reichart,\
    \ Anna Korhonen"
  bibkey: ponti-etal-2019-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1288
  month: November
  page_first: '2900'
  page_last: '2910'
  pages: "2900\u20132910"
  paper_id: '288'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1288.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1288.jpg
  title: Towards Zero-shot Language Modeling
  title_html: Towards Zero-shot Language Modeling
  url: https://www.aclweb.org/anthology/D19-1288
  year: '2019'
D19-1289:
  abstract: Explanations are central to everyday life, and are a topic of growing
    interest in the AI community. To investigate the process of providing natural
    language explanations, we leverage the dynamics of the /r/ChangeMyView subreddit
    to build a dataset with 36K naturally occurring explanations of why an argument
    is persuasive. We propose a novel word-level prediction task to investigate how
    explanations selectively reuse, or echo, information from what is being explained
    (henceforth, explanandum). We develop features to capture the properties of a
    word in the explanandum, and show that our proposed features not only have relatively
    strong predictive power on the echoing of a word in an explanation, but also enhance
    neural methods of generating explanations. In particular, while the non-contextual
    properties of a word itself are more valuable for stopwords, the interaction between
    the constituent parts of an explanandum is crucial in predicting the echoing of
    content words. We also find intriguing patterns of a word being echoed. For example,
    although nouns are generally less likely to be echoed, subjects and objects can,
    depending on their source, be more likely to be echoed in the explanations.
  address: Hong Kong, China
  attachment:
  - filename: D19-1289.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1289.Attachment.pdf
  author:
  - first: David
    full: David Atkinson
    id: david-atkinson
    last: Atkinson
  - first: Kumar Bhargav
    full: Kumar Bhargav Srinivasan
    id: kumar-bhargav-srinivasan
    last: Srinivasan
  - first: Chenhao
    full: Chenhao Tan
    id: chenhao-tan
    last: Tan
  author_string: David Atkinson, Kumar Bhargav Srinivasan, Chenhao Tan
  bibkey: atkinson-etal-2019-gets
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1289
  month: November
  page_first: '2911'
  page_last: '2921'
  pages: "2911\u20132921"
  paper_id: '289'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1289.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1289.jpg
  title: "What Gets Echoed? Understanding the \u201CPointers\u201D in Explanations\
    \ of Persuasive Arguments"
  title_html: "What Gets Echoed? Understanding the \u201CPointers\u201D in Explanations\
    \ of Persuasive Arguments"
  url: https://www.aclweb.org/anthology/D19-1289
  year: '2019'
D19-1290:
  abstract: "In argumentation, framing is used to emphasize a specific aspect of a\
    \ controversial topic while concealing others. When talking about legalizing drugs,\
    \ for instance, its economical aspect may be emphasized. In general, we call a\
    \ set of arguments that focus on the same aspect a frame. An argumentative text\
    \ has to serve the \u201Cright\u201D frame(s) to convince the audience to adopt\
    \ the author\u2019s stance (e.g., being pro or con legalizing drugs). More specifically,\
    \ an author has to choose frames that fit the audience\u2019s cultural background\
    \ and interests. This paper introduces frame identification, which is the task\
    \ of splitting a set of arguments into non-overlapping frames. We present a fully\
    \ unsupervised approach to this task, which first removes topical information\
    \ and then identifies frames using clustering. For evaluation purposes, we provide\
    \ a corpus with 12, 326 debate-portal arguments, organized along the frames of\
    \ the debates\u2019 topics. On this corpus, our approach outperforms different\
    \ strong baselines, achieving an F1-score of 0.28."
  address: Hong Kong, China
  author:
  - first: Yamen
    full: Yamen Ajjour
    id: yamen-ajjour
    last: Ajjour
  - first: Milad
    full: Milad Alshomary
    id: milad-alshomary
    last: Alshomary
  - first: Henning
    full: Henning Wachsmuth
    id: henning-wachsmuth
    last: Wachsmuth
  - first: Benno
    full: Benno Stein
    id: benno-stein
    last: Stein
  author_string: Yamen Ajjour, Milad Alshomary, Henning Wachsmuth, Benno Stein
  bibkey: ajjour-etal-2019-modeling
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1290
  month: November
  page_first: '2922'
  page_last: '2932'
  pages: "2922\u20132932"
  paper_id: '290'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1290.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1290.jpg
  title: Modeling Frames in Argumentation
  title_html: Modeling Frames in Argumentation
  url: https://www.aclweb.org/anthology/D19-1290
  year: '2019'
D19-1291:
  abstract: "Argumentation is a type of discourse where speakers try to persuade their\
    \ audience about the reasonableness of a claim by presenting supportive arguments.\
    \ Most work in argument mining has focused on modeling arguments in monologues.\
    \ We propose a computational model for argument mining in online persuasive discussion\
    \ forums that brings together the micro-level (argument as product) and macro-level\
    \ (argument as process) models of argumentation. Fundamentally, this approach\
    \ relies on identifying relations between components of arguments in a discussion\
    \ thread. Our approach for relation prediction uses contextual information in\
    \ terms of fine-tuning a pre-trained language model and leveraging discourse relations\
    \ based on Rhetorical Structure Theory. We additionally propose a candidate selection\
    \ method to automatically predict what parts of one\u2019s argument will be targeted\
    \ by other participants in the discussion. Our models obtain significant improvements\
    \ compared to recent state-of-the-art approaches using pointer networks and a\
    \ pre-trained language model."
  address: Hong Kong, China
  attachment:
  - filename: D19-1291.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1291.Attachment.zip
  author:
  - first: Tuhin
    full: Tuhin Chakrabarty
    id: tuhin-chakrabarty
    last: Chakrabarty
  - first: Christopher
    full: Christopher Hidey
    id: christopher-hidey
    last: Hidey
  - first: Smaranda
    full: Smaranda Muresan
    id: smaranda-muresan
    last: Muresan
  - first: Kathy
    full: Kathy McKeown
    id: kathleen-mckeown
    last: McKeown
  - first: Alyssa
    full: Alyssa Hwang
    id: alyssa-hwang
    last: Hwang
  author_string: Tuhin Chakrabarty, Christopher Hidey, Smaranda Muresan, Kathy McKeown,
    Alyssa Hwang
  bibkey: chakrabarty-etal-2019-ampersand
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1291
  month: November
  page_first: '2933'
  page_last: '2943'
  pages: "2933\u20132943"
  paper_id: '291'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1291.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1291.jpg
  title: 'AMPERSAND: Argument Mining for PERSuAsive oNline Discussions'
  title_html: '<span class="acl-fixed-case">AMPERSAND</span>: Argument Mining for
    <span class="acl-fixed-case">PERS</span>u<span class="acl-fixed-case">A</span>sive
    o<span class="acl-fixed-case">N</span>line Discussions'
  url: https://www.aclweb.org/anthology/D19-1291
  year: '2019'
D19-1292:
  abstract: 'Automated fact verification has been progressing owing to advancements
    in modeling and availability of large datasets. Due to the nature of the task,
    it is critical to understand the vulnerabilities of these systems against adversarial
    instances designed to make them predict incorrectly. We introduce two novel scoring
    metrics, attack potency and system resilience which take into account the correctness
    of the adversarial instances, an aspect often ignored in adversarial evaluations.
    We consider six fact verification systems from the recent Fact Extraction and
    VERification (FEVER) challenge: the four best-scoring ones and two baselines.
    We evaluate adversarial instances generated by a recently proposed state-of-the-art
    method, a paraphrasing method, and rule-based attacks devised for fact verification.
    We find that our rule-based attacks have higher potency, and that while the rankings
    among the top systems changed, they exhibited higher resilience than the baselines.'
  address: Hong Kong, China
  author:
  - first: James
    full: James Thorne
    id: james-thorne
    last: Thorne
  - first: Andreas
    full: Andreas Vlachos
    id: andreas-vlachos
    last: Vlachos
  - first: Christos
    full: Christos Christodoulopoulos
    id: christos-christodoulopoulos
    last: Christodoulopoulos
  - first: Arpit
    full: Arpit Mittal
    id: arpit-mittal
    last: Mittal
  author_string: James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit
    Mittal
  bibkey: thorne-etal-2019-evaluating
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1292
  month: November
  page_first: '2944'
  page_last: '2953'
  pages: "2944\u20132953"
  paper_id: '292'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1292.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1292.jpg
  title: Evaluating adversarial attacks against multiple fact verification systems
  title_html: Evaluating adversarial attacks against multiple fact verification systems
  url: https://www.aclweb.org/anthology/D19-1292
  year: '2019'
D19-1293:
  abstract: Annotation quality control is a critical aspect for building reliable
    corpora through linguistic annotation. In this study, we present a simple but
    powerful quality control method using two-step reason selection. We gathered sentential
    annotations of local acceptance and three related attributes through a crowdsourcing
    platform. For each attribute, the reason for the choice of the attribute value
    is selected in a two-step manner. The options given for reason selection were
    designed to facilitate the detection of a nonsensical reason selection. We assume
    that a sentential annotation that contains a nonsensical reason is less reliable
    than the one without such reason. Our method, based solely on this assumption,
    is found to retain the annotations with satisfactory quality out of the entire
    annotations mixed with those of low quality.
  address: Hong Kong, China
  attachment:
  - filename: D19-1293.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1293.Attachment.zip
  author:
  - first: Wonsuk
    full: Wonsuk Yang
    id: wonsuk-yang
    last: Yang
  - first: Seungwon
    full: Seungwon Yoon
    id: seungwon-yoon
    last: Yoon
  - first: Ada
    full: Ada Carpenter
    id: ada-carpenter
    last: Carpenter
  - first: Jong
    full: Jong Park
    id: jong-c-park
    last: Park
  author_string: Wonsuk Yang, Seungwon Yoon, Ada Carpenter, Jong Park
  bibkey: yang-etal-2019-nonsense
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1293
  month: November
  page_first: '2954'
  page_last: '2963'
  pages: "2954\u20132963"
  paper_id: '293'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1293.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1293.jpg
  title: 'Nonsense!: Quality Control via Two-Step Reason Selection for Annotating
    Local Acceptability and Related Attributes in News Editorials'
  title_html: 'Nonsense!: Quality Control via Two-Step Reason Selection for Annotating
    Local Acceptability and Related Attributes in News Editorials'
  url: https://www.aclweb.org/anthology/D19-1293
  year: '2019'
D19-1294:
  abstract: The ongoing neural revolution in machine translation has made it easier
    to model larger contexts beyond the sentence-level, which can potentially help
    resolve some discourse-level ambiguities such as pronominal anaphora, thus enabling
    better translations. Unfortunately, even when the resulting improvements are seen
    as substantial by humans, they remain virtually unnoticed by traditional automatic
    evaluation measures like BLEU, as only a few words end up being affected. Thus,
    specialized evaluation measures are needed. With this aim in mind, we contribute
    an extensive, targeted dataset that can be used as a test suite for pronoun translation,
    covering multiple source languages and different pronoun errors drawn from real
    system translations, for English. We further propose an evaluation measure to
    differentiate good and bad pronoun translations. We also conduct a user study
    to report correlations with human judgments.
  address: Hong Kong, China
  attachment:
  - filename: D19-1294.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1294.Attachment.zip
  author:
  - first: Prathyusha
    full: Prathyusha Jwalapuram
    id: prathyusha-jwalapuram
    last: Jwalapuram
  - first: Shafiq
    full: Shafiq Joty
    id: shafiq-joty
    last: Joty
  - first: Irina
    full: Irina Temnikova
    id: irina-temnikova
    last: Temnikova
  - first: Preslav
    full: Preslav Nakov
    id: preslav-nakov
    last: Nakov
  author_string: Prathyusha Jwalapuram, Shafiq Joty, Irina Temnikova, Preslav Nakov
  bibkey: jwalapuram-etal-2019-evaluating
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1294
  month: November
  page_first: '2964'
  page_last: '2975'
  pages: "2964\u20132975"
  paper_id: '294'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1294.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1294.jpg
  title: 'Evaluating Pronominal Anaphora in Machine Translation: An Evaluation Measure
    and a Test Suite'
  title_html: 'Evaluating Pronominal Anaphora in Machine Translation: An Evaluation
    Measure and a Test Suite'
  url: https://www.aclweb.org/anthology/D19-1294
  year: '2019'
D19-1295:
  abstract: We argue that external commonsense knowledge and linguistic constraints
    need to be incorporated into neural network models for mitigating data sparsity
    issues and further improving the performance of discourse parsing. Realizing that
    external knowledge and linguistic constraints may not always apply in understanding
    a particular context, we propose a regularization approach that tightly integrates
    these constraints with contexts for deriving word representations. Meanwhile,
    it balances attentions over contexts and constraints through adding a regularization
    term into the objective function. Experiments show that our knowledge regularization
    approach outperforms all previous systems on the benchmark dataset PDTB for discourse
    parsing.
  address: Hong Kong, China
  author:
  - first: Zeyu
    full: Zeyu Dai
    id: zeyu-dai
    last: Dai
  - first: Ruihong
    full: Ruihong Huang
    id: ruihong-huang
    last: Huang
  author_string: Zeyu Dai, Ruihong Huang
  bibkey: dai-huang-2019-regularization
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1295
  month: November
  page_first: '2976'
  page_last: '2987'
  pages: "2976\u20132987"
  paper_id: '295'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1295.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1295.jpg
  title: A Regularization Approach for Incorporating Event Knowledge and Coreference
    Relations into Neural Discourse Parsing
  title_html: A Regularization Approach for Incorporating Event Knowledge and Coreference
    Relations into Neural Discourse Parsing
  url: https://www.aclweb.org/anthology/D19-1295
  year: '2019'
D19-1296:
  abstract: We present a method for extracting causality knowledge from Wikipedia,
    such as Protectionism -> Trade war, where the cause and effect entities correspond
    to Wikipedia articles. Such causality knowledge is easy to verify by reading corresponding
    Wikipedia articles, to translate to multiple languages through Wikidata, and to
    connect to knowledge bases derived from Wikipedia. Our method exploits Wikipedia
    article sections that describe causality and the redundancy stemming from the
    multilinguality of Wikipedia. Experiments showed that our method achieved precision
    and recall above 98% and 64%, respectively. In particular, it could extract causalities
    whose cause and effect were written distantly in a Wikipedia article. We have
    released the code and data for further research.
  address: Hong Kong, China
  author:
  - first: Chikara
    full: Chikara Hashimoto
    id: chikara-hashimoto
    last: Hashimoto
  author_string: Chikara Hashimoto
  bibkey: hashimoto-2019-weakly
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1296
  month: November
  page_first: '2988'
  page_last: '2999'
  pages: "2988\u20132999"
  paper_id: '296'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1296.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1296.jpg
  title: Weakly Supervised Multilingual Causality Extraction from Wikipedia
  title_html: Weakly Supervised Multilingual Causality Extraction from <span class="acl-fixed-case">W</span>ikipedia
  url: https://www.aclweb.org/anthology/D19-1296
  year: '2019'
D19-1297:
  abstract: "Review summarization aims to generate a condensed summary for a review\
    \ or multiple reviews. Existing review summarization systems mainly generate summary\
    \ only based on review content and neglect the authors\u2019 attributes (e.g.,\
    \ gender, age, and occupation). In fact, when summarizing a review, users with\
    \ different attributes usually pay attention to specific aspects and have their\
    \ own word-using habits or writing styles. Therefore, we propose an Attribute-aware\
    \ Sequence Network (ASN) to take the aforementioned users\u2019 characteristics\
    \ into account, which includes three modules: an attribute encoder encodes the\
    \ attribute preferences over the words; an attribute-aware review encoder adopts\
    \ an attribute-based selective mechanism to select the important information of\
    \ a review; and an attribute-aware summary decoder incorporates attribute embedding\
    \ and attribute-specific word-using habits into word prediction. To validate our\
    \ model, we collect a new dataset TripAtt, comprising 495,440 attribute-review-summary\
    \ triplets with three kinds of attribute information: gender, age, and travel\
    \ status. Extensive experiments show that ASN achieves state-of-the-art performance\
    \ on review summarization in both auto-metric ROUGE and human evaluation."
  address: Hong Kong, China
  author:
  - first: Junjie
    full: Junjie Li
    id: junjie-li
    last: Li
  - first: Xuepeng
    full: Xuepeng Wang
    id: xuepeng-wang
    last: Wang
  - first: Dawei
    full: Dawei Yin
    id: dawei-yin
    last: Yin
  - first: Chengqing
    full: Chengqing Zong
    id: chengqing-zong
    last: Zong
  author_string: Junjie Li, Xuepeng Wang, Dawei Yin, Chengqing Zong
  bibkey: li-etal-2019-attribute
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1297
  month: November
  page_first: '3000'
  page_last: '3010'
  pages: "3000\u20133010"
  paper_id: '297'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1297.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1297.jpg
  title: Attribute-aware Sequence Network for Review Summarization
  title_html: Attribute-aware Sequence Network for Review Summarization
  url: https://www.aclweb.org/anthology/D19-1297
  year: '2019'
D19-1298:
  abstract: In this paper, we propose a novel neural single-document extractive summarization
    model for long documents, incorporating both the global context of the whole document
    and the local context within the current topic. We evaluate the model on two datasets
    of scientific papers , Pubmed and arXiv, where it outperforms previous work, both
    extractive and abstractive models, on ROUGE-1, ROUGE-2 and METEOR scores. We also
    show that, consistently with our goal, the benefits of our method become stronger
    as we apply it to longer documents. Rather surprisingly, an ablation study indicates
    that the benefits of our model seem to come exclusively from modeling the local
    context, even for the longest documents.
  address: Hong Kong, China
  attachment:
  - filename: D19-1298.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1298.Attachment.zip
  author:
  - first: Wen
    full: Wen Xiao
    id: wen-xiao
    last: Xiao
  - first: Giuseppe
    full: Giuseppe Carenini
    id: giuseppe-carenini
    last: Carenini
  author_string: Wen Xiao, Giuseppe Carenini
  bibkey: xiao-carenini-2019-extractive
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1298
  month: November
  page_first: '3011'
  page_last: '3021'
  pages: "3011\u20133021"
  paper_id: '298'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1298.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1298.jpg
  title: Extractive Summarization of Long Documents by Combining Global and Local
    Context
  title_html: Extractive Summarization of Long Documents by Combining Global and Local
    Context
  url: https://www.aclweb.org/anthology/D19-1298
  year: '2019'
D19-1299:
  abstract: Recent neural models for data-to-text generation rely on massive parallel
    pairs of data and text to learn the writing knowledge. They often assume that
    writing knowledge can be acquired from the training data alone. However, when
    people are writing, they not only rely on the data but also consider related knowledge.
    In this paper, we enhance neural data-to-text models with external knowledge in
    a simple but effective way to improve the fidelity of generated text. Besides
    relying on parallel data and text as in previous work, our model attends to relevant
    external knowledge, encoded as a temporary memory, and combines this knowledge
    with the context representation of data before generating words. This allows the
    model to infer relevant facts which are not explicitly stated in the data table
    from an external knowledge source. Experimental results on twenty-one Wikipedia
    infobox-to-text datasets show our model, KBAtt, consistently improves a state-of-the-art
    model on most of the datasets. In addition, to quantify when and why external
    knowledge is effective, we design a metric, KBGain, which shows a strong correlation
    with the observed performance boost. This result demonstrates the relevance of
    external knowledge and sparseness of original data are the main factors affecting
    system performance.
  address: Hong Kong, China
  author:
  - first: Shuang
    full: Shuang Chen
    id: shuang-chen
    last: Chen
  - first: Jinpeng
    full: Jinpeng Wang
    id: jinpeng-wang
    last: Wang
  - first: Xiaocheng
    full: Xiaocheng Feng
    id: xiaocheng-feng
    last: Feng
  - first: Feng
    full: Feng Jiang
    id: feng-jiang
    last: Jiang
  - first: Bing
    full: Bing Qin
    id: bing-qin
    last: Qin
  - first: Chin-Yew
    full: Chin-Yew Lin
    id: chin-yew-lin
    last: Lin
  author_string: Shuang Chen, Jinpeng Wang, Xiaocheng Feng, Feng Jiang, Bing Qin,
    Chin-Yew Lin
  bibkey: chen-etal-2019-enhancing
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1299
  month: November
  page_first: '3022'
  page_last: '3032'
  pages: "3022\u20133032"
  paper_id: '299'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1299.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1299.jpg
  title: Enhancing Neural Data-To-Text Generation Models with External Background
    Knowledge
  title_html: Enhancing Neural Data-To-Text Generation Models with External Background
    Knowledge
  url: https://www.aclweb.org/anthology/D19-1299
  year: '2019'
D19-1300:
  abstract: 'In this work, we re-examine the problem of extractive text summarization
    for long documents. We observe that the process of extracting summarization of
    human can be divided into two stages: 1) a rough reading stage to look for sketched
    information, and 2) a subsequent careful reading stage to select key sentences
    to form the summary. By simulating such a two-stage process, we propose a novel
    approach for extractive summarization. We formulate the problem as a contextual-bandit
    problem and solve it with policy gradient. We adopt a convolutional neural network
    to encode gist of paragraphs for rough reading, and a decision making policy with
    an adapted termination mechanism for careful reading. Experiments on the CNN and
    DailyMail datasets show that our proposed method can provide high-quality summaries
    with varied length, and significantly outperform the state-of-the-art extractive
    methods in terms of ROUGE metrics.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1300.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1300.Attachment.zip
  author:
  - first: Ling
    full: Ling Luo
    id: ling-luo
    last: Luo
  - first: Xiang
    full: Xiang Ao
    id: xiang-ao
    last: Ao
  - first: Yan
    full: Yan Song
    id: yan-song
    last: Song
  - first: Feiyang
    full: Feiyang Pan
    id: feiyang-pan
    last: Pan
  - first: Min
    full: Min Yang
    id: min-yang
    last: Yang
  - first: Qing
    full: Qing He
    id: qing-he
    last: He
  author_string: Ling Luo, Xiang Ao, Yan Song, Feiyang Pan, Min Yang, Qing He
  bibkey: luo-etal-2019-reading
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1300
  month: November
  page_first: '3033'
  page_last: '3043'
  pages: "3033\u20133043"
  paper_id: '300'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1300.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1300.jpg
  title: 'Reading Like HER: Human Reading Inspired Extractive Summarization'
  title_html: 'Reading Like <span class="acl-fixed-case">HER</span>: Human Reading
    Inspired Extractive Summarization'
  url: https://www.aclweb.org/anthology/D19-1300
  year: '2019'
D19-1301:
  abstract: 'We propose a contrastive attention mechanism to extend the sequence-to-sequence
    framework for abstractive sentence summarization task, which aims to generate
    a brief summary of a given source sentence. The proposed contrastive attention
    mechanism accommodates two categories of attention: one is the conventional attention
    that attends to relevant parts of the source sentence, the other is the opponent
    attention that attends to irrelevant or less relevant parts of the source sentence.
    Both attentions are trained in an opposite way so that the contribution from the
    conventional attention is encouraged and the contribution from the opponent attention
    is discouraged through a novel softmax and softmin functionality. Experiments
    on benchmark datasets show that, the proposed contrastive attention mechanism
    is more focused on the relevant parts for the summary than the conventional attention
    mechanism, and greatly advances the state-of-the-art performance on the abstractive
    sentence summarization task. We release the code at https://github.com/travel-go/
    Abstractive-Text-Summarization.'
  address: Hong Kong, China
  author:
  - first: Xiangyu
    full: Xiangyu Duan
    id: xiangyu-duan
    last: Duan
  - first: Hongfei
    full: Hongfei Yu
    id: hongfei-yu
    last: Yu
  - first: Mingming
    full: Mingming Yin
    id: mingming-yin
    last: Yin
  - first: Min
    full: Min Zhang
    id: min-zhang
    last: Zhang
  - first: Weihua
    full: Weihua Luo
    id: weihua-luo
    last: Luo
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  author_string: Xiangyu Duan, Hongfei Yu, Mingming Yin, Min Zhang, Weihua Luo, Yue
    Zhang
  bibkey: duan-etal-2019-contrastive
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1301
  month: November
  page_first: '3044'
  page_last: '3053'
  pages: "3044\u20133053"
  paper_id: '301'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1301.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1301.jpg
  title: Contrastive Attention Mechanism for Abstractive Sentence Summarization
  title_html: Contrastive Attention Mechanism for Abstractive Sentence Summarization
  url: https://www.aclweb.org/anthology/D19-1301
  year: '2019'
D19-1302:
  abstract: 'Cross-lingual summarization (CLS) is the task to produce a summary in
    one particular language for a source document in a different language. Existing
    methods simply divide this task into two steps: summarization and translation,
    leading to the problem of error propagation. To handle that, we present an end-to-end
    CLS framework, which we refer to as Neural Cross-Lingual Summarization (NCLS),
    for the first time. Moreover, we propose to further improve NCLS by incorporating
    two related tasks, monolingual summarization and machine translation, into the
    training process of CLS under multi-task learning. Due to the lack of supervised
    CLS data, we propose a round-trip translation strategy to acquire two high-quality
    large-scale CLS datasets based on existing monolingual summarization datasets.
    Experimental results have shown that our NCLS achieves remarkable improvement
    over traditional pipeline methods on both English-to-Chinese and Chinese-to-English
    CLS human-corrected test sets. In addition, NCLS with multi-task learning can
    further significantly improve the quality of generated summaries. We make our
    dataset and code publicly available here: http://www.nlpr.ia.ac.cn/cip/dataset.htm.'
  address: Hong Kong, China
  author:
  - first: Junnan
    full: Junnan Zhu
    id: junnan-zhu
    last: Zhu
  - first: Qian
    full: Qian Wang
    id: qian-wang
    last: Wang
  - first: Yining
    full: Yining Wang
    id: yining-wang
    last: Wang
  - first: Yu
    full: Yu Zhou
    id: yu-zhou
    last: Zhou
  - first: Jiajun
    full: Jiajun Zhang
    id: jiajun-zhang
    last: Zhang
  - first: Shaonan
    full: Shaonan Wang
    id: shaonan-wang
    last: Wang
  - first: Chengqing
    full: Chengqing Zong
    id: chengqing-zong
    last: Zong
  author_string: Junnan Zhu, Qian Wang, Yining Wang, Yu Zhou, Jiajun Zhang, Shaonan
    Wang, Chengqing Zong
  bibkey: zhu-etal-2019-ncls
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1302
  month: November
  page_first: '3054'
  page_last: '3064'
  pages: "3054\u20133064"
  paper_id: '302'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1302.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1302.jpg
  title: 'NCLS: Neural Cross-Lingual Summarization'
  title_html: '<span class="acl-fixed-case">NCLS</span>: Neural Cross-Lingual Summarization'
  url: https://www.aclweb.org/anthology/D19-1302
  year: '2019'
D19-1303:
  abstract: "Sensational headlines are headlines that capture people\u2019s attention\
    \ and generate reader interest. Conventional abstractive headline generation methods,\
    \ unlike human writers, do not optimize for maximal reader attention. In this\
    \ paper, we propose a model that generates sensational headlines without labeled\
    \ data. We first train a sensationalism scorer by classifying online headlines\
    \ with many comments (\u201Cclickbait\u201D) against a baseline of headlines generated\
    \ from a summarization model. The score from the sensationalism scorer is used\
    \ as the reward for a reinforcement learner. However, maximizing the noisy sensationalism\
    \ reward will generate unnatural phrases instead of sensational headlines. To\
    \ effectively leverage this noisy reward, we propose a novel loss function, Auto-tuned\
    \ Reinforcement Learning (ARL), to dynamically balance reinforcement learning\
    \ (RL) with maximum likelihood estimation (MLE). Human evaluation shows that 60.8%\
    \ of samples generated by our model are sensational, which is significantly better\
    \ than the Pointer-Gen baseline and other RL models."
  address: Hong Kong, China
  author:
  - first: Peng
    full: Peng Xu
    id: peng-xu
    last: Xu
  - first: Chien-Sheng
    full: Chien-Sheng Wu
    id: chien-sheng-wu
    last: Wu
  - first: Andrea
    full: Andrea Madotto
    id: andrea-madotto
    last: Madotto
  - first: Pascale
    full: Pascale Fung
    id: pascale-fung
    last: Fung
  author_string: Peng Xu, Chien-Sheng Wu, Andrea Madotto, Pascale Fung
  bibkey: xu-etal-2019-clickbait
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1303
  month: November
  page_first: '3065'
  page_last: '3075'
  pages: "3065\u20133075"
  paper_id: '303'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1303.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1303.jpg
  title: Clickbait? Sensational Headline Generation with Auto-tuned Reinforcement
    Learning
  title_html: Clickbait? Sensational Headline Generation with Auto-tuned Reinforcement
    Learning
  url: https://www.aclweb.org/anthology/D19-1303
  year: '2019'
D19-1304:
  abstract: "A quality abstractive summary should not only copy salient source texts\
    \ as summaries but should also tend to generate new conceptual words to express\
    \ concrete details. Inspired by the popular pointer generator sequence-to-sequence\
    \ model, this paper presents a concept pointer network for improving these aspects\
    \ of abstractive summarization. The network leverages knowledge-based, context-aware\
    \ conceptualizations to derive an extended set of candidate concepts. The model\
    \ then points to the most appropriate choice using both the concept set and original\
    \ source text. This joint approach generates abstractive summaries with higher-level\
    \ semantic concepts. The training model is also optimized in a way that adapts\
    \ to different data, which is based on a novel method of distant-supervised learning\
    \ guided by reference summaries and testing set. Overall, the proposed approach\
    \ provides statistically significant improvements over several state-of-the-art\
    \ models on both the DUC-2004 and Gigaword datasets. A human evaluation of the\
    \ model\u2019s abstractive abilities also supports the quality of the summaries\
    \ produced within this framework."
  address: Hong Kong, China
  attachment:
  - filename: D19-1304.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1304.Attachment.zip
  author:
  - first: Wenbo
    full: Wenbo Wang
    id: wenbo-wang
    last: Wang
  - first: Yang
    full: Yang Gao
    id: yang-gao
    last: Gao
  - first: Heyan
    full: Heyan Huang
    id: he-yan-huang
    last: Huang
  - first: Yuxiang
    full: Yuxiang Zhou
    id: yuxiang-zhou
    last: Zhou
  author_string: Wenbo Wang, Yang Gao, Heyan Huang, Yuxiang Zhou
  bibkey: wang-etal-2019-concept
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1304
  month: November
  page_first: '3076'
  page_last: '3085'
  pages: "3076\u20133085"
  paper_id: '304'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1304.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1304.jpg
  title: Concept Pointer Network for Abstractive Summarization
  title_html: Concept Pointer Network for Abstractive Summarization
  url: https://www.aclweb.org/anthology/D19-1304
  year: '2019'
D19-1305:
  abstract: "Surface realisation (SR) maps a meaning representation to a sentence\
    \ and can be viewed as consisting of three subtasks: word ordering, morphological\
    \ inflection and contraction generation (e.g., clitic attachment in Portuguese\
    \ or elision in French). We propose a modular approach to surface realisation\
    \ which models each of these components separately, and evaluate our approach\
    \ on the 10 languages covered by the SR\u201918 Surface Realisation Shared Task\
    \ shallow track. We provide a detailed evaluation of how word order, morphological\
    \ realisation and contractions are handled by the model and an analysis of the\
    \ differences in word ordering performance across languages."
  address: Hong Kong, China
  attachment:
  - filename: D19-1305.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1305.Attachment.zip
  author:
  - first: Anastasia
    full: Anastasia Shimorina
    id: anastasia-shimorina
    last: Shimorina
  - first: Claire
    full: Claire Gardent
    id: claire-gardent
    last: Gardent
  author_string: Anastasia Shimorina, Claire Gardent
  bibkey: shimorina-gardent-2019-surface
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1305
  month: November
  page_first: '3086'
  page_last: '3096'
  pages: "3086\u20133096"
  paper_id: '305'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1305.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1305.jpg
  title: Surface Realisation Using Full Delexicalisation
  title_html: Surface Realisation Using Full Delexicalisation
  url: https://www.aclweb.org/anthology/D19-1305
  year: '2019'
D19-1306:
  abstract: 'Text attribute transfer aims to automatically rewrite sentences such
    that they possess certain linguistic attributes, while simultaneously preserving
    their semantic content. This task remains challenging due to a lack of supervised
    parallel data. Existing approaches try to explicitly disentangle content and attribute
    information, but this is difficult and often results in poor content-preservation
    and ungrammaticality. In contrast, we propose a simpler approach, Iterative Matching
    and Translation (IMaT), which: (1) constructs a pseudo-parallel corpus by aligning
    a subset of semantically similar sentences from the source and the target corpora;
    (2) applies a standard sequence-to-sequence model to learn the attribute transfer;
    (3) iteratively improves the learned transfer function by refining imperfections
    in the alignment. In sentiment modification and formality transfer tasks, our
    method outperforms complex state-of-the-art systems by a large margin. As an auxiliary
    contribution, we produce a publicly-available test set with human-generated transfer
    references.'
  address: Hong Kong, China
  author:
  - first: Zhijing
    full: Zhijing Jin
    id: zhijing-jin
    last: Jin
  - first: Di
    full: Di Jin
    id: di-jin
    last: Jin
  - first: Jonas
    full: Jonas Mueller
    id: jonas-mueller
    last: Mueller
  - first: Nicholas
    full: Nicholas Matthews
    id: nicholas-matthews
    last: Matthews
  - first: Enrico
    full: Enrico Santus
    id: enrico-santus
    last: Santus
  author_string: Zhijing Jin, Di Jin, Jonas Mueller, Nicholas Matthews, Enrico Santus
  bibkey: jin-etal-2019-imat
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1306
  month: November
  page_first: '3097'
  page_last: '3109'
  pages: "3097\u20133109"
  paper_id: '306'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1306.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1306.jpg
  title: 'IMaT: Unsupervised Text Attribute Transfer via Iterative Matching and Translation'
  title_html: '<span class="acl-fixed-case">IM</span>a<span class="acl-fixed-case">T</span>:
    Unsupervised Text Attribute Transfer via Iterative Matching and Translation'
  url: https://www.aclweb.org/anthology/D19-1306
  year: '2019'
D19-1307:
  abstract: Reinforcement Learning (RL)based document summarisation systems yield
    state-of-the-art performance in terms of ROUGE scores, because they directly use
    ROUGE as the rewards during training. However, summaries with high ROUGE scores
    often receive low human judgement. To find a better reward function that can guide
    RL to generate human-appealing summaries, we learn a reward function from human
    ratings on 2,500 summaries. Our reward function only takes the document and system
    summary as input. Hence, once trained, it can be used to train RL based summarisation
    systems without using any reference summaries. We show that our learned rewards
    have significantly higher correlation with human ratings than previous approaches.
    Human evaluation experiments show that, compared to the state-of-the-art supervised-learning
    systems and ROUGE-as-rewards RL summarisation systems, the RL systems using our
    learned rewards during training generate summaries with higher human ratings.
    The learned reward function and our source code are available at https://github.com/yg211/summary-reward-no-reference.
  address: Hong Kong, China
  attachment:
  - filename: D19-1307.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1307.Attachment.pdf
  author:
  - first: Florian
    full: "Florian B\xF6hm"
    id: florian-bohm
    last: "B\xF6hm"
  - first: Yang
    full: Yang Gao
    id: yang-gao
    last: Gao
  - first: Christian M.
    full: Christian M. Meyer
    id: christian-m-meyer
    last: Meyer
  - first: Ori
    full: Ori Shapira
    id: ori-shapira
    last: Shapira
  - first: Ido
    full: Ido Dagan
    id: ido-dagan
    last: Dagan
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: "Florian B\xF6hm, Yang Gao, Christian M. Meyer, Ori Shapira, Ido\
    \ Dagan, Iryna Gurevych"
  bibkey: bohm-etal-2019-better
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1307
  month: November
  page_first: '3110'
  page_last: '3120'
  pages: "3110\u20133120"
  paper_id: '307'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1307.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1307.jpg
  title: 'Better Rewards Yield Better Summaries: Learning to Summarise Without References'
  title_html: 'Better Rewards Yield Better Summaries: Learning to Summarise Without
    References'
  url: https://www.aclweb.org/anthology/D19-1307
  year: '2019'
D19-1308:
  abstract: Generating diverse sequences is important in many NLP applications such
    as question generation or summarization that exhibit semantically one-to-many
    relationships between source and the target sequences. We present a method to
    explicitly separate diversification from generation using a general plug-and-play
    module (called SELECTOR) that wraps around and guides an existing encoder-decoder
    model. The diversification stage uses a mixture of experts to sample different
    binary masks on the source sequence for diverse content selection. The generation
    stage uses a standard encoder-decoder model given each selected content from the
    source sequence. Due to the non-differentiable nature of discrete sampling and
    the lack of ground truth labels for binary mask, we leverage a proxy for ground
    truth mask and adopt stochastic hard-EM for training. In question generation (SQuAD)
    and abstractive summarization (CNN-DM), our method demonstrates significant improvements
    in accuracy, diversity and training efficiency, including state-of-the-art top-1
    accuracy in both datasets, 6% gain in top-5 accuracy, and 3.7 times faster training
    over a state-of-the-art model. Our code is publicly available at https://github.com/clovaai/FocusSeq2Seq.
  address: Hong Kong, China
  author:
  - first: Jaemin
    full: Jaemin Cho
    id: jaemin-cho
    last: Cho
  - first: Minjoon
    full: Minjoon Seo
    id: minjoon-seo
    last: Seo
  - first: Hannaneh
    full: Hannaneh Hajishirzi
    id: hannaneh-hajishirzi
    last: Hajishirzi
  author_string: Jaemin Cho, Minjoon Seo, Hannaneh Hajishirzi
  bibkey: cho-etal-2019-mixture
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1308
  month: November
  page_first: '3121'
  page_last: '3131'
  pages: "3121\u20133131"
  paper_id: '308'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1308.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1308.jpg
  title: Mixture Content Selection for Diverse Sequence Generation
  title_html: Mixture Content Selection for Diverse Sequence Generation
  url: https://www.aclweb.org/anthology/D19-1308
  year: '2019'
D19-1309:
  abstract: Generating high-quality paraphrases is a fundamental yet challenging natural
    language processing task. Despite the effectiveness of previous work based on
    generative models, there remain problems with exposure bias in recurrent neural
    networks, and often a failure to generate realistic sentences. To overcome these
    challenges, we propose the first end-to-end conditional generative architecture
    for generating paraphrases via adversarial training, which does not depend on
    extra linguistic information. Extensive experiments on four public datasets demonstrate
    the proposed method achieves state-of-the-art results, outperforming previous
    generative architectures on both automatic metrics (BLEU, METEOR, and TER) and
    human evaluations.
  address: Hong Kong, China
  author:
  - first: Qian
    full: Qian Yang
    id: qian-yang
    last: Yang
  - first: Zhouyuan
    full: Zhouyuan Huo
    id: zhouyuan-huo
    last: Huo
  - first: Dinghan
    full: Dinghan Shen
    id: dinghan-shen
    last: Shen
  - first: Yong
    full: Yong Cheng
    id: yong-cheng
    last: Cheng
  - first: Wenlin
    full: Wenlin Wang
    id: wenlin-wang
    last: Wang
  - first: Guoyin
    full: Guoyin Wang
    id: guoyin-wang
    last: Wang
  - first: Lawrence
    full: Lawrence Carin
    id: lawrence-carin
    last: Carin
  author_string: Qian Yang, Zhouyuan Huo, Dinghan Shen, Yong Cheng, Wenlin Wang, Guoyin
    Wang, Lawrence Carin
  bibkey: yang-etal-2019-end
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1309
  month: November
  page_first: '3132'
  page_last: '3142'
  pages: "3132\u20133142"
  paper_id: '309'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1309.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1309.jpg
  title: An End-to-End Generative Architecture for Paraphrase Generation
  title_html: An End-to-End Generative Architecture for Paraphrase Generation
  url: https://www.aclweb.org/anthology/D19-1309
  year: '2019'
D19-1310:
  abstract: "Although Seq2Seq models for table-to-text generation have achieved remarkable\
    \ progress, modeling table representation in one dimension is inadequate. This\
    \ is because (1) the table consists of multiple rows and columns, which means\
    \ that encoding a table should not depend only on one dimensional sequence or\
    \ set of records and (2) most of the tables are time series data (e.g. NBA game\
    \ data, stock market data), which means that the description of the current table\
    \ may be affected by its historical data. To address aforementioned problems,\
    \ not only do we model each table cell considering other records in the same row,\
    \ we also enrich table\u2019s representation by modeling each table cell in context\
    \ of other cells in the same column or with historical (time dimension) data respectively.\
    \ In addition, we develop a table cell fusion gate to combine representations\
    \ from row, column and time dimension into one dense vector according to the saliency\
    \ of each dimension\u2019s representation. We evaluated our methods on ROTOWIRE,\
    \ a benchmark dataset of NBA basketball games. Both automatic and human evaluation\
    \ results demonstrate the effectiveness of our model with improvement of 2.66\
    \ in BLEU over the strong baseline and outperformance of state-of-the-art model."
  address: Hong Kong, China
  author:
  - first: Heng
    full: Heng Gong
    id: heng-gong
    last: Gong
  - first: Xiaocheng
    full: Xiaocheng Feng
    id: xiaocheng-feng
    last: Feng
  - first: Bing
    full: Bing Qin
    id: bing-qin
    last: Qin
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  author_string: Heng Gong, Xiaocheng Feng, Bing Qin, Ting Liu
  bibkey: gong-etal-2019-table
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1310
  month: November
  page_first: '3143'
  page_last: '3152'
  pages: "3143\u20133152"
  paper_id: '310'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1310.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1310.jpg
  title: Table-to-Text Generation with Effective Hierarchical Encoder on Three Dimensions
    (Row, Column and Time)
  title_html: Table-to-Text Generation with Effective Hierarchical Encoder on Three
    Dimensions (Row, Column and Time)
  url: https://www.aclweb.org/anthology/D19-1310
  year: '2019'
D19-1311:
  abstract: In multi-document summarization, a set of documents to be summarized is
    assumed to be on the same topic, known as the underlying topic in this paper.
    That is, the underlying topic can be collectively represented by all the documents
    in the set. Meanwhile, different documents may cover various different subtopics
    and the same subtopic can be across several documents. Inspired by topic model,
    the underlying topic of a document set can also be viewed as a collection of different
    subtopics of different importance. In this paper, we propose a summarization model
    called STDS. The model generates the underlying topic representation from both
    document view and subtopic view in parallel. The learning objective is to minimize
    the distance between the representations learned from the two views. The contextual
    information is encoded through a hierarchical RNN architecture. Sentence salience
    is estimated in a hierarchical way with subtopic salience and relative sentence
    salience, by considering the contextual information. Top ranked sentences are
    then extracted as a summary. Note that the notion of subtopic enables us to bring
    in additional information (e.g. comments to news articles) that is helpful for
    document summarization. Experimental results show that the proposed solution outperforms
    state-of-the-art methods on benchmark datasets.
  address: Hong Kong, China
  author:
  - first: Xin
    full: Xin Zheng
    id: xin-zheng
    last: Zheng
  - first: Aixin
    full: Aixin Sun
    id: aixin-sun
    last: Sun
  - first: Jing
    full: Jing Li
    id: jing-li
    last: Li
  - first: Karthik
    full: Karthik Muthuswamy
    id: karthik-muthuswamy
    last: Muthuswamy
  author_string: Xin Zheng, Aixin Sun, Jing Li, Karthik Muthuswamy
  bibkey: zheng-etal-2019-subtopic
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1311
  month: November
  page_first: '3153'
  page_last: '3162'
  pages: "3153\u20133162"
  paper_id: '311'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1311.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1311.jpg
  title: Subtopic-driven Multi-Document Summarization
  title_html: Subtopic-driven Multi-Document Summarization
  url: https://www.aclweb.org/anthology/D19-1311
  year: '2019'
D19-1312:
  abstract: "Referring Expression Generation (REG) is the task of generating contextually\
    \ appropriate references to entities. A limitation of existing REG systems is\
    \ that they rely on entity-specific supervised training, which means that they\
    \ cannot handle entities not seen during training. In this study, we address this\
    \ in two ways. First, we propose task setups in which we specifically test a REG\
    \ system\u2019s ability to generalize to entities not seen during training. Second,\
    \ we propose a profile-based deep neural network model, ProfileREG, which encodes\
    \ both the local context and an external profile of the entity to generate reference\
    \ realizations. Our model generates tokens by learning to choose between generating\
    \ pronouns, generating from a fixed vocabulary, or copying a word from the profile.\
    \ We evaluate our model on three different splits of the WebNLG dataset, and show\
    \ that it outperforms competitive baselines in all settings according to automatic\
    \ and human evaluations."
  address: Hong Kong, China
  author:
  - first: Meng
    full: Meng Cao
    id: meng-cao
    last: Cao
  - first: Jackie Chi Kit
    full: Jackie Chi Kit Cheung
    id: jackie-chi-kit-cheung
    last: Cheung
  author_string: Meng Cao, Jackie Chi Kit Cheung
  bibkey: cao-cheung-2019-referring
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1312
  month: November
  page_first: '3163'
  page_last: '3172'
  pages: "3163\u20133172"
  paper_id: '312'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1312.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1312.jpg
  title: Referring Expression Generation Using Entity Profiles
  title_html: Referring Expression Generation Using Entity Profiles
  url: https://www.aclweb.org/anthology/D19-1312
  year: '2019'
D19-1313:
  abstract: Paraphrasing plays an important role in various natural language processing
    (NLP) tasks, such as question answering, information retrieval and sentence simplification.
    Recently, neural generative models have shown promising results in paraphrase
    generation. However, prior work mainly focused on single paraphrase generation,
    while ignoring the fact that diversity is essential for enhancing generalization
    capability and robustness of downstream applications. Few works have been done
    to solve diverse paraphrase generation. In this paper, we propose a novel approach
    with two discriminators and multiple generators to generate a variety of different
    paraphrases. A reinforcement learning algorithm is applied to train our model.
    Our experiments on two real-world datasets demonstrate that our model not only
    gains a significant increase in diversity but also improves generation quality
    over several state-of-the-art baselines.
  address: Hong Kong, China
  author:
  - first: Lihua
    full: Lihua Qian
    id: lihua-qian
    last: Qian
  - first: Lin
    full: Lin Qiu
    id: lin-qiu
    last: Qiu
  - first: Weinan
    full: Weinan Zhang
    id: weinan-zhang
    last: Zhang
  - first: Xin
    full: Xin Jiang
    id: xin-jiang
    last: Jiang
  - first: Yong
    full: Yong Yu
    id: yong-yu
    last: Yu
  author_string: Lihua Qian, Lin Qiu, Weinan Zhang, Xin Jiang, Yong Yu
  bibkey: qian-etal-2019-exploring
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1313
  month: November
  page_first: '3173'
  page_last: '3182'
  pages: "3173\u20133182"
  paper_id: '313'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1313.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1313.jpg
  title: Exploring Diverse Expressions for Paraphrase Generation
  title_html: Exploring Diverse Expressions for Paraphrase Generation
  url: https://www.aclweb.org/anthology/D19-1313
  year: '2019'
D19-1314:
  abstract: Generating text from graph-based data, such as Abstract Meaning Representation
    (AMR), is a challenging task due to the inherent difficulty in how to properly
    encode the structure of a graph with labeled edges. To address this difficulty,
    we propose a novel graph-to-sequence model that encodes different but complementary
    perspectives of the structural information contained in the AMR graph. The model
    learns parallel top-down and bottom-up representations of nodes capturing contrasting
    views of the graph. We also investigate the use of different node message passing
    strategies, employing different state-of-the-art graph encoders to compute node
    representations based on incoming and outgoing perspectives. In our experiments,
    we demonstrate that the dual graph representation leads to improvements in AMR-to-text
    generation, achieving state-of-the-art results on two AMR datasets
  address: Hong Kong, China
  attachment:
  - filename: D19-1314.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1314.Attachment.zip
  author:
  - first: Leonardo F. R.
    full: Leonardo F. R. Ribeiro
    id: leonardo-f-r-ribeiro
    last: Ribeiro
  - first: Claire
    full: Claire Gardent
    id: claire-gardent
    last: Gardent
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Leonardo F. R. Ribeiro, Claire Gardent, Iryna Gurevych
  bibkey: ribeiro-etal-2019-enhancing
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1314
  month: November
  page_first: '3183'
  page_last: '3194'
  pages: "3183\u20133194"
  paper_id: '314'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1314.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1314.jpg
  title: Enhancing AMR-to-Text Generation with Dual Graph Representations
  title_html: Enhancing <span class="acl-fixed-case">AMR</span>-to-Text Generation
    with Dual Graph Representations
  url: https://www.aclweb.org/anthology/D19-1314
  year: '2019'
D19-1315:
  abstract: The automated generation of information indicating the characteristics
    of articles such as headlines, key phrases, summaries and categories helps writers
    to alleviate their workload. Previous research has tackled these tasks using neural
    abstractive summarization and classification methods. However, the outputs may
    be inconsistent if they are generated individually. The purpose of our study is
    to generate multiple outputs consistently. We introduce a multi-task learning
    model with a shared encoder and multiple decoders for each task. We propose a
    novel loss function called hierarchical consistency loss to maintain consistency
    among the attention weights of the decoders. To evaluate the consistency, we employ
    a human evaluation. The results show that our model generates more consistent
    headlines, key phrases and categories. In addition, our model outperforms the
    baseline model on the ROUGE scores, and generates more adequate and fluent headlines.
  address: Hong Kong, China
  attachment:
  - filename: D19-1315.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1315.Attachment.pdf
  author:
  - first: Toru
    full: Toru Nishino
    id: toru-nishino
    last: Nishino
  - first: Shotaro
    full: Shotaro Misawa
    id: shotaro-misawa
    last: Misawa
  - first: Ryuji
    full: Ryuji Kano
    id: ryuji-kano
    last: Kano
  - first: Tomoki
    full: Tomoki Taniguchi
    id: tomoki-taniguchi
    last: Taniguchi
  - first: Yasuhide
    full: Yasuhide Miura
    id: yasuhide-miura
    last: Miura
  - first: Tomoko
    full: Tomoko Ohkuma
    id: tomoko-ohkuma
    last: Ohkuma
  author_string: Toru Nishino, Shotaro Misawa, Ryuji Kano, Tomoki Taniguchi, Yasuhide
    Miura, Tomoko Ohkuma
  bibkey: nishino-etal-2019-keeping
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1315
  month: November
  page_first: '3195'
  page_last: '3205'
  pages: "3195\u20133205"
  paper_id: '315'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1315.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1315.jpg
  title: Keeping Consistency of Sentence Generation and Document Classification with
    Multi-Task Learning
  title_html: Keeping Consistency of Sentence Generation and Document Classification
    with Multi-Task Learning
  url: https://www.aclweb.org/anthology/D19-1315
  year: '2019'
D19-1316:
  abstract: "In this paper, we introduce a novel task called feedback comment generation\
    \ \u2014 a task of automatically generating feedback comments such as a hint or\
    \ an explanatory note for writing learning for non-native learners of English.\
    \ There has been almost no work on this task nor corpus annotated with feedback\
    \ comments. We have taken the first step by creating learner corpora consisting\
    \ of approximately 1,900 essays where all preposition errors are manually annotated\
    \ with feedback comments. We have tested three baseline methods on the dataset,\
    \ showing that a simple neural retrieval-based method sets a baseline performance\
    \ with an F-measure of 0.34 to 0.41. Finally, we have looked into the results\
    \ to explore what modifications we need to make to achieve better performance.\
    \ We also have explored problems unaddressed in this work"
  address: Hong Kong, China
  author:
  - first: Ryo
    full: Ryo Nagata
    id: ryo-nagata
    last: Nagata
  author_string: Ryo Nagata
  bibkey: nagata-2019-toward
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1316
  month: November
  page_first: '3206'
  page_last: '3215'
  pages: "3206\u20133215"
  paper_id: '316'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1316.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1316.jpg
  title: Toward a Task of Feedback Comment Generation for Writing Learning
  title_html: Toward a Task of Feedback Comment Generation for Writing Learning
  url: https://www.aclweb.org/anthology/D19-1316
  year: '2019'
D19-1317:
  abstract: Question generation (QG) is the task of generating a question from a reference
    sentence and a specified answer within the sentence. A major challenge in QG is
    to identify answer-relevant context words to finish the declarative-to-interrogative
    sentence transformation. Existing sequence-to-sequence neural models achieve this
    goal by proximity-based answer position encoding under the intuition that neighboring
    words of answers are of high possibility to be answer-relevant. However, such
    intuition may not apply to all cases especially for sentences with complex answer-relevant
    relations. Consequently, the performance of these models drops sharply when the
    relative distance between the answer fragment and other non-stop sentence words
    that also appear in the ground truth question increases. To address this issue,
    we propose a method to jointly model the unstructured sentence and the structured
    answer-relevant relation (extracted from the sentence in advance) for question
    generation. Specifically, the structured answer-relevant relation acts as the
    to the point context and it thus naturally helps keep the generated question to
    the point, while the unstructured sentence provides the full information. Extensive
    experiments show that to the point context helps our question generation model
    achieve significant improvements on several automatic evaluation metrics. Furthermore,
    our model is capable of generating diverse questions for a sentence which conveys
    multiple relations of its answer fragment.
  address: Hong Kong, China
  author:
  - first: Jingjing
    full: Jingjing Li
    id: jingjing-li
    last: Li
  - first: Yifan
    full: Yifan Gao
    id: yifan-gao
    last: Gao
  - first: Lidong
    full: Lidong Bing
    id: lidong-bing
    last: Bing
  - first: Irwin
    full: Irwin King
    id: irwin-king
    last: King
  - first: Michael R.
    full: Michael R. Lyu
    id: michael-r-lyu
    last: Lyu
  author_string: Jingjing Li, Yifan Gao, Lidong Bing, Irwin King, Michael R. Lyu
  bibkey: li-etal-2019-improving-question
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1317
  month: November
  page_first: '3216'
  page_last: '3226'
  pages: "3216\u20133226"
  paper_id: '317'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1317.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1317.jpg
  title: Improving Question Generation With to the Point Context
  title_html: Improving Question Generation With to the Point Context
  url: https://www.aclweb.org/anthology/D19-1317
  year: '2019'
D19-1318:
  abstract: Most text-to-text generation tasks, for example text summarisation and
    text simplification, require copying words from the input to the output. We introduce
    Copycat, a transformer-based pointer network for such tasks which obtains competitive
    results in abstractive text summarisation and generates more abstractive summaries.
    We propose a further extension of this architecture for automatic post-editing,
    where generation is conditioned over two inputs (source language and machine translation),
    and the model is capable of deciding where to copy information from. This approach
    achieves competitive performance when compared to state-of-the-art automated post-editing
    systems. More importantly, we show that it addresses a well-known limitation of
    automatic post-editing - overcorrecting translations - and that our novel mechanism
    for copying source language words improves the results.
  address: Hong Kong, China
  author:
  - first: Julia
    full: Julia Ive
    id: julia-ive
    last: Ive
  - first: Pranava
    full: Pranava Madhyastha
    id: pranava-swaroop-madhyastha
    last: Madhyastha
  - first: Lucia
    full: Lucia Specia
    id: lucia-specia
    last: Specia
  author_string: Julia Ive, Pranava Madhyastha, Lucia Specia
  bibkey: ive-etal-2019-deep
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1318
  month: November
  page_first: '3227'
  page_last: '3236'
  pages: "3227\u20133236"
  paper_id: '318'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1318.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1318.jpg
  title: Deep Copycat Networks for Text-to-Text Generation
  title_html: Deep Copycat Networks for Text-to-Text Generation
  url: https://www.aclweb.org/anthology/D19-1318
  year: '2019'
D19-1319:
  abstract: In this paper, we propose a novel model RevGAN that automatically generates
    controllable and personalized user reviews based on the arbitrarily given sentimental
    and stylistic information. RevGAN utilizes the combination of three novel components,
    including self-attentive recursive autoencoders, conditional discriminators, and
    personalized decoders. We test its performance on the several real-world datasets,
    where our model significantly outperforms state-of-the-art generation models in
    terms of sentence quality, coherence, personalization, and human evaluations.
    We also empirically show that the generated reviews could not be easily distinguished
    from the organically produced reviews and that they follow the same statistical
    linguistics laws.
  address: Hong Kong, China
  author:
  - first: Pan
    full: Pan Li
    id: pan-li
    last: Li
  - first: Alexander
    full: Alexander Tuzhilin
    id: alexander-tuzhilin
    last: Tuzhilin
  author_string: Pan Li, Alexander Tuzhilin
  bibkey: li-tuzhilin-2019-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1319
  month: November
  page_first: '3237'
  page_last: '3245'
  pages: "3237\u20133245"
  paper_id: '319'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1319.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1319.jpg
  title: Towards Controllable and Personalized Review Generation
  title_html: Towards Controllable and Personalized Review Generation
  url: https://www.aclweb.org/anthology/D19-1319
  year: '2019'
D19-1320:
  abstract: "Abstractive summarization approaches based on Reinforcement Learning\
    \ (RL) have recently been proposed to overcome classical likelihood maximization.\
    \ RL enables to consider complex, possibly non differentiable, metrics that globally\
    \ assess the quality and relevance of the generated outputs. ROUGE, the most used\
    \ summarization metric, is known to suffer from bias towards lexical similarity\
    \ as well as from sub-optimal accounting for fluency and readability of the generated\
    \ abstracts. We thus explore and propose alternative evaluation measures: the\
    \ reported human-evaluation analysis shows that the proposed metrics, based on\
    \ Question Answering, favorably compare to ROUGE \u2013 with the additional property\
    \ of not requiring reference summaries. Training a RL-based model on these metrics\
    \ leads to improvements (both in terms of human or automated metrics) over current\
    \ approaches that use ROUGE as reward."
  address: Hong Kong, China
  author:
  - first: Thomas
    full: Thomas Scialom
    id: thomas-scialom
    last: Scialom
  - first: Sylvain
    full: Sylvain Lamprier
    id: sylvain-lamprier
    last: Lamprier
  - first: Benjamin
    full: Benjamin Piwowarski
    id: benjamin-piwowarski
    last: Piwowarski
  - first: Jacopo
    full: Jacopo Staiano
    id: jacopo-staiano
    last: Staiano
  author_string: Thomas Scialom, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano
  bibkey: scialom-etal-2019-answers
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1320
  month: November
  page_first: '3246'
  page_last: '3256'
  pages: "3246\u20133256"
  paper_id: '320'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1320.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1320.jpg
  title: Answers Unite! Unsupervised Metrics for Reinforced Summarization Models
  title_html: Answers Unite! Unsupervised Metrics for Reinforced Summarization Models
  url: https://www.aclweb.org/anthology/D19-1320
  year: '2019'
D19-1321:
  abstract: 'Existing neural methods for data-to-text generation are still struggling
    to produce long and diverse texts: they are insufficient to model input data dynamically
    during generation, to capture inter-sentence coherence, or to generate diversified
    expressions. To address these issues, we propose a Planning-based Hierarchical
    Variational Model (PHVM). Our model first plans a sequence of groups (each group
    is a subset of input items to be covered by a sentence) and then realizes each
    sentence conditioned on the planning result and the previously generated context,
    thereby decomposing long text generation into dependent sentence generation sub-tasks.
    To capture expression diversity, we devise a hierarchical latent structure where
    a global planning latent variable models the diversity of reasonable planning
    and a sequence of local latent variables controls sentence realization. Experiments
    show that our model outperforms state-of-the-art baselines in long and diverse
    text generation.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1321.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1321.Attachment.zip
  author:
  - first: Zhihong
    full: Zhihong Shao
    id: zhihong-shao
    last: Shao
  - first: Minlie
    full: Minlie Huang
    id: minlie-huang
    last: Huang
  - first: Jiangtao
    full: Jiangtao Wen
    id: jiangtao-wen
    last: Wen
  - first: Wenfei
    full: Wenfei Xu
    id: wenfei-xu
    last: Xu
  - first: Xiaoyan
    full: Xiaoyan Zhu
    id: xiaoyan-zhu
    last: Zhu
  author_string: Zhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, Xiaoyan Zhu
  bibkey: shao-etal-2019-long
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1321
  month: November
  page_first: '3257'
  page_last: '3268'
  pages: "3257\u20133268"
  paper_id: '321'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1321.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1321.jpg
  title: Long and Diverse Text Generation with Planning-based Hierarchical Variational
    Model
  title_html: Long and Diverse Text Generation with Planning-based Hierarchical Variational
    Model
  url: https://www.aclweb.org/anthology/D19-1321
  year: '2019'
D19-1322:
  abstract: "Text style transfer is the task of transferring the style of text having\
    \ certain stylistic attributes, while preserving non-stylistic or content information.\
    \ In this work we introduce the Generative Style Transformer (GST) - a new approach\
    \ to rewriting sentences to a target style in the absence of parallel style corpora.\
    \ GST leverages the power of both, large unsupervised pre-trained language models\
    \ as well as the Transformer. GST is a part of a larger \u2018Delete Retrieve\
    \ Generate\u2019 framework, in which we also propose a novel method of deleting\
    \ style attributes from the source sentence by exploiting the inner workings of\
    \ the Transformer. Our models outperform state-of-art systems across 5 datasets\
    \ on sentiment, gender and political slant transfer. We also propose the use of\
    \ the GLEU metric as an automatic metric of evaluation of style transfer, which\
    \ we found to compare better with human ratings than the predominantly used BLEU\
    \ score."
  address: Hong Kong, China
  author:
  - first: Akhilesh
    full: Akhilesh Sudhakar
    id: akhilesh-sudhakar
    last: Sudhakar
  - first: Bhargav
    full: Bhargav Upadhyay
    id: bhargav-upadhyay
    last: Upadhyay
  - first: Arjun
    full: Arjun Maheswaran
    id: arjun-maheswaran
    last: Maheswaran
  author_string: Akhilesh Sudhakar, Bhargav Upadhyay, Arjun Maheswaran
  bibkey: sudhakar-etal-2019-transforming
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1322
  month: November
  page_first: '3269'
  page_last: '3279'
  pages: "3269\u20133279"
  paper_id: '322'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1322.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1322.jpg
  title: "\u201CTransforming\u201D Delete, Retrieve, Generate Approach for Controlled\
    \ Text Style Transfer"
  title_html: "\u201CTransforming\u201D Delete, Retrieve, Generate Approach for Controlled\
    \ Text Style Transfer"
  url: https://www.aclweb.org/anthology/D19-1322
  year: '2019'
D19-1323:
  abstract: 'Abstractive summarization systems aim to produce more coherent and concise
    summaries than their extractive counterparts. Popular neural models have achieved
    impressive results for single-document summarization, yet their outputs are often
    incoherent and unfaithful to the input. In this paper, we introduce SENECA, a
    novel System for ENtity-drivEn Coherent Abstractive summarization framework that
    leverages entity information to generate informative and coherent abstracts. Our
    framework takes a two-step approach: (1) an entity-aware content selection module
    first identifies salient sentences from the input, then (2) an abstract generation
    module conducts cross-sentence information compression and abstraction to generate
    the final summary, which is trained with rewards to promote coherence, conciseness,
    and clarity. The two components are further connected using reinforcement learning.
    Automatic evaluation shows that our model significantly outperforms previous state-of-the-art
    based on ROUGE and our proposed coherence measures on New York Times and CNN/Daily
    Mail datasets. Human judges further rate our system summaries as more informative
    and coherent than those by popular summarization models.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1323.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1323.Attachment.zip
  author:
  - first: Eva
    full: Eva Sharma
    id: eva-sharma
    last: Sharma
  - first: Luyang
    full: Luyang Huang
    id: luyang-huang
    last: Huang
  - first: Zhe
    full: Zhe Hu
    id: zhe-hu
    last: Hu
  - first: Lu
    full: Lu Wang
    id: lu-wang
    last: Wang
  author_string: Eva Sharma, Luyang Huang, Zhe Hu, Lu Wang
  bibkey: sharma-etal-2019-entity
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1323
  month: November
  page_first: '3280'
  page_last: '3291'
  pages: "3280\u20133291"
  paper_id: '323'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1323.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1323.jpg
  title: An Entity-Driven Framework for Abstractive Summarization
  title_html: An Entity-Driven Framework for Abstractive Summarization
  url: https://www.aclweb.org/anthology/D19-1323
  year: '2019'
D19-1324:
  abstract: "Recent neural network approaches to summarization are largely either\
    \ selection-based extraction or generation-based abstraction. In this work, we\
    \ present a neural model for single-document summarization based on joint extraction\
    \ and syntactic compression. Our model chooses sentences from the document, identifies\
    \ possible compressions based on constituency parses, and scores those compressions\
    \ with a neural model to produce the final summary. For learning, we construct\
    \ oracle extractive-compressive summaries, then learn both of our components jointly\
    \ with this supervision. Experimental results on the CNN/Daily Mail and New York\
    \ Times datasets show that our model achieves strong performance (comparable to\
    \ state-of-the-art systems) as evaluated by ROUGE. Moreover, our approach outperforms\
    \ an off-the-shelf compression module, and human and manual evaluation shows that\
    \ our model\u2019s output generally remains grammatical."
  address: Hong Kong, China
  attachment:
  - filename: D19-1324.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1324.Attachment.pdf
  author:
  - first: Jiacheng
    full: Jiacheng Xu
    id: jiacheng-xu
    last: Xu
  - first: Greg
    full: Greg Durrett
    id: greg-durrett
    last: Durrett
  author_string: Jiacheng Xu, Greg Durrett
  bibkey: xu-durrett-2019-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1324
  month: November
  page_first: '3292'
  page_last: '3303'
  pages: "3292\u20133303"
  paper_id: '324'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1324.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1324.jpg
  title: Neural Extractive Text Summarization with Syntactic Compression
  title_html: Neural Extractive Text Summarization with Syntactic Compression
  url: https://www.aclweb.org/anthology/D19-1324
  year: '2019'
D19-1325:
  abstract: 'Text style transfer without parallel data has achieved some practical
    success. However, in the scenario where less data is available, these methods
    may yield poor performance. In this paper, we examine domain adaptation for text
    style transfer to leverage massively available data from other domains. These
    data may demonstrate domain shift, which impedes the benefits of utilizing such
    data for training. To address this challenge, we propose simple yet effective
    domain adaptive text style transfer models, enabling domain-adaptive information
    exchange. The proposed models presumably learn from the source domain to: (i)
    distinguish stylized information and generic content information; (ii) maximally
    preserve content information; and (iii) adaptively transfer the styles in a domain-aware
    manner. We evaluate the proposed models on two style transfer tasks (sentiment
    and formality) over multiple target domains where only limited non-parallel data
    is available. Extensive experiments demonstrate the effectiveness of the proposed
    model compared to the baselines.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1325.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1325.Attachment.pdf
  author:
  - first: Dianqi
    full: Dianqi Li
    id: dianqi-li
    last: Li
  - first: Yizhe
    full: Yizhe Zhang
    id: yizhe-zhang
    last: Zhang
  - first: Zhe
    full: Zhe Gan
    id: zhe-gan
    last: Gan
  - first: Yu
    full: Yu Cheng
    id: yu-cheng
    last: Cheng
  - first: Chris
    full: Chris Brockett
    id: chris-brockett
    last: Brockett
  - first: Bill
    full: Bill Dolan
    id: bill-dolan
    last: Dolan
  - first: Ming-Ting
    full: Ming-Ting Sun
    id: ming-ting-sun
    last: Sun
  author_string: Dianqi Li, Yizhe Zhang, Zhe Gan, Yu Cheng, Chris Brockett, Bill Dolan,
    Ming-Ting Sun
  bibkey: li-etal-2019-domain
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1325
  month: November
  page_first: '3304'
  page_last: '3313'
  pages: "3304\u20133313"
  paper_id: '325'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1325.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1325.jpg
  title: Domain Adaptive Text Style Transfer
  title_html: Domain Adaptive Text Style Transfer
  url: https://www.aclweb.org/anthology/D19-1325
  year: '2019'
D19-1326:
  abstract: In this work, we focus on the task of Automatic Question Generation (AQG)
    where given a passage and an answer the task is to generate the corresponding
    question. It is desired that the generated question should be (i) grammatically
    correct (ii) answerable from the passage and (iii) specific to the given answer.
    An analysis of existing AQG models shows that they produce questions which do
    not adhere to one or more of the above-mentioned qualities. In particular, the
    generated questions look like an incomplete draft of the desired question with
    a clear scope for refinement. To alleviate this shortcoming, we propose a method
    which tries to mimic the human process of generating questions by first creating
    an initial draft and then refining it. More specifically, we propose Refine Network
    (RefNet) which contains two decoders. The second decoder uses a dual attention
    network which pays attention to both (i) the original passage and (ii) the question
    (initial draft) generated by the first decoder. In effect, it refines the question
    generated by the first decoder, thereby making it more correct and complete. We
    evaluate RefNet on three datasets, viz., SQuAD, HOTPOT-QA, and DROP, and show
    that it outperforms existing state-of-the-art methods by 7-16% on all of these
    datasets. Lastly, we show that we can improve the quality of the second decoder
    on specific metrics, such as, fluency and answerability by explicitly rewarding
    revisions that improve on the corresponding metric during training. The code has
    been made publicly available .
  address: Hong Kong, China
  attachment:
  - filename: D19-1326.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1326.Attachment.pdf
  author:
  - first: Preksha
    full: Preksha Nema
    id: preksha-nema
    last: Nema
  - first: Akash Kumar
    full: Akash Kumar Mohankumar
    id: akash-kumar-mohankumar
    last: Mohankumar
  - first: Mitesh M.
    full: Mitesh M. Khapra
    id: mitesh-m-khapra
    last: Khapra
  - first: Balaji Vasan
    full: Balaji Vasan Srinivasan
    id: balaji-vasan-srinivasan
    last: Srinivasan
  - first: Balaraman
    full: Balaraman Ravindran
    id: balaraman-ravindran
    last: Ravindran
  author_string: Preksha Nema, Akash Kumar Mohankumar, Mitesh M. Khapra, Balaji Vasan
    Srinivasan, Balaraman Ravindran
  bibkey: nema-etal-2019-lets
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1326
  month: November
  page_first: '3314'
  page_last: '3323'
  pages: "3314\u20133323"
  paper_id: '326'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1326.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1326.jpg
  title: "Let\u2019s Ask Again: Refine Network for Automatic Question Generation"
  title_html: "Let\u2019s Ask Again: Refine Network for Automatic Question Generation"
  url: https://www.aclweb.org/anthology/D19-1326
  year: '2019'
D19-1327:
  abstract: 'Despite the recent developments on neural summarization systems, the
    underlying logic behind the improvements from the systems and its corpus-dependency
    remains largely unexplored. Position of sentences in the original text, for example,
    is a well known bias for news summarization. Following in the spirit of the claim
    that summarization is a combination of sub-functions, we define three sub-aspects
    of summarization: position, importance, and diversity and conduct an extensive
    analysis of the biases of each sub-aspect with respect to the domain of nine different
    summarization corpora (e.g., news, academic papers, meeting minutes, movie script,
    books, posts). We find that while position exhibits substantial bias in news articles,
    this is not the case, for example, with academic papers and meeting minutes. Furthermore,
    our empirical study shows that different types of summarization systems (e.g.,
    neural-based) are composed of different degrees of the sub-aspects. Our study
    provides useful lessons regarding consideration of underlying sub-aspects when
    collecting a new summarization dataset or developing a new system.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1327.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1327.Attachment.pdf
  author:
  - first: Taehee
    full: Taehee Jung
    id: taehee-jung
    last: Jung
  - first: Dongyeop
    full: Dongyeop Kang
    id: dongyeop-kang
    last: Kang
  - first: Lucas
    full: Lucas Mentch
    id: lucas-mentch
    last: Mentch
  - first: Eduard
    full: Eduard Hovy
    id: eduard-hovy
    last: Hovy
  author_string: Taehee Jung, Dongyeop Kang, Lucas Mentch, Eduard Hovy
  bibkey: jung-etal-2019-earlier
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1327
  month: November
  page_first: '3324'
  page_last: '3335'
  pages: "3324\u20133335"
  paper_id: '327'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1327.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1327.jpg
  title: "Earlier Isn\u2019t Always Better: Sub-aspect Analysis on Corpus and System\
    \ Biases in Summarization"
  title_html: "Earlier Isn\u2019t Always Better: Sub-aspect Analysis on Corpus and\
    \ System Biases in Summarization"
  url: https://www.aclweb.org/anthology/D19-1327
  year: '2019'
D19-1328:
  abstract: 'The task of bilingual dictionary induction (BDI) is commonly used for
    intrinsic evaluation of cross-lingual word embeddings. The largest dataset for
    BDI was generated automatically, so its quality is dubious. We study the composition
    and quality of the test sets for five diverse languages from this dataset, with
    concerning findings: (1) a quarter of the data consists of proper nouns, which
    can be hardly indicative of BDI performance, and (2) there are pervasive gaps
    in the gold-standard targets. These issues appear to affect the ranking between
    cross-lingual embedding systems on individual languages, and the overall degree
    to which the systems differ in performance. With proper nouns removed from the
    data, the margin between the top two systems included in the study grows from
    3.4% to 17.2%. Manual verification of the predictions, on the other hand, reveals
    that gaps in the gold standard targets artificially inflate the margin between
    the two systems on English to Bulgarian BDI from 0.1% to 6.7%. We thus suggest
    that future research either avoids drawing conclusions from quantitative results
    on this BDI dataset, or accompanies such evaluation with rigorous error analysis.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1328.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1328.Attachment.zip
  author:
  - first: Yova
    full: Yova Kementchedjhieva
    id: yova-kementchedjhieva
    last: Kementchedjhieva
  - first: Mareike
    full: Mareike Hartmann
    id: mareike-hartmann
    last: Hartmann
  - first: Anders
    full: "Anders S\xF8gaard"
    id: anders-sogaard
    last: "S\xF8gaard"
  author_string: "Yova Kementchedjhieva, Mareike Hartmann, Anders S\xF8gaard"
  bibkey: kementchedjhieva-etal-2019-lost
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1328
  month: November
  page_first: '3336'
  page_last: '3341'
  pages: "3336\u20133341"
  paper_id: '328'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1328.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1328.jpg
  title: 'Lost in Evaluation: Misleading Benchmarks for Bilingual Dictionary Induction'
  title_html: 'Lost in Evaluation: Misleading Benchmarks for Bilingual Dictionary
    Induction'
  url: https://www.aclweb.org/anthology/D19-1328
  year: '2019'
D19-1329:
  abstract: 'Development sets are impractical to obtain for real low-resource languages,
    since using all available data for training is often more effective. However,
    development sets are widely used in research papers that purport to deal with
    low-resource natural language processing (NLP). Here, we aim to answer the following
    questions: Does using a development set for early stopping in the low-resource
    setting influence results as compared to a more realistic alternative, where the
    number of training epochs is tuned on development languages? And does it lead
    to overestimation or underestimation of performance? We repeat multiple experiments
    from recent work on neural models for low-resource NLP and compare results for
    models obtained by training with and without development sets. On average over
    languages, absolute accuracy differs by up to 1.4%. However, for some languages
    and tasks, differences are as big as 18.0% accuracy. Our results highlight the
    importance of realistic experimental setups in the publication of low-resource
    NLP research results.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1329.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1329.Attachment.zip
  author:
  - first: Katharina
    full: Katharina Kann
    id: katharina-kann
    last: Kann
  - first: Kyunghyun
    full: Kyunghyun Cho
    id: kyunghyun-cho
    last: Cho
  - first: Samuel R.
    full: Samuel R. Bowman
    id: samuel-bowman
    last: Bowman
  author_string: Katharina Kann, Kyunghyun Cho, Samuel R. Bowman
  bibkey: kann-etal-2019-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1329
  month: November
  page_first: '3342'
  page_last: '3349'
  pages: "3342\u20133349"
  paper_id: '329'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1329.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1329.jpg
  title: 'Towards Realistic Practices In Low-Resource Natural Language Processing:
    The Development Set'
  title_html: 'Towards Realistic Practices In Low-Resource Natural Language Processing:
    The Development Set'
  url: https://www.aclweb.org/anthology/D19-1329
  year: '2019'
D19-1330:
  abstract: In this paper, we introduce a novel interactive approach to translate
    a source language into two different languages simultaneously and interactively.
    Specifically, the generation of one language relies on not only previously generated
    outputs by itself, but also the outputs predicted in the other language. Experimental
    results on IWSLT and WMT datasets demonstrate that our method can obtain significant
    improvements over both conventional Neural Machine Translation (NMT) model and
    multilingual NMT model.
  address: Hong Kong, China
  author:
  - first: Yining
    full: Yining Wang
    id: yining-wang
    last: Wang
  - first: Jiajun
    full: Jiajun Zhang
    id: jiajun-zhang
    last: Zhang
  - first: Long
    full: Long Zhou
    id: long-zhou
    last: Zhou
  - first: Yuchen
    full: Yuchen Liu
    id: yuchen-liu
    last: Liu
  - first: Chengqing
    full: Chengqing Zong
    id: chengqing-zong
    last: Zong
  author_string: Yining Wang, Jiajun Zhang, Long Zhou, Yuchen Liu, Chengqing Zong
  bibkey: wang-etal-2019-synchronously
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1330
  month: November
  page_first: '3350'
  page_last: '3355'
  pages: "3350\u20133355"
  paper_id: '330'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1330.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1330.jpg
  title: Synchronously Generating Two Languages with Interactive Decoding
  title_html: Synchronously Generating Two Languages with Interactive Decoding
  url: https://www.aclweb.org/anthology/D19-1330
  year: '2019'
D19-1331:
  abstract: We report on search errors and model errors in neural machine translation
    (NMT). We present an exact inference procedure for neural sequence models based
    on a combination of beam search and depth-first search. We use our exact search
    to find the global best model scores under a Transformer base model for the entire
    WMT15 English-German test set. Surprisingly, beam search fails to find these global
    best model scores in most cases, even with a very large beam size of 100. For
    more than 50% of the sentences, the model in fact assigns its global best score
    to the empty translation, revealing a massive failure of neural models in properly
    accounting for adequacy. We show by constraining search with a minimum translation
    length that at the root of the problem of empty translations lies an inherent
    bias towards shorter translations. We conclude that vanilla NMT in its current
    form requires just the right amount of beam search errors, which, from a modelling
    perspective, is a highly unsatisfactory conclusion indeed, as the model often
    prefers an empty translation.
  address: Hong Kong, China
  author:
  - first: Felix
    full: Felix Stahlberg
    id: felix-stahlberg
    last: Stahlberg
  - first: Bill
    full: Bill Byrne
    id: bill-byrne
    last: Byrne
  author_string: Felix Stahlberg, Bill Byrne
  bibkey: stahlberg-byrne-2019-nmt
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1331
  month: November
  page_first: '3356'
  page_last: '3362'
  pages: "3356\u20133362"
  paper_id: '331'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1331.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1331.jpg
  title: 'On NMT Search Errors and Model Errors: Cat Got Your Tongue?'
  title_html: 'On <span class="acl-fixed-case">NMT</span> Search Errors and Model
    Errors: Cat Got Your Tongue?'
  url: https://www.aclweb.org/anthology/D19-1331
  year: '2019'
D19-1332:
  abstract: Understanding time is crucial for understanding events expressed in natural
    language. Because people rarely say the obvious, it is often necessary to have
    commonsense knowledge about various temporal aspects of events, such as duration,
    frequency, and temporal order. However, this important problem has so far received
    limited attention. This paper systematically studies this temporal commonsense
    problem. Specifically, we define five classes of temporal commonsense, and use
    crowdsourcing to develop a new dataset, MCTACO, that serves as a test set for
    this task. We find that the best current methods used on MCTACO are still far
    behind human performance, by about 20%, and discuss several directions for improvement.
    We hope that the new dataset and our study here can foster more future research
    on this topic.
  address: Hong Kong, China
  attachment:
  - filename: D19-1332.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1332.Attachment.zip
  author:
  - first: Ben
    full: Ben Zhou
    id: ben-zhou
    last: Zhou
  - first: Daniel
    full: Daniel Khashabi
    id: daniel-khashabi
    last: Khashabi
  - first: Qiang
    full: Qiang Ning
    id: qiang-ning
    last: Ning
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Ben Zhou, Daniel Khashabi, Qiang Ning, Dan Roth
  bibkey: zhou-etal-2019-going
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1332
  month: November
  page_first: '3363'
  page_last: '3369'
  pages: "3363\u20133369"
  paper_id: '332'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1332.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1332.jpg
  title: "\u201CGoing on a vacation\u201D takes longer than \u201CGoing for a walk\u201D\
    : A Study of Temporal Commonsense Understanding"
  title_html: "\u201CGoing on a vacation\u201D takes longer than \u201CGoing for a\
    \ walk\u201D: A Study of Temporal Commonsense Understanding"
  url: https://www.aclweb.org/anthology/D19-1332
  year: '2019'
D19-1333:
  abstract: Standard accuracy metrics indicate that modern reading comprehension systems
    have achieved strong performance in many question answering datasets. However,
    the extent these systems truly understand language remains unknown, and existing
    systems are not good at distinguishing distractor sentences which look related
    but do not answer the question. To address this problem, we propose QAInfomax
    as a regularizer in reading comprehension systems by maximizing mutual information
    among passages, a question, and its answer. QAInfomax helps regularize the model
    to not simply learn the superficial correlation for answering the questions. The
    experiments show that our proposed QAInfomax achieves the state-of-the-art performance
    on the benchmark Adversarial-SQuAD dataset.
  address: Hong Kong, China
  attachment:
  - filename: D19-1333.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1333.Attachment.zip
  author:
  - first: Yi-Ting
    full: Yi-Ting Yeh
    id: yi-ting-yeh
    last: Yeh
  - first: Yun-Nung
    full: Yun-Nung Chen
    id: yun-nung-chen
    last: Chen
  author_string: Yi-Ting Yeh, Yun-Nung Chen
  bibkey: yeh-chen-2019-qainfomax
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1333
  month: November
  page_first: '3370'
  page_last: '3375'
  pages: "3370\u20133375"
  paper_id: '333'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1333.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1333.jpg
  title: 'QAInfomax: Learning Robust Question Answering System by Mutual Information
    Maximization'
  title_html: '<span class="acl-fixed-case">QAI</span>nfomax: Learning Robust Question
    Answering System by Mutual Information Maximization'
  url: https://www.aclweb.org/anthology/D19-1333
  year: '2019'
D19-1334:
  abstract: Multi-hop knowledge graph (KG) reasoning is an effective and explainable
    method for predicting the target entity via reasoning paths in query answering
    (QA) task. Most previous methods assume that every relation in KGs has enough
    triples for training, regardless of those few-shot relations which cannot provide
    sufficient triples for training robust reasoning models. In fact, the performance
    of existing multi-hop reasoning methods drops significantly on few-shot relations.
    In this paper, we propose a meta-based multi-hop reasoning method (Meta-KGR),
    which adopts meta-learning to learn effective meta parameters from high-frequency
    relations that could quickly adapt to few-shot relations. We evaluate Meta-KGR
    on two public datasets sampled from Freebase and NELL, and the experimental results
    show that Meta-KGR outperforms state-of-the-art methods in few-shot scenarios.
    In the future, our codes and datasets will also be available to provide more details.
  address: Hong Kong, China
  author:
  - first: Xin
    full: Xin Lv
    id: xin-lv
    last: Lv
  - first: Yuxian
    full: Yuxian Gu
    id: yuxian-gu
    last: Gu
  - first: Xu
    full: Xu Han
    id: xu-han
    last: Han
  - first: Lei
    full: Lei Hou
    id: lei-hou
    last: Hou
  - first: Juanzi
    full: Juanzi Li
    id: juanzi-li
    last: Li
  - first: Zhiyuan
    full: Zhiyuan Liu
    id: zhiyuan-liu
    last: Liu
  author_string: Xin Lv, Yuxian Gu, Xu Han, Lei Hou, Juanzi Li, Zhiyuan Liu
  bibkey: lv-etal-2019-adapting
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1334
  month: November
  page_first: '3376'
  page_last: '3381'
  pages: "3376\u20133381"
  paper_id: '334'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1334.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1334.jpg
  title: Adapting Meta Knowledge Graph Information for Multi-Hop Reasoning over Few-Shot
    Relations
  title_html: Adapting Meta Knowledge Graph Information for Multi-Hop Reasoning over
    Few-Shot Relations
  url: https://www.aclweb.org/anthology/D19-1334
  year: '2019'
D19-1335:
  abstract: Recent studies have significantly improved the state-of-the-art on common-sense
    reasoning (CSR) benchmarks like the Winograd Schema Challenge (WSC) and SWAG.
    The question we ask in this paper is whether improved performance on these benchmarks
    represents genuine progress towards common-sense-enabled systems. We make case
    studies of both benchmarks and design protocols that clarify and qualify the results
    of previous work by analyzing threats to the validity of previous experimental
    designs. Our protocols account for several properties prevalent in common-sense
    benchmarks including size limitations, structural regularities, and variable instance
    difficulty.
  address: Hong Kong, China
  attachment:
  - filename: D19-1335.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1335.Attachment.pdf
  author:
  - first: Paul
    full: Paul Trichelair
    id: paul-trichelair
    last: Trichelair
  - first: Ali
    full: Ali Emami
    id: ali-emami
    last: Emami
  - first: Adam
    full: Adam Trischler
    id: adam-trischler
    last: Trischler
  - first: Kaheer
    full: Kaheer Suleman
    id: kaheer-suleman
    last: Suleman
  - first: Jackie Chi Kit
    full: Jackie Chi Kit Cheung
    id: jackie-chi-kit-cheung
    last: Cheung
  author_string: Paul Trichelair, Ali Emami, Adam Trischler, Kaheer Suleman, Jackie
    Chi Kit Cheung
  bibkey: trichelair-etal-2019-reasonable
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1335
  month: November
  page_first: '3382'
  page_last: '3387'
  pages: "3382\u20133387"
  paper_id: '335'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1335.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1335.jpg
  title: 'How Reasonable are Common-Sense Reasoning Tasks: A Case-Study on the Winograd
    Schema Challenge and SWAG'
  title_html: 'How Reasonable are Common-Sense Reasoning Tasks: A Case-Study on the
    <span class="acl-fixed-case">W</span>inograd Schema Challenge and <span class="acl-fixed-case">SWAG</span>'
  url: https://www.aclweb.org/anthology/D19-1335
  year: '2019'
D19-1336:
  abstract: In this paper, we focus on the task of generating a pun sentence given
    a pair of word senses. A major challenge for pun generation is the lack of large-scale
    pun corpus to guide supervised learning. To remedy this, we propose an adversarial
    generative network for pun generation (Pun-GAN). It consists of a generator to
    produce pun sentences, and a discriminator to distinguish between the generated
    pun sentences and the real sentences with specific word senses. The output of
    the discriminator is then used as a reward to train the generator via reinforcement
    learning, encouraging it to produce pun sentences which can support two word senses
    simultaneously. Experiments show that the proposed Pun-GAN can generate sentences
    that are more ambiguous and diverse in both automatic and human evaluation.
  address: Hong Kong, China
  author:
  - first: Fuli
    full: Fuli Luo
    id: fuli-luo
    last: Luo
  - first: Shunyao
    full: Shunyao Li
    id: shunyao-li
    last: Li
  - first: Pengcheng
    full: Pengcheng Yang
    id: pengcheng-yang
    last: Yang
  - first: Lei
    full: Lei Li
    id: lei-li
    last: Li
  - first: Baobao
    full: Baobao Chang
    id: baobao-chang
    last: Chang
  - first: Zhifang
    full: Zhifang Sui
    id: zhifang-sui
    last: Sui
  - first: Xu
    full: Xu Sun
    id: xu-sun
    last: Sun
  author_string: Fuli Luo, Shunyao Li, Pengcheng Yang, Lei Li, Baobao Chang, Zhifang
    Sui, Xu Sun
  bibkey: luo-etal-2019-pun
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1336
  month: November
  page_first: '3388'
  page_last: '3393'
  pages: "3388\u20133393"
  paper_id: '336'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1336.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1336.jpg
  title: 'Pun-GAN: Generative Adversarial Network for Pun Generation'
  title_html: 'Pun-<span class="acl-fixed-case">GAN</span>: Generative Adversarial
    Network for Pun Generation'
  url: https://www.aclweb.org/anthology/D19-1336
  year: '2019'
D19-1337:
  abstract: This paper explores the task of answer-aware questions generation. Based
    on the attention-based pointer generator model, we propose to incorporate an auxiliary
    task of language modeling to help question generation in a hierarchical multi-task
    learning structure. Our joint-learning model enables the encoder to learn a better
    representation of the input sequence, which will guide the decoder to generate
    more coherent and fluent questions. On both SQuAD and MARCO datasets, our multi-task
    learning model boosts the performance, achieving state-of-the-art results. Moreover,
    human evaluation further proves the high quality of our generated questions.
  address: Hong Kong, China
  author:
  - first: Wenjie
    full: Wenjie Zhou
    id: wenjie-zhou
    last: Zhou
  - first: Minghua
    full: Minghua Zhang
    id: minghua-zhang
    last: Zhang
  - first: Yunfang
    full: Yunfang Wu
    id: yunfang-wu
    last: Wu
  author_string: Wenjie Zhou, Minghua Zhang, Yunfang Wu
  bibkey: zhou-etal-2019-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1337
  month: November
  page_first: '3394'
  page_last: '3399'
  pages: "3394\u20133399"
  paper_id: '337'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1337.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1337.jpg
  title: Multi-Task Learning with Language Modeling for Question Generation
  title_html: Multi-Task Learning with Language Modeling for Question Generation
  url: https://www.aclweb.org/anthology/D19-1337
  year: '2019'
D19-1338:
  abstract: Autoregressive state transitions, where predictions are conditioned on
    past predictions, are the predominant choice for both deterministic and stochastic
    sequential models. However, autoregressive feedback exposes the evolution of the
    hidden state trajectory to potential biases from well-known train-test discrepancies.
    In this paper, we combine a latent state space model with a CRF observation model.
    We argue that such autoregressive observation models form an interesting middle
    ground that expresses local correlations on the word level but keeps the state
    evolution non-autoregressive. On unconditional sentence generation we show performance
    improvements compared to RNN and GAN baselines while avoiding some prototypical
    failure modes of autoregressive models.
  address: Hong Kong, China
  attachment:
  - filename: D19-1338.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1338.Attachment.pdf
  author:
  - first: Florian
    full: Florian Schmidt
    id: florian-schmidt
    last: Schmidt
  - first: Stephan
    full: Stephan Mandt
    id: stephan-mandt
    last: Mandt
  - first: Thomas
    full: Thomas Hofmann
    id: thomas-hofmann
    last: Hofmann
  author_string: Florian Schmidt, Stephan Mandt, Thomas Hofmann
  bibkey: schmidt-etal-2019-autoregressive
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1338
  month: November
  page_first: '3400'
  page_last: '3406'
  pages: "3400\u20133406"
  paper_id: '338'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1338.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1338.jpg
  title: Autoregressive Text Generation Beyond Feedback Loops
  title_html: Autoregressive Text Generation Beyond Feedback Loops
  url: https://www.aclweb.org/anthology/D19-1338
  year: '2019'
D19-1339:
  abstract: We present a systematic study of biases in natural language generation
    (NLG) by analyzing text generated from prompts that contain mentions of different
    demographic groups. In this work, we introduce the notion of the regard towards
    a demographic, use the varying levels of regard towards different demographics
    as a defining metric for bias in NLG, and analyze the extent to which sentiment
    scores are a relevant proxy metric for regard. To this end, we collect strategically-generated
    text from language models and manually annotate the text with both sentiment and
    regard scores. Additionally, we build an automatic regard classifier through transfer
    learning, so that we can analyze biases in unseen text. Together, these methods
    reveal the extent of the biased nature of language model generations. Our analysis
    provides a study of biases in NLG, bias metrics and correlated human judgments,
    and empirical evidence on the usefulness of our annotated dataset.
  address: Hong Kong, China
  attachment:
  - filename: D19-1339.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1339.Attachment.zip
  author:
  - first: Emily
    full: Emily Sheng
    id: emily-sheng
    last: Sheng
  - first: Kai-Wei
    full: Kai-Wei Chang
    id: kai-wei-chang
    last: Chang
  - first: Premkumar
    full: Premkumar Natarajan
    id: prem-natarajan
    last: Natarajan
  - first: Nanyun
    full: Nanyun Peng
    id: nanyun-peng
    last: Peng
  author_string: Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, Nanyun Peng
  bibkey: sheng-etal-2019-woman
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1339
  month: November
  page_first: '3407'
  page_last: '3412'
  pages: "3407\u20133412"
  paper_id: '339'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1339.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1339.jpg
  title: 'The Woman Worked as a Babysitter: On Biases in Language Generation'
  title_html: 'The Woman Worked as a Babysitter: On Biases in Language Generation'
  url: https://www.aclweb.org/anthology/D19-1339
  year: '2019'
D19-1340:
  abstract: While neural networks produce state-of-the-art performance in many NLP
    tasks, they generally learn from lexical information, which may transfer poorly
    between domains. Here, we investigate the importance that a model assigns to various
    aspects of data while learning and making predictions, specifically, in a recognizing
    textual entailment (RTE) task. By inspecting the attention weights assigned by
    the model, we confirm that most of the weights are assigned to noun phrases. To
    mitigate this dependence on lexicalized information, we experiment with two strategies
    of masking. First, we replace named entities with their corresponding semantic
    tags along with a unique identifier to indicate lexical overlap between claim
    and evidence. Second, we similarly replace other word classes in the sentence
    (nouns, verbs, adjectives, and adverbs) with their super sense tags (Ciaramita
    and Johnson, 2003). Our results show that, while performance on the in-domain
    dataset remains on par with that of the model trained on fully lexicalized data,
    it improves considerably when tested out of domain. For example, the performance
    of a state-of-the-art RTE model trained on the masked Fake News Challenge (Pomerleau
    and Rao, 2017) data and evaluated on Fact Extraction and Verification (Thorne
    et al., 2018) data improved by over 10% in accuracy score compared to the fully
    lexicalized model.
  address: Hong Kong, China
  author:
  - first: Sandeep
    full: Sandeep Suntwal
    id: sandeep-suntwal
    last: Suntwal
  - first: Mithun
    full: Mithun Paul
    id: mithun-paul
    last: Paul
  - first: Rebecca
    full: Rebecca Sharp
    id: rebecca-sharp
    last: Sharp
  - first: Mihai
    full: Mihai Surdeanu
    id: mihai-surdeanu
    last: Surdeanu
  author_string: Sandeep Suntwal, Mithun Paul, Rebecca Sharp, Mihai Surdeanu
  bibkey: suntwal-etal-2019-importance
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1340
  month: November
  page_first: '3413'
  page_last: '3418'
  pages: "3413\u20133418"
  paper_id: '340'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1340.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1340.jpg
  title: On the Importance of Delexicalization for Fact Verification
  title_html: On the Importance of Delexicalization for Fact Verification
  url: https://www.aclweb.org/anthology/D19-1340
  year: '2019'
D19-1341:
  abstract: Fact verification requires validating a claim in the context of evidence.
    We show, however, that in the popular FEVER dataset this might not necessarily
    be the case. Claim-only classifiers perform competitively with top evidence-aware
    models. In this paper, we investigate the cause of this phenomenon, identifying
    strong cues for predicting labels solely based on the claim, without considering
    any evidence. We create an evaluation set that avoids those idiosyncrasies. The
    performance of FEVER-trained models significantly drops when evaluated on this
    test set. Therefore, we introduce a regularization method which alleviates the
    effect of bias in the training data, obtaining improvements on the newly created
    test set. This work is a step towards a more sound evaluation of reasoning capabilities
    in fact verification models.
  address: Hong Kong, China
  attachment:
  - filename: D19-1341.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1341.Attachment.pdf
  author:
  - first: Tal
    full: Tal Schuster
    id: tal-schuster
    last: Schuster
  - first: Darsh
    full: Darsh Shah
    id: darsh-shah
    last: Shah
  - first: Yun Jie Serene
    full: Yun Jie Serene Yeo
    id: yun-jie-serene-yeo
    last: Yeo
  - first: Daniel
    full: Daniel Roberto Filizzola Ortiz
    id: daniel-roberto-filizzola-ortiz
    last: Roberto Filizzola Ortiz
  - first: Enrico
    full: Enrico Santus
    id: enrico-santus
    last: Santus
  - first: Regina
    full: Regina Barzilay
    id: regina-barzilay
    last: Barzilay
  author_string: Tal Schuster, Darsh Shah, Yun Jie Serene Yeo, Daniel Roberto Filizzola
    Ortiz, Enrico Santus, Regina Barzilay
  bibkey: schuster-etal-2019-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1341
  month: November
  page_first: '3419'
  page_last: '3425'
  pages: "3419\u20133425"
  paper_id: '341'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1341.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1341.jpg
  title: Towards Debiasing Fact Verification Models
  title_html: Towards Debiasing Fact Verification Models
  url: https://www.aclweb.org/anthology/D19-1341
  year: '2019'
D19-1342:
  abstract: "Aspect-level sentiment classification, which is a fine-grained sentiment\
    \ analysis task, has received lots of attention these years. There is a phenomenon\
    \ that people express both positive and negative sentiments towards an aspect\
    \ at the same time. Such opinions with conflicting sentiments, however, are ignored\
    \ by existing studies, which design models based on the absence of them. We argue\
    \ that the exclusion of conflict opinions is problematic, for the reason that\
    \ it represents an important style of human thinking \u2013 dialectic thinking.\
    \ If a real-world sentiment classification system ignores the existence of conflict\
    \ opinions when it is designed, it will incorrectly mixed conflict opinions into\
    \ other sentiment polarity categories in action. Existing models have problems\
    \ when recognizing conflicting opinions, such as data sparsity. In this paper,\
    \ we propose a multi-label classification model with dual attention mechanism\
    \ to address these problems."
  address: Hong Kong, China
  attachment:
  - filename: D19-1342.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1342.Attachment.zip
  author:
  - first: Xingwei
    full: Xingwei Tan
    id: xingwei-tan
    last: Tan
  - first: Yi
    full: Yi Cai
    id: yi-cai
    last: Cai
  - first: Changxi
    full: Changxi Zhu
    id: changxi-zhu
    last: Zhu
  author_string: Xingwei Tan, Yi Cai, Changxi Zhu
  bibkey: tan-etal-2019-recognizing
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1342
  month: November
  page_first: '3426'
  page_last: '3431'
  pages: "3426\u20133431"
  paper_id: '342'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1342.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1342.jpg
  title: Recognizing Conflict Opinions in Aspect-level Sentiment Classification with
    Dual Attention Networks
  title_html: Recognizing Conflict Opinions in Aspect-level Sentiment Classification
    with Dual Attention Networks
  url: https://www.aclweb.org/anthology/D19-1342
  year: '2019'
D19-1343:
  abstract: Deep neural network models such as long short-term memory (LSTM) and tree-LSTM
    have been proven to be effective for sentiment analysis. However, sequential LSTM
    is a bias model wherein the words in the tail of a sentence are more heavily emphasized
    than those in the header for building sentence representations. Even tree-LSTM,
    with useful structural information, could not avoid the bias problem because the
    root node will be dominant and the nodes in the bottom of the parse tree will
    be less emphasized even though they may contain salient information. To overcome
    the bias problem, this study proposes a capsule tree-LSTM model, introducing a
    dynamic routing algorithm as an aggregation layer to build sentence representation
    by assigning different weights to nodes according to their contributions to prediction.
    Experiments on Stanford Sentiment Treebank (SST) for sentiment classification
    and EmoBank for regression show that the proposed method improved the performance
    of tree-LSTM and other neural network models. In addition, the deeper the tree
    structure, the bigger the improvement.
  address: Hong Kong, China
  author:
  - first: Jin
    full: Jin Wang
    id: jin-wang
    last: Wang
  - first: Liang-Chih
    full: Liang-Chih Yu
    id: liang-chih-yu
    last: Yu
  - first: K. Robert
    full: K. Robert Lai
    id: k-robert-lai
    last: Lai
  - first: Xuejie
    full: Xuejie Zhang
    id: xuejie-zhang
    last: Zhang
  author_string: Jin Wang, Liang-Chih Yu, K. Robert Lai, Xuejie Zhang
  bibkey: wang-etal-2019-investigating
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1343
  month: November
  page_first: '3432'
  page_last: '3437'
  pages: "3432\u20133437"
  paper_id: '343'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1343.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1343.jpg
  title: Investigating Dynamic Routing in Tree-Structured LSTM for Sentiment Analysis
  title_html: Investigating Dynamic Routing in Tree-Structured <span class="acl-fixed-case">LSTM</span>
    for Sentiment Analysis
  url: https://www.aclweb.org/anthology/D19-1343
  year: '2019'
D19-1344:
  abstract: In this paper, we provide a simple and effective baseline for classifying
    both patents and papers to the well-established Cooperative Patent Classification
    (CPC). We propose a label-informative classifier based on the Wide & Deep structure,
    where the Wide part encodes string-level similarities between texts and labels,
    and the Deep part captures semantic-level similarities via non-linear transformations.
    Our model trains on millions of patents, and transfers to papers by developing
    distant-supervised training set and domain-specific features. Extensive experiments
    show that our model achieves comparable performance to the state-of-the-art model
    used in industry on both patents and papers. The output of this work should facilitate
    the searching, granting and filing of innovative ideas for patent examiners, attorneys
    and researchers.
  address: Hong Kong, China
  attachment:
  - filename: D19-1344.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1344.Attachment.zip
  author:
  - first: Muyao
    full: Muyao Niu
    id: muyao-niu
    last: Niu
  - first: Jie
    full: Jie Cai
    id: jie-cai
    last: Cai
  author_string: Muyao Niu, Jie Cai
  bibkey: niu-cai-2019-label
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1344
  month: November
  page_first: '3438'
  page_last: '3443'
  pages: "3438\u20133443"
  paper_id: '344'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1344.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1344.jpg
  title: A Label Informative Wide & Deep Classifier for Patents and Papers
  title_html: A Label Informative Wide &amp; Deep Classifier for Patents and Papers
  url: https://www.aclweb.org/anthology/D19-1344
  year: '2019'
D19-1345:
  abstract: "Recently, researches have explored the graph neural network (GNN) techniques\
    \ on text classification, since GNN does well in handling complex structures and\
    \ preserving global information. However, previous methods based on GNN are mainly\
    \ faced with the practical problems of fixed corpus level graph structure which\
    \ don\u2019t support online testing and high memory consumption. To tackle the\
    \ problems, we propose a new GNN based model that builds graphs for each input\
    \ text with global parameters sharing instead of a single graph for the whole\
    \ corpus. This method removes the burden of dependence between an individual text\
    \ and entire corpus which support online testing, but still preserve global information.\
    \ Besides, we build graphs by much smaller windows in the text, which not only\
    \ extract more local features but also significantly reduce the edge numbers as\
    \ well as memory consumption. Experiments show that our model outperforms existing\
    \ models on several text classification datasets even with consuming less memory."
  address: Hong Kong, China
  author:
  - first: Lianzhe
    full: Lianzhe Huang
    id: lianzhe-huang
    last: Huang
  - first: Dehong
    full: Dehong Ma
    id: dehong-ma
    last: Ma
  - first: Sujian
    full: Sujian Li
    id: sujian-li
    last: Li
  - first: Xiaodong
    full: Xiaodong Zhang
    id: xiaodong-zhang
    last: Zhang
  - first: Houfeng
    full: Houfeng Wang
    id: houfeng-wang
    last: Wang
  author_string: Lianzhe Huang, Dehong Ma, Sujian Li, Xiaodong Zhang, Houfeng Wang
  bibkey: huang-etal-2019-text
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1345
  month: November
  page_first: '3444'
  page_last: '3450'
  pages: "3444\u20133450"
  paper_id: '345'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1345.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1345.jpg
  title: Text Level Graph Neural Network for Text Classification
  title_html: Text Level Graph Neural Network for Text Classification
  url: https://www.aclweb.org/anthology/D19-1345
  year: '2019'
D19-1346:
  abstract: Applications such as textual entailment, plagiarism detection or document
    clustering rely on the notion of semantic similarity, and are usually approached
    with dimension reduction techniques like LDA or with embedding-based neural approaches.
    We present a scenario where semantic similarity is not enough, and we devise a
    neural approach to learn semantic relatedness. The scenario is text spotting in
    the wild, where a text in an image (e.g. street sign, advertisement or bus destination)
    must be identified and recognized. Our goal is to improve the performance of vision
    systems by leveraging semantic information. Our rationale is that the text to
    be spotted is often related to the image context in which it appears (word pairs
    such as Delta-airplane, or quarters-parking are not similar, but are clearly related).
    We show how learning a word-to-word or word-to-sentence relatedness score can
    improve the performance of text spotting systems up to 2.9 points, outperforming
    other measures in a benchmark dataset.
  address: Hong Kong, China
  author:
  - first: Ahmed
    full: Ahmed Sabir
    id: ahmed-sabir
    last: Sabir
  - first: Francesc
    full: Francesc Moreno
    id: francesc-moreno
    last: Moreno
  - first: "Llu\xEDs"
    full: "Llu\xEDs Padr\xF3"
    id: lluis-padro
    last: "Padr\xF3"
  author_string: "Ahmed Sabir, Francesc Moreno, Llu\xEDs Padr\xF3"
  bibkey: sabir-etal-2019-semantic
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1346
  month: November
  page_first: '3451'
  page_last: '3457'
  pages: "3451\u20133457"
  paper_id: '346'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1346.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1346.jpg
  title: Semantic Relatedness Based Re-ranker for Text Spotting
  title_html: Semantic Relatedness Based Re-ranker for Text Spotting
  url: https://www.aclweb.org/anthology/D19-1346
  year: '2019'
D19-1347:
  abstract: 'We propose a novel and simple method for semi-supervised text classification.
    The method stems from the hypothesis that a classifier with pretrained word embeddings
    always outperforms the same classifier with randomly initialized word embeddings,
    as empirically observed in NLP tasks. Our method first builds two sets of classifiers
    as a form of model ensemble, and then initializes their word embeddings differently:
    one using random, the other using pretrained word embeddings. We focus on different
    predictions between the two classifiers on unlabeled data while following the
    self-training framework. We also use early-stopping in meta-epoch to improve the
    performance of our method. Our method, Delta-training, outperforms the self-training
    and the co-training framework in 4 different text classification datasets, showing
    robustness against error accumulation.'
  address: Hong Kong, China
  author:
  - first: Hwiyeol
    full: Hwiyeol Jo
    id: hwiyeol-jo
    last: Jo
  - first: Ceyda
    full: Ceyda Cinarel
    id: ceyda-cinarel
    last: Cinarel
  author_string: Hwiyeol Jo, Ceyda Cinarel
  bibkey: jo-cinarel-2019-delta
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1347
  month: November
  page_first: '3458'
  page_last: '3463'
  pages: "3458\u20133463"
  paper_id: '347'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1347.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1347.jpg
  title: 'Delta-training: Simple Semi-Supervised Text Classification using Pretrained
    Word Embeddings'
  title_html: 'Delta-training: Simple Semi-Supervised Text Classification using Pretrained
    Word Embeddings'
  url: https://www.aclweb.org/anthology/D19-1347
  year: '2019'
D19-1348:
  abstract: We present 1) a work in progress method to visually segment key regions
    of scientific articles using an object detection technique augmented with contextual
    features, and 2) a novel dataset of region-labeled articles. A continuing challenge
    in scientific literature mining is the difficulty of consistently extracting high-quality
    text from formatted PDFs. To address this, we adapt the object-detection technique
    Faster R-CNN for document layout detection, incorporating contextual information
    that leverages the inherently localized nature of article contents to improve
    the region detection performance. Due to the limited availability of high-quality
    region-labels for scientific articles, we also contribute a novel dataset of region
    annotations, the first version of which covers 9 region classes and 822 article
    pages. Initial experimental results demonstrate a 23.9% absolute improvement in
    mean average precision over the baseline model by incorporating contextual features,
    and a processing speed 14x faster than a text-based technique. Ongoing work on
    further improvements is also discussed.
  address: Hong Kong, China
  attachment:
  - filename: D19-1348.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1348.Attachment.pdf
  author:
  - first: Carlos
    full: Carlos Soto
    id: carlos-soto
    last: Soto
  - first: Shinjae
    full: Shinjae Yoo
    id: shinjae-yoo
    last: Yoo
  author_string: Carlos Soto, Shinjae Yoo
  bibkey: soto-yoo-2019-visual
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1348
  month: November
  page_first: '3464'
  page_last: '3470'
  pages: "3464\u20133470"
  paper_id: '348'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1348.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1348.jpg
  title: Visual Detection with Context for Document Layout Analysis
  title_html: Visual Detection with Context for Document Layout Analysis
  url: https://www.aclweb.org/anthology/D19-1348
  year: '2019'
D19-1349:
  abstract: Probabilistic topic models such as latent Dirichlet allocation (LDA) are
    popularly used with Bayesian inference methods such as Gibbs sampling to learn
    posterior distributions over topic model parameters. We derive a novel measure
    of LDA topic quality using the variability of the posterior distributions. Compared
    to several existing baselines for automatic topic evaluation, the proposed metric
    achieves state-of-the-art correlations with human judgments of topic quality in
    experiments on three corpora. We additionally demonstrate that topic quality estimation
    can be further improved using a supervised estimator that combines multiple metrics.
  address: Hong Kong, China
  attachment:
  - filename: D19-1349.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1349.Attachment.zip
  author:
  - first: Linzi
    full: Linzi Xing
    id: linzi-xing
    last: Xing
  - first: Michael J.
    full: Michael J. Paul
    id: michael-paul
    last: Paul
  - first: Giuseppe
    full: Giuseppe Carenini
    id: giuseppe-carenini
    last: Carenini
  author_string: Linzi Xing, Michael J. Paul, Giuseppe Carenini
  bibkey: xing-etal-2019-evaluating
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1349
  month: November
  page_first: '3471'
  page_last: '3477'
  pages: "3471\u20133477"
  paper_id: '349'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1349.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1349.jpg
  title: Evaluating Topic Quality with Posterior Variability
  title_html: Evaluating Topic Quality with Posterior Variability
  url: https://www.aclweb.org/anthology/D19-1349
  year: '2019'
D19-1350:
  abstract: In recent years, advances in neural variational inference have achieved
    many successes in text processing. Examples include neural topic models which
    are typically built upon variational autoencoder (VAE) with an objective of minimising
    the error of reconstructing original documents based on the learned latent topic
    vectors. However, minimising reconstruction errors does not necessarily lead to
    high quality topics. In this paper, we borrow the idea of reinforcement learning
    and incorporate topic coherence measures as reward signals to guide the learning
    of a VAE-based topic model. Furthermore, our proposed model is able to automatically
    separating background words dynamically from topic words, thus eliminating the
    pre-processing step of filtering infrequent and/or top frequent words, typically
    required for learning traditional topic models. Experimental results on the 20
    Newsgroups and the NIPS datasets show superior performance both on perplexity
    and topic coherence measure compared to state-of-the-art neural topic models.
  address: Hong Kong, China
  author:
  - first: Lin
    full: Lin Gui
    id: lin-gui
    last: Gui
  - first: Jia
    full: Jia Leng
    id: jia-leng
    last: Leng
  - first: Gabriele
    full: Gabriele Pergola
    id: gabriele-pergola
    last: Pergola
  - first: Yu
    full: Yu Zhou
    id: yu-zhou
    last: Zhou
  - first: Ruifeng
    full: Ruifeng Xu
    id: ruifeng-xu
    last: Xu
  - first: Yulan
    full: Yulan He
    id: yulan-he
    last: He
  author_string: Lin Gui, Jia Leng, Gabriele Pergola, Yu Zhou, Ruifeng Xu, Yulan He
  bibkey: gui-etal-2019-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1350
  month: November
  page_first: '3478'
  page_last: '3483'
  pages: "3478\u20133483"
  paper_id: '350'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1350.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1350.jpg
  title: Neural Topic Model with Reinforcement Learning
  title_html: Neural Topic Model with Reinforcement Learning
  url: https://www.aclweb.org/anthology/D19-1350
  year: '2019'
D19-1351:
  abstract: Text retrieval systems often return large sets of documents, particularly
    when applied to large collections. Stopping criteria can reduce the number of
    these documents that need to be manually evaluated for relevance by predicting
    when a suitable level of recall has been achieved. In this work, a novel method
    for determining a stopping criterion is proposed that models the rate at which
    relevant documents occur using a Poisson process. This method allows a user to
    specify both a minimum desired level of recall to achieve and a desired probability
    of having achieved it. We evaluate our method on a public dataset and compare
    it with previous techniques for determining stopping criteria.
  address: Hong Kong, China
  author:
  - first: Alison
    full: Alison Sneyd
    id: alison-sneyd
    last: Sneyd
  - first: Mark
    full: Mark Stevenson
    id: mark-stevenson
    last: Stevenson
  author_string: Alison Sneyd, Mark Stevenson
  bibkey: sneyd-stevenson-2019-modelling
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1351
  month: November
  page_first: '3484'
  page_last: '3489'
  pages: "3484\u20133489"
  paper_id: '351'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1351.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1351.jpg
  title: Modelling Stopping Criteria for Search Results using Poisson Processes
  title_html: Modelling Stopping Criteria for Search Results using <span class="acl-fixed-case">P</span>oisson
    Processes
  url: https://www.aclweb.org/anthology/D19-1351
  year: '2019'
D19-1352:
  abstract: 'This paper applies BERT to ad hoc document retrieval on news articles,
    which requires addressing two challenges: relevance judgments in existing test
    collections are typically provided only at the document level, and documents often
    exceed the length that BERT was designed to handle. Our solution is to aggregate
    sentence-level evidence to rank documents. Furthermore, we are able to leverage
    passage-level relevance judgments fortuitously available in other domains to fine-tune
    BERT models that are able to capture cross-domain notions of relevance, and can
    be directly used for ranking news articles. Our simple neural ranking models achieve
    state-of-the-art effectiveness on three standard test collections.'
  address: Hong Kong, China
  author:
  - first: Zeynep
    full: Zeynep Akkalyoncu Yilmaz
    id: zeynep-akkalyoncu-yilmaz
    last: Akkalyoncu Yilmaz
  - first: Wei
    full: Wei Yang
    id: wei-yang
    last: Yang
  - first: Haotian
    full: Haotian Zhang
    id: haotian-zhang
    last: Zhang
  - first: Jimmy
    full: Jimmy Lin
    id: jimmy-lin
    last: Lin
  author_string: Zeynep Akkalyoncu Yilmaz, Wei Yang, Haotian Zhang, Jimmy Lin
  bibkey: akkalyoncu-yilmaz-etal-2019-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1352
  month: November
  page_first: '3490'
  page_last: '3496'
  pages: "3490\u20133496"
  paper_id: '352'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1352.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1352.jpg
  title: Cross-Domain Modeling of Sentence-Level Evidence for Document Retrieval
  title_html: Cross-Domain Modeling of Sentence-Level Evidence for Document Retrieval
  url: https://www.aclweb.org/anthology/D19-1352
  year: '2019'
D19-1353:
  abstract: When performing cross-language information retrieval (CLIR) for lower-resourced
    languages, a common approach is to retrieve over the output of machine translation
    (MT). However, there is no established guidance on how to optimize the resulting
    MT-IR system. In this paper, we examine the relationship between the performance
    of MT systems and both neural and term frequency-based IR models to identify how
    CLIR performance can be best predicted from MT quality. We explore performance
    at varying amounts of MT training data, byte pair encoding (BPE) merge operations,
    and across two IR collections and retrieval models. We find that the choice of
    IR collection can substantially affect the predictive power of MT tuning decisions
    and evaluation, potentially introducing dissociations between MT-only and overall
    CLIR performance.
  address: Hong Kong, China
  attachment:
  - filename: D19-1353.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1353.Attachment.zip
  author:
  - first: Constantine
    full: Constantine Lignos
    id: constantine-lignos
    last: Lignos
  - first: Daniel
    full: Daniel Cohen
    id: daniel-cohen
    last: Cohen
  - first: Yen-Chieh
    full: Yen-Chieh Lien
    id: yen-chieh-lien
    last: Lien
  - first: Pratik
    full: Pratik Mehta
    id: pratik-mehta
    last: Mehta
  - first: W. Bruce
    full: W. Bruce Croft
    id: w-bruce-croft
    last: Croft
  - first: Scott
    full: Scott Miller
    id: scott-miller
    last: Miller
  author_string: Constantine Lignos, Daniel Cohen, Yen-Chieh Lien, Pratik Mehta, W.
    Bruce Croft, Scott Miller
  bibkey: lignos-etal-2019-challenges
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1353
  month: November
  page_first: '3497'
  page_last: '3502'
  pages: "3497\u20133502"
  paper_id: '353'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1353.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1353.jpg
  title: The Challenges of Optimizing Machine Translation for Low Resource Cross-Language
    Information Retrieval
  title_html: The Challenges of Optimizing Machine Translation for Low Resource Cross-Language
    Information Retrieval
  url: https://www.aclweb.org/anthology/D19-1353
  year: '2019'
D19-1354:
  abstract: "A notable property of word embeddings is that word relationships can\
    \ exist as linear substructures in the embedding space. For example, \u2018gender\u2019\
    \ corresponds to v_woman - v_man and v_queen - v_king. This, in turn, allows word\
    \ analogies to be solved arithmetically: v_king - v_man + v_woman = v_queen. This\
    \ property is notable because it suggests that models trained on word embeddings\
    \ can easily learn such relationships as geometric translations. However, there\
    \ is no evidence that models exclusively represent relationships in this manner.\
    \ We document an alternative way in which downstream models might learn these\
    \ relationships: orthogonal and linear transformations. For example, given a translation\
    \ vector for \u2018gender\u2019, we can find an orthogonal matrix R, representing\
    \ a rotation and reflection, such that R(v_king) = v_queen and R(v_man) = v_woman.\
    \ Analogical reasoning using orthogonal transformations is almost as accurate\
    \ as using vector arithmetic; using linear transformations is more accurate than\
    \ both. Our findings suggest that these transformations can be as good a representation\
    \ of word relationships as translation vectors."
  address: Hong Kong, China
  author:
  - first: Kawin
    full: Kawin Ethayarajh
    id: kawin-ethayarajh
    last: Ethayarajh
  author_string: Kawin Ethayarajh
  bibkey: ethayarajh-2019-rotate
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1354
  month: November
  page_first: '3503'
  page_last: '3508'
  pages: "3503\u20133508"
  paper_id: '354'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1354.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1354.jpg
  title: 'Rotate King to get Queen: Word Relationships as Orthogonal Transformations
    in Embedding Space'
  title_html: 'Rotate King to get Queen: Word Relationships as Orthogonal Transformations
    in Embedding Space'
  url: https://www.aclweb.org/anthology/D19-1354
  year: '2019'
D19-1355:
  abstract: Word Sense Disambiguation (WSD) aims to find the exact sense of an ambiguous
    word in a particular context. Traditional supervised methods rarely take into
    consideration the lexical resources like WordNet, which are widely utilized in
    knowledge-based methods. Recent studies have shown the effectiveness of incorporating
    gloss (sense definition) into neural networks for WSD. However, compared with
    traditional word expert supervised methods, they have not achieved much improvement.
    In this paper, we focus on how to better leverage gloss knowledge in a supervised
    neural WSD system. We construct context-gloss pairs and propose three BERT based
    models for WSD. We fine-tune the pre-trained BERT model and achieve new state-of-the-art
    results on WSD task.
  address: Hong Kong, China
  author:
  - first: Luyao
    full: Luyao Huang
    id: luyao-huang
    last: Huang
  - first: Chi
    full: Chi Sun
    id: chi-sun
    last: Sun
  - first: Xipeng
    full: Xipeng Qiu
    id: xipeng-qiu
    last: Qiu
  - first: Xuanjing
    full: Xuanjing Huang
    id: xuan-jing-huang
    last: Huang
  author_string: Luyao Huang, Chi Sun, Xipeng Qiu, Xuanjing Huang
  bibkey: huang-etal-2019-glossbert
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1355
  month: November
  page_first: '3509'
  page_last: '3514'
  pages: "3509\u20133514"
  paper_id: '355'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1355.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1355.jpg
  title: 'GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge'
  title_html: '<span class="acl-fixed-case">G</span>loss<span class="acl-fixed-case">BERT</span>:
    <span class="acl-fixed-case">BERT</span> for Word Sense Disambiguation with Gloss
    Knowledge'
  url: https://www.aclweb.org/anthology/D19-1355
  year: '2019'
D19-1356:
  abstract: One key component in text-to-SQL is to predict the comparison relations
    between columns and their values. To the best of our knowledge, no existing models
    explicitly introduce external common knowledge to address this problem, thus their
    capabilities of predicting comparison relations are limited beyond training data.
    In this paper, we propose to leverage adjective-noun phrasing knowledge mined
    from the web to predict the comparison relations in text-to-SQL. Experimental
    results on both the original and the re-split Spider dataset show that our approach
    achieves significant improvement over state-of-the-art methods on comparison relation
    prediction.
  address: Hong Kong, China
  author:
  - first: Haoyan
    full: Haoyan Liu
    id: haoyan-liu
    last: Liu
  - first: Lei
    full: Lei Fang
    id: lei-fang
    last: Fang
  - first: Qian
    full: Qian Liu
    id: qian-liu
    last: Liu
  - first: Bei
    full: Bei Chen
    id: bei-chen
    last: Chen
  - first: Jian-Guang
    full: Jian-Guang Lou
    id: jian-guang-lou
    last: Lou
  - first: Zhoujun
    full: Zhoujun Li
    id: zhoujun-li
    last: Li
  author_string: Haoyan Liu, Lei Fang, Qian Liu, Bei Chen, Jian-Guang Lou, Zhoujun
    Li
  bibkey: liu-etal-2019-leveraging
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1356
  month: November
  page_first: '3515'
  page_last: '3520'
  pages: "3515\u20133520"
  paper_id: '356'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1356.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1356.jpg
  title: Leveraging Adjective-Noun Phrasing Knowledge for Comparison Relation Prediction
    in Text-to-SQL
  title_html: Leveraging Adjective-Noun Phrasing Knowledge for Comparison Relation
    Prediction in Text-to-<span class="acl-fixed-case">SQL</span>
  url: https://www.aclweb.org/anthology/D19-1356
  year: '2019'
D19-1357:
  abstract: Definition modeling includes acquiring word embeddings from dictionary
    definitions and generating definitions of words. While the meanings of defining
    words are important in dictionary definitions, it is crucial to capture the lexical
    semantic relations between defined words and defining words. However, thus far,
    the utilization of such relations has not been explored for definition modeling.
    In this paper, we propose definition modeling methods that use lexical semantic
    relations. To utilize implicit semantic relations in definitions, we use unsupervisedly
    obtained pattern-based word-pair embeddings that represent semantic relations
    of word pairs. Experimental results indicate that our methods improve the performance
    in learning embeddings from definitions, as well as definition generation.
  address: Hong Kong, China
  attachment:
  - filename: D19-1357.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1357.Attachment.zip
  author:
  - first: Koki
    full: Koki Washio
    id: koki-washio
    last: Washio
  - first: Satoshi
    full: Satoshi Sekine
    id: satoshi-sekine
    last: Sekine
  - first: Tsuneaki
    full: Tsuneaki Kato
    id: tsuneaki-kato
    last: Kato
  author_string: Koki Washio, Satoshi Sekine, Tsuneaki Kato
  bibkey: washio-etal-2019-bridging
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1357
  month: November
  page_first: '3521'
  page_last: '3527'
  pages: "3521\u20133527"
  paper_id: '357'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1357.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1357.jpg
  title: 'Bridging the Defined and the Defining: Exploiting Implicit Lexical Semantic
    Relations in Definition Modeling'
  title_html: 'Bridging the Defined and the Defining: Exploiting Implicit Lexical
    Semantic Relations in Definition Modeling'
  url: https://www.aclweb.org/anthology/D19-1357
  year: '2019'
D19-1358:
  abstract: We propose a simple yet effective approach for improving Korean word representations
    using additional linguistic annotation (i.e. Hanja). We employ cross-lingual transfer
    learning in training word representations by leveraging the fact that Hanja is
    closely related to Chinese. We evaluate the intrinsic quality of representations
    learned through our approach using the word analogy and similarity tests. In addition,
    we demonstrate their effectiveness on several downstream tasks, including a novel
    Korean news headline generation task.
  address: Hong Kong, China
  attachment:
  - filename: D19-1358.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1358.Attachment.zip
  author:
  - first: Kang Min
    full: Kang Min Yoo
    id: kang-min-yoo
    last: Yoo
  - first: Taeuk
    full: Taeuk Kim
    id: taeuk-kim
    last: Kim
  - first: Sang-goo
    full: Sang-goo Lee
    id: sang-goo-lee
    last: Lee
  author_string: Kang Min Yoo, Taeuk Kim, Sang-goo Lee
  bibkey: yoo-etal-2019-dont
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1358
  month: November
  page_first: '3528'
  page_last: '3533'
  pages: "3528\u20133533"
  paper_id: '358'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1358.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1358.jpg
  title: "Don\u2019t Just Scratch the Surface: Enhancing Word Representations for\
    \ Korean with Hanja"
  title_html: "Don\u2019t Just Scratch the Surface: Enhancing Word Representations\
    \ for <span class=\"acl-fixed-case\">K</span>orean with Hanja"
  url: https://www.aclweb.org/anthology/D19-1358
  year: '2019'
D19-1359:
  abstract: Current research in knowledge-based Word Sense Disambiguation (WSD) indicates
    that performances depend heavily on the Lexical Knowledge Base (LKB) employed.
    This paper introduces SyntagNet, a novel resource consisting of manually disambiguated
    lexical-semantic combinations. By capturing sense distinctions evoked by syntagmatic
    relations, SyntagNet enables knowledge-based WSD systems to establish a new state
    of the art which challenges the hitherto unrivaled performances attained by supervised
    approaches. To the best of our knowledge, SyntagNet is the first large-scale manually-curated
    resource of this kind made available to the community (at http://syntagnet.org).
  address: Hong Kong, China
  author:
  - first: Marco
    full: Marco Maru
    id: marco-maru
    last: Maru
  - first: Federico
    full: Federico Scozzafava
    id: federico-scozzafava
    last: Scozzafava
  - first: Federico
    full: Federico Martelli
    id: federico-martelli
    last: Martelli
  - first: Roberto
    full: Roberto Navigli
    id: roberto-navigli
    last: Navigli
  author_string: Marco Maru, Federico Scozzafava, Federico Martelli, Roberto Navigli
  bibkey: maru-etal-2019-syntagnet
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1359
  month: November
  page_first: '3534'
  page_last: '3540'
  pages: "3534\u20133540"
  paper_id: '359'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1359.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1359.jpg
  title: 'SyntagNet: Challenging Supervised Word Sense Disambiguation with Lexical-Semantic
    Combinations'
  title_html: '<span class="acl-fixed-case">S</span>yntag<span class="acl-fixed-case">N</span>et:
    Challenging Supervised Word Sense Disambiguation with Lexical-Semantic Combinations'
  url: https://www.aclweb.org/anthology/D19-1359
  year: '2019'
D19-1360:
  abstract: In countries that speak multiple main languages, mixing up different languages
    within a conversation is commonly called code-switching. Previous works addressing
    this challenge mainly focused on word-level aspects such as word embeddings. However,
    in many cases, languages share common subwords, especially for closely related
    languages, but also for languages that are seemingly irrelevant. Therefore, we
    propose Hierarchical Meta-Embeddings (HME) that learn to combine multiple monolingual
    word-level and subword-level embeddings to create language-agnostic lexical representations.
    On the task of Named Entity Recognition for English-Spanish code-switching data,
    our model achieves the state-of-the-art performance in the multilingual settings.
    We also show that, in cross-lingual settings, our model not only leverages closely
    related languages, but also learns from languages with different roots. Finally,
    we show that combining different subunits are crucial for capturing code-switching
    entities.
  address: Hong Kong, China
  author:
  - first: Genta Indra
    full: Genta Indra Winata
    id: genta-indra-winata
    last: Winata
  - first: Zhaojiang
    full: Zhaojiang Lin
    id: zhaojiang-lin
    last: Lin
  - first: Jamin
    full: Jamin Shin
    id: jamin-shin
    last: Shin
  - first: Zihan
    full: Zihan Liu
    id: zihan-liu
    last: Liu
  - first: Pascale
    full: Pascale Fung
    id: pascale-fung
    last: Fung
  author_string: Genta Indra Winata, Zhaojiang Lin, Jamin Shin, Zihan Liu, Pascale
    Fung
  bibkey: winata-etal-2019-hierarchical
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1360
  month: November
  page_first: '3541'
  page_last: '3547'
  pages: "3541\u20133547"
  paper_id: '360'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1360.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1360.jpg
  title: Hierarchical Meta-Embeddings for Code-Switching Named Entity Recognition
  title_html: Hierarchical Meta-Embeddings for Code-Switching Named Entity Recognition
  url: https://www.aclweb.org/anthology/D19-1360
  year: '2019'
D19-1361:
  abstract: In this paper, we develop a novel Sparse Self-Attention Fine-tuning model
    (referred as SSAF) which integrates sparsity into self-attention mechanism to
    enhance the fine-tuning performance of BERT. In particular, sparsity is introduced
    into the self-attention by replacing softmax function with a controllable sparse
    transformation when fine-tuning with BERT. It enables us to learn a structurally
    sparse attention distribution, which leads to a more interpretable representation
    for the whole input. The proposed model is evaluated on sentiment analysis, question
    answering, and natural language inference tasks. The extensive experimental results
    across multiple datasets demonstrate its effectiveness and superiority to the
    baseline methods.
  address: Hong Kong, China
  author:
  - first: Baiyun
    full: Baiyun Cui
    id: baiyun-cui
    last: Cui
  - first: Yingming
    full: Yingming Li
    id: yingming-li
    last: Li
  - first: Ming
    full: Ming Chen
    id: ming-chen
    last: Chen
  - first: Zhongfei
    full: Zhongfei Zhang
    id: zhongfei-zhang
    last: Zhang
  author_string: Baiyun Cui, Yingming Li, Ming Chen, Zhongfei Zhang
  bibkey: cui-etal-2019-fine
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1361
  month: November
  page_first: '3548'
  page_last: '3553'
  pages: "3548\u20133553"
  paper_id: '361'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1361.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1361.jpg
  title: Fine-tune BERT with Sparse Self-Attention Mechanism
  title_html: Fine-tune <span class="acl-fixed-case">BERT</span> with Sparse Self-Attention
    Mechanism
  url: https://www.aclweb.org/anthology/D19-1361
  year: '2019'
D19-1362:
  abstract: In low-resource settings, the performance of supervised labeling models
    can be improved with automatically annotated or distantly supervised data, which
    is cheap to create but often noisy. Previous works have shown that significant
    improvements can be reached by injecting information about the confusion between
    clean and noisy labels in this additional training data into the classifier training.
    However, for noise estimation, these approaches either do not take the input features
    (in our case word embeddings) into account, or they need to learn the noise modeling
    from scratch which can be difficult in a low-resource setting. We propose to cluster
    the training data using the input features and then compute different confusion
    matrices for each cluster. To the best of our knowledge, our approach is the first
    to leverage feature-dependent noise modeling with pre-initialized confusion matrices.
    We evaluate on low-resource named entity recognition settings in several languages,
    showing that our methods improve upon other confusion-matrix based methods by
    up to 9%.
  address: Hong Kong, China
  author:
  - first: Lukas
    full: Lukas Lange
    id: lukas-lange
    last: Lange
  - first: Michael A.
    full: Michael A. Hedderich
    id: michael-a-hedderich
    last: Hedderich
  - first: Dietrich
    full: Dietrich Klakow
    id: dietrich-klakow
    last: Klakow
  author_string: Lukas Lange, Michael A. Hedderich, Dietrich Klakow
  bibkey: lange-etal-2019-feature
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1362
  month: November
  page_first: '3554'
  page_last: '3559'
  pages: "3554\u20133559"
  paper_id: '362'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1362.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1362.jpg
  title: Feature-Dependent Confusion Matrices for Low-Resource NER Labeling with Noisy
    Labels
  title_html: Feature-Dependent Confusion Matrices for Low-Resource <span class="acl-fixed-case">NER</span>
    Labeling with Noisy Labels
  url: https://www.aclweb.org/anthology/D19-1362
  year: '2019'
D19-1363:
  abstract: In this paper we present a novel approach to simultaneously representing
    multiple languages in a common space. Procrustes Analysis (PA) is commonly used
    to find the optimal orthogonal word mapping in the bilingual case. The proposed
    Multi Pairwise Procrustes Analysis (MPPA) is a natural extension of the PA algorithm
    to multilingual word mapping. Unlike previous PA extensions that require a k-way
    dictionary, this approach requires only pairwise bilingual dictionaries that are
    much easier to construct.
  address: Hong Kong, China
  author:
  - first: Hagai
    full: Hagai Taitelbaum
    id: hagai-taitelbaum
    last: Taitelbaum
  - first: Gal
    full: Gal Chechik
    id: gal-chechik
    last: Chechik
  - first: Jacob
    full: Jacob Goldberger
    id: jacob-goldberger
    last: Goldberger
  author_string: Hagai Taitelbaum, Gal Chechik, Jacob Goldberger
  bibkey: taitelbaum-etal-2019-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1363
  month: November
  page_first: '3560'
  page_last: '3565'
  pages: "3560\u20133565"
  paper_id: '363'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1363.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1363.jpg
  title: A Multi-Pairwise Extension of Procrustes Analysis for Multilingual Word Translation
  title_html: A Multi-Pairwise Extension of <span class="acl-fixed-case">P</span>rocrustes
    Analysis for Multilingual Word Translation
  url: https://www.aclweb.org/anthology/D19-1363
  year: '2019'
D19-1364:
  abstract: Out-of-domain (OOD) detection for low-resource text classification is
    a realistic but understudied task. The goal is to detect the OOD cases with limited
    in-domain (ID) training data, since in machine learning applications we observe
    that training data is often insufficient. In this work, we propose an OOD-resistant
    Prototypical Network to tackle this zero-shot OOD detection and few-shot ID classification
    task. Evaluations on real-world datasets show that the proposed solution outperforms
    state-of-the-art methods in zero-shot OOD detection task, while maintaining a
    competitive performance on ID classification task.
  address: Hong Kong, China
  author:
  - first: Ming
    full: Ming Tan
    id: ming-tan
    last: Tan
  - first: Yang
    full: Yang Yu
    id: yang-yu
    last: Yu
  - first: Haoyu
    full: Haoyu Wang
    id: haoyu-wang
    last: Wang
  - first: Dakuo
    full: Dakuo Wang
    id: dakuo-wang
    last: Wang
  - first: Saloni
    full: Saloni Potdar
    id: saloni-potdar
    last: Potdar
  - first: Shiyu
    full: Shiyu Chang
    id: shiyu-chang
    last: Chang
  - first: Mo
    full: Mo Yu
    id: mo-yu
    last: Yu
  author_string: Ming Tan, Yang Yu, Haoyu Wang, Dakuo Wang, Saloni Potdar, Shiyu Chang,
    Mo Yu
  bibkey: tan-etal-2019-domain
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1364
  month: November
  page_first: '3566'
  page_last: '3572'
  pages: "3566\u20133572"
  paper_id: '364'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1364.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1364.jpg
  title: Out-of-Domain Detection for Low-Resource Text Classification Tasks
  title_html: Out-of-Domain Detection for Low-Resource Text Classification Tasks
  url: https://www.aclweb.org/anthology/D19-1364
  year: '2019'
D19-1365:
  abstract: Formality text style transfer plays an important role in various NLP applications,
    such as non-native speaker assistants and child education. Early studies normalize
    informal sentences with rules, before statistical and neural models become a prevailing
    method in the field. While a rule-based system is still a common preprocessing
    step for formality style transfer in the neural era, it could introduce noise
    if we use the rules in a naive way such as data preprocessing. To mitigate this
    problem, we study how to harness rules into a state-of-the-art neural network
    that is typically pretrained on massive corpora. We propose three fine-tuning
    methods in this paper and achieve a new state-of-the-art on benchmark datasets
  address: Hong Kong, China
  author:
  - first: Yunli
    full: Yunli Wang
    id: yunli-wang
    last: Wang
  - first: Yu
    full: Yu Wu
    id: yu-wu
    last: Wu
  - first: Lili
    full: Lili Mou
    id: lili-mou
    last: Mou
  - first: Zhoujun
    full: Zhoujun Li
    id: zhoujun-li
    last: Li
  - first: Wenhan
    full: Wenhan Chao
    id: wenhan-chao
    last: Chao
  author_string: Yunli Wang, Yu Wu, Lili Mou, Zhoujun Li, Wenhan Chao
  bibkey: wang-etal-2019-harnessing
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1365
  month: November
  page_first: '3573'
  page_last: '3578'
  pages: "3573\u20133578"
  paper_id: '365'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1365.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1365.jpg
  title: Harnessing Pre-Trained Neural Networks with Rules for Formality Style Transfer
  title_html: Harnessing Pre-Trained Neural Networks with Rules for Formality Style
    Transfer
  url: https://www.aclweb.org/anthology/D19-1365
  year: '2019'
D19-1366:
  abstract: The objective of non-parallel text style transfer, or controllable text
    generation, is to alter specific attributes (e.g. sentiment, mood, tense, politeness,
    etc) of a given text while preserving its remaining attributes and content. Generative
    adversarial network (GAN) is a popular model to ensure the transferred sentences
    are realistic and have the desired target styles. However, training GAN often
    suffers from mode collapse problem, which causes that the transferred text is
    little related to the original text. In this paper, we propose a new GAN model
    with a word-level conditional architecture and a two-phase training procedure.
    By using a style-related condition architecture before generating a word, our
    model is able to maintain style-unrelated words while changing the others. By
    separating the training procedure into reconstruction and transfer phases, our
    model is able to learn a proper text generation process, which further improves
    the content preservation. We test our model on polarity sentiment transfer and
    multiple-attribute transfer tasks. The empirical results show that our model achieves
    comparable evaluation scores in both transfer accuracy and fluency but significantly
    outperforms other state-of-the-art models in content compatibility on three real-world
    datasets.
  address: Hong Kong, China
  attachment:
  - filename: D19-1366.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1366.Attachment.zip
  author:
  - first: Chih-Te
    full: Chih-Te Lai
    id: chih-te-lai
    last: Lai
  - first: Yi-Te
    full: Yi-Te Hong
    id: yi-te-hong
    last: Hong
  - first: Hong-You
    full: Hong-You Chen
    id: hong-you-chen
    last: Chen
  - first: Chi-Jen
    full: Chi-Jen Lu
    id: chi-jen-lu
    last: Lu
  - first: Shou-De
    full: Shou-De Lin
    id: shou-de-lin
    last: Lin
  author_string: Chih-Te Lai, Yi-Te Hong, Hong-You Chen, Chi-Jen Lu, Shou-De Lin
  bibkey: lai-etal-2019-multiple
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1366
  month: November
  page_first: '3579'
  page_last: '3584'
  pages: "3579\u20133584"
  paper_id: '366'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1366.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1366.jpg
  title: Multiple Text Style Transfer by using Word-level Conditional Generative Adversarial
    Network with Two-Phase Training
  title_html: Multiple Text Style Transfer by using Word-level Conditional Generative
    Adversarial Network with Two-Phase Training
  url: https://www.aclweb.org/anthology/D19-1366
  year: '2019'
D19-1367:
  abstract: In this paper, we study differentiable neural architecture search (NAS)
    methods for natural language processing. In particular, we improve differentiable
    architecture search by removing the softmax-local constraint. Also, we apply differentiable
    NAS to named entity recognition (NER). It is the first time that differentiable
    NAS methods are adopted in NLP tasks other than language modeling. On both the
    PTB language modeling and CoNLL-2003 English NER data, our method outperforms
    strong baselines. It achieves a new state-of-the-art on the NER task.
  address: Hong Kong, China
  author:
  - first: Yufan
    full: Yufan Jiang
    id: yufan-jiang
    last: Jiang
  - first: Chi
    full: Chi Hu
    id: chi-hu
    last: Hu
  - first: Tong
    full: Tong Xiao
    id: tong-xiao
    last: Xiao
  - first: Chunliang
    full: Chunliang Zhang
    id: chunliang-zhang
    last: Zhang
  - first: Jingbo
    full: Jingbo Zhu
    id: jingbo-zhu
    last: Zhu
  author_string: Yufan Jiang, Chi Hu, Tong Xiao, Chunliang Zhang, Jingbo Zhu
  bibkey: jiang-etal-2019-improved
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1367
  month: November
  page_first: '3585'
  page_last: '3590'
  pages: "3585\u20133590"
  paper_id: '367'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1367.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1367.jpg
  title: Improved Differentiable Architecture Search for Language Modeling and Named
    Entity Recognition
  title_html: Improved Differentiable Architecture Search for Language Modeling and
    Named Entity Recognition
  url: https://www.aclweb.org/anthology/D19-1367
  year: '2019'
D19-1368:
  abstract: Bilinear models such as DistMult and ComplEx are effective methods for
    knowledge graph (KG) completion. However, they require large batch sizes, which
    becomes a performance bottleneck when training on large scale datasets due to
    memory constraints. In this paper we use occurrences of entity-relation pairs
    in the dataset to construct a joint learning model and to increase the quality
    of sampled negatives during training. We show on three standard datasets that
    when these two techniques are combined, they give a significant improvement in
    performance, especially when the batch size and the number of generated negative
    examples are low relative to the size of the dataset. We then apply our techniques
    to a dataset containing 2 million entities and demonstrate that our model outperforms
    the baseline by 2.8% absolute on hits@1.
  address: Hong Kong, China
  attachment:
  - filename: D19-1368.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1368.Attachment.zip
  author:
  - first: Esma
    full: Esma Balkir
    id: esma-balkir
    last: Balkir
  - first: Masha
    full: Masha Naslidnyk
    id: masha-naslidnyk
    last: Naslidnyk
  - first: Dave
    full: Dave Palfrey
    id: dave-palfrey
    last: Palfrey
  - first: Arpit
    full: Arpit Mittal
    id: arpit-mittal
    last: Mittal
  author_string: Esma Balkir, Masha Naslidnyk, Dave Palfrey, Arpit Mittal
  bibkey: balkir-etal-2019-using
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1368
  month: November
  page_first: '3591'
  page_last: '3596'
  pages: "3591\u20133596"
  paper_id: '368'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1368.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1368.jpg
  title: Using Pairwise Occurrence Information to Improve Knowledge Graph Completion
    on Large-Scale Datasets
  title_html: Using Pairwise Occurrence Information to Improve Knowledge Graph Completion
    on Large-Scale Datasets
  url: https://www.aclweb.org/anthology/D19-1368
  year: '2019'
D19-1369:
  abstract: "In this paper, we present a fast and reliable method based on PCA to\
    \ select the number of dimensions for word embeddings. First, we train one embedding\
    \ with a generous upper bound (e.g. 1,000) of dimensions. Then we transform the\
    \ embeddings using PCA and incrementally remove the lesser dimensions one at a\
    \ time while recording the embeddings\u2019 performance on language tasks. Lastly,\
    \ we select the number of dimensions, balancing model size and accuracy. Experiments\
    \ using various datasets and language tasks demonstrate that we are able to train\
    \ about 10 times fewer sets of embeddings while retaining optimal performance.\
    \ Researchers interested in training the best-performing embeddings for downstream\
    \ tasks, such as sentiment analysis, question answering and hypernym extraction,\
    \ as well as those interested in embedding compression should find the method\
    \ helpful."
  address: Hong Kong, China
  author:
  - first: Yu
    full: Yu Wang
    id: yu-wang
    last: Wang
  author_string: Yu Wang
  bibkey: wang-2019-single
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1369
  month: November
  page_first: '3597'
  page_last: '3602'
  pages: "3597\u20133602"
  paper_id: '369'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1369.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1369.jpg
  title: Single Training Dimension Selection for Word Embedding with PCA
  title_html: Single Training Dimension Selection for Word Embedding with <span class="acl-fixed-case">PCA</span>
  url: https://www.aclweb.org/anthology/D19-1369
  year: '2019'
D19-1370:
  abstract: When trained effectively, the Variational Autoencoder (VAE) is both a
    powerful language model and an effective representation learning framework. In
    practice, however, VAEs are trained with the evidence lower bound (ELBO) as a
    surrogate objective to the intractable marginal data likelihood. This approach
    to training yields unstable results, frequently leading to a disastrous local
    optimum known as posterior collapse. In this paper, we investigate a simple fix
    for posterior collapse which yields surprisingly effective results. The combination
    of two known heuristics, previously considered only in isolation, substantially
    improves held-out likelihood, reconstruction, and latent representation learning
    when compared with previous state-of-the-art methods. More interestingly, while
    our experiments demonstrate superiority on these principle evaluations, our method
    obtains a worse ELBO. We use these results to argue that the typical surrogate
    objective for VAEs may not be sufficient or necessarily appropriate for balancing
    the goals of representation learning and data distribution modeling.
  address: Hong Kong, China
  author:
  - first: Bohan
    full: Bohan Li
    id: bohan-li
    last: Li
  - first: Junxian
    full: Junxian He
    id: junxian-he
    last: He
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  - first: Taylor
    full: Taylor Berg-Kirkpatrick
    id: taylor-berg-kirkpatrick
    last: Berg-Kirkpatrick
  - first: Yiming
    full: Yiming Yang
    id: yiming-yang
    last: Yang
  author_string: Bohan Li, Junxian He, Graham Neubig, Taylor Berg-Kirkpatrick, Yiming
    Yang
  bibkey: li-etal-2019-surprisingly
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1370
  month: November
  page_first: '3603'
  page_last: '3614'
  pages: "3603\u20133614"
  paper_id: '370'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1370.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1370.jpg
  title: A Surprisingly Effective Fix for Deep Latent Variable Modeling of Text
  title_html: A Surprisingly Effective Fix for Deep Latent Variable Modeling of Text
  url: https://www.aclweb.org/anthology/D19-1370
  year: '2019'
D19-1371:
  abstract: Obtaining large-scale annotated data for NLP tasks in the scientific domain
    is challenging and expensive. We release SciBERT, a pretrained language model
    based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale
    labeled scientific data. SciBERT leverages unsupervised pretraining on a large
    multi-domain corpus of scientific publications to improve performance on downstream
    scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging,
    sentence classification and dependency parsing, with datasets from a variety of
    scientific domains. We demonstrate statistically significant improvements over
    BERT and achieve new state-of-the-art results on several of these tasks. The code
    and pretrained models are available at https://github.com/allenai/scibert/.
  address: Hong Kong, China
  author:
  - first: Iz
    full: Iz Beltagy
    id: iz-beltagy
    last: Beltagy
  - first: Kyle
    full: Kyle Lo
    id: kyle-lo
    last: Lo
  - first: Arman
    full: Arman Cohan
    id: arman-cohan
    last: Cohan
  author_string: Iz Beltagy, Kyle Lo, Arman Cohan
  bibkey: beltagy-etal-2019-scibert
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1371
  month: November
  page_first: '3615'
  page_last: '3620'
  pages: "3615\u20133620"
  paper_id: '371'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1371.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1371.jpg
  title: 'SciBERT: A Pretrained Language Model for Scientific Text'
  title_html: '<span class="acl-fixed-case">S</span>ci<span class="acl-fixed-case">BERT</span>:
    A Pretrained Language Model for Scientific Text'
  url: https://www.aclweb.org/anthology/D19-1371
  year: '2019'
D19-1372:
  abstract: "Much previous work has been done in attempting to identify humor in text.\
    \ In this paper we extend that capability by proposing a new task: assessing whether\
    \ or not a joke is humorous. We present a novel way of approaching this problem\
    \ by building a model that learns to identify humorous jokes based on ratings\
    \ gleaned from Reddit pages, consisting of almost 16,000 labeled instances. Using\
    \ these ratings to determine the level of humor, we then employ a Transformer\
    \ architecture for its advantages in learning from sentence context. We demonstrate\
    \ the effectiveness of this approach and show results that are comparable to human\
    \ performance. We further demonstrate our model\u2019s increased capabilities\
    \ on humor identification problems, such as the previously created datasets for\
    \ short jokes and puns. These experiments show that this method outperforms all\
    \ previous work done on these tasks, with an F-measure of 93.1% for the Puns dataset\
    \ and 98.6% on the Short Jokes dataset."
  address: Hong Kong, China
  author:
  - first: Orion
    full: Orion Weller
    id: orion-weller
    last: Weller
  - first: Kevin
    full: Kevin Seppi
    id: kevin-seppi
    last: Seppi
  author_string: Orion Weller, Kevin Seppi
  bibkey: weller-seppi-2019-humor
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1372
  month: November
  page_first: '3621'
  page_last: '3625'
  pages: "3621\u20133625"
  paper_id: '372'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1372.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1372.jpg
  title: 'Humor Detection: A Transformer Gets the Last Laugh'
  title_html: 'Humor Detection: A Transformer Gets the Last Laugh'
  url: https://www.aclweb.org/anthology/D19-1372
  year: '2019'
D19-1373:
  abstract: "One way to reduce network traffic in multi-node data-parallel stochastic\
    \ gradient descent is to only exchange the largest gradients. However, doing so\
    \ damages the gradient and degrades the model\u2019s performance. Transformer\
    \ models degrade dramatically while the impact on RNNs is smaller. We restore\
    \ gradient quality by combining the compressed global gradient with the node\u2019\
    s locally computed uncompressed gradient. Neural machine translation experiments\
    \ show that Transformer convergence is restored while RNNs converge faster. With\
    \ our method, training on 4 nodes converges up to 1.5x as fast as with uncompressed\
    \ gradients and scales 3.5x relative to single-node training."
  address: Hong Kong, China
  author:
  - first: Alham Fikri
    full: Alham Fikri Aji
    id: alham-fikri-aji
    last: Aji
  - first: Kenneth
    full: Kenneth Heafield
    id: kenneth-heafield
    last: Heafield
  - first: Nikolay
    full: Nikolay Bogoychev
    id: nikolay-bogoychev
    last: Bogoychev
  author_string: Alham Fikri Aji, Kenneth Heafield, Nikolay Bogoychev
  bibkey: aji-etal-2019-combining
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1373
  month: November
  page_first: '3626'
  page_last: '3631'
  pages: "3626\u20133631"
  paper_id: '373'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1373.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1373.jpg
  title: Combining Global Sparse Gradients with Local Gradients in Distributed Neural
    Network Training
  title_html: Combining Global Sparse Gradients with Local Gradients in Distributed
    Neural Network Training
  url: https://www.aclweb.org/anthology/D19-1373
  year: '2019'
D19-1374:
  abstract: We propose a practical scheme to train a single multilingual sequence
    labeling model that yields state of the art results and is small and fast enough
    to run on a single CPU. Starting from a public multilingual BERT checkpoint, our
    final model is 6x smaller and 27x faster, and has higher accuracy than a state-of-the-art
    multilingual baseline. We show that our model especially outperforms on low-resource
    languages, and works on codemixed input text without being explicitly trained
    on codemixed examples. We showcase the effectiveness of our method by reporting
    on part-of-speech tagging and morphological prediction on 70 treebanks and 48
    languages.
  address: Hong Kong, China
  attachment:
  - filename: D19-1374.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1374.Attachment.zip
  author:
  - first: Henry
    full: Henry Tsai
    id: henry-tsai
    last: Tsai
  - first: Jason
    full: Jason Riesa
    id: jason-riesa
    last: Riesa
  - first: Melvin
    full: Melvin Johnson
    id: melvin-johnson
    last: Johnson
  - first: Naveen
    full: Naveen Arivazhagan
    id: naveen-arivazhagan
    last: Arivazhagan
  - first: Xin
    full: Xin Li
    id: xin-li
    last: Li
  - first: Amelia
    full: Amelia Archer
    id: amelia-archer
    last: Archer
  author_string: Henry Tsai, Jason Riesa, Melvin Johnson, Naveen Arivazhagan, Xin
    Li, Amelia Archer
  bibkey: tsai-etal-2019-small
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1374
  month: November
  page_first: '3632'
  page_last: '3636'
  pages: "3632\u20133636"
  paper_id: '374'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1374.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1374.jpg
  title: Small and Practical BERT Models for Sequence Labeling
  title_html: Small and Practical <span class="acl-fixed-case">BERT</span> Models
    for Sequence Labeling
  url: https://www.aclweb.org/anthology/D19-1374
  year: '2019'
D19-1375:
  abstract: Spoken Language Understanding (SLU) converts user utterances into structured
    semantic representations. Data sparsity is one of the main obstacles of SLU due
    to the high cost of human annotation, especially when domain changes or a new
    domain comes. In this work, we propose a data augmentation method with atomic
    templates for SLU, which involves minimum human efforts. The atomic templates
    produce exemplars for fine-grained constituents of semantic representations. We
    propose an encoder-decoder model to generate the whole utterance from atomic exemplars.
    Moreover, the generator could be transferred from source domains to help a new
    domain which has little data. Experimental results show that our method achieves
    significant improvements on DSTC 2&3 dataset which is a domain adaptation setting
    of SLU.
  address: Hong Kong, China
  author:
  - first: Zijian
    full: Zijian Zhao
    id: zijian-zhao
    last: Zhao
  - first: Su
    full: Su Zhu
    id: su-zhu
    last: Zhu
  - first: Kai
    full: Kai Yu
    id: kai-yu
    last: Yu
  author_string: Zijian Zhao, Su Zhu, Kai Yu
  bibkey: zhao-etal-2019-data
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1375
  month: November
  page_first: '3637'
  page_last: '3643'
  pages: "3637\u20133643"
  paper_id: '375'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1375.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1375.jpg
  title: Data Augmentation with Atomic Templates for Spoken Language Understanding
  title_html: Data Augmentation with Atomic Templates for Spoken Language Understanding
  url: https://www.aclweb.org/anthology/D19-1375
  year: '2019'
D19-1376:
  abstract: We present PaLM, a hybrid parser and neural language model. Building on
    an RNN language model, PaLM adds an attention layer over text spans in the left
    context. An unsupervised constituency parser can be derived from its attention
    weights, using a greedy decoding algorithm. We evaluate PaLM on language modeling,
    and empirically show that it outperforms strong baselines. If syntactic annotations
    are available, the attention component can be trained in a supervised manner,
    providing syntactically-informed representations of the context, and further improving
    language modeling performance.
  address: Hong Kong, China
  attachment:
  - filename: D19-1376.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1376.Attachment.pdf
  author:
  - first: Hao
    full: Hao Peng
    id: hao-peng
    last: Peng
  - first: Roy
    full: Roy Schwartz
    id: roy-schwartz
    last: Schwartz
  - first: Noah A.
    full: Noah A. Smith
    id: noah-a-smith
    last: Smith
  author_string: Hao Peng, Roy Schwartz, Noah A. Smith
  bibkey: peng-etal-2019-palm
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1376
  month: November
  page_first: '3644'
  page_last: '3651'
  pages: "3644\u20133651"
  paper_id: '376'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1376.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1376.jpg
  title: 'PaLM: A Hybrid Parser and Language Model'
  title_html: '<span class="acl-fixed-case">P</span>a<span class="acl-fixed-case">LM</span>:
    A Hybrid Parser and Language Model'
  url: https://www.aclweb.org/anthology/D19-1376
  year: '2019'
D19-1377:
  abstract: The task of semantic parsing is highly useful for dialogue and question
    answering systems. Many datasets have been proposed to map natural language text
    into SQL, among which the recent Spider dataset provides cross-domain samples
    with multiple tables and complex queries. We build a Spider dataset for Chinese,
    which is currently a low-resource language in this task area. Interesting research
    questions arise from the uniqueness of the language, which requires word segmentation,
    and also from the fact that SQL keywords and columns of DB tables are typically
    written in English. We compare character- and word-based encoders for a semantic
    parser, and different embedding schemes. Results show that word-based semantic
    parser is subject to segmentation errors and cross-lingual word embeddings are
    useful for text-to-SQL.
  address: Hong Kong, China
  author:
  - first: Qingkai
    full: Qingkai Min
    id: qingkai-min
    last: Min
  - first: Yuefeng
    full: Yuefeng Shi
    id: yuefeng-shi
    last: Shi
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  author_string: Qingkai Min, Yuefeng Shi, Yue Zhang
  bibkey: min-etal-2019-pilot
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1377
  month: November
  page_first: '3652'
  page_last: '3658'
  pages: "3652\u20133658"
  paper_id: '377'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1377.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1377.jpg
  title: A Pilot Study for Chinese SQL Semantic Parsing
  title_html: A Pilot Study for <span class="acl-fixed-case">C</span>hinese <span
    class="acl-fixed-case">SQL</span> Semantic Parsing
  url: https://www.aclweb.org/anthology/D19-1377
  year: '2019'
D19-1378:
  abstract: State-of-the-art semantic parsers rely on auto-regressive decoding, emitting
    one symbol at a time. When tested against complex databases that are unobserved
    at training time (zero-shot), the parser often struggles to select the correct
    set of database constants in the new database, due to the local nature of decoding.
    %since their decisions are based on weak, local information only. In this work,
    we propose a semantic parser that globally reasons about the structure of the
    output query to make a more contextually-informed selection of database constants.
    We use message-passing through a graph neural network to softly select a subset
    of database constants for the output query, conditioned on the question. Moreover,
    we train a model to rank queries based on the global alignment of database constants
    to question words. We apply our techniques to the current state-of-the-art model
    for Spider, a zero-shot semantic parsing dataset with complex databases, increasing
    accuracy from 39.4% to 47.4%.
  address: Hong Kong, China
  attachment:
  - filename: D19-1378.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1378.Attachment.pdf
  author:
  - first: Ben
    full: Ben Bogin
    id: ben-bogin
    last: Bogin
  - first: Matt
    full: Matt Gardner
    id: matt-gardner
    last: Gardner
  - first: Jonathan
    full: Jonathan Berant
    id: jonathan-berant
    last: Berant
  author_string: Ben Bogin, Matt Gardner, Jonathan Berant
  bibkey: bogin-etal-2019-global
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1378
  month: November
  page_first: '3659'
  page_last: '3664'
  pages: "3659\u20133664"
  paper_id: '378'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1378.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1378.jpg
  title: Global Reasoning over Database Structures for Text-to-SQL Parsing
  title_html: Global Reasoning over Database Structures for Text-to-<span class="acl-fixed-case">SQL</span>
    Parsing
  url: https://www.aclweb.org/anthology/D19-1378
  year: '2019'
D19-1379:
  abstract: In transductive learning, an unlabeled test set is used for model training.
    Although this setting deviates from the common assumption of a completely unseen
    test set, it is applicable in many real-world scenarios, wherein the texts to
    be processed are known in advance. However, despite its practical advantages,
    transductive learning is underexplored in natural language processing. Here we
    conduct an empirical study of transductive learning for neural models and demonstrate
    its utility in syntactic and semantic tasks. Specifically, we fine-tune language
    models (LMs) on an unlabeled test set to obtain test-set-specific word representations.
    Through extensive experiments, we demonstrate that despite its simplicity, transductive
    LM fine-tuning consistently improves state-of-the-art neural models in in-domain
    and out-of-domain settings.
  address: Hong Kong, China
  attachment:
  - filename: D19-1379.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1379.Attachment.pdf
  author:
  - first: Hiroki
    full: Hiroki Ouchi
    id: hiroki-ouchi
    last: Ouchi
  - first: Jun
    full: Jun Suzuki
    id: jun-suzuki
    last: Suzuki
  - first: Kentaro
    full: Kentaro Inui
    id: kentaro-inui
    last: Inui
  author_string: Hiroki Ouchi, Jun Suzuki, Kentaro Inui
  bibkey: ouchi-etal-2019-transductive
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1379
  month: November
  page_first: '3665'
  page_last: '3671'
  pages: "3665\u20133671"
  paper_id: '379'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1379.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1379.jpg
  title: Transductive Learning of Neural Language Models for Syntactic and Semantic
    Analysis
  title_html: Transductive Learning of Neural Language Models for Syntactic and Semantic
    Analysis
  url: https://www.aclweb.org/anthology/D19-1379
  year: '2019'
D19-1380:
  abstract: Vector averaging remains one of the most popular sentence embedding methods
    in spite of its obvious disregard for syntactic structure. While more complex
    sequential or convolutional networks potentially yield superior classification
    performance, the improvements in classification accuracy are typically mediocre
    compared to the simple vector averaging. As an efficient alternative, we propose
    the use of discrete cosine transform (DCT) to compress word sequences in an order-preserving
    manner. The lower order DCT coefficients represent the overall feature patterns
    in sentences, which results in suitable embeddings for tasks that could benefit
    from syntactic features. Our results in semantic probing tasks demonstrate that
    DCT embeddings indeed preserve more syntactic information compared with vector
    averaging. With practically equivalent complexity, the model yields better overall
    performance in downstream classification tasks that correlate with syntactic features,
    which illustrates the capacity of DCT to preserve word order information.
  address: Hong Kong, China
  author:
  - first: Nada
    full: Nada Almarwani
    id: nada-almarwani
    last: Almarwani
  - first: Hanan
    full: Hanan Aldarmaki
    id: hanan-aldarmaki
    last: Aldarmaki
  - first: Mona
    full: Mona Diab
    id: mona-diab
    last: Diab
  author_string: Nada Almarwani, Hanan Aldarmaki, Mona Diab
  bibkey: almarwani-etal-2019-efficient
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1380
  month: November
  page_first: '3672'
  page_last: '3678'
  pages: "3672\u20133678"
  paper_id: '380'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1380.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1380.jpg
  title: Efficient Sentence Embedding using Discrete Cosine Transform
  title_html: Efficient Sentence Embedding using Discrete Cosine Transform
  url: https://www.aclweb.org/anthology/D19-1380
  year: '2019'
D19-1381:
  abstract: We tackle the nested and overlapping event detection task and propose
    a novel search-based neural network (SBNN) structured prediction model that treats
    the task as a search problem on a relation graph of trigger-argument structures.
    Unlike existing structured prediction tasks such as dependency parsing, the task
    targets to detect DAG structures, which constitute events, from the relation graph.
    We define actions to construct events and use all the beams in a beam search to
    detect all event structures that may be overlapping and nested. The search process
    constructs events in a bottom-up manner while modelling the global properties
    for nested and overlapping structures simultaneously using neural networks. We
    show that the model achieves performance comparable to the state-of-the-art model
    Turku Event Extraction System (TEES) on the BioNLP Cancer Genetics (CG) Shared
    Task 2013 without the use of any syntactic and hand-engineered features. Further
    analyses on the development set show that our model is more computationally efficient
    while yielding higher F1-score performance.
  address: Hong Kong, China
  author:
  - first: Kurt Junshean
    full: Kurt Junshean Espinosa
    id: kurt-junshean-espinosa
    last: Espinosa
  - first: Makoto
    full: Makoto Miwa
    id: makoto-miwa
    last: Miwa
  - first: Sophia
    full: Sophia Ananiadou
    id: sophia-ananiadou
    last: Ananiadou
  author_string: Kurt Junshean Espinosa, Makoto Miwa, Sophia Ananiadou
  bibkey: espinosa-etal-2019-search
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1381
  month: November
  page_first: '3679'
  page_last: '3686'
  pages: "3679\u20133686"
  paper_id: '381'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1381.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1381.jpg
  title: A Search-based Neural Model for Biomedical Nested and Overlapping Event Detection
  title_html: A Search-based Neural Model for Biomedical Nested and Overlapping Event
    Detection
  url: https://www.aclweb.org/anthology/D19-1381
  year: '2019'
D19-1382:
  abstract: 'Most existing work on adversarial data generation focuses on English.
    For example, PAWS (Paraphrase Adversaries from Word Scrambling) consists of challenging
    English paraphrase identification pairs from Wikipedia and Quora. We remedy this
    gap with PAWS-X, a new dataset of 23,659 human translated PAWS evaluation pairs
    in six typologically distinct languages: French, Spanish, German, Chinese, Japanese,
    and Korean. We provide baseline numbers for three models with different capacity
    to capture non-local context and sentence structure, and using different multilingual
    training and evaluation regimes. Multilingual BERT fine-tuned on PAWS English
    plus machine-translated data performs the best, with a range of 83.1-90.8 accuracy
    across the non-English languages and an average accuracy gain of 23% over the
    next best model. PAWS-X shows the effectiveness of deep, multilingual pre-training
    while also leaving considerable headroom as a new challenge to drive multilingual
    research that better captures structure and contextual information.'
  address: Hong Kong, China
  author:
  - first: Yinfei
    full: Yinfei Yang
    id: yinfei-yang
    last: Yang
  - first: Yuan
    full: Yuan Zhang
    id: yuan-zhang
    last: Zhang
  - first: Chris
    full: Chris Tar
    id: chris-tar
    last: Tar
  - first: Jason
    full: Jason Baldridge
    id: jason-baldridge
    last: Baldridge
  author_string: Yinfei Yang, Yuan Zhang, Chris Tar, Jason Baldridge
  bibkey: yang-etal-2019-paws
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1382
  month: November
  page_first: '3687'
  page_last: '3692'
  pages: "3687\u20133692"
  paper_id: '382'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1382.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1382.jpg
  title: 'PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification'
  title_html: '<span class="acl-fixed-case">PAWS</span>-X: A Cross-lingual Adversarial
    Dataset for Paraphrase Identification'
  url: https://www.aclweb.org/anthology/D19-1382
  year: '2019'
D19-1383:
  abstract: As a step toward better document-level understanding, we explore classification
    of a sequence of sentences into their corresponding categories, a task that requires
    understanding sentences in context of the document. Recent successful models for
    this task have used hierarchical models to contextualize sentence representations,
    and Conditional Random Fields (CRFs) to incorporate dependencies between subsequent
    labels. In this work, we show that pretrained language models, BERT (Devlin et
    al., 2018) in particular, can be used for this task to capture contextual dependencies
    without the need for hierarchical encoding nor a CRF. Specifically, we construct
    a joint sentence representation that allows BERT Transformer layers to directly
    utilize contextual information from all words in all sentences. Our approach achieves
    state-of-the-art results on four datasets, including a new dataset of structured
    scientific abstracts.
  address: Hong Kong, China
  author:
  - first: Arman
    full: Arman Cohan
    id: arman-cohan
    last: Cohan
  - first: Iz
    full: Iz Beltagy
    id: iz-beltagy
    last: Beltagy
  - first: Daniel
    full: Daniel King
    id: daniel-king
    last: King
  - first: Bhavana
    full: Bhavana Dalvi
    id: bhavana-dalvi
    last: Dalvi
  - first: Dan
    full: Dan Weld
    id: daniel-s-weld
    last: Weld
  author_string: Arman Cohan, Iz Beltagy, Daniel King, Bhavana Dalvi, Dan Weld
  bibkey: cohan-etal-2019-pretrained
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1383
  month: November
  page_first: '3693'
  page_last: '3699'
  pages: "3693\u20133699"
  paper_id: '383'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1383.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1383.jpg
  title: Pretrained Language Models for Sequential Sentence Classification
  title_html: Pretrained Language Models for Sequential Sentence Classification
  url: https://www.aclweb.org/anthology/D19-1383
  year: '2019'
D19-1384:
  abstract: 'We describe a multi-agent communication framework for examining high-level
    linguistic phenomena at the community-level. We demonstrate that complex linguistic
    behavior observed in natural language can be reproduced in this simple setting:
    i) the outcome of contact between communities is a function of inter- and intra-group
    connectivity; ii) linguistic contact either converges to the majority protocol,
    or in balanced cases leads to novel creole languages of lower complexity; and
    iii) a linguistic continuum emerges where neighboring languages are more mutually
    intelligible than farther removed languages. We conclude that at least some of
    the intricate properties of language evolution need not depend on complex evolved
    linguistic capabilities, but can emerge from simple social exchanges between perceptually-enabled
    agents playing communication games.'
  address: Hong Kong, China
  author:
  - first: Laura
    full: Laura Harding Graesser
    id: laura-harding-graesser
    last: Harding Graesser
  - first: Kyunghyun
    full: Kyunghyun Cho
    id: kyunghyun-cho
    last: Cho
  - first: Douwe
    full: Douwe Kiela
    id: douwe-kiela
    last: Kiela
  author_string: Laura Harding Graesser, Kyunghyun Cho, Douwe Kiela
  bibkey: harding-graesser-etal-2019-emergent
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1384
  month: November
  page_first: '3700'
  page_last: '3710'
  pages: "3700\u20133710"
  paper_id: '384'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1384.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1384.jpg
  title: Emergent Linguistic Phenomena in Multi-Agent Communication Games
  title_html: Emergent Linguistic Phenomena in Multi-Agent Communication Games
  url: https://www.aclweb.org/anthology/D19-1384
  year: '2019'
D19-1385:
  abstract: Condescending language use is caustic; it can bring dialogues to an end
    and bifurcate communities. Thus, systems for condescension detection could have
    a large positive impact. A challenge here is that condescension is often impossible
    to detect from isolated utterances, as it depends on the discourse and social
    context. To address this, we present TalkDown, a new labeled dataset of condescending
    linguistic acts in context. We show that extending a language-only model with
    representations of the discourse improves performance, and we motivate techniques
    for dealing with the low rates of condescension overall. We also use our model
    to estimate condescension rates in various online communities and relate these
    differences to differing community norms.
  address: Hong Kong, China
  author:
  - first: Zijian
    full: Zijian Wang
    id: zijian-wang
    last: Wang
  - first: Christopher
    full: Christopher Potts
    id: christopher-potts
    last: Potts
  author_string: Zijian Wang, Christopher Potts
  bibkey: wang-potts-2019-talkdown
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1385
  month: November
  page_first: '3711'
  page_last: '3719'
  pages: "3711\u20133719"
  paper_id: '385'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1385.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1385.jpg
  title: 'TalkDown: A Corpus for Condescension Detection in Context'
  title_html: '<span class="acl-fixed-case">T</span>alk<span class="acl-fixed-case">D</span>own:
    A Corpus for Condescension Detection in Context'
  url: https://www.aclweb.org/anthology/D19-1385
  year: '2019'
D19-1386:
  abstract: A key challenge in topic-focused summarization is determining what information
    should be included in the summary, a problem known as content selection. In this
    work, we propose a new method for studying content selection in topic-focused
    summarization called the summary cloze task. The goal of the summary cloze task
    is to generate the next sentence of a summary conditioned on the beginning of
    the summary, a topic, and a reference document(s). The main challenge is deciding
    what information in the references is relevant to the topic and partial summary
    and should be included in the summary. Although the cloze task does not address
    all aspects of the traditional summarization problem, the more narrow scope of
    the task allows us to collect a large-scale datset of nearly 500k summary cloze
    instances from Wikipedia. We report experimental results on this new dataset using
    various extractive models and a two-step abstractive model that first extractively
    selects a small number of sentences and then abstractively summarizes them. Our
    results show that the topic and partial summary help the models identify relevant
    content, but the task remains a significant challenge.
  address: Hong Kong, China
  attachment:
  - filename: D19-1386.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1386.Attachment.zip
  author:
  - first: Daniel
    full: Daniel Deutsch
    id: daniel-deutsch
    last: Deutsch
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Daniel Deutsch, Dan Roth
  bibkey: deutsch-roth-2019-summary
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1386
  month: November
  page_first: '3720'
  page_last: '3729'
  pages: "3720\u20133729"
  paper_id: '386'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1386.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1386.jpg
  title: 'Summary Cloze: A New Task for Content Selection in Topic-Focused Summarization'
  title_html: 'Summary Cloze: A New Task for Content Selection in Topic-Focused Summarization'
  url: https://www.aclweb.org/anthology/D19-1386
  year: '2019'
D19-1387:
  abstract: Bidirectional Encoder Representations from Transformers (BERT) represents
    the latest incarnation of pretrained language models which have recently advanced
    a wide range of natural language processing tasks. In this paper, we showcase
    how BERT can be usefully applied in text summarization and propose a general framework
    for both extractive and abstractive models. We introduce a novel document-level
    encoder based on BERT which is able to express the semantics of a document and
    obtain representations for its sentences. Our extractive model is built on top
    of this encoder by stacking several inter-sentence Transformer layers. For abstractive
    summarization, we propose a new fine-tuning schedule which adopts different optimizers
    for the encoder and the decoder as a means of alleviating the mismatch between
    the two (the former is pretrained while the latter is not). We also demonstrate
    that a two-staged fine-tuning approach can further boost the quality of the generated
    summaries. Experiments on three datasets show that our model achieves state-of-the-art
    results across the board in both extractive and abstractive settings.
  address: Hong Kong, China
  attachment:
  - filename: D19-1387.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1387.Attachment.pdf
  author:
  - first: Yang
    full: Yang Liu
    id: yang-liu
    last: Liu
  - first: Mirella
    full: Mirella Lapata
    id: mirella-lapata
    last: Lapata
  author_string: Yang Liu, Mirella Lapata
  bibkey: liu-lapata-2019-text
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1387
  month: November
  page_first: '3730'
  page_last: '3740'
  pages: "3730\u20133740"
  paper_id: '387'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1387.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1387.jpg
  title: Text Summarization with Pretrained Encoders
  title_html: Text Summarization with Pretrained Encoders
  url: https://www.aclweb.org/anthology/D19-1387
  year: '2019'
D19-1388:
  abstract: "Under special circumstances, summaries should conform to a particular\
    \ style with patterns, such as court judgments and abstracts in academic papers.\
    \ To this end, the prototype document-summary pairs can be utilized to generate\
    \ better summaries. There are two main challenges in this task: (1) the model\
    \ needs to incorporate learned patterns from the prototype, but (2) should avoid\
    \ copying contents other than the patternized words\u2014such as irrelevant facts\u2014\
    into the generated summaries. To tackle these challenges, we design a model named\
    \ Prototype Editing based Summary Generator (PESG). PESG first learns summary\
    \ patterns and prototype facts by analyzing the correlation between a prototype\
    \ document and its summary. Prototype facts are then utilized to help extract\
    \ facts from the input document. Next, an editing generator generates new summary\
    \ based on the summary pattern or extracted facts. Finally, to address the second\
    \ challenge, a fact checker is used to estimate mutual information between the\
    \ input document and generated summary, providing an additional signal for the\
    \ generator. Extensive experiments conducted on a large-scale real-world text\
    \ summarization dataset show that PESG achieves the state-of-the-art performance\
    \ in terms of both automatic metrics and human evaluations."
  address: Hong Kong, China
  author:
  - first: Shen
    full: Shen Gao
    id: shen-gao
    last: Gao
  - first: Xiuying
    full: Xiuying Chen
    id: xiuying-chen
    last: Chen
  - first: Piji
    full: Piji Li
    id: piji-li
    last: Li
  - first: Zhangming
    full: Zhangming Chan
    id: zhangming-chan
    last: Chan
  - first: Dongyan
    full: Dongyan Zhao
    id: dongyan-zhao
    last: Zhao
  - first: Rui
    full: Rui Yan
    id: rui-yan
    last: Yan
  author_string: Shen Gao, Xiuying Chen, Piji Li, Zhangming Chan, Dongyan Zhao, Rui
    Yan
  bibkey: gao-etal-2019-write
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1388
  month: November
  page_first: '3741'
  page_last: '3751'
  pages: "3741\u20133751"
  paper_id: '388'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1388.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1388.jpg
  title: How to Write Summaries with Patterns? Learning towards Abstractive Summarization
    through Prototype Editing
  title_html: How to Write Summaries with Patterns? Learning towards Abstractive Summarization
    through Prototype Editing
  url: https://www.aclweb.org/anthology/D19-1388
  year: '2019'
D19-1389:
  abstract: 'The principle of the Information Bottleneck (Tishby et al., 1999) produces
    a summary of information X optimized to predict some other relevant information
    Y. In this paper, we propose a novel approach to unsupervised sentence summarization
    by mapping the Information Bottleneck principle to a conditional language modelling
    objective: given a sentence, our approach seeks a compressed sentence that can
    best predict the next sentence. Our iterative algorithm under the Information
    Bottleneck objective searches gradually shorter subsequences of the given sentence
    while maximizing the probability of the next sentence conditioned on the summary.
    Using only pretrained language models with no direct supervision, our approach
    can efficiently perform extractive sentence summarization over a large corpus.
    Building on our unsupervised extractive summarization, we also present a new approach
    to self-supervised abstractive summarization, where a transformer-based language
    model is trained on the output summaries of our unsupervised method. Empirical
    results demonstrate that our extractive method outperforms other unsupervised
    models on multiple automatic metrics. In addition, we find that our self-supervised
    abstractive model outperforms unsupervised baselines (including our own) by human
    evaluation along multiple attributes.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1389.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1389.Attachment.zip
  author:
  - first: Peter
    full: Peter West
    id: peter-west
    last: West
  - first: Ari
    full: Ari Holtzman
    id: ari-holtzman
    last: Holtzman
  - first: Jan
    full: Jan Buys
    id: jan-buys
    last: Buys
  - first: Yejin
    full: Yejin Choi
    id: yejin-choi
    last: Choi
  author_string: Peter West, Ari Holtzman, Jan Buys, Yejin Choi
  bibkey: west-etal-2019-bottlesum
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1389
  month: November
  page_first: '3752'
  page_last: '3761'
  pages: "3752\u20133761"
  paper_id: '389'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1389.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1389.jpg
  title: 'BottleSum: Unsupervised and Self-supervised Sentence Summarization using
    the Information Bottleneck Principle'
  title_html: '<span class="acl-fixed-case">B</span>ottle<span class="acl-fixed-case">S</span>um:
    Unsupervised and Self-supervised Sentence Summarization using the Information
    Bottleneck Principle'
  url: https://www.aclweb.org/anthology/D19-1389
  year: '2019'
D19-1390:
  abstract: "Pointer Generators have been the de facto standard for modern summarization\
    \ systems. However, this architecture faces two major drawbacks: Firstly, the\
    \ pointer is limited to copying the exact words while ignoring possible inflections\
    \ or abstractions, which restricts its power of capturing richer latent alignment.\
    \ Secondly, the copy mechanism results in a strong bias towards extractive generations,\
    \ where most sentences are produced by simply copying from the source text. In\
    \ this paper, we address these problems by allowing the model to \u201Cedit\u201D\
    \ pointed tokens instead of always hard copying them. The editing is performed\
    \ by transforming the pointed word vector into a target space with a learned relation\
    \ embedding. On three large-scale summarization dataset, we show the model is\
    \ able to (1) capture more latent alignment relations than exact word matches,\
    \ (2) improve word alignment accuracy, allowing for better model interpretation\
    \ and controlling, (3) generate higher-quality summaries validated by both qualitative\
    \ and quantitative evaluations and (4) bring more abstraction to the generated\
    \ summaries."
  address: Hong Kong, China
  attachment:
  - filename: D19-1390.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1390.Attachment.pdf
  author:
  - first: Xiaoyu
    full: Xiaoyu Shen
    id: xiaoyu-shen
    last: Shen
  - first: Yang
    full: Yang Zhao
    id: yang-zhao
    last: Zhao
  - first: Hui
    full: Hui Su
    id: hui-su
    last: Su
  - first: Dietrich
    full: Dietrich Klakow
    id: dietrich-klakow
    last: Klakow
  author_string: Xiaoyu Shen, Yang Zhao, Hui Su, Dietrich Klakow
  bibkey: shen-etal-2019-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1390
  month: November
  page_first: '3762'
  page_last: '3773'
  pages: "3762\u20133773"
  paper_id: '390'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1390.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1390.jpg
  title: Improving Latent Alignment in Text Summarization by Generalizing the Pointer
    Generator
  title_html: Improving Latent Alignment in Text Summarization by Generalizing the
    Pointer Generator
  url: https://www.aclweb.org/anthology/D19-1390
  year: '2019'
D19-1391:
  abstract: "Semantic parsing aims to map natural language utterances onto machine\
    \ interpretable meaning representations, aka programs whose execution against\
    \ a real-world environment produces a denotation. Weakly-supervised semantic parsers\
    \ are trained on utterance-denotation pairs treating programs as latent. The task\
    \ is challenging due to the large search space and spuriousness of programs which\
    \ may execute to the correct answer but do not generalize to unseen examples.\
    \ Our goal is to instill an inductive bias in the parser to help it distinguish\
    \ between spurious and correct programs. We capitalize on the intuition that correct\
    \ programs would likely respect certain structural constraints were they to be\
    \ aligned to the question (e.g., program fragments are unlikely to align to overlapping\
    \ text spans) and propose to model alignments as structured latent variables.\
    \ In order to make the latent-alignment framework tractable, we decompose the\
    \ parsing task into (1) predicting a partial \u201Cabstract program\u201D and\
    \ (2) refining it while modeling structured alignments with differential dynamic\
    \ programming. We obtain state-of-the-art performance on the WikiTableQuestions\
    \ and WikiSQL datasets. When compared to a standard attention baseline, we observe\
    \ that the proposed structured-alignment mechanism is highly beneficial."
  address: Hong Kong, China
  author:
  - first: Bailin
    full: Bailin Wang
    id: bailin-wang
    last: Wang
  - first: Ivan
    full: Ivan Titov
    id: ivan-titov
    last: Titov
  - first: Mirella
    full: Mirella Lapata
    id: mirella-lapata
    last: Lapata
  author_string: Bailin Wang, Ivan Titov, Mirella Lapata
  bibkey: wang-etal-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1391
  month: November
  page_first: '3774'
  page_last: '3785'
  pages: "3774\u20133785"
  paper_id: '391'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1391.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1391.jpg
  title: Learning Semantic Parsers from Denotations with Latent Structured Alignments
    and Abstract Programs
  title_html: Learning Semantic Parsers from Denotations with Latent Structured Alignments
    and Abstract Programs
  url: https://www.aclweb.org/anthology/D19-1391
  year: '2019'
D19-1392:
  abstract: "We unify different broad-coverage semantic parsing tasks into a transduction\
    \ parsing paradigm, and propose an attention-based neural transducer that incrementally\
    \ builds meaning representation via a sequence of semantic relations. By leveraging\
    \ multiple attention mechanisms, the neural transducer can be effectively trained\
    \ without relying on a pre-trained aligner. Experiments separately conducted on\
    \ three broad-coverage semantic parsing tasks \u2013 AMR, SDP and UCCA \u2013\
    \ demonstrate that our attention-based neural transducer improves the state of\
    \ the art on both AMR and UCCA, and is competitive with the state of the art on\
    \ SDP."
  address: Hong Kong, China
  author:
  - first: Sheng
    full: Sheng Zhang
    id: sheng-zhang
    last: Zhang
  - first: Xutai
    full: Xutai Ma
    id: xutai-ma
    last: Ma
  - first: Kevin
    full: Kevin Duh
    id: kevin-duh
    last: Duh
  - first: Benjamin
    full: Benjamin Van Durme
    id: benjamin-van-durme
    last: Van Durme
  author_string: Sheng Zhang, Xutai Ma, Kevin Duh, Benjamin Van Durme
  bibkey: zhang-etal-2019-broad
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1392
  month: November
  page_first: '3786'
  page_last: '3798'
  pages: "3786\u20133798"
  paper_id: '392'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1392.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1392.jpg
  title: Broad-Coverage Semantic Parsing as Transduction
  title_html: Broad-Coverage Semantic Parsing as Transduction
  url: https://www.aclweb.org/anthology/D19-1392
  year: '2019'
D19-1393:
  abstract: 'We introduce a novel scheme for parsing a piece of text into its Abstract
    Meaning Representation (AMR): Graph Spanning based Parsing (GSP). One novel characteristic
    of GSP is that it constructs a parse graph incrementally in a top-down fashion.
    Starting from the root, at each step, a new node and its connections to existing
    nodes will be jointly predicted. The output graph spans the nodes by the distance
    to the root, following the intuition of first grasping the main ideas then digging
    into more details. The core semantic first principle emphasizes capturing the
    main ideas of a sentence, which is of great interest. We evaluate our model on
    the latest AMR sembank and achieve the state-of-the-art performance in the sense
    that no heuristic graph re-categorization is adopted. More importantly, the experiments
    show that our parser is especially good at obtaining the core semantics.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1393.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1393.Attachment.pdf
  author:
  - first: Deng
    full: Deng Cai
    id: deng-cai
    last: Cai
  - first: Wai
    full: Wai Lam
    id: wai-lam
    last: Lam
  author_string: Deng Cai, Wai Lam
  bibkey: cai-lam-2019-core
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1393
  month: November
  page_first: '3799'
  page_last: '3809'
  pages: "3799\u20133809"
  paper_id: '393'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1393.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1393.jpg
  title: 'Core Semantic First: A Top-down Approach for AMR Parsing'
  title_html: 'Core Semantic First: A Top-down Approach for <span class="acl-fixed-case">AMR</span>
    Parsing'
  url: https://www.aclweb.org/anthology/D19-1393
  year: '2019'
D19-1394:
  abstract: 'A major hurdle on the road to conversational interfaces is the difficulty
    in collecting data that maps language utterances to logical forms. One prominent
    approach for data collection has been to automatically generate pseudo-language
    paired with logical forms, and paraphrase the pseudo-language to natural language
    through crowdsourcing (Wang et al., 2015). However, this data collection procedure
    often leads to low performance on real data, due to a mismatch between the true
    distribution of examples and the distribution induced by the data collection procedure.
    In this paper, we thoroughly analyze two sources of mismatch in this process:
    the mismatch in logical form distribution and the mismatch in language distribution
    between the true and induced distributions. We quantify the effects of these mismatches,
    and propose a new data collection approach that mitigates them. Assuming access
    to unlabeled utterances from the true distribution, we combine crowdsourcing with
    a paraphrase model to detect correct logical forms for the unlabeled utterances.
    On two datasets, our method leads to 70.6 accuracy on average on the true distribution,
    compared to 51.3 in paraphrasing-based data collection.'
  address: Hong Kong, China
  author:
  - first: Jonathan
    full: Jonathan Herzig
    id: jonathan-herzig
    last: Herzig
  - first: Jonathan
    full: Jonathan Berant
    id: jonathan-berant
    last: Berant
  author_string: Jonathan Herzig, Jonathan Berant
  bibkey: herzig-berant-2019-dont
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1394
  month: November
  page_first: '3810'
  page_last: '3820'
  pages: "3810\u20133820"
  paper_id: '394'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1394.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1394.jpg
  title: "Don\u2019t paraphrase, detect! Rapid and Effective Data Collection for Semantic\
    \ Parsing"
  title_html: "Don\u2019t paraphrase, detect! Rapid and Effective Data Collection\
    \ for Semantic Parsing"
  url: https://www.aclweb.org/anthology/D19-1394
  year: '2019'
D19-1395:
  abstract: Distantly-supervised relation extraction has proven to be effective to
    find relational facts from texts. However, the existing approaches treat labels
    as independent and meaningless one-hot vectors, which cause a loss of potential
    label information for selecting valid instances. In this paper, we propose a novel
    multi-layer attention-based model to improve relation extraction with joint label
    embedding. The model makes full use of both structural information from Knowledge
    Graphs and textual information from entity descriptions to learn label embeddings
    through gating integration while avoiding the imposed noise with an attention
    mechanism. Then the learned label embeddings are used as another atten- tion over
    the instances (whose embeddings are also enhanced with the entity descriptions)
    for improving relation extraction. Extensive experiments demonstrate that our
    model significantly outperforms state-of-the-art methods.
  address: Hong Kong, China
  author:
  - first: Linmei
    full: Linmei Hu
    id: linmei-hu
    last: Hu
  - first: Luhao
    full: Luhao Zhang
    id: luhao-zhang
    last: Zhang
  - first: Chuan
    full: Chuan Shi
    id: chuan-shi
    last: Shi
  - first: Liqiang
    full: Liqiang Nie
    id: liqiang-nie
    last: Nie
  - first: Weili
    full: Weili Guan
    id: weili-guan
    last: Guan
  - first: Cheng
    full: Cheng Yang
    id: cheng-yang
    last: Yang
  author_string: Linmei Hu, Luhao Zhang, Chuan Shi, Liqiang Nie, Weili Guan, Cheng
    Yang
  bibkey: hu-etal-2019-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1395
  month: November
  page_first: '3821'
  page_last: '3829'
  pages: "3821\u20133829"
  paper_id: '395'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1395.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1395.jpg
  title: Improving Distantly-Supervised Relation Extraction with Joint Label Embedding
  title_html: Improving Distantly-Supervised Relation Extraction with Joint Label
    Embedding
  url: https://www.aclweb.org/anthology/D19-1395
  year: '2019'
D19-1396:
  abstract: The lack of word boundaries information has been seen as one of the main
    obstacles to develop a high performance Chinese named entity recognition (NER)
    system. Fortunately, the automatically constructed lexicon contains rich word
    boundaries information and word semantic information. However, integrating lexical
    knowledge in Chinese NER tasks still faces challenges when it comes to self-matched
    lexical words as well as the nearest contextual lexical words. We present a Collaborative
    Graph Network to solve these challenges. Experiments on various datasets show
    that our model not only outperforms the state-of-the-art (SOTA) results, but also
    achieves a speed that is six to fifteen times faster than that of the SOTA model.
  address: Hong Kong, China
  author:
  - first: Dianbo
    full: Dianbo Sui
    id: dianbo-sui
    last: Sui
  - first: Yubo
    full: Yubo Chen
    id: yubo-chen
    last: Chen
  - first: Kang
    full: Kang Liu
    id: kang-liu
    last: Liu
  - first: Jun
    full: Jun Zhao
    id: jun-zhao
    last: Zhao
  - first: Shengping
    full: Shengping Liu
    id: shengping-liu
    last: Liu
  author_string: Dianbo Sui, Yubo Chen, Kang Liu, Jun Zhao, Shengping Liu
  bibkey: sui-etal-2019-leverage
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1396
  month: November
  page_first: '3830'
  page_last: '3840'
  pages: "3830\u20133840"
  paper_id: '396'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1396.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1396.jpg
  title: Leverage Lexical Knowledge for Chinese Named Entity Recognition via Collaborative
    Graph Network
  title_html: Leverage Lexical Knowledge for <span class="acl-fixed-case">C</span>hinese
    Named Entity Recognition via Collaborative Graph Network
  url: https://www.aclweb.org/anthology/D19-1396
  year: '2019'
D19-1397:
  abstract: In recent years there is a surge of interest in applying distant supervision
    (DS) to automatically generate training data for relation extraction (RE). In
    this paper, we study the problem what limits the performance of DS-trained neural
    models, conduct thorough analyses, and identify a factor that can influence the
    performance greatly, shifted label distribution. Specifically, we found this problem
    commonly exists in real-world DS datasets, and without special handing, typical
    DS-RE models cannot automatically adapt to this shift, thus achieving deteriorated
    performance. To further validate our intuition, we develop a simple yet effective
    adaptation method for DS-trained models, bias adjustment, which updates models
    learned over the source domain (i.e., DS training set) with a label distribution
    estimated on the target domain (i.e., test set). Experiments demonstrate that
    bias adjustment achieves consistent performance gains on DS-trained models, especially
    on neural models, with an up to 23% relative F1 improvement, which verifies our
    assumptions. Our code and data can be found at https://github.com/INK-USC/shifted-label-distribution.
  address: Hong Kong, China
  attachment:
  - filename: D19-1397.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1397.Attachment.pdf
  author:
  - first: Qinyuan
    full: Qinyuan Ye
    id: qinyuan-ye
    last: Ye
  - first: Liyuan
    full: Liyuan Liu
    id: liyuan-liu
    last: Liu
  - first: Maosen
    full: Maosen Zhang
    id: maosen-zhang
    last: Zhang
  - first: Xiang
    full: Xiang Ren
    id: xiang-ren
    last: Ren
  author_string: Qinyuan Ye, Liyuan Liu, Maosen Zhang, Xiang Ren
  bibkey: ye-etal-2019-looking
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1397
  month: November
  page_first: '3841'
  page_last: '3850'
  pages: "3841\u20133850"
  paper_id: '397'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1397.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1397.jpg
  title: 'Looking Beyond Label Noise: Shifted Label Distribution Matters in Distantly
    Supervised Relation Extraction'
  title_html: 'Looking Beyond Label Noise: Shifted Label Distribution Matters in Distantly
    Supervised Relation Extraction'
  url: https://www.aclweb.org/anthology/D19-1397
  year: '2019'
D19-1398:
  abstract: Many existing relation extraction (RE) models make decisions globally
    using integer linear programming (ILP). However, it is nontrivial to make use
    of integer linear programming as a blackbox solver for RE. Its cost of time and
    memory may become unacceptable with the increase of data scale, and redundant
    information needs to be encoded cautiously for ILP. In this paper, we propose
    an easy first approach for relation extraction with information redundancies,
    embedded in the results produced by local sentence level extractors, during which
    conflict decisions are resolved with domain and uniqueness constraints. Information
    redundancies are leveraged to support both easy first collective inference for
    easy decisions in the first stage and ILP for hard decisions in a subsequent stage.
    Experimental study shows that our approach improves the efficiency and accuracy
    of RE, and outperforms both ILP and neural network-based methods.
  address: Hong Kong, China
  author:
  - first: Shuai
    full: Shuai Ma
    id: shuai-ma
    last: Ma
  - first: Gang
    full: Gang Wang
    id: gang-wang
    last: Wang
  - first: Yansong
    full: Yansong Feng
    id: yansong-feng
    last: Feng
  - first: Jinpeng
    full: Jinpeng Huai
    id: jinpeng-huai
    last: Huai
  author_string: Shuai Ma, Gang Wang, Yansong Feng, Jinpeng Huai
  bibkey: ma-etal-2019-easy
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1398
  month: November
  page_first: '3851'
  page_last: '3861'
  pages: "3851\u20133861"
  paper_id: '398'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1398.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1398.jpg
  title: Easy First Relation Extraction with Information Redundancy
  title_html: Easy First Relation Extraction with Information Redundancy
  url: https://www.aclweb.org/anthology/D19-1398
  year: '2019'
D19-1399:
  abstract: Dependency tree structures capture long-distance and syntactic relationships
    between words in a sentence. The syntactic relations (e.g., nominal subject, object)
    can potentially infer the existence of certain named entities. In addition, the
    performance of a named entity recognizer could benefit from the long-distance
    dependencies between the words in dependency trees. In this work, we propose a
    simple yet effective dependency-guided LSTM-CRF model to encode the complete dependency
    trees and capture the above properties for the task of named entity recognition
    (NER). The data statistics show strong correlations between the entity types and
    dependency relations. We conduct extensive experiments on several standard datasets
    and demonstrate the effectiveness of the proposed model in improving NER and achieving
    state-of-the-art performance. Our analysis reveals that the significant improvements
    mainly result from the dependency relations and long-distance interactions provided
    by dependency trees.
  address: Hong Kong, China
  attachment:
  - filename: D19-1399.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1399.Attachment.zip
  author:
  - first: Zhanming
    full: Zhanming Jie
    id: zhanming-jie
    last: Jie
  - first: Wei
    full: Wei Lu
    id: wei-lu
    last: Lu
  author_string: Zhanming Jie, Wei Lu
  bibkey: jie-lu-2019-dependency
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1399
  month: November
  page_first: '3862'
  page_last: '3872'
  pages: "3862\u20133872"
  paper_id: '399'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1399.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1399.jpg
  title: Dependency-Guided LSTM-CRF for Named Entity Recognition
  title_html: Dependency-Guided <span class="acl-fixed-case">LSTM</span>-<span class="acl-fixed-case">CRF</span>
    for Named Entity Recognition
  url: https://www.aclweb.org/anthology/D19-1399
  year: '2019'
D19-1400:
  abstract: "Large training datasets are required to achieve competitive performance\
    \ in most natural language tasks. The acquisition process for these datasets is\
    \ labor intensive, expensive, and time consuming. This process is also prone to\
    \ human errors. In this work, we show that cross-cultural differences can be harnessed\
    \ for natural language text classification. We present a transfer-learning framework\
    \ that leverages widely-available unaligned bilingual corpora for classification\
    \ tasks, using no task-specific data. Our empirical evaluation on two tasks \u2013\
    \ formality classification and sarcasm detection \u2013 shows that the cross-cultural\
    \ difference between German and American English, as manifested in product review\
    \ text, can be applied to achieve good performance for formality classification,\
    \ while the difference between Japanese and American English can be applied to\
    \ achieve good performance for sarcasm detection \u2013 both without any task-specific\
    \ labeled data."
  address: Hong Kong, China
  author:
  - first: Dor
    full: Dor Ringel
    id: dor-ringel
    last: Ringel
  - first: Gal
    full: Gal Lavee
    id: gal-lavee
    last: Lavee
  - first: Ido
    full: Ido Guy
    id: ido-guy
    last: Guy
  - first: Kira
    full: Kira Radinsky
    id: kira-radinsky
    last: Radinsky
  author_string: Dor Ringel, Gal Lavee, Ido Guy, Kira Radinsky
  bibkey: ringel-etal-2019-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1400
  month: November
  page_first: '3873'
  page_last: '3883'
  pages: "3873\u20133883"
  paper_id: '400'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1400.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1400.jpg
  title: Cross-Cultural Transfer Learning for Text Classification
  title_html: Cross-Cultural Transfer Learning for Text Classification
  url: https://www.aclweb.org/anthology/D19-1400
  year: '2019'
D19-1401:
  abstract: 'Supervised learning models often perform poorly at low-shot tasks, i.e.
    tasks for which little labeled data is available for training. One prominent approach
    for improving low-shot learning is to use unsupervised pre-trained neural models.
    Another approach is to obtain richer supervision by collecting annotator rationales
    (explanations supporting label annotations). In this work, we combine these two
    approaches to improve low-shot text classification with two novel methods: a simple
    bag-of-words embedding approach; and a more complex context-aware method, based
    on the BERT model. In experiments with two English text classification datasets,
    we demonstrate substantial performance gains from combining pre-training with
    rationales. Furthermore, our investigation of a range of train-set sizes reveals
    that the simple bag-of-words approach is the clear top performer when there are
    only a few dozen training instances or less, while more complex models, such as
    BERT or CNN, require more training data to shine.'
  address: Hong Kong, China
  author:
  - first: Oren
    full: Oren Melamud
    id: oren-melamud
    last: Melamud
  - first: Mihaela
    full: Mihaela Bornea
    id: mihaela-bornea
    last: Bornea
  - first: Ken
    full: Ken Barker
    id: ken-barker
    last: Barker
  author_string: Oren Melamud, Mihaela Bornea, Ken Barker
  bibkey: melamud-etal-2019-combining
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1401
  month: November
  page_first: '3884'
  page_last: '3893'
  pages: "3884\u20133893"
  paper_id: '401'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1401.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1401.jpg
  title: Combining Unsupervised Pre-training and Annotator Rationales to Improve Low-shot
    Text Classification
  title_html: Combining Unsupervised Pre-training and Annotator Rationales to Improve
    Low-shot Text Classification
  url: https://www.aclweb.org/anthology/D19-1401
  year: '2019'
D19-1402:
  abstract: We propose a novel on-device sequence model for text classification using
    recurrent projections. Our model ProSeqo uses dynamic recurrent projections without
    the need to store or look up any pre-trained embeddings. This results in fast
    and compact neural networks that can perform on-device inference for complex short
    and long text classification tasks. We conducted exhaustive evaluation on multiple
    text classification tasks. Results show that ProSeqo outperformed state-of-the-art
    neural and on-device approaches for short text classification tasks such as dialog
    act and intent prediction. To the best of our knowledge, ProSeqo is the first
    on-device long text classification neural model. It achieved comparable results
    to previous neural approaches for news article, answers and product categorization,
    while preserving small memory footprint and maintaining high accuracy.
  address: Hong Kong, China
  author:
  - first: Zornitsa
    full: Zornitsa Kozareva
    id: zornitsa-kozareva
    last: Kozareva
  - first: Sujith
    full: Sujith Ravi
    id: sujith-ravi
    last: Ravi
  author_string: Zornitsa Kozareva, Sujith Ravi
  bibkey: kozareva-ravi-2019-proseqo
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1402
  month: November
  page_first: '3894'
  page_last: '3903'
  pages: "3894\u20133903"
  paper_id: '402'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1402.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1402.jpg
  title: 'ProSeqo: Projection Sequence Networks for On-Device Text Classification'
  title_html: '<span class="acl-fixed-case">P</span>ro<span class="acl-fixed-case">S</span>eqo:
    Projection Sequence Networks for On-Device Text Classification'
  url: https://www.aclweb.org/anthology/D19-1402
  year: '2019'
D19-1403:
  abstract: Text classification tends to struggle when data is deficient or when it
    needs to adapt to unseen classes. In such challenging scenarios, recent studies
    have used meta-learning to simulate the few-shot task, in which new queries are
    compared to a small support set at the sample-wise level. However, this sample-wise
    comparison may be severely disturbed by the various expressions in the same class.
    Therefore, we should be able to learn a general representation of each class in
    the support set and then compare it to new queries. In this paper, we propose
    a novel Induction Network to learn such a generalized class-wise representation,
    by innovatively leveraging the dynamic routing algorithm in meta-learning. In
    this way, we find the model is able to induce and generalize better. We evaluate
    the proposed model on a well-studied sentiment classification dataset (English)
    and a real-world dialogue intent classification dataset (Chinese). Experiment
    results show that on both datasets, the proposed model significantly outperforms
    the existing state-of-the-art approaches, proving the effectiveness of class-wise
    generalization in few-shot text classification.
  address: Hong Kong, China
  author:
  - first: Ruiying
    full: Ruiying Geng
    id: ruiying-geng
    last: Geng
  - first: Binhua
    full: Binhua Li
    id: binhua-li
    last: Li
  - first: Yongbin
    full: Yongbin Li
    id: yongbin-li
    last: Li
  - first: Xiaodan
    full: Xiaodan Zhu
    id: xiaodan-zhu
    last: Zhu
  - first: Ping
    full: Ping Jian
    id: ping-jian
    last: Jian
  - first: Jian
    full: Jian Sun
    id: jian-sun
    last: Sun
  author_string: Ruiying Geng, Binhua Li, Yongbin Li, Xiaodan Zhu, Ping Jian, Jian
    Sun
  bibkey: geng-etal-2019-induction
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1403
  month: November
  page_first: '3904'
  page_last: '3913'
  pages: "3904\u20133913"
  paper_id: '403'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1403.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1403.jpg
  title: Induction Networks for Few-Shot Text Classification
  title_html: Induction Networks for Few-Shot Text Classification
  url: https://www.aclweb.org/anthology/D19-1403
  year: '2019'
D19-1404:
  abstract: "Zero-shot text classification (0Shot-TC) is a challenging NLU problem\
    \ to which little attention has been paid by the research community. 0Shot-TC\
    \ aims to associate an appropriate label with a piece of text, irrespective of\
    \ the text domain and the aspect (e.g., topic, emotion, event, etc.) described\
    \ by the label. And there are only a few articles studying 0Shot-TC, all focusing\
    \ only on topical categorization which, we argue, is just the tip of the iceberg\
    \ in 0Shot-TC. In addition, the chaotic experiments in literature make no uniform\
    \ comparison, which blurs the progress. This work benchmarks the 0Shot-TC problem\
    \ by providing unified datasets, standardized evaluations, and state-of-the-art\
    \ baselines. Our contributions include: i) The datasets we provide facilitate\
    \ studying 0Shot-TC relative to conceptually different and diverse aspects: the\
    \ \u201Ctopic\u201D aspect includes \u201Csports\u201D and \u201Cpolitics\u201D\
    \ as labels; the \u201Cemotion\u201D aspect includes \u201Cjoy\u201D and \u201C\
    anger\u201D; the \u201Csituation\u201D aspect includes \u201Cmedical assistance\u201D\
    \ and \u201Cwater shortage\u201D. ii) We extend the existing evaluation setup\
    \ (label-partially-unseen) \u2013 given a dataset, train on some labels, test\
    \ on all labels \u2013 to include a more challenging yet realistic evaluation\
    \ label-fully-unseen 0Shot-TC (Chang et al., 2008), aiming at classifying text\
    \ snippets without seeing task specific training data at all. iii) We unify the\
    \ 0Shot-TC of diverse aspects within a textual entailment formulation and study\
    \ it this way."
  address: Hong Kong, China
  author:
  - first: Wenpeng
    full: Wenpeng Yin
    id: wenpeng-yin
    last: Yin
  - first: Jamaal
    full: Jamaal Hay
    id: jamaal-hay
    last: Hay
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Wenpeng Yin, Jamaal Hay, Dan Roth
  bibkey: yin-etal-2019-benchmarking
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1404
  month: November
  page_first: '3914'
  page_last: '3923'
  pages: "3914\u20133923"
  paper_id: '404'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1404.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1404.jpg
  title: 'Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment
    Approach'
  title_html: 'Benchmarking Zero-shot Text Classification: Datasets, Evaluation and
    Entailment Approach'
  url: https://www.aclweb.org/anthology/D19-1404
  year: '2019'
D19-1405:
  abstract: While neural models show remarkable accuracy on individual predictions,
    their internal beliefs can be inconsistent across examples. In this paper, we
    formalize such inconsistency as a generalization of prediction error. We propose
    a learning framework for constraining models using logic rules to regularize them
    away from inconsistency. Our framework can leverage both labeled and unlabeled
    examples and is directly compatible with off-the-shelf learning schemes without
    model redesign. We instantiate our framework on natural language inference, where
    experiments show that enforcing invariants stated in logic can help make the predictions
    of neural models both accurate and consistent.
  address: Hong Kong, China
  author:
  - first: Tao
    full: Tao Li
    id: tao-li
    last: Li
  - first: Vivek
    full: Vivek Gupta
    id: vivek-gupta
    last: Gupta
  - first: Maitrey
    full: Maitrey Mehta
    id: maitrey-mehta
    last: Mehta
  - first: Vivek
    full: Vivek Srikumar
    id: vivek-srikumar
    last: Srikumar
  author_string: Tao Li, Vivek Gupta, Maitrey Mehta, Vivek Srikumar
  bibkey: li-etal-2019-logic
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1405
  month: November
  page_first: '3924'
  page_last: '3935'
  pages: "3924\u20133935"
  paper_id: '405'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1405.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1405.jpg
  title: A Logic-Driven Framework for Consistency of Neural Models
  title_html: A Logic-Driven Framework for Consistency of Neural Models
  url: https://www.aclweb.org/anthology/D19-1405
  year: '2019'
D19-1406:
  abstract: This paper shows that standard assessment methodology for style transfer
    has several significant problems. First, the standard metrics for style accuracy
    and semantics preservation vary significantly on different re-runs. Therefore
    one has to report error margins for the obtained results. Second, starting with
    certain values of bilingual evaluation understudy (BLEU) between input and output
    and accuracy of the sentiment transfer the optimization of these two standard
    metrics diverge from the intuitive goal of the style transfer task. Finally, due
    to the nature of the task itself, there is a specific dependence between these
    two metrics that could be easily manipulated. Under these circumstances, we suggest
    taking BLEU between input and human-written reformulations into consideration
    for benchmarks. We also propose three new architectures that outperform state
    of the art in terms of this metric.
  address: Hong Kong, China
  author:
  - first: Alexey
    full: Alexey Tikhonov
    id: alexey-tikhonov
    last: Tikhonov
  - first: Viacheslav
    full: Viacheslav Shibaev
    id: viacheslav-shibaev
    last: Shibaev
  - first: Aleksander
    full: Aleksander Nagaev
    id: aleksander-nagaev
    last: Nagaev
  - first: Aigul
    full: Aigul Nugmanova
    id: aigul-nugmanova
    last: Nugmanova
  - first: Ivan P.
    full: Ivan P. Yamshchikov
    id: ivan-p-yamshchikov
    last: Yamshchikov
  author_string: Alexey Tikhonov, Viacheslav Shibaev, Aleksander Nagaev, Aigul Nugmanova,
    Ivan P. Yamshchikov
  bibkey: tikhonov-etal-2019-style
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1406
  month: November
  page_first: '3936'
  page_last: '3945'
  pages: "3936\u20133945"
  paper_id: '406'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1406.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1406.jpg
  title: 'Style Transfer for Texts: Retrain, Report Errors, Compare with Rewrites'
  title_html: 'Style Transfer for Texts: Retrain, Report Errors, Compare with Rewrites'
  url: https://www.aclweb.org/anthology/D19-1406
  year: '2019'
D19-1407:
  abstract: "Deep latent variable models (LVM) such as variational auto-encoder (VAE)\
    \ have recently played an important role in text generation. One key factor is\
    \ the exploitation of smooth latent structures to guide the generation. However,\
    \ the representation power of VAEs is limited due to two reasons: (1) the Gaussian\
    \ assumption is often made on the variational posteriors; and meanwhile (2) a\
    \ notorious \u201Cposterior collapse\u201D issue occurs. In this paper, we advocate\
    \ sample-based representations of variational distributions for natural language,\
    \ leading to implicit latent features, which can provide flexible representation\
    \ power compared with Gaussian-based posteriors. We further develop an LVM to\
    \ directly match the aggregated posterior to the prior. It can be viewed as a\
    \ natural extension of VAEs with a regularization of maximizing mutual information,\
    \ mitigating the \u201Cposterior collapse\u201D issue. We demonstrate the effectiveness\
    \ and versatility of our models in various text generation scenarios, including\
    \ language modeling, unaligned style transfer, and dialog response generation.\
    \ The source code to reproduce our experimental results is available on GitHub."
  address: Hong Kong, China
  attachment:
  - filename: D19-1407.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1407.Attachment.zip
  author:
  - first: Le
    full: Le Fang
    id: le-fang
    last: Fang
  - first: Chunyuan
    full: Chunyuan Li
    id: chunyuan-li
    last: Li
  - first: Jianfeng
    full: Jianfeng Gao
    id: jianfeng-gao
    last: Gao
  - first: Wen
    full: Wen Dong
    id: wen-dong
    last: Dong
  - first: Changyou
    full: Changyou Chen
    id: changyou-chen
    last: Chen
  author_string: Le Fang, Chunyuan Li, Jianfeng Gao, Wen Dong, Changyou Chen
  bibkey: fang-etal-2019-implicit
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1407
  month: November
  page_first: '3946'
  page_last: '3956'
  pages: "3946\u20133956"
  paper_id: '407'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1407.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1407.jpg
  title: Implicit Deep Latent Variable Models for Text Generation
  title_html: Implicit Deep Latent Variable Models for Text Generation
  url: https://www.aclweb.org/anthology/D19-1407
  year: '2019'
D19-1408:
  abstract: 'Text emotion distribution learning (EDL) aims to develop models that
    can predict the intensity values of a sentence across a set of emotion categories.
    Existing methods based on supervised learning require a large amount of well-labelled
    training data, which is difficult to obtain due to inconsistent perception of
    fine-grained emotion intensity. In this paper, we propose a meta-learning approach
    to learn text emotion distributions from a small sample. Specifically, we propose
    to learn low-rank sentence embeddings by tensor decomposition to capture their
    contextual semantic similarity, and use K-nearest neighbors (KNNs) of each sentence
    in the embedding space to generate sample clusters. We then train a meta-learner
    that can adapt to new data with only a few training samples on the clusters, and
    further fit the meta-learner on KNNs of a testing sample for EDL. In this way,
    we effectively augment the learning ability of a model on the small sample. To
    demonstrate the performance, we compare the proposed approach with state-of-the-art
    EDL methods on a widely used EDL dataset: SemEval 2007 Task 14 (Strapparava and
    Mihalcea, 2007). Results show the superiority of our method on small-sample emotion
    distribution learning.'
  address: Hong Kong, China
  author:
  - first: Zhenjie
    full: Zhenjie Zhao
    id: zhenjie-zhao
    last: Zhao
  - first: Xiaojuan
    full: Xiaojuan Ma
    id: xiaojuan-ma
    last: Ma
  author_string: Zhenjie Zhao, Xiaojuan Ma
  bibkey: zhao-ma-2019-text
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1408
  month: November
  page_first: '3957'
  page_last: '3967'
  pages: "3957\u20133967"
  paper_id: '408'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1408.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1408.jpg
  title: 'Text Emotion Distribution Learning from Small Sample: A Meta-Learning Approach'
  title_html: 'Text Emotion Distribution Learning from Small Sample: A Meta-Learning
    Approach'
  url: https://www.aclweb.org/anthology/D19-1408
  year: '2019'
D19-1409:
  abstract: We conduct a large-scale, systematic study to evaluate the existing evaluation
    methods for natural language generation in the context of generating online product
    reviews. We compare human-based evaluators with a variety of automated evaluation
    procedures, including discriminative evaluators that measure how well machine-generated
    text can be distinguished from human-written text, as well as word overlap metrics
    that assess how similar the generated text compares to human-written references.
    We determine to what extent these different evaluators agree on the ranking of
    a dozen of state-of-the-art generators for online product reviews. We find that
    human evaluators do not correlate well with discriminative evaluators, leaving
    a bigger question of whether adversarial accuracy is the correct objective for
    natural language generation. In general, distinguishing machine-generated text
    is challenging even for human evaluators, and human decisions correlate better
    with lexical overlaps. We find lexical diversity an intriguing metric that is
    indicative of the assessments of different evaluators. A post-experiment survey
    of participants provides insights into how to evaluate and improve the quality
    of natural language generation systems.
  address: Hong Kong, China
  attachment:
  - filename: D19-1409.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1409.Attachment.pdf
  author:
  - first: Cristina
    full: Cristina Garbacea
    id: cristina-garbacea
    last: Garbacea
  - first: Samuel
    full: Samuel Carton
    id: samuel-carton
    last: Carton
  - first: Shiyan
    full: Shiyan Yan
    id: shiyan-yan
    last: Yan
  - first: Qiaozhu
    full: Qiaozhu Mei
    id: qiaozhu-mei
    last: Mei
  author_string: Cristina Garbacea, Samuel Carton, Shiyan Yan, Qiaozhu Mei
  bibkey: garbacea-etal-2019-judge
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1409
  month: November
  page_first: '3968'
  page_last: '3981'
  pages: "3968\u20133981"
  paper_id: '409'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1409.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1409.jpg
  title: 'Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models
    for Online Review Generation'
  title_html: 'Judge the Judges: A Large-Scale Evaluation Study of Neural Language
    Models for Online Review Generation'
  url: https://www.aclweb.org/anthology/D19-1409
  year: '2019'
D19-1410:
  abstract: 'BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new
    state-of-the-art performance on sentence-pair regression tasks like semantic textual
    similarity (STS). However, it requires that both sentences are fed into the network,
    which causes a massive computational overhead: Finding the most similar pair in
    a collection of 10,000 sentences requires about 50 million inference computations
    (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic
    similarity search as well as for unsupervised tasks like clustering. In this publication,
    we present Sentence-BERT (SBERT), a modification of the pretrained BERT network
    that use siamese and triplet network structures to derive semantically meaningful
    sentence embeddings that can be compared using cosine-similarity. This reduces
    the effort for finding the most similar pair from 65 hours with BERT / RoBERTa
    to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate
    SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms
    other state-of-the-art sentence embeddings methods.'
  address: Hong Kong, China
  author:
  - first: Nils
    full: Nils Reimers
    id: nils-reimers
    last: Reimers
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Nils Reimers, Iryna Gurevych
  bibkey: reimers-gurevych-2019-sentence
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1410
  month: November
  page_first: '3982'
  page_last: '3992'
  pages: "3982\u20133992"
  paper_id: '410'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1410.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1410.jpg
  title: 'Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks'
  title_html: 'Sentence-<span class="acl-fixed-case">BERT</span>: Sentence Embeddings
    using <span class="acl-fixed-case">S</span>iamese <span class="acl-fixed-case">BERT</span>-Networks'
  url: https://www.aclweb.org/anthology/D19-1410
  year: '2019'
D19-1411:
  abstract: We consider a document classification problem where document labels are
    absent but only relevant keywords of a target class and unlabeled documents are
    given. Although heuristic methods based on pseudo-labeling have been considered,
    theoretical understanding of this problem has still been limited. Moreover, previous
    methods cannot easily incorporate well-developed techniques in supervised text
    classification. In this paper, we propose a theoretically guaranteed learning
    framework that is simple to implement and has flexible choices of models, e.g.,
    linear models or neural networks. We demonstrate how to optimize the area under
    the receiver operating characteristic curve (AUC) effectively and also discuss
    how to adjust it to optimize other well-known evaluation metrics such as the accuracy
    and F1-measure. Finally, we show the effectiveness of our framework using benchmark
    datasets.
  address: Hong Kong, China
  attachment:
  - filename: D19-1411.Attachment.rar
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1411.Attachment.rar
  author:
  - first: Nontawat
    full: Nontawat Charoenphakdee
    id: nontawat-charoenphakdee
    last: Charoenphakdee
  - first: Jongyeong
    full: Jongyeong Lee
    id: jongyeong-lee
    last: Lee
  - first: Yiping
    full: Yiping Jin
    id: yiping-jin
    last: Jin
  - first: Dittaya
    full: Dittaya Wanvarie
    id: dittaya-wanvarie
    last: Wanvarie
  - first: Masashi
    full: Masashi Sugiyama
    id: masashi-sugiyama
    last: Sugiyama
  author_string: Nontawat Charoenphakdee, Jongyeong Lee, Yiping Jin, Dittaya Wanvarie,
    Masashi Sugiyama
  bibkey: charoenphakdee-etal-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1411
  month: November
  page_first: '3993'
  page_last: '4002'
  pages: "3993\u20134002"
  paper_id: '411'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1411.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1411.jpg
  title: Learning Only from Relevant Keywords and Unlabeled Documents
  title_html: Learning Only from Relevant Keywords and Unlabeled Documents
  url: https://www.aclweb.org/anthology/D19-1411
  year: '2019'
D19-1412:
  abstract: 'This paper presents a new sequence-to-sequence (seq2seq) pre-training
    method PoDA (Pre-training of Denoising Autoencoders), which learns representations
    suitable for text generation tasks. Unlike encoder-only (e.g., BERT) or decoder-only
    (e.g., OpenAI GPT) pre-training approaches, PoDA jointly pre-trains both the encoder
    and decoder by denoising the noise-corrupted text, and it also has the advantage
    of keeping the network architecture unchanged in the subsequent fine-tuning stage.
    Meanwhile, we design a hybrid model of Transformer and pointer-generator networks
    as the backbone architecture for PoDA. We conduct experiments on two text generation
    tasks: abstractive summarization, and grammatical error correction. Results on
    four datasets show that PoDA can improve model performance over strong baselines
    without using any task-specific techniques and significantly speed up convergence.'
  address: Hong Kong, China
  author:
  - first: Liang
    full: Liang Wang
    id: liang-wang
    last: Wang
  - first: Wei
    full: Wei Zhao
    id: wei-zhao
    last: Zhao
  - first: Ruoyu
    full: Ruoyu Jia
    id: ruoyu-jia
    last: Jia
  - first: Sujian
    full: Sujian Li
    id: sujian-li
    last: Li
  - first: Jingming
    full: Jingming Liu
    id: jingming-liu
    last: Liu
  author_string: Liang Wang, Wei Zhao, Ruoyu Jia, Sujian Li, Jingming Liu
  bibkey: wang-etal-2019-denoising
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1412
  month: November
  page_first: '4003'
  page_last: '4015'
  pages: "4003\u20134015"
  paper_id: '412'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1412.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1412.jpg
  title: Denoising based Sequence-to-Sequence Pre-training for Text Generation
  title_html: Denoising based Sequence-to-Sequence Pre-training for Text Generation
  url: https://www.aclweb.org/anthology/D19-1412
  year: '2019'
D19-1413:
  abstract: We introduce the dialog intent induction task and present a novel deep
    multi-view clustering approach to tackle the problem. Dialog intent induction
    aims at discovering user intents from user query utterances in human-human conversations
    such as dialogs between customer support agents and customers. Motivated by the
    intuition that a dialog intent is not only expressed in the user query utterance
    but also captured in the rest of the dialog, we split a conversation into two
    independent views and exploit multi-view clustering techniques for inducing the
    dialog intent. In par- ticular, we propose alternating-view k-means (AV-KMEANS)
    for joint multi-view represen- tation learning and clustering analysis. The key
    innovation is that the instance-view representations are updated iteratively by
    predicting the cluster assignment obtained from the alternative view, so that
    the multi-view representations of the instances lead to similar cluster assignments.
    Experiments on two public datasets show that AV-KMEANS can induce better dialog
    intent clusters than state-of-the-art unsupervised representation learning methods
    and standard multi-view clustering approaches.
  address: Hong Kong, China
  author:
  - first: Hugh
    full: Hugh Perkins
    id: hugh-perkins
    last: Perkins
  - first: Yi
    full: Yi Yang
    id: yi-yang
    last: Yang
  author_string: Hugh Perkins, Yi Yang
  bibkey: perkins-yang-2019-dialog
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1413
  month: November
  page_first: '4016'
  page_last: '4025'
  pages: "4016\u20134025"
  paper_id: '413'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1413.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1413.jpg
  title: Dialog Intent Induction with Deep Multi-View Clustering
  title_html: Dialog Intent Induction with Deep Multi-View Clustering
  url: https://www.aclweb.org/anthology/D19-1413
  year: '2019'
D19-1414:
  abstract: Recently, kernelized locality sensitive hashcodes have been successfully
    employed as representations of natural language text, especially showing high
    relevance to biomedical relation extraction tasks. In this paper, we propose to
    optimize the hashcode representations in a nearly unsupervised manner, in which
    we only use data points, but not their class labels, for learning. The optimized
    hashcode representations are then fed to a supervised classifi er following the
    prior work. This nearly unsupervised approach allows fine-grained optimization
    of each hash function, which is particularly suitable for building hashcode representations
    generalizing from a training set to a test set. We empirically evaluate the proposed
    approach for biomedical relation extraction tasks, obtaining significant accuracy
    improvements w.r.t. state-of-the-art supervised and semi-supervised approaches.
  address: Hong Kong, China
  author:
  - first: Sahil
    full: Sahil Garg
    id: sahil-garg
    last: Garg
  - first: Aram
    full: Aram Galstyan
    id: aram-galstyan
    last: Galstyan
  - first: Greg
    full: Greg Ver Steeg
    id: greg-ver-steeg
    last: Ver Steeg
  - first: Guillermo
    full: Guillermo Cecchi
    id: guillermo-a-cecchi
    last: Cecchi
  author_string: Sahil Garg, Aram Galstyan, Greg Ver Steeg, Guillermo Cecchi
  bibkey: garg-etal-2019-nearly
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1414
  month: November
  page_first: '4026'
  page_last: '4036'
  pages: "4026\u20134036"
  paper_id: '414'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1414.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1414.jpg
  title: Nearly-Unsupervised Hashcode Representations for Biomedical Relation Extraction
  title_html: Nearly-Unsupervised Hashcode Representations for Biomedical Relation
    Extraction
  url: https://www.aclweb.org/anthology/D19-1414
  year: '2019'
D19-1415:
  abstract: "While NLP systems become more pervasive, their accountability gains value\
    \ as a focal point of effort. Epistemological opaqueness of nonlinear learning\
    \ methods, such as deep learning models, can be a major drawback for their adoptions.\
    \ In this paper, we discuss the application of Layerwise Relevance Propagation\
    \ over a linguistically motivated neural architecture, the Kernel-based Deep Architecture,\
    \ in order to trace back connections between linguistic properties of input instances\
    \ and system decisions. Such connections then guide the construction of argumentations\
    \ on network\u2019s inferences, i.e., explanations based on real examples, semantically\
    \ related to the input. We propose here a methodology to evaluate the transparency\
    \ and coherence of analogy-based explanations modeling an audit stage for the\
    \ system. Quantitative analysis on two semantic tasks, i.e., question classification\
    \ and semantic role labeling, show that the explanatory capabilities (native in\
    \ KDAs) are effective and they pave the way to more complex argumentation methods."
  address: Hong Kong, China
  author:
  - first: Danilo
    full: Danilo Croce
    id: danilo-croce
    last: Croce
  - first: Daniele
    full: Daniele Rossini
    id: daniele-rossini
    last: Rossini
  - first: Roberto
    full: Roberto Basili
    id: roberto-basili
    last: Basili
  author_string: Danilo Croce, Daniele Rossini, Roberto Basili
  bibkey: croce-etal-2019-auditing
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1415
  month: November
  page_first: '4037'
  page_last: '4046'
  pages: "4037\u20134046"
  paper_id: '415'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1415.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1415.jpg
  title: Auditing Deep Learning processes through Kernel-based Explanatory Models
  title_html: Auditing Deep Learning processes through Kernel-based Explanatory Models
  url: https://www.aclweb.org/anthology/D19-1415
  year: '2019'
D19-1416:
  abstract: While broadly applicable to many natural language processing (NLP) tasks,
    variational autoencoders (VAEs) are hard to train due to the posterior collapse
    issue where the latent variable fails to encode the input data effectively. Various
    approaches have been proposed to alleviate this problem to improve the capability
    of the VAE. In this paper, we propose to introduce a mutual information (MI) term
    between the input and its latent variable to regularize the objective of the VAE.
    Since estimating the MI in the high-dimensional space is intractable, we employ
    neural networks for the estimation of the MI and provide a training algorithm
    based on the convex duality approach. Our experimental results on three benchmark
    datasets demonstrate that the proposed model, compared to the state-of-the-art
    baselines, exhibits less posterior collapse and has comparable or better performance
    in language modeling and text generation. We also qualitatively evaluate the inferred
    latent space and show that the proposed model can generate more reasonable and
    diverse sentences via linear interpolation in the latent space.
  address: Hong Kong, China
  attachment:
  - filename: D19-1416.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1416.Attachment.pdf
  author:
  - first: Dong
    full: Dong Qian
    id: dong-qian
    last: Qian
  - first: William K.
    full: William K. Cheung
    id: william-k-cheung
    last: Cheung
  author_string: Dong Qian, William K. Cheung
  bibkey: qian-cheung-2019-enhancing
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1416
  month: November
  page_first: '4047'
  page_last: '4057'
  pages: "4047\u20134057"
  paper_id: '416'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1416.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1416.jpg
  title: Enhancing Variational Autoencoders with Mutual Information Neural Estimation
    for Text Generation
  title_html: Enhancing Variational Autoencoders with Mutual Information Neural Estimation
    for Text Generation
  url: https://www.aclweb.org/anthology/D19-1416
  year: '2019'
D19-1417:
  abstract: 'The exploding cost and time needed for data labeling and model training
    are bottlenecks for training DNN models on large datasets. Identifying smaller
    representative data samples with strategies like active learning can help mitigate
    such bottlenecks. Previous works on active learning in NLP identify the problem
    of sampling bias in the samples acquired by uncertainty-based querying and develop
    costly approaches to address it. Using a large empirical study, we demonstrate
    that active set selection using the posterior entropy of deep models like FastText.zip
    (FTZ) is robust to sampling biases and to various algorithmic choices (query size
    and strategies) unlike that suggested by traditional literature. We also show
    that FTZ based query strategy produces sample sets similar to those from more
    sophisticated approaches (e.g ensemble networks). Finally, we show the effectiveness
    of the selected samples by creating tiny high-quality datasets, and utilizing
    them for fast and cheap training of large models. Based on the above, we propose
    a simple baseline for deep active text classification that outperforms the state
    of the art. We expect the presented work to be useful and informative for dataset
    compression and for problems involving active, semi-supervised or online learning
    scenarios. Code and models are available at: https://github.com/drimpossible/Sampling-Bias-Active-Learning.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1417.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1417.Attachment.pdf
  author:
  - first: Ameya
    full: Ameya Prabhu
    id: ameya-prabhu
    last: Prabhu
  - first: Charles
    full: Charles Dognin
    id: charles-dognin
    last: Dognin
  - first: Maneesh
    full: Maneesh Singh
    id: maneesh-singh
    last: Singh
  author_string: Ameya Prabhu, Charles Dognin, Maneesh Singh
  bibkey: prabhu-etal-2019-sampling
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1417
  month: November
  page_first: '4058'
  page_last: '4068'
  pages: "4058\u20134068"
  paper_id: '417'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1417.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1417.jpg
  title: 'Sampling Bias in Deep Active Classification: An Empirical Study'
  title_html: 'Sampling Bias in Deep Active Classification: An Empirical Study'
  url: https://www.aclweb.org/anthology/D19-1417
  year: '2019'
D19-1418:
  abstract: 'State-of-the-art models often make use of superficial patterns in the
    data that do not generalize well to out-of-domain or adversarial settings. For
    example, textual entailment models often learn that particular key words imply
    entailment, irrespective of context, and visual question answering models learn
    to predict prototypical answers, without considering evidence in the image. In
    this paper, we show that if we have prior knowledge of such biases, we can train
    a model to be more robust to domain shift. Our method has two stages: we (1) train
    a naive model that makes predictions exclusively based on dataset biases, and
    (2) train a robust model as part of an ensemble with the naive one in order to
    encourage it to focus on other patterns in the data that are more likely to generalize.
    Experiments on five datasets with out-of-domain test sets show significantly improved
    robustness in all settings, including a 12 point gain on a changing priors visual
    question answering dataset and a 9 point gain on an adversarial question answering
    test set.'
  address: Hong Kong, China
  author:
  - first: Christopher
    full: Christopher Clark
    id: christopher-clark
    last: Clark
  - first: Mark
    full: Mark Yatskar
    id: mark-yatskar
    last: Yatskar
  - first: Luke
    full: Luke Zettlemoyer
    id: luke-zettlemoyer
    last: Zettlemoyer
  author_string: Christopher Clark, Mark Yatskar, Luke Zettlemoyer
  bibkey: clark-etal-2019-dont
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1418
  month: November
  page_first: '4069'
  page_last: '4082'
  pages: "4069\u20134082"
  paper_id: '418'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1418.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1418.jpg
  title: "Don\u2019t Take the Easy Way Out: Ensemble Based Methods for Avoiding Known\
    \ Dataset Biases"
  title_html: "Don\u2019t Take the Easy Way Out: Ensemble Based Methods for Avoiding\
    \ Known Dataset Biases"
  url: https://www.aclweb.org/anthology/D19-1418
  year: '2019'
D19-1419:
  abstract: "Neural networks are part of many contemporary NLP systems, yet their\
    \ empirical successes come at the price of vulnerability to adversarial attacks.\
    \ Previous work has used adversarial training and data augmentation to partially\
    \ mitigate such brittleness, but these are unlikely to find worst-case adversaries\
    \ due to the complexity of the search space arising from discrete text perturbations.\
    \ In this work, we approach the problem from the opposite direction: to formally\
    \ verify a system\u2019s robustness against a predefined class of adversarial\
    \ attacks. We study text classification under synonym replacements or character\
    \ flip perturbations. We propose modeling these input perturbations as a simplex\
    \ and then using Interval Bound Propagation \u2013 a formal model verification\
    \ method. We modify the conventional log-likelihood training objective to train\
    \ models that can be efficiently verified, which would otherwise come with exponential\
    \ search complexity. The resulting models show only little difference in terms\
    \ of nominal accuracy, but have much improved verified accuracy under perturbations\
    \ and come with an efficiently computable formal guarantee on worst case adversaries."
  address: Hong Kong, China
  attachment:
  - filename: D19-1419.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1419.Attachment.zip
  author:
  - first: Po-Sen
    full: Po-Sen Huang
    id: po-sen-huang
    last: Huang
  - first: Robert
    full: Robert Stanforth
    id: robert-stanforth
    last: Stanforth
  - first: Johannes
    full: Johannes Welbl
    id: johannes-welbl
    last: Welbl
  - first: Chris
    full: Chris Dyer
    id: chris-dyer
    last: Dyer
  - first: Dani
    full: Dani Yogatama
    id: dani-yogatama
    last: Yogatama
  - first: Sven
    full: Sven Gowal
    id: sven-gowal
    last: Gowal
  - first: Krishnamurthy
    full: Krishnamurthy Dvijotham
    id: krishnamurthy-dvijotham
    last: Dvijotham
  - first: Pushmeet
    full: Pushmeet Kohli
    id: pushmeet-kohli
    last: Kohli
  author_string: Po-Sen Huang, Robert Stanforth, Johannes Welbl, Chris Dyer, Dani
    Yogatama, Sven Gowal, Krishnamurthy Dvijotham, Pushmeet Kohli
  bibkey: huang-etal-2019-achieving
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1419
  month: November
  page_first: '4083'
  page_last: '4093'
  pages: "4083\u20134093"
  paper_id: '419'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1419.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1419.jpg
  title: Achieving Verified Robustness to Symbol Substitutions via Interval Bound
    Propagation
  title_html: Achieving Verified Robustness to Symbol Substitutions via Interval Bound
    Propagation
  url: https://www.aclweb.org/anthology/D19-1419
  year: '2019'
D19-1420:
  abstract: "Selective rationalization has become a common mechanism to ensure that\
    \ predictive models reveal how they use any available features. The selection\
    \ may be soft or hard, and identifies a subset of input features relevant for\
    \ prediction. The setup can be viewed as a co-operate game between the selector\
    \ (aka rationale generator) and the predictor making use of only the selected\
    \ features. The co-operative setting may, however, be compromised for two reasons.\
    \ First, the generator typically has no direct access to the outcome it aims to\
    \ justify, resulting in poor performance. Second, there\u2019s typically no control\
    \ exerted on the information left outside the selection. We revise the overall\
    \ co-operative framework to address these challenges. We introduce an introspective\
    \ model which explicitly predicts and incorporates the outcome into the selection\
    \ process. Moreover, we explicitly control the rationale complement via an adversary\
    \ so as not to leave any useful information out of the selection. We show that\
    \ the two complementary mechanisms maintain both high predictive accuracy and\
    \ lead to comprehensive rationales."
  address: Hong Kong, China
  attachment:
  - filename: D19-1420.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1420.Attachment.zip
  author:
  - first: Mo
    full: Mo Yu
    id: mo-yu
    last: Yu
  - first: Shiyu
    full: Shiyu Chang
    id: shiyu-chang
    last: Chang
  - first: Yang
    full: Yang Zhang
    id: yang-zhang
    last: Zhang
  - first: Tommi
    full: Tommi Jaakkola
    id: tommi-jaakkola
    last: Jaakkola
  author_string: Mo Yu, Shiyu Chang, Yang Zhang, Tommi Jaakkola
  bibkey: yu-etal-2019-rethinking
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1420
  month: November
  page_first: '4094'
  page_last: '4103'
  pages: "4094\u20134103"
  paper_id: '420'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1420.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1420.jpg
  title: 'Rethinking Cooperative Rationalization: Introspective Extraction and Complement
    Control'
  title_html: 'Rethinking Cooperative Rationalization: Introspective Extraction and
    Complement Control'
  url: https://www.aclweb.org/anthology/D19-1420
  year: '2019'
D19-1421:
  abstract: 'Neural language models are usually trained using Maximum-Likelihood Estimation
    (MLE). The corresponding objective function for MLE is derived from the Kullback-Leibler
    (KL) divergence between the empirical probability distribution representing the
    data and the parametric probability distribution output by the model. However,
    the word frequency discrepancies in natural language make performance extremely
    uneven: while the perplexity is usually very low for frequent words, it is especially
    difficult to predict rare words. In this paper, we experiment with several families
    (alpha, beta and gamma) of power divergences, generalized from the KL divergence,
    for learning language models with an objective different than standard MLE. Intuitively,
    these divergences should affect the way the probability mass is spread during
    learning, notably by prioritizing performances on high or low-frequency words.
    In addition, we implement and experiment with various sampling-based objectives,
    where the computation of the output layer is only done on a small subset of the
    vocabulary. They are derived as power generalizations of a softmax approximated
    via Importance Sampling, and Noise Contrastive Estimation, for accelerated learning.
    Our experiments on the Penn Treebank and Wikitext-2 show that these power divergences
    can indeed be used to prioritize learning on the frequent or rare words, and lead
    to general performance improvements in the case of sampling-based learning.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1421.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1421.Attachment.pdf
  author:
  - first: Matthieu
    full: Matthieu Labeau
    id: matthieu-labeau
    last: Labeau
  - first: Shay B.
    full: Shay B. Cohen
    id: shay-b-cohen
    last: Cohen
  author_string: Matthieu Labeau, Shay B. Cohen
  bibkey: labeau-cohen-2019-experimenting
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1421
  month: November
  page_first: '4104'
  page_last: '4114'
  pages: "4104\u20134114"
  paper_id: '421'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1421.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1421.jpg
  title: Experimenting with Power Divergences for Language Modeling
  title_html: Experimenting with Power Divergences for Language Modeling
  url: https://www.aclweb.org/anthology/D19-1421
  year: '2019'
D19-1422:
  abstract: CRF has been used as a powerful model for statistical sequence labeling.
    For neural sequence labeling, however, BiLSTM-CRF does not always lead to better
    results compared with BiLSTM-softmax local classification. This can be because
    the simple Markov label transition model of CRF does not give much information
    gain over strong neural encoding. For better representing label sequences, we
    investigate a hierarchically-refined label attention network, which explicitly
    leverages label embeddings and captures potential long-term label dependency by
    giving each word incrementally refined label distributions with hierarchical attention.
    Results on POS tagging, NER and CCG supertagging show that the proposed model
    not only improves the overall tagging accuracy with similar number of parameters,
    but also significantly speeds up the training and testing compared to BiLSTM-CRF.
  address: Hong Kong, China
  attachment:
  - filename: D19-1422.Attachment.rar
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1422.Attachment.rar
  author:
  - first: Leyang
    full: Leyang Cui
    id: leyang-cui
    last: Cui
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  author_string: Leyang Cui, Yue Zhang
  bibkey: cui-zhang-2019-hierarchically
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1422
  month: November
  page_first: '4115'
  page_last: '4128'
  pages: "4115\u20134128"
  paper_id: '422'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1422.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1422.jpg
  title: Hierarchically-Refined Label Attention Network for Sequence Labeling
  title_html: Hierarchically-Refined Label Attention Network for Sequence Labeling
  url: https://www.aclweb.org/anthology/D19-1422
  year: '2019'
D19-1423:
  abstract: "State-of-the-art NLP models can often be fooled by adversaries that apply\
    \ seemingly innocuous label-preserving transformations (e.g., paraphrasing) to\
    \ input text. The number of possible transformations scales exponentially with\
    \ text length, so data augmentation cannot cover all transformations of an input.\
    \ This paper considers one exponentially large family of label-preserving transformations,\
    \ in which every word in the input can be replaced with a similar word. We train\
    \ the first models that are provably robust to all word substitutions in this\
    \ family. Our training procedure uses Interval Bound Propagation (IBP) to minimize\
    \ an upper bound on the worst-case loss that any combination of word substitutions\
    \ can induce. To evaluate models\u2019 robustness to these transformations, we\
    \ measure accuracy on adversarially chosen word substitutions applied to test\
    \ examples. Our IBP-trained models attain 75% adversarial accuracy on both sentiment\
    \ analysis on IMDB and natural language inference on SNLI; in comparison, on IMDB,\
    \ models trained normally and ones trained with data augmentation achieve adversarial\
    \ accuracy of only 12% and 41%, respectively."
  address: Hong Kong, China
  attachment:
  - filename: D19-1423.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1423.Attachment.zip
  author:
  - first: Robin
    full: Robin Jia
    id: robin-jia
    last: Jia
  - first: Aditi
    full: Aditi Raghunathan
    id: aditi-raghunathan
    last: Raghunathan
  - first: Kerem
    full: "Kerem G\xF6ksel"
    id: kerem-goksel
    last: "G\xF6ksel"
  - first: Percy
    full: Percy Liang
    id: percy-liang
    last: Liang
  author_string: "Robin Jia, Aditi Raghunathan, Kerem G\xF6ksel, Percy Liang"
  bibkey: jia-etal-2019-certified
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1423
  month: November
  page_first: '4129'
  page_last: '4142'
  pages: "4129\u20134142"
  paper_id: '423'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1423.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1423.jpg
  title: Certified Robustness to Adversarial Word Substitutions
  title_html: Certified Robustness to Adversarial Word Substitutions
  url: https://www.aclweb.org/anthology/D19-1423
  year: '2019'
D19-1424:
  abstract: Language model pre-training, such as BERT, has achieved remarkable results
    in many NLP tasks. However, it is unclear why the pre-training-then-fine-tuning
    paradigm can improve performance and generalization capability across different
    tasks. In this paper, we propose to visualize loss landscapes and optimization
    trajectories of fine-tuning BERT on specific datasets. First, we find that pre-training
    reaches a good initial point across downstream tasks, which leads to wider optima
    and easier optimization compared with training from scratch. We also demonstrate
    that the fine-tuning procedure is robust to overfitting, even though BERT is highly
    over-parameterized for downstream tasks. Second, the visualization results indicate
    that fine-tuning BERT tends to generalize better because of the flat and wide
    optima, and the consistency between the training loss surface and the generalization
    error surface. Third, the lower layers of BERT are more invariant during fine-tuning,
    which suggests that the layers that are close to input learn more transferable
    representations of language.
  address: Hong Kong, China
  author:
  - first: Yaru
    full: Yaru Hao
    id: yaru-hao
    last: Hao
  - first: Li
    full: Li Dong
    id: li-dong
    last: Dong
  - first: Furu
    full: Furu Wei
    id: furu-wei
    last: Wei
  - first: Ke
    full: Ke Xu
    id: ke-xu
    last: Xu
  author_string: Yaru Hao, Li Dong, Furu Wei, Ke Xu
  bibkey: hao-etal-2019-visualizing
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1424
  month: November
  page_first: '4143'
  page_last: '4152'
  pages: "4143\u20134152"
  paper_id: '424'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1424.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1424.jpg
  title: Visualizing and Understanding the Effectiveness of BERT
  title_html: Visualizing and Understanding the Effectiveness of <span class="acl-fixed-case">BERT</span>
  url: https://www.aclweb.org/anthology/D19-1424
  year: '2019'
D19-1425:
  abstract: "Despite impressive performance on many text classification tasks, deep\
    \ neural networks tend to learn frequent superficial patterns that are specific\
    \ to the training data and do not always generalize well. In this work, we observe\
    \ this limitation with respect to the task of native language identification.\
    \ We find that standard text classifiers which perform well on the test set end\
    \ up learning topical features which are confounds of the prediction task (e.g.,\
    \ if the input text mentions Sweden, the classifier predicts that the author\u2019\
    s native language is Swedish). We propose a method that represents the latent\
    \ topical confounds and a model which \u201Cunlearns\u201D confounding features\
    \ by predicting both the label of the input text and the confound; but we train\
    \ the two predictors adversarially in an alternating fashion to learn a text representation\
    \ that predicts the correct label but is less prone to using information about\
    \ the confound. We show that this model generalizes better and learns features\
    \ that are indicative of the writing style rather than the content."
  address: Hong Kong, China
  author:
  - first: Sachin
    full: Sachin Kumar
    id: sachin-kumar
    last: Kumar
  - first: Shuly
    full: Shuly Wintner
    id: shuly-wintner
    last: Wintner
  - first: Noah A.
    full: Noah A. Smith
    id: noah-a-smith
    last: Smith
  - first: Yulia
    full: Yulia Tsvetkov
    id: yulia-tsvetkov
    last: Tsvetkov
  author_string: Sachin Kumar, Shuly Wintner, Noah A. Smith, Yulia Tsvetkov
  bibkey: kumar-etal-2019-topics
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1425
  month: November
  page_first: '4153'
  page_last: '4163'
  pages: "4153\u20134163"
  paper_id: '425'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1425.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1425.jpg
  title: 'Topics to Avoid: Demoting Latent Confounds in Text Classification'
  title_html: 'Topics to Avoid: Demoting Latent Confounds in Text Classification'
  url: https://www.aclweb.org/anthology/D19-1425
  year: '2019'
D19-1426:
  abstract: "Natural language has recently been explored as a new medium of supervision\
    \ for training machine learning models. Here, we explore learning classification\
    \ tasks using language in a conversational setting \u2013 where the automated\
    \ learner does not simply receive language input from a teacher, but can proactively\
    \ engage the teacher by asking questions. We present a reinforcement learning\
    \ framework, where the learner\u2019s actions correspond to question types and\
    \ the reward for asking a question is based on how the teacher\u2019s response\
    \ changes performance of the resulting machine learning model on the learning\
    \ task. In this framework, learning good question-asking strategies corresponds\
    \ to asking sequences of questions that maximize the cumulative (discounted) reward,\
    \ and hence quickly lead to effective classifiers. Empirical analysis across three\
    \ domains shows that learned question-asking strategies expedite classifier training\
    \ by asking appropriate questions at different points in the learning process.\
    \ The approach allows learning classifiers from a blend of strategies, including\
    \ learning from observations, explanations and clarifications."
  address: Hong Kong, China
  author:
  - first: Shashank
    full: Shashank Srivastava
    id: shashank-srivastava
    last: Srivastava
  - first: Igor
    full: Igor Labutov
    id: igor-labutov
    last: Labutov
  - first: Tom
    full: Tom Mitchell
    id: tom-mitchell
    last: Mitchell
  author_string: Shashank Srivastava, Igor Labutov, Tom Mitchell
  bibkey: srivastava-etal-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1426
  month: November
  page_first: '4164'
  page_last: '4174'
  pages: "4164\u20134174"
  paper_id: '426'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1426.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1426.jpg
  title: Learning to Ask for Conversational Machine Learning
  title_html: Learning to Ask for Conversational Machine Learning
  url: https://www.aclweb.org/anthology/D19-1426
  year: '2019'
D19-1427:
  abstract: 'We focus on the problem of language modeling for code-switched language,
    in the context of automatic speech recognition (ASR). Language modeling for code-switched
    language is challenging for (at least) three reasons: (1) lack of available large-scale
    code-switched data for training; (2) lack of a replicable evaluation setup that
    is ASR directed yet isolates language modeling performance from the other intricacies
    of the ASR system; and (3) the reliance on generative modeling. We tackle these
    three issues: we propose an ASR-motivated evaluation setup which is decoupled
    from an ASR system and the choice of vocabulary, and provide an evaluation dataset
    for English-Spanish code-switching. This setup lends itself to a discriminative
    training approach, which we demonstrate to work better than generative language
    modeling. Finally, we explore a variety of training protocols and verify the effectiveness
    of training with large amounts of monolingual data followed by fine-tuning with
    small amounts of code-switched data, for both the generative and discriminative
    cases.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1427.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1427.Attachment.zip
  author:
  - first: Hila
    full: Hila Gonen
    id: hila-gonen
    last: Gonen
  - first: Yoav
    full: Yoav Goldberg
    id: yoav-goldberg
    last: Goldberg
  author_string: Hila Gonen, Yoav Goldberg
  bibkey: gonen-goldberg-2019-language
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1427
  month: November
  page_first: '4175'
  page_last: '4185'
  pages: "4175\u20134185"
  paper_id: '427'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1427.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1427.jpg
  title: 'Language Modeling for Code-Switching: Evaluation, Integration of Monolingual
    Data, and Discriminative Training'
  title_html: 'Language Modeling for Code-Switching: Evaluation, Integration of Monolingual
    Data, and Discriminative Training'
  url: https://www.aclweb.org/anthology/D19-1427
  year: '2019'
D19-1428:
  abstract: Query-based open-domain NLP tasks require information synthesis from long
    and diverse web results. Current approaches extractively select portions of web
    text as input to Sequence-to-Sequence models using methods such as TF-IDF ranking.
    We propose constructing a local graph structured knowledge base for each query,
    which compresses the web search information and reduces redundancy. We show that
    by linearizing the graph into a structured input sequence, models can encode the
    graph representations within a standard Sequence-to-Sequence setting. For two
    generative tasks with very long text input, long-form question answering and multi-document
    summarization, feeding graph representations as input can achieve better performance
    than using retrieved text portions.
  address: Hong Kong, China
  attachment:
  - filename: D19-1428.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1428.Attachment.zip
  author:
  - first: Angela
    full: Angela Fan
    id: angela-fan
    last: Fan
  - first: Claire
    full: Claire Gardent
    id: claire-gardent
    last: Gardent
  - first: "Chlo\xE9"
    full: "Chlo\xE9 Braud"
    id: chloe-braud
    last: Braud
  - first: Antoine
    full: Antoine Bordes
    id: antoine-bordes
    last: Bordes
  author_string: "Angela Fan, Claire Gardent, Chlo\xE9 Braud, Antoine Bordes"
  bibkey: fan-etal-2019-using
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1428
  month: November
  page_first: '4186'
  page_last: '4196'
  pages: "4186\u20134196"
  paper_id: '428'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1428.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1428.jpg
  title: Using Local Knowledge Graph Construction to Scale Seq2Seq Models to Multi-Document
    Inputs
  title_html: Using Local Knowledge Graph Construction to Scale <span class="acl-fixed-case">S</span>eq2<span
    class="acl-fixed-case">S</span>eq Models to Multi-Document Inputs
  url: https://www.aclweb.org/anthology/D19-1428
  year: '2019'
D19-1429:
  abstract: In sequence labeling, previous domain adaptation methods focus on the
    adaptation from the source domain to the entire target domain without considering
    the diversity of individual target domain samples, which may lead to negative
    transfer results for certain samples. Besides, an important characteristic of
    sequence labeling tasks is that different elements within a given sample may also
    have diverse domain relevance, which requires further consideration. To take the
    multi-level domain relevance discrepancy into account, in this paper, we propose
    a fine-grained knowledge fusion model with the domain relevance modeling scheme
    to control the balance between learning from the target domain data and learning
    from the source domain model. Experiments on three sequence labeling tasks show
    that our fine-grained knowledge fusion model outperforms strong baselines and
    other state-of-the-art sequence labeling domain adaptation methods.
  address: Hong Kong, China
  author:
  - first: Huiyun
    full: Huiyun Yang
    id: huiyun-yang
    last: Yang
  - first: Shujian
    full: Shujian Huang
    id: shujian-huang
    last: Huang
  - first: Xin-Yu
    full: Xin-Yu Dai
    id: xinyu-dai
    last: Dai
  - first: Jiajun
    full: Jiajun Chen
    id: jiajun-chen
    last: Chen
  author_string: Huiyun Yang, Shujian Huang, Xin-Yu Dai, Jiajun Chen
  bibkey: yang-etal-2019-fine
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1429
  month: November
  page_first: '4197'
  page_last: '4206'
  pages: "4197\u20134206"
  paper_id: '429'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1429.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1429.jpg
  title: Fine-grained Knowledge Fusion for Sequence Labeling Domain Adaptation
  title_html: Fine-grained Knowledge Fusion for Sequence Labeling Domain Adaptation
  url: https://www.aclweb.org/anthology/D19-1429
  year: '2019'
D19-1430:
  abstract: "While target-side monolingual data has been proven to be very useful\
    \ to improve neural machine translation (briefly, NMT) through back translation,\
    \ source-side monolingual data is not well investigated. In this work, we study\
    \ how to use both the source-side and target-side monolingual data for NMT, and\
    \ propose an effective strategy leveraging both of them. First, we generate synthetic\
    \ bitext by translating monolingual data from the two domains into the other domain\
    \ using the models pretrained on genuine bitext. Next, a model is trained on a\
    \ noised version of the concatenated synthetic bitext where each source sequence\
    \ is randomly corrupted. Finally, the model is fine-tuned on the genuine bitext\
    \ and a clean version of a subset of the synthetic bitext without adding any noise.\
    \ Our approach achieves state-of-the-art results on WMT16, WMT17, WMT18 English\u2194\
    German translations and WMT19 GermanGerman translations and WMT19 German\u2192\
    French translations, which demonstrate the effectiveness of our method. We also\
    \ conduct a comprehensive study on how each part in the pipeline works.French\
    \ translations, which demonstrate the effectiveness of our method. We also conduct\
    \ a comprehensive study on how each part in the pipeline works."
  address: Hong Kong, China
  attachment:
  - filename: D19-1430.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1430.Attachment.zip
  author:
  - first: Lijun
    full: Lijun Wu
    id: lijun-wu
    last: Wu
  - first: Yiren
    full: Yiren Wang
    id: yiren-wang
    last: Wang
  - first: Yingce
    full: Yingce Xia
    id: yingce-xia
    last: Xia
  - first: Tao
    full: Tao Qin
    id: tao-qin
    last: Qin
  - first: Jianhuang
    full: Jianhuang Lai
    id: jianhuang-lai
    last: Lai
  - first: Tie-Yan
    full: Tie-Yan Liu
    id: tie-yan-liu
    last: Liu
  author_string: Lijun Wu, Yiren Wang, Yingce Xia, Tao Qin, Jianhuang Lai, Tie-Yan
    Liu
  bibkey: wu-etal-2019-exploiting
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1430
  month: November
  page_first: '4207'
  page_last: '4216'
  pages: "4207\u20134216"
  paper_id: '430'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1430.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1430.jpg
  title: Exploiting Monolingual Data at Scale for Neural Machine Translation
  title_html: Exploiting Monolingual Data at Scale for Neural Machine Translation
  url: https://www.aclweb.org/anthology/D19-1430
  year: '2019'
D19-1431:
  abstract: Link prediction is an important way to complete knowledge graphs (KGs),
    while embedding-based methods, effective for link prediction in KGs, perform poorly
    on relations that only have a few associative triples. In this work, we propose
    a Meta Relational Learning (MetaR) framework to do the common but challenging
    few-shot link prediction in KGs, namely predicting new triples about a relation
    by only observing a few associative triples. We solve few-shot link prediction
    by focusing on transferring relation-specific meta information to make model learn
    the most important knowledge and learn faster, corresponding to relation meta
    and gradient meta respectively in MetaR. Empirically, our model achieves state-of-the-art
    results on few-shot link prediction KG benchmarks.
  address: Hong Kong, China
  author:
  - first: Mingyang
    full: Mingyang Chen
    id: mingyang-chen
    last: Chen
  - first: Wen
    full: Wen Zhang
    id: wen-zhang
    last: Zhang
  - first: Wei
    full: Wei Zhang
    id: wei-zhang
    last: Zhang
  - first: Qiang
    full: Qiang Chen
    id: qiang-chen
    last: Chen
  - first: Huajun
    full: Huajun Chen
    id: huajun-chen
    last: Chen
  author_string: Mingyang Chen, Wen Zhang, Wei Zhang, Qiang Chen, Huajun Chen
  bibkey: chen-etal-2019-meta
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1431
  month: November
  page_first: '4217'
  page_last: '4226'
  pages: "4217\u20134226"
  paper_id: '431'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1431.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1431.jpg
  title: Meta Relational Learning for Few-Shot Link Prediction in Knowledge Graphs
  title_html: Meta Relational Learning for Few-Shot Link Prediction in Knowledge Graphs
  url: https://www.aclweb.org/anthology/D19-1431
  year: '2019'
D19-1432:
  abstract: Language models are generally trained on data spanning a wide range of
    topics (e.g., news, reviews, fiction), but they might be applied to an a priori
    unknown target distribution (e.g., restaurant reviews). In this paper, we first
    show that training on text outside the test distribution can degrade test performance
    when using standard maximum likelihood (MLE) training. To remedy this without
    the knowledge of the test distribution, we propose an approach which trains a
    model that performs well over a wide range of potential test distributions. In
    particular, we derive a new distributionally robust optimization (DRO) procedure
    which minimizes the loss of the model over the worst-case mixture of topics with
    sufficient overlap with the training distribution. Our approach, called topic
    conditional value at risk (topic CVaR), obtains a 5.5 point perplexity reduction
    over MLE when the language models are trained on a mixture of Yelp reviews and
    news and tested only on reviews.
  address: Hong Kong, China
  author:
  - first: Yonatan
    full: Yonatan Oren
    id: yonatan-oren
    last: Oren
  - first: Shiori
    full: Shiori Sagawa
    id: shiori-sagawa
    last: Sagawa
  - first: Tatsunori
    full: Tatsunori Hashimoto
    id: tatsunori-hashimoto
    last: Hashimoto
  - first: Percy
    full: Percy Liang
    id: percy-liang
    last: Liang
  author_string: Yonatan Oren, Shiori Sagawa, Tatsunori Hashimoto, Percy Liang
  bibkey: oren-etal-2019-distributionally
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1432
  month: November
  page_first: '4227'
  page_last: '4237'
  pages: "4227\u20134237"
  paper_id: '432'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1432.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1432.jpg
  title: Distributionally Robust Language Modeling
  title_html: Distributionally Robust Language Modeling
  url: https://www.aclweb.org/anthology/D19-1432
  year: '2019'
D19-1433:
  abstract: 'Contextualized word embeddings such as ELMo and BERT provide a foundation
    for strong performance across a wide range of natural language processing tasks
    by pretraining on large corpora of unlabeled text. However, the applicability
    of this approach is unknown when the target domain varies substantially from the
    pretraining corpus. We are specifically interested in the scenario in which labeled
    data is available in only a canonical source domain such as newstext, and the
    target domain is distinct from both the labeled and pretraining texts. To address
    this scenario, we propose domain-adaptive fine-tuning, in which the contextualized
    embeddings are adapted by masked language modeling on text from the target domain.
    We test this approach on sequence labeling in two challenging domains: Early Modern
    English and Twitter. Both domains differ substantially from existing pretraining
    corpora, and domain-adaptive fine-tuning yields substantial improvements over
    strong BERT baselines, with particularly impressive results on out-of-vocabulary
    words. We conclude that domain-adaptive fine-tuning offers a simple and effective
    approach for the unsupervised adaptation of sequence labeling to difficult new
    domains.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1433.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1433.Attachment.zip
  author:
  - first: Xiaochuang
    full: Xiaochuang Han
    id: xiaochuang-han
    last: Han
  - first: Jacob
    full: Jacob Eisenstein
    id: jacob-eisenstein
    last: Eisenstein
  author_string: Xiaochuang Han, Jacob Eisenstein
  bibkey: han-eisenstein-2019-unsupervised
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1433
  month: November
  page_first: '4238'
  page_last: '4248'
  pages: "4238\u20134248"
  paper_id: '433'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1433.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1433.jpg
  title: Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence
    Labeling
  title_html: Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence
    Labeling
  url: https://www.aclweb.org/anthology/D19-1433
  year: '2019'
D19-1434:
  abstract: Incorporating Item Response Theory (IRT) into NLP tasks can provide valuable
    information about model performance and behavior. Traditionally, IRT models are
    learned using human response pattern (RP) data, presenting a significant bottleneck
    for large data sets like those required for training deep neural networks (DNNs).
    In this work we propose learning IRT models using RPs generated from artificial
    crowds of DNN models. We demonstrate the effectiveness of learning IRT models
    using DNN-generated data through quantitative and qualitative analyses for two
    NLP tasks. Parameters learned from human and machine RPs for natural language
    inference and sentiment analysis exhibit medium to large positive correlations.
    We demonstrate a use-case for latent difficulty item parameters, namely training
    set filtering, and show that using difficulty to sample training data outperforms
    baseline methods. Finally, we highlight cases where human expectation about item
    difficulty does not match difficulty as estimated from the machine RPs.
  address: Hong Kong, China
  attachment:
  - filename: D19-1434.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1434.Attachment.zip
  author:
  - first: John P.
    full: John P. Lalor
    id: john-p-lalor
    last: Lalor
  - first: Hao
    full: Hao Wu
    id: hao-wu
    last: Wu
  - first: Hong
    full: Hong Yu
    id: hong-yu
    last: Yu
  author_string: John P. Lalor, Hao Wu, Hong Yu
  bibkey: lalor-etal-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1434
  month: November
  page_first: '4249'
  page_last: '4259'
  pages: "4249\u20134259"
  paper_id: '434'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1434.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1434.jpg
  title: 'Learning Latent Parameters without Human Response Patterns: Item Response
    Theory with Artificial Crowds'
  title_html: 'Learning Latent Parameters without Human Response Patterns: Item Response
    Theory with Artificial Crowds'
  url: https://www.aclweb.org/anthology/D19-1434
  year: '2019'
D19-1435:
  abstract: 'We present a Parallel Iterative Edit (PIE) model for the problem of local
    sequence transduction arising in tasks like Grammatical error correction (GEC).
    Recent approaches are based on the popular encoder-decoder (ED) model for sequence
    to sequence learning. The ED model auto-regressively captures full dependency
    among output tokens but is slow due to sequential decoding. The PIE model does
    parallel decoding, giving up the advantage of modeling full dependency in the
    output, yet it achieves accuracy competitive with the ED model for four reasons:
    1. predicting edits instead of tokens, 2. labeling sequences instead of generating
    sequences, 3. iteratively refining predictions to capture dependencies, and 4.
    factorizing logits over edits and their token argument to harness pre-trained
    language models like BERT. Experiments on tasks spanning GEC, OCR correction and
    spell correction demonstrate that the PIE model is an accurate and significantly
    faster alternative for local sequence transduction.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1435.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1435.Attachment.pdf
  author:
  - first: Abhijeet
    full: Abhijeet Awasthi
    id: abhijeet-awasthi
    last: Awasthi
  - first: Sunita
    full: Sunita Sarawagi
    id: sunita-sarawagi
    last: Sarawagi
  - first: Rasna
    full: Rasna Goyal
    id: rasna-goyal
    last: Goyal
  - first: Sabyasachi
    full: Sabyasachi Ghosh
    id: sabyasachi-ghosh
    last: Ghosh
  - first: Vihari
    full: Vihari Piratla
    id: vihari-piratla
    last: Piratla
  author_string: Abhijeet Awasthi, Sunita Sarawagi, Rasna Goyal, Sabyasachi Ghosh,
    Vihari Piratla
  bibkey: awasthi-etal-2019-parallel
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1435
  month: November
  page_first: '4260'
  page_last: '4270'
  pages: "4260\u20134270"
  paper_id: '435'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1435.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1435.jpg
  title: Parallel Iterative Edit Models for Local Sequence Transduction
  title_html: Parallel Iterative Edit Models for Local Sequence Transduction
  url: https://www.aclweb.org/anthology/D19-1435
  year: '2019'
D19-1436:
  abstract: "Most of the existing generative adversarial networks (GAN) for text generation\
    \ suffer from the instability of reinforcement learning training algorithms such\
    \ as policy gradient, leading to unstable performance. To tackle this problem,\
    \ we propose a novel framework called Adversarial Reward Augmented Maximum Likelihood\
    \ (ARAML). During adversarial training, the discriminator assigns rewards to samples\
    \ which are acquired from a stationary distribution near the data rather than\
    \ the generator\u2019s distribution. The generator is optimized with maximum likelihood\
    \ estimation augmented by the discriminator\u2019s rewards instead of policy gradient.\
    \ Experiments show that our model can outperform state-of-the-art text GANs with\
    \ a more stable training process."
  address: Hong Kong, China
  author:
  - first: Pei
    full: Pei Ke
    id: pei-ke
    last: Ke
  - first: Fei
    full: Fei Huang
    id: fei-huang
    last: Huang
  - first: Minlie
    full: Minlie Huang
    id: minlie-huang
    last: Huang
  - first: Xiaoyan
    full: Xiaoyan Zhu
    id: xiaoyan-zhu
    last: Zhu
  author_string: Pei Ke, Fei Huang, Minlie Huang, Xiaoyan Zhu
  bibkey: ke-etal-2019-araml
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1436
  month: November
  page_first: '4271'
  page_last: '4281'
  pages: "4271\u20134281"
  paper_id: '436'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1436.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1436.jpg
  title: 'ARAML: A Stable Adversarial Training Framework for Text Generation'
  title_html: '<span class="acl-fixed-case">ARAML</span>: A Stable Adversarial Training
    Framework for Text Generation'
  url: https://www.aclweb.org/anthology/D19-1436
  year: '2019'
D19-1437:
  abstract: Most sequence-to-sequence (seq2seq) models are autoregressive; they generate
    each token by conditioning on previously generated tokens. In contrast, non-autoregressive
    seq2seq models generate all tokens in one pass, which leads to increased efficiency
    through parallel processing on hardware such as GPUs. However, directly modeling
    the joint distribution of all tokens simultaneously is challenging, and even with
    increasingly complex model structures accuracy lags significantly behind autoregressive
    models. In this paper, we propose a simple, efficient, and effective model for
    non-autoregressive sequence generation using latent variable models. Specifically,
    we turn to generative flow, an elegant technique to model complex distributions
    using neural networks, and design several layers of flow tailored for modeling
    the conditional density of sequential latent variables. We evaluate this model
    on three neural machine translation (NMT) benchmark datasets, achieving comparable
    performance with state-of-the-art non-autoregressive NMT models and almost constant
    decoding time w.r.t the sequence length.
  address: Hong Kong, China
  attachment:
  - filename: D19-1437.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1437.Attachment.zip
  author:
  - first: Xuezhe
    full: Xuezhe Ma
    id: xuezhe-ma
    last: Ma
  - first: Chunting
    full: Chunting Zhou
    id: chunting-zhou
    last: Zhou
  - first: Xian
    full: Xian Li
    id: xian-li
    last: Li
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  - first: Eduard
    full: Eduard Hovy
    id: eduard-hovy
    last: Hovy
  author_string: Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neubig, Eduard Hovy
  bibkey: ma-etal-2019-flowseq
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1437
  month: November
  page_first: '4282'
  page_last: '4292'
  pages: "4282\u20134292"
  paper_id: '437'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1437.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1437.jpg
  title: 'FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative
    Flow'
  title_html: '<span class="acl-fixed-case">F</span>low<span class="acl-fixed-case">S</span>eq:
    Non-Autoregressive Conditional Sequence Generation with Generative Flow'
  url: https://www.aclweb.org/anthology/D19-1437
  year: '2019'
D19-1438:
  abstract: Compositional generalization is a basic mechanism in human language learning,
    but current neural networks lack such ability. In this paper, we conduct fundamental
    research for encoding compositionality in neural networks. Conventional methods
    use a single representation for the input sentence, making it hard to apply prior
    knowledge of compositionality. In contrast, our approach leverages such knowledge
    with two representations, one generating attention maps, and the other mapping
    attended input words to output symbols. We reduce the entropy in each representation
    to improve generalization. Our experiments demonstrate significant improvements
    over the conventional methods in five NLP tasks including instruction learning
    and machine translation. In the SCAN domain, it boosts accuracies from 14.0% to
    98.8% in Jump task, and from 92.0% to 99.7% in TurnLeft task. It also beats human
    performance on a few-shot learning task. We hope the proposed approach can help
    ease future research towards human-level compositional language learning.
  address: Hong Kong, China
  attachment:
  - filename: D19-1438.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1438.Attachment.zip
  author:
  - first: Yuanpeng
    full: Yuanpeng Li
    id: yuanpeng-li
    last: Li
  - first: Liang
    full: Liang Zhao
    id: liang-zhao
    last: Zhao
  - first: Jianyu
    full: Jianyu Wang
    id: jianyu-wang
    last: Wang
  - first: Joel
    full: Joel Hestness
    id: joel-hestness
    last: Hestness
  author_string: Yuanpeng Li, Liang Zhao, Jianyu Wang, Joel Hestness
  bibkey: li-etal-2019-compositional
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1438
  month: November
  page_first: '4293'
  page_last: '4302'
  pages: "4293\u20134302"
  paper_id: '438'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1438.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1438.jpg
  title: Compositional Generalization for Primitive Substitutions
  title_html: Compositional Generalization for Primitive Substitutions
  url: https://www.aclweb.org/anthology/D19-1438
  year: '2019'
D19-1439:
  abstract: Pronoun resolution is a major area of natural language understanding.
    However, large-scale training sets are still scarce, since manually labelling
    data is costly. In this work, we introduce WikiCREM (Wikipedia CoREferences Masked)
    a large-scale, yet accurate dataset of pronoun disambiguation instances. We use
    a language-model-based approach for pronoun resolution in combination with our
    WikiCREM dataset. We compare a series of models on a collection of diverse and
    challenging coreference resolution problems, where we match or outperform previous
    state-of-the-art approaches on 6 out of 7 datasets, such as GAP, DPR, WNLI, PDP,
    WinoBias, and WinoGender. We release our model to be used off-the-shelf for solving
    pronoun disambiguation.
  address: Hong Kong, China
  attachment:
  - filename: D19-1439.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1439.Attachment.zip
  author:
  - first: Vid
    full: Vid Kocijan
    id: vid-kocijan
    last: Kocijan
  - first: Oana-Maria
    full: Oana-Maria Camburu
    id: oana-maria-camburu
    last: Camburu
  - first: Ana-Maria
    full: Ana-Maria Cretu
    id: ana-maria-cretu
    last: Cretu
  - first: Yordan
    full: Yordan Yordanov
    id: yordan-yordanov
    last: Yordanov
  - first: Phil
    full: Phil Blunsom
    id: phil-blunsom
    last: Blunsom
  - first: Thomas
    full: Thomas Lukasiewicz
    id: thomas-lukasiewicz
    last: Lukasiewicz
  author_string: Vid Kocijan, Oana-Maria Camburu, Ana-Maria Cretu, Yordan Yordanov,
    Phil Blunsom, Thomas Lukasiewicz
  bibkey: kocijan-etal-2019-wikicrem
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1439
  month: November
  page_first: '4303'
  page_last: '4312'
  pages: "4303\u20134312"
  paper_id: '439'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1439.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1439.jpg
  title: 'WikiCREM: A Large Unsupervised Corpus for Coreference Resolution'
  title_html: '<span class="acl-fixed-case">W</span>iki<span class="acl-fixed-case">CREM</span>:
    A Large Unsupervised Corpus for Coreference Resolution'
  url: https://www.aclweb.org/anthology/D19-1439
  year: '2019'
D19-1440:
  abstract: 'Identifying what is at the center of the meaning of a word and what discriminates
    it from other words is a fundamental natural language inference task. This paper
    describes an explicit word vector representation model (WVM) to support the identification
    of discriminative attributes. A core contribution of the paper is a quantitative
    and qualitative comparative analysis of different types of data sources and Knowledge
    Bases in the construction of explainable and explicit WVMs: (i) knowledge graphs
    built from dictionary definitions, (ii) entity-attribute-relationships graphs
    derived from images and (iii) commonsense knowledge graphs. Using a detailed quantitative
    and qualitative analysis, we demonstrate that these data sources have complementary
    semantic aspects, supporting the creation of explicit semantic vector spaces.
    The explicit vector spaces are evaluated using the task of discriminative attribute
    identification, showing comparable performance to the state-of-the-art systems
    in the task (F1-score = 0.69), while delivering full model transparency and explainability.'
  address: Hong Kong, China
  author:
  - first: Armins
    full: Armins Stepanjans
    id: armins-stepanjans
    last: Stepanjans
  - first: "Andr\xE9"
    full: "Andr\xE9 Freitas"
    id: andre-freitas
    last: Freitas
  author_string: "Armins Stepanjans, Andr\xE9 Freitas"
  bibkey: stepanjans-freitas-2019-identifying
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1440
  month: November
  page_first: '4313'
  page_last: '4322'
  pages: "4313\u20134322"
  paper_id: '440'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1440.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1440.jpg
  title: Identifying and Explaining Discriminative Attributes
  title_html: Identifying and Explaining Discriminative Attributes
  url: https://www.aclweb.org/anthology/D19-1440
  year: '2019'
D19-1441:
  abstract: "Pre-trained language models such as BERT have proven to be highly effective\
    \ for natural language processing (NLP) tasks. However, the high demand for computing\
    \ resources in training such models hinders their application in practice. In\
    \ order to alleviate this resource hunger in large-scale model training, we propose\
    \ a Patient Knowledge Distillation approach to compress an original large model\
    \ (teacher) into an equally-effective lightweight shallow network (student). Different\
    \ from previous knowledge distillation methods, which only use the output from\
    \ the last layer of the teacher network for distillation, our student model patiently\
    \ learns from multiple intermediate layers of the teacher model for incremental\
    \ knowledge extraction, following two strategies: (i) PKD-Last: learning from\
    \ the last k layers; and (ii) PKD-Skip: learning from every k layers. These two\
    \ patient distillation schemes enable the exploitation of rich information in\
    \ the teacher\u2019s hidden layers, and encourage the student model to patiently\
    \ learn from and imitate the teacher through a multi-layer distillation process.\
    \ Empirically, this translates into improved results on multiple NLP tasks with\
    \ a significant gain in training efficiency, without sacrificing model accuracy."
  address: Hong Kong, China
  author:
  - first: Siqi
    full: Siqi Sun
    id: siqi-sun
    last: Sun
  - first: Yu
    full: Yu Cheng
    id: yu-cheng
    last: Cheng
  - first: Zhe
    full: Zhe Gan
    id: zhe-gan
    last: Gan
  - first: Jingjing
    full: Jingjing Liu
    id: jingjing-liu
    last: Liu
  author_string: Siqi Sun, Yu Cheng, Zhe Gan, Jingjing Liu
  bibkey: sun-etal-2019-patient
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1441
  month: November
  page_first: '4323'
  page_last: '4332'
  pages: "4323\u20134332"
  paper_id: '441'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1441.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1441.jpg
  title: Patient Knowledge Distillation for BERT Model Compression
  title_html: Patient Knowledge Distillation for <span class="acl-fixed-case">BERT</span>
    Model Compression
  url: https://www.aclweb.org/anthology/D19-1441
  year: '2019'
D19-1442:
  abstract: Variational language models seek to estimate the posterior of latent variables
    with an approximated variational posterior. The model often assumes the variational
    posterior to be factorized even when the true posterior is not. The learned variational
    posterior under this assumption does not capture the dependency relationships
    over latent variables. We argue that this would cause a typical training problem
    called posterior collapse observed in all other variational language models. We
    propose Gaussian Copula Variational Autoencoder (VAE) to avert this problem. Copula
    is widely used to model correlation and dependencies of high-dimensional random
    variables, and therefore it is helpful to maintain the dependency relationships
    that are lost in VAE. The empirical results show that by modeling the correlation
    of latent variables explicitly using a neural parametric copula, we can avert
    this training difficulty while getting competitive results among all other VAE
    approaches.
  address: Hong Kong, China
  author:
  - first: Prince Zizhuang
    full: Prince Zizhuang Wang
    id: prince-zizhuang-wang
    last: Wang
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  author_string: Prince Zizhuang Wang, William Yang Wang
  bibkey: wang-wang-2019-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1442
  month: November
  page_first: '4333'
  page_last: '4343'
  pages: "4333\u20134343"
  paper_id: '442'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1442.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1442.jpg
  title: Neural Gaussian Copula for Variational Autoencoder
  title_html: Neural <span class="acl-fixed-case">G</span>aussian Copula for Variational
    Autoencoder
  url: https://www.aclweb.org/anthology/D19-1442
  year: '2019'
D19-1443:
  abstract: "Transformer is a powerful architecture that achieves superior performance\
    \ on various sequence learning tasks, including neural machine translation, language\
    \ understanding, and sequence prediction. At the core of the Transformer is the\
    \ attention mechanism, which concurrently processes all inputs in the streams.\
    \ In this paper, we present a new formulation of attention via the lens of the\
    \ kernel. To be more precise, we realize that the attention can be seen as applying\
    \ kernel smoother over the inputs with the kernel scores being the similarities\
    \ between inputs. This new formulation gives us a better way to understand individual\
    \ components of the Transformer\u2019s attention, such as the better way to integrate\
    \ the positional embedding. Another important advantage of our kernel-based formulation\
    \ is that it paves the way to a larger space of composing Transformer\u2019s attention.\
    \ As an example, we propose a new variant of Transformer\u2019s attention which\
    \ models the input as a product of symmetric kernels. This approach achieves competitive\
    \ performance to the current state of the art model with less computation. In\
    \ our experiments, we empirically study different kernel construction strategies\
    \ on two widely used tasks: neural machine translation and sequence prediction."
  address: Hong Kong, China
  author:
  - first: Yao-Hung Hubert
    full: Yao-Hung Hubert Tsai
    id: yao-hung-hubert-tsai
    last: Tsai
  - first: Shaojie
    full: Shaojie Bai
    id: shaojie-bai
    last: Bai
  - first: Makoto
    full: Makoto Yamada
    id: makoto-yamada
    last: Yamada
  - first: Louis-Philippe
    full: Louis-Philippe Morency
    id: louis-philippe-morency
    last: Morency
  - first: Ruslan
    full: Ruslan Salakhutdinov
    id: ruslan-salakhutdinov
    last: Salakhutdinov
  author_string: Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe
    Morency, Ruslan Salakhutdinov
  bibkey: tsai-etal-2019-transformer
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1443
  month: November
  page_first: '4344'
  page_last: '4353'
  pages: "4344\u20134353"
  paper_id: '443'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1443.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1443.jpg
  title: "Transformer Dissection: An Unified Understanding for Transformer\u2019s\
    \ Attention via the Lens of Kernel"
  title_html: "Transformer Dissection: An Unified Understanding for Transformer\u2019\
    s Attention via the Lens of Kernel"
  url: https://www.aclweb.org/anthology/D19-1443
  year: '2019'
D19-1444:
  abstract: Many tasks in natural language processing can be viewed as multi-label
    classification problems. However, most of the existing models are trained with
    the standard cross-entropy loss function and use a fixed prediction policy (e.g.,
    a threshold of 0.5) for all the labels, which completely ignores the complexity
    and dependencies among different labels. In this paper, we propose a meta-learning
    method to capture these complex label dependencies. More specifically, our method
    utilizes a meta-learner to jointly learn the training policies and prediction
    policies for different labels. The training policies are then used to train the
    classifier with the cross-entropy loss function, and the prediction policies are
    further implemented for prediction. Experimental results on fine-grained entity
    typing and text classification demonstrate that our proposed method can obtain
    more accurate multi-label classification results.
  address: Hong Kong, China
  author:
  - first: Jiawei
    full: Jiawei Wu
    id: jiawei-wu
    last: Wu
  - first: Wenhan
    full: Wenhan Xiong
    id: wenhan-xiong
    last: Xiong
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  author_string: Jiawei Wu, Wenhan Xiong, William Yang Wang
  bibkey: wu-etal-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1444
  month: November
  page_first: '4354'
  page_last: '4364'
  pages: "4354\u20134364"
  paper_id: '444'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1444.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1444.jpg
  title: 'Learning to Learn and Predict: A Meta-Learning Approach for Multi-Label
    Classification'
  title_html: 'Learning to Learn and Predict: A Meta-Learning Approach for Multi-Label
    Classification'
  url: https://www.aclweb.org/anthology/D19-1444
  year: '2019'
D19-1445:
  abstract: "BERT-based architectures currently give state-of-the-art performance\
    \ on many NLP tasks, but little is known about the exact mechanisms that contribute\
    \ to its success. In the current work, we focus on the interpretation of self-attention,\
    \ which is one of the fundamental underlying components of BERT. Using a subset\
    \ of GLUE tasks and a set of handcrafted features-of-interest, we propose the\
    \ methodology and carry out a qualitative and quantitative analysis of the information\
    \ encoded by the individual BERT\u2019s heads. Our findings suggest that there\
    \ is a limited set of attention patterns that are repeated across different heads,\
    \ indicating the overall model overparametrization. While different heads consistently\
    \ use the same attention patterns, they have varying impact on performance across\
    \ different tasks. We show that manually disabling attention in certain heads\
    \ leads to a performance improvement over the regular fine-tuned BERT models."
  address: Hong Kong, China
  attachment:
  - filename: D19-1445.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1445.Attachment.zip
  author:
  - first: Olga
    full: Olga Kovaleva
    id: olga-kovaleva
    last: Kovaleva
  - first: Alexey
    full: Alexey Romanov
    id: alexey-romanov
    last: Romanov
  - first: Anna
    full: Anna Rogers
    id: anna-rogers
    last: Rogers
  - first: Anna
    full: Anna Rumshisky
    id: anna-rumshisky
    last: Rumshisky
  author_string: Olga Kovaleva, Alexey Romanov, Anna Rogers, Anna Rumshisky
  bibkey: kovaleva-etal-2019-revealing
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1445
  month: November
  page_first: '4365'
  page_last: '4374'
  pages: "4365\u20134374"
  paper_id: '445'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1445.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1445.jpg
  title: Revealing the Dark Secrets of BERT
  title_html: Revealing the Dark Secrets of <span class="acl-fixed-case">BERT</span>
  url: https://www.aclweb.org/anthology/D19-1445
  year: '2019'
D19-1446:
  abstract: "Neural machine translation, which achieves near human-level performance\
    \ in some languages, strongly relies on the large amounts of parallel sentences,\
    \ which hinders its applicability to low-resource language pairs. Recent works\
    \ explore the possibility of unsupervised machine translation with monolingual\
    \ data only, leading to much lower accuracy compared with the supervised one.\
    \ Observing that weakly paired bilingual documents are much easier to collect\
    \ than bilingual sentences, e.g., from Wikipedia, news websites or books, in this\
    \ paper, we investigate training translation models with weakly paired bilingual\
    \ documents. Our approach contains two components. 1) We provide a simple approach\
    \ to mine implicitly bilingual sentence pairs from document pairs which can then\
    \ be used as supervised training signals. 2) We leverage the topic consistency\
    \ of two weakly paired documents and learn the sentence translation model by constraining\
    \ the word distribution-level alignments. We evaluate our method on weakly paired\
    \ documents from Wikipedia on six tasks, the widely used WMT16 German\u2194English,\
    \ WMT13 SpanishEnglish, WMT13 Spanish\u2194English and WMT16 RomanianEnglish and\
    \ WMT16 Romanian\u2194English translation tasks. We obtain 24.1/30.3, 28.1/27.6\
    \ and 30.1/27.6 BLEU points separately, outperforming previous results by more\
    \ than 5 BLEU points in each direction and reducing the gap between unsupervised\
    \ translation and supervised translation up to 50%.English translation tasks.\
    \ We obtain 24.1/30.3, 28.1/27.6 and 30.1/27.6 BLEU points separately, outperforming\
    \ previous results by more than 5 BLEU points in each direction and reducing the\
    \ gap between unsupervised translation and supervised translation up to 50%."
  address: Hong Kong, China
  author:
  - first: Lijun
    full: Lijun Wu
    id: lijun-wu
    last: Wu
  - first: Jinhua
    full: Jinhua Zhu
    id: jinhua-zhu
    last: Zhu
  - first: Di
    full: Di He
    id: di-he
    last: He
  - first: Fei
    full: Fei Gao
    id: fei-gao
    last: Gao
  - first: Tao
    full: Tao Qin
    id: tao-qin
    last: Qin
  - first: Jianhuang
    full: Jianhuang Lai
    id: jianhuang-lai
    last: Lai
  - first: Tie-Yan
    full: Tie-Yan Liu
    id: tie-yan-liu
    last: Liu
  author_string: Lijun Wu, Jinhua Zhu, Di He, Fei Gao, Tao Qin, Jianhuang Lai, Tie-Yan
    Liu
  bibkey: wu-etal-2019-machine
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1446
  month: November
  page_first: '4375'
  page_last: '4384'
  pages: "4375\u20134384"
  paper_id: '446'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1446.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1446.jpg
  title: Machine Translation With Weakly Paired Documents
  title_html: Machine Translation With Weakly Paired Documents
  url: https://www.aclweb.org/anthology/D19-1446
  year: '2019'
D19-1447:
  abstract: 'Emergent multi-agent communication protocols are very different from
    natural language and not easily interpretable by humans. We find that agents that
    were initially pretrained to produce natural language can also experience detrimental
    language drift: when a non-linguistic reward is used in a goal-based task, e.g.
    some scalar success metric, the communication protocol may easily and radically
    diverge from natural language. We recast translation as a multi-agent communication
    game and examine auxiliary training constraints for their effectiveness in mitigating
    language drift. We show that a combination of syntactic (language model likelihood)
    and semantic (visual grounding) constraints gives the best communication performance,
    allowing pre-trained agents to retain English syntax while learning to accurately
    convey the intended meaning.'
  address: Hong Kong, China
  author:
  - first: Jason
    full: Jason Lee
    id: jason-lee
    last: Lee
  - first: Kyunghyun
    full: Kyunghyun Cho
    id: kyunghyun-cho
    last: Cho
  - first: Douwe
    full: Douwe Kiela
    id: douwe-kiela
    last: Kiela
  author_string: Jason Lee, Kyunghyun Cho, Douwe Kiela
  bibkey: lee-etal-2019-countering
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1447
  month: November
  page_first: '4385'
  page_last: '4395'
  pages: "4385\u20134395"
  paper_id: '447'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1447.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1447.jpg
  title: Countering Language Drift via Visual Grounding
  title_html: Countering Language Drift via Visual Grounding
  url: https://www.aclweb.org/anthology/D19-1447
  year: '2019'
D19-1448:
  abstract: We seek to understand how the representations of individual tokens and
    the structure of the learned feature space evolve between layers in deep neural
    networks under different learning objectives. We chose the Transformers for our
    analysis as they have been shown effective with various tasks, including machine
    translation (MT), standard left-to-right language models (LM) and masked language
    modeling (MLM). Previous work used black-box probing tasks to show that the representations
    learned by the Transformer differ significantly depending on the objective. In
    this work, we use canonical correlation analysis and mutual information estimators
    to study how information flows across Transformer layers and observe that the
    choice of the objective determines this process. For example, as you go from bottom
    to top layers, information about the past in left-to-right language models gets
    vanished and predictions about the future get formed. In contrast, for MLM, representations
    initially acquire information about the context around the token, partially forgetting
    the token identity and producing a more generalized token representation. The
    token identity then gets recreated at the top MLM layers.
  address: Hong Kong, China
  attachment:
  - filename: D19-1448.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1448.Attachment.zip
  author:
  - first: Elena
    full: Elena Voita
    id: elena-voita
    last: Voita
  - first: Rico
    full: Rico Sennrich
    id: rico-sennrich
    last: Sennrich
  - first: Ivan
    full: Ivan Titov
    id: ivan-titov
    last: Titov
  author_string: Elena Voita, Rico Sennrich, Ivan Titov
  bibkey: voita-etal-2019-bottom
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1448
  month: November
  page_first: '4396'
  page_last: '4406'
  pages: "4396\u20134406"
  paper_id: '448'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1448.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1448.jpg
  title: 'The Bottom-up Evolution of Representations in the Transformer: A Study with
    Machine Translation and Language Modeling Objectives'
  title_html: 'The Bottom-up Evolution of Representations in the Transformer: A Study
    with Machine Translation and Language Modeling Objectives'
  url: https://www.aclweb.org/anthology/D19-1448
  year: '2019'
D19-1449:
  abstract: Recent efforts in cross-lingual word embedding (CLWE) learning have predominantly
    focused on fully unsupervised approaches that project monolingual embeddings into
    a shared cross-lingual space without any cross-lingual signal. The lack of any
    supervision makes such approaches conceptually attractive. Yet, their only core
    difference from (weakly) supervised projection-based CLWE methods is in the way
    they obtain a seed dictionary used to initialize an iterative self-learning procedure.
    The fully unsupervised methods have arguably become more robust, and their primary
    use case is CLWE induction for pairs of resource-poor and distant languages. In
    this paper, we question the ability of even the most robust unsupervised CLWE
    approaches to induce meaningful CLWEs in these more challenging settings. A series
    of bilingual lexicon induction (BLI) experiments with 15 diverse languages (210
    language pairs) show that fully unsupervised CLWE methods still fail for a large
    number of language pairs (e.g., they yield zero BLI performance for 87/210 pairs).
    Even when they succeed, they never surpass the performance of weakly supervised
    methods (seeded with 500-1,000 translation pairs) using the same self-learning
    procedure in any BLI setup, and the gaps are often substantial. These findings
    call for revisiting the main motivations behind fully unsupervised CLWE methods.
  address: Hong Kong, China
  attachment:
  - filename: D19-1449.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1449.Attachment.zip
  author:
  - first: Ivan
    full: "Ivan Vuli\u0107"
    id: ivan-vulic
    last: "Vuli\u0107"
  - first: Goran
    full: "Goran Glava\u0161"
    id: goran-glavas
    last: "Glava\u0161"
  - first: Roi
    full: Roi Reichart
    id: roi-reichart
    last: Reichart
  - first: Anna
    full: Anna Korhonen
    id: anna-korhonen
    last: Korhonen
  author_string: "Ivan Vuli\u0107, Goran Glava\u0161, Roi Reichart, Anna Korhonen"
  bibkey: vulic-etal-2019-really
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1449
  month: November
  page_first: '4407'
  page_last: '4418'
  pages: "4407\u20134418"
  paper_id: '449'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1449.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1449.jpg
  title: Do We Really Need Fully Unsupervised Cross-Lingual Embeddings?
  title_html: Do We Really Need Fully Unsupervised Cross-Lingual Embeddings?
  url: https://www.aclweb.org/anthology/D19-1449
  year: '2019'
D19-1450:
  abstract: Distributed representations of words which map each word to a continuous
    vector have proven useful in capturing important linguistic information not only
    in a single language but also across different languages. Current unsupervised
    adversarial approaches show that it is possible to build a mapping matrix that
    aligns two sets of monolingual word embeddings without high quality parallel data,
    such as a dictionary or a sentence-aligned corpus. However, without an additional
    step of refinement, the preliminary mapping learnt by these methods is unsatisfactory,
    leading to poor performance for typologically distant languages. In this paper,
    we propose a weakly-supervised adversarial training method to overcome this limitation,
    based on the intuition that mapping across languages is better done at the concept
    level than at the word level. We propose a concept-based adversarial training
    method which improves the performance of previous unsupervised adversarial methods
    for most languages, and especially for typologically distant language pairs.
  address: Hong Kong, China
  author:
  - first: Haozhou
    full: Haozhou Wang
    id: haozhou-wang
    last: Wang
  - first: James
    full: James Henderson
    id: james-henderson
    last: Henderson
  - first: Paola
    full: Paola Merlo
    id: paola-merlo
    last: Merlo
  author_string: Haozhou Wang, James Henderson, Paola Merlo
  bibkey: wang-etal-2019-weakly
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1450
  month: November
  page_first: '4419'
  page_last: '4430'
  pages: "4419\u20134430"
  paper_id: '450'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1450.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1450.jpg
  title: Weakly-Supervised Concept-based Adversarial Learning for Cross-lingual Word
    Embeddings
  title_html: Weakly-Supervised Concept-based Adversarial Learning for Cross-lingual
    Word Embeddings
  url: https://www.aclweb.org/anthology/D19-1450
  year: '2019'
D19-1451:
  abstract: Multilingual knowledge graphs (KGs), such as YAGO and DBpedia, represent
    entities in different languages. The task of cross-lingual entity alignment is
    to match entities in a source language with their counterparts in target languages.
    In this work, we investigate embedding-based approaches to encode entities from
    multilingual KGs into the same vector space, where equivalent entities are close
    to each other. Specifically, we apply graph convolutional networks (GCNs) to combine
    multi-aspect information of entities, including topological connections, relations,
    and attributes of entities, to learn entity embeddings. To exploit the literal
    descriptions of entities expressed in different languages, we propose two uses
    of a pretrained multilingual BERT model to bridge cross-lingual gaps. We further
    propose two strategies to integrate GCN-based and BERT-based modules to boost
    performance. Extensive experiments on two benchmark datasets demonstrate that
    our method significantly outperforms existing systems.
  address: Hong Kong, China
  author:
  - first: Hsiu-Wei
    full: Hsiu-Wei Yang
    id: hsiu-wei-yang
    last: Yang
  - first: Yanyan
    full: Yanyan Zou
    id: yanyan-zou
    last: Zou
  - first: Peng
    full: Peng Shi
    id: peng-shi
    last: Shi
  - first: Wei
    full: Wei Lu
    id: wei-lu
    last: Lu
  - first: Jimmy
    full: Jimmy Lin
    id: jimmy-lin
    last: Lin
  - first: Xu
    full: Xu Sun
    id: xu-sun
    last: Sun
  author_string: Hsiu-Wei Yang, Yanyan Zou, Peng Shi, Wei Lu, Jimmy Lin, Xu Sun
  bibkey: yang-etal-2019-aligning
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1451
  month: November
  page_first: '4431'
  page_last: '4441'
  pages: "4431\u20134441"
  paper_id: '451'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1451.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1451.jpg
  title: Aligning Cross-Lingual Entities with Multi-Aspect Information
  title_html: Aligning Cross-Lingual Entities with Multi-Aspect Information
  url: https://www.aclweb.org/anthology/D19-1451
  year: '2019'
D19-1452:
  abstract: We study cross-lingual stance detection, which aims to leverage labeled
    data in one language to identify the relative perspective (or stance) of a given
    document with respect to a claim in a different target language. In particular,
    we introduce a novel contrastive language adaptation approach applied to memory
    networks, which ensures accurate alignment of stances in the source and target
    languages, and can effectively deal with the challenge of limited labeled data
    in the target language. The evaluation results on public benchmark datasets and
    comparison against current state-of-the-art approaches demonstrate the effectiveness
    of our approach.
  address: Hong Kong, China
  author:
  - first: Mitra
    full: Mitra Mohtarami
    id: mitra-mohtarami
    last: Mohtarami
  - first: James
    full: James Glass
    id: james-glass
    last: Glass
  - first: Preslav
    full: Preslav Nakov
    id: preslav-nakov
    last: Nakov
  author_string: Mitra Mohtarami, James Glass, Preslav Nakov
  bibkey: mohtarami-etal-2019-contrastive
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1452
  month: November
  page_first: '4442'
  page_last: '4452'
  pages: "4442\u20134452"
  paper_id: '452'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1452.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1452.jpg
  title: Contrastive Language Adaptation for Cross-Lingual Stance Detection
  title_html: Contrastive Language Adaptation for Cross-Lingual Stance Detection
  url: https://www.aclweb.org/anthology/D19-1452
  year: '2019'
D19-1453:
  abstract: The state of the art in machine translation (MT) is governed by neural
    approaches, which typically provide superior translation accuracy over statistical
    approaches. However, on the closely related task of word alignment, traditional
    statistical word alignment models often remain the go-to solution. In this paper,
    we present an approach to train a Transformer model to produce both accurate translations
    and alignments. We extract discrete alignments from the attention probabilities
    learnt during regular neural machine translation model training and leverage them
    in a multi-task framework to optimize towards translation and alignment objectives.
    We demonstrate that our approach produces competitive results compared to GIZA++
    trained IBM alignment models without sacrificing translation accuracy and outperforms
    previous attempts on Transformer model based word alignment. Finally, by incorporating
    IBM model alignments into our multi-task training, we report significantly better
    alignment accuracies compared to GIZA++ on three publicly available data sets.
  address: Hong Kong, China
  attachment:
  - filename: D19-1453.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1453.Attachment.zip
  author:
  - first: Sarthak
    full: Sarthak Garg
    id: sarthak-garg
    last: Garg
  - first: Stephan
    full: Stephan Peitz
    id: stephan-peitz
    last: Peitz
  - first: Udhyakumar
    full: Udhyakumar Nallasamy
    id: udhyakumar-nallasamy
    last: Nallasamy
  - first: Matthias
    full: Matthias Paulik
    id: matthias-paulik
    last: Paulik
  author_string: Sarthak Garg, Stephan Peitz, Udhyakumar Nallasamy, Matthias Paulik
  bibkey: garg-etal-2019-jointly
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1453
  month: November
  page_first: '4453'
  page_last: '4462'
  pages: "4453\u20134462"
  paper_id: '453'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1453.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1453.jpg
  title: Jointly Learning to Align and Translate with Transformer Models
  title_html: Jointly Learning to Align and Translate with Transformer Models
  url: https://www.aclweb.org/anthology/D19-1453
  year: '2019'
D19-1454:
  abstract: "We introduce Social IQa, the first large-scale benchmark for commonsense\
    \ reasoning about social situations. Social IQa contains 38,000 multiple choice\
    \ questions for probing emotional and social intelligence in a variety of everyday\
    \ situations (e.g., Q: \u201CJordan wanted to tell Tracy a secret, so Jordan leaned\
    \ towards Tracy. Why did Jordan do this?\u201D A: \u201CMake sure no one else\
    \ could hear\u201D). Through crowdsourcing, we collect commonsense questions along\
    \ with correct and incorrect answers about social interactions, using a new framework\
    \ that mitigates stylistic artifacts in incorrect answers by asking workers to\
    \ provide the right answer to a different but related question. Empirical results\
    \ show that our benchmark is challenging for existing question-answering models\
    \ based on pretrained language models, compared to human performance (>20% gap).\
    \ Notably, we further establish Social IQa as a resource for transfer learning\
    \ of commonsense knowledge, achieving state-of-the-art performance on multiple\
    \ commonsense reasoning tasks (Winograd Schemas, COPA)."
  address: Hong Kong, China
  author:
  - first: Maarten
    full: Maarten Sap
    id: maarten-sap
    last: Sap
  - first: Hannah
    full: Hannah Rashkin
    id: hannah-rashkin
    last: Rashkin
  - first: Derek
    full: Derek Chen
    id: derek-chen
    last: Chen
  - first: Ronan
    full: Ronan Le Bras
    id: ronan-le-bras
    last: Le Bras
  - first: Yejin
    full: Yejin Choi
    id: yejin-choi
    last: Choi
  author_string: Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, Yejin Choi
  bibkey: sap-etal-2019-social
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1454
  month: November
  page_first: '4463'
  page_last: '4473'
  pages: "4463\u20134473"
  paper_id: '454'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1454.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1454.jpg
  title: 'Social IQa: Commonsense Reasoning about Social Interactions'
  title_html: 'Social <span class="acl-fixed-case">IQ</span>a: Commonsense Reasoning
    about Social Interactions'
  url: https://www.aclweb.org/anthology/D19-1454
  year: '2019'
D19-1455:
  abstract: Multi-hop QA requires a model to connect multiple pieces of evidence scattered
    in a long context to answer the question. The recently proposed HotpotQA (Yang
    et al., 2018) dataset is comprised of questions embodying four different multi-hop
    reasoning paradigms (two bridge entity setups, checking multiple properties, and
    comparing two entities), making it challenging for a single neural network to
    handle all four. In this work, we present an interpretable, controller-based Self-Assembling
    Neural Modular Network (Hu et al., 2017, 2018) for multi-hop reasoning, where
    we design four novel modules (Find, Relocate, Compare, NoOp) to perform unique
    types of language reasoning. Based on a question, our layout controller RNN dynamically
    infers a series of reasoning modules to construct the entire network. Empirically,
    we show that our dynamic, multi-hop modular network achieves significant improvements
    over the static, single-hop baseline (on both regular and adversarial evaluation).
    We further demonstrate the interpretability of our model via three analyses. First,
    the controller can softly decompose the multi-hop question into multiple single-hop
    sub-questions to promote compositional reasoning behavior of the main network.
    Second, the controller can predict layouts that conform to the layouts designed
    by human experts. Finally, the intermediate module can infer the entity that connects
    two distantly-located supporting facts by addressing the sub-question from the
    controller.
  address: Hong Kong, China
  author:
  - first: Yichen
    full: Yichen Jiang
    id: yichen-jiang
    last: Jiang
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  author_string: Yichen Jiang, Mohit Bansal
  bibkey: jiang-bansal-2019-self
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1455
  month: November
  page_first: '4474'
  page_last: '4484'
  pages: "4474\u20134484"
  paper_id: '455'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1455.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1455.jpg
  title: Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning
  title_html: Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning
  url: https://www.aclweb.org/anthology/D19-1455
  year: '2019'
D19-1456:
  abstract: "Deep learning models for semantics are generally evaluated using naturalistic\
    \ corpora. Adversarial testing methods, in which models are evaluated on new examples\
    \ with known semantic properties, have begun to reveal that good performance at\
    \ these naturalistic tasks can hide serious shortcomings. However, we should insist\
    \ that these evaluations be fair \u2013 that the models are given data sufficient\
    \ to support the requisite kinds of generalization. In this paper, we define and\
    \ motivate a formal notion of fairness in this sense. We then apply these ideas\
    \ to natural language inference by constructing very challenging but provably\
    \ fair artificial datasets and showing that standard neural models fail to generalize\
    \ in the required ways; only task-specific models that jointly compose the premise\
    \ and hypothesis are able to achieve high performance, and even these models do\
    \ not solve the task perfectly."
  address: Hong Kong, China
  attachment:
  - filename: D19-1456.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1456.Attachment.zip
  author:
  - first: Atticus
    full: Atticus Geiger
    id: atticus-geiger
    last: Geiger
  - first: Ignacio
    full: Ignacio Cases
    id: ignacio-cases
    last: Cases
  - first: Lauri
    full: Lauri Karttunen
    id: lauri-karttunen
    last: Karttunen
  - first: Christopher
    full: Christopher Potts
    id: christopher-potts
    last: Potts
  author_string: Atticus Geiger, Ignacio Cases, Lauri Karttunen, Christopher Potts
  bibkey: geiger-etal-2019-posing
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1456
  month: November
  page_first: '4485'
  page_last: '4495'
  pages: "4485\u20134495"
  paper_id: '456'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1456.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1456.jpg
  title: Posing Fair Generalization Tasks for Natural Language Inference
  title_html: Posing Fair Generalization Tasks for Natural Language Inference
  url: https://www.aclweb.org/anthology/D19-1456
  year: '2019'
D19-1457:
  abstract: "Our goal is to better comprehend procedural text, e.g., a paragraph about\
    \ photosynthesis, by not only predicting what happens, but *why* some actions\
    \ need to happen before others. Our approach builds on a prior process comprehension\
    \ framework for predicting actions\u2019 effects, to also identify subsequent\
    \ steps that those effects enable. We present our new model (XPAD) that biases\
    \ effect predictions towards those that (1) explain more of the actions in the\
    \ paragraph and (2) are more plausible with respect to background knowledge. We\
    \ also extend an existing benchmark dataset for procedural text comprehension,\
    \ ProPara, by adding the new task of explaining actions by predicting their dependencies.\
    \ We find that XPAD significantly outperforms prior systems on this task, while\
    \ maintaining the performance on the original task in ProPara. The dataset is\
    \ available at http://data.allenai.org/propara"
  address: Hong Kong, China
  author:
  - first: Bhavana
    full: Bhavana Dalvi
    id: bhavana-dalvi
    last: Dalvi
  - first: Niket
    full: Niket Tandon
    id: niket-tandon
    last: Tandon
  - first: Antoine
    full: Antoine Bosselut
    id: antoine-bosselut
    last: Bosselut
  - first: Wen-tau
    full: Wen-tau Yih
    id: wen-tau-yih
    last: Yih
  - first: Peter
    full: Peter Clark
    id: peter-clark
    last: Clark
  author_string: Bhavana Dalvi, Niket Tandon, Antoine Bosselut, Wen-tau Yih, Peter
    Clark
  bibkey: dalvi-etal-2019-everything
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1457
  month: November
  page_first: '4496'
  page_last: '4505'
  pages: "4496\u20134505"
  paper_id: '457'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1457.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1457.jpg
  title: 'Everything Happens for a Reason: Discovering the Purpose of Actions in Procedural
    Text'
  title_html: 'Everything Happens for a Reason: Discovering the Purpose of Actions
    in Procedural Text'
  url: https://www.aclweb.org/anthology/D19-1457
  year: '2019'
D19-1458:
  abstract: "The recent success of natural language understanding (NLU) systems has\
    \ been troubled by results highlighting the failure of these models to generalize\
    \ in a systematic and robust way. In this work, we introduce a diagnostic benchmark\
    \ suite, named CLUTRR, to clarify some key issues related to the robustness and\
    \ systematicity of NLU systems. Motivated by the classic work on inductive logic\
    \ programming, CLUTRR requires that an NLU system infer kinship relations between\
    \ characters in short stories. Successful performance on this task requires both\
    \ extracting relationships between entities, as well as inferring the logical\
    \ rules governing these relationships. CLUTRR allows us to precisely measure a\
    \ model\u2019s ability for systematic generalization by evaluating on held-out\
    \ combinations of logical rules, and allows us to evaluate a model\u2019s robustness\
    \ by adding curated noise facts. Our empirical results highlight a substantial\
    \ performance gap between state-of-the-art NLU models (e.g., BERT and MAC) and\
    \ a graph neural network model that works directly with symbolic inputs\u2014\
    with the graph-based model exhibiting both stronger generalization and greater\
    \ robustness."
  address: Hong Kong, China
  attachment:
  - filename: D19-1458.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1458.Attachment.zip
  author:
  - first: Koustuv
    full: Koustuv Sinha
    id: koustuv-sinha
    last: Sinha
  - first: Shagun
    full: Shagun Sodhani
    id: shagun-sodhani
    last: Sodhani
  - first: Jin
    full: Jin Dong
    id: jin-dong
    last: Dong
  - first: Joelle
    full: Joelle Pineau
    id: joelle-pineau
    last: Pineau
  - first: William L.
    full: William L. Hamilton
    id: william-l-hamilton
    last: Hamilton
  author_string: Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, William L.
    Hamilton
  bibkey: sinha-etal-2019-clutrr
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1458
  month: November
  page_first: '4506'
  page_last: '4515'
  pages: "4506\u20134515"
  paper_id: '458'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1458.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1458.jpg
  title: 'CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text'
  title_html: '<span class="acl-fixed-case">CLUTRR</span>: A Diagnostic Benchmark
    for Inductive Reasoning from Text'
  url: https://www.aclweb.org/anthology/D19-1458
  year: '2019'
D19-1459:
  abstract: "A significant barrier to progress in data-driven approaches to building\
    \ dialog systems is the lack of high quality, goal-oriented conversational data.\
    \ To help satisfy this elementary requirement, we introduce the initial release\
    \ of the Taskmaster-1 dataset which includes 13,215 task-based dialogs comprising\
    \ six domains. Two procedures were used to create this collection, each with unique\
    \ advantages. The first involves a two-person, spoken \u201CWizard of Oz\u201D\
    \ (WOz) approach in which trained agents and crowdsourced workers interact to\
    \ complete the task while the second is \u201Cself-dialog\u201D in which crowdsourced\
    \ workers write the entire dialog themselves. We do not restrict the workers to\
    \ detailed scripts or to a small knowledge base and hence we observe that our\
    \ dataset contains more realistic and diverse conversations in comparison to existing\
    \ datasets. We offer several baseline models including state of the art neural\
    \ seq2seq architectures with benchmark performance as well as qualitative human\
    \ evaluations. Dialogs are labeled with API calls and arguments, a simple and\
    \ cost effective approach which avoids the requirement of complex annotation schema.\
    \ The layer of abstraction between the dialog model and the service provider API\
    \ allows for a given model to interact with multiple services that provide similar\
    \ functionally. Finally, the dataset will evoke interest in written vs. spoken\
    \ language, discourse patterns, error handling and other linguistic phenomena\
    \ related to dialog system research, development and design."
  address: Hong Kong, China
  author:
  - first: Bill
    full: Bill Byrne
    id: bill-byrne
    last: Byrne
  - first: Karthik
    full: Karthik Krishnamoorthi
    id: karthik-krishnamoorthi
    last: Krishnamoorthi
  - first: Chinnadhurai
    full: Chinnadhurai Sankar
    id: chinnadhurai-sankar
    last: Sankar
  - first: Arvind
    full: Arvind Neelakantan
    id: arvind-neelakantan
    last: Neelakantan
  - first: Ben
    full: Ben Goodrich
    id: ben-goodrich
    last: Goodrich
  - first: Daniel
    full: Daniel Duckworth
    id: daniel-duckworth
    last: Duckworth
  - first: Semih
    full: Semih Yavuz
    id: semih-yavuz
    last: Yavuz
  - first: Amit
    full: Amit Dubey
    id: amit-dubey
    last: Dubey
  - first: Kyu-Young
    full: Kyu-Young Kim
    id: kyu-young-kim
    last: Kim
  - first: Andy
    full: Andy Cedilnik
    id: andy-cedilnik
    last: Cedilnik
  author_string: Bill Byrne, Karthik Krishnamoorthi, Chinnadhurai Sankar, Arvind Neelakantan,
    Ben Goodrich, Daniel Duckworth, Semih Yavuz, Amit Dubey, Kyu-Young Kim, Andy Cedilnik
  bibkey: byrne-etal-2019-taskmaster
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1459
  month: November
  page_first: '4516'
  page_last: '4525'
  pages: "4516\u20134525"
  paper_id: '459'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1459.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1459.jpg
  title: 'Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset'
  title_html: 'Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset'
  url: https://www.aclweb.org/anthology/D19-1459
  year: '2019'
D19-1460:
  abstract: "The need for high-quality, large-scale, goal-oriented dialogue datasets\
    \ continues to grow as virtual assistants become increasingly wide-spread. However,\
    \ publicly available datasets useful for this area are limited either in their\
    \ size, linguistic diversity, domain coverage, or annotation granularity. In this\
    \ paper, we present strategies toward curating and annotating large scale goal\
    \ oriented dialogue data. We introduce the MultiDoGO dataset to overcome these\
    \ limitations. With a total of over 81K dialogues harvested across six domains,\
    \ MultiDoGO is over 8 times the size of MultiWOZ, the other largest comparable\
    \ dialogue dataset currently available to the public. Over 54K of these harvested\
    \ conversations are annotated for intent classes and slot labels. We adopt a Wizard-of-Oz\
    \ approach wherein a crowd-sourced worker (the \u201Ccustomer\u201D) is paired\
    \ with a trained annotator (the \u201Cagent\u201D). The data curation process\
    \ was controlled via biases to ensure a diversity in dialogue flows following\
    \ variable dialogue policies. We provide distinct class label tags for agents\
    \ vs. customer utterances, along with applicable slot labels. We also compare\
    \ and contrast our strategies on annotation granularity, i.e. turn vs. sentence\
    \ level. Furthermore, we compare and contrast annotations curated by leveraging\
    \ professional annotators vs the crowd. We believe our strategies for eliciting\
    \ and annotating such a dialogue dataset scales across modalities and domains\
    \ and potentially languages in the future. To demonstrate the efficacy of our\
    \ devised strategies we establish neural baselines for classification on the agent\
    \ and customer utterances as well as slot labeling for each domain."
  address: Hong Kong, China
  attachment:
  - filename: D19-1460.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1460.Attachment.pdf
  author:
  - first: Denis
    full: Denis Peskov
    id: denis-peskov
    last: Peskov
  - first: Nancy
    full: Nancy Clarke
    id: nancy-clarke
    last: Clarke
  - first: Jason
    full: Jason Krone
    id: jason-krone
    last: Krone
  - first: Brigi
    full: Brigi Fodor
    id: brigi-fodor
    last: Fodor
  - first: Yi
    full: Yi Zhang
    id: yi-zhang
    last: Zhang
  - first: Adel
    full: Adel Youssef
    id: adel-youssef
    last: Youssef
  - first: Mona
    full: Mona Diab
    id: mona-diab
    last: Diab
  author_string: Denis Peskov, Nancy Clarke, Jason Krone, Brigi Fodor, Yi Zhang, Adel
    Youssef, Mona Diab
  bibkey: peskov-etal-2019-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1460
  month: November
  page_first: '4526'
  page_last: '4536'
  pages: "4526\u20134536"
  paper_id: '460'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1460.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1460.jpg
  title: 'Multi-Domain Goal-Oriented Dialogues (MultiDoGO): Strategies toward Curating
    and Annotating Large Scale Dialogue Data'
  title_html: 'Multi-Domain Goal-Oriented Dialogues (<span class="acl-fixed-case">M</span>ulti<span
    class="acl-fixed-case">D</span>o<span class="acl-fixed-case">GO</span>): Strategies
    toward Curating and Annotating Large Scale Dialogue Data'
  url: https://www.aclweb.org/anthology/D19-1460
  year: '2019'
D19-1461:
  abstract: "The detection of offensive language in the context of a dialogue has\
    \ become an increasingly important application of natural language processing.\
    \ The detection of trolls in public forums (Gal\xE1n-Garc\xEDa et al., 2016),\
    \ and the deployment of chatbots in the public domain (Wolf et al., 2017) are\
    \ two examples that show the necessity of guarding against adversarially offensive\
    \ behavior on the part of humans. In this work, we develop a training scheme for\
    \ a model to become robust to such human attacks by an iterative build it, break\
    \ it, fix it scheme with humans and models in the loop. In detailed experiments\
    \ we show this approach is considerably more robust than previous systems. Further,\
    \ we show that offensive language used within a conversation critically depends\
    \ on the dialogue context, and cannot be viewed as a single sentence offensive\
    \ detection task as in most previous work. Our newly collected tasks and methods\
    \ are all made open source and publicly available."
  address: Hong Kong, China
  attachment:
  - filename: D19-1461.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1461.Attachment.zip
  author:
  - first: Emily
    full: Emily Dinan
    id: emily-dinan
    last: Dinan
  - first: Samuel
    full: Samuel Humeau
    id: samuel-humeau
    last: Humeau
  - first: Bharath
    full: Bharath Chintagunta
    id: bharath-chintagunta
    last: Chintagunta
  - first: Jason
    full: Jason Weston
    id: jason-weston
    last: Weston
  author_string: Emily Dinan, Samuel Humeau, Bharath Chintagunta, Jason Weston
  bibkey: dinan-etal-2019-build
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1461
  month: November
  page_first: '4537'
  page_last: '4546'
  pages: "4537\u20134546"
  paper_id: '461'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1461.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1461.jpg
  title: 'Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial
    Human Attack'
  title_html: 'Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial
    Human Attack'
  url: https://www.aclweb.org/anthology/D19-1461
  year: '2019'
D19-1462:
  abstract: Ellipsis and co-reference are common and ubiquitous especially in multi-turn
    dialogues. In this paper, we treat the resolution of ellipsis and co-reference
    in dialogue as a problem of generating omitted or referred expressions from the
    dialogue context. We therefore propose a unified end-to-end Generative Ellipsis
    and CO-reference Resolution model (GECOR) in the context of dialogue. The model
    can generate a new pragmatically complete user utterance by alternating the generation
    and copy mode for each user utterance. A multi-task learning framework is further
    proposed to integrate the GECOR into an end-to-end task-oriented dialogue. In
    order to train both the GECOR and the multi-task learning framework, we manually
    construct a new dataset on the basis of the public dataset CamRest676 with both
    ellipsis and co-reference annotation. On this dataset, intrinsic evaluations on
    the resolution of ellipsis and co-reference show that the GECOR model significantly
    outperforms the sequence-to-sequence (seq2seq) baseline model in terms of EM,
    BLEU and F1 while extrinsic evaluations on the downstream dialogue task demonstrate
    that our multi-task learning framework with GECOR achieves a higher success rate
    of task completion than TSCP, a state-of-the-art end-to-end task-oriented dialogue
    model.
  address: Hong Kong, China
  author:
  - first: Jun
    full: Jun Quan
    id: jun-quan
    last: Quan
  - first: Deyi
    full: Deyi Xiong
    id: deyi-xiong
    last: Xiong
  - first: Bonnie
    full: Bonnie Webber
    id: bonnie-webber
    last: Webber
  - first: Changjian
    full: Changjian Hu
    id: changjian-hu
    last: Hu
  author_string: Jun Quan, Deyi Xiong, Bonnie Webber, Changjian Hu
  bibkey: quan-etal-2019-gecor
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1462
  month: November
  page_first: '4547'
  page_last: '4557'
  pages: "4547\u20134557"
  paper_id: '462'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1462.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1462.jpg
  title: 'GECOR: An End-to-End Generative Ellipsis and Co-reference Resolution Model
    for Task-Oriented Dialogue'
  title_html: '<span class="acl-fixed-case">GECOR</span>: An End-to-End Generative
    Ellipsis and Co-reference Resolution Model for Task-Oriented Dialogue'
  url: https://www.aclweb.org/anthology/D19-1462
  year: '2019'
D19-1463:
  abstract: How to incorporate external knowledge into a neural dialogue model is
    critically important for dialogue systems to behave like real humans. To handle
    this problem, memory networks are usually a great choice and a promising way.
    However, existing memory networks do not perform well when leveraging heterogeneous
    information from different sources. In this paper, we propose a novel and versatile
    external memory networks called Heterogeneous Memory Networks (HMNs), to simultaneously
    utilize user utterances, dialogue history and background knowledge tuples. In
    our method, historical sequential dialogues are encoded and stored into the context-aware
    memory enhanced by gating mechanism while grounding knowledge tuples are encoded
    and stored into the context-free memory. During decoding, the decoder augmented
    with HMNs recurrently selects each word in one response utterance from these two
    memories and a general vocabulary. Experimental results on multiple real-world
    datasets show that HMNs significantly outperform the state-of-the-art data-driven
    task-oriented dialogue models in most domains.
  address: Hong Kong, China
  author:
  - first: Zehao
    full: Zehao Lin
    id: zehao-lin
    last: Lin
  - first: Xinjing
    full: Xinjing Huang
    id: xinjing-huang
    last: Huang
  - first: Feng
    full: Feng Ji
    id: feng-ji
    last: Ji
  - first: Haiqing
    full: Haiqing Chen
    id: haiqing-chen
    last: Chen
  - first: Yin
    full: Yin Zhang
    id: yin-zhang
    last: Zhang
  author_string: Zehao Lin, Xinjing Huang, Feng Ji, Haiqing Chen, Yin Zhang
  bibkey: lin-etal-2019-task
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1463
  month: November
  page_first: '4558'
  page_last: '4567'
  pages: "4558\u20134567"
  paper_id: '463'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1463.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1463.jpg
  title: Task-Oriented Conversation Generation Using Heterogeneous Memory Networks
  title_html: Task-Oriented Conversation Generation Using Heterogeneous Memory Networks
  url: https://www.aclweb.org/anthology/D19-1463
  year: '2019'
D19-1464:
  abstract: Due to their inherent capability in semantic alignment of aspects and
    their context words, attention mechanism and Convolutional Neural Networks (CNNs)
    are widely applied for aspect-based sentiment classification. However, these models
    lack a mechanism to account for relevant syntactical constraints and long-range
    word dependencies, and hence may mistakenly recognize syntactically irrelevant
    contextual words as clues for judging aspect sentiment. To tackle this problem,
    we propose to build a Graph Convolutional Network (GCN) over the dependency tree
    of a sentence to exploit syntactical information and word dependencies. Based
    on it, a novel aspect-specific sentiment classification framework is raised. Experiments
    on three benchmarking collections illustrate that our proposed model has comparable
    effectiveness to a range of state-of-the-art models, and further demonstrate that
    both syntactical information and long-range word dependencies are properly captured
    by the graph convolution structure.
  address: Hong Kong, China
  author:
  - first: Chen
    full: Chen Zhang
    id: chen-zhang
    last: Zhang
  - first: Qiuchi
    full: Qiuchi Li
    id: qiuchi-li
    last: Li
  - first: Dawei
    full: Dawei Song
    id: dawei-song
    last: Song
  author_string: Chen Zhang, Qiuchi Li, Dawei Song
  bibkey: zhang-etal-2019-aspect
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1464
  month: November
  page_first: '4568'
  page_last: '4578'
  pages: "4568\u20134578"
  paper_id: '464'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1464.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1464.jpg
  title: Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional
    Networks
  title_html: Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional
    Networks
  url: https://www.aclweb.org/anthology/D19-1464
  year: '2019'
D19-1465:
  abstract: Aspect words, indicating opinion targets, are essential in expressing
    and understanding human opinions. To identify aspects, most previous efforts focus
    on using sequence tagging models trained on human-annotated data. This work studies
    unsupervised aspect extraction and explores how words appear in global context
    (on sentence level) and local context (conveyed by neighboring words). We propose
    a novel neural model, capable of coupling global and local representation to discover
    aspect words. Experimental results on two benchmarks, laptop and restaurant reviews,
    show that our model significantly outperforms the state-of-the-art models from
    previous studies evaluated with varying metrics. Analysis on model output show
    our ability to learn meaningful and coherent aspect representations. We further
    investigate how words distribute in global and local context, and find that aspect
    and non-aspect words do exhibit different context, interpreting our superiority
    in unsupervised aspect extraction.
  address: Hong Kong, China
  author:
  - first: Ming
    full: Ming Liao
    id: ming-liao
    last: Liao
  - first: Jing
    full: Jing Li
    id: jing-li
    last: Li
  - first: Haisong
    full: Haisong Zhang
    id: haisong-zhang
    last: Zhang
  - first: Lingzhi
    full: Lingzhi Wang
    id: lingzhi-wang
    last: Wang
  - first: Xixin
    full: Xixin Wu
    id: xixin-wu
    last: Wu
  - first: Kam-Fai
    full: Kam-Fai Wong
    id: kam-fai-wong
    last: Wong
  author_string: Ming Liao, Jing Li, Haisong Zhang, Lingzhi Wang, Xixin Wu, Kam-Fai
    Wong
  bibkey: liao-etal-2019-coupling
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1465
  month: November
  page_first: '4579'
  page_last: '4589'
  pages: "4579\u20134589"
  paper_id: '465'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1465.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1465.jpg
  title: Coupling Global and Local Context for Unsupervised Aspect Extraction
  title_html: Coupling Global and Local Context for Unsupervised Aspect Extraction
  url: https://www.aclweb.org/anthology/D19-1465
  year: '2019'
D19-1466:
  abstract: Joint extraction of aspects and sentiments can be effectively formulated
    as a sequence labeling problem. However, such formulation hinders the effectiveness
    of supervised methods due to the lack of annotated sequence data in many domains.
    To address this issue, we firstly explore an unsupervised domain adaptation setting
    for this task. Prior work can only use common syntactic relations between aspect
    and opinion words to bridge the domain gaps, which highly relies on external linguistic
    resources. To resolve it, we propose a novel Selective Adversarial Learning (SAL)
    method to align the inferred correlation vectors that automatically capture their
    latent relations. The SAL method can dynamically learn an alignment weight for
    each word such that more important words can possess higher alignment weights
    to achieve fine-grained (word-level) adaptation. Empirically, extensive experiments
    demonstrate the effectiveness of the proposed SAL method.
  address: Hong Kong, China
  attachment:
  - filename: D19-1466.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1466.Attachment.pdf
  author:
  - first: Zheng
    full: Zheng Li
    id: zheng-li
    last: Li
  - first: Xin
    full: Xin Li
    id: xin-li
    last: Li
  - first: Ying
    full: Ying Wei
    id: ying-wei
    last: Wei
  - first: Lidong
    full: Lidong Bing
    id: lidong-bing
    last: Bing
  - first: Yu
    full: Yu Zhang
    id: yu-zhang
    last: Zhang
  - first: Qiang
    full: Qiang Yang
    id: qiang-yang
    last: Yang
  author_string: Zheng Li, Xin Li, Ying Wei, Lidong Bing, Yu Zhang, Qiang Yang
  bibkey: li-etal-2019-transferable
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1466
  month: November
  page_first: '4590'
  page_last: '4600'
  pages: "4590\u20134600"
  paper_id: '466'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1466.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1466.jpg
  title: Transferable End-to-End Aspect-based Sentiment Analysis with Selective Adversarial
    Learning
  title_html: Transferable End-to-End Aspect-based Sentiment Analysis with Selective
    Adversarial Learning
  url: https://www.aclweb.org/anthology/D19-1466
  year: '2019'
D19-1467:
  abstract: Aspect level sentiment classification is a fine-grained sentiment analysis
    task. To detect the sentiment towards a particular aspect in a sentence, previous
    studies have developed various attention-based methods for generating aspect-specific
    sentence representations. However, the attention may inherently introduce noise
    and downgrade the performance. In this paper, we propose constrained attention
    networks (CAN), a simple yet effective solution, to regularize the attention for
    multi-aspect sentiment analysis, which alleviates the drawback of the attention
    mechanism. Specifically, we introduce orthogonal regularization on multiple aspects
    and sparse regularization on each single aspect. Experimental results on two public
    datasets demonstrate the effectiveness of our approach. We further extend our
    approach to multi-task settings and outperform the state-of-the-art methods.
  address: Hong Kong, China
  attachment:
  - filename: D19-1467.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1467.Attachment.zip
  author:
  - first: Mengting
    full: Mengting Hu
    id: mengting-hu
    last: Hu
  - first: Shiwan
    full: Shiwan Zhao
    id: shiwan-zhao
    last: Zhao
  - first: Li
    full: Li Zhang
    id: li-zhang
    last: Zhang
  - first: Keke
    full: Keke Cai
    id: keke-cai
    last: Cai
  - first: Zhong
    full: Zhong Su
    id: zhong-su
    last: Su
  - first: Renhong
    full: Renhong Cheng
    id: renhong-cheng
    last: Cheng
  - first: Xiaowei
    full: Xiaowei Shen
    id: xiaowei-shen
    last: Shen
  author_string: Mengting Hu, Shiwan Zhao, Li Zhang, Keke Cai, Zhong Su, Renhong Cheng,
    Xiaowei Shen
  bibkey: hu-etal-2019-constrained
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1467
  month: November
  page_first: '4601'
  page_last: '4610'
  pages: "4601\u20134610"
  paper_id: '467'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1467.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1467.jpg
  title: 'CAN: Constrained Attention Networks for Multi-Aspect Sentiment Analysis'
  title_html: '<span class="acl-fixed-case">CAN</span>: Constrained Attention Networks
    for Multi-Aspect Sentiment Analysis'
  url: https://www.aclweb.org/anthology/D19-1467
  year: '2019'
D19-1468:
  abstract: User-generated reviews can be decomposed into fine-grained segments (e.g.,
    sentences, clauses), each evaluating a different aspect of the principal entity
    (e.g., price, quality, appearance). Automatically detecting these aspects can
    be useful for both users and downstream opinion mining applications. Current supervised
    approaches for learning aspect classifiers require many fine-grained aspect labels,
    which are labor-intensive to obtain. And, unfortunately, unsupervised topic models
    often fail to capture the aspects of interest. In this work, we consider weakly
    supervised approaches for training aspect classifiers that only require the user
    to provide a small set of seed words (i.e., weakly positive indicators) for the
    aspects of interest. First, we show that current weakly supervised approaches
    fail to leverage the predictive power of seed words for aspect detection. Next,
    we propose a student-teacher approach that effectively leverages seed words in
    a bag-of-words classifier (teacher); in turn, we use the teacher to train a second
    model (student) that is potentially more powerful (e.g., a neural network that
    uses pre-trained word embeddings). Finally, we show that iterative co-training
    can be used to cope with noisy seed words, leading to both improved teacher and
    student models. Our proposed approach consistently outperforms previous weakly
    supervised approaches (by 14.1 absolute F1 points on average) in six different
    domains of product reviews and six multilingual datasets of restaurant reviews.
  address: Hong Kong, China
  attachment:
  - filename: D19-1468.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1468.Attachment.zip
  author:
  - first: Giannis
    full: Giannis Karamanolakis
    id: giannis-karamanolakis
    last: Karamanolakis
  - first: Daniel
    full: Daniel Hsu
    id: daniel-hsu
    last: Hsu
  - first: Luis
    full: Luis Gravano
    id: luis-gravano
    last: Gravano
  author_string: Giannis Karamanolakis, Daniel Hsu, Luis Gravano
  bibkey: karamanolakis-etal-2019-leveraging
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1468
  month: November
  page_first: '4611'
  page_last: '4621'
  pages: "4611\u20134621"
  paper_id: '468'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1468.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1468.jpg
  title: Leveraging Just a Few Keywords for Fine-Grained Aspect Detection Through
    Weakly Supervised Co-Training
  title_html: Leveraging Just a Few Keywords for Fine-Grained Aspect Detection Through
    Weakly Supervised Co-Training
  url: https://www.aclweb.org/anthology/D19-1468
  year: '2019'
D19-1469:
  abstract: "Computing author intent from multimodal data like Instagram posts requires\
    \ modeling a complex relationship between text and image. For example, a caption\
    \ might evoke an ironic contrast with the image, so neither caption nor image\
    \ is a mere transcript of the other. Instead they combine\u2014via what has been\
    \ called meaning multiplication (Bateman et al.)- to create a new meaning that\
    \ has a more complex relation to the literal meanings of text and image. Here\
    \ we introduce a multimodal dataset of 1299 Instagram posts labeled for three\
    \ orthogonal taxonomies: the authorial intent behind the image-caption pair, the\
    \ contextual relationship between the literal meanings of the image and caption,\
    \ and the semiotic relationship between the signified meanings of the image and\
    \ caption. We build a baseline deep multimodal classifier to validate the taxonomy,\
    \ showing that employing both text and image improves intent detection by 9.6\
    \ compared to using only the image modality, demonstrating the commonality of\
    \ non-intersective meaning multiplication. The gain with multimodality is greatest\
    \ when the image and caption diverge semiotically. Our dataset offers a new resource\
    \ for the study of the rich meanings that result from pairing text and image."
  address: Hong Kong, China
  author:
  - first: Julia
    full: Julia Kruk
    id: julia-kruk
    last: Kruk
  - first: Jonah
    full: Jonah Lubin
    id: jonah-lubin
    last: Lubin
  - first: Karan
    full: Karan Sikka
    id: karan-sikka
    last: Sikka
  - first: Xiao
    full: Xiao Lin
    id: xiao-lin
    last: Lin
  - first: Dan
    full: Dan Jurafsky
    id: dan-jurafsky
    last: Jurafsky
  - first: Ajay
    full: Ajay Divakaran
    id: ajay-divakaran
    last: Divakaran
  author_string: Julia Kruk, Jonah Lubin, Karan Sikka, Xiao Lin, Dan Jurafsky, Ajay
    Divakaran
  bibkey: kruk-etal-2019-integrating
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1469
  month: November
  page_first: '4622'
  page_last: '4632'
  pages: "4622\u20134632"
  paper_id: '469'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1469.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1469.jpg
  title: 'Integrating Text and Image: Determining Multimodal Document Intent in Instagram
    Posts'
  title_html: 'Integrating Text and Image: Determining Multimodal Document Intent
    in <span class="acl-fixed-case">I</span>nstagram Posts'
  url: https://www.aclweb.org/anthology/D19-1469
  year: '2019'
D19-1470:
  abstract: "The prevalent use of social media leads to a vast amount of online conversations\
    \ being produced on a daily basis. It presents a concrete challenge for individuals\
    \ to better discover and engage in social media discussions. In this paper, we\
    \ present a novel framework to automatically recommend conversations to users\
    \ based on their prior conversation behaviors. Built on neural collaborative filtering,\
    \ our model explores deep semantic features that measure how a user\u2019s preferences\
    \ match an ongoing conversation\u2019s context. Furthermore, to identify salient\
    \ characteristics from interleaving user interactions, our model incorporates\
    \ graph-structured networks, where both replying relations and temporal features\
    \ are encoded as conversation context. Experimental results on two large-scale\
    \ datasets collected from Twitter and Reddit show that our model yields better\
    \ performance than previous state-of-the-art models, which only utilize lexical\
    \ features and ignore past user interactions in the conversations."
  address: Hong Kong, China
  author:
  - first: Xingshan
    full: Xingshan Zeng
    id: xingshan-zeng
    last: Zeng
  - first: Jing
    full: Jing Li
    id: jing-li
    last: Li
  - first: Lu
    full: Lu Wang
    id: lu-wang
    last: Wang
  - first: Kam-Fai
    full: Kam-Fai Wong
    id: kam-fai-wong
    last: Wong
  author_string: Xingshan Zeng, Jing Li, Lu Wang, Kam-Fai Wong
  bibkey: zeng-etal-2019-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1470
  month: November
  page_first: '4633'
  page_last: '4643'
  pages: "4633\u20134643"
  paper_id: '470'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1470.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1470.jpg
  title: Neural Conversation Recommendation with Online Interaction Modeling
  title_html: Neural Conversation Recommendation with Online Interaction Modeling
  url: https://www.aclweb.org/anthology/D19-1470
  year: '2019'
D19-1471:
  abstract: Recently, neural networks based on multi-task learning have achieved promising
    performance on fake news detection, which focuses on learning shared features
    among tasks as complementarity features to serve different tasks. However, in
    most of the existing approaches, the shared features are completely assigned to
    different tasks without selection, which may lead to some useless and even adverse
    features integrated into specific tasks. In this paper, we design a sifted multi-task
    learning method with a selected sharing layer for fake news detection. The selected
    sharing layer adopts gate mechanism and attention mechanism to filter and select
    shared feature flows between tasks. Experiments on two public and widely used
    competition datasets, i.e. RumourEval and PHEME, demonstrate that our proposed
    method achieves the state-of-the-art performance and boosts the F1-score by more
    than 0.87%, 1.31%, respectively.
  address: Hong Kong, China
  author:
  - first: Lianwei
    full: Lianwei Wu
    id: lianwei-wu
    last: Wu
  - first: Yuan
    full: Yuan Rao
    id: yuan-rao
    last: Rao
  - first: Haolin
    full: Haolin Jin
    id: haolin-jin
    last: Jin
  - first: Ambreen
    full: Ambreen Nazir
    id: ambreen-nazir
    last: Nazir
  - first: Ling
    full: Ling Sun
    id: ling-sun
    last: Sun
  author_string: Lianwei Wu, Yuan Rao, Haolin Jin, Ambreen Nazir, Ling Sun
  bibkey: wu-etal-2019-different
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1471
  month: November
  page_first: '4644'
  page_last: '4653'
  pages: "4644\u20134653"
  paper_id: '471'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1471.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1471.jpg
  title: 'Different Absorption from the Same Sharing: Sifted Multi-task Learning for
    Fake News Detection'
  title_html: 'Different Absorption from the Same Sharing: Sifted Multi-task Learning
    for Fake News Detection'
  url: https://www.aclweb.org/anthology/D19-1471
  year: '2019'
D19-1472:
  abstract: "We present a text-based framework for investigating moral sentiment change\
    \ of the public via longitudinal corpora. Our framework is based on the premise\
    \ that language use can inform people\u2019s moral perception toward right or\
    \ wrong, and we build our methodology by exploring moral biases learned from diachronic\
    \ word embeddings. We demonstrate how a parameter-free model supports inference\
    \ of historical shifts in moral sentiment toward concepts such as slavery and\
    \ democracy over centuries at three incremental levels: moral relevance, moral\
    \ polarity, and fine-grained moral dimensions. We apply this methodology to visualizing\
    \ moral time courses of individual concepts and analyzing the relations between\
    \ psycholinguistic variables and rates of moral sentiment change at scale. Our\
    \ work offers opportunities for applying natural language processing toward characterizing\
    \ moral sentiment change in society."
  address: Hong Kong, China
  attachment:
  - filename: D19-1472.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1472.Attachment.zip
  author:
  - first: Jing Yi
    full: Jing Yi Xie
    id: jing-yi-xie
    last: Xie
  - first: Renato
    full: Renato Ferreira Pinto Junior
    id: renato-ferreira-pinto-junior
    last: Ferreira Pinto Junior
  - first: Graeme
    full: Graeme Hirst
    id: graeme-hirst
    last: Hirst
  - first: Yang
    full: Yang Xu
    id: yang-xu
    last: Xu
  author_string: Jing Yi Xie, Renato Ferreira Pinto Junior, Graeme Hirst, Yang Xu
  bibkey: xie-etal-2019-text
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1472
  month: November
  page_first: '4654'
  page_last: '4663'
  pages: "4654\u20134663"
  paper_id: '472'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1472.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1472.jpg
  title: Text-based inference of moral sentiment change
  title_html: Text-based inference of moral sentiment change
  url: https://www.aclweb.org/anthology/D19-1472
  year: '2019'
D19-1473:
  abstract: "Causal interpretation of correlational findings from observational studies\
    \ has been a major type of misinformation in science communication. Prior studies\
    \ on identifying inappropriate use of causal language relied on manual content\
    \ analysis, which is not scalable for examining a large volume of science publications.\
    \ In this study, we first annotated a corpus of over 3,000 PubMed research conclusion\
    \ sentences, then developed a BERT-based prediction model that classifies conclusion\
    \ sentences into \u201Cno relationship\u201D, \u201Ccorrelational\u201D, \u201C\
    conditional causal\u201D, and \u201Cdirect causal\u201D categories, achieving\
    \ an accuracy of 0.90 and a macro-F1 of 0.88. We then applied the prediction model\
    \ to measure the causal language use in the research conclusions of about 38,000\
    \ observational studies in PubMed. The prediction result shows that 21.7% studies\
    \ used direct causal language exclusively in their conclusions, and 32.4% used\
    \ some direct causal language. We also found that the ratio of causal language\
    \ use differs among authors from different countries, challenging the notion of\
    \ a shared consensus on causal language use in the global science community. Our\
    \ prediction model could also be used to help identify the inappropriate use of\
    \ causal language in science publications."
  address: Hong Kong, China
  author:
  - first: Bei
    full: Bei Yu
    id: bei-yu
    last: Yu
  - first: Yingya
    full: Yingya Li
    id: yingya-li
    last: Li
  - first: Jun
    full: Jun Wang
    id: jun-wang
    last: Wang
  author_string: Bei Yu, Yingya Li, Jun Wang
  bibkey: yu-etal-2019-detecting
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1473
  month: November
  page_first: '4664'
  page_last: '4674'
  pages: "4664\u20134674"
  paper_id: '473'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1473.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1473.jpg
  title: Detecting Causal Language Use in Science Findings
  title_html: Detecting Causal Language Use in Science Findings
  url: https://www.aclweb.org/anthology/D19-1473
  year: '2019'
D19-1474:
  abstract: Current research on hate speech analysis is typically oriented towards
    monolingual and single classification tasks. In this paper, we present a new multilingual
    multi-aspect hate speech analysis dataset and use it to test the current state-of-the-art
    multilingual multitask learning approaches. We evaluate our dataset in various
    classification settings, then we discuss how to leverage our annotations in order
    to improve hate speech detection and classification in general.
  address: Hong Kong, China
  author:
  - first: Nedjma
    full: Nedjma Ousidhoum
    id: nedjma-ousidhoum
    last: Ousidhoum
  - first: Zizheng
    full: Zizheng Lin
    id: zizheng-lin
    last: Lin
  - first: Hongming
    full: Hongming Zhang
    id: hongming-zhang
    last: Zhang
  - first: Yangqiu
    full: Yangqiu Song
    id: yangqiu-song
    last: Song
  - first: Dit-Yan
    full: Dit-Yan Yeung
    id: dit-yan-yeung
    last: Yeung
  author_string: Nedjma Ousidhoum, Zizheng Lin, Hongming Zhang, Yangqiu Song, Dit-Yan
    Yeung
  bibkey: ousidhoum-etal-2019-multilingual
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1474
  month: November
  page_first: '4675'
  page_last: '4684'
  pages: "4675\u20134684"
  paper_id: '474'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1474.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1474.jpg
  title: Multilingual and Multi-Aspect Hate Speech Analysis
  title_html: Multilingual and Multi-Aspect Hate Speech Analysis
  url: https://www.aclweb.org/anthology/D19-1474
  year: '2019'
D19-1475:
  abstract: We contribute the largest publicly available dataset of naturally occurring
    factual claims for the purpose of automatic claim verification. It is collected
    from 26 fact checking websites in English, paired with textual sources and rich
    metadata, and labelled for veracity by human expert journalists. We present an
    in-depth analysis of the dataset, highlighting characteristics and challenges.
    Further, we present results for automatic veracity prediction, both with established
    baselines and with a novel method for joint ranking of evidence pages and predicting
    veracity that outperforms all baselines. Significant performance increases are
    achieved by encoding evidence, and by modelling metadata. Our best-performing
    model achieves a Macro F1 of 49.2%, showing that this is a challenging testbed
    for claim veracity prediction.
  address: Hong Kong, China
  author:
  - first: Isabelle
    full: Isabelle Augenstein
    id: isabelle-augenstein
    last: Augenstein
  - first: Christina
    full: Christina Lioma
    id: christina-lioma
    last: Lioma
  - first: Dongsheng
    full: Dongsheng Wang
    id: dongsheng-wang
    last: Wang
  - first: Lucas
    full: Lucas Chaves Lima
    id: lucas-chaves-lima
    last: Chaves Lima
  - first: Casper
    full: Casper Hansen
    id: casper-hansen
    last: Hansen
  - first: Christian
    full: Christian Hansen
    id: christian-hansen
    last: Hansen
  - first: Jakob Grue
    full: Jakob Grue Simonsen
    id: jakob-grue-simonsen
    last: Simonsen
  author_string: Isabelle Augenstein, Christina Lioma, Dongsheng Wang, Lucas Chaves
    Lima, Casper Hansen, Christian Hansen, Jakob Grue Simonsen
  bibkey: augenstein-etal-2019-multifc
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1475
  month: November
  page_first: '4685'
  page_last: '4697'
  pages: "4685\u20134697"
  paper_id: '475'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1475.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1475.jpg
  title: 'MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking
    of Claims'
  title_html: '<span class="acl-fixed-case">M</span>ulti<span class="acl-fixed-case">FC</span>:
    A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims'
  url: https://www.aclweb.org/anthology/D19-1475
  year: '2019'
D19-1476:
  abstract: Textual network embeddings aim to learn a low-dimensional representation
    for every node in the network so that both the structural and textual information
    from the networks can be well preserved in the representations. Traditionally,
    the structural and textual embeddings were learned by models that rarely take
    the mutual influences between them into account. In this paper, a deep neural
    architecture is proposed to effectively fuse the two kinds of informations into
    one representation. The novelties of the proposed architecture are manifested
    in the aspects of a newly defined objective function, the complementary information
    fusion method for structural and textual features, and the mutual gate mechanism
    for textual feature extraction. Experimental results show that the proposed model
    outperforms the comparing methods on all three datasets.
  address: Hong Kong, China
  author:
  - first: Zenan
    full: Zenan Xu
    id: zenan-xu
    last: Xu
  - first: Qinliang
    full: Qinliang Su
    id: qinliang-su
    last: Su
  - first: Xiaojun
    full: Xiaojun Quan
    id: xiaojun-quan
    last: Quan
  - first: Weijia
    full: Weijia Zhang
    id: weijia-zhang
    last: Zhang
  author_string: Zenan Xu, Qinliang Su, Xiaojun Quan, Weijia Zhang
  bibkey: xu-etal-2019-deep
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1476
  month: November
  page_first: '4698'
  page_last: '4706'
  pages: "4698\u20134706"
  paper_id: '476'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1476.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1476.jpg
  title: A Deep Neural Information Fusion Architecture for Textual Network Embeddings
  title_html: A Deep Neural Information Fusion Architecture for Textual Network Embeddings
  url: https://www.aclweb.org/anthology/D19-1476
  year: '2019'
D19-1477:
  abstract: Information about individuals can help to better understand what they
    say, particularly in social media where texts are short. Current approaches to
    modelling social media users pay attention to their social connections, but exploit
    this information in a static way, treating all connections uniformly. This ignores
    the fact, well known in sociolinguistics, that an individual may be part of several
    communities which are not equally relevant in all communicative situations. We
    present a model based on Graph Attention Networks that captures this observation.
    It dynamically explores the social graph of a user, computes a user representation
    given the most relevant connections for a target task, and combines it with linguistic
    information to make a prediction. We apply our model to three different tasks,
    evaluate it against alternative models, and analyse the results extensively, showing
    that it significantly outperforms other current methods.
  address: Hong Kong, China
  attachment:
  - filename: D19-1477.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1477.Attachment.pdf
  author:
  - first: Marco
    full: Marco Del Tredici
    id: marco-del-tredici
    last: Del Tredici
  - first: Diego
    full: Diego Marcheggiani
    id: diego-marcheggiani
    last: Marcheggiani
  - first: Sabine
    full: Sabine Schulte im Walde
    id: sabine-schulte-im-walde
    last: Schulte im Walde
  - first: Raquel
    full: "Raquel Fern\xE1ndez"
    id: raquel-fernandez
    last: "Fern\xE1ndez"
  author_string: "Marco Del Tredici, Diego Marcheggiani, Sabine Schulte im Walde,\
    \ Raquel Fern\xE1ndez"
  bibkey: del-tredici-etal-2019-shall
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1477
  month: November
  page_first: '4707'
  page_last: '4717'
  pages: "4707\u20134717"
  paper_id: '477'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1477.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1477.jpg
  title: 'You Shall Know a User by the Company It Keeps: Dynamic Representations for
    Social Media Users in NLP'
  title_html: 'You Shall Know a User by the Company It Keeps: Dynamic Representations
    for Social Media Users in <span class="acl-fixed-case">NLP</span>'
  url: https://www.aclweb.org/anthology/D19-1477
  year: '2019'
D19-1478:
  abstract: Insightful findings in political science often require researchers to
    analyze documents of a certain subject or type, yet these documents are usually
    contained in large corpora that do not distinguish between pertinent and non-pertinent
    documents. In contrast, we can find corpora that label relevant documents but
    have limitations (e.g., from a single source or era), preventing their use for
    political science research. To bridge this gap, we present adaptive ensembling,
    an unsupervised domain adaptation framework, equipped with a novel text classification
    model and time-aware training to ensure our methods work well with diachronic
    corpora. Experiments on an expert-annotated dataset show that our framework outperforms
    strong benchmarks. Further analysis indicates that our methods are more stable,
    learn better representations, and extract cleaner corpora for fine-grained analysis.
  address: Hong Kong, China
  attachment:
  - filename: D19-1478.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1478.Attachment.pdf
  author:
  - first: Shrey
    full: Shrey Desai
    id: shrey-desai
    last: Desai
  - first: Barea
    full: Barea Sinno
    id: barea-sinno
    last: Sinno
  - first: Alex
    full: Alex Rosenfeld
    id: alex-rosenfeld
    last: Rosenfeld
  - first: Junyi Jessy
    full: Junyi Jessy Li
    id: junyi-jessy-li
    last: Li
  author_string: Shrey Desai, Barea Sinno, Alex Rosenfeld, Junyi Jessy Li
  bibkey: desai-etal-2019-adaptive
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1478
  month: November
  page_first: '4718'
  page_last: '4730'
  pages: "4718\u20134730"
  paper_id: '478'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1478.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1478.jpg
  title: 'Adaptive Ensembling: Unsupervised Domain Adaptation for Political Document
    Analysis'
  title_html: 'Adaptive Ensembling: Unsupervised Domain Adaptation for Political Document
    Analysis'
  url: https://www.aclweb.org/anthology/D19-1478
  year: '2019'
D19-1480:
  abstract: Accurate estimation of user location is important for many online services.
    Previous neural network based methods largely ignore the hierarchical structure
    among locations. In this paper, we propose a hierarchical location prediction
    neural network for Twitter user geolocation. Our model first predicts the home
    country for a user, then uses the country result to guide the city-level prediction.
    In addition, we employ a character-aware word embedding layer to overcome the
    noisy information in tweets. With the feature fusion layer, our model can accommodate
    various feature combinations and achieves state-of-the-art results over three
    commonly used benchmarks under different feature settings. It not only improves
    the prediction accuracy but also greatly reduces the mean error distance.
  address: Hong Kong, China
  author:
  - first: Binxuan
    full: Binxuan Huang
    id: binxuan-huang
    last: Huang
  - first: Kathleen
    full: Kathleen Carley
    id: kathleen-m-carley
    last: Carley
  author_string: Binxuan Huang, Kathleen Carley
  bibkey: huang-carley-2019-hierarchical
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1480
  month: November
  page_first: '4732'
  page_last: '4742'
  pages: "4732\u20134742"
  paper_id: '480'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1480.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1480.jpg
  title: A Hierarchical Location Prediction Neural Network for Twitter User Geolocation
  title_html: A Hierarchical Location Prediction Neural Network for Twitter User Geolocation
  url: https://www.aclweb.org/anthology/D19-1480
  year: '2019'
D19-1481:
  abstract: 'Online discussions often derail into toxic exchanges between participants.
    Recent efforts mostly focused on detecting antisocial behavior after the fact,
    by analyzing single comments in isolation. To provide more timely notice to human
    moderators, a system needs to preemptively detect that a conversation is heading
    towards derailment before it actually turns toxic. This means modeling derailment
    as an emerging property of a conversation rather than as an isolated utterance-level
    event. Forecasting emerging conversational properties, however, poses several
    inherent modeling challenges. First, since conversations are dynamic, a forecasting
    model needs to capture the flow of the discussion, rather than properties of individual
    comments. Second, real conversations have an unknown horizon: they can end or
    derail at any time; thus a practical forecasting model needs to assess the risk
    in an online fashion, as the conversation develops. In this work we introduce
    a conversational forecasting model that learns an unsupervised representation
    of conversational dynamics and exploits it to predict future derailment as the
    conversation develops. By applying this model to two new diverse datasets of online
    conversations with labels for antisocial events, we show that it outperforms state-of-the-art
    systems at forecasting derailment.'
  address: Hong Kong, China
  author:
  - first: Jonathan P.
    full: Jonathan P. Chang
    id: jonathan-p-chang
    last: Chang
  - first: Cristian
    full: Cristian Danescu-Niculescu-Mizil
    id: cristian-danescu-niculescu-mizil
    last: Danescu-Niculescu-Mizil
  author_string: Jonathan P. Chang, Cristian Danescu-Niculescu-Mizil
  bibkey: chang-danescu-niculescu-mizil-2019-trouble
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1481
  month: November
  page_first: '4743'
  page_last: '4754'
  pages: "4743\u20134754"
  paper_id: '481'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1481.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1481.jpg
  title: 'Trouble on the Horizon: Forecasting the Derailment of Online Conversations
    as they Develop'
  title_html: 'Trouble on the Horizon: Forecasting the Derailment of Online Conversations
    as they Develop'
  url: https://www.aclweb.org/anthology/D19-1481
  year: '2019'
D19-1482:
  abstract: Countering online hate speech is a critical yet challenging task, but
    one which can be aided by the use of Natural Language Processing (NLP) techniques.
    Previous research has primarily focused on the development of NLP methods to automatically
    and effectively detect online hate speech while disregarding further action needed
    to calm and discourage individuals from using hate speech in the future. In addition,
    most existing hate speech datasets treat each post as an isolated instance, ignoring
    the conversational context. In this paper, we propose a novel task of generative
    hate speech intervention, where the goal is to automatically generate responses
    to intervene during online conversations that contain hate speech. As a part of
    this work, we introduce two fully-labeled large-scale hate speech intervention
    datasets collected from Gab and Reddit. These datasets provide conversation segments,
    hate speech labels, as well as intervention responses written by Mechanical Turk
    Workers. In this paper, we also analyze the datasets to understand the common
    intervention strategies and explore the performance of common automatic response
    generation methods on these new datasets to provide a benchmark for future research.
  address: Hong Kong, China
  author:
  - first: Jing
    full: Jing Qian
    id: jing-qian
    last: Qian
  - first: Anna
    full: Anna Bethke
    id: anna-bethke
    last: Bethke
  - first: Yinyin
    full: Yinyin Liu
    id: yinyin-liu
    last: Liu
  - first: Elizabeth
    full: Elizabeth Belding
    id: elizabeth-belding
    last: Belding
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  author_string: Jing Qian, Anna Bethke, Yinyin Liu, Elizabeth Belding, William Yang
    Wang
  bibkey: qian-etal-2019-benchmark
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1482
  month: November
  page_first: '4755'
  page_last: '4764'
  pages: "4755\u20134764"
  paper_id: '482'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1482.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1482.jpg
  title: A Benchmark Dataset for Learning to Intervene in Online Hate Speech
  title_html: A Benchmark Dataset for Learning to Intervene in Online Hate Speech
  url: https://www.aclweb.org/anthology/D19-1482
  year: '2019'
D19-1483:
  abstract: "Gang-involved youth in cities such as Chicago sometimes post on social\
    \ media to express their aggression towards rival gangs and previous research\
    \ has demonstrated that a deep learning approach can predict aggression and loss\
    \ in posts. To address the possibility of bias in this sensitive application,\
    \ we developed an approach to systematically interpret the state of the art model.\
    \ We found, surprisingly, that it frequently bases its predictions on stop words\
    \ such as \u201Ca\u201D or \u201Con\u201D, an approach that could harm social\
    \ media users who have no aggressive intentions. To tackle this bias, domain experts\
    \ annotated the rationales, highlighting words that explain why a tweet is labeled\
    \ as \u201Caggression\u201D. These new annotations enable us to quantitatively\
    \ measure how justified the model predictions are, and build models that drastically\
    \ reduce bias. Our study shows that in high stake scenarios, accuracy alone cannot\
    \ guarantee a good system and we need new evaluation methods."
  address: Hong Kong, China
  attachment:
  - filename: D19-1483.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1483.Attachment.pdf
  author:
  - first: Ruiqi
    full: Ruiqi Zhong
    id: ruiqi-zhong
    last: Zhong
  - first: Yanda
    full: Yanda Chen
    id: yanda-chen
    last: Chen
  - first: Desmond
    full: Desmond Patton
    id: desmond-patton
    last: Patton
  - first: Charlotte
    full: Charlotte Selous
    id: charlotte-selous
    last: Selous
  - first: Kathy
    full: Kathy McKeown
    id: kathleen-mckeown
    last: McKeown
  author_string: Ruiqi Zhong, Yanda Chen, Desmond Patton, Charlotte Selous, Kathy
    McKeown
  bibkey: zhong-etal-2019-detecting
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1483
  month: November
  page_first: '4765'
  page_last: '4775'
  pages: "4765\u20134775"
  paper_id: '483'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1483.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1483.jpg
  title: Detecting and Reducing Bias in a High Stakes Domain
  title_html: Detecting and Reducing Bias in a High Stakes Domain
  url: https://www.aclweb.org/anthology/D19-1483
  year: '2019'
D19-1484:
  abstract: In contrast to many decades of research on oral code-switching, the study
    of written multilingual productions has only recently enjoyed a surge of interest.
    Many open questions remain regarding the sociolinguistic underpinnings of written
    code-switching, and progress has been limited by a lack of suitable resources.
    We introduce a novel, large, and diverse dataset of written code-switched productions,
    curated from topical threads of multiple bilingual communities on the Reddit discussion
    platform, and explore questions that were mainly addressed in the context of spoken
    language thus far. We investigate whether findings in oral code-switching concerning
    content and style, as well as speaker proficiency, are carried over into written
    code-switching in discussion forums. The released dataset can further facilitate
    a range of research and practical activities.
  address: Hong Kong, China
  attachment:
  - filename: D19-1484.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1484.Attachment.zip
  author:
  - first: Ella
    full: Ella Rabinovich
    id: ella-rabinovich
    last: Rabinovich
  - first: Masih
    full: Masih Sultani
    id: masih-sultani
    last: Sultani
  - first: Suzanne
    full: Suzanne Stevenson
    id: suzanne-stevenson
    last: Stevenson
  author_string: Ella Rabinovich, Masih Sultani, Suzanne Stevenson
  bibkey: rabinovich-etal-2019-codeswitch
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1484
  month: November
  page_first: '4776'
  page_last: '4786'
  pages: "4776\u20134786"
  paper_id: '484'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1484.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1484.jpg
  title: 'CodeSwitch-Reddit: Exploration of Written Multilingual Discourse in Online
    Discussion Forums'
  title_html: '<span class="acl-fixed-case">C</span>ode<span class="acl-fixed-case">S</span>witch-<span
    class="acl-fixed-case">R</span>eddit: Exploration of Written Multilingual Discourse
    in Online Discussion Forums'
  url: https://www.aclweb.org/anthology/D19-1484
  year: '2019'
D19-1485:
  abstract: "Automatically verifying rumorous information has become an important\
    \ and challenging task in natural language processing and social media analytics.\
    \ Previous studies reveal that people\u2019s stances towards rumorous messages\
    \ can provide indicative clues for identifying the veracity of rumors, and thus\
    \ determining the stances of public reactions is a crucial preceding step for\
    \ rumor veracity prediction. In this paper, we propose a hierarchical multi-task\
    \ learning framework for jointly predicting rumor stance and veracity on Twitter,\
    \ which consists of two components. The bottom component of our framework classifies\
    \ the stances of tweets in a conversation discussing a rumor via modeling the\
    \ structural property based on a novel graph convolutional network. The top component\
    \ predicts the rumor veracity by exploiting the temporal dynamics of stance evolution.\
    \ Experimental results on two benchmark datasets show that our method outperforms\
    \ previous methods in both rumor stance classification and veracity prediction."
  address: Hong Kong, China
  author:
  - first: Penghui
    full: Penghui Wei
    id: penghui-wei
    last: Wei
  - first: Nan
    full: Nan Xu
    id: nan-xu
    last: Xu
  - first: Wenji
    full: Wenji Mao
    id: wenji-mao
    last: Mao
  author_string: Penghui Wei, Nan Xu, Wenji Mao
  bibkey: wei-etal-2019-modeling
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1485
  month: November
  page_first: '4787'
  page_last: '4798'
  pages: "4787\u20134798"
  paper_id: '485'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1485.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1485.jpg
  title: Modeling Conversation Structure and Temporal Dynamics for Jointly Predicting
    Rumor Stance and Veracity
  title_html: Modeling Conversation Structure and Temporal Dynamics for Jointly Predicting
    Rumor Stance and Veracity
  url: https://www.aclweb.org/anthology/D19-1485
  year: '2019'
D19-1486:
  abstract: 'Intent classification is an important building block of dialogue systems.
    With the burgeoning of conversational AI, existing systems are not capable of
    handling numerous fast-emerging intents, which motivates zero-shot intent classification.
    Nevertheless, research on this problem is still in the incipient stage and few
    methods are available. A recently proposed zero-shot intent classification method,
    IntentCapsNet, has been shown to achieve state-of-the-art performance. However,
    it has two unaddressed limitations: (1) it cannot deal with polysemy when extracting
    semantic capsules; (2) it hardly recognizes the utterances of unseen intents in
    the generalized zero-shot intent classification setting. To overcome these limitations,
    we propose to reconstruct capsule networks for zero-shot intent classification.
    First, we introduce a dimensional attention mechanism to fight against polysemy.
    Second, we reconstruct the transformation matrices for unseen intents by utilizing
    abundant latent information of the labeled utterances, which significantly improves
    the model generalization ability. Experimental results on two task-oriented dialogue
    datasets in different languages show that our proposed method outperforms IntentCapsNet
    and other strong baselines.'
  address: Hong Kong, China
  author:
  - first: Han
    full: Han Liu
    id: han-liu
    last: Liu
  - first: Xiaotong
    full: Xiaotong Zhang
    id: xiaotong-zhang
    last: Zhang
  - first: Lu
    full: Lu Fan
    id: lu-fan
    last: Fan
  - first: Xuandi
    full: Xuandi Fu
    id: xuandi-fu
    last: Fu
  - first: Qimai
    full: Qimai Li
    id: qimai-li
    last: Li
  - first: Xiao-Ming
    full: Xiao-Ming Wu
    id: xiao-ming-wu
    last: Wu
  - first: Albert Y.S.
    full: Albert Y.S. Lam
    id: albert-y-s-lam
    last: Lam
  author_string: Han Liu, Xiaotong Zhang, Lu Fan, Xuandi Fu, Qimai Li, Xiao-Ming Wu,
    Albert Y.S. Lam
  bibkey: liu-etal-2019-reconstructing
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1486
  month: November
  page_first: '4799'
  page_last: '4809'
  pages: "4799\u20134809"
  paper_id: '486'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1486.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1486.jpg
  title: Reconstructing Capsule Networks for Zero-shot Intent Classification
  title_html: Reconstructing Capsule Networks for Zero-shot Intent Classification
  url: https://www.aclweb.org/anthology/D19-1486
  year: '2019'
D19-1487:
  abstract: Person-job fit has been an important task which aims to automatically
    match job positions with suitable candidates. Previous methods mainly focus on
    solving the match task in single-domain setting, which may not work well when
    labeled data is limited. We study the domain adaptation problem for person-job
    fit. We first propose a deep global match network for capturing the global semantic
    interactions between two sentences from a job posting and a candidate resume respectively.
    Furthermore, we extend the match network and implement domain adaptation in three
    levels, sentence-level representation, sentence-level match, and global match.
    Extensive experiment results on a large real-world dataset consisting of six domains
    have demonstrated the effectiveness of the proposed model, especially when there
    is not sufficient labeled data.
  address: Hong Kong, China
  author:
  - first: Shuqing
    full: Shuqing Bian
    id: shuqing-bian
    last: Bian
  - first: Wayne Xin
    full: Wayne Xin Zhao
    id: wayne-xin-zhao
    last: Zhao
  - first: Yang
    full: Yang Song
    id: yang-song
    last: Song
  - first: Tao
    full: Tao Zhang
    id: tao-zhang
    last: Zhang
  - first: Ji-Rong
    full: Ji-Rong Wen
    id: ji-rong-wen
    last: Wen
  author_string: Shuqing Bian, Wayne Xin Zhao, Yang Song, Tao Zhang, Ji-Rong Wen
  bibkey: bian-etal-2019-domain
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1487
  month: November
  page_first: '4810'
  page_last: '4820'
  pages: "4810\u20134820"
  paper_id: '487'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1487.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1487.jpg
  title: Domain Adaptation for Person-Job Fit with Transferable Deep Global Match
    Network
  title_html: Domain Adaptation for Person-Job Fit with Transferable Deep Global Match
    Network
  url: https://www.aclweb.org/anthology/D19-1487
  year: '2019'
D19-1488:
  abstract: Short text classification has found rich and critical applications in
    news and tweet tagging to help users find relevant information. Due to lack of
    labeled training data in many practical use cases, there is a pressing need for
    studying semi-supervised short text classification. Most existing studies focus
    on long texts and achieve unsatisfactory performance on short texts due to the
    sparsity and limited labeled data. In this paper, we propose a novel heterogeneous
    graph neural network based method for semi-supervised short text classification,
    leveraging full advantage of few labeled data and large unlabeled data through
    information propagation along the graph. In particular, we first present a flexible
    HIN (heterogeneous information network) framework for modeling the short texts,
    which can integrate any type of additional information as well as capture their
    relations to address the semantic sparsity. Then, we propose Heterogeneous Graph
    ATtention networks (HGAT) to embed the HIN for short text classification based
    on a dual-level attention mechanism, including node-level and type-level attentions.
    The attention mechanism can learn the importance of different neighboring nodes
    as well as the importance of different node (information) types to a current node.
    Extensive experimental results have demonstrated that our proposed model outperforms
    state-of-the-art methods across six benchmark datasets significantly.
  address: Hong Kong, China
  author:
  - first: Hu
    full: Hu Linmei
    id: hu-linmei
    last: Linmei
  - first: Tianchi
    full: Tianchi Yang
    id: tianchi-yang
    last: Yang
  - first: Chuan
    full: Chuan Shi
    id: chuan-shi
    last: Shi
  - first: Houye
    full: Houye Ji
    id: houye-ji
    last: Ji
  - first: Xiaoli
    full: Xiaoli Li
    id: xiaoli-li
    last: Li
  author_string: Hu Linmei, Tianchi Yang, Chuan Shi, Houye Ji, Xiaoli Li
  bibkey: linmei-etal-2019-heterogeneous
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1488
  month: November
  page_first: '4821'
  page_last: '4830'
  pages: "4821\u20134830"
  paper_id: '488'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1488.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1488.jpg
  title: Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification
  title_html: Heterogeneous Graph Attention Networks for Semi-supervised Short Text
    Classification
  url: https://www.aclweb.org/anthology/D19-1488
  year: '2019'
D19-1489:
  abstract: "The readability of a digital text can influence people\u2019s ability\
    \ to learn new things about a range topics from digital resources (e.g., Wikipedia,\
    \ WebMD). Readability also impacts search rankings, and is used to evaluate the\
    \ performance of NLP systems. Despite this, we lack a thorough understanding of\
    \ how to validly measure readability at scale, especially for domain-specific\
    \ texts. In this work, we present a comparison of the validity of well-known readability\
    \ measures and introduce a novel approach, Smart Cloze, which is designed to address\
    \ shortcomings of existing measures. We compare these approaches across four different\
    \ corpora: crowdworker-generated stories, Wikipedia articles, security and privacy\
    \ advice, and health information. On these corpora, we evaluate the convergent\
    \ and content validity of each measure, and detail tradeoffs in score precision,\
    \ domain-specificity, and participant burden. These results provide a foundation\
    \ for more accurate readability measurements and better evaluation of new natural-language-processing\
    \ systems and tools."
  address: Hong Kong, China
  author:
  - first: Elissa
    full: Elissa Redmiles
    id: elissa-redmiles
    last: Redmiles
  - first: Lisa
    full: Lisa Maszkiewicz
    id: lisa-maszkiewicz
    last: Maszkiewicz
  - first: Emily
    full: Emily Hwang
    id: emily-hwang
    last: Hwang
  - first: Dhruv
    full: Dhruv Kuchhal
    id: dhruv-kuchhal
    last: Kuchhal
  - first: Everest
    full: Everest Liu
    id: everest-liu
    last: Liu
  - first: Miraida
    full: Miraida Morales
    id: miraida-morales
    last: Morales
  - first: Denis
    full: Denis Peskov
    id: denis-peskov
    last: Peskov
  - first: Sudha
    full: Sudha Rao
    id: sudha-rao
    last: Rao
  - first: Rock
    full: Rock Stevens
    id: rock-stevens
    last: Stevens
  - first: Kristina
    full: "Kristina Gligori\u0107"
    id: kristina-gligoric
    last: "Gligori\u0107"
  - first: Sean
    full: Sean Kross
    id: sean-kross
    last: Kross
  - first: Michelle
    full: Michelle Mazurek
    id: michelle-mazurek
    last: Mazurek
  - first: Hal
    full: "Hal Daum\xE9 III"
    id: hal-daume-iii
    last: "Daum\xE9 III"
  author_string: "Elissa Redmiles, Lisa Maszkiewicz, Emily Hwang, Dhruv Kuchhal, Everest\
    \ Liu, Miraida Morales, Denis Peskov, Sudha Rao, Rock Stevens, Kristina Gligori\u0107\
    , Sean Kross, Michelle Mazurek, Hal Daum\xE9 III"
  bibkey: redmiles-etal-2019-comparing
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1489
  month: November
  page_first: '4831'
  page_last: '4842'
  pages: "4831\u20134842"
  paper_id: '489'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1489.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1489.jpg
  title: Comparing and Developing Tools to Measure the Readability of Domain-Specific
    Texts
  title_html: Comparing and Developing Tools to Measure the Readability of Domain-Specific
    Texts
  url: https://www.aclweb.org/anthology/D19-1489
  year: '2019'
D19-1490:
  abstract: With the development of NLP technologies, news can be automatically categorized
    and labeled according to a variety of characteristics, at the same time be represented
    as low dimensional embeddings. However, it lacks a systematic approach that effectively
    integrates the inherited features and inter-textual knowledge of news to represent
    the collective information with a dense vector. With the aim of filling this gap,
    the News2vec model is proposed to allow the distributed representation of news
    taking into account its associated features. To describe the cross-document linkages
    between news, a network consisting of news and its attributes is constructed.
    Moreover, the News2vec model treats the news node as a bag of features by developing
    the Subnode model. Based on the biased random walk and the skip-gram model, each
    news feature is mapped to a vector, and the news is thus represented as the sum
    of its features. This approach offers an easy solution to create embeddings for
    unseen news nodes based on its attributes. To evaluate our model, dimension reduction
    plots and correlation heat-maps are created to visualize the news vectors, together
    with the application of two downstream tasks, the stock movement prediction and
    news recommendation. By comparing with other established text/sentence embedding
    models, we show that News2vec achieves state-of-the-art performance on these news-related
    tasks.
  address: Hong Kong, China
  author:
  - first: Ye
    full: Ye Ma
    id: ye-ma
    last: Ma
  - first: Lu
    full: Lu Zong
    id: lu-zong
    last: Zong
  - first: Yikang
    full: Yikang Yang
    id: yikang-yang
    last: Yang
  - first: Jionglong
    full: Jionglong Su
    id: jionglong-su
    last: Su
  author_string: Ye Ma, Lu Zong, Yikang Yang, Jionglong Su
  bibkey: ma-etal-2019-news2vec
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1490
  month: November
  page_first: '4843'
  page_last: '4852'
  pages: "4843\u20134852"
  paper_id: '490'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1490.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1490.jpg
  title: 'News2vec: News Network Embedding with Subnode Information'
  title_html: '<span class="acl-fixed-case">N</span>ews2vec: News Network Embedding
    with Subnode Information'
  url: https://www.aclweb.org/anthology/D19-1490
  year: '2019'
D19-1491:
  abstract: This paper presents a novel architecture for recursive context-aware lexical
    simplification, REC-LS, that is capable of (1) making use of the wider context
    when detecting the words in need of simplification and suggesting alternatives,
    and (2) taking previous simplification steps into account. We show that our system
    outputs lexical simplifications that are grammatically correct and semantically
    appropriate, and outperforms the current state-of-the-art systems in lexical simplification.
  address: Hong Kong, China
  attachment:
  - filename: D19-1491.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1491.Attachment.pdf
  author:
  - first: Sian
    full: Sian Gooding
    id: sian-gooding
    last: Gooding
  - first: Ekaterina
    full: Ekaterina Kochmar
    id: ekaterina-kochmar
    last: Kochmar
  author_string: Sian Gooding, Ekaterina Kochmar
  bibkey: gooding-kochmar-2019-recursive
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1491
  month: November
  page_first: '4853'
  page_last: '4863'
  pages: "4853\u20134863"
  paper_id: '491'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1491.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1491.jpg
  title: Recursive Context-Aware Lexical Simplification
  title_html: Recursive Context-Aware Lexical Simplification
  url: https://www.aclweb.org/anthology/D19-1491
  year: '2019'
D19-1492:
  abstract: "Electronic Health Records (EHRs) contain both structured content and\
    \ unstructured (text) content about a patient\u2019s medical history. In the unstructured\
    \ text parts, there are common sections such as Assessment and Plan, Social History,\
    \ and Medications. These sections help physicians find information easily and\
    \ can be used by an information retrieval system to return specific information\
    \ sought by a user. However, it is common that the exact format of sections in\
    \ a particular EHR does not adhere to known patterns. Therefore, being able to\
    \ predict sections and headers in EHRs automatically is beneficial to physicians.\
    \ Prior approaches in EHR section prediction have only used text data from EHRs\
    \ and have required significant manual annotation. We propose using sections from\
    \ medical literature (e.g., textbooks, journals, web content) that contain content\
    \ similar to that found in EHR sections. Our approach uses data from a different\
    \ kind of source where labels are provided without the need of a time-consuming\
    \ annotation effort. We use this data to train two models: an RNN and a BERT-based\
    \ model. We apply the learned models along with source data via transfer learning\
    \ to predict sections in EHRs. Our results show that medical literature can provide\
    \ helpful supervision signal for this classification task."
  address: Hong Kong, China
  author:
  - first: Sara
    full: Sara Rosenthal
    id: sara-rosenthal
    last: Rosenthal
  - first: Ken
    full: Ken Barker
    id: ken-barker
    last: Barker
  - first: Zhicheng
    full: Zhicheng Liang
    id: zhicheng-liang
    last: Liang
  author_string: Sara Rosenthal, Ken Barker, Zhicheng Liang
  bibkey: rosenthal-etal-2019-leveraging
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1492
  month: November
  page_first: '4864'
  page_last: '4873'
  pages: "4864\u20134873"
  paper_id: '492'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1492.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1492.jpg
  title: Leveraging Medical Literature for Section Prediction in Electronic Health
    Records
  title_html: Leveraging Medical Literature for Section Prediction in Electronic Health
    Records
  url: https://www.aclweb.org/anthology/D19-1492
  year: '2019'
D19-1493:
  abstract: "News recommendation is important for online news platforms to help users\
    \ find interested news and alleviate information overload. Existing news recommendation\
    \ methods usually rely on the news click history to model user interest. However,\
    \ these methods may suffer from the data sparsity problem, since the news click\
    \ behaviors of many users in online news platforms are usually very limited. Fortunately,\
    \ some other kinds of user behaviors such as webpage browsing and search queries\
    \ can also provide useful clues of users\u2019 news reading interest. In this\
    \ paper, we propose a neural news recommendation approach which can exploit heterogeneous\
    \ user behaviors. Our approach contains two major modules, i.e., news representation\
    \ and user representation. In the news representation module, we learn representations\
    \ of news from their titles via CNN networks, and apply attention networks to\
    \ select important words. In the user representation module, we propose an attentive\
    \ multi-view learning framework to learn unified representations of users from\
    \ their heterogeneous behaviors such as search queries, clicked news and browsed\
    \ webpages. In addition, we use word- and record-level attentions to select informative\
    \ words and behavior records. Experiments on a real-world dataset validate the\
    \ effectiveness of our approach."
  address: Hong Kong, China
  author:
  - first: Chuhan
    full: Chuhan Wu
    id: chuhan-wu
    last: Wu
  - first: Fangzhao
    full: Fangzhao Wu
    id: fangzhao-wu
    last: Wu
  - first: Mingxiao
    full: Mingxiao An
    id: mingxiao-an
    last: An
  - first: Tao
    full: Tao Qi
    id: tao-qi
    last: Qi
  - first: Jianqiang
    full: Jianqiang Huang
    id: jianqiang-huang
    last: Huang
  - first: Yongfeng
    full: Yongfeng Huang
    id: yongfeng-huang
    last: Huang
  - first: Xing
    full: Xing Xie
    id: xing-xie
    last: Xie
  author_string: Chuhan Wu, Fangzhao Wu, Mingxiao An, Tao Qi, Jianqiang Huang, Yongfeng
    Huang, Xing Xie
  bibkey: wu-etal-2019-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1493
  month: November
  page_first: '4874'
  page_last: '4883'
  pages: "4874\u20134883"
  paper_id: '493'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1493.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1493.jpg
  title: Neural News Recommendation with Heterogeneous User Behavior
  title_html: Neural News Recommendation with Heterogeneous User Behavior
  url: https://www.aclweb.org/anthology/D19-1493
  year: '2019'
D19-1494:
  abstract: User and item representation learning is critical for recommendation.
    Many of existing recommendation methods learn representations of users and items
    based on their ratings and reviews. However, the user-user and item-item relatedness
    are usually not considered in these methods, which may be insufficient. In this
    paper, we propose a neural recommendation approach which can utilize useful information
    from both review content and user-item graphs. Since reviews and graphs have different
    characteristics, we propose to use a multi-view learning framework to incorporate
    them as different views. In the review content-view, we propose to use a hierarchical
    model to first learn sentence representations from words, then learn review representations
    from sentences, and finally learn user/item representations from reviews. In addition,
    we propose to incorporate a three-level attention network into this view to select
    important words, sentences and reviews for learning informative user and item
    representations. In the graph-view, we propose a hierarchical graph neural network
    to jointly model the user-item, user-user and item-item relatedness by capturing
    the first- and second-order interactions between users and items in the user-item
    graph. In addition, we apply attention mechanism to model the importance of these
    interactions to learn informative user and item representations. Extensive experiments
    on four benchmark datasets validate the effectiveness of our approach.
  address: Hong Kong, China
  author:
  - first: Chuhan
    full: Chuhan Wu
    id: chuhan-wu
    last: Wu
  - first: Fangzhao
    full: Fangzhao Wu
    id: fangzhao-wu
    last: Wu
  - first: Tao
    full: Tao Qi
    id: tao-qi
    last: Qi
  - first: Suyu
    full: Suyu Ge
    id: suyu-ge
    last: Ge
  - first: Yongfeng
    full: Yongfeng Huang
    id: yongfeng-huang
    last: Huang
  - first: Xing
    full: Xing Xie
    id: xing-xie
    last: Xie
  author_string: Chuhan Wu, Fangzhao Wu, Tao Qi, Suyu Ge, Yongfeng Huang, Xing Xie
  bibkey: wu-etal-2019-reviews
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1494
  month: November
  page_first: '4884'
  page_last: '4893'
  pages: "4884\u20134893"
  paper_id: '494'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1494.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1494.jpg
  title: 'Reviews Meet Graphs: Enhancing User and Item Representations for Recommendation
    with Hierarchical Attentive Graph Neural Network'
  title_html: 'Reviews Meet Graphs: Enhancing User and Item Representations for Recommendation
    with Hierarchical Attentive Graph Neural Network'
  url: https://www.aclweb.org/anthology/D19-1494
  year: '2019'
D19-1495:
  abstract: Prior work has proposed effective methods to learn event representations
    that can capture syntactic and semantic information over text corpus, demonstrating
    their effectiveness for downstream tasks such as script event prediction. On the
    other hand, events extracted from raw texts lacks of commonsense knowledge, such
    as the intents and emotions of the event participants, which are useful for distinguishing
    event pairs when there are only subtle differences in their surface realizations.
    To address this issue, this paper proposes to leverage external commonsense knowledge
    about the intent and sentiment of the event. Experiments on three event-related
    tasks, i.e., event similarity, script event prediction and stock market prediction,
    show that our model obtains much better event embeddings for the tasks, achieving
    78% improvements on hard similarity task, yielding more precise inferences on
    subsequent events under given contexts, and better accuracies in predicting the
    volatilities of the stock market.
  address: Hong Kong, China
  author:
  - first: Xiao
    full: Xiao Ding
    id: xiao-ding
    last: Ding
  - first: Kuo
    full: Kuo Liao
    id: kuo-liao
    last: Liao
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  - first: Zhongyang
    full: Zhongyang Li
    id: zhongyang-li
    last: Li
  - first: Junwen
    full: Junwen Duan
    id: junwen-duan
    last: Duan
  author_string: Xiao Ding, Kuo Liao, Ting Liu, Zhongyang Li, Junwen Duan
  bibkey: ding-etal-2019-event-representation
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1495
  month: November
  page_first: '4894'
  page_last: '4903'
  pages: "4894\u20134903"
  paper_id: '495'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1495.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1495.jpg
  title: Event Representation Learning Enhanced with External Commonsense Knowledge
  title_html: Event Representation Learning Enhanced with External Commonsense Knowledge
  url: https://www.aclweb.org/anthology/D19-1495
  year: '2019'
D19-1496:
  abstract: Adversarial attacks against machine learning models have threatened various
    real-world applications such as spam filtering and sentiment analysis. In this
    paper, we propose a novel framework, learning to discriminate perturbations (DISP),
    to identify and adjust malicious perturbations, thereby blocking adversarial attacks
    for text classification models. To identify adversarial attacks, a perturbation
    discriminator validates how likely a token in the text is perturbed and provides
    a set of potential perturbations. For each potential perturbation, an embedding
    estimator learns to restore the embedding of the original word based on the context
    and a replacement token is chosen based on approximate kNN search. DISP can block
    adversarial attacks for any NLP model without modifying the model structure or
    training procedure. Extensive experiments on two benchmark datasets demonstrate
    that DISP significantly outperforms baseline methods in blocking adversarial attacks
    for text classification. In addition, in-depth analysis shows the robustness of
    DISP across different situations.
  address: Hong Kong, China
  author:
  - first: Yichao
    full: Yichao Zhou
    id: yichao-zhou
    last: Zhou
  - first: Jyun-Yu
    full: Jyun-Yu Jiang
    id: jyun-yu-jiang
    last: Jiang
  - first: Kai-Wei
    full: Kai-Wei Chang
    id: kai-wei-chang
    last: Chang
  - first: Wei
    full: Wei Wang
    id: wei-wang
    last: Wang
  author_string: Yichao Zhou, Jyun-Yu Jiang, Kai-Wei Chang, Wei Wang
  bibkey: zhou-etal-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1496
  month: November
  page_first: '4904'
  page_last: '4913'
  pages: "4904\u20134913"
  paper_id: '496'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1496.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1496.jpg
  title: Learning to Discriminate Perturbations for Blocking Adversarial Attacks in
    Text Classification
  title_html: Learning to Discriminate Perturbations for Blocking Adversarial Attacks
    in Text Classification
  url: https://www.aclweb.org/anthology/D19-1496
  year: '2019'
D19-1497:
  abstract: Citation count prediction (CCP) has been an important research task for
    automatically estimating the future impact of a scholarly paper. Previous studies
    mainly focus on extracting or mining useful features from the paper itself or
    the associated authors. An important kind of data signals, peer review text, has
    not been utilized for the CCP task. In this paper, we take the initiative to utilize
    peer review data for the CCP task with a neural prediction model. Our focus is
    to learn a comprehensive semantic representation for peer review text for improving
    the prediction performance. To achieve this goal, we incorporate the abstract-review
    match mechanism and the cross-review match mechanism to learn deep features from
    peer review text. We also consider integrating hand-crafted features via a wide
    component. The deep and wide components jointly make the prediction. Extensive
    experiments have demonstrated the usefulness of the peer review data and the effectiveness
    of the proposed model. Our dataset has been released online.
  address: Hong Kong, China
  author:
  - first: Siqing
    full: Siqing Li
    id: siqing-li
    last: Li
  - first: Wayne Xin
    full: Wayne Xin Zhao
    id: wayne-xin-zhao
    last: Zhao
  - first: Eddy Jing
    full: Eddy Jing Yin
    id: eddy-jing-yin
    last: Yin
  - first: Ji-Rong
    full: Ji-Rong Wen
    id: ji-rong-wen
    last: Wen
  author_string: Siqing Li, Wayne Xin Zhao, Eddy Jing Yin, Ji-Rong Wen
  bibkey: li-etal-2019-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1497
  month: November
  page_first: '4914'
  page_last: '4924'
  pages: "4914\u20134924"
  paper_id: '497'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1497.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1497.jpg
  title: A Neural Citation Count Prediction Model based on Peer Review Text
  title_html: A Neural Citation Count Prediction Model based on Peer Review Text
  url: https://www.aclweb.org/anthology/D19-1497
  year: '2019'
D19-1498:
  abstract: Document-level relation extraction is a complex human process that requires
    logical inference to extract relationships between named entities in text. Existing
    approaches use graph-based neural models with words as nodes and edges as relations
    between them, to encode relations across sentences. These models are node-based,
    i.e., they form pair representations based solely on the two target node representations.
    However, entity relations can be better expressed through unique edge representations
    formed as paths between nodes. We thus propose an edge-oriented graph neural model
    for document-level relation extraction. The model utilises different types of
    nodes and edges to create a document-level graph. An inference mechanism on the
    graph edges enables to learn intra- and inter-sentence relations using multi-instance
    learning internally. Experiments on two document-level biomedical datasets for
    chemical-disease and gene-disease associations show the usefulness of the proposed
    edge-oriented approach.
  address: Hong Kong, China
  author:
  - first: Fenia
    full: Fenia Christopoulou
    id: fenia-christopoulou
    last: Christopoulou
  - first: Makoto
    full: Makoto Miwa
    id: makoto-miwa
    last: Miwa
  - first: Sophia
    full: Sophia Ananiadou
    id: sophia-ananiadou
    last: Ananiadou
  author_string: Fenia Christopoulou, Makoto Miwa, Sophia Ananiadou
  bibkey: christopoulou-etal-2019-connecting
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1498
  month: November
  page_first: '4925'
  page_last: '4936'
  pages: "4925\u20134936"
  paper_id: '498'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1498.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1498.jpg
  title: 'Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented
    Graphs'
  title_html: 'Connecting the Dots: Document-level Neural Relation Extraction with
    Edge-oriented Graphs'
  url: https://www.aclweb.org/anthology/D19-1498
  year: '2019'
D19-1499:
  abstract: Text style transfer task requires the model to transfer a sentence of
    one style to another style while retaining its original content meaning, which
    is a challenging problem that has long suffered from the shortage of parallel
    data. In this paper, we first propose a semi-supervised text style transfer model
    that combines the small-scale parallel data with the large-scale nonparallel data.
    With these two types of training data, we introduce a projection function between
    the latent space of different styles and design two constraints to train it. We
    also introduce two other simple but effective semi-supervised methods to compare
    with. To evaluate the performance of the proposed methods, we build and release
    a novel style transfer dataset that alters sentences between the style of ancient
    Chinese poem and the modern Chinese.
  address: Hong Kong, China
  author:
  - first: Mingyue
    full: Mingyue Shang
    id: mingyue-shang
    last: Shang
  - first: Piji
    full: Piji Li
    id: piji-li
    last: Li
  - first: Zhenxin
    full: Zhenxin Fu
    id: zhenxin-fu
    last: Fu
  - first: Lidong
    full: Lidong Bing
    id: lidong-bing
    last: Bing
  - first: Dongyan
    full: Dongyan Zhao
    id: dongyan-zhao
    last: Zhao
  - first: Shuming
    full: Shuming Shi
    id: shuming-shi
    last: Shi
  - first: Rui
    full: Rui Yan
    id: rui-yan
    last: Yan
  author_string: Mingyue Shang, Piji Li, Zhenxin Fu, Lidong Bing, Dongyan Zhao, Shuming
    Shi, Rui Yan
  bibkey: shang-etal-2019-semi
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1499
  month: November
  page_first: '4937'
  page_last: '4946'
  pages: "4937\u20134946"
  paper_id: '499'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1499.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1499.jpg
  title: 'Semi-supervised Text Style Transfer: Cross Projection in Latent Space'
  title_html: 'Semi-supervised Text Style Transfer: Cross Projection in Latent Space'
  url: https://www.aclweb.org/anthology/D19-1499
  year: '2019'
D19-1500:
  abstract: Privacy policies are long and complex documents that are difficult for
    users to read and understand. Yet, they have legal effects on how user data can
    be collected, managed and used. Ideally, we would like to empower users to inform
    themselves about the issues that matter to them, and enable them to selectively
    explore these issues. We present PrivacyQA, a corpus consisting of 1750 questions
    about the privacy policies of mobile applications, and over 3500 expert annotations
    of relevant answers. We observe that a strong neural baseline underperforms human
    performance by almost 0.3 F1 on PrivacyQA, suggesting considerable room for improvement
    for future systems. Further, we use this dataset to categorically identify challenges
    to question answerability, with domain-general implications for any question answering
    system. The PrivacyQA corpus offers a challenging corpus for question answering,
    with genuine real world utility.
  address: Hong Kong, China
  author:
  - first: Abhilasha
    full: Abhilasha Ravichander
    id: abhilasha-ravichander
    last: Ravichander
  - first: Alan W
    full: Alan W Black
    id: alan-w-black
    last: Black
  - first: Shomir
    full: Shomir Wilson
    id: shomir-wilson
    last: Wilson
  - first: Thomas
    full: Thomas Norton
    id: thomas-norton
    last: Norton
  - first: Norman
    full: Norman Sadeh
    id: norman-sadeh
    last: Sadeh
  author_string: Abhilasha Ravichander, Alan W Black, Shomir Wilson, Thomas Norton,
    Norman Sadeh
  bibkey: ravichander-etal-2019-question
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1500
  month: November
  page_first: '4947'
  page_last: '4958'
  pages: "4947\u20134958"
  paper_id: '500'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1500.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1500.jpg
  title: 'Question Answering for Privacy Policies: Combining Computational and Legal
    Perspectives'
  title_html: 'Question Answering for Privacy Policies: Combining Computational and
    Legal Perspectives'
  url: https://www.aclweb.org/anthology/D19-1500
  year: '2019'
D19-1501:
  abstract: Different from other text generation tasks, in product description generation,
    it is of vital importance to generate faithful descriptions that stick to the
    product attribute information. However, little attention has been paid to this
    problem. To bridge this gap we propose a model named Fidelity-oriented Product
    Description Generator (FPDG). FPDG takes the entity label of each word into account,
    since the product attribute information is always conveyed by entity words. Specifically,
    we first propose a Recurrent Neural Network (RNN) decoder based on the Entity-label-guided
    Long Short-Term Memory (ELSTM) cell, taking both the embedding and the entity
    label of each word as input. Second, we establish a keyword memory that stores
    the entity labels as keys and keywords as values, and FPDG will attend to keywords
    through attending to their entity labels. Experiments conducted a large-scale
    real-world product description dataset show that our model achieves the state-of-the-art
    performance in terms of both traditional generation metrics as well as human evaluations.
    Specifically, FPDG increases the fidelity of the generated descriptions by 25%.
  address: Hong Kong, China
  author:
  - first: Zhangming
    full: Zhangming Chan
    id: zhangming-chan
    last: Chan
  - first: Xiuying
    full: Xiuying Chen
    id: xiuying-chen
    last: Chen
  - first: Yongliang
    full: Yongliang Wang
    id: yongliang-wang
    last: Wang
  - first: Juntao
    full: Juntao Li
    id: juntao-li
    last: Li
  - first: Zhiqiang
    full: Zhiqiang Zhang
    id: zhiqiang-zhang
    last: Zhang
  - first: Kun
    full: Kun Gai
    id: kun-gai
    last: Gai
  - first: Dongyan
    full: Dongyan Zhao
    id: dongyan-zhao
    last: Zhao
  - first: Rui
    full: Rui Yan
    id: rui-yan
    last: Yan
  author_string: Zhangming Chan, Xiuying Chen, Yongliang Wang, Juntao Li, Zhiqiang
    Zhang, Kun Gai, Dongyan Zhao, Rui Yan
  bibkey: chan-etal-2019-stick
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1501
  month: November
  page_first: '4959'
  page_last: '4968'
  pages: "4959\u20134968"
  paper_id: '501'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1501.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1501.jpg
  title: 'Stick to the Facts: Learning towards a Fidelity-oriented E-Commerce Product
    Description Generation'
  title_html: 'Stick to the Facts: Learning towards a Fidelity-oriented E-Commerce
    Product Description Generation'
  url: https://www.aclweb.org/anthology/D19-1501
  year: '2019'
D19-1502:
  abstract: This paper addresses the problem of inferring the fine-grained type of
    an entity from a knowledge base. We convert this problem into the task of graph-based
    semi-supervised classification, and propose Hierarchical Multi Graph Convolutional
    Network (HMGCN), a novel Deep Learning architecture to tackle this problem. We
    construct three kinds of connectivity matrices to capture different kinds of semantic
    correlations between entities. A recursive regularization is proposed to model
    the subClassOf relations between types in given type hierarchy. Extensive experiments
    with two large-scale public datasets show that our proposed method significantly
    outperforms four state-of-the-art methods.
  address: Hong Kong, China
  author:
  - first: Hailong
    full: Hailong Jin
    id: hailong-jin
    last: Jin
  - first: Lei
    full: Lei Hou
    id: lei-hou
    last: Hou
  - first: Juanzi
    full: Juanzi Li
    id: juanzi-li
    last: Li
  - first: Tiansi
    full: Tiansi Dong
    id: tiansi-dong
    last: Dong
  author_string: Hailong Jin, Lei Hou, Juanzi Li, Tiansi Dong
  bibkey: jin-etal-2019-fine
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1502
  month: November
  page_first: '4969'
  page_last: '4978'
  pages: "4969\u20134978"
  paper_id: '502'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1502.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1502.jpg
  title: Fine-Grained Entity Typing via Hierarchical Multi Graph Convolutional Networks
  title_html: Fine-Grained Entity Typing via Hierarchical Multi Graph Convolutional
    Networks
  url: https://www.aclweb.org/anthology/D19-1502
  year: '2019'
D19-1503:
  abstract: Recently we proposed the Span Attribute Tagging (SAT) Model to infer clinical
    entities (e.g., symptoms) and their properties (e.g., duration). It tackles the
    challenge of large label space and limited training data using a hierarchical
    two-stage approach that identifies the span of interest in a tagging step and
    assigns labels to the span in a classification step. We extend the SAT model to
    jointly infer not only entities and their properties but also relations between
    them. Most relation extraction models restrict inferring relations between tokens
    within a few neighboring sentences, mainly to avoid high computational complexity.
    In contrast, our proposed Relation-SAT (R-SAT) model is computationally efficient
    and can infer relations over the entire conversation, spanning an average duration
    of 10 minutes. We evaluate our model on a corpus of clinical conversations. When
    the entities are given, the R-SAT outperforms baselines in identifying relations
    between symptoms and their properties by about 32% (0.82 vs 0.62 F-score) and
    by about 50% (0.60 vs 0.41 F-score) on medications and their properties. On the
    more difficult task of jointly inferring entities and relations, the R-SAT model
    achieves a performance of 0.34 and 0.45 for symptoms and medications respectively,
    which is significantly better than 0.18 and 0.35 for the baseline model. The contributions
    of different components of the model are quantified using ablation analysis.
  address: Hong Kong, China
  author:
  - first: Nan
    full: Nan Du
    id: nan-du
    last: Du
  - first: Mingqiu
    full: Mingqiu Wang
    id: mingqiu-wang
    last: Wang
  - first: Linh
    full: Linh Tran
    id: linh-tran
    last: Tran
  - first: Gang
    full: Gang Lee
    id: gang-lee
    last: Lee
  - first: Izhak
    full: Izhak Shafran
    id: izhak-shafran
    last: Shafran
  author_string: Nan Du, Mingqiu Wang, Linh Tran, Gang Lee, Izhak Shafran
  bibkey: du-etal-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1503
  month: November
  page_first: '4979'
  page_last: '4990'
  pages: "4979\u20134990"
  paper_id: '503'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1503.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1503.jpg
  title: Learning to Infer Entities, Properties and their Relations from Clinical
    Conversations
  title_html: Learning to Infer Entities, Properties and their Relations from Clinical
    Conversations
  url: https://www.aclweb.org/anthology/D19-1503
  year: '2019'
D19-1504:
  abstract: Despite great scalability on large data and their ability to understand
    correlations between topics, spectral topic models have not been widely used due
    to the absence of reliability in real data and lack of practical implementations.
    This paper aims to solidify the foundations of spectral topic inference and provide
    a practical implementation for anchor-based topic modeling. Beginning with vocabulary
    curation, we scrutinize every single inference step with other viable options.
    We also evaluate our matrix-based approach against popular alternatives including
    a tensor-based spectral method as well as probabilistic algorithms. Our quantitative
    and qualitative experiments demonstrate the power of Rectified Anchor Word algorithm
    in various real datasets, providing a complete guide to practical correlated topic
    modeling.
  address: Hong Kong, China
  attachment:
  - filename: D19-1504.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1504.Attachment.zip
  author:
  - first: Moontae
    full: Moontae Lee
    id: moontae-lee
    last: Lee
  - first: Sungjun
    full: Sungjun Cho
    id: sungjun-cho
    last: Cho
  - first: David
    full: David Bindel
    id: david-bindel
    last: Bindel
  - first: David
    full: David Mimno
    id: david-mimno
    last: Mimno
  author_string: Moontae Lee, Sungjun Cho, David Bindel, David Mimno
  bibkey: lee-etal-2019-practical
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1504
  month: November
  page_first: '4991'
  page_last: '5001'
  pages: "4991\u20135001"
  paper_id: '504'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1504.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1504.jpg
  title: Practical Correlated Topic Modeling and Analysis via the Rectified Anchor
    Word Algorithm
  title_html: Practical Correlated Topic Modeling and Analysis via the Rectified Anchor
    Word Algorithm
  url: https://www.aclweb.org/anthology/D19-1504
  year: '2019'
D19-1505:
  abstract: "Management of collaborative documents can be difficult, given the profusion\
    \ of edits and comments that multiple authors make during a document\u2019s evolution.\
    \ Reliably modeling the relationship between edits and comments is a crucial step\
    \ towards helping the user keep track of a document in flux. A number of authoring\
    \ tasks, such as categorizing and summarizing edits, detecting completed to-dos,\
    \ and visually rearranging comments could benefit from such a contribution. Thus,\
    \ in this paper we explore the relationship between comments and edits by defining\
    \ two novel, related tasks: Comment Ranking and Edit Anchoring. We begin by collecting\
    \ a dataset with more than half a million comment-edit pairs based on Wikipedia\
    \ revision histories. We then propose a hierarchical multi-layer deep neural-network\
    \ to model the relationship between edits and comments. Our architecture tackles\
    \ both Comment Ranking and Edit Anchoring tasks by encoding specific edit actions\
    \ such as additions and deletions, while also accounting for document context.\
    \ In a number of evaluation settings, our experimental results show that our approach\
    \ outperforms several strong baselines significantly. We are able to achieve a\
    \ precision@1 of 71.0% and a precision@3 of 94.4% for Comment Ranking, while we\
    \ achieve 74.4% accuracy on Edit Anchoring."
  address: Hong Kong, China
  author:
  - first: Xuchao
    full: Xuchao Zhang
    id: xuchao-zhang
    last: Zhang
  - first: Dheeraj
    full: Dheeraj Rajagopal
    id: dheeraj-rajagopal
    last: Rajagopal
  - first: Michael
    full: Michael Gamon
    id: michael-gamon
    last: Gamon
  - first: Sujay Kumar
    full: Sujay Kumar Jauhar
    id: sujay-kumar-jauhar
    last: Jauhar
  - first: ChangTien
    full: ChangTien Lu
    id: changtien-lu
    last: Lu
  author_string: Xuchao Zhang, Dheeraj Rajagopal, Michael Gamon, Sujay Kumar Jauhar,
    ChangTien Lu
  bibkey: zhang-etal-2019-modeling
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1505
  month: November
  page_first: '5002'
  page_last: '5011'
  pages: "5002\u20135011"
  paper_id: '505'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1505.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1505.jpg
  title: Modeling the Relationship between User Comments and Edits in Document Revision
  title_html: Modeling the Relationship between User Comments and Edits in Document
    Revision
  url: https://www.aclweb.org/anthology/D19-1505
  year: '2019'
D19-1506:
  abstract: Recently, there has been a great interest in the development of small
    and accurate neural networks that run entirely on devices such as mobile phones,
    smart watches and IoT. This enables user privacy, consistent user experience and
    low latency. Although a wide range of applications have been targeted from wake
    word detection to short text classification, yet there are no on-device networks
    for long text classification. We propose a novel projection attention neural network
    PRADO that combines trainable projections with attention and convolutions. We
    evaluate our approach on multiple large document text classification tasks. Our
    results show the effectiveness of the trainable projection model in finding semantically
    similar phrases and reaching high performance while maintaining compact size.
    Using this approach, we train tiny neural networks just 200 Kilobytes in size
    that improve over prior CNN and LSTM models and achieve near state of the art
    performance on multiple long document classification tasks. We also apply our
    model for transfer learning, show its robustness and ability to further improve
    the performance in limited data scenarios.
  address: Hong Kong, China
  author:
  - first: Prabhu
    full: Prabhu Kaliamoorthi
    id: prabhu-kaliamoorthi
    last: Kaliamoorthi
  - first: Sujith
    full: Sujith Ravi
    id: sujith-ravi
    last: Ravi
  - first: Zornitsa
    full: Zornitsa Kozareva
    id: zornitsa-kozareva
    last: Kozareva
  author_string: Prabhu Kaliamoorthi, Sujith Ravi, Zornitsa Kozareva
  bibkey: kaliamoorthi-etal-2019-prado
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1506
  month: November
  page_first: '5012'
  page_last: '5021'
  pages: "5012\u20135021"
  paper_id: '506'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1506.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1506.jpg
  title: 'PRADO: Projection Attention Networks for Document Classification On-Device'
  title_html: '<span class="acl-fixed-case">PRADO</span>: Projection Attention Networks
    for Document Classification On-Device'
  url: https://www.aclweb.org/anthology/D19-1506
  year: '2019'
D19-1507:
  abstract: Current neural query auto-completion (QAC) systems rely on character-level
    language models, but they slow down when queries are long. We present how to utilize
    subword language models for the fast and accurate generation of query completion
    candidates. Representing queries with subwords shorten a decoding length significantly.
    To deal with issues coming from introducing subword language model, we develop
    a retrace algorithm and a reranking method by approximate marginalization. As
    a result, our model achieves up to 2.5 times faster while maintaining a similar
    quality of generated results compared to the character-level baseline. Also, we
    propose a new evaluation metric, mean recoverable length (MRL), measuring how
    many upcoming characters the model could complete correctly. It provides more
    explicit meaning and eliminates the need for prefix length sampling for existing
    rank-based metrics. Moreover, we performed a comprehensive analysis with ablation
    study to figure out the importance of each component.
  address: Hong Kong, China
  author:
  - first: Gyuwan
    full: Gyuwan Kim
    id: gyuwan-kim
    last: Kim
  author_string: Gyuwan Kim
  bibkey: kim-2019-subword
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1507
  month: November
  page_first: '5022'
  page_last: '5032'
  pages: "5022\u20135032"
  paper_id: '507'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1507.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1507.jpg
  title: Subword Language Model for Query Auto-Completion
  title_html: Subword Language Model for Query Auto-Completion
  url: https://www.aclweb.org/anthology/D19-1507
  year: '2019'
D19-1508:
  abstract: Symptom diagnosis is a challenging yet profound problem in natural language
    processing. Most previous research focus on investigating the standard electronic
    medical records for symptom diagnosis, while the dialogues between doctors and
    patients that contain more rich information are not well studied. In this paper,
    we first construct a dialogue symptom diagnosis dataset based on an online medical
    forum with a large amount of dialogues between patients and doctors. Then, we
    provide some benchmark models on this dataset to boost the research of dialogue
    symptom diagnosis. In order to further enhance the performance of symptom diagnosis
    over dialogues, we propose a global attention mechanism to capture more symptom
    related information, and build a symptom graph to model the associations between
    symptoms rather than treating each symptom independently. Experimental results
    show that both the global attention and symptom graph are effective to boost dialogue
    symptom diagnosis. In particular, our proposed model achieves the state-of-the-art
    performance on the constructed dataset.
  address: Hong Kong, China
  author:
  - first: Xinzhu
    full: Xinzhu Lin
    id: xinzhu-lin
    last: Lin
  - first: Xiahui
    full: Xiahui He
    id: xiahui-he
    last: He
  - first: Qin
    full: Qin Chen
    id: qin-chen
    last: Chen
  - first: Huaixiao
    full: Huaixiao Tou
    id: huaixiao-tou
    last: Tou
  - first: Zhongyu
    full: Zhongyu Wei
    id: zhongyu-wei
    last: Wei
  - first: Ting
    full: Ting Chen
    id: ting-chen
    last: Chen
  author_string: Xinzhu Lin, Xiahui He, Qin Chen, Huaixiao Tou, Zhongyu Wei, Ting
    Chen
  bibkey: lin-etal-2019-enhancing
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1508
  month: November
  page_first: '5033'
  page_last: '5042'
  pages: "5033\u20135042"
  paper_id: '508'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1508.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1508.jpg
  title: Enhancing Dialogue Symptom Diagnosis with Global Attention and Symptom Graph
  title_html: Enhancing Dialogue Symptom Diagnosis with Global Attention and Symptom
    Graph
  url: https://www.aclweb.org/anthology/D19-1508
  year: '2019'
D19-1509:
  abstract: "Counterfactual reasoning requires predicting how alternative events,\
    \ contrary to what actually happened, might have resulted in different outcomes.\
    \ Despite being considered a necessary component of AI-complete systems, few resources\
    \ have been developed for evaluating counterfactual reasoning in narratives. In\
    \ this paper, we propose Counterfactual Story Rewriting: given an original story\
    \ and an intervening counterfactual event, the task is to minimally revise the\
    \ story to make it compatible with the given counterfactual event. Solving this\
    \ task will require deep understanding of causal narrative chains and counterfactual\
    \ invariance, and integration of such story reasoning capabilities into conditional\
    \ language generation models. We present TIMETRAVEL, a new dataset of 29,849 counterfactual\
    \ rewritings, each with the original story, a counterfactual event, and human-generated\
    \ revision of the original story compatible with the counterfactual event. Additionally,\
    \ we include 81,407 counterfactual \u201Cbranches\u201D without a rewritten storyline\
    \ to support future work on semi- or un-supervised approaches to counterfactual\
    \ story rewriting. Finally, we evaluate the counterfactual rewriting capacities\
    \ of several competitive baselines based on pretrained language models, and assess\
    \ whether common overlap and model-based automatic metrics for text generation\
    \ correlate well with human scores for counterfactual rewriting."
  address: Hong Kong, China
  attachment:
  - filename: D19-1509.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1509.Attachment.pdf
  author:
  - first: Lianhui
    full: Lianhui Qin
    id: lianhui-qin
    last: Qin
  - first: Antoine
    full: Antoine Bosselut
    id: antoine-bosselut
    last: Bosselut
  - first: Ari
    full: Ari Holtzman
    id: ari-holtzman
    last: Holtzman
  - first: Chandra
    full: Chandra Bhagavatula
    id: chandra-bhagavatula
    last: Bhagavatula
  - first: Elizabeth
    full: Elizabeth Clark
    id: elizabeth-clark
    last: Clark
  - first: Yejin
    full: Yejin Choi
    id: yejin-choi
    last: Choi
  author_string: Lianhui Qin, Antoine Bosselut, Ari Holtzman, Chandra Bhagavatula,
    Elizabeth Clark, Yejin Choi
  bibkey: qin-etal-2019-counterfactual
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1509
  month: November
  page_first: '5043'
  page_last: '5053'
  pages: "5043\u20135053"
  paper_id: '509'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1509.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1509.jpg
  title: Counterfactual Story Reasoning and Generation
  title_html: Counterfactual Story Reasoning and Generation
  url: https://www.aclweb.org/anthology/D19-1509
  year: '2019'
D19-1510:
  abstract: 'We propose LaserTagger - a sequence tagging approach that casts text
    generation as a text editing task. Target texts are reconstructed from the inputs
    using three main edit operations: keeping a token, deleting it, and adding a phrase
    before the token. To predict the edit operations, we propose a novel model, which
    combines a BERT encoder with an autoregressive Transformer decoder. This approach
    is evaluated on English text on four tasks: sentence fusion, sentence splitting,
    abstractive summarization, and grammar correction. LaserTagger achieves new state-of-the-art
    results on three of these tasks, performs comparably to a set of strong seq2seq
    baselines with a large number of training examples, and outperforms them when
    the number of examples is limited. Furthermore, we show that at inference time
    tagging can be more than two orders of magnitude faster than comparable seq2seq
    models, making it more attractive for running in a live environment.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1510.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1510.Attachment.zip
  author:
  - first: Eric
    full: Eric Malmi
    id: eric-malmi
    last: Malmi
  - first: Sebastian
    full: Sebastian Krause
    id: sebastian-krause
    last: Krause
  - first: Sascha
    full: Sascha Rothe
    id: sascha-rothe
    last: Rothe
  - first: Daniil
    full: Daniil Mirylenka
    id: daniil-mirylenka
    last: Mirylenka
  - first: Aliaksei
    full: Aliaksei Severyn
    id: aliaksei-severyn
    last: Severyn
  author_string: Eric Malmi, Sebastian Krause, Sascha Rothe, Daniil Mirylenka, Aliaksei
    Severyn
  bibkey: malmi-etal-2019-encode
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1510
  month: November
  page_first: '5054'
  page_last: '5065'
  pages: "5054\u20135065"
  paper_id: '510'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1510.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1510.jpg
  title: 'Encode, Tag, Realize: High-Precision Text Editing'
  title_html: 'Encode, Tag, Realize: High-Precision Text Editing'
  url: https://www.aclweb.org/anthology/D19-1510
  year: '2019'
D19-1511:
  abstract: Generating intriguing question is a key step towards building human-like
    open-domain chatbots. Although some recent works have focused on this task, compared
    with questions raised by humans, significant gaps remain in maintaining semantic
    coherence with post, which may result in generating dull or deviated questions.
    We observe that the answer has strong semantic coherence to its question and post,
    which can be used to guide question generation. Thus, we devise two methods to
    further enhance semantic coherence between post and question under the guidance
    of answer. First, the coherence score between generated question and answer is
    used as the reward function in a reinforcement learning framework, to encourage
    the cases that are consistent with the answer in semantic. Second, we incorporate
    adversarial training to explicitly control question generation in the direction
    of question-answer coherence. Extensive experiments show that our two methods
    outperform state-of-the-art baseline algorithms with large margins in raising
    semantic coherent questions.
  address: Hong Kong, China
  author:
  - first: Weichao
    full: Weichao Wang
    id: weichao-wang
    last: Wang
  - first: Shi
    full: Shi Feng
    id: shi-feng
    last: Feng
  - first: Daling
    full: Daling Wang
    id: daling-wang
    last: Wang
  - first: Yifei
    full: Yifei Zhang
    id: yifei-zhang
    last: Zhang
  author_string: Weichao Wang, Shi Feng, Daling Wang, Yifei Zhang
  bibkey: wang-etal-2019-answer
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1511
  month: November
  page_first: '5066'
  page_last: '5076'
  pages: "5066\u20135076"
  paper_id: '511'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1511.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1511.jpg
  title: Answer-guided and Semantic Coherent Question Generation in Open-domain Conversation
  title_html: Answer-guided and Semantic Coherent Question Generation in Open-domain
    Conversation
  url: https://www.aclweb.org/anthology/D19-1511
  year: '2019'
D19-1512:
  abstract: "Automatic news comment generation is beneficial for real applications\
    \ but has not attracted enough attention from the research community. In this\
    \ paper, we propose a \u201Cread-attend-comment\u201D procedure for news comment\
    \ generation and formalize the procedure with a reading network and a generation\
    \ network. The reading network comprehends a news article and distills some important\
    \ points from it, then the generation network creates a comment by attending to\
    \ the extracted discrete points and the news title. We optimize the model in an\
    \ end-to-end manner by maximizing a variational lower bound of the true objective\
    \ using the back-propagation algorithm. Experimental results on two public datasets\
    \ indicate that our model can significantly outperform existing methods in terms\
    \ of both automatic evaluation and human judgment."
  address: Hong Kong, China
  author:
  - first: Ze
    full: Ze Yang
    id: ze-yang
    last: Yang
  - first: Can
    full: Can Xu
    id: can-xu
    last: Xu
  - first: Wei
    full: Wei Wu
    id: wei-wu
    last: Wu
  - first: Zhoujun
    full: Zhoujun Li
    id: zhoujun-li
    last: Li
  author_string: Ze Yang, Can Xu, Wei Wu, Zhoujun Li
  bibkey: yang-etal-2019-read
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1512
  month: November
  page_first: '5077'
  page_last: '5089'
  pages: "5077\u20135089"
  paper_id: '512'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1512.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1512.jpg
  title: 'Read, Attend and Comment: A Deep Architecture for Automatic News Comment
    Generation'
  title_html: 'Read, Attend and Comment: A Deep Architecture for Automatic News Comment
    Generation'
  url: https://www.aclweb.org/anthology/D19-1512
  year: '2019'
D19-1513:
  abstract: Text generation is among the most fundamental tasks in natural language
    processing. In this paper, we propose a text generation model that learns semantics
    and structural features simultaneously. This model captures structural features
    by a sequential variational autoencoder component and leverages a topic modeling
    component based on Gaussian distribution to enhance the recognition of text semantics.
    To make the reconstructed text more coherent to the topics, the model further
    adapts the encoder of the topic modeling component for a discriminator. The results
    of experiments over several datasets demonstrate that our model outperforms several
    states of the art models in terms of text perplexity and topic coherence. Moreover,
    the latent representations learned by our model is superior to others in a text
    classification task. Finally, given the input texts, our model can generate meaningful
    texts which hold similar structures but under different topics.
  address: Hong Kong, China
  attachment:
  - filename: D19-1513.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1513.Attachment.zip
  author:
  - first: Hongyin
    full: Hongyin Tang
    id: hongyin-tang
    last: Tang
  - first: Miao
    full: Miao Li
    id: miao-li
    last: Li
  - first: Beihong
    full: Beihong Jin
    id: beihong-jin
    last: Jin
  author_string: Hongyin Tang, Miao Li, Beihong Jin
  bibkey: tang-etal-2019-topic
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1513
  month: November
  page_first: '5090'
  page_last: '5099'
  pages: "5090\u20135099"
  paper_id: '513'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1513.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1513.jpg
  title: 'A Topic Augmented Text Generation Model: Joint Learning of Semantics and
    Structural Features'
  title_html: 'A Topic Augmented Text Generation Model: Joint Learning of Semantics
    and Structural Features'
  url: https://www.aclweb.org/anthology/D19-1513
  year: '2019'
D19-1514:
  abstract: 'Vision-and-language reasoning requires an understanding of visual concepts,
    language semantics, and, most importantly, the alignment and relationships between
    these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder
    Representations from Transformers) framework to learn these vision-and-language
    connections. In LXMERT, we build a large-scale Transformer model that consists
    of three encoders: an object relationship encoder, a language encoder, and a cross-modality
    encoder. Next, to endow our model with the capability of connecting vision and
    language semantics, we pre-train the model with large amounts of image-and-sentence
    pairs, via five diverse representative pre-training tasks: masked language modeling,
    masked object prediction (feature regression and label classification), cross-modality
    matching, and image question answering. These tasks help in learning both intra-modality
    and cross-modality relationships. After fine-tuning from our pre-trained parameters,
    our model achieves the state-of-the-art results on two visual question answering
    datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained
    cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2,
    and improve the previous best result by 22% absolute (54% to 76%). Lastly, we
    demonstrate detailed ablation studies to prove that both our novel model components
    and pre-training strategies significantly contribute to our strong results. Code
    and pre-trained models publicly available at: https://github.com/airsplay/lxmert'
  address: Hong Kong, China
  author:
  - first: Hao
    full: Hao Tan
    id: hao-tan
    last: Tan
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  author_string: Hao Tan, Mohit Bansal
  bibkey: tan-bansal-2019-lxmert
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1514
  month: November
  page_first: '5100'
  page_last: '5111'
  pages: "5100\u20135111"
  paper_id: '514'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1514.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1514.jpg
  title: 'LXMERT: Learning Cross-Modality Encoder Representations from Transformers'
  title_html: '<span class="acl-fixed-case">LXMERT</span>: Learning Cross-Modality
    Encoder Representations from Transformers'
  url: https://www.aclweb.org/anthology/D19-1514
  year: '2019'
D19-1515:
  abstract: The phrase grounding task aims to ground each entity mention in a given
    caption of an image to a corresponding region in that image. Although there are
    clear dependencies between how different mentions of the same caption should be
    grounded, previous structured prediction methods that aim to capture such dependencies
    need to resort to approximate inference or non-differentiable losses. In this
    paper, we formulate phrase grounding as a sequence labeling task where we treat
    candidate regions as potential labels, and use neural chain Conditional Random
    Fields (CRFs) to model dependencies among regions for adjacent mentions. In contrast
    to standard sequence labeling tasks, the phrase grounding task is defined such
    that there may be multiple correct candidate regions. To address this multiplicity
    of gold labels, we define so-called Soft-Label Chain CRFs, and present an algorithm
    that enables convenient end-to-end training. Our method establishes a new state-of-the-art
    on phrase grounding on the Flickr30k Entities dataset. Analysis shows that our
    model benefits both from the entity dependencies captured by the CRF and from
    the soft-label training regime. Our code is available at github.com/liujch1998/SoftLabelCCRF
  address: Hong Kong, China
  attachment:
  - filename: D19-1515.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1515.Attachment.zip
  author:
  - first: Jiacheng
    full: Jiacheng Liu
    id: jiacheng-liu
    last: Liu
  - first: Julia
    full: Julia Hockenmaier
    id: julia-hockenmaier
    last: Hockenmaier
  author_string: Jiacheng Liu, Julia Hockenmaier
  bibkey: liu-hockenmaier-2019-phrase
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1515
  month: November
  page_first: '5112'
  page_last: '5122'
  pages: "5112\u20135122"
  paper_id: '515'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1515.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1515.jpg
  title: Phrase Grounding by Soft-Label Chain Conditional Random Field
  title_html: Phrase Grounding by Soft-Label Chain Conditional Random Field
  url: https://www.aclweb.org/anthology/D19-1515
  year: '2019'
D19-1516:
  abstract: Grounding a pronoun to a visual object it refers to requires complex reasoning
    from various information sources, especially in conversational scenarios. For
    example, when people in a conversation talk about something all speakers can see,
    they often directly use pronouns (e.g., it) to refer to it without previous introduction.
    This fact brings a huge challenge for modern natural language understanding systems,
    particularly conventional context-based pronoun coreference models. To tackle
    this challenge, in this paper, we formally define the task of visual-aware pronoun
    coreference resolution (PCR) and introduce VisPro, a large-scale dialogue PCR
    dataset, to investigate whether and how the visual information can help resolve
    pronouns in dialogues. We then propose a novel visual-aware PCR model, VisCoref,
    for this task and conduct comprehensive experiments and case studies on our dataset.
    Results demonstrate the importance of the visual information in this PCR case
    and show the effectiveness of the proposed model.
  address: Hong Kong, China
  author:
  - first: Xintong
    full: Xintong Yu
    id: xintong-yu
    last: Yu
  - first: Hongming
    full: Hongming Zhang
    id: hongming-zhang
    last: Zhang
  - first: Yangqiu
    full: Yangqiu Song
    id: yangqiu-song
    last: Song
  - first: Yan
    full: Yan Song
    id: yan-song
    last: Song
  - first: Changshui
    full: Changshui Zhang
    id: changshui-zhang
    last: Zhang
  author_string: Xintong Yu, Hongming Zhang, Yangqiu Song, Yan Song, Changshui Zhang
  bibkey: yu-etal-2019-see
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1516
  month: November
  page_first: '5123'
  page_last: '5132'
  pages: "5123\u20135132"
  paper_id: '516'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1516.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1516.jpg
  title: 'What You See is What You Get: Visual Pronoun Coreference Resolution in Dialogues'
  title_html: 'What You See is What You Get: Visual Pronoun Coreference Resolution
    in Dialogues'
  url: https://www.aclweb.org/anthology/D19-1516
  year: '2019'
D19-1517:
  abstract: "Multimodal semantic comprehension has attracted increasing research interests\
    \ recently such as visual question answering and caption generation. However,\
    \ due to the data limitation, fine-grained semantic comprehension has not been\
    \ well investigated, which requires to capture semantic details of multimodal\
    \ contents. In this work, we introduce \u201CYouMakeup\u201D, a large-scale multimodal\
    \ instructional video dataset to support fine-grained semantic comprehension research\
    \ in specific domain. YouMakeup contains 2,800 videos from YouTube, spanning more\
    \ than 420 hours in total. Each video is annotated with a sequence of natural\
    \ language descriptions for instructional steps, grounded in temporal video range\
    \ and spatial facial areas. The annotated steps in a video involve subtle difference\
    \ in actions, products and regions, which requires fine-grained understanding\
    \ and reasoning both temporally and spatially. In order to evaluate models\u2019\
    \ ability for fined-grained comprehension, we further propose two groups of tasks\
    \ including generation tasks and visual question answering from different aspects.\
    \ We also establish a baseline of step caption generation for future comparison.\
    \ The dataset will be publicly available at https://github. com/AIM3-RUC/YouMakeup\
    \ to support research investigation in fine-grained semantic comprehension."
  address: Hong Kong, China
  author:
  - first: Weiying
    full: Weiying Wang
    id: weiying-wang
    last: Wang
  - first: Yongcheng
    full: Yongcheng Wang
    id: yongcheng-wang1
    last: Wang
  - first: Shizhe
    full: Shizhe Chen
    id: shizhe-chen
    last: Chen
  - first: Qin
    full: Qin Jin
    id: qin-jin
    last: Jin
  author_string: Weiying Wang, Yongcheng Wang, Shizhe Chen, Qin Jin
  bibkey: wang-etal-2019-youmakeup
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1517
  month: November
  page_first: '5133'
  page_last: '5143'
  pages: "5133\u20135143"
  paper_id: '517'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1517.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1517.jpg
  title: 'YouMakeup: A Large-Scale Domain-Specific Multimodal Dataset for Fine-Grained
    Semantic Comprehension'
  title_html: '<span class="acl-fixed-case">Y</span>ou<span class="acl-fixed-case">M</span>akeup:
    A Large-Scale Domain-Specific Multimodal Dataset for Fine-Grained Semantic Comprehension'
  url: https://www.aclweb.org/anthology/D19-1517
  year: '2019'
D19-1518:
  abstract: 'In this paper, we focus on natural language video localization: localizing
    (ie, grounding) a natural language description in a long and untrimmed video sequence.
    All currently published models for addressing this problem can be categorized
    into two types: (i) top-down approach: it does classification and regression for
    a set of pre-cut video segment candidates; (ii) bottom-up approach: it directly
    predicts probabilities for each video frame as the temporal boundaries (ie, start
    and end time point). However, both two approaches suffer several limitations:
    the former is computation-intensive for densely placed candidates, while the latter
    has trailed the performance of the top-down counterpart thus far. To this end,
    we propose a novel dense bottom-up framework: DEnse Bottom-Up Grounding (DEBUG).
    DEBUG regards all frames falling in the ground truth segment as foreground, and
    each foreground frame regresses the unique distances from its location to bi-directional
    ground truth boundaries. Extensive experiments on three challenging benchmarks
    (TACoS, Charades-STA, and ActivityNet Captions) show that DEBUG is able to match
    the speed of bottom-up models while surpassing the performance of the state-of-the-art
    top-down models.'
  address: Hong Kong, China
  author:
  - first: Chujie
    full: Chujie Lu
    id: chujie-lu
    last: Lu
  - first: Long
    full: Long Chen
    id: long-chen
    last: Chen
  - first: Chilie
    full: Chilie Tan
    id: chilie-tan
    last: Tan
  - first: Xiaolin
    full: Xiaolin Li
    id: xiaolin-li
    last: Li
  - first: Jun
    full: Jun Xiao
    id: jun-xiao
    last: Xiao
  author_string: Chujie Lu, Long Chen, Chilie Tan, Xiaolin Li, Jun Xiao
  bibkey: lu-etal-2019-debug
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1518
  month: November
  page_first: '5144'
  page_last: '5153'
  pages: "5144\u20135153"
  paper_id: '518'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1518.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1518.jpg
  title: 'DEBUG: A Dense Bottom-Up Grounding Approach for Natural Language Video Localization'
  title_html: '<span class="acl-fixed-case">DEBUG</span>: A Dense Bottom-Up Grounding
    Approach for Natural Language Video Localization'
  url: https://www.aclweb.org/anthology/D19-1518
  year: '2019'
D19-1519:
  abstract: Everyone makes mistakes. So do human annotators when curating labels for
    named entity recognition (NER). Such label mistakes might hurt model training
    and interfere model comparison. In this study, we dive deep into one of the widely-adopted
    NER benchmark datasets, CoNLL03 NER. We are able to identify label mistakes in
    about 5.38% test sentences, which is a significant ratio considering that the
    state-of-the-art test F1 score is already around 93%. Therefore, we manually correct
    these label mistakes and form a cleaner test set. Our re-evaluation of popular
    models on this corrected test set leads to more accurate assessments, compared
    to those on the original test set. More importantly, we propose a simple yet effective
    framework, CrossWeigh, to handle label mistakes during NER model training. Specifically,
    it partitions the training data into several folds and train independent NER models
    to identify potential mistakes in each fold. Then it adjusts the weights of training
    data accordingly to train the final NER model. Extensive experiments demonstrate
    significant improvements of plugging various NER models into our proposed framework
    on three datasets. All implementations and corrected test set are available at
    our Github repo https://github.com/ZihanWangKi/CrossWeigh.
  address: Hong Kong, China
  author:
  - first: Zihan
    full: Zihan Wang
    id: zihan-wang
    last: Wang
  - first: Jingbo
    full: Jingbo Shang
    id: jingbo-shang
    last: Shang
  - first: Liyuan
    full: Liyuan Liu
    id: liyuan-liu
    last: Liu
  - first: Lihao
    full: Lihao Lu
    id: lihao-lu
    last: Lu
  - first: Jiacheng
    full: Jiacheng Liu
    id: jiacheng-liu
    last: Liu
  - first: Jiawei
    full: Jiawei Han
    id: jiawei-han
    last: Han
  author_string: Zihan Wang, Jingbo Shang, Liyuan Liu, Lihao Lu, Jiacheng Liu, Jiawei
    Han
  bibkey: wang-etal-2019-crossweigh
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1519
  month: November
  page_first: '5154'
  page_last: '5163'
  pages: "5154\u20135163"
  paper_id: '519'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1519.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1519.jpg
  title: 'CrossWeigh: Training Named Entity Tagger from Imperfect Annotations'
  title_html: '<span class="acl-fixed-case">C</span>ross<span class="acl-fixed-case">W</span>eigh:
    Training Named Entity Tagger from Imperfect Annotations'
  url: https://www.aclweb.org/anthology/D19-1519
  year: '2019'
D19-1520:
  abstract: 'Most state-of-the-art models for named entity recognition (NER) rely
    on the availability of large amounts of labeled data, making them challenging
    to extend to new, lower-resourced languages. However, there are now many proposed
    solutions to this problem involving either cross-lingual transfer learning, which
    learns from other highly resourced languages, or active learning, which efficiently
    selects effective training data based on model predictions. In this paper, we
    ask the question: given this recent progress, and some amount of human annotation,
    what is the most effective method for efficiently creating high-quality entity
    recognizers in under-resourced languages? Based on extensive experimentation using
    both simulated and real human annotation, we settle on a recipe of starting with
    a cross-lingual transferred model, then performing targeted annotation of only
    uncertain entity spans in the target language, minimizing annotator effort. Results
    demonstrate that cross-lingual transfer is a powerful tool when very little data
    can be annotated, but an entity-targeted annotation strategy can achieve competitive
    accuracy quickly, with just one-tenth of training data.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1520.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1520.Attachment.pdf
  author:
  - first: Aditi
    full: Aditi Chaudhary
    id: aditi-chaudhary
    last: Chaudhary
  - first: Jiateng
    full: Jiateng Xie
    id: jiateng-xie
    last: Xie
  - first: Zaid
    full: Zaid Sheikh
    id: zaid-sheikh
    last: Sheikh
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  - first: Jaime
    full: Jaime Carbonell
    id: jaime-g-carbonell
    last: Carbonell
  author_string: Aditi Chaudhary, Jiateng Xie, Zaid Sheikh, Graham Neubig, Jaime Carbonell
  bibkey: chaudhary-etal-2019-little
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1520
  month: November
  page_first: '5164'
  page_last: '5174'
  pages: "5164\u20135174"
  paper_id: '520'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1520.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1520.jpg
  title: 'A Little Annotation does a Lot of Good: A Study in Bootstrapping Low-resource
    Named Entity Recognizers'
  title_html: 'A Little Annotation does a Lot of Good: A Study in Bootstrapping Low-resource
    Named Entity Recognizers'
  url: https://www.aclweb.org/anthology/D19-1520
  year: '2019'
D19-1521:
  abstract: This paper studies keyphrase extraction in real-world scenarios where
    documents are from diverse domains and have variant content quality. We curate
    and release OpenKP, a large scale open domain keyphrase extraction dataset with
    near one hundred thousand web documents and expert keyphrase annotations. To handle
    the variations of domain and content quality, we develop BLING-KPE, a neural keyphrase
    extraction model that goes beyond language understanding using visual presentations
    of documents and weak supervision from search queries. Experimental results on
    OpenKP confirm the effectiveness of BLING-KPE and the contributions of its neural
    architecture, visual features, and search log weak supervision. Zero-shot evaluations
    on DUC-2001 demonstrate the improved generalization ability of learning from the
    open domain data compared to a specific domain.
  address: Hong Kong, China
  attachment:
  - filename: D19-1521.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1521.Attachment.zip
  author:
  - first: Lee
    full: Lee Xiong
    id: lee-xiong
    last: Xiong
  - first: Chuan
    full: Chuan Hu
    id: chuan-hu
    last: Hu
  - first: Chenyan
    full: Chenyan Xiong
    id: chenyan-xiong
    last: Xiong
  - first: Daniel
    full: Daniel Campos
    id: daniel-campos
    last: Campos
  - first: Arnold
    full: Arnold Overwijk
    id: arnold-overwijk
    last: Overwijk
  author_string: Lee Xiong, Chuan Hu, Chenyan Xiong, Daniel Campos, Arnold Overwijk
  bibkey: xiong-etal-2019-open
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1521
  month: November
  page_first: '5175'
  page_last: '5184'
  pages: "5175\u20135184"
  paper_id: '521'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1521.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1521.jpg
  title: Open Domain Web Keyphrase Extraction Beyond Language Modeling
  title_html: Open Domain Web Keyphrase Extraction Beyond Language Modeling
  url: https://www.aclweb.org/anthology/D19-1521
  year: '2019'
D19-1522:
  abstract: Knowledge graphs are structured representations of real world facts. However,
    they typically contain only a small subset of all possible facts. Link prediction
    is a task of inferring missing facts based on existing ones. We propose TuckER,
    a relatively straightforward but powerful linear model based on Tucker decomposition
    of the binary tensor representation of knowledge graph triples. TuckER outperforms
    previous state-of-the-art models across standard link prediction datasets, acting
    as a strong baseline for more elaborate models. We show that TuckER is a fully
    expressive model, derive sufficient bounds on its embedding dimensionalities and
    demonstrate that several previously introduced linear models can be viewed as
    special cases of TuckER.
  address: Hong Kong, China
  attachment:
  - filename: D19-1522.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1522.Attachment.zip
  author:
  - first: Ivana
    full: Ivana Balazevic
    id: ivana-balazevic
    last: Balazevic
  - first: Carl
    full: Carl Allen
    id: carl-allen
    last: Allen
  - first: Timothy
    full: Timothy Hospedales
    id: timothy-hospedales
    last: Hospedales
  author_string: Ivana Balazevic, Carl Allen, Timothy Hospedales
  bibkey: balazevic-etal-2019-tucker
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1522
  month: November
  page_first: '5185'
  page_last: '5194'
  pages: "5185\u20135194"
  paper_id: '522'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1522.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1522.jpg
  title: 'TuckER: Tensor Factorization for Knowledge Graph Completion'
  title_html: '<span class="acl-fixed-case">T</span>uck<span class="acl-fixed-case">ER</span>:
    Tensor Factorization for Knowledge Graph Completion'
  url: https://www.aclweb.org/anthology/D19-1522
  year: '2019'
D19-1523:
  abstract: "Due to the black-box nature of deep learning models, methods for explaining\
    \ the models\u2019 results are crucial to gain trust from humans and support collaboration\
    \ between AIs and humans. In this paper, we consider several model-agnostic and\
    \ model-specific explanation methods for CNNs for text classification and conduct\
    \ three human-grounded evaluations, focusing on different purposes of explanations:\
    \ (1) revealing model behavior, (2) justifying model predictions, and (3) helping\
    \ humans investigate uncertain predictions. The results highlight dissimilar qualities\
    \ of the various explanation methods we consider and show the degree to which\
    \ these methods could serve for each purpose."
  address: Hong Kong, China
  attachment:
  - filename: D19-1523.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1523.Attachment.zip
  author:
  - first: Piyawat
    full: Piyawat Lertvittayakumjorn
    id: piyawat-lertvittayakumjorn
    last: Lertvittayakumjorn
  - first: Francesca
    full: Francesca Toni
    id: francesca-toni
    last: Toni
  author_string: Piyawat Lertvittayakumjorn, Francesca Toni
  bibkey: lertvittayakumjorn-toni-2019-human
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1523
  month: November
  page_first: '5195'
  page_last: '5205'
  pages: "5195\u20135205"
  paper_id: '523'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1523.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1523.jpg
  title: Human-grounded Evaluations of Explanation Methods for Text Classification
  title_html: Human-grounded Evaluations of Explanation Methods for Text Classification
  url: https://www.aclweb.org/anthology/D19-1523
  year: '2019'
D19-1524:
  abstract: We introduce a new task of modeling the role and function for on-line
    resource citations in scientific literature. By categorizing the on-line resources
    and analyzing the purpose of resource citations in scientific texts, it can greatly
    help resource search and recommendation systems to better understand and manage
    the scientific resources. For this novel task, we are the first to create an annotation
    scheme, which models the different granularity of information from a hierarchical
    perspective. And we construct a dataset SciRes, which includes 3,088 manually
    annotated resource contexts. In this paper, we propose a possible solution by
    using a multi-task framework to build the scientific resource classifier (SciResCLF)
    for jointly recognizing the role and function types. Then we use the classification
    results to help a scientific resource recommendation (SciResREC) task. Experiments
    show that our model achieves the best results on both the classification task
    and the recommendation task. The SciRes dataset is released for future research.
  address: Hong Kong, China
  attachment:
  - filename: D19-1524.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1524.Attachment.pdf
  author:
  - first: He
    full: He Zhao
    id: he-zhao
    last: Zhao
  - first: Zhunchen
    full: Zhunchen Luo
    id: zhunchen-luo
    last: Luo
  - first: Chong
    full: Chong Feng
    id: chong-feng
    last: Feng
  - first: Anqing
    full: Anqing Zheng
    id: anqing-zheng
    last: Zheng
  - first: Xiaopeng
    full: Xiaopeng Liu
    id: xiaopeng-liu
    last: Liu
  author_string: He Zhao, Zhunchen Luo, Chong Feng, Anqing Zheng, Xiaopeng Liu
  bibkey: zhao-etal-2019-context
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1524
  month: November
  page_first: '5206'
  page_last: '5215'
  pages: "5206\u20135215"
  paper_id: '524'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1524.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1524.jpg
  title: A Context-based Framework for Modeling the Role and Function of On-line Resource
    Citations in Scientific Literature
  title_html: A Context-based Framework for Modeling the Role and Function of On-line
    Resource Citations in Scientific Literature
  url: https://www.aclweb.org/anthology/D19-1524
  year: '2019'
D19-1525:
  abstract: "In this work, we develop methods to repurpose text classification neural\
    \ networks for alternate tasks without modifying the network architecture or parameters.\
    \ We propose a context based vocabulary remapping method that performs a computationally\
    \ inexpensive input transformation to reprogram a victim classification model\
    \ for a new set of sequences. We propose algorithms for training such an input\
    \ transformation in both white box and black box settings where the adversary\
    \ may or may not have access to the victim model\u2019s architecture and parameters.\
    \ We demonstrate the application of our model and the vulnerability of neural\
    \ networks by adversarially repurposing various text-classification models including\
    \ LSTM, bi-directional LSTM and CNN for alternate classification tasks."
  address: Hong Kong, China
  attachment:
  - filename: D19-1525.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1525.Attachment.pdf
  author:
  - first: Paarth
    full: Paarth Neekhara
    id: paarth-neekhara
    last: Neekhara
  - first: Shehzeen
    full: Shehzeen Hussain
    id: shehzeen-hussain
    last: Hussain
  - first: Shlomo
    full: Shlomo Dubnov
    id: shlomo-dubnov
    last: Dubnov
  - first: Farinaz
    full: Farinaz Koushanfar
    id: farinaz-koushanfar
    last: Koushanfar
  author_string: Paarth Neekhara, Shehzeen Hussain, Shlomo Dubnov, Farinaz Koushanfar
  bibkey: neekhara-etal-2019-adversarial
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1525
  month: November
  page_first: '5216'
  page_last: '5225'
  pages: "5216\u20135225"
  paper_id: '525'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1525.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1525.jpg
  title: Adversarial Reprogramming of Text Classification Neural Networks
  title_html: Adversarial Reprogramming of Text Classification Neural Networks
  url: https://www.aclweb.org/anthology/D19-1525
  year: '2019'
D19-1526:
  abstract: Hashing is promising for large-scale information retrieval tasks thanks
    to the efficiency of distance evaluation between binary codes. Generative hashing
    is often used to generate hashing codes in an unsupervised way. However, existing
    generative hashing methods only considered the use of simple priors, like Gaussian
    and Bernoulli priors, which limits these methods to further improve their performance.
    In this paper, two mixture-prior generative models are proposed, under the objective
    to produce high-quality hashing codes for documents. Specifically, a Gaussian
    mixture prior is first imposed onto the variational auto-encoder (VAE), followed
    by a separate step to cast the continuous latent representation of VAE into binary
    code. To avoid the performance loss caused by the separate casting, a model using
    a Bernoulli mixture prior is further developed, in which an end-to-end training
    is admitted by resorting to the straight-through (ST) discrete gradient estimator.
    Experimental results on several benchmark datasets demonstrate that the proposed
    methods, especially the one using Bernoulli mixture priors, consistently outperform
    existing ones by a substantial margin.
  address: Hong Kong, China
  author:
  - first: Wei
    full: Wei Dong
    id: wei-dong
    last: Dong
  - first: Qinliang
    full: Qinliang Su
    id: qinliang-su
    last: Su
  - first: Dinghan
    full: Dinghan Shen
    id: dinghan-shen
    last: Shen
  - first: Changyou
    full: Changyou Chen
    id: changyou-chen
    last: Chen
  author_string: Wei Dong, Qinliang Su, Dinghan Shen, Changyou Chen
  bibkey: dong-etal-2019-document
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1526
  month: November
  page_first: '5226'
  page_last: '5235'
  pages: "5226\u20135235"
  paper_id: '526'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1526.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1526.jpg
  title: Document Hashing with Mixture-Prior Generative Models
  title_html: Document Hashing with Mixture-Prior Generative Models
  url: https://www.aclweb.org/anthology/D19-1526
  year: '2019'
D19-1527:
  abstract: 'Retrieval of relevant vectors produced by representation learning critically
    influences the efficiency in natural language processing (NLP) tasks. In this
    paper, we demonstrate an efficient method for searching vectors via a typical
    non-metric matching function: inner product. Our method, which constructs an approximate
    Inner Product Delaunay Graph (IPDG) for top-1 Maximum Inner Product Search (MIPS),
    transforms retrieving the most suitable latent vectors into a graph search problem
    with great benefits of efficiency. Experiments on data representations learned
    for different machine learning tasks verify the outperforming effectiveness and
    efficiency of the proposed IPDG.'
  address: Hong Kong, China
  author:
  - first: Shulong
    full: Shulong Tan
    id: shulong-tan
    last: Tan
  - first: Zhixin
    full: Zhixin Zhou
    id: zhixin-zhou
    last: Zhou
  - first: Zhaozhuo
    full: Zhaozhuo Xu
    id: zhaozhuo-xu
    last: Xu
  - first: Ping
    full: Ping Li
    id: ping-li
    last: Li
  author_string: Shulong Tan, Zhixin Zhou, Zhaozhuo Xu, Ping Li
  bibkey: tan-etal-2019-efficient
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1527
  month: November
  page_first: '5236'
  page_last: '5246'
  pages: "5236\u20135246"
  paper_id: '527'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1527.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1527.jpg
  title: On Efficient Retrieval of Top Similarity Vectors
  title_html: On Efficient Retrieval of Top Similarity Vectors
  url: https://www.aclweb.org/anthology/D19-1527
  year: '2019'
D19-1528:
  abstract: Conventional word embeddings represent words with fixed vectors, which
    are usually trained based on co-occurrence patterns among words. In doing so,
    however, the power of such representations is limited, where the same word might
    be functionalized separately under different syntactic relations. To address this
    limitation, one solution is to incorporate relational dependencies of different
    words into their embeddings. Therefore, in this paper, we propose a multiplex
    word embedding model, which can be easily extended according to various relations
    among words. As a result, each word has a center embedding to represent its overall
    semantics, and several relational embeddings to represent its relational dependencies.
    Compared to existing models, our model can effectively distinguish words with
    respect to different relations without introducing unnecessary sparseness. Moreover,
    to accommodate various relations, we use a small dimension for relational embeddings
    and our model is able to keep their effectiveness. Experiments on selectional
    preference acquisition and word similarity demonstrate the effectiveness of the
    proposed model, and a further study of scalability also proves that our embeddings
    only need 1/20 of the original embedding size to achieve better performance.
  address: Hong Kong, China
  author:
  - first: Hongming
    full: Hongming Zhang
    id: hongming-zhang
    last: Zhang
  - first: Jiaxin
    full: Jiaxin Bai
    id: jiaxin-bai
    last: Bai
  - first: Yan
    full: Yan Song
    id: yan-song
    last: Song
  - first: Kun
    full: Kun Xu
    id: kun-xu
    last: Xu
  - first: Changlong
    full: Changlong Yu
    id: changlong-yu
    last: Yu
  - first: Yangqiu
    full: Yangqiu Song
    id: yangqiu-song
    last: Song
  - first: Wilfred
    full: Wilfred Ng
    id: wilfred-ng
    last: Ng
  - first: Dong
    full: Dong Yu
    id: dong-yu
    last: Yu
  author_string: Hongming Zhang, Jiaxin Bai, Yan Song, Kun Xu, Changlong Yu, Yangqiu
    Song, Wilfred Ng, Dong Yu
  bibkey: zhang-etal-2019-multiplex
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1528
  month: November
  page_first: '5247'
  page_last: '5256'
  pages: "5247\u20135256"
  paper_id: '528'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1528.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1528.jpg
  title: Multiplex Word Embeddings for Selectional Preference Acquisition
  title_html: Multiplex Word Embeddings for Selectional Preference Acquisition
  url: https://www.aclweb.org/anthology/D19-1528
  year: '2019'
D19-1529:
  abstract: It is challenging to deploy deep neural nets on memory-constrained devices
    due to the explosion of numbers of parameters. Especially, the input embedding
    layer and Softmax layer usually dominate the memory usage in an RNN-based language
    model. For example, input embedding and Softmax matrices in IWSLT-2014 German-to-English
    data set account for more than 80% of the total model parameters. To compress
    these embedding layers, we propose MulCode, a novel multi-way multiplicative neural
    compressor. MulCode learns an adaptively created matrix and its multiplicative
    compositions. Together with a prior weighted loss, Multicode is more effective
    than the state-of-the-art compression methods. On the IWSLT-2014 machine translation
    data set, MulCode achieved 17 times compression rate for the embedding and Softmax
    matrices, and when combined with quantization technique, our method can achieve
    41.38 times compression rate with very little loss in performance.
  address: Hong Kong, China
  author:
  - first: Yukun
    full: Yukun Ma
    id: yukun-ma
    last: Ma
  - first: Patrick H.
    full: Patrick H. Chen
    id: patrick-h-chen
    last: Chen
  - first: Cho-Jui
    full: Cho-Jui Hsieh
    id: cho-jui-hsieh
    last: Hsieh
  author_string: Yukun Ma, Patrick H. Chen, Cho-Jui Hsieh
  bibkey: ma-etal-2019-mulcode
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1529
  month: November
  page_first: '5257'
  page_last: '5266'
  pages: "5257\u20135266"
  paper_id: '529'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1529.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1529.jpg
  title: 'MulCode: A Multiplicative Multi-way Model for Compressing Neural Language
    Model'
  title_html: '<span class="acl-fixed-case">M</span>ul<span class="acl-fixed-case">C</span>ode:
    A Multiplicative Multi-way Model for Compressing Neural Language Model'
  url: https://www.aclweb.org/anthology/D19-1529
  year: '2019'
D19-1530:
  abstract: 'This paper treats gender bias latent in word embeddings. Previous mitigation
    attempts rely on the operationalisation of gender bias as a projection over a
    linear subspace. An alternative approach is Counterfactual Data Augmentation (CDA),
    in which a corpus is duplicated and augmented to remove bias, e.g. by swapping
    all inherently-gendered words in the copy. We perform an empirical comparison
    of these approaches on the English Gigaword and Wikipedia, and find that whilst
    both successfully reduce direct bias and perform well in tasks which quantify
    embedding quality, CDA variants outperform projection-based methods at the task
    of drawing non-biased gender analogies by an average of 19% across both corpora.
    We propose two improvements to CDA: Counterfactual Data Substitution (CDS), a
    variant of CDA in which potentially biased text is randomly substituted to avoid
    duplication, and the Names Intervention, a novel name-pairing technique that vastly
    increases the number of words being treated. CDA/S with the Names Intervention
    is the only approach which is able to mitigate indirect gender bias: following
    debiasing, previously biased words are significantly less clustered according
    to gender (cluster purity is reduced by 49%), thus improving on the state-of-the-art
    for bias mitigation.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1530.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1530.Attachment.zip
  author:
  - first: Rowan
    full: Rowan Hall Maudslay
    id: rowan-hall-maudslay
    last: Hall Maudslay
  - first: Hila
    full: Hila Gonen
    id: hila-gonen
    last: Gonen
  - first: Ryan
    full: Ryan Cotterell
    id: ryan-cotterell
    last: Cotterell
  - first: Simone
    full: Simone Teufel
    id: simone-teufel
    last: Teufel
  author_string: Rowan Hall Maudslay, Hila Gonen, Ryan Cotterell, Simone Teufel
  bibkey: hall-maudslay-etal-2019-name
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1530
  month: November
  page_first: '5267'
  page_last: '5275'
  pages: "5267\u20135275"
  paper_id: '530'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1530.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1530.jpg
  title: "It\u2019s All in the Name: Mitigating Gender Bias with Name-Based Counterfactual\
    \ Data Substitution"
  title_html: "It\u2019s All in the Name: Mitigating Gender Bias with Name-Based Counterfactual\
    \ Data Substitution"
  url: https://www.aclweb.org/anthology/D19-1530
  year: '2019'
D19-1531:
  abstract: Recent studies have shown that word embeddings exhibit gender bias inherited
    from the training corpora. However, most studies to date have focused on quantifying
    and mitigating such bias only in English. These analyses cannot be directly extended
    to languages that exhibit morphological agreement on gender, such as Spanish and
    French. In this paper, we propose new metrics for evaluating gender bias in word
    embeddings of these languages and further demonstrate evidence of gender bias
    in bilingual embeddings which align these languages with English. Finally, we
    extend an existing approach to mitigate gender bias in word embedding of these
    languages under both monolingual and bilingual settings. Experiments on modified
    Word Embedding Association Test, word similarity, word translation, and word pair
    translation tasks show that the proposed approaches can effectively reduce the
    gender bias while preserving the utility of the original embeddings.
  address: Hong Kong, China
  author:
  - first: Pei
    full: Pei Zhou
    id: pei-zhou
    last: Zhou
  - first: Weijia
    full: Weijia Shi
    id: weijia-shi
    last: Shi
  - first: Jieyu
    full: Jieyu Zhao
    id: jieyu-zhao
    last: Zhao
  - first: Kuan-Hao
    full: Kuan-Hao Huang
    id: kuan-hao-huang
    last: Huang
  - first: Muhao
    full: Muhao Chen
    id: muhao-chen
    last: Chen
  - first: Ryan
    full: Ryan Cotterell
    id: ryan-cotterell
    last: Cotterell
  - first: Kai-Wei
    full: Kai-Wei Chang
    id: kai-wei-chang
    last: Chang
  author_string: Pei Zhou, Weijia Shi, Jieyu Zhao, Kuan-Hao Huang, Muhao Chen, Ryan
    Cotterell, Kai-Wei Chang
  bibkey: zhou-etal-2019-examining
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1531
  month: November
  page_first: '5276'
  page_last: '5284'
  pages: "5276\u20135284"
  paper_id: '531'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1531.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1531.jpg
  title: Examining Gender Bias in Languages with Grammatical Gender
  title_html: Examining Gender Bias in Languages with Grammatical Gender
  url: https://www.aclweb.org/anthology/D19-1531
  year: '2019'
D19-1532:
  abstract: Words in different languages rarely cover the exact same semantic space.
    This work characterizes differences in meaning between words across languages
    using semantic relations that have been used to relate the meaning of English
    words. However, because of translation ambiguity, semantic relations are not always
    preserved by translation. We introduce a cross-lingual relation classifier trained
    only with English examples and a bilingual dictionary. Our classifier relies on
    a novel attention-based distillation approach to account for translation ambiguity
    when transferring knowledge from English to cross-lingual settings. On new English-Chinese
    and English-Hindi test sets, the resulting models largely outperform baselines
    that more naively rely on bilingual embeddings or dictionaries for cross-lingual
    transfer, and approach the performance of fully supervised systems on English
    tasks.
  address: Hong Kong, China
  attachment:
  - filename: D19-1532.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1532.Attachment.pdf
  author:
  - first: Yogarshi
    full: Yogarshi Vyas
    id: yogarshi-vyas
    last: Vyas
  - first: Marine
    full: Marine Carpuat
    id: marine-carpuat
    last: Carpuat
  author_string: Yogarshi Vyas, Marine Carpuat
  bibkey: vyas-carpuat-2019-weakly
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1532
  month: November
  page_first: '5285'
  page_last: '5296'
  pages: "5285\u20135296"
  paper_id: '532'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1532.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1532.jpg
  title: Weakly Supervised Cross-lingual Semantic Relation Classification via Knowledge
    Distillation
  title_html: Weakly Supervised Cross-lingual Semantic Relation Classification via
    Knowledge Distillation
  url: https://www.aclweb.org/anthology/D19-1532
  year: '2019'
D19-1533:
  abstract: Contextualized word representations are able to give different representations
    for the same word in different contexts, and they have been shown to be effective
    in downstream natural language processing tasks, such as question answering, named
    entity recognition, and sentiment analysis. However, evaluation on word sense
    disambiguation (WSD) in prior work shows that using contextualized word representations
    does not outperform the state-of-the-art approach that makes use of non-contextualized
    word embeddings. In this paper, we explore different strategies of integrating
    pre-trained contextualized word representations and our best strategy achieves
    accuracies exceeding the best prior published accuracies by significant margins
    on multiple benchmark WSD datasets.
  address: Hong Kong, China
  author:
  - first: Christian
    full: Christian Hadiwinoto
    id: christian-hadiwinoto
    last: Hadiwinoto
  - first: Hwee Tou
    full: Hwee Tou Ng
    id: hwee-tou-ng
    last: Ng
  - first: Wee Chung
    full: Wee Chung Gan
    id: wee-chung-gan
    last: Gan
  author_string: Christian Hadiwinoto, Hwee Tou Ng, Wee Chung Gan
  bibkey: hadiwinoto-etal-2019-improved
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1533
  month: November
  page_first: '5297'
  page_last: '5306'
  pages: "5297\u20135306"
  paper_id: '533'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1533.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1533.jpg
  title: Improved Word Sense Disambiguation Using Pre-Trained Contextualized Word
    Representations
  title_html: Improved Word Sense Disambiguation Using Pre-Trained Contextualized
    Word Representations
  url: https://www.aclweb.org/anthology/D19-1533
  year: '2019'
D19-1534:
  abstract: "The ability to understand and work with numbers (numeracy) is critical\
    \ for many complex reasoning tasks. Currently, most NLP models treat numbers in\
    \ text in the same way as other tokens\u2014they embed them as distributed vectors.\
    \ Is this enough to capture numeracy? We begin by investigating the numerical\
    \ reasoning capabilities of a state-of-the-art question answering model on the\
    \ DROP dataset. We find this model excels on questions that require numerical\
    \ reasoning, i.e., it already captures numeracy. To understand how this capability\
    \ emerges, we probe token embedding methods (e.g., BERT, GloVe) on synthetic list\
    \ maximum, number decoding, and addition tasks. A surprising degree of numeracy\
    \ is naturally present in standard embeddings. For example, GloVe and word2vec\
    \ accurately encode magnitude for numbers up to 1,000. Furthermore, character-level\
    \ embeddings are even more precise\u2014ELMo captures numeracy the best for all\
    \ pre-trained methods\u2014but BERT, which uses sub-word units, is less exact."
  address: Hong Kong, China
  attachment:
  - filename: D19-1534.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1534.Attachment.zip
  author:
  - first: Eric
    full: Eric Wallace
    id: eric-wallace
    last: Wallace
  - first: Yizhong
    full: Yizhong Wang
    id: yizhong-wang
    last: Wang
  - first: Sujian
    full: Sujian Li
    id: sujian-li
    last: Li
  - first: Sameer
    full: Sameer Singh
    id: sameer-singh
    last: Singh
  - first: Matt
    full: Matt Gardner
    id: matt-gardner
    last: Gardner
  author_string: Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, Matt Gardner
  bibkey: wallace-etal-2019-nlp
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1534
  month: November
  page_first: '5307'
  page_last: '5315'
  pages: "5307\u20135315"
  paper_id: '534'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1534.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1534.jpg
  title: Do NLP Models Know Numbers? Probing Numeracy in Embeddings
  title_html: Do <span class="acl-fixed-case">NLP</span> Models Know Numbers? Probing
    Numeracy in Embeddings
  url: https://www.aclweb.org/anthology/D19-1534
  year: '2019'
D19-1535:
  abstract: Context-dependent semantic parsing has proven to be an important yet challenging
    task. To leverage the advances in context-independent semantic parsing, we propose
    to perform follow-up query analysis, aiming to restate context-dependent natural
    language queries with contextual information. To accomplish the task, we propose
    STAR, a novel approach with a well-designed two-phase process. It is parser-independent
    and able to handle multifarious follow-up scenarios in different domains. Experiments
    on the FollowUp dataset show that STAR outperforms the state-of-the-art baseline
    by a large margin of nearly 8%. The superiority on parsing results verifies the
    feasibility of follow-up query analysis. We also explore the extensibility of
    STAR on the SQA dataset, which is very promising.
  address: Hong Kong, China
  author:
  - first: Qian
    full: Qian Liu
    id: qian-liu
    last: Liu
  - first: Bei
    full: Bei Chen
    id: bei-chen
    last: Chen
  - first: Haoyan
    full: Haoyan Liu
    id: haoyan-liu
    last: Liu
  - first: Jian-Guang
    full: Jian-Guang Lou
    id: jian-guang-lou
    last: Lou
  - first: Lei
    full: Lei Fang
    id: lei-fang
    last: Fang
  - first: Bin
    full: Bin Zhou
    id: bin-zhou
    last: Zhou
  - first: Dongmei
    full: Dongmei Zhang
    id: dongmei-zhang
    last: Zhang
  author_string: Qian Liu, Bei Chen, Haoyan Liu, Jian-Guang Lou, Lei Fang, Bin Zhou,
    Dongmei Zhang
  bibkey: liu-etal-2019-split
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1535
  month: November
  page_first: '5316'
  page_last: '5326'
  pages: "5316\u20135326"
  paper_id: '535'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1535.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1535.jpg
  title: A Split-and-Recombine Approach for Follow-up Query Analysis
  title_html: A Split-and-Recombine Approach for Follow-up Query Analysis
  url: https://www.aclweb.org/anthology/D19-1535
  year: '2019'
D19-1536:
  abstract: We propose Text2Math, a model for semantically parsing text into math
    expressions. The model can be used to solve different math related problems including
    arithmetic word problems and equation parsing problems. Unlike previous approaches,
    we tackle the problem from an end-to-end structured prediction perspective where
    our algorithm aims to predict the complete math expression at once as a tree structure,
    where minimal manual efforts are involved in the process. Empirical results on
    benchmark datasets demonstrate the efficacy of our approach.
  address: Hong Kong, China
  attachment:
  - filename: D19-1536.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1536.Attachment.zip
  author:
  - first: Yanyan
    full: Yanyan Zou
    id: yanyan-zou
    last: Zou
  - first: Wei
    full: Wei Lu
    id: wei-lu
    last: Lu
  author_string: Yanyan Zou, Wei Lu
  bibkey: zou-lu-2019-text2math
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1536
  month: November
  page_first: '5327'
  page_last: '5337'
  pages: "5327\u20135337"
  paper_id: '536'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1536.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1536.jpg
  title: 'Text2Math: End-to-end Parsing Text into Math Expressions'
  title_html: '<span class="acl-fixed-case">T</span>ext2<span class="acl-fixed-case">M</span>ath:
    End-to-end Parsing Text into Math Expressions'
  url: https://www.aclweb.org/anthology/D19-1536
  year: '2019'
D19-1537:
  abstract: We focus on the cross-domain context-dependent text-to-SQL generation
    task. Based on the observation that adjacent natural language questions are often
    linguistically dependent and their corresponding SQL queries tend to overlap,
    we utilize the interaction history by editing the previous predicted query to
    improve the generation quality. Our editing mechanism views SQL as sequences and
    reuses generation results at the token level in a simple manner. It is flexible
    to change individual tokens and robust to error propagation. Furthermore, to deal
    with complex table structures in different domains, we employ an utterance-table
    encoder and a table-aware decoder to incorporate the context of the user utterance
    and the table schema. We evaluate our approach on the SParC dataset and demonstrate
    the benefit of editing compared with the state-of-the-art baselines which generate
    SQL from scratch. Our code is available at https://github.com/ryanzhumich/sparc_atis_pytorch.
  address: Hong Kong, China
  author:
  - first: Rui
    full: Rui Zhang
    id: rui-zhang
    last: Zhang
  - first: Tao
    full: Tao Yu
    id: tao-yu
    last: Yu
  - first: Heyang
    full: Heyang Er
    id: heyang-er
    last: Er
  - first: Sungrok
    full: Sungrok Shim
    id: sungrok-shim
    last: Shim
  - first: Eric
    full: Eric Xue
    id: eric-xue
    last: Xue
  - first: Xi Victoria
    full: Xi Victoria Lin
    id: xi-victoria-lin
    last: Lin
  - first: Tianze
    full: Tianze Shi
    id: tianze-shi
    last: Shi
  - first: Caiming
    full: Caiming Xiong
    id: caiming-xiong
    last: Xiong
  - first: Richard
    full: Richard Socher
    id: richard-socher
    last: Socher
  - first: Dragomir
    full: Dragomir Radev
    id: dragomir-radev
    last: Radev
  author_string: Rui Zhang, Tao Yu, Heyang Er, Sungrok Shim, Eric Xue, Xi Victoria
    Lin, Tianze Shi, Caiming Xiong, Richard Socher, Dragomir Radev
  bibkey: zhang-etal-2019-editing
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1537
  month: November
  page_first: '5338'
  page_last: '5349'
  pages: "5338\u20135349"
  paper_id: '537'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1537.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1537.jpg
  title: Editing-Based SQL Query Generation for Cross-Domain Context-Dependent Questions
  title_html: Editing-Based <span class="acl-fixed-case">SQL</span> Query Generation
    for Cross-Domain Context-Dependent Questions
  url: https://www.aclweb.org/anthology/D19-1537
  year: '2019'
D19-1538:
  abstract: Recently, semantic role labeling (SRL) has earned a series of success
    with even higher performance improvements, which can be mainly attributed to syntactic
    integration and enhanced word representation. However, most of these efforts focus
    on English, while SRL on multiple languages more than English has received relatively
    little attention so that is kept underdevelopment. Thus this paper intends to
    fill the gap on multilingual SRL with special focus on the impact of syntax and
    contextualized word representation. Unlike existing work, we propose a novel method
    guided by syntactic rule to prune arguments, which enables us to integrate syntax
    into multilingual SRL model simply and effectively. We present a unified SRL model
    designed for multiple languages together with the proposed uniform syntax enhancement.
    Our model achieves new state-of-the-art results on the CoNLL-2009 benchmarks of
    all seven languages. Besides, we pose a discussion on the syntactic role among
    different languages and verify the effectiveness of deep enhanced representation
    for multilingual SRL.
  address: Hong Kong, China
  author:
  - first: Shexia
    full: Shexia He
    id: shexia-he
    last: He
  - first: Zuchao
    full: Zuchao Li
    id: zuchao-li
    last: Li
  - first: Hai
    full: Hai Zhao
    id: hai-zhao
    last: Zhao
  author_string: Shexia He, Zuchao Li, Hai Zhao
  bibkey: he-etal-2019-syntax
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1538
  month: November
  page_first: '5350'
  page_last: '5359'
  pages: "5350\u20135359"
  paper_id: '538'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1538.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1538.jpg
  title: Syntax-aware Multilingual Semantic Role Labeling
  title_html: Syntax-aware Multilingual Semantic Role Labeling
  url: https://www.aclweb.org/anthology/D19-1538
  year: '2019'
D19-1539:
  abstract: We present a new approach for pretraining a bi-directional transformer
    model that provides significant performance gains across a variety of language
    understanding problems. Our model solves a cloze-style word reconstruction task,
    where each word is ablated and must be predicted given the rest of the text. Experiments
    demonstrate large performance gains on GLUE and new state of the art results on
    NER as well as constituency parsing benchmarks, consistent with BERT. We also
    present a detailed analysis of a number of factors that contribute to effective
    pretraining, including data domain and size, model capacity, and variations on
    the cloze objective.
  address: Hong Kong, China
  author:
  - first: Alexei
    full: Alexei Baevski
    id: alexei-baevski
    last: Baevski
  - first: Sergey
    full: Sergey Edunov
    id: sergey-edunov
    last: Edunov
  - first: Yinhan
    full: Yinhan Liu
    id: yinhan-liu
    last: Liu
  - first: Luke
    full: Luke Zettlemoyer
    id: luke-zettlemoyer
    last: Zettlemoyer
  - first: Michael
    full: Michael Auli
    id: michael-auli
    last: Auli
  author_string: Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, Michael
    Auli
  bibkey: baevski-etal-2019-cloze
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1539
  month: November
  page_first: '5360'
  page_last: '5369'
  pages: "5360\u20135369"
  paper_id: '539'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1539.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1539.jpg
  title: Cloze-driven Pretraining of Self-attention Networks
  title_html: Cloze-driven Pretraining of Self-attention Networks
  url: https://www.aclweb.org/anthology/D19-1539
  year: '2019'
D19-1540:
  abstract: "A core problem of information retrieval (IR) is relevance matching, which\
    \ is to rank documents by relevance to a user\u2019s query. On the other hand,\
    \ many NLP problems, such as question answering and paraphrase identification,\
    \ can be considered variants of semantic matching, which is to measure the semantic\
    \ distance between two pieces of short texts. While at a high level both relevance\
    \ and semantic matching require modeling textual similarity, many existing techniques\
    \ for one cannot be easily adapted to the other. To bridge this gap, we propose\
    \ a novel model, HCAN (Hybrid Co-Attention Network), that comprises (1) a hybrid\
    \ encoder module that includes ConvNet-based and LSTM-based encoders, (2) a relevance\
    \ matching module that measures soft term matches with importance weighting at\
    \ multiple granularities, and (3) a semantic matching module with co-attention\
    \ mechanisms that capture context-aware semantic relatedness. Evaluations on multiple\
    \ IR and NLP benchmarks demonstrate state-of-the-art effectiveness compared to\
    \ approaches that do not exploit pretraining on external data. Extensive ablation\
    \ studies suggest that relevance and semantic matching signals are complementary\
    \ across many problem settings, regardless of the choice of underlying encoders."
  address: Hong Kong, China
  author:
  - first: Jinfeng
    full: Jinfeng Rao
    id: jinfeng-rao
    last: Rao
  - first: Linqing
    full: Linqing Liu
    id: linqing-liu
    last: Liu
  - first: Yi
    full: Yi Tay
    id: yi-tay
    last: Tay
  - first: Wei
    full: Wei Yang
    id: wei-yang
    last: Yang
  - first: Peng
    full: Peng Shi
    id: peng-shi
    last: Shi
  - first: Jimmy
    full: Jimmy Lin
    id: jimmy-lin
    last: Lin
  author_string: Jinfeng Rao, Linqing Liu, Yi Tay, Wei Yang, Peng Shi, Jimmy Lin
  bibkey: rao-etal-2019-bridging
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1540
  month: November
  page_first: '5370'
  page_last: '5381'
  pages: "5370\u20135381"
  paper_id: '540'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1540.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1540.jpg
  title: Bridging the Gap between Relevance Matching and Semantic Matching for Short
    Text Similarity Modeling
  title_html: Bridging the Gap between Relevance Matching and Semantic Matching for
    Short Text Similarity Modeling
  url: https://www.aclweb.org/anthology/D19-1540
  year: '2019'
D19-1541:
  abstract: Semantic role labeling (SRL) aims to identify the predicate-argument structure
    of a sentence. Inspired by the strong correlation between syntax and semantics,
    previous works pay much attention to improve SRL performance on exploiting syntactic
    knowledge, achieving significant results. Pipeline methods based on automatic
    syntactic trees and multi-task learning (MTL) approaches using standard syntactic
    trees are two common research orientations. In this paper, we adopt a simple unified
    span-based model for both span-based and word-based Chinese SRL as a strong baseline.
    Besides, we present a MTL framework that includes the basic SRL module and a dependency
    parser module. Different from the commonly used hard parameter sharing strategy
    in MTL, the main idea is to extract implicit syntactic representations from the
    dependency parser as external inputs for the basic SRL model. Experiments on the
    benchmarks of Chinese Proposition Bank 1.0 and CoNLL-2009 Chinese datasets show
    that our proposed framework can effectively improve the performance over the strong
    baselines. With the external BERT representations, our framework achieves new
    state-of-the-art 87.54 and 88.5 F1 scores on the two test data of the two benchmarks,
    respectively. In-depth analysis are conducted to gain more insights on the proposed
    framework and the effectiveness of syntax.
  address: Hong Kong, China
  author:
  - first: Qingrong
    full: Qingrong Xia
    id: qingrong-xia
    last: Xia
  - first: Zhenghua
    full: Zhenghua Li
    id: zhenghua-li
    last: Li
  - first: Min
    full: Min Zhang
    id: min-zhang
    last: Zhang
  author_string: Qingrong Xia, Zhenghua Li, Min Zhang
  bibkey: xia-etal-2019-syntax
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1541
  month: November
  page_first: '5382'
  page_last: '5392'
  pages: "5382\u20135392"
  paper_id: '541'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1541.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1541.jpg
  title: A Syntax-aware Multi-task Learning Framework for Chinese Semantic Role Labeling
  title_html: A Syntax-aware Multi-task Learning Framework for <span class="acl-fixed-case">C</span>hinese
    Semantic Role Labeling
  url: https://www.aclweb.org/anthology/D19-1541
  year: '2019'
D19-1542:
  abstract: "A semantic equivalence assessment is defined as a task that assesses\
    \ semantic equivalence in a sentence pair by binary judgment (i.e., paraphrase\
    \ identification) or grading (i.e., semantic textual similarity measurement).\
    \ It constitutes a set of tasks crucial for research on natural language understanding.\
    \ Recently, BERT realized a breakthrough in sentence representation learning (Devlin\
    \ et al., 2019), which is broadly transferable to various NLP tasks. While BERT\u2019\
    s performance improves by increasing its model size, the required computational\
    \ power is an obstacle preventing practical applications from adopting the technology.\
    \ Herein, we propose to inject phrasal paraphrase relations into BERT in order\
    \ to generate suitable representations for semantic equivalence assessment instead\
    \ of increasing the model size. Experiments on standard natural language understanding\
    \ tasks confirm that our method effectively improves a smaller BERT model while\
    \ maintaining the model size. The generated model exhibits superior performance\
    \ compared to a larger BERT model on semantic equivalence assessment tasks. Furthermore,\
    \ it achieves larger performance gains on tasks with limited training datasets\
    \ for fine-tuning, which is a property desirable for transfer learning."
  address: Hong Kong, China
  author:
  - first: Yuki
    full: Yuki Arase
    id: yuki-arase
    last: Arase
  - first: "Jun\u2019ichi"
    full: "Jun\u2019ichi Tsujii"
    id: junichi-tsujii
    last: Tsujii
  author_string: "Yuki Arase, Jun\u2019ichi Tsujii"
  bibkey: arase-tsujii-2019-transfer
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1542
  month: November
  page_first: '5393'
  page_last: '5404'
  pages: "5393\u20135404"
  paper_id: '542'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1542.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1542.jpg
  title: 'Transfer Fine-Tuning: A BERT Case Study'
  title_html: 'Transfer Fine-Tuning: A <span class="acl-fixed-case">BERT</span> Case
    Study'
  url: https://www.aclweb.org/anthology/D19-1542
  year: '2019'
D19-1543:
  abstract: On text-to-SQL generation, the input utterance usually contains lots of
    tokens that are related to column names or cells in the table, called table-related
    tokens. These table-related tokens are troublesome for the downstream neural semantic
    parser because it brings complex semantics and hinders the sharing across the
    training examples. However, existing approaches either ignore handling these tokens
    before the semantic parser or simply use deterministic approaches based on string-match
    or word embedding similarity. In this work, we propose a more efficient approach
    to handle table-related tokens before the semantic parser. First, we formulate
    it as a sequential tagging problem and propose a two-stage anonymization model
    to learn the semantic relationship between tables and input utterances. Then,
    we leverage the implicit supervision from SQL queries by policy gradient to guide
    the training. Experiments demonstrate that our approach consistently improves
    performances of different neural semantic parsers and significantly outperforms
    deterministic approaches.
  address: Hong Kong, China
  author:
  - first: Zhen
    full: Zhen Dong
    id: zhen-dong
    last: Dong
  - first: Shizhao
    full: Shizhao Sun
    id: shizhao-sun
    last: Sun
  - first: Hongzhi
    full: Hongzhi Liu
    id: hongzhi-liu
    last: Liu
  - first: Jian-Guang
    full: Jian-Guang Lou
    id: jian-guang-lou
    last: Lou
  - first: Dongmei
    full: Dongmei Zhang
    id: dongmei-zhang
    last: Zhang
  author_string: Zhen Dong, Shizhao Sun, Hongzhi Liu, Jian-Guang Lou, Dongmei Zhang
  bibkey: dong-etal-2019-data
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1543
  month: November
  page_first: '5405'
  page_last: '5414'
  pages: "5405\u20135414"
  paper_id: '543'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1543.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1543.jpg
  title: Data-Anonymous Encoding for Text-to-SQL Generation
  title_html: Data-Anonymous Encoding for Text-to-<span class="acl-fixed-case">SQL</span>
    Generation
  url: https://www.aclweb.org/anthology/D19-1543
  year: '2019'
D19-1544:
  abstract: 'Semantic role labeling (SRL) involves extracting propositions (i.e. predicates
    and their typed arguments) from natural language sentences. State-of-the-art SRL
    models rely on powerful encoders (e.g., LSTMs) and do not model non-local interaction
    between arguments. We propose a new approach to modeling these interactions while
    maintaining efficient inference. Specifically, we use Capsule Networks (Sabour
    et al., 2017): each proposition is encoded as a tuple of capsules, one capsule
    per argument type (i.e. role). These tuples serve as embeddings of entire propositions.
    In every network layer, the capsules interact with each other and with representations
    of words in the sentence. Each iteration results in updated proposition embeddings
    and updated predictions about the SRL structure. Our model substantially outperforms
    the non-refinement baseline model on all 7 CoNLL-2019 languages and achieves state-of-the-art
    results on 5 languages (including English) for dependency SRL. We analyze the
    types of mistakes corrected by the refinement procedure. For example, each role
    is typically (but not always) filled with at most one argument. Whereas enforcing
    this approximate constraint is not useful with the modern SRL system, iterative
    procedure corrects the mistakes by capturing this intuition in a flexible and
    context-sensitive way.'
  address: Hong Kong, China
  author:
  - first: Xinchi
    full: Xinchi Chen
    id: xinchi-chen
    last: Chen
  - first: Chunchuan
    full: Chunchuan Lyu
    id: chunchuan-lyu
    last: Lyu
  - first: Ivan
    full: Ivan Titov
    id: ivan-titov
    last: Titov
  author_string: Xinchi Chen, Chunchuan Lyu, Ivan Titov
  bibkey: chen-etal-2019-capturing
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1544
  month: November
  page_first: '5415'
  page_last: '5425'
  pages: "5415\u20135425"
  paper_id: '544'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1544.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1544.jpg
  title: Capturing Argument Interaction in Semantic Role Labeling with Capsule Networks
  title_html: Capturing Argument Interaction in Semantic Role Labeling with Capsule
    Networks
  url: https://www.aclweb.org/anthology/D19-1544
  year: '2019'
D19-1545:
  abstract: "Programmers typically organize executable source code using high-level\
    \ coding patterns or idiomatic structures such as nested loops, exception handlers\
    \ and recursive blocks, rather than as individual code tokens. In contrast, state\
    \ of the art (SOTA) semantic parsers still map natural language instructions to\
    \ source code by building the code syntax tree one node at a time. In this paper,\
    \ we introduce an iterative method to extract code idioms from large source code\
    \ corpora by repeatedly collapsing most-frequent depth-2 subtrees of their syntax\
    \ trees, and train semantic parsers to apply these idioms during decoding. Applying\
    \ idiom-based decoding on a recent context-dependent semantic parsing task improves\
    \ the SOTA by 2.2% BLEU score while reducing training time by more than 50%. This\
    \ improved speed enables us to scale up the model by training on an extended training\
    \ set that is 5\xD7 larger, to further move up the SOTA by an additional 2.3%\
    \ BLEU and 0.9% exact match. Finally, idioms also significantly improve accuracy\
    \ of semantic parsing to SQL on the ATIS-SQL dataset, when training data is limited.\
    \ larger, to further move up the SOTA by an additional 2.3% BLEU and 0.9% exact\
    \ match. Finally, idioms also significantly improve accuracy of semantic parsing\
    \ to SQL on the ATIS-SQL dataset, when training data is limited."
  address: Hong Kong, China
  attachment:
  - filename: D19-1545.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1545.Attachment.zip
  author:
  - first: Srinivasan
    full: Srinivasan Iyer
    id: srinivasan-iyer
    last: Iyer
  - first: Alvin
    full: Alvin Cheung
    id: alvin-cheung
    last: Cheung
  - first: Luke
    full: Luke Zettlemoyer
    id: luke-zettlemoyer
    last: Zettlemoyer
  author_string: Srinivasan Iyer, Alvin Cheung, Luke Zettlemoyer
  bibkey: iyer-etal-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1545
  month: November
  page_first: '5426'
  page_last: '5435'
  pages: "5426\u20135435"
  paper_id: '545'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1545.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1545.jpg
  title: Learning Programmatic Idioms for Scalable Semantic Parsing
  title_html: Learning Programmatic Idioms for Scalable Semantic Parsing
  url: https://www.aclweb.org/anthology/D19-1545
  year: '2019'
D19-1546:
  abstract: 'Interactive programming with interleaved code snippet cells and natural
    language markdown is recently gaining popularity in the form of Jupyter notebooks,
    which accelerate prototyping and collaboration. To study code generation conditioned
    on a long context history, we present JuICe, a corpus of 1.5 million examples
    with a curated test set of 3.7K instances based on online programming assignments.
    Compared with existing contextual code generation datasets, JuICe provides refined
    human-curated data, open-domain code, and an order of magnitude more training
    data. Using JuICe, we train models for two tasks: (1) generation of the API call
    sequence in a code cell, and (2) full code cell generation, both conditioned on
    the NL-Code history up to a particular code cell. Experiments using current baseline
    code generation models show that both context and distant supervision aid in generation,
    and that the dataset is challenging for current systems.'
  address: Hong Kong, China
  author:
  - first: Rajas
    full: Rajas Agashe
    id: rajas-agashe
    last: Agashe
  - first: Srinivasan
    full: Srinivasan Iyer
    id: srinivasan-iyer
    last: Iyer
  - first: Luke
    full: Luke Zettlemoyer
    id: luke-zettlemoyer
    last: Zettlemoyer
  author_string: Rajas Agashe, Srinivasan Iyer, Luke Zettlemoyer
  bibkey: agashe-etal-2019-juice
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1546
  month: November
  page_first: '5436'
  page_last: '5446'
  pages: "5436\u20135446"
  paper_id: '546'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1546.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1546.jpg
  title: 'JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based
    Code Generation'
  title_html: '<span class="acl-fixed-case">J</span>u<span class="acl-fixed-case">IC</span>e:
    A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code
    Generation'
  url: https://www.aclweb.org/anthology/D19-1546
  year: '2019'
D19-1547:
  abstract: 'As a promising paradigm, interactive semantic parsing has shown to improve
    both semantic parsing accuracy and user confidence in the results. In this paper,
    we propose a new, unified formulation of the interactive semantic parsing problem,
    where the goal is to design a model-based intelligent agent. The agent maintains
    its own state as the current predicted semantic parse, decides whether and where
    human intervention is needed, and generates a clarification question in natural
    language. A key part of the agent is a world model: it takes a percept (either
    an initial question or subsequent feedback from the user) and transitions to a
    new state. We then propose a simple yet remarkably effective instantiation of
    our framework, demonstrated on two text-to-SQL datasets (WikiSQL and Spider) with
    different state-of-the-art base semantic parsers. Compared to an existing interactive
    semantic parsing approach that treats the base parser as a black box, our approach
    solicits less user feedback but yields higher run-time accuracy.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1547.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1547.Attachment.pdf
  author:
  - first: Ziyu
    full: Ziyu Yao
    id: ziyu-yao
    last: Yao
  - first: Yu
    full: Yu Su
    id: yu-su
    last: Su
  - first: Huan
    full: Huan Sun
    id: huan-sun
    last: Sun
  - first: Wen-tau
    full: Wen-tau Yih
    id: wen-tau-yih
    last: Yih
  author_string: Ziyu Yao, Yu Su, Huan Sun, Wen-tau Yih
  bibkey: yao-etal-2019-model
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1547
  month: November
  page_first: '5447'
  page_last: '5458'
  pages: "5447\u20135458"
  paper_id: '547'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1547.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1547.jpg
  title: 'Model-based Interactive Semantic Parsing: A Unified Framework and A Text-to-SQL
    Case Study'
  title_html: 'Model-based Interactive Semantic Parsing: A Unified Framework and A
    Text-to-<span class="acl-fixed-case">SQL</span> Case Study'
  url: https://www.aclweb.org/anthology/D19-1547
  year: '2019'
D19-1548:
  abstract: Recent studies on AMR-to-text generation often formalize the task as a
    sequence-to-sequence (seq2seq) learning problem by converting an Abstract Meaning
    Representation (AMR) graph into a word sequences. Graph structures are further
    modeled into the seq2seq framework in order to utilize the structural information
    in the AMR graphs. However, previous approaches only consider the relations between
    directly connected concepts while ignoring the rich structure in AMR graphs. In
    this paper we eliminate such a strong limitation and propose a novel structure-aware
    self-attention approach to better model the relations between indirectly connected
    concepts in the state-of-the-art seq2seq model, i.e. the Transformer. In particular,
    a few different methods are explored to learn structural representations between
    two concepts. Experimental results on English AMR benchmark datasets show that
    our approach significantly outperforms the state-of-the-art with 29.66 and 31.82
    BLEU scores on LDC2015E86 and LDC2017T10, respectively. To the best of our knowledge,
    these are the best results achieved so far by supervised models on the benchmarks.
  address: Hong Kong, China
  author:
  - first: Jie
    full: Jie Zhu
    id: jie-zhu
    last: Zhu
  - first: Junhui
    full: Junhui Li
    id: junhui-li
    last: Li
  - first: Muhua
    full: Muhua Zhu
    id: muhua-zhu
    last: Zhu
  - first: Longhua
    full: Longhua Qian
    id: longhua-qian
    last: Qian
  - first: Min
    full: Min Zhang
    id: min-zhang
    last: Zhang
  - first: Guodong
    full: Guodong Zhou
    id: guodong-zhou
    last: Zhou
  author_string: Jie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, Guodong Zhou
  bibkey: zhu-etal-2019-modeling
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1548
  month: November
  page_first: '5459'
  page_last: '5468'
  pages: "5459\u20135468"
  paper_id: '548'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1548.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1548.jpg
  title: Modeling Graph Structure in Transformer for Better AMR-to-Text Generation
  title_html: Modeling Graph Structure in Transformer for Better <span class="acl-fixed-case">AMR</span>-to-Text
    Generation
  url: https://www.aclweb.org/anthology/D19-1548
  year: '2019'
D19-1549:
  abstract: Aspect level sentiment classification aims to identify the sentiment expressed
    towards an aspect given a context sentence. Previous neural network based methods
    largely ignore the syntax structure in one sentence. In this paper, we propose
    a novel target-dependent graph attention network (TD-GAT) for aspect level sentiment
    classification, which explicitly utilizes the dependency relationship among words.
    Using the dependency graph, it propagates sentiment features directly from the
    syntactic context of an aspect target. In our experiments, we show our method
    outperforms multiple baselines with GloVe embeddings. We also demonstrate that
    using BERT representations further substantially boosts the performance.
  address: Hong Kong, China
  author:
  - first: Binxuan
    full: Binxuan Huang
    id: binxuan-huang
    last: Huang
  - first: Kathleen
    full: Kathleen Carley
    id: kathleen-m-carley
    last: Carley
  author_string: Binxuan Huang, Kathleen Carley
  bibkey: huang-carley-2019-syntax
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1549
  month: November
  page_first: '5469'
  page_last: '5477'
  pages: "5469\u20135477"
  paper_id: '549'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1549.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1549.jpg
  title: Syntax-Aware Aspect Level Sentiment Classification with Graph Attention Networks
  title_html: Syntax-Aware Aspect Level Sentiment Classification with Graph Attention
    Networks
  url: https://www.aclweb.org/anthology/D19-1549
  year: '2019'
D19-1550:
  abstract: "Targeted sentiment analysis is the task of jointly predicting target\
    \ entities and their associated sentiment information. Existing research efforts\
    \ mostly regard this joint task as a sequence labeling problem, building models\
    \ that can capture explicit structures in the output space. However, the importance\
    \ of capturing implicit global structural information that resides in the input\
    \ space is largely unexplored. In this work, we argue that both types of information\
    \ (implicit and explicit structural information) are crucial for building a successful\
    \ targeted sentiment analysis model. Our experimental results show that properly\
    \ capturing both information is able to lead to better performance than competitive\
    \ existing approaches. We also conduct extensive experiments to investigate our\
    \ model\u2019s effectiveness and robustness."
  address: Hong Kong, China
  attachment:
  - filename: D19-1550.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1550.Attachment.pdf
  author:
  - first: Hao
    full: Hao Li
    id: hao-li
    last: Li
  - first: Wei
    full: Wei Lu
    id: wei-lu
    last: Lu
  author_string: Hao Li, Wei Lu
  bibkey: li-lu-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1550
  month: November
  page_first: '5478'
  page_last: '5488'
  pages: "5478\u20135488"
  paper_id: '550'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1550.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1550.jpg
  title: Learning Explicit and Implicit Structures for Targeted Sentiment Analysis
  title_html: Learning Explicit and Implicit Structures for Targeted Sentiment Analysis
  url: https://www.aclweb.org/anthology/D19-1550
  year: '2019'
D19-1551:
  abstract: Aspect-level sentiment classification is a crucial task for sentiment
    analysis, which aims to identify the sentiment polarities of specific targets
    in their context. The main challenge comes from multi-aspect sentences, which
    express multiple sentiment polarities towards different targets, resulting in
    overlapped feature representation. However, most existing neural models tend to
    utilize static pooling operation or attention mechanism to identify sentimental
    words, which therefore insufficient for dealing with overlapped features. To solve
    this problem, we propose to utilize capsule network to construct vector-based
    feature representation and cluster features by an EM routing algorithm. Furthermore,
    interactive attention mechanism is introduced in the capsule routing procedure
    to model the semantic relationship between aspect terms and context. The iterative
    routing also enables encoding sentence from a global perspective. Experimental
    results on three datasets show that our proposed model achieves state-of-the-art
    performance.
  address: Hong Kong, China
  author:
  - first: Chunning
    full: Chunning Du
    id: chunning-du
    last: Du
  - first: Haifeng
    full: Haifeng Sun
    id: haifeng-sun
    last: Sun
  - first: Jingyu
    full: Jingyu Wang
    id: jingyu-wang
    last: Wang
  - first: Qi
    full: Qi Qi
    id: qi-qi
    last: Qi
  - first: Jianxin
    full: Jianxin Liao
    id: jianxin-liao
    last: Liao
  - first: Tong
    full: Tong Xu
    id: tong-xu
    last: Xu
  - first: Ming
    full: Ming Liu
    id: ming-liu
    last: Liu
  author_string: Chunning Du, Haifeng Sun, Jingyu Wang, Qi Qi, Jianxin Liao, Tong
    Xu, Ming Liu
  bibkey: du-etal-2019-capsule
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1551
  month: November
  page_first: '5489'
  page_last: '5498'
  pages: "5489\u20135498"
  paper_id: '551'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1551.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1551.jpg
  title: Capsule Network with Interactive Attention for Aspect-Level Sentiment Classification
  title_html: Capsule Network with Interactive Attention for Aspect-Level Sentiment
    Classification
  url: https://www.aclweb.org/anthology/D19-1551
  year: '2019'
D19-1552:
  abstract: "There have been a recent line of works to automatically predict the emotions\
    \ of posts in social media. Existing approaches consider the posts individually\
    \ and predict their emotions independently. Different from previous researches,\
    \ we explore the dependence among relevant posts via the authors\u2019 backgrounds,\
    \ since the authors with similar backgrounds, e.g., gender, location, tend to\
    \ express similar emotions. However, such personal attributes are not easy to\
    \ obtain in most social media websites, and it is hard to capture attributes-aware\
    \ words to connect similar people. Accordingly, we propose a Neural Personal Discrimination\
    \ (NPD) approach to address above challenges by determining personal attributes\
    \ from posts, and connecting relevant posts with similar attributes to jointly\
    \ learn their emotions. In particular, we employ adversarial discriminators to\
    \ determine the personal attributes, with attention mechanisms to aggregate attributes-aware\
    \ words. In this way, social correlationship among different posts can be better\
    \ addressed. Experimental results show the usefulness of personal attributes,\
    \ and the effectiveness of our proposed NPD approach in capturing such personal\
    \ attributes with significant gains over the state-of-the-art models."
  address: Hong Kong, China
  attachment:
  - filename: D19-1552.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1552.Attachment.zip
  author:
  - first: Xiabing
    full: Xiabing Zhou
    id: xiabing-zhou
    last: Zhou
  - first: Zhongqing
    full: Zhongqing Wang
    id: zhongqing-wang
    last: Wang
  - first: Shoushan
    full: Shoushan Li
    id: shoushan-li
    last: Li
  - first: Guodong
    full: Guodong Zhou
    id: guodong-zhou
    last: Zhou
  - first: Min
    full: Min Zhang
    id: min-zhang
    last: Zhang
  author_string: Xiabing Zhou, Zhongqing Wang, Shoushan Li, Guodong Zhou, Min Zhang
  bibkey: zhou-etal-2019-emotion
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1552
  month: November
  page_first: '5499'
  page_last: '5507'
  pages: "5499\u20135507"
  paper_id: '552'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1552.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1552.jpg
  title: Emotion Detection with Neural Personal Discrimination
  title_html: Emotion Detection with Neural Personal Discrimination
  url: https://www.aclweb.org/anthology/D19-1552
  year: '2019'
D19-1553:
  abstract: The task of unsupervised sentiment modification aims to reverse the sentiment
    polarity of the input text while preserving its semantic content without any parallel
    data. Most previous work follows a two-step process. They first separate the content
    from the original sentiment, and then directly generate text with the target sentiment
    only based on the content produced by the first step. However, the second step
    bears both the target sentiment addition and content reconstruction, thus resulting
    in a lack of specific information like proper nouns in the generated text. To
    remedy this, we propose a specificity-driven cascading approach in this work,
    which can effectively increase the specificity of the generated text and further
    improve content preservation. In addition, we propose a more reasonable metric
    to evaluate sentiment modification. The experiments show that our approach outperforms
    competitive baselines by a large margin, which achieves 11% and 38% relative improvements
    of the overall metric on the Yelp and Amazon datasets, respectively.
  address: Hong Kong, China
  author:
  - first: Pengcheng
    full: Pengcheng Yang
    id: pengcheng-yang
    last: Yang
  - first: Junyang
    full: Junyang Lin
    id: junyang-lin
    last: Lin
  - first: Jingjing
    full: Jingjing Xu
    id: jingjing-xu
    last: Xu
  - first: Jun
    full: Jun Xie
    id: jun-xie
    last: Xie
  - first: Qi
    full: Qi Su
    id: qi-su
    last: Su
  - first: Xu
    full: Xu Sun
    id: xu-sun
    last: Sun
  author_string: Pengcheng Yang, Junyang Lin, Jingjing Xu, Jun Xie, Qi Su, Xu Sun
  bibkey: yang-etal-2019-specificity
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1553
  month: November
  page_first: '5508'
  page_last: '5517'
  pages: "5508\u20135517"
  paper_id: '553'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1553.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1553.jpg
  title: Specificity-Driven Cascading Approach for Unsupervised Sentiment Modification
  title_html: Specificity-Driven Cascading Approach for Unsupervised Sentiment Modification
  url: https://www.aclweb.org/anthology/D19-1553
  year: '2019'
D19-1554:
  abstract: Recent work has shown that current text classification models are fragile
    and sensitive to simple perturbations. In this work, we propose a novel adversarial
    training approach, LexicalAT, to improve the robustness of current classification
    models. The proposed approach consists of a generator and a classifier. The generator
    learns to generate examples to attack the classifier while the classifier learns
    to defend these attacks. Considering the diversity of attacks, the generator uses
    a large-scale lexical knowledge base, WordNet, to generate attacking examples
    by replacing some words in training examples with their synonyms (e.g., sad and
    unhappy), neighbor words (e.g., fox and wolf), or super-superior words (e.g.,
    chair and armchair). Due to the discrete generation step in the generator, we
    use policy gradient, a reinforcement learning approach, to train the two modules.
    Experiments show LexicalAT outperforms strong baselines and reduces test errors
    on various neural networks, including CNN, RNN, and BERT.
  address: Hong Kong, China
  author:
  - first: Jingjing
    full: Jingjing Xu
    id: jingjing-xu
    last: Xu
  - first: Liang
    full: Liang Zhao
    id: liang-zhao
    last: Zhao
  - first: Hanqi
    full: Hanqi Yan
    id: hanqi-yan
    last: Yan
  - first: Qi
    full: Qi Zeng
    id: qi-zeng
    last: Zeng
  - first: Yun
    full: Yun Liang
    id: yun-liang
    last: Liang
  - first: Xu
    full: Xu Sun
    id: xu-sun
    last: Sun
  author_string: Jingjing Xu, Liang Zhao, Hanqi Yan, Qi Zeng, Yun Liang, Xu Sun
  bibkey: xu-etal-2019-lexicalat
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1554
  month: November
  page_first: '5518'
  page_last: '5527'
  pages: "5518\u20135527"
  paper_id: '554'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1554.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1554.jpg
  title: 'LexicalAT: Lexical-Based Adversarial Reinforcement Training for Robust Sentiment
    Classification'
  title_html: '<span class="acl-fixed-case">L</span>exical<span class="acl-fixed-case">AT</span>:
    Lexical-Based Adversarial Reinforcement Training for Robust Sentiment Classification'
  url: https://www.aclweb.org/anthology/D19-1554
  year: '2019'
D19-1555:
  abstract: Opinionated text often involves attributes such as authorship and location
    that influence the sentiments expressed for different aspects. We posit that structural
    and semantic correspondence is both prevalent in opinionated text, especially
    when associated with attributes, and crucial in accurately revealing its latent
    aspect and sentiment structure. However, it is not recognized by existing approaches.
    We propose Trait, an unsupervised probabilistic model that discovers aspects and
    sentiments from text and associates them with different attributes. To this end,
    Trait infers and leverages structural and semantic correspondence using a Markov
    Random Field. We show empirically that by incorporating attributes explicitly
    Trait significantly outperforms state-of-the-art baselines both by generating
    attribute profiles that accord with our intuitions, as shown via visualization,
    and yielding topics of greater semantic cohesion.
  address: Hong Kong, China
  author:
  - first: Zhe
    full: Zhe Zhang
    id: zhe-zhang
    last: Zhang
  - first: Munindar
    full: Munindar Singh
    id: munindar-p-singh
    last: Singh
  author_string: Zhe Zhang, Munindar Singh
  bibkey: zhang-singh-2019-leveraging
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1555
  month: November
  page_first: '5528'
  page_last: '5538'
  pages: "5528\u20135538"
  paper_id: '555'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1555.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1555.jpg
  title: Leveraging Structural and Semantic Correspondence for Attribute-Oriented
    Aspect Sentiment Discovery
  title_html: Leveraging Structural and Semantic Correspondence for Attribute-Oriented
    Aspect Sentiment Discovery
  url: https://www.aclweb.org/anthology/D19-1555
  year: '2019'
D19-1556:
  abstract: The task of predicting fine grained user opinion based on spontaneous
    spoken language is a key problem arising in the development of Computational Agents
    as well as in the development of social network based opinion miners. Unfortunately,
    gathering reliable data on which a model can be trained is notoriously difficult
    and existing works rely only on coarsely labeled opinions. In this work we aim
    at bridging the gap separating fine grained opinion models already developed for
    written language and coarse grained models developed for spontaneous multimodal
    opinion mining. We take advantage of the implicit hierarchical structure of opinions
    to build a joint fine and coarse grained opinion model that exploits different
    views of the opinion expression. The resulting model shares some properties with
    attention-based models and is shown to provide competitive results on a recently
    released multimodal fine grained annotated corpus.
  address: Hong Kong, China
  attachment:
  - filename: D19-1556.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1556.Attachment.pdf
  author:
  - first: Alexandre
    full: Alexandre Garcia
    id: alexandre-garcia
    last: Garcia
  - first: Pierre
    full: Pierre Colombo
    id: pierre-colombo
    last: Colombo
  - first: Florence
    full: "Florence d\u2019Alch\xE9-Buc"
    id: florence-dalche-buc
    last: "d\u2019Alch\xE9-Buc"
  - first: Slim
    full: Slim Essid
    id: slim-essid
    last: Essid
  - first: "Chlo\xE9"
    full: "Chlo\xE9 Clavel"
    id: chloe-clavel
    last: Clavel
  author_string: "Alexandre Garcia, Pierre Colombo, Florence d\u2019Alch\xE9-Buc,\
    \ Slim Essid, Chlo\xE9 Clavel"
  bibkey: garcia-etal-2019-token
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1556
  month: November
  page_first: '5539'
  page_last: '5548'
  pages: "5539\u20135548"
  paper_id: '556'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1556.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1556.jpg
  title: 'From the Token to the Review: A Hierarchical Multimodal approach to Opinion
    Mining'
  title_html: 'From the Token to the Review: A Hierarchical Multimodal approach to
    Opinion Mining'
  url: https://www.aclweb.org/anthology/D19-1556
  year: '2019'
D19-1557:
  abstract: This paper proposes a way to improve the performance of existing algorithms
    for text classification in domains with strong language semantics. A proposed
    domain adaptation layer learns weights to combine a generic and a domain specific
    (DS) word embedding into a domain adapted (DA) embedding. The DA word embeddings
    are then used as inputs to a generic encoder + classifier framework to perform
    a downstream task such as classification. This adaptation layer is particularly
    suited to data sets that are modest in size, and which are, therefore, not ideal
    candidates for (re)training a deep neural network architecture. Results on binary
    and multi-class classification tasks using popular encoder architectures, including
    current state-of-the-art methods (with and without the shallow adaptation layer)
    show the effectiveness of the proposed approach.
  address: Hong Kong, China
  attachment:
  - filename: D19-1557.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1557.Attachment.zip
  author:
  - first: Prathusha
    full: Prathusha K Sarma
    id: prathusha-kameswara-sarma
    last: K Sarma
  - first: Yingyu
    full: Yingyu Liang
    id: yingyu-liang
    last: Liang
  - first: William
    full: William Sethares
    id: william-sethares
    last: Sethares
  author_string: Prathusha K Sarma, Yingyu Liang, William Sethares
  bibkey: k-sarma-etal-2019-shallow
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1557
  month: November
  page_first: '5549'
  page_last: '5558'
  pages: "5549\u20135558"
  paper_id: '557'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1557.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1557.jpg
  title: Shallow Domain Adaptive Embeddings for Sentiment Analysis
  title_html: Shallow Domain Adaptive Embeddings for Sentiment Analysis
  url: https://www.aclweb.org/anthology/D19-1557
  year: '2019'
D19-1558:
  abstract: Cross-domain sentiment classification has drawn much attention in recent
    years. Most existing approaches focus on learning domain-invariant representations
    in both the source and target domains, while few of them pay attention to the
    domain-specific information. Despite the non-transferability of the domain-specific
    information, simultaneously learning domain-dependent representations can facilitate
    the learning of domain-invariant representations. In this paper, we focus on aspect-level
    cross-domain sentiment classification, and propose to distill the domain-invariant
    sentiment features with the help of an orthogonal domain-dependent task, i.e.
    aspect detection, which is built on the aspects varying widely in different domains.
    We conduct extensive experiments on three public datasets and the experimental
    results demonstrate the effectiveness of our method.
  address: Hong Kong, China
  author:
  - first: Mengting
    full: Mengting Hu
    id: mengting-hu
    last: Hu
  - first: Yike
    full: Yike Wu
    id: yike-wu
    last: Wu
  - first: Shiwan
    full: Shiwan Zhao
    id: shiwan-zhao
    last: Zhao
  - first: Honglei
    full: Honglei Guo
    id: honglei-guo
    last: Guo
  - first: Renhong
    full: Renhong Cheng
    id: renhong-cheng
    last: Cheng
  - first: Zhong
    full: Zhong Su
    id: zhong-su
    last: Su
  author_string: Mengting Hu, Yike Wu, Shiwan Zhao, Honglei Guo, Renhong Cheng, Zhong
    Su
  bibkey: hu-etal-2019-domain
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1558
  month: November
  page_first: '5559'
  page_last: '5568'
  pages: "5559\u20135568"
  paper_id: '558'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1558.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1558.jpg
  title: Domain-Invariant Feature Distillation for Cross-Domain Sentiment Classification
  title_html: Domain-Invariant Feature Distillation for Cross-Domain Sentiment Classification
  url: https://www.aclweb.org/anthology/D19-1558
  year: '2019'
D19-1559:
  abstract: Aspect based sentiment analysis (ABSA) aims to identify the sentiment
    polarity towards the given aspect in a sentence, while previous models typically
    exploit an aspect-independent (weakly associative) encoder for sentence representation
    generation. In this paper, we propose a novel Aspect-Guided Deep Transition model,
    named AGDT, which utilizes the given aspect to guide the sentence encoding from
    scratch with the specially-designed deep transition architecture. Furthermore,
    an aspect-oriented objective is designed to enforce AGDT to reconstruct the given
    aspect with the generated sentence representation. In doing so, our AGDT can accurately
    generate aspect-specific sentence representation, and thus conduct more accurate
    sentiment predictions. Experimental results on multiple SemEval datasets demonstrate
    the effectiveness of our proposed approach, which significantly outperforms the
    best reported results with the same setting.
  address: Hong Kong, China
  author:
  - first: Yunlong
    full: Yunlong Liang
    id: yunlong-liang
    last: Liang
  - first: Fandong
    full: Fandong Meng
    id: fandong-meng
    last: Meng
  - first: Jinchao
    full: Jinchao Zhang
    id: jinchao-zhang
    last: Zhang
  - first: Jinan
    full: Jinan Xu
    id: jinan-xu
    last: Xu
  - first: Yufeng
    full: Yufeng Chen
    id: yufeng-chen
    last: Chen
  - first: Jie
    full: Jie Zhou
    id: jie-zhou
    last: Zhou
  author_string: Yunlong Liang, Fandong Meng, Jinchao Zhang, Jinan Xu, Yufeng Chen,
    Jie Zhou
  bibkey: liang-etal-2019-novel
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1559
  month: November
  page_first: '5569'
  page_last: '5580'
  pages: "5569\u20135580"
  paper_id: '559'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1559.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1559.jpg
  title: A Novel Aspect-Guided Deep Transition Model for Aspect Based Sentiment Analysis
  title_html: A Novel Aspect-Guided Deep Transition Model for Aspect Based Sentiment
    Analysis
  url: https://www.aclweb.org/anthology/D19-1559
  year: '2019'
D19-1560:
  abstract: Recently, neural networks have shown promising results on Document-level
    Aspect Sentiment Classification (DASC). However, these approaches often offer
    little transparency w.r.t. their inner working mechanisms and lack interpretability.
    In this paper, to simulating the steps of analyzing aspect sentiment in a document
    by human beings, we propose a new Hierarchical Reinforcement Learning (HRL) approach
    to DASC. This approach incorporates clause selection and word selection strategies
    to tackle the data noise problem in the task of DASC. First, a high-level policy
    is proposed to select aspect-relevant clauses and discard noisy clauses. Then,
    a low-level policy is proposed to select sentiment-relevant words and discard
    noisy words inside the selected clauses. Finally, a sentiment rating predictor
    is designed to provide reward signals to guide both clause and word selection.
    Experimental results demonstrate the impressive effectiveness of the proposed
    approach to DASC over the state-of-the-art baselines.
  address: Hong Kong, China
  author:
  - first: Jingjing
    full: Jingjing Wang
    id: jingjing-wang
    last: Wang
  - first: Changlong
    full: Changlong Sun
    id: changlong-sun
    last: Sun
  - first: Shoushan
    full: Shoushan Li
    id: shoushan-li
    last: Li
  - first: Jiancheng
    full: Jiancheng Wang
    id: jiancheng-wang
    last: Wang
  - first: Luo
    full: Luo Si
    id: luo-si
    last: Si
  - first: Min
    full: Min Zhang
    id: min-zhang
    last: Zhang
  - first: Xiaozhong
    full: Xiaozhong Liu
    id: xiaozhong-liu
    last: Liu
  - first: Guodong
    full: Guodong Zhou
    id: guodong-zhou
    last: Zhou
  author_string: Jingjing Wang, Changlong Sun, Shoushan Li, Jiancheng Wang, Luo Si,
    Min Zhang, Xiaozhong Liu, Guodong Zhou
  bibkey: wang-etal-2019-human
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1560
  month: November
  page_first: '5581'
  page_last: '5590'
  pages: "5581\u20135590"
  paper_id: '560'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1560.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1560.jpg
  title: 'Human-Like Decision Making: Document-level Aspect Sentiment Classification
    via Hierarchical Reinforcement Learning'
  title_html: 'Human-Like Decision Making: Document-level Aspect Sentiment Classification
    via Hierarchical Reinforcement Learning'
  url: https://www.aclweb.org/anthology/D19-1560
  year: '2019'
D19-1561:
  abstract: 'In Natural Language Understanding, the task of response generation is
    usually focused on responses to short texts, such as tweets or a turn in a dialog.
    Here we present a novel task of producing a critical response to a long argumentative
    text, and suggest a method based on general rebuttal arguments to address it.
    We do this in the context of the recently-suggested task of listening comprehension
    over argumentative content: given a speech on some specified topic, and a list
    of relevant arguments, the goal is to determine which of the arguments appear
    in the speech. The general rebuttals we describe here (in English) overcome the
    need for topic-specific arguments to be provided, by proving to be applicable
    for a large set of topics. This allows creating responses beyond the scope of
    topics for which specific arguments are available. All data collected during this
    work is freely available for research.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1561.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1561.Attachment.zip
  author:
  - first: Matan
    full: Matan Orbach
    id: matan-orbach
    last: Orbach
  - first: Yonatan
    full: Yonatan Bilu
    id: yonatan-bilu
    last: Bilu
  - first: Ariel
    full: Ariel Gera
    id: ariel-gera
    last: Gera
  - first: Yoav
    full: Yoav Kantor
    id: yoav-kantor
    last: Kantor
  - first: Lena
    full: Lena Dankin
    id: lena-dankin
    last: Dankin
  - first: Tamar
    full: Tamar Lavee
    id: tamar-lavee
    last: Lavee
  - first: Lili
    full: Lili Kotlerman
    id: lili-kotlerman
    last: Kotlerman
  - first: Shachar
    full: Shachar Mirkin
    id: shachar-mirkin
    last: Mirkin
  - first: Michal
    full: Michal Jacovi
    id: michal-jacovi
    last: Jacovi
  - first: Ranit
    full: Ranit Aharonov
    id: ranit-aharonov
    last: Aharonov
  - first: Noam
    full: Noam Slonim
    id: noam-slonim
    last: Slonim
  author_string: Matan Orbach, Yonatan Bilu, Ariel Gera, Yoav Kantor, Lena Dankin,
    Tamar Lavee, Lili Kotlerman, Shachar Mirkin, Michal Jacovi, Ranit Aharonov, Noam
    Slonim
  bibkey: orbach-etal-2019-dataset
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1561
  month: November
  page_first: '5591'
  page_last: '5601'
  pages: "5591\u20135601"
  paper_id: '561'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1561.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1561.jpg
  title: A Dataset of General-Purpose Rebuttal
  title_html: A Dataset of General-Purpose Rebuttal
  url: https://www.aclweb.org/anthology/D19-1561
  year: '2019'
D19-1562:
  abstract: 'Text attributes, such as user and product information in product reviews,
    have been used to improve the performance of sentiment classification models.
    The de facto standard method is to incorporate them as additional biases in the
    attention mechanism, and more performance gains are achieved by extending the
    model architecture. In this paper, we show that the above method is the least
    effective way to represent and inject attributes. To demonstrate this hypothesis,
    unlike previous models with complicated architectures, we limit our base model
    to a simple BiLSTM with attention classifier, and instead focus on how and where
    the attributes should be incorporated in the model. We propose to represent attributes
    as chunk-wise importance weight matrices and consider four locations in the model
    (i.e., embedding, encoding, attention, classifier) to inject attributes. Experiments
    show that our proposed method achieves significant improvements over the standard
    approach and that attention mechanism is the worst location to inject attributes,
    contradicting prior work. We also outperform the state-of-the-art despite our
    use of a simple base model. Finally, we show that these representations transfer
    well to other tasks. Model implementation and datasets are released here: https://github.com/rktamplayo/CHIM.'
  address: Hong Kong, China
  author:
  - first: Reinald Kim
    full: Reinald Kim Amplayo
    id: reinald-kim-amplayo
    last: Amplayo
  author_string: Reinald Kim Amplayo
  bibkey: amplayo-2019-rethinking
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1562
  month: November
  page_first: '5602'
  page_last: '5613'
  pages: "5602\u20135613"
  paper_id: '562'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1562.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1562.jpg
  title: Rethinking Attribute Representation and Injection for Sentiment Classification
  title_html: Rethinking Attribute Representation and Injection for Sentiment Classification
  url: https://www.aclweb.org/anthology/D19-1562
  year: '2019'
D19-1563:
  abstract: Emotion cause analysis, which aims to identify the reasons behind emotions,
    is a key topic in sentiment analysis. A variety of neural network models have
    been proposed recently, however, these previous models mostly focus on the learning
    architecture with local textual information, ignoring the discourse and prior
    knowledge, which play crucial roles in human text comprehension. In this paper,
    we propose a new method to extract emotion cause with a hierarchical neural model
    and knowledge-based regularizations, which aims to incorporate discourse context
    information and restrain the parameters by sentiment lexicon and common knowledge.
    The experimental results demonstrate that our proposed method achieves the state-of-the-art
    performance on two public datasets in different languages (Chinese and English),
    outperforming a number of competitive baselines by at least 2.08% in F-measure.
  address: Hong Kong, China
  author:
  - first: Chuang
    full: Chuang Fan
    id: chuang-fan
    last: Fan
  - first: Hongyu
    full: Hongyu Yan
    id: hongyu-yan
    last: Yan
  - first: Jiachen
    full: Jiachen Du
    id: jiachen-du
    last: Du
  - first: Lin
    full: Lin Gui
    id: lin-gui
    last: Gui
  - first: Lidong
    full: Lidong Bing
    id: lidong-bing
    last: Bing
  - first: Min
    full: Min Yang
    id: min-yang
    last: Yang
  - first: Ruifeng
    full: Ruifeng Xu
    id: ruifeng-xu
    last: Xu
  - first: Ruibin
    full: Ruibin Mao
    id: ruibin-mao
    last: Mao
  author_string: Chuang Fan, Hongyu Yan, Jiachen Du, Lin Gui, Lidong Bing, Min Yang,
    Ruifeng Xu, Ruibin Mao
  bibkey: fan-etal-2019-knowledge
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1563
  month: November
  page_first: '5614'
  page_last: '5624'
  pages: "5614\u20135624"
  paper_id: '563'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1563.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1563.jpg
  title: A Knowledge Regularized Hierarchical Approach for Emotion Cause Analysis
  title_html: A Knowledge Regularized Hierarchical Approach for Emotion Cause Analysis
  url: https://www.aclweb.org/anthology/D19-1563
  year: '2019'
D19-1564:
  abstract: We explore the task of automatic assessment of argument quality. To that
    end, we actively collected 6.3k arguments, more than a factor of five compared
    to previously examined data. Each argument was explicitly and carefully annotated
    for its quality. In addition, 14k pairs of arguments were annotated independently,
    identifying the higher quality argument in each pair. In spite of the inherent
    subjective nature of the task, both annotation schemes led to surprisingly consistent
    results. We release the labeled datasets to the community. Furthermore, we suggest
    neural methods based on a recently released language model, for argument ranking
    as well as for argument-pair classification. In the former task, our results are
    comparable to state-of-the-art; in the latter task our results significantly outperform
    earlier methods.
  address: Hong Kong, China
  attachment:
  - filename: D19-1564.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1564.Attachment.zip
  author:
  - first: Assaf
    full: Assaf Toledo
    id: assaf-toledo
    last: Toledo
  - first: Shai
    full: Shai Gretz
    id: shai-gretz
    last: Gretz
  - first: Edo
    full: Edo Cohen-Karlik
    id: edo-cohen-karlik
    last: Cohen-Karlik
  - first: Roni
    full: Roni Friedman
    id: roni-friedman
    last: Friedman
  - first: Elad
    full: Elad Venezian
    id: elad-venezian
    last: Venezian
  - first: Dan
    full: Dan Lahav
    id: dan-lahav
    last: Lahav
  - first: Michal
    full: Michal Jacovi
    id: michal-jacovi
    last: Jacovi
  - first: Ranit
    full: Ranit Aharonov
    id: ranit-aharonov
    last: Aharonov
  - first: Noam
    full: Noam Slonim
    id: noam-slonim
    last: Slonim
  author_string: Assaf Toledo, Shai Gretz, Edo Cohen-Karlik, Roni Friedman, Elad Venezian,
    Dan Lahav, Michal Jacovi, Ranit Aharonov, Noam Slonim
  bibkey: toledo-etal-2019-automatic
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1564
  month: November
  page_first: '5625'
  page_last: '5635'
  pages: "5625\u20135635"
  paper_id: '564'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1564.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1564.jpg
  title: Automatic Argument Quality Assessment - New Datasets and Methods
  title_html: Automatic Argument Quality Assessment - New Datasets and Methods
  url: https://www.aclweb.org/anthology/D19-1564
  year: '2019'
D19-1565:
  abstract: "Propaganda aims at influencing people\u2019s mindset with the purpose\
    \ of advancing a specific agenda. Previous work has addressed propaganda detection\
    \ at document level, typically labelling all articles from a propagandistic news\
    \ outlet as propaganda. Such noisy gold labels inevitably affect the quality of\
    \ any learning system trained on them. A further issue with most existing systems\
    \ is the lack of explainability. To overcome these limitations, we propose a novel\
    \ task: performing fine-grained analysis of texts by detecting all fragments that\
    \ contain propaganda techniques as well as their type. In particular, we create\
    \ a corpus of news articles manually annotated at fragment level with eighteen\
    \ propaganda techniques and propose a suitable evaluation measure. We further\
    \ design a novel multi-granularity neural network, and we show that it outperforms\
    \ several strong BERT-based baselines."
  address: Hong Kong, China
  attachment:
  - filename: D19-1565.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1565.Attachment.zip
  author:
  - first: Giovanni
    full: Giovanni Da San Martino
    id: giovanni-da-san-martino
    last: Da San Martino
  - first: Seunghak
    full: Seunghak Yu
    id: seunghak-yu
    last: Yu
  - first: Alberto
    full: "Alberto Barr\xF3n-Cede\xF1o"
    id: alberto-barron-cedeno
    last: "Barr\xF3n-Cede\xF1o"
  - first: Rostislav
    full: Rostislav Petrov
    id: rostislav-petrov
    last: Petrov
  - first: Preslav
    full: Preslav Nakov
    id: preslav-nakov
    last: Nakov
  author_string: "Giovanni Da San Martino, Seunghak Yu, Alberto Barr\xF3n-Cede\xF1\
    o, Rostislav Petrov, Preslav Nakov"
  bibkey: da-san-martino-etal-2019-fine
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1565
  month: November
  page_first: '5636'
  page_last: '5646'
  pages: "5636\u20135646"
  paper_id: '565'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1565.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1565.jpg
  title: Fine-Grained Analysis of Propaganda in News Article
  title_html: Fine-Grained Analysis of Propaganda in News Article
  url: https://www.aclweb.org/anthology/D19-1565
  year: '2019'
D19-1566:
  abstract: In recent times, multi-modal analysis has been an emerging and highly
    sought-after field at the intersection of natural language processing, computer
    vision, and speech processing. The prime objective of such studies is to leverage
    the diversified information, (e.g., textual, acoustic and visual), for learning
    a model. The effective interaction among these modalities often leads to a better
    system in terms of performance. In this paper, we introduce a recurrent neural
    network based approach for the multi-modal sentiment and emotion analysis. The
    proposed model learns the inter-modal interaction among the participating modalities
    through an auto-encoder mechanism. We employ a context-aware attention module
    to exploit the correspondence among the neighboring utterances. We evaluate our
    proposed approach for five standard multi-modal affect analysis datasets. Experimental
    results suggest the efficacy of the proposed model for both sentiment and emotion
    analysis over various existing state-of-the-art systems.
  address: Hong Kong, China
  author:
  - first: Dushyant Singh
    full: Dushyant Singh Chauhan
    id: dushyant-singh-chauhan
    last: Chauhan
  - first: Md Shad
    full: Md Shad Akhtar
    id: md-shad-akhtar
    last: Akhtar
  - first: Asif
    full: Asif Ekbal
    id: asif-ekbal
    last: Ekbal
  - first: Pushpak
    full: Pushpak Bhattacharyya
    id: pushpak-bhattacharyya
    last: Bhattacharyya
  author_string: Dushyant Singh Chauhan, Md Shad Akhtar, Asif Ekbal, Pushpak Bhattacharyya
  bibkey: chauhan-etal-2019-context
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1566
  month: November
  page_first: '5647'
  page_last: '5657'
  pages: "5647\u20135657"
  paper_id: '566'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1566.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1566.jpg
  title: Context-aware Interactive Attention for Multi-modal Sentiment and Emotion
    Analysis
  title_html: Context-aware Interactive Attention for Multi-modal Sentiment and Emotion
    Analysis
  url: https://www.aclweb.org/anthology/D19-1566
  year: '2019'
D19-1567:
  abstract: 'Text classification has been one of the major problems in natural language
    processing. With the advent of deep learning, convolutional neural network (CNN)
    has been a popular solution to this task. However, CNNs which were first proposed
    for images, face many crucial challenges in the context of text processing, namely
    in their elementary blocks: convolution filters and max pooling. These challenges
    have largely been overlooked by the most existing CNN models proposed for text
    classification. In this paper, we present an experimental study on the fundamental
    blocks of CNNs in text categorization. Based on this critique, we propose Sequential
    Convolutional Attentive Recurrent Network (SCARN). The proposed SCARN model utilizes
    both the advantages of recurrent and convolutional structures efficiently in comparison
    to previously proposed recurrent convolutional models. We test our model on different
    text classification datasets across tasks like sentiment analysis and question
    classification. Extensive experiments establish that SCARN outperforms other recurrent
    convolutional architectures with significantly less parameters. Furthermore, SCARN
    achieves better performance compared to equally large various deep CNN and LSTM
    architectures.'
  address: Hong Kong, China
  author:
  - first: Avinash
    full: Avinash Madasu
    id: avinash-madasu
    last: Madasu
  - first: Vijjini
    full: Vijjini Anvesh Rao
    id: vijjini-anvesh-rao
    last: Anvesh Rao
  author_string: Avinash Madasu, Vijjini Anvesh Rao
  bibkey: madasu-anvesh-rao-2019-sequential
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1567
  month: November
  page_first: '5658'
  page_last: '5667'
  pages: "5658\u20135667"
  paper_id: '567'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1567.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1567.jpg
  title: Sequential Learning of Convolutional Features for Effective Text Classification
  title_html: Sequential Learning of Convolutional Features for Effective Text Classification
  url: https://www.aclweb.org/anthology/D19-1567
  year: '2019'
D19-1568:
  abstract: "Research in the social sciences and psychology has shown that the persuasiveness\
    \ of an argument depends not only the language employed, but also on attributes\
    \ of the source/communicator, the audience, and the appropriateness and strength\
    \ of the argument\u2019s claims given the pragmatic and discourse context of the\
    \ argument. Among these characteristics of persuasive arguments, prior work in\
    \ NLP does not explicitly investigate the effect of the pragmatic and discourse\
    \ context when determining argument quality. This paper presents a new dataset\
    \ to initiate the study of this aspect of argumentation: it consists of a diverse\
    \ collection of arguments covering 741 controversial topics and comprising over\
    \ 47,000 claims. We further propose predictive models that incorporate the pragmatic\
    \ and discourse context of argumentative claims and show that they outperform\
    \ models that rely only on claim-specific linguistic features for predicting the\
    \ perceived impact of individual claims within a particular line of argument."
  address: Hong Kong, China
  author:
  - first: Esin
    full: Esin Durmus
    id: esin-durmus
    last: Durmus
  - first: Faisal
    full: Faisal Ladhak
    id: faisal-ladhak
    last: Ladhak
  - first: Claire
    full: Claire Cardie
    id: claire-cardie
    last: Cardie
  author_string: Esin Durmus, Faisal Ladhak, Claire Cardie
  bibkey: durmus-etal-2019-role
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1568
  month: November
  page_first: '5668'
  page_last: '5678'
  pages: "5668\u20135678"
  paper_id: '568'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1568.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1568.jpg
  title: The Role of Pragmatic and Discourse Context in Determining Argument Impact
  title_html: The Role of Pragmatic and Discourse Context in Determining Argument
    Impact
  url: https://www.aclweb.org/anthology/D19-1568
  year: '2019'
D19-1569:
  abstract: We propose a method based on neural networks to identify the sentiment
    polarity of opinion words expressed on a specific aspect of a sentence. Although
    a large majority of works typically focus on leveraging the expressive power of
    neural networks in handling this task, we explore the possibility of integrating
    dependency trees with neural networks for representation learning. To this end,
    we present a convolution over a dependency tree (CDT) model which exploits a Bi-directional
    Long Short Term Memory (Bi-LSTM) to learn representations for features of a sentence,
    and further enhance the embeddings with a graph convolutional network (GCN) which
    operates directly on the dependency tree of the sentence. Our approach propagates
    both contextual and dependency information from opinion words to aspect words,
    offering discriminative properties for supervision. Experimental results ranks
    our approach as the new state-of-the-art in aspect-based sentiment classification.
  address: Hong Kong, China
  author:
  - first: Kai
    full: Kai Sun
    id: kai-sun
    last: Sun
  - first: Richong
    full: Richong Zhang
    id: richong-zhang
    last: Zhang
  - first: Samuel
    full: Samuel Mensah
    id: samuel-mensah
    last: Mensah
  - first: Yongyi
    full: Yongyi Mao
    id: yongyi-mao
    last: Mao
  - first: Xudong
    full: Xudong Liu
    id: xudong-liu
    last: Liu
  author_string: Kai Sun, Richong Zhang, Samuel Mensah, Yongyi Mao, Xudong Liu
  bibkey: sun-etal-2019-aspect
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1569
  month: November
  page_first: '5679'
  page_last: '5688'
  pages: "5679\u20135688"
  paper_id: '569'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1569.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1569.jpg
  title: Aspect-Level Sentiment Analysis Via Convolution over Dependency Tree
  title_html: Aspect-Level Sentiment Analysis Via Convolution over Dependency Tree
  url: https://www.aclweb.org/anthology/D19-1569
  year: '2019'
D19-1570:
  abstract: 'Many Data Augmentation (DA) methods have been proposed for neural machine
    translation. Existing works measure the superiority of DA methods in terms of
    their performance on a specific test set, but we find that some DA methods do
    not exhibit consistent improvements across translation tasks. Based on the observation,
    this paper makes an initial attempt to answer a fundamental question: what benefits,
    which are consistent across different methods and tasks, does DA in general obtain?
    Inspired by recent theoretic advances in deep learning, the paper understands
    DA from two perspectives towards the generalization ability of a model: input
    sensitivity and prediction margin, which are defined independent of specific test
    set thereby may lead to findings with relatively low variance. Extensive experiments
    show that relatively consistent benefits across five DA methods and four translation
    tasks are achieved regarding both perspectives.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1570.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1570.Attachment.pdf
  author:
  - first: Guanlin
    full: Guanlin Li
    id: guanlin-li
    last: Li
  - first: Lemao
    full: Lemao Liu
    id: lemao-liu
    last: Liu
  - first: Guoping
    full: Guoping Huang
    id: guoping-huang
    last: Huang
  - first: Conghui
    full: Conghui Zhu
    id: conghui-zhu
    last: Zhu
  - first: Tiejun
    full: Tiejun Zhao
    id: tiejun-zhao
    last: Zhao
  author_string: Guanlin Li, Lemao Liu, Guoping Huang, Conghui Zhu, Tiejun Zhao
  bibkey: li-etal-2019-understanding
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1570
  month: November
  page_first: '5689'
  page_last: '5695'
  pages: "5689\u20135695"
  paper_id: '570'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1570.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1570.jpg
  title: 'Understanding Data Augmentation in Neural Machine Translation: Two Perspectives
    towards Generalization'
  title_html: 'Understanding Data Augmentation in Neural Machine Translation: Two
    Perspectives towards Generalization'
  url: https://www.aclweb.org/anthology/D19-1570
  year: '2019'
D19-1571:
  abstract: "Previous work on neural noisy channel modeling relied on latent variable\
    \ models that incrementally process the source and target sentence. This makes\
    \ decoding decisions based on partial source prefixes even though the full source\
    \ is available. We pursue an alternative approach based on standard sequence to\
    \ sequence models which utilize the entire source. These models perform remarkably\
    \ well as channel models, even though they have neither been trained on, nor designed\
    \ to factor over incomplete target sentences. Experiments with neural language\
    \ models trained on billions of words show that noisy channel models can outperform\
    \ a direct model by up to 3.2 BLEU on WMT\u201917 German-English translation.\
    \ We evaluate on four language-pairs and our channel models consistently outperform\
    \ strong alternatives such right-to-left reranking models and ensembles of direct\
    \ models."
  address: Hong Kong, China
  attachment:
  - filename: D19-1571.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1571.Attachment.pdf
  author:
  - first: Kyra
    full: Kyra Yee
    id: kyra-yee
    last: Yee
  - first: Yann
    full: Yann Dauphin
    id: yann-dauphin
    last: Dauphin
  - first: Michael
    full: Michael Auli
    id: michael-auli
    last: Auli
  author_string: Kyra Yee, Yann Dauphin, Michael Auli
  bibkey: yee-etal-2019-simple
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1571
  month: November
  page_first: '5696'
  page_last: '5701'
  pages: "5696\u20135701"
  paper_id: '571'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1571.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1571.jpg
  title: Simple and Effective Noisy Channel Modeling for Neural Machine Translation
  title_html: Simple and Effective Noisy Channel Modeling for Neural Machine Translation
  url: https://www.aclweb.org/anthology/D19-1571
  year: '2019'
D19-1572:
  abstract: Pretrained language models are promising particularly for low-resource
    languages as they only require unlabelled data. However, training existing models
    requires huge amounts of compute, while pretrained cross-lingual models often
    underperform on low-resource languages. We propose Multi-lingual language model
    Fine-Tuning (MultiFiT) to enable practitioners to train and fine-tune language
    models efficiently in their own language. In addition, we propose a zero-shot
    method using an existing pretrained cross-lingual model. We evaluate our methods
    on two widely used cross-lingual classification datasets where they outperform
    models pretrained on orders of magnitude more data and compute. We release all
    models and code.
  address: Hong Kong, China
  attachment:
  - filename: D19-1572.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1572.Attachment.zip
  author:
  - first: Julian
    full: Julian Eisenschlos
    id: julian-eisenschlos
    last: Eisenschlos
  - first: Sebastian
    full: Sebastian Ruder
    id: sebastian-ruder
    last: Ruder
  - first: Piotr
    full: Piotr Czapla
    id: piotr-czapla
    last: Czapla
  - first: Marcin
    full: Marcin Kadras
    id: marcin-kadras
    last: Kadras
  - first: Sylvain
    full: Sylvain Gugger
    id: sylvain-gugger
    last: Gugger
  - first: Jeremy
    full: Jeremy Howard
    id: jeremy-howard
    last: Howard
  author_string: Julian Eisenschlos, Sebastian Ruder, Piotr Czapla, Marcin Kadras,
    Sylvain Gugger, Jeremy Howard
  bibkey: eisenschlos-etal-2019-multifit
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1572
  month: November
  page_first: '5702'
  page_last: '5707'
  pages: "5702\u20135707"
  paper_id: '572'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1572.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1572.jpg
  title: 'MultiFiT: Efficient Multi-lingual Language Model Fine-tuning'
  title_html: '<span class="acl-fixed-case">M</span>ulti<span class="acl-fixed-case">F</span>i<span
    class="acl-fixed-case">T</span>: Efficient Multi-lingual Language Model Fine-tuning'
  url: https://www.aclweb.org/anthology/D19-1572
  year: '2019'
D19-1573:
  abstract: Due to the unparallelizable nature of the autoregressive factorization,
    AutoRegressive Translation (ART) models have to generate tokens sequentially during
    decoding and thus suffer from high inference latency. Non-AutoRegressive Translation
    (NART) models were proposed to reduce the inference time, but could only achieve
    inferior translation accuracy. In this paper, we proposed a novel approach to
    leveraging the hints from hidden states and word alignments to help the training
    of NART models. The results achieve significant improvement over previous NART
    models for the WMT14 En-De and De-En datasets and are even comparable to a strong
    LSTM-based ART baseline but one order of magnitude faster in inference.
  address: Hong Kong, China
  attachment:
  - filename: D19-1573.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1573.Attachment.zip
  author:
  - first: Zhuohan
    full: Zhuohan Li
    id: zhuohan-li
    last: Li
  - first: Zi
    full: Zi Lin
    id: zi-lin
    last: Lin
  - first: Di
    full: Di He
    id: di-he
    last: He
  - first: Fei
    full: Fei Tian
    id: fei-tian
    last: Tian
  - first: Tao
    full: Tao Qin
    id: tao-qin
    last: Qin
  - first: Liwei
    full: Liwei Wang
    id: liwei-wang
    last: Wang
  - first: Tie-Yan
    full: Tie-Yan Liu
    id: tie-yan-liu
    last: Liu
  author_string: Zhuohan Li, Zi Lin, Di He, Fei Tian, Tao Qin, Liwei Wang, Tie-Yan
    Liu
  bibkey: li-etal-2019-hint
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1573
  month: November
  page_first: '5708'
  page_last: '5713'
  pages: "5708\u20135713"
  paper_id: '573'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1573.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1573.jpg
  title: Hint-Based Training for Non-Autoregressive Machine Translation
  title_html: Hint-Based Training for Non-Autoregressive Machine Translation
  url: https://www.aclweb.org/anthology/D19-1573
  year: '2019'
D19-1574:
  abstract: 'This paper explores the task of leveraging typology in the context of
    cross-lingual dependency parsing. While this linguistic information has shown
    great promise in pre-neural parsing, results for neural architectures have been
    mixed. The aim of our investigation is to better understand this state-of-the-art.
    Our main findings are as follows: 1) The benefit of typological information is
    derived from coarsely grouping languages into syntactically-homogeneous clusters
    rather than from learning to leverage variations along individual typological
    dimensions in a compositional manner; 2) Typology consistent with the actual corpus
    statistics yields better transfer performance; 3) Typological similarity is only
    a rough proxy of cross-lingual transferability with respect to parsing.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1574.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1574.Attachment.zip
  author:
  - first: Adam
    full: Adam Fisch
    id: adam-fisch
    last: Fisch
  - first: Jiang
    full: Jiang Guo
    id: jiang-guo
    last: Guo
  - first: Regina
    full: Regina Barzilay
    id: regina-barzilay
    last: Barzilay
  author_string: Adam Fisch, Jiang Guo, Regina Barzilay
  bibkey: fisch-etal-2019-working
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1574
  month: November
  page_first: '5714'
  page_last: '5720'
  pages: "5714\u20135720"
  paper_id: '574'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1574.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1574.jpg
  title: 'Working Hard or Hardly Working: Challenges of Integrating Typology into
    Neural Dependency Parsers'
  title_html: 'Working Hard or Hardly Working: Challenges of Integrating Typology
    into Neural Dependency Parsers'
  url: https://www.aclweb.org/anthology/D19-1574
  year: '2019'
D19-1575:
  abstract: This paper investigates the problem of learning cross-lingual representations
    in a contextual space. We propose Cross-Lingual BERT Transformation (CLBT), a
    simple and efficient approach to generate cross-lingual contextualized word embeddings
    based on publicly available pre-trained BERT models (Devlin et al., 2018). In
    this approach, a linear transformation is learned from contextual word alignments
    to align the contextualized embeddings independently trained in different languages.
    We demonstrate the effectiveness of this approach on zero-shot cross-lingual transfer
    parsing. Experiments show that our embeddings substantially outperform the previous
    state-of-the-art that uses static embeddings. We further compare our approach
    with XLM (Lample and Conneau, 2019), a recently proposed cross-lingual language
    model trained with massive parallel data, and achieve highly competitive results.
  address: Hong Kong, China
  attachment:
  - filename: D19-1575.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1575.Attachment.pdf
  author:
  - first: Yuxuan
    full: Yuxuan Wang
    id: yuxuan-wang
    last: Wang
  - first: Wanxiang
    full: Wanxiang Che
    id: wanxiang-che
    last: Che
  - first: Jiang
    full: Jiang Guo
    id: jiang-guo
    last: Guo
  - first: Yijia
    full: Yijia Liu
    id: yijia-liu
    last: Liu
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  author_string: Yuxuan Wang, Wanxiang Che, Jiang Guo, Yijia Liu, Ting Liu
  bibkey: wang-etal-2019-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1575
  month: November
  page_first: '5721'
  page_last: '5727'
  pages: "5721\u20135727"
  paper_id: '575'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1575.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1575.jpg
  title: Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing
  title_html: Cross-Lingual <span class="acl-fixed-case">BERT</span> Transformation
    for Zero-Shot Dependency Parsing
  url: https://www.aclweb.org/anthology/D19-1575
  year: '2019'
D19-1576:
  abstract: The key to multilingual grammar induction is to couple grammar parameters
    of different languages together by exploiting the similarity between languages.
    Previous work relies on linguistic phylogenetic knowledge to specify similarity
    between languages. In this work, we propose a novel universal grammar induction
    approach that represents language identities with continuous vectors and employs
    a neural network to predict grammar parameters based on the representation. Without
    any prior linguistic phylogenetic knowledge, we automatically capture similarity
    between languages with the vector representations and softly tie the grammar parameters
    of different languages. In our experiments, we apply our approach to 15 languages
    across 8 language families and subfamilies in the Universal Dependency Treebank
    dataset, and we observe substantial performance gain on average over monolingual
    and multilingual baselines.
  address: Hong Kong, China
  author:
  - first: Wenjuan
    full: Wenjuan Han
    id: wenjuan-han
    last: Han
  - first: Ge
    full: Ge Wang
    id: ge-wang
    last: Wang
  - first: Yong
    full: Yong Jiang
    id: yong-jiang
    last: Jiang
  - first: Kewei
    full: Kewei Tu
    id: kewei-tu
    last: Tu
  author_string: Wenjuan Han, Ge Wang, Yong Jiang, Kewei Tu
  bibkey: han-etal-2019-multilingual
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1576
  month: November
  page_first: '5728'
  page_last: '5733'
  pages: "5728\u20135733"
  paper_id: '576'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1576.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1576.jpg
  title: Multilingual Grammar Induction with Continuous Language Identification
  title_html: Multilingual Grammar Induction with Continuous Language Identification
  url: https://www.aclweb.org/anthology/D19-1576
  year: '2019'
D19-1577:
  abstract: "Many of the world\u2019s languages employ grammatical gender on the lexeme.\
    \ For instance, in Spanish, house \u201Ccasa\u201D is feminine, whereas the word\
    \ for paper \u201Cpapel\u201D is masculine. To a speaker of a genderless language,\
    \ this categorization seems to exist with neither rhyme nor reason. But, is the\
    \ association of nouns to gender classes truly arbitrary? In this work, we present\
    \ the first large-scale investigation of the arbitrariness of gender assignment\
    \ that uses canonical correlation analysis as a method for correlating the gender\
    \ of inanimate nouns with their lexical semantic meaning. We find that the gender\
    \ systems of 18 languages exhibit a significant correlation with an externally\
    \ grounded definition of lexical semantics."
  address: Hong Kong, China
  author:
  - first: Adina
    full: Adina Williams
    id: adina-williams
    last: Williams
  - first: Damian
    full: Damian Blasi
    id: damian-blasi
    last: Blasi
  - first: Lawrence
    full: Lawrence Wolf-Sonkin
    id: lawrence-wolf-sonkin
    last: Wolf-Sonkin
  - first: Hanna
    full: Hanna Wallach
    id: hanna-wallach
    last: Wallach
  - first: Ryan
    full: Ryan Cotterell
    id: ryan-cotterell
    last: Cotterell
  author_string: Adina Williams, Damian Blasi, Lawrence Wolf-Sonkin, Hanna Wallach,
    Ryan Cotterell
  bibkey: williams-etal-2019-quantifying
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1577
  month: November
  page_first: '5734'
  page_last: '5739'
  pages: "5734\u20135739"
  paper_id: '577'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1577.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1577.jpg
  title: Quantifying the Semantic Core of Gender Systems
  title_html: Quantifying the Semantic Core of Gender Systems
  url: https://www.aclweb.org/anthology/D19-1577
  year: '2019'
D19-1578:
  abstract: "Data-driven statistical Natural Language Processing (NLP) techniques\
    \ leverage large amounts of language data to build models that can understand\
    \ language. However, most language data reflect the public discourse at the time\
    \ the data was produced, and hence NLP models are susceptible to learning incidental\
    \ associations around named referents at a particular point in time, in addition\
    \ to general linguistic meaning. An NLP system designed to model notions such\
    \ as sentiment and toxicity should ideally produce scores that are independent\
    \ of the identity of such entities mentioned in text and their social associations.\
    \ For example, in a general purpose sentiment analysis system, a phrase such as\
    \ I hate Katy Perry should be interpreted as having the same sentiment as I hate\
    \ Taylor Swift. Based on this idea, we propose a generic evaluation framework,\
    \ Perturbation Sensitivity Analysis, which detects unintended model biases related\
    \ to named entities, and requires no new annotations or corpora. We demonstrate\
    \ the utility of this analysis by employing it on two different NLP models \u2014\
    \ a sentiment model and a toxicity model \u2014 applied on online comments in\
    \ English language from four different genres."
  address: Hong Kong, China
  author:
  - first: Vinodkumar
    full: Vinodkumar Prabhakaran
    id: vinodkumar-prabhakaran
    last: Prabhakaran
  - first: Ben
    full: Ben Hutchinson
    id: ben-hutchinson
    last: Hutchinson
  - first: Margaret
    full: Margaret Mitchell
    id: margaret-mitchell
    last: Mitchell
  author_string: Vinodkumar Prabhakaran, Ben Hutchinson, Margaret Mitchell
  bibkey: prabhakaran-etal-2019-perturbation
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1578
  month: November
  page_first: '5740'
  page_last: '5745'
  pages: "5740\u20135745"
  paper_id: '578'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1578.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1578.jpg
  title: Perturbation Sensitivity Analysis to Detect Unintended Model Biases
  title_html: Perturbation Sensitivity Analysis to Detect Unintended Model Biases
  url: https://www.aclweb.org/anthology/D19-1578
  year: '2019'
D19-1579:
  abstract: "In this paper, we pose the question: do people talk about women and men\
    \ in different ways? We introduce two datasets and a novel integration of approaches\
    \ for automatically inferring gender associations from language, discovering coherent\
    \ word clusters, and labeling the clusters for the semantic concepts they represent.\
    \ The datasets allow us to compare how people write about women and men in two\
    \ different settings \u2013 one set draws from celebrity news and the other from\
    \ student reviews of computer science professors. We demonstrate that there are\
    \ large-scale differences in the ways that people talk about women and men and\
    \ that these differences vary across domains. Human evaluations show that our\
    \ methods significantly outperform strong baselines."
  address: Hong Kong, China
  attachment:
  - filename: D19-1579.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1579.Attachment.pdf
  author:
  - first: Serina
    full: Serina Chang
    id: serina-chang
    last: Chang
  - first: Kathy
    full: Kathy McKeown
    id: kathleen-mckeown
    last: McKeown
  author_string: Serina Chang, Kathy McKeown
  bibkey: chang-mckeown-2019-automatically
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1579
  month: November
  page_first: '5746'
  page_last: '5752'
  pages: "5746\u20135752"
  paper_id: '579'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1579.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1579.jpg
  title: Automatically Inferring Gender Associations from Language
  title_html: Automatically Inferring Gender Associations from Language
  url: https://www.aclweb.org/anthology/D19-1579
  year: '2019'
D19-1580:
  abstract: Official reports of hate crimes in the US are under-reported relative
    to the actual number of such incidents. Further, despite statistical approximations,
    there are no official reports from a large number of US cities regarding incidents
    of hate. Here, we first demonstrate that event extraction and multi-instance learning,
    applied to a corpus of local news articles, can be used to predict instances of
    hate crime. We then use the trained model to detect incidents of hate in cities
    for which the FBI lacks statistics. Lastly, we train models on predicting homicide
    and kidnapping, compare the predictions to FBI reports, and establish that incidents
    of hate are indeed under-reported, compared to other types of crimes, in local
    press.
  address: Hong Kong, China
  author:
  - first: Aida
    full: Aida Mostafazadeh Davani
    id: aida-mostafazadeh-davani
    last: Mostafazadeh Davani
  - first: Leigh
    full: Leigh Yeh
    id: leigh-yeh
    last: Yeh
  - first: Mohammad
    full: Mohammad Atari
    id: mohammad-atari
    last: Atari
  - first: Brendan
    full: Brendan Kennedy
    id: brendan-kennedy
    last: Kennedy
  - first: Gwenyth
    full: Gwenyth Portillo Wightman
    id: gwenyth-portillo-wightman
    last: Portillo Wightman
  - first: Elaine
    full: Elaine Gonzalez
    id: elaine-gonzalez
    last: Gonzalez
  - first: Natalie
    full: Natalie Delong
    id: natalie-delong
    last: Delong
  - first: Rhea
    full: Rhea Bhatia
    id: rhea-bhatia
    last: Bhatia
  - first: Arineh
    full: Arineh Mirinjian
    id: arineh-mirinjian
    last: Mirinjian
  - first: Xiang
    full: Xiang Ren
    id: xiang-ren
    last: Ren
  - first: Morteza
    full: Morteza Dehghani
    id: morteza-dehghani
    last: Dehghani
  author_string: Aida Mostafazadeh Davani, Leigh Yeh, Mohammad Atari, Brendan Kennedy,
    Gwenyth Portillo Wightman, Elaine Gonzalez, Natalie Delong, Rhea Bhatia, Arineh
    Mirinjian, Xiang Ren, Morteza Dehghani
  bibkey: mostafazadeh-davani-etal-2019-reporting
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1580
  month: November
  page_first: '5753'
  page_last: '5757'
  pages: "5753\u20135757"
  paper_id: '580'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1580.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1580.jpg
  title: 'Reporting the Unreported: Event Extraction for Analyzing the Local Representation
    of Hate Crimes'
  title_html: 'Reporting the Unreported: Event Extraction for Analyzing the Local
    Representation of Hate Crimes'
  url: https://www.aclweb.org/anthology/D19-1580
  year: '2019'
D19-1581:
  abstract: Recognizing affective events that trigger positive or negative sentiment
    has a wide range of natural language processing applications but remains a challenging
    problem mainly because the polarity of an event is not necessarily predictable
    from its constituent words. In this paper, we propose to propagate affective polarity
    using discourse relations. Our method is simple and only requires a very small
    seed lexicon and a large raw corpus. Our experiments using Japanese data show
    that our method learns affective events effectively without manually labeled data.
    It also improves supervised learning results when labeled data are small.
  address: Hong Kong, China
  author:
  - first: Jun
    full: Jun Saito
    id: jun-saito
    last: Saito
  - first: Yugo
    full: Yugo Murawaki
    id: yugo-murawaki
    last: Murawaki
  - first: Sadao
    full: Sadao Kurohashi
    id: sadao-kurohashi
    last: Kurohashi
  author_string: Jun Saito, Yugo Murawaki, Sadao Kurohashi
  bibkey: saito-etal-2019-minimally
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1581
  month: November
  page_first: '5758'
  page_last: '5765'
  pages: "5758\u20135765"
  paper_id: '581'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1581.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1581.jpg
  title: Minimally Supervised Learning of Affective Events Using Discourse Relations
  title_html: Minimally Supervised Learning of Affective Events Using Discourse Relations
  url: https://www.aclweb.org/anthology/D19-1581
  year: '2019'
D19-1582:
  abstract: Syntactic relations are broadly used in many NLP tasks. For event detection,
    syntactic relation representations based on dependency tree can better capture
    the interrelations between candidate trigger words and related entities than sentence
    representations. But, existing studies only use first-order syntactic relations
    (i.e., the arcs) in dependency trees to identify trigger words. For this reason,
    this paper proposes a new method for event detection, which uses a dependency
    tree based graph convolution network with aggregative attention to explicitly
    model and aggregate multi-order syntactic representations in sentences. Experimental
    comparison with state-of-the-art baselines shows the superiority of the proposed
    method.
  address: Hong Kong, China
  author:
  - first: Haoran
    full: Haoran Yan
    id: haoran-yan
    last: Yan
  - first: Xiaolong
    full: Xiaolong Jin
    id: xiaolong-jin
    last: Jin
  - first: Xiangbin
    full: Xiangbin Meng
    id: xiangbin-meng
    last: Meng
  - first: Jiafeng
    full: Jiafeng Guo
    id: jiafeng-guo
    last: Guo
  - first: Xueqi
    full: Xueqi Cheng
    id: xueqi-cheng
    last: Cheng
  author_string: Haoran Yan, Xiaolong Jin, Xiangbin Meng, Jiafeng Guo, Xueqi Cheng
  bibkey: yan-etal-2019-event
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1582
  month: November
  page_first: '5766'
  page_last: '5770'
  pages: "5766\u20135770"
  paper_id: '582'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1582.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1582.jpg
  title: Event Detection with Multi-Order Graph Convolution and Aggregated Attention
  title_html: Event Detection with Multi-Order Graph Convolution and Aggregated Attention
  url: https://www.aclweb.org/anthology/D19-1582
  year: '2019'
D19-1583:
  abstract: "Scalar implicatures are language features that imply the negation of\
    \ stronger statements, e.g., \u201CShe was married twice\u201D typically implicates\
    \ that she was not married thrice. In this paper we discuss the importance of\
    \ scalar implicatures in the context of textual information extraction. We investigate\
    \ how textual features can be used to predict whether a given text segment mentions\
    \ all objects standing in a certain relationship with a certain subject. Preliminary\
    \ results on Wikipedia indicate that this prediction is feasible, and yields informative\
    \ assessments."
  address: Hong Kong, China
  author:
  - first: Simon
    full: Simon Razniewski
    id: simon-razniewski
    last: Razniewski
  - first: Nitisha
    full: Nitisha Jain
    id: nitisha-jain
    last: Jain
  - first: Paramita
    full: Paramita Mirza
    id: paramita-mirza
    last: Mirza
  - first: Gerhard
    full: Gerhard Weikum
    id: gerhard-weikum
    last: Weikum
  author_string: Simon Razniewski, Nitisha Jain, Paramita Mirza, Gerhard Weikum
  bibkey: razniewski-etal-2019-coverage
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1583
  month: November
  page_first: '5771'
  page_last: '5776'
  pages: "5771\u20135776"
  paper_id: '583'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1583.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1583.jpg
  title: Coverage of Information Extraction from Sentences and Paragraphs
  title_html: Coverage of Information Extraction from Sentences and Paragraphs
  url: https://www.aclweb.org/anthology/D19-1583
  year: '2019'
D19-1584:
  abstract: Existing event extraction methods classify each argument role independently,
    ignoring the conceptual correlations between different argument roles. In this
    paper, we propose a Hierarchical Modular Event Argument Extraction (HMEAE) model,
    to provide effective inductive bias from the concept hierarchy of event argument
    roles. Specifically, we design a neural module network for each basic unit of
    the concept hierarchy, and then hierarchically compose relevant unit modules with
    logical operations into a role-oriented modular network to classify a specific
    argument role. As many argument roles share the same high-level unit module, their
    correlation can be utilized to extract specific event arguments better. Experiments
    on real-world datasets show that HMEAE can effectively leverage useful knowledge
    from the concept hierarchy and significantly outperform the state-of-the-art baselines.
    The source code can be obtained from https://github.com/thunlp/HMEAE.
  address: Hong Kong, China
  attachment:
  - filename: D19-1584.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1584.Attachment.zip
  author:
  - first: Xiaozhi
    full: Xiaozhi Wang
    id: xiaozhi-wang
    last: Wang
  - first: Ziqi
    full: Ziqi Wang
    id: ziqi-wang
    last: Wang
  - first: Xu
    full: Xu Han
    id: xu-han
    last: Han
  - first: Zhiyuan
    full: Zhiyuan Liu
    id: zhiyuan-liu
    last: Liu
  - first: Juanzi
    full: Juanzi Li
    id: juanzi-li
    last: Li
  - first: Peng
    full: Peng Li
    id: peng-li
    last: Li
  - first: Maosong
    full: Maosong Sun
    id: maosong-sun
    last: Sun
  - first: Jie
    full: Jie Zhou
    id: jie-zhou
    last: Zhou
  - first: Xiang
    full: Xiang Ren
    id: xiang-ren
    last: Ren
  author_string: Xiaozhi Wang, Ziqi Wang, Xu Han, Zhiyuan Liu, Juanzi Li, Peng Li,
    Maosong Sun, Jie Zhou, Xiang Ren
  bibkey: wang-etal-2019-hmeae
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1584
  month: November
  page_first: '5777'
  page_last: '5783'
  pages: "5777\u20135783"
  paper_id: '584'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1584.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1584.jpg
  title: 'HMEAE: Hierarchical Modular Event Argument Extraction'
  title_html: '<span class="acl-fixed-case">HMEAE</span>: Hierarchical Modular Event
    Argument Extraction'
  url: https://www.aclweb.org/anthology/D19-1584
  year: '2019'
D19-1585:
  abstract: 'We examine the capabilities of a unified, multi-task framework for three
    information extraction tasks: named entity recognition, relation extraction, and
    event extraction. Our framework (called DyGIE++) accomplishes all tasks by enumerating,
    refining, and scoring text spans designed to capture local (within-sentence) and
    global (cross-sentence) context. Our framework achieves state-of-the-art results
    across all tasks, on four datasets from a variety of domains. We perform experiments
    comparing different techniques to construct span representations. Contextualized
    embeddings like BERT perform well at capturing relationships among entities in
    the same or adjacent sentences, while dynamic span graph updates model long-range
    cross-sentence relationships. For instance, propagating span representations via
    predicted coreference links can enable the model to disambiguate challenging entity
    mentions. Our code is publicly available at https://github.com/dwadden/dygiepp
    and can be easily adapted for new tasks or datasets.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1585.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1585.Attachment.pdf
  author:
  - first: David
    full: David Wadden
    id: david-wadden
    last: Wadden
  - first: Ulme
    full: Ulme Wennberg
    id: ulme-wennberg
    last: Wennberg
  - first: Yi
    full: Yi Luan
    id: yi-luan
    last: Luan
  - first: Hannaneh
    full: Hannaneh Hajishirzi
    id: hannaneh-hajishirzi
    last: Hajishirzi
  author_string: David Wadden, Ulme Wennberg, Yi Luan, Hannaneh Hajishirzi
  bibkey: wadden-etal-2019-entity
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1585
  month: November
  page_first: '5784'
  page_last: '5789'
  pages: "5784\u20135789"
  paper_id: '585'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1585.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1585.jpg
  title: Entity, Relation, and Event Extraction with Contextualized Span Representations
  title_html: Entity, Relation, and Event Extraction with Contextualized Span Representations
  url: https://www.aclweb.org/anthology/D19-1585
  year: '2019'
D19-1586:
  abstract: 'Implicit discourse relation classification is one of the most difficult
    tasks in discourse parsing. Previous studies have generally focused on extracting
    better representations of the relational arguments. In order to solve the task,
    it is however additionally necessary to capture what events are expected to cause
    or follow each other. Current discourse relation classifiers fall short in this
    respect. We here show that this shortcoming can be effectively addressed by using
    the bidirectional encoder representation from transformers (BERT) proposed by
    Devlin et al. (2019), which were trained on a next-sentence prediction task, and
    thus encode a representation of likely next sentences. The BERT-based model outperforms
    the current state of the art in 11-way classification by 8% points on the standard
    PDTB dataset. Our experiments also demonstrate that the model can be successfully
    ported to other domains: on the BioDRB dataset, the model outperforms the state
    of the art system around 15% points.'
  address: Hong Kong, China
  author:
  - first: Wei
    full: Wei Shi
    id: wei-shi
    last: Shi
  - first: Vera
    full: Vera Demberg
    id: vera-demberg
    last: Demberg
  author_string: Wei Shi, Vera Demberg
  bibkey: shi-demberg-2019-next
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1586
  month: November
  page_first: '5790'
  page_last: '5796'
  pages: "5790\u20135796"
  paper_id: '586'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1586.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1586.jpg
  title: Next Sentence Prediction helps Implicit Discourse Relation Classification
    within and across Domains
  title_html: Next Sentence Prediction helps Implicit Discourse Relation Classification
    within and across Domains
  url: https://www.aclweb.org/anthology/D19-1586
  year: '2019'
D19-1587:
  abstract: Rhetorical Structure Theory (RST) parsing is crucial for many downstream
    NLP tasks that require a discourse structure for a text. Most of the previous
    RST parsers have been based on supervised learning approaches. That is, they require
    an annotated corpus of sufficient size and quality, and heavily rely on the language
    and domain dependent corpus. In this paper, we present two language-independent
    unsupervised RST parsing methods based on dynamic programming. The first one builds
    the optimal tree in terms of a dissimilarity score function that is defined for
    splitting a text span into smaller ones. The second builds the optimal tree in
    terms of a similarity score function that is defined for merging two adjacent
    spans into a large one. Experimental results on English and German RST treebanks
    showed that our parser based on span merging achieved the best score, around 0.8
    F1 score, which is close to the scores of the previous supervised parsers. score,
    which is close to the scores of the previous supervised parsers.
  address: Hong Kong, China
  author:
  - first: Naoki
    full: Naoki Kobayashi
    id: naoki-kobayashi
    last: Kobayashi
  - first: Tsutomu
    full: Tsutomu Hirao
    id: tsutomu-hirao
    last: Hirao
  - first: Kengo
    full: Kengo Nakamura
    id: kengo-nakamura
    last: Nakamura
  - first: Hidetaka
    full: Hidetaka Kamigaito
    id: hidetaka-kamigaito
    last: Kamigaito
  - first: Manabu
    full: Manabu Okumura
    id: manabu-okumura
    last: Okumura
  - first: Masaaki
    full: Masaaki Nagata
    id: masaaki-nagata
    last: Nagata
  author_string: Naoki Kobayashi, Tsutomu Hirao, Kengo Nakamura, Hidetaka Kamigaito,
    Manabu Okumura, Masaaki Nagata
  bibkey: kobayashi-etal-2019-split
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1587
  month: November
  page_first: '5797'
  page_last: '5802'
  pages: "5797\u20135802"
  paper_id: '587'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1587.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1587.jpg
  title: 'Split or Merge: Which is Better for Unsupervised RST Parsing?'
  title_html: 'Split or Merge: Which is Better for Unsupervised <span class="acl-fixed-case">RST</span>
    Parsing?'
  url: https://www.aclweb.org/anthology/D19-1587
  year: '2019'
D19-1588:
  abstract: We apply BERT to coreference resolution, achieving a new state of the
    art on the GAP (+11.5 F1) and OntoNotes (+3.9 F1) benchmarks. A qualitative analysis
    of model predictions indicates that, compared to ELMo and BERT-base, BERT-large
    is particularly better at distinguishing between related but distinct entities
    (e.g., President and CEO), but that there is still room for improvement in modeling
    document-level context, conversations, and mention paraphrasing. We will release
    all code and trained models upon publication.
  address: Hong Kong, China
  author:
  - first: Mandar
    full: Mandar Joshi
    id: mandar-joshi
    last: Joshi
  - first: Omer
    full: Omer Levy
    id: omer-levy
    last: Levy
  - first: Luke
    full: Luke Zettlemoyer
    id: luke-zettlemoyer
    last: Zettlemoyer
  - first: Daniel
    full: Daniel Weld
    id: daniel-s-weld
    last: Weld
  author_string: Mandar Joshi, Omer Levy, Luke Zettlemoyer, Daniel Weld
  bibkey: joshi-etal-2019-bert
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1588
  month: November
  page_first: '5803'
  page_last: '5808'
  pages: "5803\u20135808"
  paper_id: '588'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1588.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1588.jpg
  title: 'BERT for Coreference Resolution: Baselines and Analysis'
  title_html: '<span class="acl-fixed-case">BERT</span> for Coreference Resolution:
    Baselines and Analysis'
  url: https://www.aclweb.org/anthology/D19-1588
  year: '2019'
D19-1589:
  abstract: 'Generating a long, coherent text such as a paragraph requires a high-level
    control of different levels of relations between sentences (e.g., tense, coreference).
    We call such a logical connection between sentences as a (paragraph) flow. In
    order to produce a coherent flow of text, we explore two forms of intersentential
    relations in a paragraph: one is a human-created linguistical relation that forms
    a structure (e.g., discourse tree) and the other is a relation from latent representation
    learned from the sentences themselves. Our two proposed models incorporate each
    form of relations into document-level language models: the former is a supervised
    model that jointly learns a language model as well as discourse relation prediction,
    and the latter is an unsupervised model that is hierarchically conditioned by
    a recurrent neural network (RNN) over the latent information. Our proposed models
    with both forms of relations outperform the baselines in partially conditioned
    paragraph generation task. Our codes and data are publicly available.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1589.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1589.Attachment.pdf
  author:
  - first: Dongyeop
    full: Dongyeop Kang
    id: dongyeop-kang
    last: Kang
  - first: Eduard
    full: Eduard Hovy
    id: eduard-hovy
    last: Hovy
  author_string: Dongyeop Kang, Eduard Hovy
  bibkey: kang-hovy-2019-linguistic
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1589
  month: November
  page_first: '5809'
  page_last: '5815'
  pages: "5809\u20135815"
  paper_id: '589'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1589.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1589.jpg
  title: Linguistic Versus Latent Relations for Modeling Coherent Flow in Paragraphs
  title_html: Linguistic Versus Latent Relations for Modeling Coherent Flow in Paragraphs
  url: https://www.aclweb.org/anthology/D19-1589
  year: '2019'
D19-1590:
  abstract: "We propose new BERT-based methods for recognizing event causality such\
    \ as \u201Csmoke cigarettes\u201D \u2013> \u201Cdie of lung cancer\u201D written\
    \ in web texts. In our methods, we grasp each annotator\u2019s policy by training\
    \ multiple classifiers, each of which predicts the labels given by a single annotator,\
    \ and combine the resulting classifiers\u2019 outputs to predict the final labels\
    \ determined by majority vote. Furthermore, we investigate the effect of supplying\
    \ background knowledge to our classifiers. Since BERT models are pre-trained with\
    \ a large corpus, some sort of background knowledge for event causality may be\
    \ learned during pre-training. Our experiments with a Japanese dataset suggest\
    \ that this is actually the case: Performance improved when we pre-trained the\
    \ BERT models with web texts containing a large number of event causalities instead\
    \ of Wikipedia articles or randomly sampled web texts. However, this effect was\
    \ limited. Therefore, we further improved performance by simply adding texts related\
    \ to an input causality candidate as background knowledge to the input of the\
    \ BERT models. We believe these findings indicate a promising future research\
    \ direction."
  address: Hong Kong, China
  author:
  - first: Kazuma
    full: Kazuma Kadowaki
    id: kazuma-kadowaki
    last: Kadowaki
  - first: Ryu
    full: Ryu Iida
    id: ryu-iida
    last: Iida
  - first: Kentaro
    full: Kentaro Torisawa
    id: kentaro-torisawa
    last: Torisawa
  - first: Jong-Hoon
    full: Jong-Hoon Oh
    id: jong-hoon-oh
    last: Oh
  - first: Julien
    full: Julien Kloetzer
    id: julien-kloetzer
    last: Kloetzer
  author_string: Kazuma Kadowaki, Ryu Iida, Kentaro Torisawa, Jong-Hoon Oh, Julien
    Kloetzer
  bibkey: kadowaki-etal-2019-event
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1590
  month: November
  page_first: '5816'
  page_last: '5822'
  pages: "5816\u20135822"
  paper_id: '590'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1590.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1590.jpg
  title: "Event Causality Recognition Exploiting Multiple Annotators\u2019 Judgments\
    \ and Background Knowledge"
  title_html: "Event Causality Recognition Exploiting Multiple Annotators\u2019 Judgments\
    \ and Background Knowledge"
  url: https://www.aclweb.org/anthology/D19-1590
  year: '2019'
D19-1591:
  abstract: Memory neurons of long short-term memory (LSTM) networks encode and process
    information in powerful yet mysterious ways. While there has been work to analyze
    their behavior in carrying low-level information such as linguistic properties,
    how they directly contribute to label prediction remains unclear. We find inspiration
    from biologists and study the affinity between individual neurons and labels,
    propose a novel metric to quantify the sensitivity of neurons to each label, and
    conduct experiments to show the validity of our proposed metric. We discover that
    some neurons are trained to specialize on a subset of labels, and while dropping
    an arbitrary neuron has little effect on the overall accuracy of the model, dropping
    label-specialized neurons predictably and significantly degrades prediction accuracy
    on the associated label. We further examine the consistency of neuron-label affinity
    across different models. These observations provide insight into the inner mechanisms
    of LSTMs.
  address: Hong Kong, China
  author:
  - first: Ji
    full: Ji Xin
    id: ji-xin
    last: Xin
  - first: Jimmy
    full: Jimmy Lin
    id: jimmy-lin
    last: Lin
  - first: Yaoliang
    full: Yaoliang Yu
    id: yaoliang-yu
    last: Yu
  author_string: Ji Xin, Jimmy Lin, Yaoliang Yu
  bibkey: xin-etal-2019-part
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1591
  month: November
  page_first: '5823'
  page_last: '5830'
  pages: "5823\u20135830"
  paper_id: '591'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1591.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1591.jpg
  title: What Part of the Neural Network Does This? Understanding LSTMs by Measuring
    and Dissecting Neurons
  title_html: What Part of the Neural Network Does This? Understanding <span class="acl-fixed-case">LSTM</span>s
    by Measuring and Dissecting Neurons
  url: https://www.aclweb.org/anthology/D19-1591
  year: '2019'
D19-1592:
  abstract: Recurrent neural networks can learn to predict upcoming words remarkably
    well on average; in syntactically complex contexts, however, they often assign
    unexpectedly high probabilities to ungrammatical words. We investigate to what
    extent these shortcomings can be mitigated by increasing the size of the network
    and the corpus on which it is trained. We find that gains from increasing network
    size are minimal beyond a certain point. Likewise, expanding the training corpus
    yields diminishing returns; we estimate that the training corpus would need to
    be unrealistically large for the models to match human performance. A comparison
    to GPT and BERT, Transformer-based models trained on billions of words, reveals
    that these models perform even more poorly than our LSTMs in some constructions.
    Our results make the case for more data efficient architectures.
  address: Hong Kong, China
  attachment:
  - filename: D19-1592.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1592.Attachment.zip
  author:
  - first: Marten
    full: Marten van Schijndel
    id: marten-van-schijndel
    last: van Schijndel
  - first: Aaron
    full: Aaron Mueller
    id: aaron-mueller
    last: Mueller
  - first: Tal
    full: Tal Linzen
    id: tal-linzen
    last: Linzen
  author_string: Marten van Schijndel, Aaron Mueller, Tal Linzen
  bibkey: van-schijndel-etal-2019-quantity
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1592
  month: November
  page_first: '5831'
  page_last: '5837'
  pages: "5831\u20135837"
  paper_id: '592'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1592.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1592.jpg
  title: "Quantity doesn\u2019t buy quality syntax with neural language models"
  title_html: "Quantity doesn\u2019t buy quality syntax with neural language models"
  url: https://www.aclweb.org/anthology/D19-1592
  year: '2019'
D19-1593:
  abstract: 'Representational Similarity Analysis (RSA) is a technique developed by
    neuroscientists for comparing activity patterns of different measurement modalities
    (e.g., fMRI, electrophysiology, behavior). As a framework, RSA has several advantages
    over existing approaches to interpretation of language encoders based on probing
    or diagnostic classification: namely, it does not require large training samples,
    is not prone to overfitting, and it enables a more transparent comparison between
    the representational geometries of different models and modalities. We demonstrate
    the utility of RSA by establishing a previously unknown correspondence between
    widely-employed pretrained language encoders and human processing difficulty via
    eye-tracking data, showcasing its potential in the interpretability toolbox for
    neural models.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1593.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1593.Attachment.pdf
  author:
  - first: Mostafa
    full: Mostafa Abdou
    id: mostafa-abdou
    last: Abdou
  - first: Artur
    full: Artur Kulmizev
    id: artur-kulmizev
    last: Kulmizev
  - first: Felix
    full: Felix Hill
    id: felix-hill
    last: Hill
  - first: Daniel M.
    full: Daniel M. Low
    id: daniel-m-low
    last: Low
  - first: Anders
    full: "Anders S\xF8gaard"
    id: anders-sogaard
    last: "S\xF8gaard"
  author_string: "Mostafa Abdou, Artur Kulmizev, Felix Hill, Daniel M. Low, Anders\
    \ S\xF8gaard"
  bibkey: abdou-etal-2019-higher
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1593
  month: November
  page_first: '5838'
  page_last: '5845'
  pages: "5838\u20135845"
  paper_id: '593'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1593.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1593.jpg
  title: Higher-order Comparisons of Sentence Encoder Representations
  title_html: Higher-order Comparisons of Sentence Encoder Representations
  url: https://www.aclweb.org/anthology/D19-1593
  year: '2019'
D19-1594:
  abstract: "Domain-specific training typically makes NLP systems work better. We\
    \ show that this extends to cognitive modeling as well by relating the states\
    \ of a neural phrase-structure parser to electrophysiological measures from human\
    \ participants. These measures were recorded as participants listened to a spoken\
    \ recitation of the same literary text that was supplied as input to the neural\
    \ parser. Given more training data, the system derives a better cognitive model\
    \ \u2014 but only when the training examples come from the same textual genre.\
    \ This finding is consistent with the idea that humans adapt syntactic expectations\
    \ to particular genres during language comprehension (Kaan and Chun, 2018; Branigan\
    \ and Pickering, 2017)."
  address: Hong Kong, China
  author:
  - first: John
    full: John Hale
    id: john-hale
    last: Hale
  - first: Adhiguna
    full: Adhiguna Kuncoro
    id: adhiguna-kuncoro
    last: Kuncoro
  - first: Keith
    full: Keith Hall
    id: keith-hall
    last: Hall
  - first: Chris
    full: Chris Dyer
    id: chris-dyer
    last: Dyer
  - first: Jonathan
    full: Jonathan Brennan
    id: jonathan-brennan
    last: Brennan
  author_string: John Hale, Adhiguna Kuncoro, Keith Hall, Chris Dyer, Jonathan Brennan
  bibkey: hale-etal-2019-text
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1594
  month: November
  page_first: '5846'
  page_last: '5852'
  pages: "5846\u20135852"
  paper_id: '594'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1594.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1594.jpg
  title: Text Genre and Training Data Size in Human-like Parsing
  title_html: Text Genre and Training Data Size in Human-like Parsing
  url: https://www.aclweb.org/anthology/D19-1594
  year: '2019'
D19-1595:
  abstract: Feature norm datasets of human conceptual knowledge, collected in surveys
    of human volunteers, yield highly interpretable models of word meaning and play
    an important role in neurolinguistic research on semantic cognition. However,
    these datasets are limited in size due to practical obstacles associated with
    exhaustively listing properties for a large number of words. In contrast, the
    development of distributional modelling techniques and the availability of vast
    text corpora have allowed researchers to construct effective vector space models
    of word meaning over large lexicons. However, this comes at the cost of interpretable,
    human-like information about word meaning. We propose a method for mapping human
    property knowledge onto a distributional semantic space, which adapts the word2vec
    architecture to the task of modelling concept features. Our approach gives a measure
    of concept and feature affinity in a single semantic space, which makes for easy
    and efficient ranking of candidate human-derived semantic properties for arbitrary
    words. We compare our model with a previous approach, and show that it performs
    better on several evaluation tasks. Finally, we discuss how our method could be
    used to develop efficient sampling techniques to extend existing feature norm
    datasets in a reliable way.
  address: Hong Kong, China
  author:
  - first: Steven
    full: Steven Derby
    id: steven-derby
    last: Derby
  - first: Paul
    full: Paul Miller
    id: paul-miller
    last: Miller
  - first: Barry
    full: Barry Devereux
    id: barry-devereux
    last: Devereux
  author_string: Steven Derby, Paul Miller, Barry Devereux
  bibkey: derby-etal-2019-feature2vec
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1595
  month: November
  page_first: '5853'
  page_last: '5859'
  pages: "5853\u20135859"
  paper_id: '595'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1595.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1595.jpg
  title: 'Feature2Vec: Distributional semantic modelling of human property knowledge'
  title_html: '<span class="acl-fixed-case">F</span>eature2<span class="acl-fixed-case">V</span>ec:
    Distributional semantic modelling of human property knowledge'
  url: https://www.aclweb.org/anthology/D19-1595
  year: '2019'
D19-1596:
  abstract: "While models for Visual Question Answering (VQA) have steadily improved\
    \ over the years, interacting with one quickly reveals that these models lack\
    \ consistency. For instance, if a model answers \u201Cred\u201D to \u201CWhat\
    \ color is the balloon?\u201D, it might answer \u201Cno\u201D if asked, \u201C\
    Is the balloon red?\u201D. These responses violate simple notions of entailment\
    \ and raise questions about how effectively VQA models ground language. In this\
    \ work, we introduce a dataset, ConVQA, and metrics that enable quantitative evaluation\
    \ of consistency in VQA. For a given observable fact in an image (e.g. the balloon\u2019\
    s color), we generate a set of logically consistent question-answer (QA) pairs\
    \ (e.g. Is the balloon red?) and also collect a human-annotated set of common-sense\
    \ based consistent QA pairs (e.g. Is the balloon the same color as tomato sauce?).\
    \ Further, we propose a consistency-improving data augmentation module, a Consistency\
    \ Teacher Module (CTM). CTM automatically generates entailed (or similar-intent)\
    \ questions for a source QA pair and fine-tunes the VQA model if the VQA\u2019\
    s answer to the entailed question is consistent with the source QA pair. We demonstrate\
    \ that our CTM-based training improves the consistency of VQA models on the Con-VQA\
    \ datasets and is a strong baseline for further research."
  address: Hong Kong, China
  attachment:
  - filename: D19-1596.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1596.Attachment.pdf
  author:
  - first: Arijit
    full: Arijit Ray
    id: arijit-ray
    last: Ray
  - first: Karan
    full: Karan Sikka
    id: karan-sikka
    last: Sikka
  - first: Ajay
    full: Ajay Divakaran
    id: ajay-divakaran
    last: Divakaran
  - first: Stefan
    full: Stefan Lee
    id: stefan-lee
    last: Lee
  - first: Giedrius
    full: Giedrius Burachas
    id: giedrius-burachas
    last: Burachas
  author_string: Arijit Ray, Karan Sikka, Ajay Divakaran, Stefan Lee, Giedrius Burachas
  bibkey: ray-etal-2019-sunny
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1596
  month: November
  page_first: '5860'
  page_last: '5865'
  pages: "5860\u20135865"
  paper_id: '596'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1596.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1596.jpg
  title: Sunny and Dark Outside?! Improving Answer Consistency in VQA through Entailed
    Question Generation
  title_html: Sunny and Dark Outside?! Improving Answer Consistency in <span class="acl-fixed-case">VQA</span>
    through Entailed Question Generation
  url: https://www.aclweb.org/anthology/D19-1596
  year: '2019'
D19-1597:
  abstract: "Scenario-based question answering (SQA) has attracted increasing research\
    \ attention. It typically requires retrieving and integrating knowledge from multiple\
    \ sources, and applying general knowledge to a specific case described by a scenario.\
    \ SQA widely exists in the medical, geography, and legal domains\u2014both in\
    \ practice and in the exams. In this paper, we introduce the GeoSQA dataset. It\
    \ consists of 1,981 scenarios and 4,110 multiple-choice questions in the geography\
    \ domain at high school level, where diagrams (e.g., maps, charts) have been manually\
    \ annotated with natural language descriptions to benefit NLP research. Benchmark\
    \ results on a variety of state-of-the-art methods for question answering, textual\
    \ entailment, and reading comprehension demonstrate the unique challenges presented\
    \ by SQA for future research."
  address: Hong Kong, China
  author:
  - first: Zixian
    full: Zixian Huang
    id: zixian-huang
    last: Huang
  - first: Yulin
    full: Yulin Shen
    id: yulin-shen
    last: Shen
  - first: Xiao
    full: Xiao Li
    id: xiao-li
    last: Li
  - first: "Yu\u2019ang"
    full: "Yu\u2019ang Wei"
    id: yuang-wei
    last: Wei
  - first: Gong
    full: Gong Cheng
    id: gong-cheng
    last: Cheng
  - first: Lin
    full: Lin Zhou
    id: lin-zhou
    last: Zhou
  - first: Xinyu
    full: Xinyu Dai
    id: xinyu-dai
    last: Dai
  - first: Yuzhong
    full: Yuzhong Qu
    id: yuzhong-qu
    last: Qu
  author_string: "Zixian Huang, Yulin Shen, Xiao Li, Yu\u2019ang Wei, Gong Cheng,\
    \ Lin Zhou, Xinyu Dai, Yuzhong Qu"
  bibkey: huang-etal-2019-geosqa
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1597
  month: November
  page_first: '5866'
  page_last: '5871'
  pages: "5866\u20135871"
  paper_id: '597'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1597.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1597.jpg
  title: 'GeoSQA: A Benchmark for Scenario-based Question Answering in the Geography
    Domain at High School Level'
  title_html: '<span class="acl-fixed-case">G</span>eo<span class="acl-fixed-case">SQA</span>:
    A Benchmark for Scenario-based Question Answering in the Geography Domain at High
    School Level'
  url: https://www.aclweb.org/anthology/D19-1597
  year: '2019'
D19-1598:
  abstract: Theory of mind, i.e., the ability to reason about intents and beliefs
    of agents is an important task in artificial intelligence and central to resolving
    ambiguous references in natural language dialogue. In this work, we revisit the
    evaluation of theory of mind through question answering. We show that current
    evaluation methods are flawed and that existing benchmark tasks can be solved
    without theory of mind due to dataset biases. Based on prior work, we propose
    an improved evaluation protocol and dataset in which we explicitly control for
    data regularities via a careful examination of the answer space. We show that
    state-of-the-art methods which are successful on existing benchmarks fail to solve
    theory-of-mind tasks in our proposed approach.
  address: Hong Kong, China
  attachment:
  - filename: D19-1598.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1598.Attachment.pdf
  author:
  - first: Matthew
    full: Matthew Le
    id: matthew-le
    last: Le
  - first: Y-Lan
    full: Y-Lan Boureau
    id: y-lan-boureau
    last: Boureau
  - first: Maximilian
    full: Maximilian Nickel
    id: maximilian-nickel
    last: Nickel
  author_string: Matthew Le, Y-Lan Boureau, Maximilian Nickel
  bibkey: le-etal-2019-revisiting
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1598
  month: November
  page_first: '5872'
  page_last: '5877'
  pages: "5872\u20135877"
  paper_id: '598'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1598.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1598.jpg
  title: Revisiting the Evaluation of Theory of Mind through Question Answering
  title_html: Revisiting the Evaluation of Theory of Mind through Question Answering
  url: https://www.aclweb.org/anthology/D19-1598
  year: '2019'
D19-1599:
  abstract: BERT model has been successfully applied to open-domain QA tasks. However,
    previous work trains BERT by viewing passages corresponding to the same question
    as independent training instances, which may cause incomparable scores for answers
    from different passages. To tackle this issue, we propose a multi-passage BERT
    model to globally normalize answer scores across all passages of the same question,
    and this change enables our QA model find better answers by utilizing more passages.
    In addition, we find that splitting articles into passages with the length of
    100 words by sliding window improves performance by 4%. By leveraging a passage
    ranker to select high-quality passages, multi-passage BERT gains additional 2%.
    Experiments on four standard benchmarks showed that our multi-passage BERT outperforms
    all state-of-the-art models on all benchmarks. In particular, on the OpenSQuAD
    dataset, our model gains 21.4% EM and 21.5% F1 over all non-BERT models, and 5.8%
    EM and 6.5% F1 over BERT-based models.
  address: Hong Kong, China
  author:
  - first: Zhiguo
    full: Zhiguo Wang
    id: zhiguo-wang
    last: Wang
  - first: Patrick
    full: Patrick Ng
    id: patrick-ng
    last: Ng
  - first: Xiaofei
    full: Xiaofei Ma
    id: xiaofei-ma
    last: Ma
  - first: Ramesh
    full: Ramesh Nallapati
    id: ramesh-nallapati
    last: Nallapati
  - first: Bing
    full: Bing Xiang
    id: bing-xiang
    last: Xiang
  author_string: Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nallapati, Bing Xiang
  bibkey: wang-etal-2019-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1599
  month: November
  page_first: '5878'
  page_last: '5882'
  pages: "5878\u20135882"
  paper_id: '599'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1599.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1599.jpg
  title: 'Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question
    Answering'
  title_html: 'Multi-passage <span class="acl-fixed-case">BERT</span>: A Globally
    Normalized <span class="acl-fixed-case">BERT</span> Model for Open-domain Question
    Answering'
  url: https://www.aclweb.org/anthology/D19-1599
  year: '2019'
D19-1600:
  abstract: 'Machine Reading Comprehension (MRC) has become enormously popular recently
    and has attracted a lot of attention. However, the existing reading comprehension
    datasets are mostly in English. In this paper, we introduce a Span-Extraction
    dataset for Chinese machine reading comprehension to add language diversities
    in this area. The dataset is composed by near 20,000 real questions annotated
    on Wikipedia paragraphs by human experts. We also annotated a challenge set which
    contains the questions that need comprehensive understanding and multi-sentence
    inference throughout the context. We present several baseline systems as well
    as anonymous submissions for demonstrating the difficulties in this dataset. With
    the release of the dataset, we hosted the Second Evaluation Workshop on Chinese
    Machine Reading Comprehension (CMRC 2018). We hope the release of the dataset
    could further accelerate the Chinese machine reading comprehension research. Resources
    are available: https://github.com/ymcui/cmrc2018'
  address: Hong Kong, China
  author:
  - first: Yiming
    full: Yiming Cui
    id: yiming-cui
    last: Cui
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  - first: Wanxiang
    full: Wanxiang Che
    id: wanxiang-che
    last: Che
  - first: Li
    full: Li Xiao
    id: li-xiao
    last: Xiao
  - first: Zhipeng
    full: Zhipeng Chen
    id: zhipeng-chen
    last: Chen
  - first: Wentao
    full: Wentao Ma
    id: wentao-ma
    last: Ma
  - first: Shijin
    full: Shijin Wang
    id: shijin-wang
    last: Wang
  - first: Guoping
    full: Guoping Hu
    id: guoping-hu
    last: Hu
  author_string: Yiming Cui, Ting Liu, Wanxiang Che, Li Xiao, Zhipeng Chen, Wentao
    Ma, Shijin Wang, Guoping Hu
  bibkey: cui-etal-2019-span
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1600
  month: November
  page_first: '5883'
  page_last: '5889'
  pages: "5883\u20135889"
  paper_id: '600'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1600.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1600.jpg
  title: A Span-Extraction Dataset for Chinese Machine Reading Comprehension
  title_html: A Span-Extraction Dataset for <span class="acl-fixed-case">C</span>hinese
    Machine Reading Comprehension
  url: https://www.aclweb.org/anthology/D19-1600
  year: '2019'
D19-1601:
  abstract: 'This paper studies the problem of non-factoid question answering, where
    the answer may span over multiple sentences. Existing solutions can be categorized
    into representation- and interaction-focused approaches. We combine their complementary
    strength, by a hybrid approach allowing multi-granular interactions, but represented
    at word level, enabling an easy integration with strong word-level signals. Specifically,
    we propose MICRON: Multigranular Interaction for Contextualizing RepresentatiON,
    a novel approach which derives contextualized uni-gram representation from n-grams.
    Our contributions are as follows: First, we enable multi-granular matches between
    question and answer n-grams. Second, by contextualizing word representation with
    surrounding n-grams, MICRON can naturally utilize word-based signals for query
    term weighting, known to be effective in information retrieval. We validate MICRON
    in two public non-factoid question answering datasets: WikiPassageQA and InsuranceQA,
    showing our model achieves the state of the art among baselines with reported
    performances on both datasets.-grams. Second, by contextualizing word representation
    with surrounding n-grams, MICRON can naturally utilize word-based signals for
    query term weighting, known to be effective in information retrieval. We validate
    MICRON in two public non-factoid question answering datasets: WikiPassageQA and
    InsuranceQA, showing our model achieves the state of the art among baselines with
    reported performances on both datasets.'
  address: Hong Kong, China
  author:
  - first: Hojae
    full: Hojae Han
    id: hojae-han
    last: Han
  - first: Seungtaek
    full: Seungtaek Choi
    id: seungtaek-choi
    last: Choi
  - first: Haeju
    full: Haeju Park
    id: haeju-park
    last: Park
  - first: Seung-won
    full: Seung-won Hwang
    id: seung-won-hwang
    last: Hwang
  author_string: Hojae Han, Seungtaek Choi, Haeju Park, Seung-won Hwang
  bibkey: han-etal-2019-micron
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1601
  month: November
  page_first: '5890'
  page_last: '5895'
  pages: "5890\u20135895"
  paper_id: '601'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1601.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1601.jpg
  title: 'MICRON: Multigranular Interaction for Contextualizing RepresentatiON in
    Non-factoid Question Answering'
  title_html: '<span class="acl-fixed-case">MICRON</span>: Multigranular Interaction
    for Contextualizing <span class="acl-fixed-case">R</span>epresentati<span class="acl-fixed-case">ON</span>
    in Non-factoid Question Answering'
  url: https://www.aclweb.org/anthology/D19-1601
  year: '2019'
D19-1602:
  abstract: Leveraging external knowledge is an emerging trend in machine comprehension
    task. Previous work usually utilizes knowledge graphs such as ConceptNet as external
    knowledge, and extracts triples from them to enhance the initial representation
    of the machine comprehension context. However, such method cannot capture the
    structural information in the knowledge graph. To this end, we propose a Structural
    Knowledge Graph-aware Network(SKG) model, constructing sub-graphs for entities
    in the machine comprehension context. Our method dynamically updates the representation
    of the knowledge according to the structural information of the constructed sub-graph.
    Experiments show that SKG achieves state-of-the-art performance on the ReCoRD
    dataset.
  address: Hong Kong, China
  author:
  - first: Delai
    full: Delai Qiu
    id: delai-qiu
    last: Qiu
  - first: Yuanzhe
    full: Yuanzhe Zhang
    id: yuanzhe-zhang
    last: Zhang
  - first: Xinwei
    full: Xinwei Feng
    id: xinwei-feng
    last: Feng
  - first: Xiangwen
    full: Xiangwen Liao
    id: xiangwen-liao
    last: Liao
  - first: Wenbin
    full: Wenbin Jiang
    id: wenbin-jiang
    last: Jiang
  - first: Yajuan
    full: Yajuan Lyu
    id: yajuan-lyu
    last: Lyu
  - first: Kang
    full: Kang Liu
    id: kang-liu
    last: Liu
  - first: Jun
    full: Jun Zhao
    id: jun-zhao
    last: Zhao
  author_string: Delai Qiu, Yuanzhe Zhang, Xinwei Feng, Xiangwen Liao, Wenbin Jiang,
    Yajuan Lyu, Kang Liu, Jun Zhao
  bibkey: qiu-etal-2019-machine
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1602
  month: November
  page_first: '5896'
  page_last: '5901'
  pages: "5896\u20135901"
  paper_id: '602'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1602.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1602.jpg
  title: Machine Reading Comprehension Using Structural Knowledge Graph-aware Network
  title_html: Machine Reading Comprehension Using Structural Knowledge Graph-aware
    Network
  url: https://www.aclweb.org/anthology/D19-1602
  year: '2019'
D19-1603:
  abstract: We present a novel approach to answering sequential questions based on
    structured objects such as knowledge bases or tables without using a logical form
    as an intermediate representation. We encode tables as graphs using a graph neural
    network model based on the Transformer architecture. The answers are then selected
    from the encoded graph using a pointer network. This model is appropriate for
    processing conversations around structured data, where the attention mechanism
    that selects the answers to a question can also be used to resolve conversational
    references. We demonstrate the validity of this approach with competitive results
    on the Sequential Question Answering (SQA) task.
  address: Hong Kong, China
  author:
  - first: Thomas
    full: Thomas Mueller
    id: thomas-mueller
    last: Mueller
  - first: Francesco
    full: Francesco Piccinno
    id: francesco-piccinno
    last: Piccinno
  - first: Peter
    full: Peter Shaw
    id: peter-shaw
    last: Shaw
  - first: Massimo
    full: Massimo Nicosia
    id: massimo-nicosia
    last: Nicosia
  - first: Yasemin
    full: Yasemin Altun
    id: yasemin-altun
    last: Altun
  author_string: Thomas Mueller, Francesco Piccinno, Peter Shaw, Massimo Nicosia,
    Yasemin Altun
  bibkey: mueller-etal-2019-answering
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1603
  month: November
  page_first: '5902'
  page_last: '5910'
  pages: "5902\u20135910"
  paper_id: '603'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1603.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1603.jpg
  title: Answering Conversational Questions on Structured Data without Logical Forms
  title_html: Answering Conversational Questions on Structured Data without Logical
    Forms
  url: https://www.aclweb.org/anthology/D19-1603
  year: '2019'
D19-1604:
  abstract: In this paper, we establish the effectiveness of using hard negatives,
    coupled with a siamese network and a suitable loss function, for the tasks of
    answer selection and answer triggering. We show that the choice of sampling strategy
    is key for achieving improved performance on these tasks. Evaluating on recent
    answer selection datasets - InsuranceQA, SelQA, and an internal QA dataset, we
    show that using hard negatives with relatively simple model architectures (bag
    of words and LSTM-CNN) drives significant performance gains. On InsuranceQA, this
    strategy alone improves over previously reported results by a minimum of 1.6 points
    in P@1. Using hard negatives with a Transformer encoder provides a further improvement
    of 2.3 points. Further, we propose to use quadruplet loss for answer triggering,
    with the aim of producing globally meaningful similarity scores. We show that
    quadruplet loss function coupled with the selection of hard negatives enables
    bag-of-words models to improve F1 score by 2.3 points over previous baselines,
    on SelQA answer triggering dataset. Our results provide key insights into answer
    selection and answer triggering tasks.
  address: Hong Kong, China
  attachment:
  - filename: D19-1604.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1604.Attachment.pdf
  author:
  - first: Sawan
    full: Sawan Kumar
    id: sawan-kumar
    last: Kumar
  - first: Shweta
    full: Shweta Garg
    id: shweta-garg
    last: Garg
  - first: Kartik
    full: Kartik Mehta
    id: kartik-mehta
    last: Mehta
  - first: Nikhil
    full: Nikhil Rasiwasia
    id: nikhil-rasiwasia
    last: Rasiwasia
  author_string: Sawan Kumar, Shweta Garg, Kartik Mehta, Nikhil Rasiwasia
  bibkey: kumar-etal-2019-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1604
  month: November
  page_first: '5911'
  page_last: '5917'
  pages: "5911\u20135917"
  paper_id: '604'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1604.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1604.jpg
  title: Improving Answer Selection and Answer Triggering using Hard Negatives
  title_html: Improving Answer Selection and Answer Triggering using Hard Negatives
  url: https://www.aclweb.org/anthology/D19-1604
  year: '2019'
D19-1605:
  abstract: "Question answering is an AI-complete problem, but existing datasets lack\
    \ key elements of language understanding such as coreference and ellipsis resolution.\
    \ We consider sequential question answering: multiple questions are asked one-by-one\
    \ in a conversation between a questioner and an answerer. Answering these questions\
    \ is only possible through understanding the conversation history. We introduce\
    \ the task of question-in-context rewriting: given the context of a conversation\u2019\
    s history, rewrite a context-dependent into a self-contained question with the\
    \ same answer. We construct, CANARD, a dataset of 40,527 questions based on QuAC\
    \ (Choi et al., 2018) and train Seq2Seq models for incorporating context into\
    \ standalone questions."
  address: Hong Kong, China
  attachment:
  - filename: D19-1605.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1605.Attachment.zip
  author:
  - first: Ahmed
    full: Ahmed Elgohary
    id: ahmed-elgohary
    last: Elgohary
  - first: Denis
    full: Denis Peskov
    id: denis-peskov
    last: Peskov
  - first: Jordan
    full: Jordan Boyd-Graber
    id: jordan-boyd-graber
    last: Boyd-Graber
  author_string: Ahmed Elgohary, Denis Peskov, Jordan Boyd-Graber
  bibkey: elgohary-etal-2019-unpack
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1605
  month: November
  page_first: '5918'
  page_last: '5924'
  pages: "5918\u20135924"
  paper_id: '605'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1605.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1605.jpg
  title: Can You Unpack That? Learning to Rewrite Questions-in-Context
  title_html: Can You Unpack That? Learning to Rewrite Questions-in-Context
  url: https://www.aclweb.org/anthology/D19-1605
  year: '2019'
D19-1606:
  abstract: "Machine comprehension of texts longer than a single sentence often requires\
    \ coreference resolution. However, most current reading comprehension benchmarks\
    \ do not contain complex coreferential phenomena and hence fail to evaluate the\
    \ ability of models to resolve coreference. We present a new crowdsourced dataset\
    \ containing more than 24K span-selection questions that require resolving coreference\
    \ among entities in over 4.7K English paragraphs from Wikipedia. Obtaining questions\
    \ focused on such phenomena is challenging, because it is hard to avoid lexical\
    \ cues that shortcut complex reasoning. We deal with this issue by using a strong\
    \ baseline model as an adversary in the crowdsourcing loop, which helps crowdworkers\
    \ avoid writing questions with exploitable surface cues. We show that state-of-the-art\
    \ reading comprehension models perform significantly worse than humans on this\
    \ benchmark\u2014the best model performance is 70.5 F1, while the estimated human\
    \ performance is 93.4 F1."
  address: Hong Kong, China
  author:
  - first: Pradeep
    full: Pradeep Dasigi
    id: pradeep-dasigi
    last: Dasigi
  - first: Nelson F.
    full: Nelson F. Liu
    id: nelson-f-liu
    last: Liu
  - first: Ana
    full: "Ana Marasovi\u0107"
    id: ana-marasovic
    last: "Marasovi\u0107"
  - first: Noah A.
    full: Noah A. Smith
    id: noah-a-smith
    last: Smith
  - first: Matt
    full: Matt Gardner
    id: matt-gardner
    last: Gardner
  author_string: "Pradeep Dasigi, Nelson F. Liu, Ana Marasovi\u0107, Noah A. Smith,\
    \ Matt Gardner"
  bibkey: dasigi-etal-2019-quoref
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1606
  month: November
  page_first: '5925'
  page_last: '5932'
  pages: "5925\u20135932"
  paper_id: '606'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1606.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1606.jpg
  title: 'Quoref: A Reading Comprehension Dataset with Questions Requiring Coreferential
    Reasoning'
  title_html: '<span class="acl-fixed-case">Q</span>uoref: A Reading Comprehension
    Dataset with Questions Requiring Coreferential Reasoning'
  url: https://www.aclweb.org/anthology/D19-1606
  year: '2019'
D19-1607:
  abstract: Because it is not feasible to collect training data for every language,
    there is a growing interest in cross-lingual transfer learning. In this paper,
    we systematically explore zero-shot cross-lingual transfer learning on reading
    comprehension tasks with language representation model pre-trained on multi-lingual
    corpus. The experimental results show that with pre-trained language representation
    zero-shot learning is feasible, and translating the source data into the target
    language is not necessary and even degrades the performance. We further explore
    what does the model learn in zero-shot setting.
  address: Hong Kong, China
  attachment:
  - filename: D19-1607.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1607.Attachment.pdf
  author:
  - first: Tsung-Yuan
    full: Tsung-Yuan Hsu
    id: tsung-yuan-hsu
    last: Hsu
  - first: Chi-Liang
    full: Chi-Liang Liu
    id: chi-liang-liu
    last: Liu
  - first: Hung-yi
    full: Hung-yi Lee
    id: hung-yi-lee1
    last: Lee
  author_string: Tsung-Yuan Hsu, Chi-Liang Liu, Hung-yi Lee
  bibkey: hsu-etal-2019-zero
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1607
  month: November
  page_first: '5933'
  page_last: '5940'
  pages: "5933\u20135940"
  paper_id: '607'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1607.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1607.jpg
  title: Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual
    Language Representation Model
  title_html: Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with
    Multi-lingual Language Representation Model
  url: https://www.aclweb.org/anthology/D19-1607
  year: '2019'
D19-1608:
  abstract: "We introduce the first open-domain dataset, called QuaRTz, for reasoning\
    \ about textual qualitative relationships. QuaRTz contains general qualitative\
    \ statements, e.g., \u201CA sunscreen with a higher SPF protects the skin longer.\u201D\
    , twinned with 3864 crowdsourced situated questions, e.g., \u201CBilly is wearing\
    \ sunscreen with a lower SPF than Lucy. Who will be best protected from the sun?\u201D\
    , plus annotations of the properties being compared. Unlike previous datasets,\
    \ the general knowledge is textual and not tied to a fixed set of relationships,\
    \ and tests a system\u2019s ability to comprehend and apply textual qualitative\
    \ knowledge in a novel setting. We find state-of-the-art results are substantially\
    \ (20%) below human performance, presenting an open challenge to the NLP community."
  address: Hong Kong, China
  author:
  - first: Oyvind
    full: Oyvind Tafjord
    id: oyvind-tafjord
    last: Tafjord
  - first: Matt
    full: Matt Gardner
    id: matt-gardner
    last: Gardner
  - first: Kevin
    full: Kevin Lin
    id: kevin-lin
    last: Lin
  - first: Peter
    full: Peter Clark
    id: peter-clark
    last: Clark
  author_string: Oyvind Tafjord, Matt Gardner, Kevin Lin, Peter Clark
  bibkey: tafjord-etal-2019-quartz
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1608
  month: November
  page_first: '5941'
  page_last: '5946'
  pages: "5941\u20135946"
  paper_id: '608'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1608.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1608.jpg
  title: 'QuaRTz: An Open-Domain Dataset of Qualitative Relationship Questions'
  title_html: '<span class="acl-fixed-case">Q</span>ua<span class="acl-fixed-case">RT</span>z:
    An Open-Domain Dataset of Qualitative Relationship Questions'
  url: https://www.aclweb.org/anthology/D19-1608
  year: '2019'
D19-1609:
  abstract: "Reading comprehension models have been successfully applied to extractive\
    \ text answers, but it is unclear how best to generalize these models to abstractive\
    \ numerical answers. We enable a BERT-based reading comprehension model to perform\
    \ lightweight numerical reasoning. We augment the model with a predefined set\
    \ of executable \u2018programs\u2019 which encompass simple arithmetic as well\
    \ as extraction. Rather than having to learn to manipulate numbers directly, the\
    \ model can pick a program and execute it. On the recent Discrete Reasoning Over\
    \ Passages (DROP) dataset, designed to challenge reading comprehension models,\
    \ we show a 33% absolute improvement by adding shallow programs. The model can\
    \ learn to predict new operations when appropriate in a math word problem setting\
    \ (Roy and Roth, 2015) with very few training examples."
  address: Hong Kong, China
  attachment:
  - filename: D19-1609.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1609.Attachment.zip
  author:
  - first: Daniel
    full: Daniel Andor
    id: daniel-andor
    last: Andor
  - first: Luheng
    full: Luheng He
    id: luheng-he
    last: He
  - first: Kenton
    full: Kenton Lee
    id: kenton-lee
    last: Lee
  - first: Emily
    full: Emily Pitler
    id: emily-pitler
    last: Pitler
  author_string: Daniel Andor, Luheng He, Kenton Lee, Emily Pitler
  bibkey: andor-etal-2019-giving
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1609
  month: November
  page_first: '5947'
  page_last: '5952'
  pages: "5947\u20135952"
  paper_id: '609'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1609.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1609.jpg
  title: 'Giving BERT a Calculator: Finding Operations and Arguments with Reading
    Comprehension'
  title_html: 'Giving <span class="acl-fixed-case">BERT</span> a Calculator: Finding
    Operations and Arguments with Reading Comprehension'
  url: https://www.aclweb.org/anthology/D19-1609
  year: '2019'
D19-1610:
  abstract: 'Answer selection is an important research problem, with applications
    in many areas. Previous deep learning based approaches for the task mainly adopt
    the Compare-Aggregate architecture that performs word-level comparison followed
    by aggregation. In this work, we take a departure from the popular Compare-Aggregate
    architecture, and instead, propose a new gated self-attention memory network for
    the task. Combined with a simple transfer learning technique from a large-scale
    online corpus, our model outperforms previous methods by a large margin, achieving
    new state-of-the-art results on two standard answer selection datasets: TrecQA
    and WikiQA.'
  address: Hong Kong, China
  author:
  - first: Tuan
    full: Tuan Lai
    id: tuan-lai
    last: Lai
  - first: Quan Hung
    full: Quan Hung Tran
    id: quan-hung-tran
    last: Tran
  - first: Trung
    full: Trung Bui
    id: trung-bui
    last: Bui
  - first: Daisuke
    full: Daisuke Kihara
    id: daisuke-kihara
    last: Kihara
  author_string: Tuan Lai, Quan Hung Tran, Trung Bui, Daisuke Kihara
  bibkey: lai-etal-2019-gated
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1610
  month: November
  page_first: '5953'
  page_last: '5959'
  pages: "5953\u20135959"
  paper_id: '610'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1610.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1610.jpg
  title: A Gated Self-attention Memory Network for Answer Selection
  title_html: A Gated Self-attention Memory Network for Answer Selection
  url: https://www.aclweb.org/anthology/D19-1610
  year: '2019'
D19-1611:
  abstract: Paraphrase generation is an interesting and challenging NLP task which
    has numerous practical applications. In this paper, we analyze datasets commonly
    used for paraphrase generation research, and show that simply parroting input
    sentences surpasses state-of-the-art models in the literature when evaluated on
    standard metrics. Our findings illustrate that a model could be seemingly adept
    at generating paraphrases, despite only making trivial changes to the input sentence
    or even none at all.
  address: Hong Kong, China
  attachment:
  - filename: D19-1611.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1611.Attachment.zip
  author:
  - first: Hong-Ren
    full: Hong-Ren Mao
    id: hong-ren-mao
    last: Mao
  - first: Hung-Yi
    full: Hung-Yi Lee
    id: hung-yi-lee
    last: Lee
  author_string: Hong-Ren Mao, Hung-Yi Lee
  bibkey: mao-lee-2019-polly
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1611
  month: November
  page_first: '5960'
  page_last: '5968'
  pages: "5960\u20135968"
  paper_id: '611'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1611.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1611.jpg
  title: 'Polly Want a Cracker: Analyzing Performance of Parroting on Paraphrase Generation
    Datasets'
  title_html: 'Polly Want a Cracker: Analyzing Performance of Parroting on Paraphrase
    Generation Datasets'
  url: https://www.aclweb.org/anthology/D19-1611
  year: '2019'
D19-1612:
  abstract: Search applications often display shortened sentences which must contain
    certain query terms and must fit within the space constraints of a user interface.
    This work introduces a new transition-based sentence compression technique developed
    for such settings. Our query-focused method constructs length and lexically constrained
    compressions in linear time, by growing a subgraph in the dependency parse of
    a sentence. This theoretically efficient approach achieves an 11x empirical speedup
    over baseline ILP methods, while better reconstructing gold constrained shortenings.
    Such speedups help query-focused applications, because users are measurably hindered
    by interface lags. Additionally, our technique does not require an ILP solver
    or a GPU.
  address: Hong Kong, China
  attachment:
  - filename: D19-1612.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1612.Attachment.zip
  author:
  - first: Abram
    full: Abram Handler
    id: abram-handler
    last: Handler
  - first: Brendan
    full: "Brendan O\u2019Connor"
    id: brendan-oconnor
    last: "O\u2019Connor"
  author_string: "Abram Handler, Brendan O\u2019Connor"
  bibkey: handler-oconnor-2019-query
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1612
  month: November
  page_first: '5969'
  page_last: '5975'
  pages: "5969\u20135975"
  paper_id: '612'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1612.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1612.jpg
  title: Query-focused Sentence Compression in Linear Time
  title_html: Query-focused Sentence Compression in Linear Time
  url: https://www.aclweb.org/anthology/D19-1612
  year: '2019'
D19-1613:
  abstract: "Existing approaches to recipe generation are unable to create recipes\
    \ for users with culinary preferences but incomplete knowledge of ingredients\
    \ in specific dishes. We propose a new task of personalized recipe generation\
    \ to help these users: expanding a name and incomplete ingredient details into\
    \ complete natural-text instructions aligned with the user\u2019s historical preferences.\
    \ We attend on technique- and recipe-level representations of a user\u2019s previously\
    \ consumed recipes, fusing these \u2018user-aware\u2019 representations in an\
    \ attention fusion layer to control recipe text generation. Experiments on a new\
    \ dataset of 180K recipes and 700K interactions show our model\u2019s ability\
    \ to generate plausible and personalized recipes compared to non-personalized\
    \ baselines."
  address: Hong Kong, China
  attachment:
  - filename: D19-1613.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1613.Attachment.pdf
  author:
  - first: Bodhisattwa Prasad
    full: Bodhisattwa Prasad Majumder
    id: bodhisattwa-prasad-majumder
    last: Majumder
  - first: Shuyang
    full: Shuyang Li
    id: shuyang-li
    last: Li
  - first: Jianmo
    full: Jianmo Ni
    id: jianmo-ni
    last: Ni
  - first: Julian
    full: Julian McAuley
    id: julian-mcauley
    last: McAuley
  author_string: Bodhisattwa Prasad Majumder, Shuyang Li, Jianmo Ni, Julian McAuley
  bibkey: majumder-etal-2019-generating
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1613
  month: November
  page_first: '5976'
  page_last: '5982'
  pages: "5976\u20135982"
  paper_id: '613'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1613.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1613.jpg
  title: Generating Personalized Recipes from Historical User Preferences
  title_html: Generating Personalized Recipes from Historical User Preferences
  url: https://www.aclweb.org/anthology/D19-1613
  year: '2019'
D19-1614:
  abstract: The neural seq2seq based question generation (QG) is prone to generating
    generic and undiversified questions that are poorly relevant to the given passage
    and target answer. In this paper, we propose two methods to address the issue.
    (1) By a partial copy mechanism, we prioritize words that are morphologically
    close to words in the input passage when generating questions; (2) By a QA-based
    reranker, from the n-best list of question candidates, we select questions that
    are preferred by both the QA and QG model. Experiments and analyses demonstrate
    that the proposed two methods substantially improve the relevance of generated
    questions to passages and answers.
  address: Hong Kong, China
  author:
  - first: Jiazuo
    full: Jiazuo Qiu
    id: jiazuo-qiu
    last: Qiu
  - first: Deyi
    full: Deyi Xiong
    id: deyi-xiong
    last: Xiong
  author_string: Jiazuo Qiu, Deyi Xiong
  bibkey: qiu-xiong-2019-generating
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1614
  month: November
  page_first: '5983'
  page_last: '5987'
  pages: "5983\u20135987"
  paper_id: '614'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1614.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1614.jpg
  title: Generating Highly Relevant Questions
  title_html: Generating Highly Relevant Questions
  url: https://www.aclweb.org/anthology/D19-1614
  year: '2019'
D19-1615:
  abstract: Stories generated with neural language models have shown promise in grammatical
    and stylistic consistency. However, the generated stories are still lacking in
    common sense reasoning, e.g., they often contain sentences deprived of world knowledge.
    We propose a simple multi-task learning scheme to achieve quantitatively better
    common sense reasoning in language models by leveraging auxiliary training signals
    from datasets designed to provide common sense grounding. When combined with our
    two-stage fine-tuning pipeline, our method achieves improved common sense reasoning
    and state-of-the-art perplexity on the WritingPrompts (Fan et al., 2018) story
    generation dataset.
  address: Hong Kong, China
  attachment:
  - filename: D19-1615.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1615.Attachment.pdf
  author:
  - first: Huanru Henry
    full: Huanru Henry Mao
    id: huanru-henry-mao
    last: Mao
  - first: Bodhisattwa Prasad
    full: Bodhisattwa Prasad Majumder
    id: bodhisattwa-prasad-majumder
    last: Majumder
  - first: Julian
    full: Julian McAuley
    id: julian-mcauley
    last: McAuley
  - first: Garrison
    full: Garrison Cottrell
    id: garrison-cottrell
    last: Cottrell
  author_string: Huanru Henry Mao, Bodhisattwa Prasad Majumder, Julian McAuley, Garrison
    Cottrell
  bibkey: mao-etal-2019-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1615
  month: November
  page_first: '5988'
  page_last: '5993'
  pages: "5988\u20135993"
  paper_id: '615'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1615.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1615.jpg
  title: Improving Neural Story Generation by Targeted Common Sense Grounding
  title_html: Improving Neural Story Generation by Targeted Common Sense Grounding
  url: https://www.aclweb.org/anthology/D19-1615
  year: '2019'
D19-1616:
  abstract: "Text summarization is considered as a challenging task in the NLP community.\
    \ The availability of datasets for the task of multilingual text summarization\
    \ is rare, and such datasets are difficult to construct. In this work, we build\
    \ an abstract text summarizer for the German language text using the state-of-the-art\
    \ \u201CTransformer\u201D model. We propose an iterative data augmentation approach\
    \ which uses synthetic data along with the real summarization data for the German\
    \ language. To generate synthetic data, the Common Crawl (German) dataset is exploited,\
    \ which covers different domains. The synthetic data is effective for the low\
    \ resource condition and is particularly helpful for our multilingual scenario\
    \ where availability of summarizing data is still a challenging issue. The data\
    \ are also useful in deep learning scenarios where the neural models require a\
    \ large amount of training data for utilization of its capacity. The obtained\
    \ summarization performance is measured in terms of ROUGE and BLEU score. We achieve\
    \ an absolute improvement of +1.5 and +16.0 in ROUGE1 F1 (R1_F1) on the development\
    \ and test sets, respectively, compared to the system which does not rely on data\
    \ augmentation."
  address: Hong Kong, China
  author:
  - first: Shantipriya
    full: Shantipriya Parida
    id: shantipriya-parida
    last: Parida
  - first: Petr
    full: Petr Motlicek
    id: petr-motlicek
    last: Motlicek
  author_string: Shantipriya Parida, Petr Motlicek
  bibkey: parida-motlicek-2019-abstract
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1616
  month: November
  page_first: '5994'
  page_last: '5998'
  pages: "5994\u20135998"
  paper_id: '616'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1616.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1616.jpg
  title: 'Abstract Text Summarization: A Low Resource Challenge'
  title_html: 'Abstract Text Summarization: A Low Resource Challenge'
  url: https://www.aclweb.org/anthology/D19-1616
  year: '2019'
D19-1617:
  abstract: We present a novel approach for generating poetry automatically for the
    morphologically rich Finnish language by using a genetic algorithm. The approach
    improves the state of the art of the previous Finnish poem generators by introducing
    a higher degree of freedom in terms of structural creativity. Our approach is
    evaluated and described within the paradigm of computational creativity, where
    the fitness functions of the genetic algorithm are assimilated with the notion
    of aesthetics. The output is considered to be a poem 81.5% of the time by human
    evaluators.
  address: Hong Kong, China
  author:
  - first: Mika
    full: "Mika H\xE4m\xE4l\xE4inen"
    id: mika-hamalainen
    last: "H\xE4m\xE4l\xE4inen"
  - first: Khalid
    full: Khalid Alnajjar
    id: khalid-alnajjar
    last: Alnajjar
  author_string: "Mika H\xE4m\xE4l\xE4inen, Khalid Alnajjar"
  bibkey: hamalainen-alnajjar-2019-generating
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1617
  month: November
  page_first: '5999'
  page_last: '6004'
  pages: "5999\u20136004"
  paper_id: '617'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1617.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1617.jpg
  title: Generating Modern Poetry Automatically in Finnish
  title_html: Generating Modern Poetry Automatically in <span class="acl-fixed-case">F</span>innish
  url: https://www.aclweb.org/anthology/D19-1617
  year: '2019'
D19-1618:
  abstract: We propose SUM-QE, a novel Quality Estimation model for summarization
    based on BERT. The model addresses linguistic quality aspects that are only indirectly
    captured by content-based approaches to summary evaluation, without involving
    comparison with human references. SUM-QE achieves very high correlations with
    human ratings, outperforming simpler models addressing these linguistic aspects.
    Predictions of the SUM-QE model can be used for system development, and to inform
    users of the quality of automatically produced summaries and other types of generated
    text.
  address: Hong Kong, China
  attachment:
  - filename: D19-1618.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1618.Attachment.zip
  author:
  - first: Stratos
    full: Stratos Xenouleas
    id: stratos-xenouleas
    last: Xenouleas
  - first: Prodromos
    full: Prodromos Malakasiotis
    id: prodromos-malakasiotis
    last: Malakasiotis
  - first: Marianna
    full: Marianna Apidianaki
    id: marianna-apidianaki
    last: Apidianaki
  - first: Ion
    full: Ion Androutsopoulos
    id: ion-androutsopoulos
    last: Androutsopoulos
  author_string: Stratos Xenouleas, Prodromos Malakasiotis, Marianna Apidianaki, Ion
    Androutsopoulos
  bibkey: xenouleas-etal-2019-sum
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1618
  month: November
  page_first: '6005'
  page_last: '6011'
  pages: "6005\u20136011"
  paper_id: '618'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1618.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1618.jpg
  title: 'SUM-QE: a BERT-based Summary Quality Estimation Model'
  title_html: '<span class="acl-fixed-case">SUM</span>-<span class="acl-fixed-case">QE</span>:
    a <span class="acl-fixed-case">BERT</span>-based Summary Quality Estimation Model'
  url: https://www.aclweb.org/anthology/D19-1618
  year: '2019'
D19-1619:
  abstract: Generating paraphrases from given sentences involves decoding words step
    by step from a large vocabulary. To learn a decoder, supervised learning which
    maximizes the likelihood of tokens always suffers from the exposure bias. Although
    both reinforcement learning (RL) and imitation learning (IL) have been widely
    used to alleviate the bias, the lack of direct comparison leads to only a partial
    image on their benefits. In this work, we present an empirical study on how RL
    and IL can help boost the performance of generating paraphrases, with the pointer-generator
    as a base model. Experiments on the benchmark datasets show that (1) imitation
    learning is constantly better than reinforcement learning; and (2) the pointer-generator
    models with imitation learning outperform the state-of-the-art methods with a
    large margin.
  address: Hong Kong, China
  attachment:
  - filename: D19-1619.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1619.Attachment.zip
  author:
  - first: Wanyu
    full: Wanyu Du
    id: wanyu-du
    last: Du
  - first: Yangfeng
    full: Yangfeng Ji
    id: yangfeng-ji
    last: Ji
  author_string: Wanyu Du, Yangfeng Ji
  bibkey: du-ji-2019-empirical
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1619
  month: November
  page_first: '6012'
  page_last: '6018'
  pages: "6012\u20136018"
  paper_id: '619'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1619.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1619.jpg
  title: An Empirical Comparison on Imitation Learning and Reinforcement Learning
    for Paraphrase Generation
  title_html: An Empirical Comparison on Imitation Learning and Reinforcement Learning
    for Paraphrase Generation
  url: https://www.aclweb.org/anthology/D19-1619
  year: '2019'
D19-1620:
  abstract: "Sentence position is a strong feature for news summarization, since the\
    \ lead often (but not always) summarizes the key points of the article. In this\
    \ paper, we show that recent neural systems excessively exploit this trend, which\
    \ although powerful for many inputs, is also detrimental when summarizing documents\
    \ where important content should be extracted from later parts of the article.\
    \ We propose two techniques to make systems sensitive to the importance of content\
    \ in different parts of the article. The first technique employs \u2018unbiased\u2019\
    \ data; i.e., randomly shuffled sentences of the source document, to pretrain\
    \ the model. The second technique uses an auxiliary ROUGE-based loss that encourages\
    \ the model to distribute importance scores throughout a document by mimicking\
    \ sentence-level ROUGE scores on the training data. We show that these techniques\
    \ significantly improve the performance of a competitive reinforcement learning\
    \ based extractive system, with the auxiliary loss being more powerful than pretraining."
  address: Hong Kong, China
  attachment:
  - filename: D19-1620.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1620.Attachment.zip
  author:
  - first: Matt
    full: Matt Grenander
    id: matt-grenander
    last: Grenander
  - first: Yue
    full: Yue Dong
    id: yue-dong
    last: Dong
  - first: Jackie Chi Kit
    full: Jackie Chi Kit Cheung
    id: jackie-chi-kit-cheung
    last: Cheung
  - first: Annie
    full: Annie Louis
    id: annie-louis
    last: Louis
  author_string: Matt Grenander, Yue Dong, Jackie Chi Kit Cheung, Annie Louis
  bibkey: grenander-etal-2019-countering
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1620
  month: November
  page_first: '6019'
  page_last: '6024'
  pages: "6019\u20136024"
  paper_id: '620'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1620.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1620.jpg
  title: Countering the Effects of Lead Bias in News Summarization via Multi-Stage
    Training and Auxiliary Losses
  title_html: Countering the Effects of Lead Bias in News Summarization via Multi-Stage
    Training and Auxiliary Losses
  url: https://www.aclweb.org/anthology/D19-1620
  year: '2019'
D19-1621:
  abstract: 'Existing recurrent neural language models often fail to capture higher-level
    structure present in text: for example, rhyming patterns present in poetry. Much
    prior work on poetry generation uses manually defined constraints which are satisfied
    during decoding using either specialized decoding procedures or rejection sampling.
    The rhyming constraints themselves are typically not learned by the generator.
    We propose an alternate approach that uses a structured discriminator to learn
    a poetry generator that directly captures rhyming constraints in a generative
    adversarial setup. By causing the discriminator to compare poems based only on
    a learned similarity matrix of pairs of line ending words, the proposed approach
    is able to successfully learn rhyming patterns in two different English poetry
    datasets (Sonnet and Limerick) without explicitly being provided with any phonetic
    information'
  address: Hong Kong, China
  author:
  - first: Harsh
    full: Harsh Jhamtani
    id: harsh-jhamtani
    last: Jhamtani
  - first: Sanket Vaibhav
    full: Sanket Vaibhav Mehta
    id: sanket-vaibhav-mehta
    last: Mehta
  - first: Jaime
    full: Jaime Carbonell
    id: jaime-g-carbonell
    last: Carbonell
  - first: Taylor
    full: Taylor Berg-Kirkpatrick
    id: taylor-berg-kirkpatrick
    last: Berg-Kirkpatrick
  author_string: Harsh Jhamtani, Sanket Vaibhav Mehta, Jaime Carbonell, Taylor Berg-Kirkpatrick
  bibkey: jhamtani-etal-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1621
  month: November
  page_first: '6025'
  page_last: '6031'
  pages: "6025\u20136031"
  paper_id: '621'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1621.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1621.jpg
  title: Learning Rhyming Constraints using Structured Adversaries
  title_html: Learning Rhyming Constraints using Structured Adversaries
  url: https://www.aclweb.org/anthology/D19-1621
  year: '2019'
D19-1622:
  abstract: Question generation is a challenging task which aims to ask a question
    based on an answer and relevant context. The existing works suffer from the mismatching
    between question type and answer, i.e. generating a question with type how while
    the answer is a personal name. We propose to automatically predict the question
    type based on the input answer and context. Then, the question type is fused into
    a seq2seq model to guide the question generation, so as to deal with the mismatching
    problem. We achieve significant improvement on the accuracy of question type prediction
    and finally obtain state-of-the-art results for question generation on both SQuAD
    and MARCO datasets. while the answer is a personal name. We propose to automatically
    predict the question type based on the input answer and context. Then, the question
    type is fused into a seq2seq model to guide the question generation, so as to
    deal with the mismatching problem. We achieve significant improvement on the accuracy
    of question type prediction and finally obtain state-of-the-art results for question
    generation on both SQuAD and MARCO datasets.
  address: Hong Kong, China
  author:
  - first: Wenjie
    full: Wenjie Zhou
    id: wenjie-zhou
    last: Zhou
  - first: Minghua
    full: Minghua Zhang
    id: minghua-zhang
    last: Zhang
  - first: Yunfang
    full: Yunfang Wu
    id: yunfang-wu
    last: Wu
  author_string: Wenjie Zhou, Minghua Zhang, Yunfang Wu
  bibkey: zhou-etal-2019-question
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1622
  month: November
  page_first: '6032'
  page_last: '6037'
  pages: "6032\u20136037"
  paper_id: '622'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1622.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1622.jpg
  title: Question-type Driven Question Generation
  title_html: Question-type Driven Question Generation
  url: https://www.aclweb.org/anthology/D19-1622
  year: '2019'
D19-1623:
  abstract: Deep reinforcement learning (RL) has been a commonly-used strategy for
    the abstractive summarization task to address both the exposure bias and non-differentiable
    task issues. However, the conventional reward Rouge-L simply looks for exact n-grams
    matches between candidates and annotated references, which inevitably makes the
    generated sentences repetitive and incoherent. In this paper, instead of Rouge-L,
    we explore the practicability of utilizing the distributional semantics to measure
    the matching degrees. With distributional semantics, sentence-level evaluation
    can be obtained, and semantically-correct phrases can also be generated without
    being limited to the surface form of the reference sentences. Human judgments
    on Gigaword and CNN/Daily Mail datasets show that our proposed distributional
    semantics reward (DSR) has distinct superiority in capturing the lexical and compositional
    diversity of natural language.
  address: Hong Kong, China
  author:
  - first: Siyao
    full: Siyao Li
    id: siyao-li
    last: Li
  - first: Deren
    full: Deren Lei
    id: deren-lei
    last: Lei
  - first: Pengda
    full: Pengda Qin
    id: pengda-qin
    last: Qin
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  author_string: Siyao Li, Deren Lei, Pengda Qin, William Yang Wang
  bibkey: li-etal-2019-deep
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1623
  month: November
  page_first: '6038'
  page_last: '6044'
  pages: "6038\u20136044"
  paper_id: '623'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1623.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1623.jpg
  title: Deep Reinforcement Learning with Distributional Semantic Rewards for Abstractive
    Summarization
  title_html: Deep Reinforcement Learning with Distributional Semantic Rewards for
    Abstractive Summarization
  url: https://www.aclweb.org/anthology/D19-1623
  year: '2019'
D19-1624:
  abstract: Most deep learning approaches for text-to-SQL generation are limited to
    the WikiSQL dataset, which only supports very simple queries over a single table.
    We focus on the Spider dataset, a complex and cross-domain text-to-SQL task, which
    includes complex queries over multiple tables. In this paper, we propose a SQL
    clause-wise decoding neural architecture with a self-attention based database
    schema encoder to address the Spider task. Each of the clause-specific decoders
    consists of a set of sub-modules, which is defined by the syntax of each clause.
    Additionally, our model works recursively to support nested queries. When evaluated
    on the Spider dataset, our approach achieves 4.6% and 9.8% accuracy gain in the
    test and dev sets, respectively. In addition, we show that our model is significantly
    more effective at predicting complex and nested queries than previous work.
  address: Hong Kong, China
  author:
  - first: Dongjun
    full: Dongjun Lee
    id: dongjun-lee
    last: Lee
  author_string: Dongjun Lee
  bibkey: lee-2019-clause
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1624
  month: November
  page_first: '6045'
  page_last: '6051'
  pages: "6045\u20136051"
  paper_id: '624'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1624.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1624.jpg
  title: Clause-Wise and Recursive Decoding for Complex and Cross-Domain Text-to-SQL
    Generation
  title_html: Clause-Wise and Recursive Decoding for Complex and Cross-Domain Text-to-<span
    class="acl-fixed-case">SQL</span> Generation
  url: https://www.aclweb.org/anthology/D19-1624
  year: '2019'
D19-1625:
  abstract: "How do adjectives project from a noun to its parts? If a motorcycle is\
    \ red, are its wheels red? Is a nuclear submarine\u2019s captain nuclear? These\
    \ questions are easy for humans to judge using our commonsense understanding of\
    \ the world, but are difficult for computers. To attack this challenge, we crowdsource\
    \ a set of human judgments that answer the English-language question \u201CGiven\
    \ a whole described by an adjective, does the adjective also describe a given\
    \ part?\u201D We build strong baselines for this task with a classification approach.\
    \ Our findings indicate that, despite the recent successes of large language models\
    \ on tasks aimed to assess commonsense knowledge, these models do not greatly\
    \ outperform simple word-level models based on pre-trained word embeddings. This\
    \ provides evidence that the amount of commonsense knowledge encoded in these\
    \ language models does not extend far beyond that already baked into the word\
    \ embeddings. Our dataset will serve as a useful testbed for future research in\
    \ commonsense reasoning, especially as it relates to adjectives and objects"
  address: Hong Kong, China
  attachment:
  - filename: D19-1625.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1625.Attachment.pdf
  author:
  - first: James
    full: James Mullenbach
    id: james-mullenbach
    last: Mullenbach
  - first: Jonathan
    full: Jonathan Gordon
    id: jonathan-gordon
    last: Gordon
  - first: Nanyun
    full: Nanyun Peng
    id: nanyun-peng
    last: Peng
  - first: Jonathan
    full: Jonathan May
    id: jonathan-may
    last: May
  author_string: James Mullenbach, Jonathan Gordon, Nanyun Peng, Jonathan May
  bibkey: mullenbach-etal-2019-nuclear
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1625
  month: November
  page_first: '6052'
  page_last: '6058'
  pages: "6052\u20136058"
  paper_id: '625'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1625.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1625.jpg
  title: Do Nuclear Submarines Have Nuclear Captains? A Challenge Dataset for Commonsense
    Reasoning over Adjectives and Objects
  title_html: Do Nuclear Submarines Have Nuclear Captains? A Challenge Dataset for
    Commonsense Reasoning over Adjectives and Objects
  url: https://www.aclweb.org/anthology/D19-1625
  year: '2019'
D19-1626:
  abstract: In this work, we propose an aggregation method to combine the Bidirectional
    Encoder Representations from Transformer (BERT) with a MatchLSTM layer for Sequence
    Matching. Given a sentence pair, we extract the output representations of it from
    BERT. Then we extend BERT with a MatchLSTM layer to get further interaction of
    the sentence pair for sequence matching tasks. Taking natural language inference
    as an example, we split BERT output into two parts, which is from premise sentence
    and hypothesis sentence. At each position of the hypothesis sentence, both the
    weighted representation of the premise sentence and the representation of the
    current token are fed into LSTM. We jointly train the aggregation layer and pre-trained
    layer for sequence matching. We conduct an experiment on two publicly available
    datasets, WikiQA and SNLI. Experiments show that our model achieves significantly
    improvement compared with state-of-the-art methods on both datasets.
  address: Hong Kong, China
  author:
  - first: Bo
    full: Bo Shao
    id: bo-shao
    last: Shao
  - first: Yeyun
    full: Yeyun Gong
    id: yeyun-gong
    last: Gong
  - first: Weizhen
    full: Weizhen Qi
    id: weizhen-qi
    last: Qi
  - first: Nan
    full: Nan Duan
    id: nan-duan
    last: Duan
  - first: Xiaola
    full: Xiaola Lin
    id: xiaola-lin
    last: Lin
  author_string: Bo Shao, Yeyun Gong, Weizhen Qi, Nan Duan, Xiaola Lin
  bibkey: shao-etal-2019-aggregating
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1626
  month: November
  page_first: '6059'
  page_last: '6063'
  pages: "6059\u20136063"
  paper_id: '626'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1626.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1626.jpg
  title: Aggregating Bidirectional Encoder Representations Using MatchLSTM for Sequence
    Matching
  title_html: Aggregating Bidirectional Encoder Representations Using <span class="acl-fixed-case">M</span>atch<span
    class="acl-fixed-case">LSTM</span> for Sequence Matching
  url: https://www.aclweb.org/anthology/D19-1626
  year: '2019'
D19-1627:
  abstract: Contextualized word embeddings have boosted many NLP tasks compared with
    traditional static word embeddings. However, the word with a specific sense may
    have different contextualized embeddings due to its various contexts. To further
    investigate what contextualized word embeddings capture, this paper analyzes whether
    they can indicate the corresponding sense definitions and proposes a general framework
    that is capable of explaining word meanings given contextualized word embeddings
    for better interpretation. The experiments show that both ELMo and BERT embeddings
    can be well interpreted via a readable textual form, and the findings may benefit
    the research community for a better understanding of what the embeddings capture.
  address: Hong Kong, China
  attachment:
  - filename: D19-1627.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1627.Attachment.zip
  author:
  - first: Ting-Yun
    full: Ting-Yun Chang
    id: ting-yun-chang
    last: Chang
  - first: Yun-Nung
    full: Yun-Nung Chen
    id: yun-nung-chen
    last: Chen
  author_string: Ting-Yun Chang, Yun-Nung Chen
  bibkey: chang-chen-2019-word
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1627
  month: November
  page_first: '6064'
  page_last: '6070'
  pages: "6064\u20136070"
  paper_id: '627'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1627.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1627.jpg
  title: What Does This Word Mean? Explaining Contextualized Embeddings with Natural
    Language Definition
  title_html: What Does This Word Mean? Explaining Contextualized Embeddings with
    Natural Language Definition
  url: https://www.aclweb.org/anthology/D19-1627
  year: '2019'
D19-1628:
  abstract: Pre-trained BERT contextualized representations have achieved state-of-the-art
    results on multiple downstream NLP tasks by fine-tuning with task-specific data.
    While there has been a lot of focus on task-specific fine-tuning, there has been
    limited work on improving the pre-trained representations. In this paper, we explore
    ways of improving the pre-trained contextual representations for the task of automatic
    short answer grading, a critical component of intelligent tutoring systems. We
    show that the pre-trained BERT model can be improved by augmenting data from the
    domain-specific resources like textbooks. We also present a new approach to use
    labeled short answering grading data for further enhancement of the language model.
    Empirical evaluation on multi-domain datasets shows that task-specific fine-tuning
    on the enhanced pre-trained language model achieves superior performance for short
    answer grading.
  address: Hong Kong, China
  author:
  - first: Chul
    full: Chul Sung
    id: chul-sung
    last: Sung
  - first: Tejas
    full: Tejas Dhamecha
    id: tejas-dhamecha
    last: Dhamecha
  - first: Swarnadeep
    full: Swarnadeep Saha
    id: swarnadeep-saha
    last: Saha
  - first: Tengfei
    full: Tengfei Ma
    id: tengfei-ma
    last: Ma
  - first: Vinay
    full: Vinay Reddy
    id: vinay-reddy
    last: Reddy
  - first: Rishi
    full: Rishi Arora
    id: rishi-arora
    last: Arora
  author_string: Chul Sung, Tejas Dhamecha, Swarnadeep Saha, Tengfei Ma, Vinay Reddy,
    Rishi Arora
  bibkey: sung-etal-2019-pre
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1628
  month: November
  page_first: '6071'
  page_last: '6075'
  pages: "6071\u20136075"
  paper_id: '628'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1628.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1628.jpg
  title: Pre-Training BERT on Domain Resources for Short Answer Grading
  title_html: Pre-Training <span class="acl-fixed-case">BERT</span> on Domain Resources
    for Short Answer Grading
  url: https://www.aclweb.org/anthology/D19-1628
  year: '2019'
D19-1629:
  abstract: "We introduce WIQA, the first large-scale dataset of \u201CWhat if...\u201D\
    \ questions over procedural text. WIQA contains a collection of paragraphs, each\
    \ annotated with multiple influence graphs describing how one change affects another,\
    \ and a large (40k) collection of \u201CWhat if...?\u201D multiple-choice questions\
    \ derived from these. For example, given a paragraph about beach erosion, would\
    \ stormy weather hasten or decelerate erosion? WIQA contains three kinds of questions:\
    \ perturbations to steps mentioned in the paragraph; external (out-of-paragraph)\
    \ perturbations requiring commonsense knowledge; and irrelevant (no effect) perturbations.\
    \ We find that state-of-the-art models achieve 73.8% accuracy, well below the\
    \ human performance of 96.3%. We analyze the challenges, in particular tracking\
    \ chains of influences, and present the dataset as an open challenge to the community."
  address: Hong Kong, China
  attachment:
  - filename: D19-1629.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1629.Attachment.zip
  author:
  - first: Niket
    full: Niket Tandon
    id: niket-tandon
    last: Tandon
  - first: Bhavana
    full: Bhavana Dalvi
    id: bhavana-dalvi
    last: Dalvi
  - first: Keisuke
    full: Keisuke Sakaguchi
    id: keisuke-sakaguchi
    last: Sakaguchi
  - first: Peter
    full: Peter Clark
    id: peter-clark
    last: Clark
  - first: Antoine
    full: Antoine Bosselut
    id: antoine-bosselut
    last: Bosselut
  author_string: Niket Tandon, Bhavana Dalvi, Keisuke Sakaguchi, Peter Clark, Antoine
    Bosselut
  bibkey: tandon-etal-2019-wiqa
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1629
  month: November
  page_first: '6076'
  page_last: '6085'
  pages: "6076\u20136085"
  paper_id: '629'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1629.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1629.jpg
  title: "WIQA: A dataset for \u201CWhat if...\u201D reasoning over procedural text"
  title_html: "<span class=\"acl-fixed-case\">WIQA</span>: A dataset for \u201CWhat\
    \ if...\u201D reasoning over procedural text"
  url: https://www.aclweb.org/anthology/D19-1629
  year: '2019'
D19-1630:
  abstract: 'Natural language inference (NLI) datasets (e.g., MultiNLI) were collected
    by soliciting hypotheses for a given premise from annotators. Such data collection
    led to annotation artifacts: systems can identify the premise-hypothesis relationship
    without observing the premise (e.g., negation in hypothesis being indicative of
    contradiction). We address this problem by recasting the CommitmentBank for NLI,
    which contains items involving reasoning over the extent to which a speaker is
    committed to complements of clause-embedding verbs under entailment-canceling
    environments (conditional, negation, modal and question). Instead of being constructed
    to stand in certain relationships with the premise, hypotheses in the recast CommitmentBank
    are the complements of the clause-embedding verb in each premise, leading to no
    annotation artifacts in the hypothesis. A state-of-the-art BERT-based model performs
    well on the CommitmentBank with 85% F1. However analysis of model behavior shows
    that the BERT models still do not capture the full complexity of pragmatic reasoning,
    nor encode some of the linguistic generalizations, highlighting room for improvement.'
  address: Hong Kong, China
  author:
  - first: Nanjiang
    full: Nanjiang Jiang
    id: nanjiang-jiang
    last: Jiang
  - first: Marie-Catherine
    full: Marie-Catherine de Marneffe
    id: marie-catherine-de-marneffe
    last: de Marneffe
  author_string: Nanjiang Jiang, Marie-Catherine de Marneffe
  bibkey: jiang-de-marneffe-2019-evaluating
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1630
  month: November
  page_first: '6086'
  page_last: '6091'
  pages: "6086\u20136091"
  paper_id: '630'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1630.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1630.jpg
  title: 'Evaluating BERT for natural language inference: A case study on the CommitmentBank'
  title_html: 'Evaluating <span class="acl-fixed-case">BERT</span> for natural language
    inference: A case study on the <span class="acl-fixed-case">C</span>ommitment<span
    class="acl-fixed-case">B</span>ank'
  url: https://www.aclweb.org/anthology/D19-1630
  year: '2019'
D19-1631:
  abstract: Recently, biomedical version of embeddings obtained from language models
    such as BioELMo have shown state-of-the-art results for the textual inference
    task in the medical domain. In this paper, we explore how to incorporate structured
    domain knowledge, available in the form of a knowledge graph (UMLS), for the Medical
    NLI task. Specifically, we experiment with fusing embeddings obtained from knowledge
    graph with the state-of-the-art approaches for NLI task (ESIM model). We also
    experiment with fusing the domain-specific sentiment information for the task.
    Experiments conducted on MedNLI dataset clearly show that this strategy improves
    the baseline BioELMo architecture for the Medical NLI task.
  address: Hong Kong, China
  attachment:
  - filename: D19-1631.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1631.Attachment.pdf
  author:
  - first: Soumya
    full: Soumya Sharma
    id: soumya-sharma
    last: Sharma
  - first: Bishal
    full: Bishal Santra
    id: bishal-santra
    last: Santra
  - first: Abhik
    full: Abhik Jana
    id: abhik-jana
    last: Jana
  - first: Santosh
    full: Santosh Tokala
    id: santosh-tokala
    last: Tokala
  - first: Niloy
    full: Niloy Ganguly
    id: niloy-ganguly
    last: Ganguly
  - first: Pawan
    full: Pawan Goyal
    id: pawan-goyal
    last: Goyal
  author_string: Soumya Sharma, Bishal Santra, Abhik Jana, Santosh Tokala, Niloy Ganguly,
    Pawan Goyal
  bibkey: sharma-etal-2019-incorporating
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1631
  month: November
  page_first: '6092'
  page_last: '6097'
  pages: "6092\u20136097"
  paper_id: '631'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1631.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1631.jpg
  title: Incorporating Domain Knowledge into Medical NLI using Knowledge Graphs
  title_html: Incorporating Domain Knowledge into Medical <span class="acl-fixed-case">NLI</span>
    using Knowledge Graphs
  url: https://www.aclweb.org/anthology/D19-1631
  year: '2019'
D19-1632:
  abstract: "For machine translation, a vast majority of language pairs in the world\
    \ are considered low-resource because they have little parallel data available.\
    \ Besides the technical challenges of learning with limited supervision, it is\
    \ difficult to evaluate methods trained on low-resource language pairs because\
    \ of the lack of freely and publicly available benchmarks. In this work, we introduce\
    \ the FLORES evaluation datasets for Nepali\u2013English and Sinhala\u2013 English,\
    \ based on sentences translated from Wikipedia. Compared to English, these are\
    \ languages with very different morphology and syntax, for which little out-of-domain\
    \ parallel data is available and for which relatively large amounts of monolingual\
    \ data are freely available. We describe our process to collect and cross-check\
    \ the quality of translations, and we report baseline performance using several\
    \ learning settings: fully supervised, weakly supervised, semi-supervised, and\
    \ fully unsupervised. Our experiments demonstrate that current state-of-the-art\
    \ methods perform rather poorly on this benchmark, posing a challenge to the research\
    \ community working on low-resource MT. Data and code to reproduce our experiments\
    \ are available at https://github.com/facebookresearch/flores."
  address: Hong Kong, China
  attachment:
  - filename: D19-1632.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1632.Attachment.zip
  author:
  - first: Francisco
    full: "Francisco Guzm\xE1n"
    id: francisco-guzman
    last: "Guzm\xE1n"
  - first: Peng-Jen
    full: Peng-Jen Chen
    id: peng-jen-chen
    last: Chen
  - first: Myle
    full: Myle Ott
    id: myle-ott
    last: Ott
  - first: Juan
    full: Juan Pino
    id: juan-pino
    last: Pino
  - first: Guillaume
    full: Guillaume Lample
    id: guillaume-lample
    last: Lample
  - first: Philipp
    full: Philipp Koehn
    id: philipp-koehn
    last: Koehn
  - first: Vishrav
    full: Vishrav Chaudhary
    id: vishrav-chaudhary
    last: Chaudhary
  - first: "Marc\u2019Aurelio"
    full: "Marc\u2019Aurelio Ranzato"
    id: marcaurelio-ranzato
    last: Ranzato
  author_string: "Francisco Guzm\xE1n, Peng-Jen Chen, Myle Ott, Juan Pino, Guillaume\
    \ Lample, Philipp Koehn, Vishrav Chaudhary, Marc\u2019Aurelio Ranzato"
  bibkey: guzman-etal-2019-flores
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1632
  month: November
  page_first: '6098'
  page_last: '6111'
  pages: "6098\u20136111"
  paper_id: '632'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1632.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1632.jpg
  title: "The FLORES Evaluation Datasets for Low-Resource Machine Translation: Nepali\u2013\
    English and Sinhala\u2013English"
  title_html: "The <span class=\"acl-fixed-case\">FLORES</span> Evaluation Datasets\
    \ for Low-Resource Machine Translation: <span class=\"acl-fixed-case\">N</span>epali\u2013\
    <span class=\"acl-fixed-case\">E</span>nglish and <span class=\"acl-fixed-case\"\
    >S</span>inhala\u2013<span class=\"acl-fixed-case\">E</span>nglish"
  url: https://www.aclweb.org/anthology/D19-1632
  year: '2019'
D19-1633:
  abstract: Most machine translation systems generate text autoregressively from left
    to right. We, instead, use a masked language modeling objective to train a model
    to predict any subset of the target words, conditioned on both the input text
    and a partially masked target translation. This approach allows for efficient
    iterative decoding, where we first predict all of the target words non-autoregressively,
    and then repeatedly mask out and regenerate the subset of words that the model
    is least confident about. By applying this strategy for a constant number of iterations,
    our model improves state-of-the-art performance levels for non-autoregressive
    and parallel decoding translation models by over 4 BLEU on average. It is also
    able to reach within about 1 BLEU point of a typical left-to-right transformer
    model, while decoding significantly faster.
  address: Hong Kong, China
  author:
  - first: Marjan
    full: Marjan Ghazvininejad
    id: marjan-ghazvininejad
    last: Ghazvininejad
  - first: Omer
    full: Omer Levy
    id: omer-levy
    last: Levy
  - first: Yinhan
    full: Yinhan Liu
    id: yinhan-liu
    last: Liu
  - first: Luke
    full: Luke Zettlemoyer
    id: luke-zettlemoyer
    last: Zettlemoyer
  author_string: Marjan Ghazvininejad, Omer Levy, Yinhan Liu, Luke Zettlemoyer
  bibkey: ghazvininejad-etal-2019-mask
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1633
  month: November
  page_first: '6112'
  page_last: '6121'
  pages: "6112\u20136121"
  paper_id: '633'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1633.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1633.jpg
  title: 'Mask-Predict: Parallel Decoding of Conditional Masked Language Models'
  title_html: 'Mask-Predict: Parallel Decoding of Conditional Masked Language Models'
  url: https://www.aclweb.org/anthology/D19-1633
  year: '2019'
D19-1634:
  abstract: Automatic post-editing (APE), which aims to correct errors in the output
    of machine translation systems in a post-processing step, is an important task
    in natural language processing. While recent work has achieved considerable performance
    gains by using neural networks, how to model the copying mechanism for APE remains
    a challenge. In this work, we propose a new method for modeling copying for APE.
    To better identify translation errors, our method learns the representations of
    source sentences and system outputs in an interactive way. These representations
    are used to explicitly indicate which words in the system outputs should be copied.
    Finally, CopyNet (Gu et.al., 2016) can be combined with our method to place the
    copied words in correct positions in post-edited translations. Experiments on
    the datasets of the WMT 2016-2017 APE shared tasks show that our approach outperforms
    all best published results.
  address: Hong Kong, China
  author:
  - first: Xuancheng
    full: Xuancheng Huang
    id: xuancheng-huang
    last: Huang
  - first: Yang
    full: Yang Liu
    id: yang-liu
    last: Liu
  - first: Huanbo
    full: Huanbo Luan
    id: huanbo-luan
    last: Luan
  - first: Jingfang
    full: Jingfang Xu
    id: jingfang-xu
    last: Xu
  - first: Maosong
    full: Maosong Sun
    id: maosong-sun
    last: Sun
  author_string: Xuancheng Huang, Yang Liu, Huanbo Luan, Jingfang Xu, Maosong Sun
  bibkey: huang-etal-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1634
  month: November
  page_first: '6122'
  page_last: '6132'
  pages: "6122\u20136132"
  paper_id: '634'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1634.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1634.jpg
  title: Learning to Copy for Automatic Post-Editing
  title_html: Learning to Copy for Automatic Post-Editing
  url: https://www.aclweb.org/anthology/D19-1634
  year: '2019'
D19-1635:
  abstract: 'Word embeddings have been widely used to study gender stereotypes in
    texts. One key problem regarding existing bias scores is to evaluate their validities:
    do they really reflect true bias levels? For a small set of words (e.g. occupations),
    we can rely on human annotations or external data. However, for most words, evaluating
    the correctness of them is still an open problem. In this work, we utilize word
    association test, which contains rich types of word connections annotated by human
    participants, to explore how gender stereotypes spread within our minds. Specifically,
    we use random walk on word association graph to derive bias scores for a large
    amount of words. Experiments show that these bias scores correlate well with bias
    in the real world. More importantly, comparing with word-embedding-based bias
    scores, it provides a different perspective on gender stereotypes in words.'
  address: Hong Kong, China
  author:
  - first: Yupei
    full: Yupei Du
    id: yupei-du
    last: Du
  - first: Yuanbin
    full: Yuanbin Wu
    id: yuanbin-wu
    last: Wu
  - first: Man
    full: Man Lan
    id: man-lan
    last: Lan
  author_string: Yupei Du, Yuanbin Wu, Man Lan
  bibkey: du-etal-2019-exploring
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1635
  month: November
  page_first: '6133'
  page_last: '6143'
  pages: "6133\u20136143"
  paper_id: '635'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1635.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1635.jpg
  title: Exploring Human Gender Stereotypes with Word Association Test
  title_html: Exploring Human Gender Stereotypes with Word Association Test
  url: https://www.aclweb.org/anthology/D19-1635
  year: '2019'
D19-1636:
  abstract: "In this paper, we propose a novel framework for sarcasm generation; the\
    \ system takes a literal negative opinion as input and translates it into a sarcastic\
    \ version. Our framework does not require any paired data for training. Sarcasm\
    \ emanates from context-incongruity which becomes apparent as the sentence unfolds.\
    \ Our framework introduces incongruity into the literal input version through\
    \ modules that: (a) filter factual content from the input opinion, (b) retrieve\
    \ incongruous phrases related to the filtered facts and (c) synthesize sarcastic\
    \ text from the incongruous filtered and incongruous phrases. The framework employs\
    \ reinforced neural sequence to sequence learning and information retrieval and\
    \ is trained only using unlabeled non-sarcastic and sarcastic opinions. Since\
    \ no labeled dataset exists for such a task, for evaluation, we manually prepare\
    \ a benchmark dataset containing literal opinions and their sarcastic paraphrases.\
    \ Qualitative and quantitative performance analyses on the data reveal our system\u2019\
    s superiority over baselines built using known unsupervised statistical and neural\
    \ machine translation and style transfer techniques."
  address: Hong Kong, China
  author:
  - first: Abhijit
    full: Abhijit Mishra
    id: abhijit-mishra
    last: Mishra
  - first: Tarun
    full: Tarun Tater
    id: tarun-tater
    last: Tater
  - first: Karthik
    full: Karthik Sankaranarayanan
    id: karthik-sankaranarayanan
    last: Sankaranarayanan
  author_string: Abhijit Mishra, Tarun Tater, Karthik Sankaranarayanan
  bibkey: mishra-etal-2019-modular
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1636
  month: November
  page_first: '6144'
  page_last: '6154'
  pages: "6144\u20136154"
  paper_id: '636'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1636.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1636.jpg
  title: A Modular Architecture for Unsupervised Sarcasm Generation
  title_html: A Modular Architecture for Unsupervised Sarcasm Generation
  url: https://www.aclweb.org/anthology/D19-1636
  year: '2019'
D19-1637:
  abstract: Classical Chinese poetry is a jewel in the treasure house of Chinese culture.
    Previous poem generation models only allow users to employ keywords to interfere
    the meaning of generated poems, leaving the dominion of generation to the model.
    In this paper, we propose a novel task of generating classical Chinese poems from
    vernacular, which allows users to have more control over the semantic of generated
    poems. We adapt the approach of unsupervised machine translation (UMT) to our
    task. We use segmentation-based padding and reinforcement learning to address
    under-translation and over-translation respectively. According to experiments,
    our approach significantly improve the perplexity and BLEU compared with typical
    UMT models. Furthermore, we explored guidelines on how to write the input vernacular
    to generate better poems. Human evaluation showed our approach can generate high-quality
    poems which are comparable to amateur poems.
  address: Hong Kong, China
  attachment:
  - filename: D19-1637.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1637.Attachment.pdf
  author:
  - first: Zhichao
    full: Zhichao Yang
    id: zhichao-yang
    last: Yang
  - first: Pengshan
    full: Pengshan Cai
    id: pengshan-cai
    last: Cai
  - first: Yansong
    full: Yansong Feng
    id: yansong-feng
    last: Feng
  - first: Fei
    full: Fei Li
    id: fei-li
    last: Li
  - first: Weijiang
    full: Weijiang Feng
    id: weijiang-feng
    last: Feng
  - first: Elena Suet-Ying
    full: Elena Suet-Ying Chiu
    id: elena-suet-ying-chiu
    last: Chiu
  - first: Hong
    full: Hong Yu
    id: hong-yu
    last: Yu
  author_string: Zhichao Yang, Pengshan Cai, Yansong Feng, Fei Li, Weijiang Feng,
    Elena Suet-Ying Chiu, Hong Yu
  bibkey: yang-etal-2019-generating
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1637
  month: November
  page_first: '6155'
  page_last: '6164'
  pages: "6155\u20136164"
  paper_id: '637'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1637.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1637.jpg
  title: Generating Classical Chinese Poems from Vernacular Chinese
  title_html: Generating Classical <span class="acl-fixed-case">C</span>hinese Poems
    from Vernacular <span class="acl-fixed-case">C</span>hinese
  url: https://www.aclweb.org/anthology/D19-1637
  year: '2019'
D19-1638:
  abstract: 'We present set to ordered text, a natural language generation task applied
    to automatically generating discharge instructions from admission ICD (International
    Classification of Diseases) codes. This task differs from other natural language
    generation tasks in the following ways: (1) The input is a set of identifiable
    entities (ICD codes) where the relations between individual entity are not explicitly
    specified. (2) The output text is not a narrative description (e.g. news articles)
    composed from the input. Rather, inferences are made from the input (symptoms
    specified in ICD codes) to generate the output (instructions). (3) There is an
    optimal order in which each sentence (instruction) should appear in the output.
    Unlike most other tasks, neither the input (ICD codes) nor their corresponding
    symptoms appear in the output, so the ordering of the output instructions needs
    to be learned in an unsupervised fashion. Based on clinical intuition, we hypothesize
    that each instruction in the output is mapped to a subset of ICD codes specified
    in the input. We propose a neural architecture that jointly models (a) subset
    selection: choosing relevant subsets from a set of input entities; (b) content
    ordering: learning the order of instructions; and (c) text generation: representing
    the instructions corresponding to the selected subsets in natural language. In
    addition, we penalize redundancy during beam search to improve tractability for
    long text generation. Our model outperforms baseline models in BLEU scores and
    human evaluation. We plan to extend this work to other tasks such as recipe generation
    from ingredients.'
  address: Hong Kong, China
  author:
  - first: Litton
    full: Litton J Kurisinkel
    id: litton-j-kurisinkel
    last: J Kurisinkel
  - first: Nancy
    full: Nancy Chen
    id: nancy-chen
    last: Chen
  author_string: Litton J Kurisinkel, Nancy Chen
  bibkey: j-kurisinkel-chen-2019-set
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1638
  month: November
  page_first: '6165'
  page_last: '6175'
  pages: "6165\u20136175"
  paper_id: '638'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1638.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1638.jpg
  title: 'Set to Ordered Text: Generating Discharge Instructions from Medical Billing
    Codes'
  title_html: 'Set to Ordered Text: Generating Discharge Instructions from Medical
    Billing Codes'
  url: https://www.aclweb.org/anthology/D19-1638
  year: '2019'
D19-1639:
  abstract: Phonological processes are context-dependent sound changes in natural
    languages. We present an unsupervised approach to learning human-readable descriptions
    of phonological processes from collections of related utterances. Our approach
    builds upon a technique from the programming languages community called *constraint-based
    program synthesis*. We contribute a novel encoding of the learning problem into
    Boolean Satisfiability constraints, which enables both data efficiency and fast
    inference. We evaluate our system on textbook phonology problems and datasets
    from the literature, and show that it achieves high accuracy at interactive speeds.
  address: Hong Kong, China
  author:
  - first: Shraddha
    full: Shraddha Barke
    id: shraddha-barke
    last: Barke
  - first: Rose
    full: Rose Kunkel
    id: rose-kunkel
    last: Kunkel
  - first: Nadia
    full: Nadia Polikarpova
    id: nadia-polikarpova
    last: Polikarpova
  - first: Eric
    full: Eric Meinhardt
    id: eric-meinhardt
    last: Meinhardt
  - first: Eric
    full: Eric Bakovic
    id: eric-bakovic
    last: Bakovic
  - first: Leon
    full: Leon Bergen
    id: leon-bergen
    last: Bergen
  author_string: Shraddha Barke, Rose Kunkel, Nadia Polikarpova, Eric Meinhardt, Eric
    Bakovic, Leon Bergen
  bibkey: barke-etal-2019-constraint
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1639
  month: November
  page_first: '6176'
  page_last: '6186'
  pages: "6176\u20136186"
  paper_id: '639'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1639.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1639.jpg
  title: Constraint-based Learning of Phonological Processes
  title_html: Constraint-based Learning of Phonological Processes
  url: https://www.aclweb.org/anthology/D19-1639
  year: '2019'
D19-1640:
  abstract: The task of Chinese text spam detection is very challenging due to both
    glyph and phonetic variations of Chinese characters. This paper proposes a novel
    framework to jointly model Chinese variational, semantic, and contextualized representations
    for Chinese text spam detection task. In particular, a Variation Family-enhanced
    Graph Embedding (VFGE) algorithm is designed based on a Chinese character variation
    graph. The VFGE can learn both the graph embeddings of the Chinese characters
    (local) and the latent variation families (global). Furthermore, an enhanced bidirectional
    language model, with a combination gate function and an aggregation learning function,
    is proposed to integrate the graph and text information while capturing the sequential
    information. Extensive experiments have been conducted on both SMS and review
    datasets, to show the proposed method outperforms a series of state-of-the-art
    models for Chinese spam detection.
  address: Hong Kong, China
  author:
  - first: Zhuoren
    full: Zhuoren Jiang
    id: zhuoren-jiang
    last: Jiang
  - first: Zhe
    full: Zhe Gao
    id: zhe-gao
    last: Gao
  - first: Guoxiu
    full: Guoxiu He
    id: guoxiu-he
    last: He
  - first: Yangyang
    full: Yangyang Kang
    id: yangyang-kang
    last: Kang
  - first: Changlong
    full: Changlong Sun
    id: changlong-sun
    last: Sun
  - first: Qiong
    full: Qiong Zhang
    id: qiong-zhang
    last: Zhang
  - first: Luo
    full: Luo Si
    id: luo-si
    last: Si
  - first: Xiaozhong
    full: Xiaozhong Liu
    id: xiaozhong-liu
    last: Liu
  author_string: Zhuoren Jiang, Zhe Gao, Guoxiu He, Yangyang Kang, Changlong Sun,
    Qiong Zhang, Luo Si, Xiaozhong Liu
  bibkey: jiang-etal-2019-detect
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1640
  month: November
  page_first: '6187'
  page_last: '6196'
  pages: "6187\u20136196"
  paper_id: '640'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1640.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1640.jpg
  title: 'Detect Camouflaged Spam Content via StoneSkipping: Graph and Text Joint
    Embedding for Chinese Character Variation Representation'
  title_html: 'Detect Camouflaged Spam Content via <span class="acl-fixed-case">S</span>tone<span
    class="acl-fixed-case">S</span>kipping: Graph and Text Joint Embedding for <span
    class="acl-fixed-case">C</span>hinese Character Variation Representation'
  url: https://www.aclweb.org/anthology/D19-1640
  year: '2019'
D19-1641:
  abstract: 'We propose a fine-grained entity typing model with a novel attention
    mechanism and a hybrid type classifier. We advance existing methods in two aspects:
    feature extraction and type prediction. To capture richer contextual information,
    we adopt contextualized word representations instead of fixed word embeddings
    used in previous work. In addition, we propose a two-step mention-aware attention
    mechanism to enable the model to focus on important words in mentions and contexts.
    We also present a hybrid classification method beyond binary relevance to exploit
    type inter-dependency with latent type representation. Instead of independently
    predicting each type, we predict a low-dimensional vector that encodes latent
    type features and reconstruct the type vector from this latent representation.
    Experiment results on multiple data sets show that our model significantly advances
    the state-of-the-art on fine-grained entity typing, obtaining up to 6.1% and 5.5%
    absolute gains in macro averaged F-score and micro averaged F-score respectively.'
  address: Hong Kong, China
  author:
  - first: Ying
    full: Ying Lin
    id: ying-lin
    last: Lin
  - first: Heng
    full: Heng Ji
    id: heng-ji
    last: Ji
  author_string: Ying Lin, Heng Ji
  bibkey: lin-ji-2019-attentive
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1641
  month: November
  page_first: '6197'
  page_last: '6202'
  pages: "6197\u20136202"
  paper_id: '641'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1641.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1641.jpg
  title: An Attentive Fine-Grained Entity Typing Model with Latent Type Representation
  title_html: An Attentive Fine-Grained Entity Typing Model with Latent Type Representation
  url: https://www.aclweb.org/anthology/D19-1641
  year: '2019'
D19-1642:
  abstract: Determining temporal relations (e.g., before or after) between events
    has been a challenging natural language understanding task, partly due to the
    difficulty to generate large amounts of high-quality training data. Consequently,
    neural approaches have not been widely used on it, or showed only moderate improvements.
    This paper proposes a new neural system that achieves about 10% absolute improvement
    in accuracy over the previous best system (25% error reduction) on two benchmark
    datasets. The proposed system is trained on the state-of-the-art MATRES dataset
    and applies contextualized word embeddings, a Siamese encoder of a temporal common
    sense knowledge base, and global inference via integer linear programming (ILP).
    We suggest that the new approach could serve as a strong baseline for future research
    in this area.
  address: Hong Kong, China
  attachment:
  - filename: D19-1642.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1642.Attachment.zip
  author:
  - first: Qiang
    full: Qiang Ning
    id: qiang-ning
    last: Ning
  - first: Sanjay
    full: Sanjay Subramanian
    id: sanjay-subramanian
    last: Subramanian
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Qiang Ning, Sanjay Subramanian, Dan Roth
  bibkey: ning-etal-2019-improved
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1642
  month: November
  page_first: '6203'
  page_last: '6209'
  pages: "6203\u20136209"
  paper_id: '642'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1642.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1642.jpg
  title: An Improved Neural Baseline for Temporal Relation Extraction
  title_html: An Improved Neural Baseline for Temporal Relation Extraction
  url: https://www.aclweb.org/anthology/D19-1642
  year: '2019'
D19-1643:
  abstract: Fine-grained entity typing is a challenging problem since it usually involves
    a relatively large tag set and may require to understand the context of the entity
    mention. In this paper, we use entity linking to help with the fine-grained entity
    type classification process. We propose a deep neural model that makes predictions
    based on both the context and the information obtained from entity linking results.
    Experimental results on two commonly used datasets demonstrates the effectiveness
    of our approach. On both datasets, it achieves more than 5% absolute strict accuracy
    improvement over the state of the art.
  address: Hong Kong, China
  author:
  - first: Hongliang
    full: Hongliang Dai
    id: hongliang-dai
    last: Dai
  - first: Donghong
    full: Donghong Du
    id: donghong-du
    last: Du
  - first: Xin
    full: Xin Li
    id: xin-li
    last: Li
  - first: Yangqiu
    full: Yangqiu Song
    id: yangqiu-song
    last: Song
  author_string: Hongliang Dai, Donghong Du, Xin Li, Yangqiu Song
  bibkey: dai-etal-2019-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1643
  month: November
  page_first: '6210'
  page_last: '6215'
  pages: "6210\u20136215"
  paper_id: '643'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1643.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1643.jpg
  title: Improving Fine-grained Entity Typing with Entity Linking
  title_html: Improving Fine-grained Entity Typing with Entity Linking
  url: https://www.aclweb.org/anthology/D19-1643
  year: '2019'
D19-1644:
  abstract: 'In medical documents, it is possible that an entity of interest not only
    contains a discontiguous sequence of words but also overlaps with another entity.
    Entities of such structures are intrinsically hard to recognize due to the large
    space of possible entity combinations. In this work, we propose a neural two-stage
    approach to recognizing discontiguous and overlapping entities by decomposing
    this problem into two subtasks: 1) it first detects all the overlapping spans
    that either form entities on their own or present as segments of discontiguous
    entities, based on the representation of segmental hypergraph, 2) next it learns
    to combine these segments into discontiguous entities with a classifier, which
    filters out other incorrect combinations of segments. Two neural components are
    designed for these subtasks respectively and they are learned jointly using a
    shared encoder for text. Our model achieves the state-of-the-art performance in
    a standard dataset, even in the absence of external features that previous methods
    used.'
  address: Hong Kong, China
  author:
  - first: Bailin
    full: Bailin Wang
    id: bailin-wang
    last: Wang
  - first: Wei
    full: Wei Lu
    id: wei-lu
    last: Lu
  author_string: Bailin Wang, Wei Lu
  bibkey: wang-lu-2019-combining
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1644
  month: November
  page_first: '6216'
  page_last: '6224'
  pages: "6216\u20136224"
  paper_id: '644'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1644.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1644.jpg
  title: 'Combining Spans into Entities: A Neural Two-Stage Approach for Recognizing
    Discontiguous Entities'
  title_html: 'Combining Spans into Entities: A Neural Two-Stage Approach for Recognizing
    Discontiguous Entities'
  url: https://www.aclweb.org/anthology/D19-1644
  year: '2019'
D19-1645:
  abstract: Most existing relation extraction approaches exclusively target binary
    relations, and n-ary relation extraction is relatively unexplored. Current state-of-the-art
    n-ary relation extraction method is based on a supervised learning approach and,
    therefore, may suffer from the lack of sufficient relation labels. In this paper,
    we propose a novel approach to cross-sentence n-ary relation extraction based
    on universal schemas. To alleviate the sparsity problem and to leverage inherent
    decomposability of n-ary relations, we propose to learn relation representations
    of lower-arity facts that result from decomposing higher-arity facts. The proposed
    method computes a score of a new n-ary fact by aggregating scores of its decomposed
    lower-arity facts. We conduct experiments with datasets for ternary relation extraction
    and empirically show that our method improves the n-ary relation extraction performance
    compared to previous methods.
  address: Hong Kong, China
  attachment:
  - filename: D19-1645.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1645.Attachment.zip
  author:
  - first: Kosuke
    full: Kosuke Akimoto
    id: kosuke-akimoto
    last: Akimoto
  - first: Takuya
    full: Takuya Hiraoka
    id: takuya-hiraoka
    last: Hiraoka
  - first: Kunihiko
    full: Kunihiko Sadamasa
    id: kunihiko-sadamasa
    last: Sadamasa
  - first: Mathias
    full: Mathias Niepert
    id: mathias-niepert
    last: Niepert
  author_string: Kosuke Akimoto, Takuya Hiraoka, Kunihiko Sadamasa, Mathias Niepert
  bibkey: akimoto-etal-2019-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1645
  month: November
  page_first: '6225'
  page_last: '6231'
  pages: "6225\u20136231"
  paper_id: '645'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1645.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1645.jpg
  title: Cross-Sentence N-ary Relation Extraction using Lower-Arity Universal Schemas
  title_html: Cross-Sentence N-ary Relation Extraction using Lower-Arity Universal
    Schemas
  url: https://www.aclweb.org/anthology/D19-1645
  year: '2019'
D19-1646:
  abstract: Current region-based NER models only rely on fully-annotated training
    data to learn effective region encoder, which often face the training data bottleneck.
    To alleviate this problem, this paper proposes Gazetteer-Enhanced Attentive Neural
    Networks, which can enhance region-based NER by learning name knowledge of entity
    mentions from easily-obtainable gazetteers, rather than only from fully-annotated
    data. Specially, we first propose an attentive neural network (ANN), which explicitly
    models the mention-context association and therefore is convenient for integrating
    externally-learned knowledge. Then we design an auxiliary gazetteer network, which
    can effectively encode name regularity of mentions only using gazetteers. Finally,
    the learned gazetteer network is incorporated into ANN for better NER. Experiments
    show that our ANN can achieve the state-of-the-art performance on ACE2005 named
    entity recognition benchmark. Besides, incorporating gazetteer network can further
    improve the performance and significantly reduce the requirement of training data.
  address: Hong Kong, China
  author:
  - first: Hongyu
    full: Hongyu Lin
    id: hongyu-lin
    last: Lin
  - first: Yaojie
    full: Yaojie Lu
    id: yaojie-lu
    last: Lu
  - first: Xianpei
    full: Xianpei Han
    id: xianpei-han
    last: Han
  - first: Le
    full: Le Sun
    id: le-sun
    last: Sun
  - first: Bin
    full: Bin Dong
    id: bin-dong
    last: Dong
  - first: Shanshan
    full: Shanshan Jiang
    id: shanshan-jiang
    last: Jiang
  author_string: Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun, Bin Dong, Shanshan Jiang
  bibkey: lin-etal-2019-gazetteer
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1646
  month: November
  page_first: '6232'
  page_last: '6237'
  pages: "6232\u20136237"
  paper_id: '646'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1646.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1646.jpg
  title: Gazetteer-Enhanced Attentive Neural Networks for Named Entity Recognition
  title_html: Gazetteer-Enhanced Attentive Neural Networks for Named Entity Recognition
  url: https://www.aclweb.org/anthology/D19-1646
  year: '2019'
D19-1647:
  abstract: Attributing a particular property to a person by naming another person,
    who is typically wellknown for the respective property, is called a Vossian Antonomasia
    (VA). This subtpye of metonymy, which overlaps with metaphor, has a specific syntax
    and is especially frequent in journalistic texts. While identifying Vossian Antonomasia
    is of particular interest in the study of stylistics, it is also a source of errors
    in relation and fact extraction as an explicitly mentioned entity occurs only
    metaphorically and should not be associated with respective contexts. Despite
    rather simple syntactic variations, the automatic extraction of VA was never addressed
    as yet since it requires a deeper semantic understanding of mentioned entities
    and underlying relations. In this paper, we propose a first method for the extraction
    of VAs that works completely automatically. Our approaches use named entity recognition,
    distant supervision based on Wikidata, and a bi-directional LSTM for postprocessing.
    The evaluation on 1.8 million articles of the New York Times corpus shows that
    our approach significantly outperforms the only existing semi-automatic approach
    for VA identification by more than 30 percentage points in precision.
  address: Hong Kong, China
  author:
  - first: Michel
    full: Michel Schwab
    id: michel-schwab
    last: Schwab
  - first: Robert
    full: "Robert J\xE4schke"
    id: robert-jaschke
    last: "J\xE4schke"
  - first: Frank
    full: Frank Fischer
    id: frank-fischer
    last: Fischer
  - first: Jannik
    full: "Jannik Str\xF6tgen"
    id: jannik-strotgen
    last: "Str\xF6tgen"
  author_string: "Michel Schwab, Robert J\xE4schke, Frank Fischer, Jannik Str\xF6\
    tgen"
  bibkey: schwab-etal-2019-buster
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1647
  month: November
  page_first: '6238'
  page_last: '6243'
  pages: "6238\u20136243"
  paper_id: '647'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1647.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1647.jpg
  title: "\u201CA Buster Keaton of Linguistics\u201D: First Automated Approaches for\
    \ the Extraction of Vossian Antonomasia"
  title_html: "\u201CA Buster Keaton of Linguistics\u201D: First Automated Approaches\
    \ for the Extraction of Vossian Antonomasia"
  url: https://www.aclweb.org/anthology/D19-1647
  year: '2019'
D19-1648:
  abstract: "We propose a method to improve named entity recognition (NER) for chemical\
    \ compounds using multi-task learning by jointly training a chemical NER model\
    \ and a chemical com- pound paraphrase model. Our method en- ables the long short-term\
    \ memory (LSTM) of the NER model to capture chemical com- pound paraphrases by\
    \ sharing the parameters of the LSTM and character embeddings be- tween the two\
    \ models. The experimental re- sults on the BioCreative IV\u2019s CHEMDNER task\
    \ show that our method improves chemi- cal NER and achieves state-of-the-art perfor-\
    \ mance."
  address: Hong Kong, China
  author:
  - first: Taiki
    full: Taiki Watanabe
    id: taiki-watanabe
    last: Watanabe
  - first: Akihiro
    full: Akihiro Tamura
    id: akihiro-tamura
    last: Tamura
  - first: Takashi
    full: Takashi Ninomiya
    id: takashi-ninomiya
    last: Ninomiya
  - first: Takuya
    full: Takuya Makino
    id: takuya-makino
    last: Makino
  - first: Tomoya
    full: Tomoya Iwakura
    id: tomoya-iwakura
    last: Iwakura
  author_string: Taiki Watanabe, Akihiro Tamura, Takashi Ninomiya, Takuya Makino,
    Tomoya Iwakura
  bibkey: watanabe-etal-2019-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1648
  month: November
  page_first: '6244'
  page_last: '6249'
  pages: "6244\u20136249"
  paper_id: '648'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1648.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1648.jpg
  title: Multi-Task Learning for Chemical Named Entity Recognition with Chemical Compound
    Paraphrasing
  title_html: Multi-Task Learning for Chemical Named Entity Recognition with Chemical
    Compound Paraphrasing
  url: https://www.aclweb.org/anthology/D19-1648
  year: '2019'
D19-1649:
  abstract: 'We present FewRel 2.0, a more challenging task to investigate two aspects
    of few-shot relation classification models: (1) Can they adapt to a new domain
    with only a handful of instances? (2) Can they detect none-of-the-above (NOTA)
    relations? To construct FewRel 2.0, we build upon the FewRel dataset by adding
    a new test set in a quite different domain, and a NOTA relation choice. With the
    new dataset and extensive experimental analysis, we found (1) that the state-of-the-art
    few-shot relation classification models struggle on these two aspects, and (2)
    that the commonly-used techniques for domain adaptation and NOTA detection still
    cannot handle the two challenges well. Our research calls for more attention and
    further efforts to these two real-world issues. All details and resources about
    the dataset and baselines are released at https://github.com/thunlp/fewrel.'
  address: Hong Kong, China
  author:
  - first: Tianyu
    full: Tianyu Gao
    id: tianyu-gao
    last: Gao
  - first: Xu
    full: Xu Han
    id: xu-han
    last: Han
  - first: Hao
    full: Hao Zhu
    id: hao-zhu
    last: Zhu
  - first: Zhiyuan
    full: Zhiyuan Liu
    id: zhiyuan-liu
    last: Liu
  - first: Peng
    full: Peng Li
    id: peng-li
    last: Li
  - first: Maosong
    full: Maosong Sun
    id: maosong-sun
    last: Sun
  - first: Jie
    full: Jie Zhou
    id: jie-zhou
    last: Zhou
  author_string: Tianyu Gao, Xu Han, Hao Zhu, Zhiyuan Liu, Peng Li, Maosong Sun, Jie
    Zhou
  bibkey: gao-etal-2019-fewrel
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1649
  month: November
  page_first: '6250'
  page_last: '6255'
  pages: "6250\u20136255"
  paper_id: '649'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1649.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1649.jpg
  title: 'FewRel 2.0: Towards More Challenging Few-Shot Relation Classification'
  title_html: '<span class="acl-fixed-case">F</span>ew<span class="acl-fixed-case">R</span>el
    2.0: Towards More Challenging Few-Shot Relation Classification'
  url: https://www.aclweb.org/anthology/D19-1649
  year: '2019'
D19-1650:
  abstract: For those languages which use it, capitalization is an important signal
    for the fundamental NLP tasks of Named Entity Recognition (NER) and Part of Speech
    (POS) tagging. In fact, it is such a strong signal that model performance on these
    tasks drops sharply in common lowercased scenarios, such as noisy web text or
    machine translation outputs. In this work, we perform a systematic analysis of
    solutions to this problem, modifying only the casing of the train or test data
    using lowercasing and truecasing methods. While prior work and first impressions
    might suggest training a caseless model, or using a truecaser at test time, we
    show that the most effective strategy is a concatenation of cased and lowercased
    training data, producing a single model with high performance on both cased and
    uncased text. As shown in our experiments, this result holds across tasks and
    input representations. Finally, we show that our proposed solution gives an 8%
    F1 improvement in mention detection on noisy out-of-domain Twitter data.
  address: Hong Kong, China
  author:
  - first: Stephen
    full: Stephen Mayhew
    id: stephen-mayhew
    last: Mayhew
  - first: Tatiana
    full: Tatiana Tsygankova
    id: tatiana-tsygankova
    last: Tsygankova
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Stephen Mayhew, Tatiana Tsygankova, Dan Roth
  bibkey: mayhew-etal-2019-ner
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1650
  month: November
  page_first: '6256'
  page_last: '6261'
  pages: "6256\u20136261"
  paper_id: '650'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1650.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1650.jpg
  title: ner and pos when nothing is capitalized
  title_html: <span class="acl-fixed-case">ner and pos when nothing is capitalized</span>
  url: https://www.aclweb.org/anthology/D19-1650
  year: '2019'
D19-1651:
  abstract: "Open Information Extraction (Open IE) systems have been traditionally\
    \ evaluated via manual annotation. Recently, an automated evaluator with a benchmark\
    \ dataset (OIE2016) was released \u2013 it scores Open IE systems automatically\
    \ by matching system predictions with predictions in the benchmark dataset. Unfortunately,\
    \ our analysis reveals that its data is rather noisy, and the tuple matching in\
    \ the evaluator has issues, making the results of automated comparisons less trustworthy.\
    \ We contribute CaRB, an improved dataset and framework for testing Open IE systems.\
    \ To the best of our knowledge, CaRB is the first crowdsourced Open IE dataset\
    \ and it also makes substantive changes in the matching code and metrics. NLP\
    \ experts annotate CaRB\u2019s dataset to be more accurate than OIE2016. Moreover,\
    \ we find that on one pair of Open IE systems, CaRB framework provides contradictory\
    \ results to OIE2016. Human assessment verifies that CaRB\u2019s ranking of the\
    \ two systems is the accurate ranking. We release the CaRB framework along with\
    \ its crowdsourced dataset."
  address: Hong Kong, China
  attachment:
  - filename: D19-1651.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1651.Attachment.zip
  author:
  - first: Sangnie
    full: Sangnie Bhardwaj
    id: sangnie-bhardwaj
    last: Bhardwaj
  - first: Samarth
    full: Samarth Aggarwal
    id: samarth-aggarwal
    last: Aggarwal
  - first: Mausam
    full: Mausam Mausam
    id: mausam-mausam
    last: Mausam
  author_string: Sangnie Bhardwaj, Samarth Aggarwal, Mausam Mausam
  bibkey: bhardwaj-etal-2019-carb
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1651
  month: November
  page_first: '6262'
  page_last: '6267'
  pages: "6262\u20136267"
  paper_id: '651'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1651.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1651.jpg
  title: 'CaRB: A Crowdsourced Benchmark for Open IE'
  title_html: '<span class="acl-fixed-case">C</span>a<span class="acl-fixed-case">RB</span>:
    A Crowdsourced Benchmark for Open <span class="acl-fixed-case">IE</span>'
  url: https://www.aclweb.org/anthology/D19-1651
  year: '2019'
D19-1652:
  abstract: The task of entity recognition has traditionally been modelled as a sequence
    labelling task. However, this usually requires a large amount of fine-grained
    data annotated at the token level, which in turn can be expensive and cumbersome
    to obtain. In this work, we aim to circumvent this requirement of word-level annotated
    data. To achieve this, we propose a novel architecture for entity recognition
    from a corpus containing weak binary presence/absence labels, which are relatively
    easier to obtain. We show that our proposed weakly supervised model, trained solely
    on a multi-label classification task, performs reasonably well on the task of
    entity recognition, despite not having access to any token-level ground truth
    data.
  address: Hong Kong, China
  attachment:
  - filename: D19-1652.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1652.Attachment.pdf
  author:
  - first: Barun
    full: Barun Patra
    id: barun-patra
    last: Patra
  - first: Joel Ruben Antony
    full: Joel Ruben Antony Moniz
    id: joel-ruben-antony-moniz
    last: Moniz
  author_string: Barun Patra, Joel Ruben Antony Moniz
  bibkey: patra-moniz-2019-weakly
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1652
  month: November
  page_first: '6268'
  page_last: '6273'
  pages: "6268\u20136273"
  paper_id: '652'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1652.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1652.jpg
  title: Weakly Supervised Attention Networks for Entity Recognition
  title_html: Weakly Supervised Attention Networks for Entity Recognition
  url: https://www.aclweb.org/anthology/D19-1652
  year: '2019'
D19-1653:
  abstract: 'In online arguments, identifying how users construct their arguments
    to persuade others is important in order to understand a persuasive strategy directly.
    However, existing research lacks empirical investigations on highly semantic aspects
    of elementary units (EUs), such as propositions for a persuasive online argument.
    Therefore, this paper focuses on a pilot study, revealing a persuasion strategy
    using EUs. Our contributions are as follows: (1) annotating five types of EUs
    in a persuasive forum, the so-called ChangeMyView, (2) revealing both intuitive
    and non-intuitive strategic insights for the persuasion by analyzing 4612 annotated
    EUs, and (3) proposing baseline neural models that identify the EU boundary and
    type. Our observations imply that EUs definitively characterize online persuasion
    strategies.'
  address: Hong Kong, China
  author:
  - first: Gaku
    full: Gaku Morio
    id: gaku-morio
    last: Morio
  - first: Ryo
    full: Ryo Egawa
    id: ryo-egawa
    last: Egawa
  - first: Katsuhide
    full: Katsuhide Fujita
    id: katsuhide-fujita
    last: Fujita
  author_string: Gaku Morio, Ryo Egawa, Katsuhide Fujita
  bibkey: morio-etal-2019-revealing
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1653
  month: November
  page_first: '6274'
  page_last: '6279'
  pages: "6274\u20136279"
  paper_id: '653'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1653.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1653.jpg
  title: Revealing and Predicting Online Persuasion Strategy with Elementary Units
  title_html: Revealing and Predicting Online Persuasion Strategy with Elementary
    Units
  url: https://www.aclweb.org/anthology/D19-1653
  year: '2019'
D19-1654:
  abstract: Aspect-based sentiment analysis (ABSA) has attracted increasing attention
    recently due to its broad applications. In existing ABSA datasets, most sentences
    contain only one aspect or multiple aspects with the same sentiment polarity,
    which makes ABSA task degenerate to sentence-level sentiment analysis. In this
    paper, we present a new large-scale Multi-Aspect Multi-Sentiment (MAMS) dataset,
    in which each sentence contains at least two different aspects with different
    sentiment polarities. The release of this dataset would push forward the research
    in this field. In addition, we propose simple yet effective CapsNet and CapsNet-BERT
    models which combine the strengths of recent NLP advances. Experiments on our
    new dataset show that the proposed model significantly outperforms the state-of-the-art
    baseline methods
  address: Hong Kong, China
  author:
  - first: Qingnan
    full: Qingnan Jiang
    id: qingnan-jiang
    last: Jiang
  - first: Lei
    full: Lei Chen
    id: lei-chen
    last: Chen
  - first: Ruifeng
    full: Ruifeng Xu
    id: ruifeng-xu
    last: Xu
  - first: Xiang
    full: Xiang Ao
    id: xiang-ao
    last: Ao
  - first: Min
    full: Min Yang
    id: min-yang
    last: Yang
  author_string: Qingnan Jiang, Lei Chen, Ruifeng Xu, Xiang Ao, Min Yang
  bibkey: jiang-etal-2019-challenge
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1654
  month: November
  page_first: '6280'
  page_last: '6285'
  pages: "6280\u20136285"
  paper_id: '654'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1654.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1654.jpg
  title: A Challenge Dataset and Effective Models for Aspect-Based Sentiment Analysis
  title_html: A Challenge Dataset and Effective Models for Aspect-Based Sentiment
    Analysis
  url: https://www.aclweb.org/anthology/D19-1654
  year: '2019'
D19-1655:
  abstract: "Deep neural networks (DNNs) can fit (or even over-fit) the training data\
    \ very well. If a DNN model is trained using data with noisy labels and tested\
    \ on data with clean labels, the model may perform poorly. This paper studies\
    \ the problem of learning with noisy labels for sentence-level sentiment classification.\
    \ We propose a novel DNN model called NetAb (as shorthand for convolutional neural\
    \ Networks with Ab-networks) to handle noisy labels during training. NetAb consists\
    \ of two convolutional neural networks, one with a noise transition layer for\
    \ dealing with the input noisy labels and the other for predicting \u2018clean\u2019\
    \ labels. We train the two networks using their respective loss functions in a\
    \ mutual reinforcement manner. Experimental results demonstrate the effectiveness\
    \ of the proposed model."
  address: Hong Kong, China
  author:
  - first: Hao
    full: Hao Wang
    id: hao-wang
    last: Wang
  - first: Bing
    full: Bing Liu
    id: bing-liu
    last: Liu
  - first: Chaozhuo
    full: Chaozhuo Li
    id: chaozhuo-li
    last: Li
  - first: Yan
    full: Yan Yang
    id: yan-yang
    last: Yang
  - first: Tianrui
    full: Tianrui Li
    id: tianrui-li
    last: Li
  author_string: Hao Wang, Bing Liu, Chaozhuo Li, Yan Yang, Tianrui Li
  bibkey: wang-etal-2019-learning-noisy
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1655
  month: November
  page_first: '6286'
  page_last: '6292'
  pages: "6286\u20136292"
  paper_id: '655'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1655.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1655.jpg
  title: Learning with Noisy Labels for Sentence-level Sentiment Classification
  title_html: Learning with Noisy Labels for Sentence-level Sentiment Classification
  url: https://www.aclweb.org/anthology/D19-1655
  year: '2019'
D19-1656:
  abstract: We introduce a new dataset for multi-class emotion analysis from long-form
    narratives in English. The Dataset for Emotions of Narrative Sequences (DENS)
    was collected from both classic literature available on Project Gutenberg and
    modern online narratives avail- able on Wattpad, annotated using Amazon Mechanical
    Turk. A number of statistics and baseline benchmarks are provided for the dataset.
    Of the tested techniques, we find that the fine-tuning of a pre-trained BERT model
    achieves the best results, with an average micro-F1 score of 60.4%. Our results
    show that the dataset provides a novel opportunity in emotion analysis that requires
    moving beyond existing sentence-level techniques.
  address: Hong Kong, China
  attachment:
  - filename: D19-1656.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1656.Attachment.zip
  author:
  - first: Chen
    full: Chen Liu
    id: chen-liu
    last: Liu
  - first: Muhammad
    full: Muhammad Osama
    id: muhammad-osama
    last: Osama
  - first: Anderson
    full: Anderson De Andrade
    id: anderson-de-andrade
    last: De Andrade
  author_string: Chen Liu, Muhammad Osama, Anderson De Andrade
  bibkey: liu-etal-2019-dens
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1656
  month: November
  page_first: '6293'
  page_last: '6298'
  pages: "6293\u20136298"
  paper_id: '656'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1656.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1656.jpg
  title: 'DENS: A Dataset for Multi-class Emotion Analysis'
  title_html: '<span class="acl-fixed-case">DENS</span>: A Dataset for Multi-class
    Emotion Analysis'
  url: https://www.aclweb.org/anthology/D19-1656
  year: '2019'
D19-1657:
  abstract: Stance detection aims to detect whether the opinion holder is in support
    of or against a given target. Recent works show improvements in stance detection
    by using either the attention mechanism or sentiment information. In this paper,
    we propose a multi-task framework that incorporates target-specific attention
    mechanism and at the same time takes sentiment classification as an auxiliary
    task. Moreover, we used a sentiment lexicon and constructed a stance lexicon to
    provide guidance for the attention layer. Experimental results show that the proposed
    model significantly outperforms state-of-the-art deep learning methods on the
    SemEval-2016 dataset.
  address: Hong Kong, China
  author:
  - first: Yingjie
    full: Yingjie Li
    id: yingjie-li
    last: Li
  - first: Cornelia
    full: Cornelia Caragea
    id: cornelia-caragea
    last: Caragea
  author_string: Yingjie Li, Cornelia Caragea
  bibkey: li-caragea-2019-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1657
  month: November
  page_first: '6299'
  page_last: '6305'
  pages: "6299\u20136305"
  paper_id: '657'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1657.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1657.jpg
  title: Multi-Task Stance Detection with Sentiment and Stance Lexicons
  title_html: Multi-Task Stance Detection with Sentiment and Stance Lexicons
  url: https://www.aclweb.org/anthology/D19-1657
  year: '2019'
D19-1658:
  abstract: "Based on massive amounts of data, recent pretrained contextual representation\
    \ models have made significant strides in advancing a number of different English\
    \ NLP tasks. However, for other languages, relevant training data may be lacking,\
    \ while state-of-the-art deep learning methods are known to be data-hungry. In\
    \ this paper, we present an elegantly simple robust self-learning framework to\
    \ include unlabeled non-English samples in the fine-tuning process of pretrained\
    \ multilingual representation models. We leverage a multilingual model\u2019s\
    \ own predictions on unlabeled non-English data in order to obtain additional\
    \ information that can be used during further fine-tuning. Compared with original\
    \ multilingual models and other cross-lingual classification models, we observe\
    \ significant gains in effectiveness on document and sentiment classification\
    \ for a range of diverse languages."
  address: Hong Kong, China
  author:
  - first: Xin
    full: Xin Dong
    id: xin-luna-dong
    last: Dong
  - first: Gerard
    full: Gerard de Melo
    id: gerard-de-melo
    last: de Melo
  author_string: Xin Dong, Gerard de Melo
  bibkey: dong-de-melo-2019-robust
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1658
  month: November
  page_first: '6306'
  page_last: '6310'
  pages: "6306\u20136310"
  paper_id: '658'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1658.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1658.jpg
  title: A Robust Self-Learning Framework for Cross-Lingual Text Classification
  title_html: A Robust Self-Learning Framework for Cross-Lingual Text Classification
  url: https://www.aclweb.org/anthology/D19-1658
  year: '2019'
D19-1659:
  abstract: 'Flipping sentiment while preserving sentence meaning is challenging because
    parallel sentences with the same content but different sentiment polarities are
    not always available for model learning. We introduce a method for acquiring imperfectly
    aligned sentences from non-parallel corpora and propose a model that learns to
    minimize the sentiment and content losses in a fully end-to-end manner. Our model
    is simple and offers well-balanced results across two domains: Yelp restaurant
    and Amazon product reviews.'
  address: Hong Kong, China
  author:
  - first: Canasai
    full: Canasai Kruengkrai
    id: canasai-kruengkrai
    last: Kruengkrai
  author_string: Canasai Kruengkrai
  bibkey: kruengkrai-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1659
  month: November
  page_first: '6311'
  page_last: '6316'
  pages: "6311\u20136316"
  paper_id: '659'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1659.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1659.jpg
  title: Learning to Flip the Sentiment of Reviews from Non-Parallel Corpora
  title_html: Learning to Flip the Sentiment of Reviews from Non-Parallel Corpora
  url: https://www.aclweb.org/anthology/D19-1659
  year: '2019'
D19-1660:
  abstract: Twitter is used for various applications such as disaster monitoring and
    news material gathering. In these applications, each Tweet is classified into
    pre-defined classes. These classes have a semantic relationship with each other
    and can be classified into a hierarchical structure, which is regarded as important
    information. Label texts of pre-defined classes themselves also include important
    clues for classification. Therefore, we propose a method that can consider the
    hierarchical structure of labels and label texts themselves. We conducted evaluation
    over the Text REtrieval Conference (TREC) 2018 Incident Streams (IS) track dataset,
    and we found that our method outperformed the methods of the conference participants.
  address: Hong Kong, China
  author:
  - first: Taro
    full: Taro Miyazaki
    id: taro-miyazaki
    last: Miyazaki
  - first: Kiminobu
    full: Kiminobu Makino
    id: kiminobu-makino
    last: Makino
  - first: Yuka
    full: Yuka Takei
    id: yuka-takei
    last: Takei
  - first: Hiroki
    full: Hiroki Okamoto
    id: hiroki-okamoto
    last: Okamoto
  - first: Jun
    full: Jun Goto
    id: jun-goto
    last: Goto
  author_string: Taro Miyazaki, Kiminobu Makino, Yuka Takei, Hiroki Okamoto, Jun Goto
  bibkey: miyazaki-etal-2019-label
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1660
  month: November
  page_first: '6317'
  page_last: '6322'
  pages: "6317\u20136322"
  paper_id: '660'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1660.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1660.jpg
  title: Label Embedding using Hierarchical Structure of Labels for Twitter Classification
  title_html: Label Embedding using Hierarchical Structure of Labels for Twitter Classification
  url: https://www.aclweb.org/anthology/D19-1660
  year: '2019'
D19-1661:
  abstract: Word embeddings have demonstrated strong performance on NLP tasks. However,
    lack of interpretability and the unsupervised nature of word embeddings have limited
    their use within computational social science and digital humanities. We propose
    the use of informative priors to create interpretable and domain-informed dimensions
    for probabilistic word embeddings. Experimental results show that sensible priors
    can capture latent semantic concepts better than or on-par with the current state
    of the art, while retaining the simplicity and generalizability of using priors.
  address: Hong Kong, China
  attachment:
  - filename: D19-1661.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1661.Attachment.zip
  author:
  - first: Miriam
    full: Miriam Hurtado Bodell
    id: miriam-hurtado-bodell
    last: Hurtado Bodell
  - first: Martin
    full: Martin Arvidsson
    id: martin-arvidsson
    last: Arvidsson
  - first: "M\xE5ns"
    full: "M\xE5ns Magnusson"
    id: mans-magnusson
    last: Magnusson
  author_string: "Miriam Hurtado Bodell, Martin Arvidsson, M\xE5ns Magnusson"
  bibkey: hurtado-bodell-etal-2019-interpretable
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1661
  month: November
  page_first: '6323'
  page_last: '6329'
  pages: "6323\u20136329"
  paper_id: '661'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1661.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1661.jpg
  title: Interpretable Word Embeddings via Informative Priors
  title_html: Interpretable Word Embeddings via Informative Priors
  url: https://www.aclweb.org/anthology/D19-1661
  year: '2019'
D19-1662:
  abstract: Elazar and Goldberg (2018) showed that protected attributes can be extracted
    from the representations of a debiased neural network for mention detection at
    above-chance levels, by evaluating a diagnostic classifier on a held-out subsample
    of the data it was trained on. We revisit their experiments and conduct a series
    of follow-up experiments showing that, in fact, the diagnostic classifier generalizes
    poorly to both new in-domain samples and new domains, indicating that it relies
    on correlations specific to their particular data sample. We further show that
    a diagnostic classifier trained on the biased baseline neural network also does
    not generalize to new samples. In other words, the biases detected in Elazar and
    Goldberg (2018) seem restricted to their particular data sample, and would therefore
    not bias the decisions of the model on new samples, whether in-domain or out-of-domain.
    In light of this, we discuss better methodologies for detecting bias in our models.
  address: Hong Kong, China
  attachment:
  - filename: D19-1662.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1662.Attachment.pdf
  author:
  - first: Maria
    full: Maria Barrett
    id: maria-barrett
    last: Barrett
  - first: Yova
    full: Yova Kementchedjhieva
    id: yova-kementchedjhieva
    last: Kementchedjhieva
  - first: Yanai
    full: Yanai Elazar
    id: yanai-elazar
    last: Elazar
  - first: Desmond
    full: Desmond Elliott
    id: desmond-elliott
    last: Elliott
  - first: Anders
    full: "Anders S\xF8gaard"
    id: anders-sogaard
    last: "S\xF8gaard"
  author_string: "Maria Barrett, Yova Kementchedjhieva, Yanai Elazar, Desmond Elliott,\
    \ Anders S\xF8gaard"
  bibkey: barrett-etal-2019-adversarial
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1662
  month: November
  page_first: '6330'
  page_last: '6335'
  pages: "6330\u20136335"
  paper_id: '662'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1662.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1662.jpg
  title: Adversarial Removal of Demographic Attributes Revisited
  title_html: Adversarial Removal of Demographic Attributes Revisited
  url: https://www.aclweb.org/anthology/D19-1662
  year: '2019'
D19-1663:
  abstract: In this paper we propose a deep learning framework for sarcasm target
    detection in predefined sarcastic texts. Identification of sarcasm targets can
    help in many core natural language processing tasks such as aspect based sentiment
    analysis, opinion mining etc. To begin with, we perform an empirical study of
    the socio-linguistic features and identify those that are statistically significant
    in indicating sarcasm targets (p-values in the range(0.05,0.001)). Finally, we
    present a deep-learning framework augmented with socio-linguistic features to
    detect sarcasm targets in sarcastic book-snippets and tweets.We achieve a huge
    improvement in the performance in terms of exact match and dice scores compared
    to the current state-of-the-art baseline.
  address: Hong Kong, China
  author:
  - first: Jasabanta
    full: Jasabanta Patro
    id: jasabanta-patro
    last: Patro
  - first: Srijan
    full: Srijan Bansal
    id: srijan-bansal
    last: Bansal
  - first: Animesh
    full: Animesh Mukherjee
    id: animesh-mukherjee
    last: Mukherjee
  author_string: Jasabanta Patro, Srijan Bansal, Animesh Mukherjee
  bibkey: patro-etal-2019-deep
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1663
  month: November
  page_first: '6336'
  page_last: '6342'
  pages: "6336\u20136342"
  paper_id: '663'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1663.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1663.jpg
  title: A deep-learning framework to detect sarcasm targets
  title_html: A deep-learning framework to detect sarcasm targets
  url: https://www.aclweb.org/anthology/D19-1663
  year: '2019'
D19-1664:
  abstract: 'The increasing prevalence of political bias in news media calls for greater
    public awareness of it, as well as robust methods for its detection. While prior
    work in NLP has primarily focused on the lexical bias captured by linguistic attributes
    such as word choice and syntax, other types of bias stem from the actual content
    selected for inclusion in the text. In this work, we investigate the effects of
    informational bias: factual content that can nevertheless be deployed to sway
    reader opinion. We first produce a new dataset, BASIL, of 300 news articles annotated
    with 1,727 bias spans and find evidence that informational bias appears in news
    articles more frequently than lexical bias. We further study our annotations to
    observe how informational bias surfaces in news articles by different media outlets.
    Lastly, a baseline model for informational bias prediction is presented by fine-tuning
    BERT on our labeled data, indicating the challenges of the task and future directions.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1664.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1664.Attachment.zip
  author:
  - first: Lisa
    full: Lisa Fan
    id: lisa-fan
    last: Fan
  - first: Marshall
    full: Marshall White
    id: marshall-white
    last: White
  - first: Eva
    full: Eva Sharma
    id: eva-sharma
    last: Sharma
  - first: Ruisi
    full: Ruisi Su
    id: ruisi-su
    last: Su
  - first: Prafulla Kumar
    full: Prafulla Kumar Choubey
    id: prafulla-kumar-choubey
    last: Choubey
  - first: Ruihong
    full: Ruihong Huang
    id: ruihong-huang
    last: Huang
  - first: Lu
    full: Lu Wang
    id: lu-wang
    last: Wang
  author_string: Lisa Fan, Marshall White, Eva Sharma, Ruisi Su, Prafulla Kumar Choubey,
    Ruihong Huang, Lu Wang
  bibkey: fan-etal-2019-plain
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1664
  month: November
  page_first: '6343'
  page_last: '6349'
  pages: "6343\u20136349"
  paper_id: '664'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1664.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1664.jpg
  title: 'In Plain Sight: Media Bias Through the Lens of Factual Reporting'
  title_html: 'In Plain Sight: Media Bias Through the Lens of Factual Reporting'
  url: https://www.aclweb.org/anthology/D19-1664
  year: '2019'
D19-1665:
  abstract: Stance detection in social media is a well-studied task in a variety of
    domains. Nevertheless, previous work has mostly focused on multiclass versions
    of the problem, where the labels are mutually exclusive, and typically positive,
    negative or neutral. In this paper, we address versions of the task in which an
    utterance can have multiple labels, thus corresponding to multilabel classification.
    We propose a method that explicitly incorporates label dependencies in the training
    objective and compare it against a variety of baselines, as well as a reduction
    of multilabel to multiclass learning. In experiments with three datasets, we find
    that our proposed method improves upon all baselines on two out of three datasets.
    We also show that the reduction of multilabel to multiclass classification can
    be very competitive, especially in cases where the output consists of a small
    number of labels and one can enumerate over all label combinations.
  address: Hong Kong, China
  attachment:
  - filename: D19-1665.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1665.Attachment.zip
  author:
  - first: William
    full: William Ferreira
    id: william-ferreira
    last: Ferreira
  - first: Andreas
    full: Andreas Vlachos
    id: andreas-vlachos
    last: Vlachos
  author_string: William Ferreira, Andreas Vlachos
  bibkey: ferreira-vlachos-2019-incorporating
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1665
  month: November
  page_first: '6350'
  page_last: '6354'
  pages: "6350\u20136354"
  paper_id: '665'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1665.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1665.jpg
  title: Incorporating Label Dependencies in Multilabel Stance Detection
  title_html: Incorporating Label Dependencies in Multilabel Stance Detection
  url: https://www.aclweb.org/anthology/D19-1665
  year: '2019'
D19-1666:
  abstract: Sports broadcasters inject drama into play-by-play commentary by building
    team and player narratives through subjective analyses and anecdotes. Prior studies
    based on small datasets and manual coding show that such theatrics evince commentator
    bias in sports broadcasts. To examine this phenomenon, we assemble FOOTBALL, which
    contains 1,455 broadcast transcripts from American football games across six decades
    that are automatically annotated with 250K player mentions and linked with racial
    metadata. We identify major confounding factors for researchers examining racial
    bias in FOOTBALL, and perform a computational analysis that supports conclusions
    from prior social science studies.
  address: Hong Kong, China
  attachment:
  - filename: D19-1666.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1666.Attachment.pdf
  author:
  - first: Jack
    full: Jack Merullo
    id: jack-merullo
    last: Merullo
  - first: Luke
    full: Luke Yeh
    id: luke-yeh
    last: Yeh
  - first: Abram
    full: Abram Handler
    id: abram-handler
    last: Handler
  - first: Alvin
    full: Alvin Grissom II
    id: alvin-grissom-ii
    last: Grissom II
  - first: Brendan
    full: "Brendan O\u2019Connor"
    id: brendan-oconnor
    last: "O\u2019Connor"
  - first: Mohit
    full: Mohit Iyyer
    id: mohit-iyyer
    last: Iyyer
  author_string: "Jack Merullo, Luke Yeh, Abram Handler, Alvin Grissom II, Brendan\
    \ O\u2019Connor, Mohit Iyyer"
  bibkey: merullo-etal-2019-investigating
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1666
  month: November
  page_first: '6355'
  page_last: '6361'
  pages: "6355\u20136361"
  paper_id: '666'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1666.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1666.jpg
  title: Investigating Sports Commentator Bias within a Large Corpus of American Football
    Broadcasts
  title_html: Investigating Sports Commentator Bias within a Large Corpus of <span
    class="acl-fixed-case">A</span>merican Football Broadcasts
  url: https://www.aclweb.org/anthology/D19-1666
  year: '2019'
D19-1667:
  abstract: Judgment prediction for legal cases has attracted much research efforts
    for its practice use, of which the ultimate goal is prison term prediction. While
    existing work merely predicts the total prison term, in reality a defendant is
    often charged with multiple crimes. In this paper, we argue that charge-based
    prison term prediction (CPTP) not only better fits realistic needs, but also makes
    the total prison term prediction more accurate and interpretable. We collect the
    first large-scale structured data for CPTP and evaluate several competitive baselines.
    Based on the observation that fine-grained feature selection is the key to achieving
    good performance, we propose the Deep Gating Network (DGN) for charge-specific
    feature selection and aggregation. Experiments show that DGN achieves the state-of-the-art
    performance.
  address: Hong Kong, China
  author:
  - first: Huajie
    full: Huajie Chen
    id: huajie-chen
    last: Chen
  - first: Deng
    full: Deng Cai
    id: deng-cai
    last: Cai
  - first: Wei
    full: Wei Dai
    id: wei-dai
    last: Dai
  - first: Zehui
    full: Zehui Dai
    id: zehui-dai
    last: Dai
  - first: Yadong
    full: Yadong Ding
    id: yadong-ding
    last: Ding
  author_string: Huajie Chen, Deng Cai, Wei Dai, Zehui Dai, Yadong Ding
  bibkey: chen-etal-2019-charge
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1667
  month: November
  page_first: '6362'
  page_last: '6367'
  pages: "6362\u20136367"
  paper_id: '667'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1667.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1667.jpg
  title: Charge-Based Prison Term Prediction with Deep Gating Network
  title_html: Charge-Based Prison Term Prediction with Deep Gating Network
  url: https://www.aclweb.org/anthology/D19-1667
  year: '2019'
D19-1668:
  abstract: "Ancient History relies on disciplines such as Epigraphy, the study of\
    \ ancient inscribed texts, for evidence of the recorded past. However, these texts,\
    \ \u201Cinscriptions\u201D, are often damaged over the centuries, and illegible\
    \ parts of the text must be restored by specialists, known as epigraphists. This\
    \ work presents Pythia, the first ancient text restoration model that recovers\
    \ missing characters from a damaged text input using deep neural networks. Its\
    \ architecture is carefully designed to handle long-term context information,\
    \ and deal efficiently with missing or corrupted character and word representations.\
    \ To train it, we wrote a non-trivial pipeline to convert PHI, the largest digital\
    \ corpus of ancient Greek inscriptions, to machine actionable text, which we call\
    \ PHI-ML. On PHI-ML, Pythia\u2019s predictions achieve a 30.1% character error\
    \ rate, compared to the 57.3% of human epigraphists. Moreover, in 73.5% of cases\
    \ the ground-truth sequence was among the Top-20 hypotheses of Pythia, which effectively\
    \ demonstrates the impact of this assistive method on the field of digital epigraphy,\
    \ and sets the state-of-the-art in ancient text restoration."
  address: Hong Kong, China
  attachment:
  - filename: D19-1668.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1668.Attachment.zip
  author:
  - first: Yannis
    full: Yannis Assael
    id: yannis-assael
    last: Assael
  - first: Thea
    full: Thea Sommerschield
    id: thea-sommerschield
    last: Sommerschield
  - first: Jonathan
    full: Jonathan Prag
    id: jonathan-prag
    last: Prag
  author_string: Yannis Assael, Thea Sommerschield, Jonathan Prag
  bibkey: assael-etal-2019-restoring
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1668
  month: November
  page_first: '6368'
  page_last: '6375'
  pages: "6368\u20136375"
  paper_id: '668'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1668.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1668.jpg
  title: 'Restoring ancient text using deep learning: a case study on Greek epigraphy'
  title_html: 'Restoring ancient text using deep learning: a case study on <span class="acl-fixed-case">G</span>reek
    epigraphy'
  url: https://www.aclweb.org/anthology/D19-1668
  year: '2019'
D19-1669:
  abstract: We propose a novel tensor embedding method that can effectively extract
    lexical features for humor recognition. Specifically, we use word-word co-occurrence
    to encode the contextual content of documents, and then decompose the tensor to
    get corresponding vector representations. We show that this simple method can
    capture features of lexical humor effectively for continuous humor recognition.
    In particular, we achieve a distance of 0.887 on a global humor ranking task,
    comparable to the top performing systems from SemEval 2017 Task 6B (Potash et
    al., 2017) but without the need for any external training corpus. In addition,
    we further show that this approach is also beneficial for small sample humor recognition
    tasks through a semi-supervised label propagation procedure, which achieves about
    0.7 accuracy on the 16000 One-Liners (Mihalcea and Strapparava, 2005) and Pun
    of the Day (Yang et al., 2015) humour classification datasets using only 10% of
    known labels.
  address: Hong Kong, China
  author:
  - first: Zhenjie
    full: Zhenjie Zhao
    id: zhenjie-zhao
    last: Zhao
  - first: Andrew
    full: Andrew Cattle
    id: andrew-cattle
    last: Cattle
  - first: Evangelos
    full: Evangelos Papalexakis
    id: evangelos-papalexakis
    last: Papalexakis
  - first: Xiaojuan
    full: Xiaojuan Ma
    id: xiaojuan-ma
    last: Ma
  author_string: Zhenjie Zhao, Andrew Cattle, Evangelos Papalexakis, Xiaojuan Ma
  bibkey: zhao-etal-2019-embedding
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1669
  month: November
  page_first: '6376'
  page_last: '6381'
  pages: "6376\u20136381"
  paper_id: '669'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1669.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1669.jpg
  title: Embedding Lexical Features via Tensor Decomposition for Small Sample Humor
    Recognition
  title_html: Embedding Lexical Features via Tensor Decomposition for Small Sample
    Humor Recognition
  url: https://www.aclweb.org/anthology/D19-1669
  year: '2019'
D19-1670:
  abstract: 'We present EDA: easy data augmentation techniques for boosting performance
    on text classification tasks. EDA consists of four simple but powerful operations:
    synonym replacement, random insertion, random swap, and random deletion. On five
    text classification tasks, we show that EDA improves performance for both convolutional
    and recurrent neural networks. EDA demonstrates particularly strong results for
    smaller datasets; on average, across five datasets, training with EDA while using
    only 50% of the available training set achieved the same accuracy as normal training
    with all available data. We also performed extensive ablation studies and suggest
    parameters for practical use.'
  address: Hong Kong, China
  attachment:
  - filename: D19-1670.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1670.Attachment.zip
  author:
  - first: Jason
    full: Jason Wei
    id: jason-wei
    last: Wei
  - first: Kai
    full: Kai Zou
    id: kai-zou
    last: Zou
  author_string: Jason Wei, Kai Zou
  bibkey: wei-zou-2019-eda
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1670
  month: November
  page_first: '6382'
  page_last: '6388'
  pages: "6382\u20136388"
  paper_id: '670'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1670.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1670.jpg
  title: 'EDA: Easy Data Augmentation Techniques for Boosting Performance on Text
    Classification Tasks'
  title_html: '<span class="acl-fixed-case">EDA</span>: Easy Data Augmentation Techniques
    for Boosting Performance on Text Classification Tasks'
  url: https://www.aclweb.org/anthology/D19-1670
  year: '2019'
D19-1671:
  abstract: News recommendation can help users find interested news and alleviate
    information overload. Precisely modeling news and users is critical for news recommendation,
    and capturing the contexts of words and news is important to learn news and user
    representations. In this paper, we propose a neural news recommendation approach
    with multi-head self-attention (NRMS). The core of our approach is a news encoder
    and a user encoder. In the news encoder, we use multi-head self-attentions to
    learn news representations from news titles by modeling the interactions between
    words. In the user encoder, we learn representations of users from their browsed
    news and use multi-head self-attention to capture the relatedness between the
    news. Besides, we apply additive attention to learn more informative news and
    user representations by selecting important words and news. Experiments on a real-world
    dataset validate the effectiveness and efficiency of our approach.
  address: Hong Kong, China
  author:
  - first: Chuhan
    full: Chuhan Wu
    id: chuhan-wu
    last: Wu
  - first: Fangzhao
    full: Fangzhao Wu
    id: fangzhao-wu
    last: Wu
  - first: Suyu
    full: Suyu Ge
    id: suyu-ge
    last: Ge
  - first: Tao
    full: Tao Qi
    id: tao-qi
    last: Qi
  - first: Yongfeng
    full: Yongfeng Huang
    id: yongfeng-huang
    last: Huang
  - first: Xing
    full: Xing Xie
    id: xing-xie
    last: Xie
  author_string: Chuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng Huang, Xing Xie
  bibkey: wu-etal-2019-neural-news
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1671
  month: November
  page_first: '6389'
  page_last: '6394'
  pages: "6389\u20136394"
  paper_id: '671'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1671.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1671.jpg
  title: Neural News Recommendation with Multi-Head Self-Attention
  title_html: Neural News Recommendation with Multi-Head Self-Attention
  url: https://www.aclweb.org/anthology/D19-1671
  year: '2019'
D19-1672:
  abstract: 'Building named entity recognition (NER) models for languages that do
    not have much training data is a challenging task. While recent work has shown
    promising results on cross-lingual transfer from high-resource languages, it is
    unclear what knowledge is transferred. In this paper, we first propose a simple
    and efficient neural architecture for cross-lingual NER. Experiments show that
    our model achieves competitive performance with the state-of-the-art. We further
    explore how transfer learning works for cross-lingual NER on two transferable
    factors: sequential order and multilingual embedding. Our results shed light on
    future research for improving cross-lingual NER.'
  address: Hong Kong, China
  author:
  - first: Xiaolei
    full: Xiaolei Huang
    id: xiaolei-huang
    last: Huang
  - first: Jonathan
    full: Jonathan May
    id: jonathan-may
    last: May
  - first: Nanyun
    full: Nanyun Peng
    id: nanyun-peng
    last: Peng
  author_string: Xiaolei Huang, Jonathan May, Nanyun Peng
  bibkey: huang-etal-2019-matters
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1672
  month: November
  page_first: '6395'
  page_last: '6401'
  pages: "6395\u20136401"
  paper_id: '672'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1672.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1672.jpg
  title: 'What Matters for Neural Cross-Lingual Named Entity Recognition: An Empirical
    Analysis'
  title_html: 'What Matters for Neural Cross-Lingual Named Entity Recognition: An
    Empirical Analysis'
  url: https://www.aclweb.org/anthology/D19-1672
  year: '2019'
D19-1673:
  abstract: Humor plays important role in human communication, which makes it important
    problem for natural language processing. Prior work on the analysis of humor focuses
    on whether text is humorous or not, or the degree of funniness, but this is insufficient
    to explain why it is funny. We therefore create a dataset on humor with 9,123
    manually annotated jokes in Chinese. We propose a novel annotation scheme to give
    scenarios of how humor arises in text. Specifically, our annotations of linguistic
    humor not only contain the degree of funniness, like previous work, but they also
    contain key words that trigger humor as well as character relationship, scene,
    and humor categories. We report reasonable agreement between annota-tors. We also
    conduct an analysis and exploration of the dataset. To the best of our knowledge,
    we are the first to approach humor annotation for exploring the underlying mechanism
    of the use of humor, which may contribute to a significantly deeper analysis of
    humor. We also contribute with a scarce and valuable dataset, which we will release
    publicly.
  address: Hong Kong, China
  author:
  - first: Dongyu
    full: Dongyu Zhang
    id: dongyu-zhang
    last: Zhang
  - first: Heting
    full: Heting Zhang
    id: heting-zhang
    last: Zhang
  - first: Xikai
    full: Xikai Liu
    id: xikai-liu
    last: Liu
  - first: Hongfei
    full: Hongfei Lin
    id: hongfei-lin
    last: Lin
  - first: Feng
    full: Feng Xia
    id: feng-xia
    last: Xia
  author_string: Dongyu Zhang, Heting Zhang, Xikai Liu, Hongfei Lin, Feng Xia
  bibkey: zhang-etal-2019-telling
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1673
  month: November
  page_first: '6402'
  page_last: '6407'
  pages: "6402\u20136407"
  paper_id: '673'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1673.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1673.jpg
  title: 'Telling the Whole Story: A Manually Annotated Chinese Dataset for the Analysis
    of Humor in Jokes'
  title_html: 'Telling the Whole Story: A Manually Annotated <span class="acl-fixed-case">C</span>hinese
    Dataset for the Analysis of Humor in Jokes'
  url: https://www.aclweb.org/anthology/D19-1673
  year: '2019'
D19-1674:
  abstract: "An anagram is a sentence or a phrase that is made by permutating the\
    \ characters of an input sentence or a phrase. For example, \u201CTrims cash\u201D\
    \ is an anagram of \u201CChristmas\u201D. Existing automatic anagram generation\
    \ methods can find possible combinations of words form an anagram. However, they\
    \ do not pay much attention to the naturalness of the generated anagrams. In this\
    \ paper, we show that simple depth-first search can yield natural anagrams when\
    \ it is combined with modern neural language models. Human evaluation results\
    \ show that the proposed method can generate significantly more natural anagrams\
    \ than baseline methods."
  address: Hong Kong, China
  author:
  - first: Masaaki
    full: Masaaki Nishino
    id: masaaki-nishino
    last: Nishino
  - first: Sho
    full: Sho Takase
    id: sho-takase
    last: Takase
  - first: Tsutomu
    full: Tsutomu Hirao
    id: tsutomu-hirao
    last: Hirao
  - first: Masaaki
    full: Masaaki Nagata
    id: masaaki-nagata
    last: Nagata
  author_string: Masaaki Nishino, Sho Takase, Tsutomu Hirao, Masaaki Nagata
  bibkey: nishino-etal-2019-generating
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1674
  month: November
  page_first: '6408'
  page_last: '6412'
  pages: "6408\u20136412"
  paper_id: '674'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1674.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1674.jpg
  title: 'Generating Natural Anagrams: Towards Language Generation Under Hard Combinatorial
    Constraints'
  title_html: 'Generating Natural Anagrams: Towards Language Generation Under Hard
    Combinatorial Constraints'
  url: https://www.aclweb.org/anthology/D19-1674
  year: '2019'
D19-1675:
  abstract: "Controversial claims are abundant in online media and discussion forums.\
    \ A better understanding of such claims requires analyzing them from different\
    \ perspectives. Stance classification is a necessary step for inferring these\
    \ perspectives in terms of supporting or opposing the claim. In this work, we\
    \ present a neural network model for stance classification leveraging BERT representations\
    \ and augmenting them with a novel consistency constraint. Experiments on the\
    \ Perspectrum dataset, consisting of claims and users\u2019 perspectives from\
    \ various debate websites, demonstrate the effectiveness of our approach over\
    \ state-of-the-art baselines."
  address: Hong Kong, China
  author:
  - first: Kashyap
    full: Kashyap Popat
    id: kashyap-popat
    last: Popat
  - first: Subhabrata
    full: Subhabrata Mukherjee
    id: subhabrata-mukherjee
    last: Mukherjee
  - first: Andrew
    full: Andrew Yates
    id: andrew-yates
    last: Yates
  - first: Gerhard
    full: Gerhard Weikum
    id: gerhard-weikum
    last: Weikum
  author_string: Kashyap Popat, Subhabrata Mukherjee, Andrew Yates, Gerhard Weikum
  bibkey: popat-etal-2019-stancy
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1675
  month: November
  page_first: '6413'
  page_last: '6418'
  pages: "6413\u20136418"
  paper_id: '675'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1675.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1675.jpg
  title: 'STANCY: Stance Classification Based on Consistency Cues'
  title_html: '<span class="acl-fixed-case">STANCY</span>: Stance Classification Based
    on Consistency Cues'
  url: https://www.aclweb.org/anthology/D19-1675
  year: '2019'
D19-1676:
  abstract: This paper explores different approaches to multilingual intent classification
    in a low resource setting. Recent advances in multilingual text representations
    promise cross-lingual transfer for classifiers. We investigate the potential for
    this transfer in an applied industrial setting and compare to multilingual classification
    using machine translated text. Our results show that while the recently developed
    methods show promise, practical application calls for a combination of techniques
    for useful results.
  address: Hong Kong, China
  author:
  - first: Talaat
    full: Talaat Khalil
    id: talaat-khalil
    last: Khalil
  - first: Kornel
    full: "Kornel Kie\u0142czewski"
    id: kornel-kielczewski
    last: "Kie\u0142czewski"
  - first: Georgios Christos
    full: Georgios Christos Chouliaras
    id: georgios-christos-chouliaras
    last: Chouliaras
  - first: Amina
    full: Amina Keldibek
    id: amina-keldibek
    last: Keldibek
  - first: Maarten
    full: Maarten Versteegh
    id: maarten-versteegh
    last: Versteegh
  author_string: "Talaat Khalil, Kornel Kie\u0142czewski, Georgios Christos Chouliaras,\
    \ Amina Keldibek, Maarten Versteegh"
  bibkey: khalil-etal-2019-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1676
  month: November
  page_first: '6419'
  page_last: '6424'
  pages: "6419\u20136424"
  paper_id: '676'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1676.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1676.jpg
  title: Cross-lingual intent classification in a low resource industrial setting
  title_html: Cross-lingual intent classification in a low resource industrial setting
  url: https://www.aclweb.org/anthology/D19-1676
  year: '2019'
D19-1677:
  abstract: We continue the study of generating se-mantically correct regular expressions
    from natural language descriptions (NL). The current state-of-the-art model SemRegex
    produces regular expressions from NLs by rewarding the reinforced learning based
    on the semantic (rather than syntactic) equivalence between two regular expressions.
    Since the regular expression equivalence problem is PSPACE-complete, we introduce
    the EQ_Reg model for computing the simi-larity of two regular expressions using
    deep neural networks. Our EQ_Reg mod-el essentially softens the equivalence of
    two regular expressions when used as a reward function. We then propose a new
    regex generation model, SoftRegex, us-ing the EQ_Reg model, and empirically demonstrate
    that SoftRegex substantially reduces the training time (by a factor of at least
    3.6) and produces state-of-the-art results on three benchmark datasets.
  address: Hong Kong, China
  author:
  - first: Jun-U
    full: Jun-U Park
    id: jun-u-park
    last: Park
  - first: Sang-Ki
    full: Sang-Ki Ko
    id: sang-ki-ko
    last: Ko
  - first: Marco
    full: Marco Cognetta
    id: marco-cognetta
    last: Cognetta
  - first: Yo-Sub
    full: Yo-Sub Han
    id: yo-sub-han
    last: Han
  author_string: Jun-U Park, Sang-Ki Ko, Marco Cognetta, Yo-Sub Han
  bibkey: park-etal-2019-softregex
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1677
  month: November
  page_first: '6425'
  page_last: '6431'
  pages: "6425\u20136431"
  paper_id: '677'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1677.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1677.jpg
  title: 'SoftRegex: Generating Regex from Natural Language Descriptions using Softened
    Regex Equivalence'
  title_html: '<span class="acl-fixed-case">S</span>oft<span class="acl-fixed-case">R</span>egex:
    Generating Regex from Natural Language Descriptions using Softened Regex Equivalence'
  url: https://www.aclweb.org/anthology/D19-1677
  year: '2019'
D19-1678:
  abstract: "Monitoring patients in ICU is a challenging and high-cost task. Hence,\
    \ predicting the condition of patients during their ICU stay can help provide\
    \ better acute care and plan the hospital\u2019s resources. There has been continuous\
    \ progress in machine learning research for ICU management, and most of this work\
    \ has focused on using time series signals recorded by ICU instruments. In our\
    \ work, we show that adding clinical notes as another modality improves the performance\
    \ of the model for three benchmark tasks: in-hospital mortality prediction, modeling\
    \ decompensation, and length of stay forecasting that play an important role in\
    \ ICU management. While the time-series data is measured at regular intervals,\
    \ doctor notes are charted at irregular times, making it challenging to model\
    \ them together. We propose a method to model them jointly, achieving considerable\
    \ improvement across benchmark tasks over baseline time-series model."
  address: Hong Kong, China
  author:
  - first: Swaraj
    full: Swaraj Khadanga
    id: swaraj-khadanga
    last: Khadanga
  - first: Karan
    full: Karan Aggarwal
    id: karan-aggarwal
    last: Aggarwal
  - first: Shafiq
    full: Shafiq Joty
    id: shafiq-joty
    last: Joty
  - first: Jaideep
    full: Jaideep Srivastava
    id: jaideep-srivastava
    last: Srivastava
  author_string: Swaraj Khadanga, Karan Aggarwal, Shafiq Joty, Jaideep Srivastava
  bibkey: khadanga-etal-2019-using
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1678
  month: November
  page_first: '6432'
  page_last: '6437'
  pages: "6432\u20136437"
  paper_id: '678'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1678.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1678.jpg
  title: Using Clinical Notes with Time Series Data for ICU Management
  title_html: Using Clinical Notes with Time Series Data for <span class="acl-fixed-case">ICU</span>
    Management
  url: https://www.aclweb.org/anthology/D19-1678
  year: '2019'
D19-1679:
  abstract: "We present a machine foreign-language teacher that modifies text in a\
    \ student\u2019s native language (L1) by replacing some word tokens with glosses\
    \ in a foreign language (L2), in such a way that the student can acquire L2 vocabulary\
    \ simply by reading the resulting macaronic text. The machine teacher uses no\
    \ supervised data from human students. Instead, to guide the machine teacher\u2019\
    s choice of which words to replace, we equip a cloze language model with a training\
    \ procedure that can incrementally learn representations for novel words, and\
    \ use this model as a proxy for the word guessing and learning ability of real\
    \ human students. We use Mechanical Turk to evaluate two variants of the student\
    \ model: (i) one that generates a representation for a novel word using only surrounding\
    \ context and (ii) an extension that also uses the spelling of the novel word."
  address: Hong Kong, China
  attachment:
  - filename: D19-1679.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1679.Attachment.pdf
  author:
  - first: Adithya
    full: Adithya Renduchintala
    id: adithya-renduchintala
    last: Renduchintala
  - first: Philipp
    full: Philipp Koehn
    id: philipp-koehn
    last: Koehn
  - first: Jason
    full: Jason Eisner
    id: jason-eisner
    last: Eisner
  author_string: Adithya Renduchintala, Philipp Koehn, Jason Eisner
  bibkey: renduchintala-etal-2019-spelling
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1679
  month: November
  page_first: '6438'
  page_last: '6443'
  pages: "6438\u20136443"
  paper_id: '679'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1679.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1679.jpg
  title: Spelling-Aware Construction of Macaronic Texts for Teaching Foreign-Language
    Vocabulary
  title_html: Spelling-Aware Construction of Macaronic Texts for Teaching Foreign-Language
    Vocabulary
  url: https://www.aclweb.org/anthology/D19-1679
  year: '2019'
D19-1680:
  abstract: Solving long-lasting problems such as food insecurity requires a comprehensive
    understanding of interventions applied by governments and international humanitarian
    assistance organizations, and their results and consequences. Towards achieving
    this grand goal, a crucial first step is to extract past interventions and when
    and where they have been applied, from hundreds of thousands of reports automatically.
    In this paper, we developed a corpus annotated with interventions to foster research,
    and developed an information extraction system for extracting interventions and
    their location and time from text. We demonstrate early, very encouraging results
    on extracting interventions.
  address: Hong Kong, China
  author:
  - first: Bonan
    full: Bonan Min
    id: bonan-min
    last: Min
  - first: Yee Seng
    full: Yee Seng Chan
    id: yee-seng-chan
    last: Chan
  - first: Haoling
    full: Haoling Qiu
    id: haoling-qiu
    last: Qiu
  - first: Joshua
    full: Joshua Fasching
    id: joshua-fasching
    last: Fasching
  author_string: Bonan Min, Yee Seng Chan, Haoling Qiu, Joshua Fasching
  bibkey: min-etal-2019-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1680
  month: November
  page_first: '6444'
  page_last: '6448'
  pages: "6444\u20136448"
  paper_id: '680'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1680.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1680.jpg
  title: Towards Machine Reading for Interventions from Humanitarian-Assistance Program
    Literature
  title_html: Towards Machine Reading for Interventions from Humanitarian-Assistance
    Program Literature
  url: https://www.aclweb.org/anthology/D19-1680
  year: '2019'
D19-1681:
  abstract: Following navigation instructions in natural language (NL) requires a
    composition of language, action, and knowledge of the environment. Knowledge of
    the environment may be provided via visual sensors or as a symbolic world representation
    referred to as a map. Previous work on map-based NL navigation relied on small
    artificial worlds with a fixed set of entities known in advance. Here we introduce
    the Realistic Urban Navigation (RUN) task, aimed at interpreting NL navigation
    instructions based on a real, dense, urban map. Using Amazon Mechanical Turk,
    we collected a dataset of 2515 instructions aligned with actual routes over three
    regions of Manhattan. We then empirically study which aspects of a neural architecture
    are important for the RUN success, and empirically show that entity abstraction,
    attention over words and worlds, and a constantly updating world-state, significantly
    contribute to task accuracy.
  address: Hong Kong, China
  attachment:
  - filename: D19-1681.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1681.Attachment.zip
  author:
  - first: Tzuf
    full: Tzuf Paz-Argaman
    id: tzuf-paz-argaman
    last: Paz-Argaman
  - first: Reut
    full: Reut Tsarfaty
    id: reut-tsarfaty
    last: Tsarfaty
  author_string: Tzuf Paz-Argaman, Reut Tsarfaty
  bibkey: paz-argaman-tsarfaty-2019-run
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1681
  month: November
  page_first: '6449'
  page_last: '6455'
  pages: "6449\u20136455"
  paper_id: '681'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1681.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1681.jpg
  title: 'RUN through the Streets: A New Dataset and Baseline Models for Realistic
    Urban Navigation'
  title_html: '<span class="acl-fixed-case">RUN</span> through the Streets: A New
    Dataset and Baseline Models for Realistic Urban Navigation'
  url: https://www.aclweb.org/anthology/D19-1681
  year: '2019'
D19-1682:
  abstract: In multi-party chat, it is common for multiple conversations to occur
    concurrently, leading to intermingled conversation threads in chat logs. In this
    work, we propose a novel Context-Aware Thread Detection (CATD) model that automatically
    disentangles these conversation threads. We evaluate our model on four real-world
    datasets and demonstrate an overall im-provement in thread detection accuracy
    over state-of-the-art benchmarks.
  address: Hong Kong, China
  attachment:
  - filename: D19-1682.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-1682.Attachment.zip
  author:
  - first: Ming
    full: Ming Tan
    id: ming-tan
    last: Tan
  - first: Dakuo
    full: Dakuo Wang
    id: dakuo-wang
    last: Wang
  - first: Yupeng
    full: Yupeng Gao
    id: yupeng-gao
    last: Gao
  - first: Haoyu
    full: Haoyu Wang
    id: haoyu-wang
    last: Wang
  - first: Saloni
    full: Saloni Potdar
    id: saloni-potdar
    last: Potdar
  - first: Xiaoxiao
    full: Xiaoxiao Guo
    id: xiaoxiao-guo
    last: Guo
  - first: Shiyu
    full: Shiyu Chang
    id: shiyu-chang
    last: Chang
  - first: Mo
    full: Mo Yu
    id: mo-yu
    last: Yu
  author_string: Ming Tan, Dakuo Wang, Yupeng Gao, Haoyu Wang, Saloni Potdar, Xiaoxiao
    Guo, Shiyu Chang, Mo Yu
  bibkey: tan-etal-2019-context
  bibtype: inproceedings
  booktitle: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)
  booktitle_html: Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)
  doi: 10.18653/v1/D19-1682
  month: November
  page_first: '6456'
  page_last: '6461'
  pages: "6456\u20136461"
  paper_id: '682'
  parent_volume_id: D19-1
  pdf: https://www.aclweb.org/anthology/D19-1682.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-1682.jpg
  title: Context-Aware Conversation Thread Detection in Multi-Party Chat
  title_html: Context-Aware Conversation Thread Detection in Multi-Party Chat
  url: https://www.aclweb.org/anthology/D19-1682
  year: '2019'
D19-2001:
  abstract: "Deep learning has become the dominant approach to NLP problems, especially\
    \ when applied on large scale corpora. Recent progress on unsupervised pre-training\
    \ techniques such as BERT, ELMo, GPT-2, and language modeling in general, when\
    \ applied on large corpora, is shown to be effective in improving a wide variety\
    \ of downstream tasks. These techniques push the limits of available hardware,\
    \ requiring specialized frameworks optimized for GPU, ASIC, and distributed cloud-based\
    \ training.A few complexities pose challenges to scale these models and algorithms\
    \ effectively. Compared to other areas where deep learning is applied, these NLP\
    \ models contain a variety of moving parts: text normalization and tokenization,\
    \ word representation at subword-level and word-level, variable-length models\
    \ such as RNN and attention, and sequential decoder based on beam search, among\
    \ others.In this hands-on tutorial, we take a closer look at the challenges from\
    \ these complexities and see how with proper tooling with Apache MXNet and GluonNLP,\
    \ we can overcome these challenges and achieve state-of-the-art results for real-world\
    \ problems. GluonNLP is a powerful new toolkit that combines MXNet\u2019s speed,\
    \ the flexibility of Gluon, and an extensive new library automating the most laborious\
    \ aspects of deep learning for NLP."
  address: Hong Kong, China
  author:
  - first: Haibin
    full: Haibin Lin
    id: haibin-lin
    last: Lin
  - first: Xingjian
    full: Xingjian Shi
    id: xingjian-shi
    last: Shi
  - first: Leonard
    full: Leonard Lausen
    id: leonard-lausen
    last: Lausen
  - first: Aston
    full: Aston Zhang
    id: aston-zhang
    last: Zhang
  - first: He
    full: He He
    id: he-he
    last: He
  - first: Sheng
    full: Sheng Zha
    id: sheng-zha
    last: Zha
  - first: Alexander
    full: Alexander Smola
    id: alexander-smola
    last: Smola
  author_string: Haibin Lin, Xingjian Shi, Leonard Lausen, Aston Zhang, He He, Sheng
    Zha, Alexander Smola
  bibkey: lin-etal-2019-dive
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): Tutorial Abstracts'
  month: November
  paper_id: '1'
  parent_volume_id: D19-2
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-2001.jpg
  title: Dive into Deep Learning for Natural Language Processing
  title_html: Dive into Deep Learning for Natural Language Processing
  year: '2019'
D19-2002:
  abstract: Multilingual communities exhibit code-mixing, that is, mixing of two or
    more socially stable languages in a single conversation, sometimes even in a single
    utterance. This phenomenon has been widely studied by linguists and interaction
    scientists in the spoken language of such communities. However, with the prevalence
    of social media and other informal interactive platforms, code-switching is now
    also ubiquitously observed in user-generated text. As multilingual communities
    are more the norm from a global perspective, it becomes essential that code-switched
    text and speech are adequately handled by language technologies and NUIs.Code-mixing
    is extremely prevalent in all multilingual societies. Current studies have shown
    that as much as 20% of user generated content from some geographies, like South
    Asia, parts of Europe, and Singapore, are code-mixed. Thus, it is very important
    to handle code-mixed content as a part of NLP systems and applications for these
    geographies.In the past 5 years, there has been an active interest in computational
    models for code-mixing with a substantive research outcome in terms of publications,
    datasets and systems. However, it is not easy to find a single point of access
    for a complete and coherent overview of the research. This tutorial is expecting
    to fill this gap and provide new researchers in the area with a foundation in
    both linguistic and computational aspects of code-mixing. We hope that this then
    becomes a starting point for those who wish to pursue research, design, development
    and deployment of code-mixed systems in multilingual societies.
  address: Hong Kong, China
  author:
  - first: Monojit
    full: Monojit Choudhury
    id: monojit-choudhury
    last: Choudhury
  - first: Anirudh
    full: Anirudh Srinivasan
    id: anirudh-srinivasan
    last: Srinivasan
  - first: Sandipan
    full: Sandipan Dandapat
    id: sandipan-dandapat
    last: Dandapat
  author_string: Monojit Choudhury, Anirudh Srinivasan, Sandipan Dandapat
  bibkey: choudhury-etal-2019-processing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): Tutorial Abstracts'
  month: November
  paper_id: '2'
  parent_volume_id: D19-2
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-2002.jpg
  title: Processing and Understanding Mixed Language Data
  title_html: Processing and Understanding Mixed Language Data
  year: '2019'
D19-2003:
  abstract: 'A fundamental long-term goal of conversational AI is to merge two main
    dialogue system paradigms into a standalone multi-purpose system. Such a system
    should be capable of conversing about arbitrary topics (Paradigm 1: open-domain
    dialogue systems), and simultaneously assist humans with completing a wide range
    of tasks with well-defined semantics such as restaurant search and booking, customer
    service applications, or ticket bookings (Paradigm 2: task-based dialogue systems).The
    recent developmental leaps in conversational AI technology are undoubtedly linked
    to more and more sophisticated deep learning algorithms that capture patterns
    in increasing amounts of data generated by various data collection mechanisms.
    The goal of this tutorial is therefore twofold. First, it aims at familiarising
    the research community with the recent advances in algorithmic design of statistical
    dialogue systems for both open-domain and task-based dialogue paradigms. The focus
    of the tutorial is on recently introduced end-to-end learning for dialogue systems
    and their relation to more common modular systems. In theory, learning end-to-end
    from data offers seamless and unprecedented portability of dialogue systems to
    a wide spectrum of tasks and languages. From a practical point of view, there
    are still plenty of research challenges and opportunities remaining: in this tutorial
    we analyse this gap between theory and practice, and introduce the research community
    with the main advantages as well as with key practical limitations of current
    end-to-end dialogue learning.The critical requirement of each statistical dialogue
    system is the data at hand. The system cannot provide assistance for the task
    without having appropriate task-related data to learn from. Therefore, the second
    major goal of this tutorial is to provide a comprehensive overview of the current
    approaches to data collection for dialogue, and analyse the current gaps and challenges
    with diverse data collection protocols, as well as their relation to and current
    limitations of data-driven end-to-end dialogue modeling. We will again analyse
    this relation and limitations both from research and industry perspective, and
    provide key insights on the application of state-of-the-art methodology into industry-scale
    conversational AI systems.'
  address: Hong Kong, China
  author:
  - first: Tsung-Hsien
    full: Tsung-Hsien Wen
    id: tsung-hsien-wen
    last: Wen
  - first: Pei-Hao
    full: Pei-Hao Su
    id: pei-hao-su
    last: Su
  - first: "Pawe\u0142"
    full: "Pawe\u0142 Budzianowski"
    id: pawel-budzianowski
    last: Budzianowski
  - first: "I\xF1igo"
    full: "I\xF1igo Casanueva"
    id: inigo-casanueva
    last: Casanueva
  - first: Ivan
    full: "Ivan Vuli\u0107"
    id: ivan-vulic
    last: "Vuli\u0107"
  author_string: "Tsung-Hsien Wen, Pei-Hao Su, Pawe\u0142 Budzianowski, I\xF1igo Casanueva,\
    \ Ivan Vuli\u0107"
  bibkey: wen-etal-2019-data
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): Tutorial Abstracts'
  month: November
  paper_id: '3'
  parent_volume_id: D19-2
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-2003.jpg
  title: Data Collection and End-to-End Learning for Conversational AI
  title_html: Data Collection and End-to-End Learning for Conversational AI
  year: '2019'
D19-2004:
  abstract: Recent advances in data-driven machine learning techniques (e.g., deep
    neural networks) have revolutionized many natural language processing applications.
    These approaches automatically learn how to make decisions based on the statistics
    and diagnostic information from large amounts of training data. Despite the remarkable
    accuracy of machine learning in various applications, learning algorithms run
    the risk of relying on societal biases encoded in the training data to make predictions.
    This often occurs even when gender and ethnicity information is not explicitly
    provided to the system because learning algorithms are able to discover implicit
    associations between individuals and their demographic information based on other
    variables such as names, titles, home addresses, etc. Therefore, machine learning
    algorithms risk potentially encouraging unfair and discriminatory decision making
    and raise serious privacy concerns. Without properly quantifying and reducing
    the reliance on such correlations, broad adoption of these models might have the
    undesirable effect of magnifying harmful stereotypes or implicit biases that rely
    on sensitive demographic attributes.In this tutorial, we will review the history
    of bias and fairness studies in machine learning and language processing and present
    recent community effort in quantifying and mitigating bias in natural language
    processing models for a wide spectrum of tasks, including word embeddings, co-reference
    resolution, machine translation, and vision-and-language tasks. In particular,
    we will focus on the following topics:+ Definitions of fairness and bias.+ Data,
    algorithms, and models that propagate and even amplify social bias to NLP applications
    and metrics to quantify these biases.+ Algorithmic solutions; learning objective;
    design principles to prevent social bias in NLP systems and their potential drawbacks.The
    tutorial will bring researchers and practitioners to be aware of this issue, and
    encourage the research community to propose innovative solutions to promote fairness
    in NLP.
  address: Hong Kong, China
  author:
  - first: Kai-Wei
    full: Kai-Wei Chang
    id: kai-wei-chang
    last: Chang
  - first: Vinod
    full: Vinod Prabhakaran
    id: vinod-prabhakaran
    last: Prabhakaran
  - first: Vicente
    full: Vicente Ordonez
    id: vicente-ordonez
    last: Ordonez
  author_string: Kai-Wei Chang, Vinod Prabhakaran, Vicente Ordonez
  bibkey: chang-etal-2019-bias
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): Tutorial Abstracts'
  month: November
  paper_id: '4'
  parent_volume_id: D19-2
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-2004.jpg
  title: Bias and Fairness in Natural Language Processing
  title_html: Bias and Fairness in Natural Language Processing
  year: '2019'
D19-2005:
  abstract: "This tutorial provides a comprehensive guide to the process of discreteness\
    \ in neural NLP.As a gentle start, we will briefly introduce the background of\
    \ deep learning based NLP, where we point out the ubiquitous discreteness of natural\
    \ language and its challenges in neural information processing. Particularly,\
    \ we will focus on how such discreteness plays a role in the input space, the\
    \ latent space, and the output space of a neural network. In each part, we will\
    \ provide examples, discuss machine learning techniques, as well as demonstrate\
    \ NLP applications.\"Graph-based Deep Learning in Natural Language Processing\t\
    Shikhar Vashishth, Naganand Yadati and Partha Talukdar\tThis tutorial aims to\
    \ introduce recent advances in graph-based deep learning techniques such as Graph\
    \ Convolutional Networks (GCNs) for Natural Language Processing (NLP). It provides\
    \ a brief introduction to deep learning methods on non-Euclidean domains such\
    \ as graphs and justifies their relevance in NLP. It then covers recent advances\
    \ in applying graph-based deep learning methods for various NLP tasks, such as\
    \ semantic role labeling, machine translation, relationship extraction, and many\
    \ more.Semantic Specialization of Distributional Word Vectors\tGoran Glava\u015B\
    , Edoardo Maria Ponti and Ivan Vuli\u0107\t\"Distributional word vectors have\
    \ become an indispensable component of most state-of-art NLP models. As a major\
    \ artefact of the underlying distributional hypothesis, distributional word vector\
    \ spaces conflate various paradigmatic and syntagmatic lexico-semantic relations.\
    \ For example, relations such as synonymy/similarity (e.g., car-automobile) or\
    \ lexical entailment (e.g., car-vehicle) often cannot be distinguished from antonymy\
    \ (e.g., black-white), meronymy (e.g., car-wheel) or broader thematic relatedness\
    \ (e.g., car-driver) based on the distances in the distributional vector space.\
    \ This inherent property of distributional spaces often harms performance in downstream\
    \ applications, since different lexico-semantic relations support different classes\
    \ of NLP applications. For instance, Semantic Similarity provides guidance for\
    \ Paraphrasing, Dialogue State Tracking, and Text Simplification, Lexical Entailment\
    \ supports Natural Language Inference and Taxonomy Induction, whereas broader\
    \ thematic relatedness yields gains for Named Entity Recognition, Parsing, and\
    \ Text Classification and Retrieval.A plethora of methods have been proposed to\
    \ emphasize specific lexico-semantic relations in a reshaped (i.e., specialized)\
    \ vector space. A common solution is to move beyond purely unsupervised word representation\
    \ learning and include external lexico-semantic knowledge, in a process commonly\
    \ referred to as semantic specialization. In this tutorial, we provide a thorough\
    \ overview of specialization methods, covering: 1) joint specialization methods,\
    \ which augment distributional learning objectives with external linguistic constraints,\
    \ 2) post-processing retrofitting models, which fine-tune pre-trained distributional\
    \ vectors to better reflect external linguistic constraints, and 3) the most recently\
    \ proposed post-specialization methods that generalize the perturbations of the\
    \ post-processing methods to the whole distributional space. In addition to providing\
    \ a comprehensive overview of specialization methods, we will introduce the most\
    \ recent developments, such as (among others): handling asymmetric relations (e.g.,\
    \ hypernymy-hyponymy) in Euclidean and hyperbolic spaces by accounting for vector\
    \ magnitude as well as for vector distance; cross-lingual transfer of semantic\
    \ specialization for languages without external lexico-semantic resources; downstream\
    \ effects of specializing distributional vector spaces; injecting external knowledge\
    \ into unsupervised pretraining architectures such as ELMo or BERT."
  address: Hong Kong, China
  author:
  - first: Lili
    full: Lili Mou
    id: lili-mou
    last: Mou
  - first: Hao
    full: Hao Zhou
    id: hao-zhou
    last: Zhou
  - first: Lei
    full: Lei Li
    id: lei-li
    last: Li
  author_string: Lili Mou, Hao Zhou, Lei Li
  bibkey: mou-etal-2019-discreteness
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): Tutorial Abstracts'
  month: November
  paper_id: '5'
  parent_volume_id: D19-2
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-2005.jpg
  title: Discreteness in Neural Natural Language Processing
  title_html: Discreteness in Neural Natural Language Processing
  year: '2019'
D19-2006:
  abstract: This tutorial aims to introduce recent advances in graph-based deep learning
    techniques such as Graph Convolutional Networks (GCNs) for Natural Language Processing
    (NLP). It provides a brief introduction to deep learning methods on non-Euclidean
    domains such as graphs and justifies their relevance in NLP. It then covers recent
    advances in applying graph-based deep learning methods for various NLP tasks,
    such as semantic role labeling, machine translation, relationship extraction,
    and many more.
  address: Hong Kong, China
  author:
  - first: Shikhar
    full: Shikhar Vashishth
    id: shikhar-vashishth
    last: Vashishth
  - first: Naganand
    full: Naganand Yadati
    id: naganand-yadati
    last: Yadati
  - first: Partha
    full: Partha Talukdar
    id: partha-talukdar
    last: Talukdar
  author_string: Shikhar Vashishth, Naganand Yadati, Partha Talukdar
  bibkey: vashishth-etal-2019-graph
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): Tutorial Abstracts'
  month: November
  paper_id: '6'
  parent_volume_id: D19-2
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-2006.jpg
  title: Graph-based Deep Learning in Natural Language Processing
  title_html: Graph-based Deep Learning in Natural Language Processing
  year: '2019'
D19-2007:
  abstract: 'Distributional word vectors have become an indispensable component of
    most state-of-art NLP models. As a major artefact of the underlying distributional
    hypothesis, distributional word vector spaces conflate various paradigmatic and
    syntagmatic lexico-semantic relations. For example, relations such as synonymy/similarity
    (e.g., car-automobile) or lexical entailment (e.g., car-vehicle) often cannot
    be distinguished from antonymy (e.g., black-white), meronymy (e.g., car-wheel)
    or broader thematic relatedness (e.g., car-driver) based on the distances in the
    distributional vector space. This inherent property of distributional spaces often
    harms performance in downstream applications, since different lexico-semantic
    relations support different classes of NLP applications. For instance, Semantic
    Similarity provides guidance for Paraphrasing, Dialogue State Tracking, and Text
    Simplification, Lexical Entailment supports Natural Language Inference and Taxonomy
    Induction, whereas broader thematic relatedness yields gains for Named Entity
    Recognition, Parsing, and Text Classification and Retrieval.A plethora of methods
    have been proposed to emphasize specific lexico-semantic relations in a reshaped
    (i.e., specialized) vector space. A common solution is to move beyond purely unsupervised
    word representation learning and include external lexico-semantic knowledge, in
    a process commonly referred to as semantic specialization. In this tutorial, we
    provide a thorough overview of specialization methods, covering: 1) joint specialization
    methods, which augment distributional learning objectives with external linguistic
    constraints, 2) post-processing retrofitting models, which fine-tune pre-trained
    distributional vectors to better reflect external linguistic constraints, and
    3) the most recently proposed post-specialization methods that generalize the
    perturbations of the post-processing methods to the whole distributional space.
    In addition to providing a comprehensive overview of specialization methods, we
    will introduce the most recent developments, such as (among others): handling
    asymmetric relations (e.g., hypernymy-hyponymy) in Euclidean and hyperbolic spaces
    by accounting for vector magnitude as well as for vector distance; cross-lingual
    transfer of semantic specialization for languages without external lexico-semantic
    resources; downstream effects of specializing distributional vector spaces; injecting
    external knowledge into unsupervised pretraining architectures such as ELMo or
    BERT.'
  address: Hong Kong, China
  author:
  - first: Goran
    full: "Goran Glava\u015B"
    id: goran-glavas1
    last: "Glava\u015B"
  - first: Edoardo
    full: Edoardo Maria Ponti
    id: edoardo-maria-ponti1
    last: Maria Ponti
  - first: Ivan
    full: "Ivan Vuli\u0107"
    id: ivan-vulic
    last: "Vuli\u0107"
  author_string: "Goran Glava\u015B, Edoardo Maria Ponti, Ivan Vuli\u0107"
  bibkey: glavas-etal-2019-semantic
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): Tutorial Abstracts'
  month: November
  paper_id: '7'
  parent_volume_id: D19-2
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-2007.jpg
  title: Semantic Specialization of Distributional Word Vectors
  title_html: Semantic Specialization of Distributional Word Vectors
  year: '2019'
D19-3000:
  address: Hong Kong, China
  author:
  - first: Sebastian
    full: "Sebastian Pad\xF3"
    id: sebastian-pado
    last: "Pad\xF3"
  - first: Ruihong
    full: Ruihong Huang
    id: ruihong-huang
    last: Huang
  author_string: "Sebastian Pad\xF3, Ruihong Huang"
  bibkey: emnlp-2019-2019-empirical
  bibtype: proceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  month: November
  paper_id: '0'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3000.jpg
  title: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  title_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  url: https://www.aclweb.org/anthology/D19-3000
  year: '2019'
D19-3001:
  abstract: 'We present ABSApp, a portable system for weakly-supervised aspect-based
    sentiment ex- traction. The system is interpretable and user friendly and does
    not require labeled training data, hence can be rapidly and cost-effectively used
    across different domains in applied setups. The system flow includes three stages:
    First, it generates domain-specific aspect and opinion lexicons based on an unlabeled
    dataset; second, it enables the user to view and edit those lexicons (weak supervision);
    and finally, it enables the user to select an unlabeled target dataset from the
    same domain, classify it, and generate an aspect-based sentiment report. ABSApp
    has been successfully used in a number of real-life use cases, among them movie
    review analysis and convention impact analysis.'
  address: Hong Kong, China
  author:
  - first: Oren
    full: Oren Pereg
    id: oren-pereg
    last: Pereg
  - first: Daniel
    full: Daniel Korat
    id: daniel-korat
    last: Korat
  - first: Moshe
    full: Moshe Wasserblat
    id: moshe-wasserblat
    last: Wasserblat
  - first: Jonathan
    full: Jonathan Mamou
    id: jonathan-mamou
    last: Mamou
  - first: Ido
    full: Ido Dagan
    id: ido-dagan
    last: Dagan
  author_string: Oren Pereg, Daniel Korat, Moshe Wasserblat, Jonathan Mamou, Ido Dagan
  bibkey: pereg-etal-2019-absapp
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3001
  month: November
  page_first: '1'
  page_last: '6'
  pages: "1\u20136"
  paper_id: '1'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3001.jpg
  title: 'ABSApp: A Portable Weakly-Supervised Aspect-Based Sentiment Extraction System'
  title_html: '<span class="acl-fixed-case">ABSA</span>pp: A Portable Weakly-Supervised
    Aspect-Based Sentiment Extraction System'
  url: https://www.aclweb.org/anthology/D19-3001
  year: '2019'
D19-3002:
  abstract: "Neural NLP models are increasingly accurate but are imperfect and opaque\u2014\
    they break in counterintuitive ways and leave end users puzzled at their behavior.\
    \ Model interpretation methods ameliorate this opacity by providing explanations\
    \ for specific model predictions. Unfortunately, existing interpretation codebases\
    \ make it difficult to apply these methods to new models and tasks, which hinders\
    \ adoption for practitioners and burdens interpretability researchers. We introduce\
    \ AllenNLP Interpret, a flexible framework for interpreting NLP models. The toolkit\
    \ provides interpretation primitives (e.g., input gradients) for any AllenNLP\
    \ model and task, a suite of built-in interpretation methods, and a library of\
    \ front-end visualization components. We demonstrate the toolkit\u2019s flexibility\
    \ and utility by implementing live demos for five interpretation methods (e.g.,\
    \ saliency maps and adversarial attacks) on a variety of models and tasks (e.g.,\
    \ masked language modeling using BERT and reading comprehension using BiDAF).\
    \ These demos, alongside our code and tutorials, are available at https://allennlp.org/interpret."
  address: Hong Kong, China
  author:
  - first: Eric
    full: Eric Wallace
    id: eric-wallace
    last: Wallace
  - first: Jens
    full: Jens Tuyls
    id: jens-tuyls
    last: Tuyls
  - first: Junlin
    full: Junlin Wang
    id: junlin-wang
    last: Wang
  - first: Sanjay
    full: Sanjay Subramanian
    id: sanjay-subramanian
    last: Subramanian
  - first: Matt
    full: Matt Gardner
    id: matt-gardner
    last: Gardner
  - first: Sameer
    full: Sameer Singh
    id: sameer-singh
    last: Singh
  author_string: Eric Wallace, Jens Tuyls, Junlin Wang, Sanjay Subramanian, Matt Gardner,
    Sameer Singh
  bibkey: wallace-etal-2019-allennlp
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3002
  month: November
  page_first: '7'
  page_last: '12'
  pages: "7\u201312"
  paper_id: '2'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3002.jpg
  title: 'AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models'
  title_html: '<span class="acl-fixed-case">A</span>llen<span class="acl-fixed-case">NLP</span>
    Interpret: A Framework for Explaining Predictions of <span class="acl-fixed-case">NLP</span>
    Models'
  url: https://www.aclweb.org/anthology/D19-3002
  year: '2019'
D19-3003:
  abstract: In this paper, we describe ALTER, an auxiliary text rewriting tool that
    facilitates the rewriting process for natural language generation tasks, such
    as paraphrasing, text simplification, fairness-aware text rewriting, and text
    style transfer. Our tool is characterized by two features, i) recording of word-level
    revision histories and ii) flexible auxiliary edit support and feedback to annotators.
    The text rewriting assist and traceable rewriting history are potentially beneficial
    to the future research of natural language generation.
  address: Hong Kong, China
  author:
  - first: Qiongkai
    full: Qiongkai Xu
    id: qiongkai-xu
    last: Xu
  - first: Chenchen
    full: Chenchen Xu
    id: chenchen-xu
    last: Xu
  - first: Lizhen
    full: Lizhen Qu
    id: lizhen-qu
    last: Qu
  author_string: Qiongkai Xu, Chenchen Xu, Lizhen Qu
  bibkey: xu-etal-2019-alter
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3003
  month: November
  page_first: '13'
  page_last: '18'
  pages: "13\u201318"
  paper_id: '3'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3003.jpg
  title: 'ALTER: Auxiliary Text Rewriting Tool for Natural Language Generation'
  title_html: '<span class="acl-fixed-case">ALTER</span>: Auxiliary Text Rewriting
    Tool for Natural Language Generation'
  url: https://www.aclweb.org/anthology/D19-3003
  year: '2019'
D19-3004:
  abstract: We present Birch, a system that applies BERT to document retrieval via
    integration with the open-source Anserini information retrieval toolkit to demonstrate
    end-to-end search over large document collections. Birch implements simple ranking
    models that achieve state-of-the-art effectiveness on standard TREC newswire and
    social media test collections. This demonstration focuses on technical challenges
    in the integration of NLP and IR capabilities, along with the design rationale
    behind our approach to tightly-coupled integration between Python (to support
    neural networks) and the Java Virtual Machine (to support document retrieval using
    the open-source Lucene search library). We demonstrate integration of Birch with
    an existing search interface as well as interactive notebooks that highlight its
    capabilities in an easy-to-understand manner.
  address: Hong Kong, China
  author:
  - first: Zeynep
    full: Zeynep Akkalyoncu Yilmaz
    id: zeynep-akkalyoncu-yilmaz
    last: Akkalyoncu Yilmaz
  - first: Shengjin
    full: Shengjin Wang
    id: shengjin-wang
    last: Wang
  - first: Wei
    full: Wei Yang
    id: wei-yang
    last: Yang
  - first: Haotian
    full: Haotian Zhang
    id: haotian-zhang
    last: Zhang
  - first: Jimmy
    full: Jimmy Lin
    id: jimmy-lin
    last: Lin
  author_string: Zeynep Akkalyoncu Yilmaz, Shengjin Wang, Wei Yang, Haotian Zhang,
    Jimmy Lin
  bibkey: akkalyoncu-yilmaz-etal-2019-applying
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3004
  month: November
  page_first: '19'
  page_last: '24'
  pages: "19\u201324"
  paper_id: '4'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3004.jpg
  title: Applying BERT to Document Retrieval with Birch
  title_html: Applying <span class="acl-fixed-case">BERT</span> to Document Retrieval
    with Birch
  url: https://www.aclweb.org/anthology/D19-3004
  year: '2019'
D19-3005:
  abstract: The Knowledge Graph Induction Service (KGIS) is an end-to-end knowledge
    induction system. One of its main capabilities is to automatically induce taxonomies
    from input documents using a hybrid approach that takes advantage of linguistic
    patterns, semantic web and neural networks. KGIS allows the user to semi-automatically
    curate and expand the induced taxonomy through a component called Smart SpreadSheet
    by exploiting distributional semantics. In this paper, we describe these taxonomy
    induction and expansion features of KGIS. A screencast video demonstrating the
    system is available in https://ibm.box.com/v/emnlp-2019-demo .
  address: Hong Kong, China
  author:
  - first: Nicolas Rodolfo
    full: Nicolas Rodolfo Fauceglia
    id: nicolas-rodolfo-fauceglia
    last: Fauceglia
  - first: Alfio
    full: Alfio Gliozzo
    id: alfio-gliozzo
    last: Gliozzo
  - first: Sarthak
    full: Sarthak Dash
    id: sarthak-dash
    last: Dash
  - first: Md. Faisal Mahbub
    full: Md. Faisal Mahbub Chowdhury
    id: md-faisal-mahbub-chowdhury
    last: Chowdhury
  - first: Nandana
    full: Nandana Mihindukulasooriya
    id: nandana-mihindukulasooriya
    last: Mihindukulasooriya
  author_string: Nicolas Rodolfo Fauceglia, Alfio Gliozzo, Sarthak Dash, Md. Faisal
    Mahbub Chowdhury, Nandana Mihindukulasooriya
  bibkey: fauceglia-etal-2019-automatic
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3005
  month: November
  page_first: '25'
  page_last: '30'
  pages: "25\u201330"
  paper_id: '5'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3005.jpg
  title: Automatic Taxonomy Induction and Expansion
  title_html: Automatic Taxonomy Induction and Expansion
  url: https://www.aclweb.org/anthology/D19-3005
  year: '2019'
D19-3006:
  abstract: 'This paper introduces a novel orchestration framework, called CFO (Computation
    Flow Orchestrator), for building, experimenting with, and deploying interactive
    NLP (Natural Language Processing) and IR (Information Retrieval) systems to production
    environments. We then demonstrate a question answering system built using this
    framework which incorporates state-of-the-art BERT based MRC (Machine Reading
    Com- prehension) with IR components to enable end-to-end answer retrieval. Results
    from the demo system are shown to be high quality in both academic and industry
    domain specific settings. Finally, we discuss best practices when (pre-)training
    BERT based MRC models for production systems. Screencast links: - Short video
    (< 3 min): http: //ibm.biz/gaama_demo - Supplementary long video (< 13 min): http://ibm.biz/gaama_cfo_demo'
  address: Hong Kong, China
  author:
  - first: Rishav
    full: Rishav Chakravarti
    id: rishav-chakravarti
    last: Chakravarti
  - first: Cezar
    full: Cezar Pendus
    id: cezar-pendus
    last: Pendus
  - first: Andrzej
    full: Andrzej Sakrajda
    id: andrzej-sakrajda
    last: Sakrajda
  - first: Anthony
    full: Anthony Ferritto
    id: anthony-ferritto
    last: Ferritto
  - first: Lin
    full: Lin Pan
    id: lin-pan
    last: Pan
  - first: Michael
    full: Michael Glass
    id: michael-glass
    last: Glass
  - first: Vittorio
    full: Vittorio Castelli
    id: vittorio-castelli
    last: Castelli
  - first: J William
    full: J William Murdock
    id: j-william-murdock
    last: Murdock
  - first: Radu
    full: Radu Florian
    id: radu-florian
    last: Florian
  - first: Salim
    full: Salim Roukos
    id: salim-roukos
    last: Roukos
  - first: Avi
    full: Avi Sil
    id: avirup-sil
    last: Sil
  author_string: Rishav Chakravarti, Cezar Pendus, Andrzej Sakrajda, Anthony Ferritto,
    Lin Pan, Michael Glass, Vittorio Castelli, J William Murdock, Radu Florian, Salim
    Roukos, Avi Sil
  bibkey: chakravarti-etal-2019-cfo
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3006
  month: November
  page_first: '31'
  page_last: '36'
  pages: "31\u201336"
  paper_id: '6'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3006.jpg
  title: 'CFO: A Framework for Building Production NLP Systems'
  title_html: '<span class="acl-fixed-case">CFO</span>: A Framework for Building Production
    <span class="acl-fixed-case">NLP</span> Systems'
  url: https://www.aclweb.org/anthology/D19-3006
  year: '2019'
D19-3007:
  abstract: "Language model is a vital component in modern automatic speech recognition\
    \ (ASR) systems. Since \u201Cone-size-fits-all\u201D language model works suboptimally\
    \ for conversational speeches, language model adaptation (LMA) is considered as\
    \ a promising solution for solving this problem. In order to compare the state-of-the-art\
    \ LMA techniques and systematically demonstrate their effect in conversational\
    \ speech recognition, we develop a novel toolkit named Chameleon, which includes\
    \ the state-of-the-art cache-based and topic-based LMA techniques. This demonstration\
    \ does not only vividly visualize underlying working mechanisms of a variety of\
    \ the state-of-the-art LMA models but also provide an interface for the user to\
    \ customize the hyperparameters of them. With this demonstration, the audience\
    \ can experience the effect of LMA in an interactive and real-time fashion. We\
    \ wish this demonstration would inspire more research on better language model\
    \ techniques for ASR."
  address: Hong Kong, China
  author:
  - first: Yuanfeng
    full: Yuanfeng Song
    id: yuanfeng-song
    last: Song
  - first: Di
    full: Di Jiang
    id: di-jiang
    last: Jiang
  - first: Weiwei
    full: Weiwei Zhao
    id: weiwei-zhao
    last: Zhao
  - first: Qian
    full: Qian Xu
    id: qian-xu
    last: Xu
  - first: Raymond Chi-Wing
    full: Raymond Chi-Wing Wong
    id: raymond-chi-wing-wong
    last: Wong
  - first: Qiang
    full: Qiang Yang
    id: qiang-yang
    last: Yang
  author_string: Yuanfeng Song, Di Jiang, Weiwei Zhao, Qian Xu, Raymond Chi-Wing Wong,
    Qiang Yang
  bibkey: song-etal-2019-chameleon
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3007
  month: November
  page_first: '37'
  page_last: '42'
  pages: "37\u201342"
  paper_id: '7'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3007.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3007.jpg
  title: 'Chameleon: A Language Model Adaptation Toolkit for Automatic Speech Recognition
    of Conversational Speech'
  title_html: '<span class="acl-fixed-case">C</span>hameleon: A Language Model Adaptation
    Toolkit for Automatic Speech Recognition of Conversational Speech'
  url: https://www.aclweb.org/anthology/D19-3007
  year: '2019'
D19-3008:
  abstract: An acrostic is a form of writing that the first token of each line (or
    other recurring features in the text) forms a meaningful sequence. In this paper
    we present a generalized acrostic generation system that can hide certain message
    in a flexible pattern specified by the users. Different from previous works that
    focus on rule-based solutions, here we adopt a neural- based sequence-to-sequence
    model to achieve this goal. Besides acrostic, users are also allowed to specify
    the rhyme and length of the output sequences. Based on our knowledge, this is
    the first neural-based natural language generation system that demonstrates the
    capability of performing micro-level control over output sentences.
  address: Hong Kong, China
  author:
  - first: Liang-Hsin
    full: Liang-Hsin Shen
    id: liang-hsin-shen
    last: Shen
  - first: Pei-Lun
    full: Pei-Lun Tai
    id: pei-lun-tai
    last: Tai
  - first: Chao-Chung
    full: Chao-Chung Wu
    id: chao-chung-wu
    last: Wu
  - first: Shou-De
    full: Shou-De Lin
    id: shou-de-lin
    last: Lin
  author_string: Liang-Hsin Shen, Pei-Lun Tai, Chao-Chung Wu, Shou-De Lin
  bibkey: shen-etal-2019-controlling
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3008
  month: November
  page_first: '43'
  page_last: '48'
  pages: "43\u201348"
  paper_id: '8'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3008.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3008.jpg
  title: Controlling Sequence-to-Sequence Models - A Demonstration on Neural-based
    Acrostic Generator
  title_html: Controlling Sequence-to-Sequence Models - A Demonstration on Neural-based
    Acrostic Generator
  url: https://www.aclweb.org/anthology/D19-3008
  year: '2019'
D19-3009:
  abstract: 'We introduce EASSE, a Python package aiming to facilitate and standardise
    automatic evaluation and comparison of Sentence Simplification (SS) systems. EASSE
    provides a single access point to a broad range of evaluation resources: standard
    automatic metrics for assessing SS outputs (e.g. SARI), word-level accuracy scores
    for certain simplification transformations, reference-independent quality estimation
    features (e.g. compression ratio), and standard test data for SS evaluation (e.g.
    TurkCorpus). Finally, EASSE generates easy-to-visualise reports on the various
    metrics and features above and on how a particular SS output fares against reference
    simplifications. Through experiments, we show that these functionalities allow
    for better comparison and understanding of the performance of SS systems.'
  address: Hong Kong, China
  author:
  - first: Fernando
    full: Fernando Alva-Manchego
    id: fernando-alva-manchego
    last: Alva-Manchego
  - first: Louis
    full: Louis Martin
    id: louis-martin
    last: Martin
  - first: Carolina
    full: Carolina Scarton
    id: carolina-scarton
    last: Scarton
  - first: Lucia
    full: Lucia Specia
    id: lucia-specia
    last: Specia
  author_string: Fernando Alva-Manchego, Louis Martin, Carolina Scarton, Lucia Specia
  bibkey: alva-manchego-etal-2019-easse
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3009
  month: November
  page_first: '49'
  page_last: '54'
  pages: "49\u201354"
  paper_id: '9'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3009.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3009.jpg
  title: 'EASSE: Easier Automatic Sentence Simplification Evaluation'
  title_html: '<span class="acl-fixed-case">EASSE</span>: Easier Automatic Sentence
    Simplification Evaluation'
  url: https://www.aclweb.org/anthology/D19-3009
  year: '2019'
D19-3010:
  abstract: "There is renewed interest in simulating language emergence among deep\
    \ neural agents that communicate to jointly solve a task, spurred by the practical\
    \ aim to develop language-enabled interactive AIs, as well as by theoretical questions\
    \ about the evolution of human language. However, optimizing deep architectures\
    \ connected by a discrete communication channel (such as that in which language\
    \ emerges) is technically challenging. We introduce EGG, a toolkit that greatly\
    \ simplifies the implementation of emergent-language communication games. EGG\u2019\
    s modular design provides a set of building blocks that the user can combine to\
    \ create new games, easily navigating the optimization and architecture space.\
    \ We hope that the tool will lower the technical barrier, and encourage researchers\
    \ from various backgrounds to do original work in this exciting area."
  address: Hong Kong, China
  author:
  - first: Eugene
    full: Eugene Kharitonov
    id: eugene-kharitonov
    last: Kharitonov
  - first: Rahma
    full: Rahma Chaabouni
    id: rahma-chaabouni
    last: Chaabouni
  - first: Diane
    full: Diane Bouchacourt
    id: diane-bouchacourt
    last: Bouchacourt
  - first: Marco
    full: Marco Baroni
    id: marco-baroni
    last: Baroni
  author_string: Eugene Kharitonov, Rahma Chaabouni, Diane Bouchacourt, Marco Baroni
  bibkey: kharitonov-etal-2019-egg
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3010
  month: November
  page_first: '55'
  page_last: '60'
  pages: "55\u201360"
  paper_id: '10'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3010.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3010.jpg
  title: 'EGG: a toolkit for research on Emergence of lanGuage in Games'
  title_html: '<span class="acl-fixed-case">EGG</span>: a toolkit for research on
    Emergence of lan<span class="acl-fixed-case">G</span>uage in Games'
  url: https://www.aclweb.org/anthology/D19-3010
  year: '2019'
D19-3011:
  abstract: Large vocabulary domain-agnostic Automatic Speech Recognition (ASR) systems
    often mistranscribe domain-specific words and phrases. Since these generic ASR
    systems are the first component of most voice assistants in production, building
    Natural Language Understanding (NLU) systems that are robust to these errors can
    be a challenging task. In this paper, we focus on handling ASR errors in named
    entities, specifically person names, for a voice-based collaboration assistant.
    We demonstrate an effective method for resolving person names that are mistranscribed
    by black-box ASR systems, using character and phoneme-based information retrieval
    techniques and contextual information, which improves accuracy by 40.8% on our
    production system. We provide a live interactive demo to further illustrate the
    nuances of this problem and the effectiveness of our solution.
  address: Hong Kong, China
  author:
  - first: Arushi
    full: Arushi Raghuvanshi
    id: arushi-raghuvanshi
    last: Raghuvanshi
  - first: Vijay
    full: Vijay Ramakrishnan
    id: vijay-ramakrishnan
    last: Ramakrishnan
  - first: Varsha
    full: Varsha Embar
    id: varsha-embar
    last: Embar
  - first: Lucien
    full: Lucien Carroll
    id: lucien-carroll
    last: Carroll
  - first: Karthik
    full: Karthik Raghunathan
    id: karthik-raghunathan
    last: Raghunathan
  author_string: Arushi Raghuvanshi, Vijay Ramakrishnan, Varsha Embar, Lucien Carroll,
    Karthik Raghunathan
  bibkey: raghuvanshi-etal-2019-entity
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3011
  month: November
  page_first: '61'
  page_last: '66'
  pages: "61\u201366"
  paper_id: '11'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3011.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3011.jpg
  title: Entity resolution for noisy ASR transcripts
  title_html: Entity resolution for noisy <span class="acl-fixed-case">ASR</span>
    transcripts
  url: https://www.aclweb.org/anthology/D19-3011
  year: '2019'
D19-3012:
  abstract: Semantic parsing aims to map natural language utterances into structured
    meaning representations. We present a modular platform, EUSP (Easy-to-Use Semantic
    Parsing PlatForm), that facilitates developers to build semantic parser from scratch.
    Instead of requiring a large amount of training data or complex grammar knowledge,
    in our platform developers can build grammar-based semantic parser or neural-based
    semantic parser through configure files which specify the modules and components
    that compose semantic parsing system. A high quality grammar-based semantic parsing
    system only requires domain lexicons rather than costly training data for a semantic
    parser. Furthermore, we provide a browser-based method to generate the semantic
    parsing system to minimize the difficulty of development. Experimental results
    show that the neural-based semantic parser system achieves competitive performance
    on semantic parsing task, and grammar-based semantic parsers significantly improve
    the performance of a business search engine.
  address: Hong Kong, China
  author:
  - first: Bo
    full: Bo An
    id: bo-an
    last: An
  - first: Chen
    full: Chen Bo
    id: chen-bo
    last: Bo
  - first: Xianpei
    full: Xianpei Han
    id: xianpei-han
    last: Han
  - first: Le
    full: Le Sun
    id: le-sun
    last: Sun
  author_string: Bo An, Chen Bo, Xianpei Han, Le Sun
  bibkey: an-etal-2019-eusp
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3012
  month: November
  page_first: '67'
  page_last: '72'
  pages: "67\u201372"
  paper_id: '12'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3012.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3012.jpg
  title: 'EUSP: An Easy-to-Use Semantic Parsing PlatForm'
  title_html: '<span class="acl-fixed-case">EUSP</span>: An Easy-to-Use Semantic Parsing
    <span class="acl-fixed-case">P</span>lat<span class="acl-fixed-case">F</span>orm'
  url: https://www.aclweb.org/anthology/D19-3012
  year: '2019'
D19-3013:
  abstract: "Our proposed system FAMULUS helps students learn to diagnose based on\
    \ automatic feedback in virtual patient simulations, and it supports instructors\
    \ in labeling training data. Diagnosing is an exceptionally difficult skill to\
    \ obtain but vital for many different professions (e.g., medical doctors, teachers).\
    \ Previous case simulation systems are limited to multiple-choice questions and\
    \ thus cannot give constructive individualized feedback on a student\u2019s diagnostic\
    \ reasoning process. Given initially only limited data, we leverage a (replaceable)\
    \ NLP model to both support experts in their further data annotation with automatic\
    \ suggestions, and we provide automatic feedback for students. We argue that because\
    \ the central model consistently improves, our interactive approach encourages\
    \ both students and instructors to recurrently use the tool, and thus accelerate\
    \ the speed of data creation and annotation. We show results from two user studies\
    \ on diagnostic reasoning in medicine and teacher education and outline how our\
    \ system can be extended to further use cases."
  address: Hong Kong, China
  author:
  - first: Jonas
    full: Jonas Pfeiffer
    id: jonas-pfeiffer
    last: Pfeiffer
  - first: Christian M.
    full: Christian M. Meyer
    id: christian-m-meyer
    last: Meyer
  - first: Claudia
    full: Claudia Schulz
    id: claudia-schulz
    last: Schulz
  - first: Jan
    full: Jan Kiesewetter
    id: jan-kiesewetter
    last: Kiesewetter
  - first: Jan
    full: Jan Zottmann
    id: jan-zottmann
    last: Zottmann
  - first: Michael
    full: Michael Sailer
    id: michael-sailer
    last: Sailer
  - first: Elisabeth
    full: Elisabeth Bauer
    id: elisabeth-bauer
    last: Bauer
  - first: Frank
    full: Frank Fischer
    id: frank-fischer
    last: Fischer
  - first: Martin R.
    full: Martin R. Fischer
    id: martin-r-fischer
    last: Fischer
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Jonas Pfeiffer, Christian M. Meyer, Claudia Schulz, Jan Kiesewetter,
    Jan Zottmann, Michael Sailer, Elisabeth Bauer, Frank Fischer, Martin R. Fischer,
    Iryna Gurevych
  bibkey: pfeiffer-etal-2019-famulus
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3013
  month: November
  page_first: '73'
  page_last: '78'
  pages: "73\u201378"
  paper_id: '13'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3013.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3013.jpg
  title: 'FAMULUS: Interactive Annotation and Feedback Generation for Teaching Diagnostic
    Reasoning'
  title_html: '<span class="acl-fixed-case">FAMULUS</span>: Interactive Annotation
    and Feedback Generation for Teaching Diagnostic Reasoning'
  url: https://www.aclweb.org/anthology/D19-3013
  year: '2019'
D19-3014:
  abstract: "Gunrock is the winner of the 2018 Amazon Alexa Prize, as evaluated by\
    \ coherence and engagement from both real users and Amazon-selected expert conversationalists.\
    \ We focus on understanding complex sentences and having in-depth conversations\
    \ in open domains. In this paper, we introduce some innovative system designs\
    \ and related validation analysis. Overall, we found that users produce longer\
    \ sentences to Gunrock, which are directly related to users\u2019 engagement (e.g.,\
    \ ratings, number of turns). Additionally, users\u2019 backstory queries about\
    \ Gunrock are positively correlated to user satisfaction. Finally, we found dialog\
    \ flows that interleave facts and personal opinions and stories lead to better\
    \ user satisfaction."
  address: Hong Kong, China
  author:
  - first: Dian
    full: Dian Yu
    id: dian-yu
    last: Yu
  - first: Michelle
    full: Michelle Cohn
    id: michelle-cohn
    last: Cohn
  - first: Yi Mang
    full: Yi Mang Yang
    id: yi-mang-yang
    last: Yang
  - first: Chun Yen
    full: Chun Yen Chen
    id: chun-yen-chen
    last: Chen
  - first: Weiming
    full: Weiming Wen
    id: weiming-wen
    last: Wen
  - first: Jiaping
    full: Jiaping Zhang
    id: jiaping-zhang
    last: Zhang
  - first: Mingyang
    full: Mingyang Zhou
    id: mingyang-zhou
    last: Zhou
  - first: Kevin
    full: Kevin Jesse
    id: kevin-jesse
    last: Jesse
  - first: Austin
    full: Austin Chau
    id: austin-chau
    last: Chau
  - first: Antara
    full: Antara Bhowmick
    id: antara-bhowmick
    last: Bhowmick
  - first: Shreenath
    full: Shreenath Iyer
    id: shreenath-iyer
    last: Iyer
  - first: Giritheja
    full: Giritheja Sreenivasulu
    id: giritheja-sreenivasulu
    last: Sreenivasulu
  - first: Sam
    full: Sam Davidson
    id: sam-davidson
    last: Davidson
  - first: Ashwin
    full: Ashwin Bhandare
    id: ashwin-bhandare
    last: Bhandare
  - first: Zhou
    full: Zhou Yu
    id: zhou-yu
    last: Yu
  author_string: Dian Yu, Michelle Cohn, Yi Mang Yang, Chun Yen Chen, Weiming Wen,
    Jiaping Zhang, Mingyang Zhou, Kevin Jesse, Austin Chau, Antara Bhowmick, Shreenath
    Iyer, Giritheja Sreenivasulu, Sam Davidson, Ashwin Bhandare, Zhou Yu
  bibkey: yu-etal-2019-gunrock
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3014
  month: November
  page_first: '79'
  page_last: '84'
  pages: "79\u201384"
  paper_id: '14'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3014.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3014.jpg
  title: 'Gunrock: A Social Bot for Complex and Engaging Long Conversations'
  title_html: '<span class="acl-fixed-case">G</span>unrock: A Social Bot for Complex
    and Engaging Long Conversations'
  url: https://www.aclweb.org/anthology/D19-3014
  year: '2019'
D19-3015:
  abstract: Exploration and analysis of potential data sources is a significant challenge
    in the application of NLP techniques to novel information domains. We describe
    HARE, a system for highlighting relevant information in document collections to
    support ranking and triage, which provides tools for post-processing and qualitative
    analysis for model development and tuning. We apply HARE to the use case of narrative
    descriptions of mobility information in clinical data, and demonstrate its utility
    in comparing candidate embedding features. We provide a web-based interface for
    annotation visualization and document ranking, with a modular backend to support
    interoperability with existing annotation tools. Our system is available online
    at https://github.com/OSU-slatelab/HARE.
  address: Hong Kong, China
  author:
  - first: Denis
    full: Denis Newman-Griffis
    id: denis-newman-griffis
    last: Newman-Griffis
  - first: Eric
    full: Eric Fosler-Lussier
    id: eric-fosler-lussier
    last: Fosler-Lussier
  author_string: Denis Newman-Griffis, Eric Fosler-Lussier
  bibkey: newman-griffis-fosler-lussier-2019-hare
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3015
  month: November
  page_first: '85'
  page_last: '90'
  pages: "85\u201390"
  paper_id: '15'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3015.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3015.jpg
  title: 'HARE: a Flexible Highlighting Annotator for Ranking and Exploration'
  title_html: '<span class="acl-fixed-case">HARE</span>: a Flexible Highlighting Annotator
    for Ranking and Exploration'
  url: https://www.aclweb.org/anthology/D19-3015
  year: '2019'
D19-3016:
  abstract: Used for simple commands recognition on devices from smart speakers to
    mobile phones, keyword spotting systems are everywhere. Ubiquitous as well are
    web applications, which have grown in popularity and complexity over the last
    decade. However, despite their obvious advantages in natural language interaction,
    voice-enabled web applications are still few and far between. We attempt to bridge
    this gap with Honkling, a novel, JavaScript-based keyword spotting system. Purely
    client-side and cross-device compatible, Honkling can be deployed directly on
    user devices. Our in-browser implementation enables seamless personalization,
    which can greatly improve model quality; in the presence of underrepresented,
    non-American user accents, we can achieve up to an absolute 10% increase in accuracy
    in the personalized model with only a few examples.
  address: Hong Kong, China
  author:
  - first: Jaejun
    full: Jaejun Lee
    id: jaejun-lee
    last: Lee
  - first: Raphael
    full: Raphael Tang
    id: raphael-tang
    last: Tang
  - first: Jimmy
    full: Jimmy Lin
    id: jimmy-lin
    last: Lin
  author_string: Jaejun Lee, Raphael Tang, Jimmy Lin
  bibkey: lee-etal-2019-honkling
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3016
  month: November
  page_first: '91'
  page_last: '96'
  pages: "91\u201396"
  paper_id: '16'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3016.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3016.jpg
  title: 'Honkling: In-Browser Personalization for Ubiquitous Keyword Spotting'
  title_html: '<span class="acl-fixed-case">H</span>onkling: In-Browser Personalization
    for Ubiquitous Keyword Spotting'
  url: https://www.aclweb.org/anthology/D19-3016
  year: '2019'
D19-3017:
  abstract: Legal Tech is developed to help people with legal services and solve legal
    problems via machines. To achieve this, one of the key requirements for machines
    is to utilize legal knowledge and comprehend legal context. This can be fulfilled
    by natural language processing (NLP) techniques, for instance, text representation,
    text categorization, question answering (QA) and natural language inference, etc.
    To this end, we introduce a freely available Chinese Legal Tech system (IFlyLegal)
    that benefits from multiple NLP tasks. It is an integrated system that performs
    legal consulting, multi-way law searching, and legal document analysis by exploiting
    techniques such as deep contextual representations and various attention mechanisms.
    To our knowledge, IFlyLegal is the first Chinese legal system that employs up-to-date
    NLP techniques and caters for needs of different user groups, such as lawyers,
    judges, procurators, and clients. Since Jan, 2019, we have gathered 2,349 users
    and 28,238 page views (till June, 23, 2019).
  address: Hong Kong, China
  author:
  - first: Ziyue
    full: Ziyue Wang
    id: ziyue-wang
    last: Wang
  - first: Baoxin
    full: Baoxin Wang
    id: baoxin-wang
    last: Wang
  - first: Xingyi
    full: Xingyi Duan
    id: xingyi-duan
    last: Duan
  - first: Dayong
    full: Dayong Wu
    id: dayong-wu
    last: Wu
  - first: Shijin
    full: Shijin Wang
    id: shijin-wang
    last: Wang
  - first: Guoping
    full: Guoping Hu
    id: guoping-hu
    last: Hu
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  author_string: Ziyue Wang, Baoxin Wang, Xingyi Duan, Dayong Wu, Shijin Wang, Guoping
    Hu, Ting Liu
  bibkey: wang-etal-2019-iflylegal
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3017
  month: November
  page_first: '97'
  page_last: '102'
  pages: "97\u2013102"
  paper_id: '17'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3017.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3017.jpg
  title: 'IFlyLegal: A Chinese Legal System for Consultation, Law Searching, and Document
    Analysis'
  title_html: '<span class="acl-fixed-case">IF</span>ly<span class="acl-fixed-case">L</span>egal:
    A <span class="acl-fixed-case">C</span>hinese Legal System for Consultation, Law
    Searching, and Document Analysis'
  url: https://www.aclweb.org/anthology/D19-3017
  year: '2019'
D19-3018:
  abstract: In this paper, we demonstrate an Interactive Machine Translation interface,
    that assists human translators with on-the-fly hints and suggestions. This makes
    the end-to-end translation process faster, more efficient and creates high-quality
    translations. We augment the OpenNMT backend with a mechanism to accept the user
    input and generate conditioned translations.
  address: Hong Kong, China
  author:
  - first: Sebastin
    full: Sebastin Santy
    id: sebastin-santy
    last: Santy
  - first: Sandipan
    full: Sandipan Dandapat
    id: sandipan-dandapat
    last: Dandapat
  - first: Monojit
    full: Monojit Choudhury
    id: monojit-choudhury
    last: Choudhury
  - first: Kalika
    full: Kalika Bali
    id: kalika-bali
    last: Bali
  author_string: Sebastin Santy, Sandipan Dandapat, Monojit Choudhury, Kalika Bali
  bibkey: santy-etal-2019-inmt
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3018
  month: November
  page_first: '103'
  page_last: '108'
  pages: "103\u2013108"
  paper_id: '18'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3018.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3018.jpg
  title: 'INMT: Interactive Neural Machine Translation Prediction'
  title_html: '<span class="acl-fixed-case">INMT</span>: Interactive Neural Machine
    Translation Prediction'
  url: https://www.aclweb.org/anthology/D19-3018
  year: '2019'
D19-3019:
  abstract: We present Joey NMT, a minimalist neural machine translation toolkit based
    on PyTorch that is specifically designed for novices. Joey NMT provides many popular
    NMT features in a small and simple code base, so that novices can easily and quickly
    learn to use it and adapt it to their needs. Despite its focus on simplicity,
    Joey NMT supports classic architectures (RNNs, transformers), fast beam search,
    weight tying, and more, and achieves performance comparable to more complex toolkits
    on standard benchmarks. We evaluate the accessibility of our toolkit in a user
    study where novices with general knowledge about Pytorch and NMT and experts work
    through a self-contained Joey NMT tutorial, showing that novices perform almost
    as well as experts in a subsequent code quiz. Joey NMT is available at https://github.com/joeynmt/joeynmt.
  address: Hong Kong, China
  author:
  - first: Julia
    full: Julia Kreutzer
    id: julia-kreutzer
    last: Kreutzer
  - first: Joost
    full: Joost Bastings
    id: joost-bastings
    last: Bastings
  - first: Stefan
    full: Stefan Riezler
    id: stefan-riezler
    last: Riezler
  author_string: Julia Kreutzer, Joost Bastings, Stefan Riezler
  bibkey: kreutzer-etal-2019-joey
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3019
  month: November
  page_first: '109'
  page_last: '114'
  pages: "109\u2013114"
  paper_id: '19'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3019.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3019.jpg
  title: 'Joey NMT: A Minimalist NMT Toolkit for Novices'
  title_html: 'Joey <span class="acl-fixed-case">NMT</span>: A Minimalist <span class="acl-fixed-case">NMT</span>
    Toolkit for Novices'
  url: https://www.aclweb.org/anthology/D19-3019
  year: '2019'
D19-3020:
  abstract: Automatically identifying rumours in social media and assessing their
    veracity is an important task with downstream applications in journalism. A significant
    challenge is how to keep rumour analysis tools up-to-date as new information becomes
    available for particular rumours that spread in a social network. This paper presents
    a novel open-source web-based rumour analysis tool that can continuous learn from
    journalists. The system features a rumour annotation service that allows journalists
    to easily provide feedback for a given social media post through a web-based interface.
    The feedback allows the system to improve an underlying state-of-the-art neural
    network-based rumour classification model. The system can be easily integrated
    as a service into existing tools and platforms used by journalists using a REST
    API.
  address: Hong Kong, China
  author:
  - first: Twin
    full: Twin Karmakharm
    id: twin-karmakharm
    last: Karmakharm
  - first: Nikolaos
    full: Nikolaos Aletras
    id: nikolaos-aletras
    last: Aletras
  - first: Kalina
    full: Kalina Bontcheva
    id: kalina-bontcheva
    last: Bontcheva
  author_string: Twin Karmakharm, Nikolaos Aletras, Kalina Bontcheva
  bibkey: karmakharm-etal-2019-journalist
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3020
  month: November
  page_first: '115'
  page_last: '120'
  pages: "115\u2013120"
  paper_id: '20'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3020.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3020.jpg
  title: 'Journalist-in-the-Loop: Continuous Learning as a Service for Rumour Analysis'
  title_html: 'Journalist-in-the-Loop: Continuous Learning as a Service for Rumour
    Analysis'
  url: https://www.aclweb.org/anthology/D19-3020
  year: '2019'
D19-3021:
  abstract: "Dialogue systems have the potential to change how people interact with\
    \ machines but are highly dependent on the quality of the data used to train them.It\
    \ is therefore important to develop good dialogue annotation tools which can improve\
    \ the speed and quality of dialogue data annotation. With this in mind, we introduce\
    \ LIDA, an annotation tool designed specifically for conversation data. As far\
    \ as we know, LIDA is the first dialogue annotation system that handles the entire\
    \ dialogue annotation pipeline from raw text, as may be the output of transcription\
    \ services, to structured conversation data. Furthermore it supports the integration\
    \ of arbitrary machine learning mod-els as annotation recommenders and also has\
    \ a dedicated interface to resolve inter-annotator disagreements such as after\
    \ crowdsourcing an-notations for a dataset. LIDA is fully open source, documented\
    \ and publicly available.[https://github.com/Wluper/lida] \u2013> Screen Cast:\
    \ https://vimeo.com/329824847"
  address: Hong Kong, China
  author:
  - first: Edward
    full: Edward Collins
    id: edward-collins
    last: Collins
  - first: Nikolai
    full: Nikolai Rozanov
    id: nikolai-rozanov
    last: Rozanov
  - first: Bingbing
    full: Bingbing Zhang
    id: bingbing-zhang
    last: Zhang
  author_string: Edward Collins, Nikolai Rozanov, Bingbing Zhang
  bibkey: collins-etal-2019-lida
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3021
  month: November
  page_first: '121'
  page_last: '126'
  pages: "121\u2013126"
  paper_id: '21'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3021.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3021.jpg
  title: 'LIDA: Lightweight Interactive Dialogue Annotator'
  title_html: '<span class="acl-fixed-case">LIDA</span>: Lightweight Interactive Dialogue
    Annotator'
  url: https://www.aclweb.org/anthology/D19-3021
  year: '2019'
D19-3022:
  abstract: We present LINSPECTOR WEB , an open source multilingual inspector to analyze
    word representations. Our system provides researchers working in low-resource
    settings with an easily accessible web based probing tool to gain quick insights
    into their word embeddings especially outside of the English language. To do this
    we employ 16 simple linguistic probing tasks such as gender, case marking, and
    tense for a diverse set of 28 languages. We support probing of static word embeddings
    along with pretrained AllenNLP models that are commonly used for NLP downstream
    tasks such as named entity recognition, natural language inference and dependency
    parsing. The results are visualized in a polar chart and also provided as a table.
    LINSPECTOR WEB is available as an offline tool or at https://linspector.ukp.informatik.tu-darmstadt.de.
  address: Hong Kong, China
  author:
  - first: Max
    full: Max Eichler
    id: max-eichler
    last: Eichler
  - first: "G\xF6zde G\xFCl"
    full: "G\xF6zde G\xFCl \u015Eahin"
    id: gozde-gul-sahin
    last: "\u015Eahin"
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: "Max Eichler, G\xF6zde G\xFCl \u015Eahin, Iryna Gurevych"
  bibkey: eichler-etal-2019-linspector
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3022
  month: November
  page_first: '127'
  page_last: '132'
  pages: "127\u2013132"
  paper_id: '22'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3022.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3022.jpg
  title: 'LINSPECTOR WEB: A Multilingual Probing Suite for Word Representations'
  title_html: '<span class="acl-fixed-case">LINSPECTOR</span> <span class="acl-fixed-case">WEB</span>:
    A Multilingual Probing Suite for Word Representations'
  url: https://www.aclweb.org/anthology/D19-3022
  year: '2019'
D19-3023:
  abstract: 'Massive Open Online Courses (MOOCs) have developed rapidly and attracted
    large number of learners. In this work, we present MAssistant system, a personal
    knowledge assistant for MOOC learners. MAssistant helps users to trace the concepts
    they have learned in MOOCs, and to build their own concept graphs. There are three
    key components in MAssistant: (i) a large-scale concept graph built from open
    data sources, which contains concepts in various domains and relations among them;
    (ii) a browser extension which interacts with learners when they are watching
    video lectures, and presents important concepts to them; (iii) a web application
    allowing users to explore their personal concept graphs, which are built based
    on their learning activities on MOOCs. MAssistant will facilitate the knowledge
    management task for MOOC learners, and make the learning on MOOCs easier.'
  address: Hong Kong, China
  author:
  - first: Lan
    full: Lan Jiang
    id: lan-jiang
    last: Jiang
  - first: Shuhan
    full: Shuhan Hu
    id: shuhan-hu
    last: Hu
  - first: Mingyu
    full: Mingyu Huang
    id: mingyu-huang
    last: Huang
  - first: Zhichun
    full: Zhichun Wang
    id: zhichun-wang
    last: Wang
  - first: Jinjian
    full: Jinjian Yang
    id: jinjian-yang
    last: Yang
  - first: Xiaoju
    full: Xiaoju Ye
    id: xiaoju-ye
    last: Ye
  - first: Wei
    full: Wei Zheng
    id: wei-zheng
    last: Zheng
  author_string: Lan Jiang, Shuhan Hu, Mingyu Huang, Zhichun Wang, Jinjian Yang, Xiaoju
    Ye, Wei Zheng
  bibkey: jiang-etal-2019-massistant
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3023
  month: November
  page_first: '133'
  page_last: '138'
  pages: "133\u2013138"
  paper_id: '23'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3023.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3023.jpg
  title: 'MAssistant: A Personal Knowledge Assistant for MOOC Learners'
  title_html: '<span class="acl-fixed-case">MA</span>ssistant: A Personal Knowledge
    Assistant for <span class="acl-fixed-case">MOOC</span> Learners'
  url: https://www.aclweb.org/anthology/D19-3023
  year: '2019'
D19-3024:
  abstract: 'An interface for building, improving and customising a given Named Entity
    Recognition and Linking (NER+L) model for biomedical domain text, and the efficient
    collation of accurate research use case specific training data and subsequent
    model training. Screencast demo available here: https://www.youtube.com/watch?v=lM914DQjvSo'
  address: Hong Kong, China
  author:
  - first: Thomas
    full: Thomas Searle
    id: thomas-searle
    last: Searle
  - first: Zeljko
    full: Zeljko Kraljevic
    id: zeljko-kraljevic
    last: Kraljevic
  - first: Rebecca
    full: Rebecca Bendayan
    id: rebecca-bendayan
    last: Bendayan
  - first: Daniel
    full: Daniel Bean
    id: daniel-bean
    last: Bean
  - first: Richard
    full: Richard Dobson
    id: richard-dobson
    last: Dobson
  author_string: Thomas Searle, Zeljko Kraljevic, Rebecca Bendayan, Daniel Bean, Richard
    Dobson
  bibkey: searle-etal-2019-medcattrainer
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3024
  month: November
  page_first: '139'
  page_last: '144'
  pages: "139\u2013144"
  paper_id: '24'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3024.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3024.jpg
  title: 'MedCATTrainer: A Biomedical Free Text Annotation Interface with Active Learning
    and Research Use Case Specific Customisation'
  title_html: '<span class="acl-fixed-case">M</span>ed<span class="acl-fixed-case">CATT</span>rainer:
    A Biomedical Free Text Annotation Interface with Active Learning and Research
    Use Case Specific Customisation'
  url: https://www.aclweb.org/anthology/D19-3024
  year: '2019'
D19-3025:
  abstract: "We demonstrate a conversational system which engages the user through\
    \ a multi-modal, multi-turn dialog over the user\u2019s memories. The system can\
    \ perform QA over memories by responding to user queries to recall specific attributes\
    \ and associated media (e.g. photos) of past episodic memories. The system can\
    \ also make proactive suggestions to surface related events or facts from past\
    \ memories to make conversations more engaging and natural. To implement such\
    \ a system, we collect a new corpus of memory grounded conversations, which comprises\
    \ human-to-human role-playing dialogs given synthetic memory graphs with simulated\
    \ attributes. Our proof-of-concept system operates on these synthetic memory graphs,\
    \ however it can be trained and applied to real-world user memory data (e.g. photo\
    \ albums, etc.) We present the architecture of the proposed conversational system,\
    \ and example queries that the system supports."
  address: Hong Kong, China
  author:
  - first: Seungwhan
    full: Seungwhan Moon
    id: seungwhan-moon
    last: Moon
  - first: Pararth
    full: Pararth Shah
    id: pararth-shah
    last: Shah
  - first: Rajen
    full: Rajen Subba
    id: rajen-subba
    last: Subba
  - first: Anuj
    full: Anuj Kumar
    id: anuj-kumar
    last: Kumar
  author_string: Seungwhan Moon, Pararth Shah, Rajen Subba, Anuj Kumar
  bibkey: moon-etal-2019-memory
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3025
  month: November
  page_first: '145'
  page_last: '150'
  pages: "145\u2013150"
  paper_id: '25'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3025.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3025.jpg
  title: Memory Grounded Conversational Reasoning
  title_html: Memory Grounded Conversational Reasoning
  url: https://www.aclweb.org/anthology/D19-3025
  year: '2019'
D19-3026:
  abstract: "The main alternatives nowadays to deal with sequences are Recurrent Neural\
    \ Networks (RNN) architectures and the Transformer. In this context, Both RNN\u2019\
    s and Transformer have been used as an encoder-decoder architecture with multiple\
    \ layers in each module. Far beyond this, these architectures are the basis for\
    \ the contextual word embeddings which are revolutionizing most natural language\
    \ downstream applications. However, intermediate representations in either the\
    \ RNN or Transformer architectures can be difficult to interpret. To make these\
    \ layer representations more accessible and meaningful, we introduce a web-based\
    \ tool that visualizes them both at the sentence and token level. We present three\
    \ use cases. The first analyses gender issues in contextual word embeddings. The\
    \ second and third are showing multilingual intermediate representations for sentences\
    \ and tokens and the evolution of these intermediate representations along with\
    \ the multiple layers of the decoder and in the context of multilingual machine\
    \ translation."
  address: Hong Kong, China
  author:
  - first: Carlos
    full: Carlos Escolano
    id: carlos-escolano
    last: Escolano
  - first: Marta R.
    full: "Marta R. Costa-juss\xE0"
    id: marta-r-costa-jussa
    last: "Costa-juss\xE0"
  - first: Elora
    full: Elora Lacroux
    id: elora-lacroux
    last: Lacroux
  - first: Pere-Pau
    full: "Pere-Pau V\xE1zquez"
    id: pere-pau-vazquez
    last: "V\xE1zquez"
  author_string: "Carlos Escolano, Marta R. Costa-juss\xE0, Elora Lacroux, Pere-Pau\
    \ V\xE1zquez"
  bibkey: escolano-etal-2019-multilingual
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3026
  month: November
  page_first: '151'
  page_last: '156'
  pages: "151\u2013156"
  paper_id: '26'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3026.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3026.jpg
  title: Multilingual, Multi-scale and Multi-layer Visualization of Intermediate Representations
  title_html: Multilingual, Multi-scale and Multi-layer Visualization of Intermediate
    Representations
  url: https://www.aclweb.org/anthology/D19-3026
  year: '2019'
D19-3027:
  abstract: MY-AKKHARA is a method used to input Burmese texts encoded in the Unicode
    standard, based on commonly accepted Latin transcription. By using this method,
    arbitrary Burmese strings can be accurately inputted with 26 lowercase Latin letters.
    Meanwhile, the 26 uppercase Latin letters are designed as shortcuts of lowercase
    letter sequences. The frequency of Burmese characters is considered in MY-AKKHARA
    to realize an efficient keystroke distribution on a QWERTY keyboard. Given that
    the Unicode standard has not been extensively used in digitization of Burmese,
    we hope that MY-AKKHARA can contribute to the widespread use of Unicode in Myanmar
    and can provide a platform for smart input methods for Burmese in the future.
    An implementation of MY-AKKHARA running in Windows is released at http://www2.nict.go.jp/astrec-att/member/ding/my-akkhara.html
  address: Hong Kong, China
  author:
  - first: Chenchen
    full: Chenchen Ding
    id: chenchen-ding
    last: Ding
  - first: Masao
    full: Masao Utiyama
    id: masao-utiyama
    last: Utiyama
  - first: Eiichiro
    full: Eiichiro Sumita
    id: eiichiro-sumita
    last: Sumita
  author_string: Chenchen Ding, Masao Utiyama, Eiichiro Sumita
  bibkey: ding-etal-2019-akkhara
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3027
  month: November
  page_first: '157'
  page_last: '162'
  pages: "157\u2013162"
  paper_id: '27'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3027.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3027.jpg
  title: 'MY-AKKHARA: A Romanization-based Burmese (Myanmar) Input Method'
  title_html: '<span class="acl-fixed-case">MY</span>-<span class="acl-fixed-case">AKKHARA</span>:
    A <span class="acl-fixed-case">R</span>omanization-based <span class="acl-fixed-case">B</span>urmese
    (<span class="acl-fixed-case">M</span>yanmar) Input Method'
  url: https://www.aclweb.org/anthology/D19-3027
  year: '2019'
D19-3028:
  abstract: 'Deep Neural Networks (DNN) have been widely employed in industry to address
    various Natural Language Processing (NLP) tasks. However, many engineers find
    it a big overhead when they have to choose from multiple frameworks, compare different
    types of models, and understand various optimization mechanisms. An NLP toolkit
    for DNN models with both generality and flexibility can greatly improve the productivity
    of engineers by saving their learning cost and guiding them to find optimal solutions
    to their tasks. In this paper, we introduce NeuronBlocks, a toolkit encapsulating
    a suite of neural network modules as building blocks to construct various DNN
    models with complex architecture. This toolkit empowers engineers to build, train,
    and test various NLP models through simple configuration of JSON files. The experiments
    on several NLP datasets such as GLUE, WikiQA and CoNLL-2003 demonstrate the effectiveness
    of NeuronBlocks. Code: https://github.com/Microsoft/NeuronBlocks Demo: https://youtu.be/x6cOpVSZcdo'
  address: Hong Kong, China
  author:
  - first: Ming
    full: Ming Gong
    id: ming-gong
    last: Gong
  - first: Linjun
    full: Linjun Shou
    id: linjun-shou
    last: Shou
  - first: Wutao
    full: Wutao Lin
    id: wutao-lin
    last: Lin
  - first: Zhijie
    full: Zhijie Sang
    id: zhijie-sang
    last: Sang
  - first: Quanjia
    full: Quanjia Yan
    id: quanjia-yan
    last: Yan
  - first: Ze
    full: Ze Yang
    id: ze-yang
    last: Yang
  - first: Feixiang
    full: Feixiang Cheng
    id: feixiang-cheng
    last: Cheng
  - first: Daxin
    full: Daxin Jiang
    id: daxin-jiang
    last: Jiang
  author_string: Ming Gong, Linjun Shou, Wutao Lin, Zhijie Sang, Quanjia Yan, Ze Yang,
    Feixiang Cheng, Daxin Jiang
  bibkey: gong-etal-2019-neuronblocks
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3028
  month: November
  page_first: '163'
  page_last: '168'
  pages: "163\u2013168"
  paper_id: '28'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3028.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3028.jpg
  title: 'NeuronBlocks: Building Your NLP DNN Models Like Playing Lego'
  title_html: '<span class="acl-fixed-case">N</span>euron<span class="acl-fixed-case">B</span>locks:
    Building Your <span class="acl-fixed-case">NLP</span> <span class="acl-fixed-case">DNN</span>
    Models Like Playing <span class="acl-fixed-case">L</span>ego'
  url: https://www.aclweb.org/anthology/D19-3028
  year: '2019'
D19-3029:
  abstract: OpenNRE is an open-source and extensible toolkit that provides a unified
    framework to implement neural models for relation extraction (RE). Specifically,
    by implementing typical RE methods, OpenNRE not only allows developers to train
    custom models to extract structured relational facts from the plain text but also
    supports quick model validation for researchers. Besides, OpenNRE provides various
    functional RE modules based on both TensorFlow and PyTorch to maintain sufficient
    modularity and extensibility, making it becomes easy to incorporate new models
    into the framework. Besides the toolkit, we also release an online system to meet
    real-time extraction without any training and deploying. Meanwhile, the online
    system can extract facts in various scenarios as well as aligning the extracted
    facts to Wikidata, which may benefit various downstream knowledge-driven applications
    (e.g., information retrieval and question answering). More details of the toolkit
    and online system can be obtained from http://github.com/thunlp/OpenNRE.
  address: Hong Kong, China
  author:
  - first: Xu
    full: Xu Han
    id: xu-han
    last: Han
  - first: Tianyu
    full: Tianyu Gao
    id: tianyu-gao
    last: Gao
  - first: Yuan
    full: Yuan Yao
    id: yuan-yao
    last: Yao
  - first: Deming
    full: Deming Ye
    id: deming-ye
    last: Ye
  - first: Zhiyuan
    full: Zhiyuan Liu
    id: zhiyuan-liu
    last: Liu
  - first: Maosong
    full: Maosong Sun
    id: maosong-sun
    last: Sun
  author_string: Xu Han, Tianyu Gao, Yuan Yao, Deming Ye, Zhiyuan Liu, Maosong Sun
  bibkey: han-etal-2019-opennre
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3029
  month: November
  page_first: '169'
  page_last: '174'
  pages: "169\u2013174"
  paper_id: '29'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3029.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3029.jpg
  title: 'OpenNRE: An Open and Extensible Toolkit for Neural Relation Extraction'
  title_html: '<span class="acl-fixed-case">O</span>pen<span class="acl-fixed-case">NRE</span>:
    An Open and Extensible Toolkit for Neural Relation Extraction'
  url: https://www.aclweb.org/anthology/D19-3029
  year: '2019'
D19-3030:
  abstract: Generating syntactically and semantically valid and relevant questions
    from paragraphs is useful with many applications. Manual generation is a labour-intensive
    task, as it requires the reading, parsing and understanding of long passages of
    text. A number of question generation models based on sequence-to-sequence techniques
    have recently been proposed. Most of them generate questions from sentences only,
    and none of them is publicly available as an easy-to-use service. In this paper,
    we demonstrate ParaQG, a Web-based system for generating questions from sentences
    and paragraphs. ParaQG incorporates a number of novel functionalities to make
    the question generation process user-friendly. It provides an interactive interface
    for a user to select answers with visual insights on generation of questions.
    It also employs various faceted views to group similar questions as well as filtering
    techniques to eliminate unanswerable questions.
  address: Hong Kong, China
  author:
  - first: Vishwajeet
    full: Vishwajeet Kumar
    id: vishwajeet-kumar
    last: Kumar
  - first: Sivaanandh
    full: Sivaanandh Muneeswaran
    id: sivaanandh-muneeswaran
    last: Muneeswaran
  - first: Ganesh
    full: Ganesh Ramakrishnan
    id: ganesh-ramakrishnan
    last: Ramakrishnan
  - first: Yuan-Fang
    full: Yuan-Fang Li
    id: yuan-fang-li
    last: Li
  author_string: Vishwajeet Kumar, Sivaanandh Muneeswaran, Ganesh Ramakrishnan, Yuan-Fang
    Li
  bibkey: kumar-etal-2019-paraqg
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3030
  month: November
  page_first: '175'
  page_last: '180'
  pages: "175\u2013180"
  paper_id: '30'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3030.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3030.jpg
  title: 'ParaQG: A System for Generating Questions and Answers from Paragraphs'
  title_html: '<span class="acl-fixed-case">P</span>ara<span class="acl-fixed-case">QG</span>:
    A System for Generating Questions and Answers from Paragraphs'
  url: https://www.aclweb.org/anthology/D19-3030
  year: '2019'
D19-3031:
  abstract: 'We present PolyResponse, a conversational search engine that supports
    task-oriented dialogue. It is a retrieval-based approach that bypasses the complex
    multi-component design of traditional task-oriented dialogue systems and the use
    of explicit semantics in the form of task-specific ontologies. The PolyResponse
    engine is trained on hundreds of millions of examples extracted from real conversations:
    it learns what responses are appropriate in different conversational contexts.
    It then ranks a large index of text and visual responses according to their similarity
    to the given context, and narrows down the list of relevant entities during the
    multi-turn conversation. We introduce a restaurant search and booking system powered
    by the PolyResponse engine, currently available in 8 different languages.'
  address: Hong Kong, China
  author:
  - first: Matthew
    full: Matthew Henderson
    id: matthew-henderson
    last: Henderson
  - first: Ivan
    full: "Ivan Vuli\u0107"
    id: ivan-vulic
    last: "Vuli\u0107"
  - first: "I\xF1igo"
    full: "I\xF1igo Casanueva"
    id: inigo-casanueva
    last: Casanueva
  - first: "Pawe\u0142"
    full: "Pawe\u0142 Budzianowski"
    id: pawel-budzianowski
    last: Budzianowski
  - first: Daniela
    full: Daniela Gerz
    id: daniela-gerz
    last: Gerz
  - first: Sam
    full: Sam Coope
    id: sam-coope
    last: Coope
  - first: Georgios
    full: Georgios Spithourakis
    id: georgios-spithourakis
    last: Spithourakis
  - first: Tsung-Hsien
    full: Tsung-Hsien Wen
    id: tsung-hsien-wen
    last: Wen
  - first: Nikola
    full: "Nikola Mrk\u0161i\u0107"
    id: nikola-mrksic
    last: "Mrk\u0161i\u0107"
  - first: Pei-Hao
    full: Pei-Hao Su
    id: pei-hao-su
    last: Su
  author_string: "Matthew Henderson, Ivan Vuli\u0107, I\xF1igo Casanueva, Pawe\u0142\
    \ Budzianowski, Daniela Gerz, Sam Coope, Georgios Spithourakis, Tsung-Hsien Wen,\
    \ Nikola Mrk\u0161i\u0107, Pei-Hao Su"
  bibkey: henderson-etal-2019-polyresponse
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3031
  month: November
  page_first: '181'
  page_last: '186'
  pages: "181\u2013186"
  paper_id: '31'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3031.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3031.jpg
  title: 'PolyResponse: A Rank-based Approach to Task-Oriented Dialogue with Application
    in Restaurant Search and Booking'
  title_html: '<span class="acl-fixed-case">P</span>oly<span class="acl-fixed-case">R</span>esponse:
    A Rank-based Approach to Task-Oriented Dialogue with Application in Restaurant
    Search and Booking'
  url: https://www.aclweb.org/anthology/D19-3031
  year: '2019'
D19-3032:
  abstract: We present PyOpenDial, a Python-based domain-independent, open-source
    toolkit for spoken dialogue systems. Recent advances in core components of dialogue
    systems, such as speech recognition, language understanding, dialogue management,
    and language generation, harness deep learning to achieve state-of-the-art performance.
    The original OpenDial, implemented in Java, provides a plugin architecture to
    integrate external modules, but lacks Python bindings, making it difficult to
    interface with popular deep learning frameworks such as Tensorflow or PyTorch.
    To this end, we re-implemented OpenDial in Python and extended the toolkit with
    a number of novel functionalities for neural dialogue state tracking and action
    planning. We describe the overall architecture and its extensions, and illustrate
    their use on an example where the system response model is implemented with a
    recurrent neural network.
  address: Hong Kong, China
  author:
  - first: Youngsoo
    full: Youngsoo Jang
    id: youngsoo-jang
    last: Jang
  - first: Jongmin
    full: Jongmin Lee
    id: jongmin-lee
    last: Lee
  - first: Jaeyoung
    full: Jaeyoung Park
    id: jaeyoung-park
    last: Park
  - first: Kyeng-Hun
    full: Kyeng-Hun Lee
    id: kyeng-hun-lee
    last: Lee
  - first: Pierre
    full: Pierre Lison
    id: pierre-lison
    last: Lison
  - first: Kee-Eung
    full: Kee-Eung Kim
    id: kee-eung-kim
    last: Kim
  author_string: Youngsoo Jang, Jongmin Lee, Jaeyoung Park, Kyeng-Hun Lee, Pierre
    Lison, Kee-Eung Kim
  bibkey: jang-etal-2019-pyopendial
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3032
  month: November
  page_first: '187'
  page_last: '192'
  pages: "187\u2013192"
  paper_id: '32'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3032.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3032.jpg
  title: 'PyOpenDial: A Python-based Domain-Independent Toolkit for Developing Spoken
    Dialogue Systems with Probabilistic Rules'
  title_html: '<span class="acl-fixed-case">P</span>y<span class="acl-fixed-case">O</span>pen<span
    class="acl-fixed-case">D</span>ial: A Python-based Domain-Independent Toolkit
    for Developing Spoken Dialogue Systems with Probabilistic Rules'
  url: https://www.aclweb.org/anthology/D19-3032
  year: '2019'
D19-3033:
  abstract: "We introduce Redcoat, a web-based annotation tool that supports collaborative\
    \ hierarchical entity typing. As an annotation tool, Redcoat also facilitates\
    \ knowledge elicitation by allowing the creation and continuous refinement of\
    \ concept hierarchies during annotation. It aims to minimise not only annotation\
    \ time but the time it takes for project creators to set up and distribute projects\
    \ to annotators. Projects created using the web-based interface can be rapidly\
    \ distributed to a list of email addresses. Redcoat handles the propagation of\
    \ documents amongst annotators and automatically scales the annotation workload\
    \ depending on the number of active annotators. In this paper we discuss these\
    \ key features and outline Redcoat\u2019s system architecture. We also highlight\
    \ Redcoat\u2019s unique benefits over existing annotation tools via a qualitative\
    \ comparison."
  address: Hong Kong, China
  author:
  - first: Michael
    full: Michael Stewart
    id: michael-stewart
    last: Stewart
  - first: Wei
    full: Wei Liu
    id: wei-liu
    last: Liu
  - first: Rachel
    full: Rachel Cardell-Oliver
    id: rachel-cardell-oliver
    last: Cardell-Oliver
  author_string: Michael Stewart, Wei Liu, Rachel Cardell-Oliver
  bibkey: stewart-etal-2019-redcoat
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3033
  month: November
  page_first: '193'
  page_last: '198'
  pages: "193\u2013198"
  paper_id: '33'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3033.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3033.jpg
  title: 'Redcoat: A Collaborative Annotation Tool for Hierarchical Entity Typing'
  title_html: '<span class="acl-fixed-case">R</span>edcoat: A Collaborative Annotation
    Tool for Hierarchical Entity Typing'
  url: https://www.aclweb.org/anthology/D19-3033
  year: '2019'
D19-3034:
  abstract: "We introduce Seagle, a platform for comparative evaluation of semantic\
    \ text encoding models on information retrieval (IR) tasks. Seagle implements\
    \ (1) word embedding aggregators, which represent texts as algebraic aggregations\
    \ of pretrained word embeddings and (2) pretrained semantic encoders, and allows\
    \ for their comparative evaluation on arbitrary (monolingual and cross-lingual)\
    \ IR collections. We benchmark Seagle\u2019s models on monolingual document retrieval\
    \ and cross-lingual sentence retrieval. Seagle functionality can be exploited\
    \ via an easy-to-use web interface and its modular backend (micro-service architecture)\
    \ can easily be extended with additional semantic search models."
  address: Hong Kong, China
  author:
  - first: Fabian David
    full: Fabian David Schmidt
    id: fabian-david-schmidt
    last: Schmidt
  - first: Markus
    full: Markus Dietsche
    id: markus-dietsche
    last: Dietsche
  - first: Simone Paolo
    full: Simone Paolo Ponzetto
    id: simone-paolo-ponzetto
    last: Ponzetto
  - first: Goran
    full: "Goran Glava\u0161"
    id: goran-glavas
    last: "Glava\u0161"
  author_string: "Fabian David Schmidt, Markus Dietsche, Simone Paolo Ponzetto, Goran\
    \ Glava\u0161"
  bibkey: schmidt-etal-2019-seagle
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3034
  month: November
  page_first: '199'
  page_last: '204'
  pages: "199\u2013204"
  paper_id: '34'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3034.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3034.jpg
  title: 'SEAGLE: A Platform for Comparative Evaluation of Semantic Encoders for Information
    Retrieval'
  title_html: '<span class="acl-fixed-case">SEAGLE</span>: A Platform for Comparative
    Evaluation of Semantic Encoders for Information Retrieval'
  url: https://www.aclweb.org/anthology/D19-3034
  year: '2019'
D19-3035:
  abstract: Computational stylometry has become an increasingly important aspect of
    literary criticism, but many humanists lack the technical expertise or language-specific
    NLP resources required to exploit computational methods. We demonstrate a stylometry
    toolkit for analysis of Latin literary texts, which is freely available at www.qcrit.org/stylometry.
    Our toolkit generates data for a diverse range of literary features and has an
    intuitive point-and-click interface. The features included have proven effective
    for multiple literary studies and are calculated using custom heuristics without
    the need for syntactic parsing. As such, the toolkit models one approach to the
    user-friendly generation of stylometric data, which could be extended to other
    premodern and non-English languages underserved by standard NLP resources.
  address: Hong Kong, China
  author:
  - first: Thomas J.
    full: Thomas J. Bolt
    id: thomas-j-bolt
    last: Bolt
  - first: Jeffrey H.
    full: Jeffrey H. Flynt
    id: jeffrey-h-flynt
    last: Flynt
  - first: Pramit
    full: Pramit Chaudhuri
    id: pramit-chaudhuri
    last: Chaudhuri
  - first: Joseph P.
    full: Joseph P. Dexter
    id: joseph-p-dexter
    last: Dexter
  author_string: Thomas J. Bolt, Jeffrey H. Flynt, Pramit Chaudhuri, Joseph P. Dexter
  bibkey: bolt-etal-2019-stylometry
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3035
  month: November
  page_first: '205'
  page_last: '210'
  pages: "205\u2013210"
  paper_id: '35'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3035.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3035.jpg
  title: A Stylometry Toolkit for Latin Literature
  title_html: A Stylometry Toolkit for <span class="acl-fixed-case">L</span>atin Literature
  url: https://www.aclweb.org/anthology/D19-3035
  year: '2019'
D19-3036:
  abstract: We present a novel system providing summaries for Computer Science publications.
    Through a qualitative user study, we identified the most valuable scenarios for
    discovery, exploration and understanding of scientific documents. Based on these
    findings, we built a system that retrieves and summarizes scientific documents
    for a given information need, either in form of a free-text query or by choosing
    categorized values such as scientific tasks, datasets and more. Our system ingested
    270,000 papers, and its summarization module aims to generate concise yet detailed
    summaries. We validated our approach with human experts.
  address: Hong Kong, China
  author:
  - first: Shai
    full: Shai Erera
    id: shai-erera
    last: Erera
  - first: Michal
    full: Michal Shmueli-Scheuer
    id: michal-shmueli-scheuer
    last: Shmueli-Scheuer
  - first: Guy
    full: Guy Feigenblat
    id: guy-feigenblat
    last: Feigenblat
  - first: Ora
    full: Ora Peled Nakash
    id: ora-peled-nakash
    last: Peled Nakash
  - first: Odellia
    full: Odellia Boni
    id: odellia-boni
    last: Boni
  - first: Haggai
    full: Haggai Roitman
    id: haggai-roitman
    last: Roitman
  - first: Doron
    full: Doron Cohen
    id: doron-cohen
    last: Cohen
  - first: Bar
    full: Bar Weiner
    id: bar-weiner
    last: Weiner
  - first: Yosi
    full: Yosi Mass
    id: yosi-mass
    last: Mass
  - first: Or
    full: Or Rivlin
    id: or-rivlin
    last: Rivlin
  - first: Guy
    full: Guy Lev
    id: guy-lev
    last: Lev
  - first: Achiya
    full: Achiya Jerbi
    id: achiya-jerbi
    last: Jerbi
  - first: Jonathan
    full: Jonathan Herzig
    id: jonathan-herzig
    last: Herzig
  - first: Yufang
    full: Yufang Hou
    id: yufang-hou
    last: Hou
  - first: Charles
    full: Charles Jochim
    id: charles-jochim
    last: Jochim
  - first: Martin
    full: Martin Gleize
    id: martin-gleize
    last: Gleize
  - first: Francesca
    full: Francesca Bonin
    id: francesca-bonin
    last: Bonin
  - first: Francesca
    full: Francesca Bonin
    id: francesca-bonin
    last: Bonin
  - first: David
    full: David Konopnicki
    id: david-konopnicki
    last: Konopnicki
  author_string: Shai Erera, Michal Shmueli-Scheuer, Guy Feigenblat, Ora Peled Nakash,
    Odellia Boni, Haggai Roitman, Doron Cohen, Bar Weiner, Yosi Mass, Or Rivlin, Guy
    Lev, Achiya Jerbi, Jonathan Herzig, Yufang Hou, Charles Jochim, Martin Gleize,
    Francesca Bonin, Francesca Bonin, David Konopnicki
  bibkey: erera-etal-2019-summarization
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3036
  month: November
  page_first: '211'
  page_last: '216'
  pages: "211\u2013216"
  paper_id: '36'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3036.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3036.jpg
  title: A Summarization System for Scientific Documents
  title_html: A Summarization System for Scientific Documents
  url: https://www.aclweb.org/anthology/D19-3036
  year: '2019'
D19-3037:
  abstract: Short vowels, aka diacritics, are more often omitted when writing different
    varieties of Arabic including Modern Standard Arabic (MSA), Classical Arabic (CA),
    and Dialectal Arabic (DA). However, diacritics are required to properly pronounce
    words, which makes diacritic restoration (a.k.a. diacritization) essential for
    language learning and text-to-speech applications. In this paper, we present a
    system for diacritizing MSA, CA, and two varieties of DA, namely Moroccan and
    Tunisian. The system uses a character level sequence-to-sequence deep learning
    model that requires no feature engineering and beats all previous SOTA systems
    for all the Arabic varieties that we test on.
  address: Hong Kong, China
  author:
  - first: Hamdy
    full: Hamdy Mubarak
    id: hamdy-mubarak
    last: Mubarak
  - first: Ahmed
    full: Ahmed Abdelali
    id: ahmed-abdelali
    last: Abdelali
  - first: Kareem
    full: Kareem Darwish
    id: kareem-darwish
    last: Darwish
  - first: Mohamed
    full: Mohamed Eldesouki
    id: mohamed-eldesouki
    last: Eldesouki
  - first: Younes
    full: Younes Samih
    id: younes-samih
    last: Samih
  - first: Hassan
    full: Hassan Sajjad
    id: hassan-sajjad
    last: Sajjad
  author_string: Hamdy Mubarak, Ahmed Abdelali, Kareem Darwish, Mohamed Eldesouki,
    Younes Samih, Hassan Sajjad
  bibkey: mubarak-etal-2019-system
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3037
  month: November
  page_first: '217'
  page_last: '222'
  pages: "217\u2013222"
  paper_id: '37'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3037.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3037.jpg
  title: A System for Diacritizing Four Varieties of Arabic
  title_html: A System for Diacritizing Four Varieties of <span class="acl-fixed-case">A</span>rabic
  url: https://www.aclweb.org/anthology/D19-3037
  year: '2019'
D19-3038:
  abstract: "We introduce Tanbih, a news aggregator with intelligent analysis tools\
    \ to help readers understanding what\u2019s behind a news story. Our system displays\
    \ news grouped into events and generates media profiles that show the general\
    \ factuality of reporting, the degree of propagandistic content, hyper-partisanship,\
    \ leading political ideology, general frame of reporting, and stance with respect\
    \ to various claims and topics of a news outlet. In addition, we automatically\
    \ analyse each article to detect whether it is propagandistic and to determine\
    \ its stance with respect to a number of controversial topics."
  address: Hong Kong, China
  author:
  - first: Yifan
    full: Yifan Zhang
    id: yifan-zhang
    last: Zhang
  - first: Giovanni
    full: Giovanni Da San Martino
    id: giovanni-da-san-martino
    last: Da San Martino
  - first: Alberto
    full: "Alberto Barr\xF3n-Cede\xF1o"
    id: alberto-barron-cedeno
    last: "Barr\xF3n-Cede\xF1o"
  - first: Salvatore
    full: Salvatore Romeo
    id: salvatore-romeo
    last: Romeo
  - first: Jisun
    full: Jisun An
    id: jisun-an
    last: An
  - first: Haewoon
    full: Haewoon Kwak
    id: haewoon-kwak
    last: Kwak
  - first: Todor
    full: Todor Staykovski
    id: todor-staykovski
    last: Staykovski
  - first: Israa
    full: Israa Jaradat
    id: israa-jaradat
    last: Jaradat
  - first: Georgi
    full: Georgi Karadzhov
    id: georgi-karadzhov
    last: Karadzhov
  - first: Ramy
    full: Ramy Baly
    id: ramy-baly
    last: Baly
  - first: Kareem
    full: Kareem Darwish
    id: kareem-darwish
    last: Darwish
  - first: James
    full: James Glass
    id: james-glass
    last: Glass
  - first: Preslav
    full: Preslav Nakov
    id: preslav-nakov
    last: Nakov
  author_string: "Yifan Zhang, Giovanni Da San Martino, Alberto Barr\xF3n-Cede\xF1\
    o, Salvatore Romeo, Jisun An, Haewoon Kwak, Todor Staykovski, Israa Jaradat, Georgi\
    \ Karadzhov, Ramy Baly, Kareem Darwish, James Glass, Preslav Nakov"
  bibkey: zhang-etal-2019-tanbih
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3038
  month: November
  page_first: '223'
  page_last: '228'
  pages: "223\u2013228"
  paper_id: '38'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3038.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3038.jpg
  title: 'Tanbih: Get To Know What You Are Reading'
  title_html: '<span class="acl-fixed-case">T</span>anbih: Get To Know What You Are
    Reading'
  url: https://www.aclweb.org/anthology/D19-3038
  year: '2019'
D19-3039:
  abstract: Language technologies play a key role in assisting people with their writing.
    Although there has been steady progress in e.g., grammatical error correction
    (GEC), human writers are yet to benefit from this progress due to the high development
    cost of integrating with writing software. We propose TEASPN, a protocol and an
    open-source framework for achieving integrated writing assistance environments.
    The protocol standardizes the way writing software communicates with servers that
    implement such technologies, allowing developers and researchers to integrate
    the latest developments in natural language processing (NLP) with low cost. As
    a result, users can enjoy the integrated experience in their favorite writing
    software. The results from experiments with human participants show that users
    use a wide range of technologies and rate their writing experience favorably,
    allowing them to write more fluent text.
  address: Hong Kong, China
  author:
  - first: Masato
    full: Masato Hagiwara
    id: masato-hagiwara
    last: Hagiwara
  - first: Takumi
    full: Takumi Ito
    id: takumi-ito
    last: Ito
  - first: Tatsuki
    full: Tatsuki Kuribayashi
    id: tatsuki-kuribayashi
    last: Kuribayashi
  - first: Jun
    full: Jun Suzuki
    id: jun-suzuki
    last: Suzuki
  - first: Kentaro
    full: Kentaro Inui
    id: kentaro-inui
    last: Inui
  author_string: Masato Hagiwara, Takumi Ito, Tatsuki Kuribayashi, Jun Suzuki, Kentaro
    Inui
  bibkey: hagiwara-etal-2019-teaspn
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3039
  month: November
  page_first: '229'
  page_last: '234'
  pages: "229\u2013234"
  paper_id: '39'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3039.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3039.jpg
  title: 'TEASPN: Framework and Protocol for Integrated Writing Assistance Environments'
  title_html: '<span class="acl-fixed-case">TEASPN</span>: Framework and Protocol
    for Integrated Writing Assistance Environments'
  url: https://www.aclweb.org/anthology/D19-3039
  year: '2019'
D19-3040:
  abstract: We present a writing prototype feedback system, TellMeWhy, to provide
    explanations of errors in submitted essays. In our approach, the sentence with
    corrections is analyzed to identify error types and problem words, aimed at customizing
    explanations based on the context of the error. The method involves learning the
    relation of errors and problem words, generating common feedback patterns, and
    extracting grammar patterns, collocations and example sentences. At run-time,
    a sentence with corrections is classified, and the problem word and template are
    identified to provide detailed explanations. Preliminary evaluation shows that
    the method has potential to improve existing commercial writing services.
  address: Hong Kong, China
  author:
  - first: Yi-Huei
    full: Yi-Huei Lai
    id: yi-huei-lai
    last: Lai
  - first: Jason
    full: Jason Chang
    id: jason-s-chang
    last: Chang
  author_string: Yi-Huei Lai, Jason Chang
  bibkey: lai-chang-2019-tellmewhy
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3040
  month: November
  page_first: '235'
  page_last: '240'
  pages: "235\u2013240"
  paper_id: '40'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3040.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3040.jpg
  title: 'TellMeWhy: Learning to Explain Corrective Feedback for Second Language Learners'
  title_html: '<span class="acl-fixed-case">T</span>ell<span class="acl-fixed-case">M</span>e<span
    class="acl-fixed-case">W</span>hy: Learning to Explain Corrective Feedback for
    Second Language Learners'
  url: https://www.aclweb.org/anthology/D19-3040
  year: '2019'
D19-3041:
  abstract: Existing works, including ELMO and BERT, have revealed the importance
    of pre-training for NLP tasks. While there does not exist a single pre-training
    model that works best in all cases, it is of necessity to develop a framework
    that is able to deploy various pre-training models efficiently. For this purpose,
    we propose an assemble-on-demand pre-training toolkit, namely Universal Encoder
    Representations (UER). UER is loosely coupled, and encapsulated with rich modules.
    By assembling modules on demand, users can either reproduce a state-of-the-art
    pre-training model or develop a pre-training model that remains unexplored. With
    UER, we have built a model zoo, which contains pre-trained models based on different
    corpora, encoders, and targets (objectives). With proper pre-trained models, we
    could achieve new state-of-the-art results on a range of downstream datasets.
  address: Hong Kong, China
  author:
  - first: Zhe
    full: Zhe Zhao
    id: zhe-zhao
    last: Zhao
  - first: Hui
    full: Hui Chen
    id: hui-chen
    last: Chen
  - first: Jinbin
    full: Jinbin Zhang
    id: jinbin-zhang
    last: Zhang
  - first: Xin
    full: Xin Zhao
    id: wayne-xin-zhao
    last: Zhao
  - first: Tao
    full: Tao Liu
    id: tao-liu
    last: Liu
  - first: Wei
    full: Wei Lu
    id: wei-lu
    last: Lu
  - first: Xi
    full: Xi Chen
    id: xi-chen
    last: Chen
  - first: Haotang
    full: Haotang Deng
    id: haotang-deng
    last: Deng
  - first: Qi
    full: Qi Ju
    id: qi-ju
    last: Ju
  - first: Xiaoyong
    full: Xiaoyong Du
    id: xiaoyong-du
    last: Du
  author_string: Zhe Zhao, Hui Chen, Jinbin Zhang, Xin Zhao, Tao Liu, Wei Lu, Xi Chen,
    Haotang Deng, Qi Ju, Xiaoyong Du
  bibkey: zhao-etal-2019-uer
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3041
  month: November
  page_first: '241'
  page_last: '246'
  pages: "241\u2013246"
  paper_id: '41'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3041.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3041.jpg
  title: 'UER: An Open-Source Toolkit for Pre-training Models'
  title_html: '<span class="acl-fixed-case">UER</span>: An Open-Source Toolkit for
    Pre-training Models'
  url: https://www.aclweb.org/anthology/D19-3041
  year: '2019'
D19-3042:
  abstract: 'There are tons of news generated every day reflecting the change of key
    roles such as people, organizations and political parties. Analyzing the trend
    of these key roles can help understand the information flow in a more effective
    way. In this paper, we present a demonstration system that visualizes the news
    trend of key roles based on natural language processing techniques. Specifically,
    we apply semantic role labelling to understand relationships between key roles
    in the news. We also train a dynamic word embedding model to align representations
    of words in different time periods to measure how the similarities between a key
    role and news topics change over time. Note: The github link to our demo jupyter
    notebook and screencast video is https://github.com/kasinxc/Visualizing-Trend-of-Key-Roles-in-News-Articles'
  address: Hong Kong, China
  author:
  - first: Chen
    full: Chen Xia
    id: chen-xia
    last: Xia
  - first: Haoxiang
    full: Haoxiang Zhang
    id: haoxiang-zhang
    last: Zhang
  - first: Jacob
    full: Jacob Moghtader
    id: jacob-moghtader
    last: Moghtader
  - first: Allen
    full: Allen Wu
    id: allen-wu
    last: Wu
  - first: Kai-Wei
    full: Kai-Wei Chang
    id: kai-wei-chang
    last: Chang
  author_string: Chen Xia, Haoxiang Zhang, Jacob Moghtader, Allen Wu, Kai-Wei Chang
  bibkey: xia-etal-2019-visualizing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3042
  month: November
  page_first: '247'
  page_last: '252'
  pages: "247\u2013252"
  paper_id: '42'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3042.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3042.jpg
  title: Visualizing Trends of Key Roles in News Articles
  title_html: Visualizing Trends of Key Roles in News Articles
  url: https://www.aclweb.org/anthology/D19-3042
  year: '2019'
D19-3043:
  abstract: Automatic evaluation of text generation tasks (e.g. machine translation,
    text summarization, image captioning and video description) usually relies heavily
    on task-specific metrics, such as BLEU and ROUGE. They, however, are abstract
    numbers and are not perfectly aligned with human assessment. This suggests inspecting
    detailed examples as a complement to identify system error patterns. In this paper,
    we present VizSeq, a visual analysis toolkit for instance-level and corpus-level
    system evaluation on a wide variety of text generation tasks. It supports multimodal
    sources and multiple text references, providing visualization in Jupyter notebook
    or a web app interface. It can be used locally or deployed onto public servers
    for centralized data hosting and benchmarking. It covers most common n-gram based
    metrics accelerated with multiprocessing, and also provides latest embedding-based
    metrics such as BERTScore.
  address: Hong Kong, China
  author:
  - first: Changhan
    full: Changhan Wang
    id: changhan-wang
    last: Wang
  - first: Anirudh
    full: Anirudh Jain
    id: anirudh-jain
    last: Jain
  - first: Danlu
    full: Danlu Chen
    id: danlu-chen
    last: Chen
  - first: Jiatao
    full: Jiatao Gu
    id: jiatao-gu
    last: Gu
  author_string: Changhan Wang, Anirudh Jain, Danlu Chen, Jiatao Gu
  bibkey: wang-etal-2019-vizseq
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3043
  month: November
  page_first: '253'
  page_last: '258'
  pages: "253\u2013258"
  paper_id: '43'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3043.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3043.jpg
  title: 'VizSeq: a visual analysis toolkit for text generation tasks'
  title_html: '<span class="acl-fixed-case">V</span>iz<span class="acl-fixed-case">S</span>eq:
    a visual analysis toolkit for text generation tasks'
  url: https://www.aclweb.org/anthology/D19-3043
  year: '2019'
D19-3044:
  abstract: "For languages with simple morphology such as English, automatic annotation\
    \ pipelines such as spaCy or Stanford\u2019s CoreNLP successfully serve projects\
    \ in academia and the industry. For many morphologically-rich languages (MRLs),\
    \ similar pipelines show sub-optimal performance that limits their applicability\
    \ for text analysis in research and the industry. The sub-optimal performance\
    \ is mainly due to errors in early morphological disambiguation decisions, that\
    \ cannot be recovered later on in the pipeline, yielding incoherent annotations\
    \ on the whole. This paper describes the design and use of the ONLP suite, a joint\
    \ morpho-syntactic infrastructure for processing Modern Hebrew texts. The joint\
    \ inference over morphology and syntax substantially limits error propagation,\
    \ and leads to high accuracy. ONLP provides rich and expressive annotations which\
    \ already serve diverse academic and commercial needs. Its accompanying demo further\
    \ serves educational activities, introducing Hebrew NLP intricacies to researchers\
    \ and non-researchers alike."
  address: Hong Kong, China
  author:
  - first: Reut
    full: Reut Tsarfaty
    id: reut-tsarfaty
    last: Tsarfaty
  - first: Shoval
    full: Shoval Sadde
    id: shoval-sadde
    last: Sadde
  - first: Stav
    full: Stav Klein
    id: stav-klein
    last: Klein
  - first: Amit
    full: Amit Seker
    id: amit-seker
    last: Seker
  author_string: Reut Tsarfaty, Shoval Sadde, Stav Klein, Amit Seker
  bibkey: tsarfaty-etal-2019-whats
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP): System Demonstrations'
  booktitle_html: 'Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP): System Demonstrations'
  doi: 10.18653/v1/D19-3044
  month: November
  page_first: '259'
  page_last: '264'
  pages: "259\u2013264"
  paper_id: '44'
  parent_volume_id: D19-3
  pdf: https://www.aclweb.org/anthology/D19-3044.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-3044.jpg
  title: "What\u2019s Wrong with Hebrew NLP? And How to Make it Right"
  title_html: "What\u2019s Wrong with <span class=\"acl-fixed-case\">H</span>ebrew\
    \ <span class=\"acl-fixed-case\">NLP</span>? And How to Make it Right"
  url: https://www.aclweb.org/anthology/D19-3044
  year: '2019'
D19-5000:
  address: Hong Kong, China
  author:
  - first: Anna
    full: Anna Feldman
    id: anna-feldman
    last: Feldman
  - first: Giovanni
    full: Giovanni Da San Martino
    id: giovanni-da-san-martino
    last: Da San Martino
  - first: Alberto
    full: "Alberto Barr\xF3n-Cede\xF1o"
    id: alberto-barron-cedeno
    last: "Barr\xF3n-Cede\xF1o"
  - first: Chris
    full: Chris Brew
    id: chris-brew
    last: Brew
  - first: Chris
    full: Chris Leberknight
    id: chris-leberknight
    last: Leberknight
  - first: Preslav
    full: Preslav Nakov
    id: preslav-nakov
    last: Nakov
  author_string: "Anna Feldman, Giovanni Da San Martino, Alberto Barr\xF3n-Cede\xF1\
    o, Chris Brew, Chris Leberknight, Preslav Nakov"
  bibkey: emnlp-2019-natural
  bibtype: proceedings
  booktitle: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  month: November
  paper_id: '0'
  parent_volume_id: D19-50
  pdf: https://www.aclweb.org/anthology/D19-5000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5000.jpg
  title: 'Proceedings of the Second Workshop on Natural Language Processing for Internet
    Freedom: Censorship, Disinformation, and Propaganda'
  title_html: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  url: https://www.aclweb.org/anthology/D19-5000
  year: '2019'
D19-5001:
  abstract: Widespread Chinese social media applications such as Weibo are widely
    known for monitoring and deleting posts to conform to Chinese government requirements.
    In this paper, we focus on analyzing a dataset of censored and uncensored posts
    in Weibo. Despite previous work that only considers text content of posts, we
    take a multi-modal approach that takes into account both text and image content.
    We categorize this dataset into 14 categories that have the potential to be censored
    on Weibo, and seek to quantify censorship by topic. Specifically, we investigate
    how different factors interact to affect censorship. We also investigate how consistently
    and how quickly different topics are censored. To this end, we have assembled
    an image dataset with 18,966 images, as well as a text dataset with 994 posts
    from 14 categories. We then utilized deep learning, CNN localization, and NLP
    techniques to analyze the target dataset and extract categories, for further analysis
    to better understand censorship mechanisms in Weibo. We found that sentiment is
    the only indicator of censorship that is consistent across the variety of topics
    we identified. Our finding matches with recently leaked logs from Sina Weibo.
    We also discovered that most categories like those related to anti-government
    actions (e.g. protest) or categories related to politicians (e.g. Xi Jinping)
    are often censored, whereas some categories such as crisis-related categories
    (e.g. rainstorm) are less frequently censored. We also found that censored posts
    across all categories are deleted in three hours on average.
  address: Hong Kong, China
  author:
  - first: Meisam
    full: Meisam Navaki Arefi
    id: meisam-navaki-arefi
    last: Navaki Arefi
  - first: Rajkumar
    full: Rajkumar Pandi
    id: rajkumar-pandi
    last: Pandi
  - first: Michael Carl
    full: Michael Carl Tschantz
    id: michael-carl-tschantz
    last: Tschantz
  - first: Jedidiah R.
    full: Jedidiah R. Crandall
    id: jedidiah-r-crandall
    last: Crandall
  - first: King-wa
    full: King-wa Fu
    id: king-wa-fu
    last: Fu
  - first: Dahlia
    full: Dahlia Qiu Shi
    id: dahlia-qiu-shi
    last: Qiu Shi
  - first: Miao
    full: Miao Sha
    id: miao-sha
    last: Sha
  author_string: Meisam Navaki Arefi, Rajkumar Pandi, Michael Carl Tschantz, Jedidiah
    R. Crandall, King-wa Fu, Dahlia Qiu Shi, Miao Sha
  bibkey: navaki-arefi-etal-2019-assessing
  bibtype: inproceedings
  booktitle: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  booktitle_html: 'Proceedings of the Second Workshop on Natural Language Processing
    for Internet Freedom: Censorship, Disinformation, and Propaganda'
  doi: 10.18653/v1/D19-5001
  month: November
  page_first: '1'
  page_last: '9'
  pages: "1\u20139"
  paper_id: '1'
  parent_volume_id: D19-50
  pdf: https://www.aclweb.org/anthology/D19-5001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5001.jpg
  title: 'Assessing Post Deletion in Sina Weibo: Multi-modal Classification of Hot
    Topics'
  title_html: 'Assessing Post Deletion in Sina <span class="acl-fixed-case">W</span>eibo:
    Multi-modal Classification of Hot Topics'
  url: https://www.aclweb.org/anthology/D19-5001
  year: '2019'
D19-5002:
  abstract: Abusive text is a serious problem in social media and causes many issues
    among users as the number of users and the content volume increase. There are
    several attempts for detecting or preventing abusive text effectively. One simple
    yet effective approach is to use an abusive lexicon and determine the existence
    of an abusive word in text. This approach works well even when an abusive word
    is obfuscated. On the other hand, it is still a challenging problem to determine
    abusiveness in a text having no explicit abusive words. Especially, it is hard
    to identify sarcasm or offensiveness in context without any abusive words. We
    tackle this problem using an ensemble deep learning model. Our model consists
    of two parts of extracting local features and global features, which are crucial
    for identifying implicit abusiveness in context level. We evaluate our model using
    three benchmark data. Our model outperforms all the previous models for detecting
    abusiveness in a text data without abusive words. Furthermore, we combine our
    model and an abusive lexicon method. The experimental results show that our model
    has at least 4% better performance compared with the previous approaches for identifying
    text abusiveness in case of with/without abusive words.
  address: Hong Kong, China
  author:
  - first: Ju-Hyoung
    full: Ju-Hyoung Lee
    id: ju-hyoung-lee
    last: Lee
  - first: Jun-U
    full: Jun-U Park
    id: jun-u-park
    last: Park
  - first: Jeong-Won
    full: Jeong-Won Cha
    id: jeong-won-cha
    last: Cha
  - first: Yo-Sub
    full: Yo-Sub Han
    id: yo-sub-han
    last: Han
  author_string: Ju-Hyoung Lee, Jun-U Park, Jeong-Won Cha, Yo-Sub Han
  bibkey: lee-etal-2019-detecting
  bibtype: inproceedings
  booktitle: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  booktitle_html: 'Proceedings of the Second Workshop on Natural Language Processing
    for Internet Freedom: Censorship, Disinformation, and Propaganda'
  doi: 10.18653/v1/D19-5002
  month: November
  page_first: '10'
  page_last: '19'
  pages: "10\u201319"
  paper_id: '2'
  parent_volume_id: D19-50
  pdf: https://www.aclweb.org/anthology/D19-5002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5002.jpg
  title: Detecting context abusiveness using hierarchical deep learning
  title_html: Detecting context abusiveness using hierarchical deep learning
  url: https://www.aclweb.org/anthology/D19-5002
  year: '2019'
D19-5003:
  abstract: Social media has reportedly been (ab)used by Russian troll farms to promote
    political agendas. Specifically, state-affiliated actors disguise themselves as
    native citizens of the United States to promote discord and promote their political
    motives. Therefore, developing methods to automatically detect Russian trolls
    can ensure fair elections and possibly reduce political extremism by stopping
    trolls that produce discord. While data exists for some troll organizations (e.g.,
    Internet Research Agency), it is challenging to collect ground-truth accounts
    for new troll farms in a timely fashion. In this paper, we study the impact the
    number of labeled troll accounts has on detection performance. We analyze the
    use of self-supervision with less than 100 troll accounts as training data. We
    improve classification performance by nearly 4% F1. Furthermore, in combination
    with self-supervision, we also explore novel features for troll detection grounded
    in stylometry. Intuitively, we assume that the writing style is consistent across
    troll accounts because a single troll organization employee may control multiple
    user accounts. Overall, we improve on models based on words features by ~9% F1.
  address: Hong Kong, China
  author:
  - first: Nayeema
    full: Nayeema Nasrin
    id: nayeema-nasrin
    last: Nasrin
  - first: Kim-Kwang
    full: Kim-Kwang Raymond Choo
    id: kim-kwang-raymond-choo
    last: Raymond Choo
  - first: Myung
    full: Myung Ko
    id: myung-ko
    last: Ko
  - first: Anthony
    full: Anthony Rios
    id: anthony-rios
    last: Rios
  author_string: Nayeema Nasrin, Kim-Kwang Raymond Choo, Myung Ko, Anthony Rios
  bibkey: nasrin-etal-2019-many
  bibtype: inproceedings
  booktitle: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  booktitle_html: 'Proceedings of the Second Workshop on Natural Language Processing
    for Internet Freedom: Censorship, Disinformation, and Propaganda'
  doi: 10.18653/v1/D19-5003
  month: November
  page_first: '20'
  page_last: '30'
  pages: "20\u201330"
  paper_id: '3'
  parent_volume_id: D19-50
  pdf: https://www.aclweb.org/anthology/D19-5003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5003.jpg
  title: How Many Users Are Enough? Exploring Semi-Supervision and Stylometric Features
    to Uncover a Russian Troll Farm
  title_html: How Many Users Are Enough? Exploring Semi-Supervision and Stylometric
    Features to Uncover a <span class="acl-fixed-case">R</span>ussian Troll Farm
  url: https://www.aclweb.org/anthology/D19-5003
  year: '2019'
D19-5004:
  abstract: The blurry line between nefarious fake news and protected-speech satire
    has been a notorious struggle for social media platforms. Further to the efforts
    of reducing exposure to misinformation on social media, purveyors of fake news
    have begun to masquerade as satire sites to avoid being demoted. In this work,
    we address the challenge of automatically classifying fake news versus satire.
    Previous work have studied whether fake news and satire can be distinguished based
    on language differences. Contrary to fake news, satire stories are usually humorous
    and carry some political or social message. We hypothesize that these nuances
    could be identified using semantic and linguistic cues. Consequently, we train
    a machine learning method using semantic representation, with a state-of-the-art
    contextual language model, and with linguistic features based on textual coherence
    metrics. Empirical evaluation attests to the merits of our approach compared to
    the language-based baseline and sheds light on the nuances between fake news and
    satire. As avenues for future work, we consider studying additional linguistic
    features related to the humor aspect, and enriching the data with current news
    events, to help identify a political or social message.
  address: Hong Kong, China
  author:
  - first: Or
    full: Or Levi
    id: or-levi
    last: Levi
  - first: Pedram
    full: Pedram Hosseini
    id: pedram-hosseini
    last: Hosseini
  - first: Mona
    full: Mona Diab
    id: mona-diab
    last: Diab
  - first: David
    full: David Broniatowski
    id: david-broniatowski
    last: Broniatowski
  author_string: Or Levi, Pedram Hosseini, Mona Diab, David Broniatowski
  bibkey: levi-etal-2019-identifying
  bibtype: inproceedings
  booktitle: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  booktitle_html: 'Proceedings of the Second Workshop on Natural Language Processing
    for Internet Freedom: Censorship, Disinformation, and Propaganda'
  doi: 10.18653/v1/D19-5004
  month: November
  page_first: '31'
  page_last: '35'
  pages: "31\u201335"
  paper_id: '4'
  parent_volume_id: D19-50
  pdf: https://www.aclweb.org/anthology/D19-5004.pdf
  publisher: Association for Computational Linguistics
  revision:
  - id: '1'
    url: https://www.aclweb.org/anthology/D19-5004v1.pdf
    value: D19-5004v1
  - explanation: 'We corrected a few citations'' bibtex entry and made sure they''re
      based on the standard format of the official template.

      - We added a short footnote (#2) just to clarify how we calculated the baseline
      in our paper (this was already available in our GitHub but we thought it may
      be better to be also in our paper)

      - We paraphrased a few sentences in the related work section in one of the paragraphs
      to make sure they are distinguishable from the original paper.

      - We added one sentence to the caption of Table 3 to make sure our numbers can
      be clearly interpreted by the readers.'
    id: '2'
    url: https://www.aclweb.org/anthology/D19-5004v2.pdf
    value: D19-5004v2
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5004.jpg
  title: 'Identifying Nuances in Fake News vs. Satire: Using Semantic and Linguistic
    Cues'
  title_html: 'Identifying Nuances in Fake News vs. Satire: Using Semantic and Linguistic
    Cues'
  url: https://www.aclweb.org/anthology/D19-5004
  year: '2019'
D19-5005:
  abstract: Calls to action on social media are known to be effective means of mobilization
    in social movements, and a frequent target of censorship. We investigate the possibility
    of their automatic detection and their potential for predicting real-world protest
    events, on historical data of Bolotnaya protests in Russia (2011-2013). We find
    that political calls to action can be annotated and detected with relatively high
    accuracy, and that in our sample their volume has a moderate positive correlation
    with rally attendance.
  address: Hong Kong, China
  author:
  - first: Anna
    full: Anna Rogers
    id: anna-rogers
    last: Rogers
  - first: Olga
    full: Olga Kovaleva
    id: olga-kovaleva
    last: Kovaleva
  - first: Anna
    full: Anna Rumshisky
    id: anna-rumshisky
    last: Rumshisky
  author_string: Anna Rogers, Olga Kovaleva, Anna Rumshisky
  bibkey: rogers-etal-2019-calls
  bibtype: inproceedings
  booktitle: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  booktitle_html: 'Proceedings of the Second Workshop on Natural Language Processing
    for Internet Freedom: Censorship, Disinformation, and Propaganda'
  doi: 10.18653/v1/D19-5005
  month: November
  page_first: '36'
  page_last: '44'
  pages: "36\u201344"
  paper_id: '5'
  parent_volume_id: D19-50
  pdf: https://www.aclweb.org/anthology/D19-5005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5005.jpg
  title: 'Calls to Action on Social Media: Detection, Social Impact, and Censorship
    Potential'
  title_html: 'Calls to Action on Social Media: Detection, Social Impact, and Censorship
    Potential'
  url: https://www.aclweb.org/anthology/D19-5005
  year: '2019'
D19-5006:
  abstract: Digital media enables not only fast sharing of information, but also disinformation.
    One prominent case of an event leading to circulation of disinformation on social
    media is the MH17 plane crash. Studies analysing the spread of information about
    this event on Twitter have focused on small, manually annotated datasets, or used
    proxys for data annotation. In this work, we examine to what extent text classifiers
    can be used to label data for subsequent content analysis, in particular we focus
    on predicting pro-Russian and pro-Ukrainian Twitter content related to the MH17
    plane crash. Even though we find that a neural classifier improves over a hashtag
    based baseline, labeling pro-Russian and pro-Ukrainian content with high precision
    remains a challenging problem. We provide an error analysis underlining the difficulty
    of the task and identify factors that might help improve classification in future
    work. Finally, we show how the classifier can facilitate the annotation task for
    human annotators.
  address: Hong Kong, China
  author:
  - first: Mareike
    full: Mareike Hartmann
    id: mareike-hartmann
    last: Hartmann
  - first: Yevgeniy
    full: Yevgeniy Golovchenko
    id: yevgeniy-golovchenko
    last: Golovchenko
  - first: Isabelle
    full: Isabelle Augenstein
    id: isabelle-augenstein
    last: Augenstein
  author_string: Mareike Hartmann, Yevgeniy Golovchenko, Isabelle Augenstein
  bibkey: hartmann-etal-2019-mapping
  bibtype: inproceedings
  booktitle: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  booktitle_html: 'Proceedings of the Second Workshop on Natural Language Processing
    for Internet Freedom: Censorship, Disinformation, and Propaganda'
  doi: 10.18653/v1/D19-5006
  month: November
  page_first: '45'
  page_last: '55'
  pages: "45\u201355"
  paper_id: '6'
  parent_volume_id: D19-50
  pdf: https://www.aclweb.org/anthology/D19-5006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5006.jpg
  title: Mapping (Dis-)Information Flow about the MH17 Plane Crash
  title_html: Mapping (Dis-)Information Flow about the <span class="acl-fixed-case">MH</span>17
    Plane Crash
  url: https://www.aclweb.org/anthology/D19-5006
  year: '2019'
D19-5007:
  abstract: Considering diverse aspects of an argumentative issue is an essential
    step for mitigating a biased opinion and making reasonable decisions. A related
    generation model can produce flexible results that cover a wide range of topics,
    compared to the retrieval-based method that may show unstable performance for
    unseen data. In this paper, we study the problem of generating sentential arguments
    from multiple perspectives, and propose a neural method to address this problem.
    Our model, ArgDiver (Argument generation model from diverse perspectives), in
    a way a conversational system, successfully generates high-quality sentential
    arguments. At the same time, the automatically generated arguments by our model
    show a higher diversity than those generated by any other baseline models. We
    believe that our work provides evidence for the potential of a good generation
    model in providing diverse perspectives on a controversial topic.
  address: Hong Kong, China
  author:
  - first: ChaeHun
    full: ChaeHun Park
    id: chaehun-park
    last: Park
  - first: Wonsuk
    full: Wonsuk Yang
    id: wonsuk-yang
    last: Yang
  - first: Jong
    full: Jong Park
    id: jong-c-park
    last: Park
  author_string: ChaeHun Park, Wonsuk Yang, Jong Park
  bibkey: park-etal-2019-generating
  bibtype: inproceedings
  booktitle: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  booktitle_html: 'Proceedings of the Second Workshop on Natural Language Processing
    for Internet Freedom: Censorship, Disinformation, and Propaganda'
  doi: 10.18653/v1/D19-5007
  month: November
  page_first: '56'
  page_last: '65'
  pages: "56\u201365"
  paper_id: '7'
  parent_volume_id: D19-50
  pdf: https://www.aclweb.org/anthology/D19-5007.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5007.jpg
  title: Generating Sentential Arguments from Diverse Perspectives on Controversial
    Topic
  title_html: Generating Sentential Arguments from Diverse Perspectives on Controversial
    Topic
  url: https://www.aclweb.org/anthology/D19-5007
  year: '2019'
D19-5008:
  abstract: Social media platforms have been used for information and news gathering,
    and they are very valuable in many applications. However, they also lead to the
    spreading of rumors and fake news. Many efforts have been taken to detect and
    debunk rumors on social media by analyzing their content and social context using
    machine learning techniques. This paper gives an overview of the recent studies
    in the rumor detection field. It provides a comprehensive list of datasets used
    for rumor detection, and reviews the important studies based on what types of
    information they exploit and the approaches they take. And more importantly, we
    also present several new directions for future research.
  address: Hong Kong, China
  author:
  - first: Quanzhi
    full: Quanzhi Li
    id: quanzhi-li
    last: Li
  - first: Qiong
    full: Qiong Zhang
    id: qiong-zhang
    last: Zhang
  - first: Luo
    full: Luo Si
    id: luo-si
    last: Si
  - first: Yingchi
    full: Yingchi Liu
    id: yingchi-liu
    last: Liu
  author_string: Quanzhi Li, Qiong Zhang, Luo Si, Yingchi Liu
  bibkey: li-etal-2019-rumor
  bibtype: inproceedings
  booktitle: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  booktitle_html: 'Proceedings of the Second Workshop on Natural Language Processing
    for Internet Freedom: Censorship, Disinformation, and Propaganda'
  doi: 10.18653/v1/D19-5008
  month: November
  page_first: '66'
  page_last: '75'
  pages: "66\u201375"
  paper_id: '8'
  parent_volume_id: D19-50
  pdf: https://www.aclweb.org/anthology/D19-5008.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5008.jpg
  title: 'Rumor Detection on Social Media: Datasets, Methods and Opportunities'
  title_html: 'Rumor Detection on Social Media: Datasets, Methods and Opportunities'
  url: https://www.aclweb.org/anthology/D19-5008
  year: '2019'
D19-5009:
  abstract: "Many discussions on online platforms suffer from users offending others\
    \ by using abusive terminology, threatening each other, or being sarcastic. Since\
    \ an automatic detection of abusive language can support human moderators of online\
    \ discussion platforms, detecting abusiveness has recently received increased\
    \ attention. However, the existing approaches simply train one classifier for\
    \ the whole variety of abusiveness. In contrast, our approach is to distinguish\
    \ explicitly abusive cases from the more \u201Cshadowed\u201D ones. By dynamically\
    \ extending a lexicon of abusive terms (e.g., including new obfuscations of abusive\
    \ terms), our approach can support a moderator with explicit unraveled explanations\
    \ for why something was flagged as abusive: due to known explicitly abusive terms,\
    \ due to newly detected (obfuscated) terms, or due to shadowed cases."
  address: Hong Kong, China
  author:
  - first: Wei-Fan
    full: Wei-Fan Chen
    id: wei-fan-chen
    last: Chen
  - first: Khalid
    full: Khalid Al Khatib
    id: khalid-al-khatib
    last: Al Khatib
  - first: Matthias
    full: Matthias Hagen
    id: matthias-hagen
    last: Hagen
  - first: Henning
    full: Henning Wachsmuth
    id: henning-wachsmuth
    last: Wachsmuth
  - first: Benno
    full: Benno Stein
    id: benno-stein
    last: Stein
  author_string: Wei-Fan Chen, Khalid Al Khatib, Matthias Hagen, Henning Wachsmuth,
    Benno Stein
  bibkey: chen-etal-2019-unraveling
  bibtype: inproceedings
  booktitle: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  booktitle_html: 'Proceedings of the Second Workshop on Natural Language Processing
    for Internet Freedom: Censorship, Disinformation, and Propaganda'
  doi: 10.18653/v1/D19-5009
  month: November
  page_first: '76'
  page_last: '82'
  pages: "76\u201382"
  paper_id: '9'
  parent_volume_id: D19-50
  pdf: https://www.aclweb.org/anthology/D19-5009.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5009.jpg
  title: Unraveling the Search Space of Abusive Language in Wikipedia with Dynamic
    Lexicon Acquisition
  title_html: Unraveling the Search Space of Abusive Language in <span class="acl-fixed-case">W</span>ikipedia
    with Dynamic Lexicon Acquisition
  url: https://www.aclweb.org/anthology/D19-5009
  year: '2019'
D19-5010:
  abstract: The goal of fine-grained propaganda detection is to determine whether
    a given sentence uses propaganda techniques (sentence-level) or to recognize which
    techniques are used (fragment-level). This paper presents the sys- tem of our
    participation in the sentence-level subtask of the propaganda detection shared
    task. In order to better utilize the document information, we construct context-dependent
    input pairs (sentence-title pair and sentence- context pair) to fine-tune the
    pretrained BERT, and we also use the undersampling method to tackle the problem
    of imbalanced data.
  address: Hong Kong, China
  author:
  - first: Wenjun
    full: Wenjun Hou
    id: wenjun-hou
    last: Hou
  - first: Ying
    full: Ying Chen
    id: ying-chen
    last: Chen
  author_string: Wenjun Hou, Ying Chen
  bibkey: hou-chen-2019-caunlp
  bibtype: inproceedings
  booktitle: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  booktitle_html: 'Proceedings of the Second Workshop on Natural Language Processing
    for Internet Freedom: Censorship, Disinformation, and Propaganda'
  doi: 10.18653/v1/D19-5010
  month: November
  page_first: '83'
  page_last: '86'
  pages: "83\u201386"
  paper_id: '10'
  parent_volume_id: D19-50
  pdf: https://www.aclweb.org/anthology/D19-5010.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5010.jpg
  title: 'CAUnLP at NLP4IF 2019 Shared Task: Context-Dependent BERT for Sentence-Level
    Propaganda Detection'
  title_html: '<span class="acl-fixed-case">CAU</span>n<span class="acl-fixed-case">LP</span>
    at <span class="acl-fixed-case">NLP</span>4<span class="acl-fixed-case">IF</span>
    2019 Shared Task: Context-Dependent <span class="acl-fixed-case">BERT</span> for
    Sentence-Level Propaganda Detection'
  url: https://www.aclweb.org/anthology/D19-5010
  year: '2019'
D19-5011:
  abstract: "This paper presents the winning solution of the Fragment Level Classification\
    \ (FLC) task in the Fine Grained Propaganda Detection competition at the NLP4IF\u2019\
    19 workshop. The goal of the FLC task is to detect and classify textual segments\
    \ that correspond to one of the 18 given propaganda techniques in a news articles\
    \ dataset. The main idea of our solution is to perform word-level classification\
    \ using fine-tuned BERT, a popular pre-trained language model. Besides presenting\
    \ the model and its evaluation results, we also investigate the attention heads\
    \ in the model, which provide insights into what the model learns, as well as\
    \ aspects for potential improvements."
  address: Hong Kong, China
  author:
  - first: Shehel
    full: Shehel Yoosuf
    id: shehel-yoosuf
    last: Yoosuf
  - first: Yin
    full: Yin Yang
    id: yin-yang
    last: Yang
  author_string: Shehel Yoosuf, Yin Yang
  bibkey: yoosuf-yang-2019-fine
  bibtype: inproceedings
  booktitle: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  booktitle_html: 'Proceedings of the Second Workshop on Natural Language Processing
    for Internet Freedom: Censorship, Disinformation, and Propaganda'
  doi: 10.18653/v1/D19-5011
  month: November
  page_first: '87'
  page_last: '91'
  pages: "87\u201391"
  paper_id: '11'
  parent_volume_id: D19-50
  pdf: https://www.aclweb.org/anthology/D19-5011.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5011.jpg
  title: Fine-Grained Propaganda Detection with Fine-Tuned BERT
  title_html: Fine-Grained Propaganda Detection with Fine-Tuned <span class="acl-fixed-case">BERT</span>
  url: https://www.aclweb.org/anthology/D19-5011
  year: '2019'
D19-5012:
  abstract: This paper describes our system (MIC-CIS) details and results of participation
    in the fine grained propaganda detection shared task 2019. To address the tasks
    of sentence (SLC) and fragment level (FLC) propaganda detection, we explore different
    neural architectures (e.g., CNN, LSTM-CRF and BERT) and extract linguistic (e.g.,
    part-of-speech, named entity, readability, sentiment, emotion, etc.), layout and
    topical features. Specifically, we have designed multi-granularity and multi-tasking
    neural architectures to jointly perform both the sentence and fragment level propaganda
    detection. Additionally, we investigate different ensemble schemes such as majority-voting,
    relax-voting, etc. to boost overall system performance. Compared to the other
    participating systems, our submissions are ranked 3rd and 4th in FLC and SLC tasks,
    respectively.
  address: Hong Kong, China
  author:
  - first: Pankaj
    full: Pankaj Gupta
    id: pankaj-gupta
    last: Gupta
  - first: Khushbu
    full: Khushbu Saxena
    id: khushbu-saxena
    last: Saxena
  - first: Usama
    full: Usama Yaseen
    id: usama-yaseen
    last: Yaseen
  - first: Thomas
    full: Thomas Runkler
    id: thomas-runkler
    last: Runkler
  - first: Hinrich
    full: "Hinrich Sch\xFCtze"
    id: hinrich-schutze
    last: "Sch\xFCtze"
  author_string: "Pankaj Gupta, Khushbu Saxena, Usama Yaseen, Thomas Runkler, Hinrich\
    \ Sch\xFCtze"
  bibkey: gupta-etal-2019-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  booktitle_html: 'Proceedings of the Second Workshop on Natural Language Processing
    for Internet Freedom: Censorship, Disinformation, and Propaganda'
  doi: 10.18653/v1/D19-5012
  month: November
  page_first: '92'
  page_last: '97'
  pages: "92\u201397"
  paper_id: '12'
  parent_volume_id: D19-50
  pdf: https://www.aclweb.org/anthology/D19-5012.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5012.jpg
  title: Neural Architectures for Fine-Grained Propaganda Detection in News
  title_html: Neural Architectures for Fine-Grained Propaganda Detection in News
  url: https://www.aclweb.org/anthology/D19-5012
  year: '2019'
D19-5013:
  abstract: This paper presents the CUNLP submission for the NLP4IF 2019 shared-task
    on Fine-Grained Propaganda Detection. Our system finished 5th out of 26 teams
    on the sentence-level classification task and 5th out of 11 teams on the fragment-level
    classification task based on our scores on the blind test set. We present our
    models, a discussion of our ablation studies and experiments, and an analysis
    of our performance on all eighteen propaganda techniques present in the corpus
    of the shared task.
  address: Hong Kong, China
  author:
  - first: Tariq
    full: Tariq Alhindi
    id: tariq-alhindi
    last: Alhindi
  - first: Jonas
    full: Jonas Pfeiffer
    id: jonas-pfeiffer
    last: Pfeiffer
  - first: Smaranda
    full: Smaranda Muresan
    id: smaranda-muresan
    last: Muresan
  author_string: Tariq Alhindi, Jonas Pfeiffer, Smaranda Muresan
  bibkey: alhindi-etal-2019-fine
  bibtype: inproceedings
  booktitle: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  booktitle_html: 'Proceedings of the Second Workshop on Natural Language Processing
    for Internet Freedom: Censorship, Disinformation, and Propaganda'
  doi: 10.18653/v1/D19-5013
  month: November
  page_first: '98'
  page_last: '102'
  pages: "98\u2013102"
  paper_id: '13'
  parent_volume_id: D19-50
  pdf: https://www.aclweb.org/anthology/D19-5013.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5013.jpg
  title: Fine-Tuned Neural Models for Propaganda Detection at the Sentence and Fragment
    levels
  title_html: Fine-Tuned Neural Models for Propaganda Detection at the Sentence and
    Fragment levels
  url: https://www.aclweb.org/anthology/D19-5013
  year: '2019'
D19-5014:
  abstract: "On the NLP4IF 2019 sentence level propaganda classification task, we\
    \ used a BERT language model that was pre-trained on Wikipedia and BookCorpus\
    \ as team ltuorp ranking #1 of 26. It uses deep learning in the form of an attention\
    \ transformer. We substituted the final layer of the neural network to a linear\
    \ real valued output neuron from a layer of softmaxes. The backpropagation trained\
    \ the entire neural network and not just the last layer. Training took 3 epochs\
    \ and on our computation resources this took approximately one day. The pre-trained\
    \ model consisted of uncased words and there were 12-layers, 768-hidden neurons\
    \ with 12-heads for a total of 110 million parameters. The articles used in the\
    \ training data promote divisive language similar to state-actor-funded influence\
    \ operations on social media. Twitter shows state-sponsored examples designed\
    \ to maximize division occurring across political lines, ranging from \u201CObama\
    \ calls me a clinger, Hillary calls me deplorable, ... and Trump calls me an American\u201D\
    \ oriented to the political right, to Russian propaganda featuring \u201CBlack\
    \ Lives Matter\u201D material with suggestions of institutional racism in US police\
    \ forces oriented to the political left. We hope that raising awareness through\
    \ our work will reduce the polarizing dialogue for the betterment of nations."
  address: Hong Kong, China
  author:
  - first: Norman
    full: Norman Mapes
    id: norman-mapes
    last: Mapes
  - first: Anna
    full: Anna White
    id: anna-white
    last: White
  - first: Radhika
    full: Radhika Medury
    id: radhika-medury
    last: Medury
  - first: Sumeet
    full: Sumeet Dua
    id: sumeet-dua
    last: Dua
  author_string: Norman Mapes, Anna White, Radhika Medury, Sumeet Dua
  bibkey: mapes-etal-2019-divisive
  bibtype: inproceedings
  booktitle: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  booktitle_html: 'Proceedings of the Second Workshop on Natural Language Processing
    for Internet Freedom: Censorship, Disinformation, and Propaganda'
  doi: 10.18653/v1/D19-5014
  month: November
  page_first: '103'
  page_last: '106'
  pages: "103\u2013106"
  paper_id: '14'
  parent_volume_id: D19-50
  pdf: https://www.aclweb.org/anthology/D19-5014.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5014.jpg
  title: Divisive Language and Propaganda Detection using Multi-head Attention Transformers
    with Deep Learning BERT-based Language Models for Binary Classification
  title_html: Divisive Language and Propaganda Detection using Multi-head Attention
    Transformers with Deep Learning <span class="acl-fixed-case">BERT</span>-based
    Language Models for Binary Classification
  url: https://www.aclweb.org/anthology/D19-5014
  year: '2019'
D19-5015:
  abstract: "Bias is ubiquitous in most online sources of natural language, from news\
    \ media to social networks. Given the steady shift in news consumption behavior\
    \ from traditional outlets to online sources, the automatic detection of propaganda,\
    \ in which information is shaped to purposefully foster a predetermined agenda,\
    \ is an increasingly crucial task. To this goal, we explore the task of sentence-level\
    \ propaganda detection, and experiment with both handcrafted features and learned\
    \ dense semantic representations. We also experiment with random undersampling\
    \ of the majority class (non-propaganda) to curb the influence of class distribution\
    \ on the system\u2019s performance, leading to marked improvements on the minority\
    \ class (propaganda). Our best performing system uses pre-trained ELMo word embeddings,\
    \ followed by a bidirectional LSTM and an attention layer. We have submitted a\
    \ 5-model ensemble of our best performing system to the NLP4IF shared task on\
    \ sentence-level propaganda detection (team LIACC), achieving rank 10 among 25\
    \ participants, with 59.5 F1-score."
  address: Hong Kong, China
  author:
  - first: "Andr\xE9"
    full: "Andr\xE9 Ferreira Cruz"
    id: andre-ferreira-cruz
    last: Ferreira Cruz
  - first: Gil
    full: Gil Rocha
    id: gil-rocha
    last: Rocha
  - first: Henrique
    full: Henrique Lopes Cardoso
    id: henrique-lopes-cardoso
    last: Lopes Cardoso
  author_string: "Andr\xE9 Ferreira Cruz, Gil Rocha, Henrique Lopes Cardoso"
  bibkey: ferreira-cruz-etal-2019-sentence
  bibtype: inproceedings
  booktitle: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  booktitle_html: 'Proceedings of the Second Workshop on Natural Language Processing
    for Internet Freedom: Censorship, Disinformation, and Propaganda'
  doi: 10.18653/v1/D19-5015
  month: November
  page_first: '107'
  page_last: '112'
  pages: "107\u2013112"
  paper_id: '15'
  parent_volume_id: D19-50
  pdf: https://www.aclweb.org/anthology/D19-5015.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5015.jpg
  title: 'On Sentence Representations for Propaganda Detection: From Handcrafted Features
    to Word Embeddings'
  title_html: 'On Sentence Representations for Propaganda Detection: From Handcrafted
    Features to Word Embeddings'
  url: https://www.aclweb.org/anthology/D19-5015
  year: '2019'
D19-5016:
  abstract: "The internet and the high use of social media have enabled the modern-day\
    \ journalism to publish, share and spread news that is difficult to distinguish\
    \ if it is true or fake. Defining \u201Cfake news\u201D is not well established\
    \ yet, however, it can be categorized under several labels: false, biased, or\
    \ framed to mislead the readers that are characterized as propaganda. Digital\
    \ content production technologies with logical fallacies and emotional language\
    \ can be used as propaganda techniques to gain more readers or mislead the audience.\
    \ Recently, several researchers have proposed deep learning (DL) models to address\
    \ this issue. This research paper provides an ensemble deep learning model using\
    \ BiLSTM, XGBoost, and BERT to detect propaganda. The proposed model has been\
    \ applied on the dataset provided by the challenge NLP4IF 2019, Task 1 Sentence\
    \ Level Classification (SLC) and it shows a significant performance over the baseline\
    \ model."
  address: Hong Kong, China
  author:
  - first: Hani
    full: Hani Al-Omari
    id: hani-al-omari
    last: Al-Omari
  - first: Malak
    full: Malak Abdullah
    id: malak-abdullah
    last: Abdullah
  - first: Ola
    full: Ola AlTiti
    id: ola-altiti
    last: AlTiti
  - first: Samira
    full: Samira Shaikh
    id: samira-shaikh
    last: Shaikh
  author_string: Hani Al-Omari, Malak Abdullah, Ola AlTiti, Samira Shaikh
  bibkey: al-omari-etal-2019-justdeep
  bibtype: inproceedings
  booktitle: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  booktitle_html: 'Proceedings of the Second Workshop on Natural Language Processing
    for Internet Freedom: Censorship, Disinformation, and Propaganda'
  doi: 10.18653/v1/D19-5016
  month: November
  page_first: '113'
  page_last: '118'
  pages: "113\u2013118"
  paper_id: '16'
  parent_volume_id: D19-50
  pdf: https://www.aclweb.org/anthology/D19-5016.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5016.jpg
  title: 'JUSTDeep at NLP4IF 2019 Task 1: Propaganda Detection using Ensemble Deep
    Learning Models'
  title_html: '<span class="acl-fixed-case">JUSTD</span>eep at <span class="acl-fixed-case">NLP</span>4<span
    class="acl-fixed-case">IF</span> 2019 Task 1: Propaganda Detection using Ensemble
    Deep Learning Models'
  url: https://www.aclweb.org/anthology/D19-5016
  year: '2019'
D19-5017:
  abstract: Various propaganda techniques are used to manipulate peoples perspectives
    in order to foster a predetermined agenda such as by the use of logical fallacies
    or appealing to the emotions of the audience. In this paper, we develop a Logistic
    Regression-based tool that automatically classifies whether a sentence is propagandistic
    or not. We utilize features like TF-IDF, BERT vector, sentence length, readability
    grade level, emotion feature, LIWC feature and emphatic content feature to help
    us differentiate these two categories. The linguistic and semantic features combination
    results in 66.16% of F1 score, which outperforms the baseline hugely.
  address: Hong Kong, China
  author:
  - first: Jinfen
    full: Jinfen Li
    id: jinfen-li
    last: Li
  - first: Zhihao
    full: Zhihao Ye
    id: zhihao-ye
    last: Ye
  - first: Lu
    full: Lu Xiao
    id: lu-xiao
    last: Xiao
  author_string: Jinfen Li, Zhihao Ye, Lu Xiao
  bibkey: li-etal-2019-detection
  bibtype: inproceedings
  booktitle: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  booktitle_html: 'Proceedings of the Second Workshop on Natural Language Processing
    for Internet Freedom: Censorship, Disinformation, and Propaganda'
  doi: 10.18653/v1/D19-5017
  month: November
  page_first: '119'
  page_last: '124'
  pages: "119\u2013124"
  paper_id: '17'
  parent_volume_id: D19-50
  pdf: https://www.aclweb.org/anthology/D19-5017.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5017.jpg
  title: Detection of Propaganda Using Logistic Regression
  title_html: Detection of Propaganda Using Logistic Regression
  url: https://www.aclweb.org/anthology/D19-5017
  year: '2019'
D19-5018:
  abstract: The automatic identification of propaganda has gained significance in
    recent years due to technological and social changes in the way news is generated
    and consumed. That this task can be addressed effectively using BERT, a powerful
    new architecture which can be fine-tuned for text classification tasks, is not
    surprising. However, propaganda detection, like other tasks that deal with news
    documents and other forms of decontextualized social communication (e.g. sentiment
    analysis), inherently deals with data whose categories are simultaneously imbalanced
    and dissimilar. We show that BERT, while capable of handling imbalanced classes
    with no additional data augmentation, does not generalise well when the training
    and test data are sufficiently dissimilar (as is often the case with news sources,
    whose topics evolve over time). We show how to address this problem by providing
    a statistical measure of similarity between datasets and a method of incorporating
    cost-weighting into BERT when the training and test sets are dissimilar. We test
    these methods on the Propaganda Techniques Corpus (PTC) and achieve the second
    highest score on sentence-level propaganda classification.
  address: Hong Kong, China
  author:
  - first: Harish
    full: Harish Tayyar Madabushi
    id: harish-tayyar-madabushi
    last: Tayyar Madabushi
  - first: Elena
    full: Elena Kochkina
    id: elena-kochkina
    last: Kochkina
  - first: Michael
    full: Michael Castelle
    id: michael-castelle
    last: Castelle
  author_string: Harish Tayyar Madabushi, Elena Kochkina, Michael Castelle
  bibkey: tayyar-madabushi-etal-2019-cost
  bibtype: inproceedings
  booktitle: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  booktitle_html: 'Proceedings of the Second Workshop on Natural Language Processing
    for Internet Freedom: Censorship, Disinformation, and Propaganda'
  doi: 10.18653/v1/D19-5018
  month: November
  page_first: '125'
  page_last: '134'
  pages: "125\u2013134"
  paper_id: '18'
  parent_volume_id: D19-50
  pdf: https://www.aclweb.org/anthology/D19-5018.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5018.jpg
  title: Cost-Sensitive BERT for Generalisable Sentence Classification on Imbalanced
    Data
  title_html: Cost-Sensitive <span class="acl-fixed-case">BERT</span> for Generalisable
    Sentence Classification on Imbalanced Data
  url: https://www.aclweb.org/anthology/D19-5018
  year: '2019'
D19-5019:
  abstract: In this paper, we describe our system used in the shared task for fine-grained
    propaganda analysis at sentence level. Despite the challenging nature of the task,
    our pretrained BERT model (team YMJA) fine tuned on the training dataset provided
    by the shared task scored 0.62 F1 on the test set and ranked third among 25 teams
    who participated in the contest. We present a set of illustrative experiments
    to better understand the performance of our BERT model on this shared task. Further,
    we explore beyond the given dataset for false-positive cases that likely to be
    produced by our system. We show that despite the high performance on the given
    testset, our system may have the tendency of classifying opinion pieces as propaganda
    and cannot distinguish quotations of propaganda speech from actual usage of propaganda
    techniques.
  address: Hong Kong, China
  author:
  - first: Yiqing
    full: Yiqing Hua
    id: yiqing-hua
    last: Hua
  author_string: Yiqing Hua
  bibkey: hua-2019-understanding
  bibtype: inproceedings
  booktitle: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  booktitle_html: 'Proceedings of the Second Workshop on Natural Language Processing
    for Internet Freedom: Censorship, Disinformation, and Propaganda'
  doi: 10.18653/v1/D19-5019
  month: November
  page_first: '135'
  page_last: '138'
  pages: "135\u2013138"
  paper_id: '19'
  parent_volume_id: D19-50
  pdf: https://www.aclweb.org/anthology/D19-5019.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5019.jpg
  title: Understanding BERT performance in propaganda analysis
  title_html: Understanding <span class="acl-fixed-case">BERT</span> performance in
    propaganda analysis
  url: https://www.aclweb.org/anthology/D19-5019
  year: '2019'
D19-5020:
  abstract: "In this paper, we describe our team\u2019s effort on the fine-grained\
    \ propaganda detection on sentence level classification (SLC) task of NLP4IF 2019\
    \ workshop co-located with the EMNLP-IJCNLP 2019 conference. Our top performing\
    \ system results come from applying ensemble average on three pretrained models\
    \ to make their predictions. The first two models use the uncased and cased versions\
    \ of Bidirectional Encoder Representations from Transformers (BERT) (Devlin et\
    \ al., 2018) while the third model uses Universal Sentence Encoder (USE) (Cer\
    \ et al. 2018). Out of 26 participating teams, our system is ranked in the first\
    \ place with 68.8312 F1-score on the development dataset and in the sixth place\
    \ with 61.3870 F1-score on the testing dataset."
  address: Hong Kong, China
  author:
  - first: Ali
    full: Ali Fadel
    id: ali-fadel
    last: Fadel
  - first: Ibraheem
    full: Ibraheem Tuffaha
    id: ibraheem-tuffaha
    last: Tuffaha
  - first: Mahmoud
    full: Mahmoud Al-Ayyoub
    id: mahmoud-al-ayyoub
    last: Al-Ayyoub
  author_string: Ali Fadel, Ibraheem Tuffaha, Mahmoud Al-Ayyoub
  bibkey: fadel-etal-2019-pretrained
  bibtype: inproceedings
  booktitle: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  booktitle_html: 'Proceedings of the Second Workshop on Natural Language Processing
    for Internet Freedom: Censorship, Disinformation, and Propaganda'
  doi: 10.18653/v1/D19-5020
  month: November
  page_first: '139'
  page_last: '142'
  pages: "139\u2013142"
  paper_id: '20'
  parent_volume_id: D19-50
  pdf: https://www.aclweb.org/anthology/D19-5020.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5020.jpg
  title: Pretrained Ensemble Learning for Fine-Grained Propaganda Detection
  title_html: Pretrained Ensemble Learning for Fine-Grained Propaganda Detection
  url: https://www.aclweb.org/anthology/D19-5020
  year: '2019'
D19-5021:
  abstract: 'In this paper, we describe our approach and system description for NLP4IF
    2019 Workshop: Shared Task on Fine-Grained Propaganda Detection. Given a sentence
    from a news article, the task is to detect whether the sentence contains a propagandistic
    agenda or not. The main contribution of our work is to evaluate the effectiveness
    of various transfer learning approaches like ELMo, BERT, and RoBERTa for propaganda
    detection. We show the use of Document Embeddings on the top of Stacked Embeddings
    combined with LSTM for identification of propagandistic context in the sentence.
    We further provide analysis of these models to show the effect of oversampling
    on the provided dataset. In the final test-set evaluation, our system ranked 21st
    with F1-score of 0.43 in the SLC Task.'
  address: Hong Kong, China
  author:
  - first: Kartik
    full: Kartik Aggarwal
    id: kartik-aggarwal
    last: Aggarwal
  - first: Anubhav
    full: Anubhav Sadana
    id: anubhav-sadana
    last: Sadana
  author_string: Kartik Aggarwal, Anubhav Sadana
  bibkey: aggarwal-sadana-2019-nsit
  bibtype: inproceedings
  booktitle: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  booktitle_html: 'Proceedings of the Second Workshop on Natural Language Processing
    for Internet Freedom: Censorship, Disinformation, and Propaganda'
  doi: 10.18653/v1/D19-5021
  month: November
  page_first: '143'
  page_last: '147'
  pages: "143\u2013147"
  paper_id: '21'
  parent_volume_id: D19-50
  pdf: https://www.aclweb.org/anthology/D19-5021.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5021.jpg
  title: 'NSIT@NLP4IF-2019: Propaganda Detection from News Articles using Transfer
    Learning'
  title_html: '<span class="acl-fixed-case">NSIT</span>@<span class="acl-fixed-case">NLP</span>4<span
    class="acl-fixed-case">IF</span>-2019: Propaganda Detection from News Articles
    using Transfer Learning'
  url: https://www.aclweb.org/anthology/D19-5021
  year: '2019'
D19-5022:
  abstract: 'In recent years, the need for communication increased in online social
    media. Propaganda is a mechanism which was used throughout history to influence
    public opinion and it is gaining a new dimension with the rising interest of online
    social media. This paper presents our submission to NLP4IF-2019 Shared Task SLC:
    Sentence-level Propaganda Detection in news articles. The challenge of this task
    is to build a robust binary classifier able to provide corresponding propaganda
    labels, propaganda or non-propaganda. Our model relies on a unified neural network,
    which consists of several deep leaning modules, namely BERT, BiLSTM and Capsule,
    to solve the sentencelevel propaganda classification problem. In addition, we
    take a pre-training approach on a somewhat similar task (i.e., emotion classification)
    improving results against the cold-start model. Among the 26 participant teams
    in the NLP4IF-2019 Task SLC, our solution ranked 12th with an F1-score 0.5868
    on the official test data. Our proposed solution indicates promising results since
    our system significantly exceeds the baseline approach of the organizers by 0.1521
    and is slightly lower than the winning system by 0.0454.'
  address: Hong Kong, China
  author:
  - first: George-Alexandru
    full: George-Alexandru Vlad
    id: george-alexandru-vlad
    last: Vlad
  - first: Mircea-Adrian
    full: Mircea-Adrian Tanase
    id: mircea-adrian-tanase
    last: Tanase
  - first: Cristian
    full: Cristian Onose
    id: cristian-onose
    last: Onose
  - first: Dumitru-Clementin
    full: Dumitru-Clementin Cercel
    id: dumitru-clementin-cercel
    last: Cercel
  author_string: George-Alexandru Vlad, Mircea-Adrian Tanase, Cristian Onose, Dumitru-Clementin
    Cercel
  bibkey: vlad-etal-2019-sentence
  bibtype: inproceedings
  booktitle: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  booktitle_html: 'Proceedings of the Second Workshop on Natural Language Processing
    for Internet Freedom: Censorship, Disinformation, and Propaganda'
  doi: 10.18653/v1/D19-5022
  month: November
  page_first: '148'
  page_last: '154'
  pages: "148\u2013154"
  paper_id: '22'
  parent_volume_id: D19-50
  pdf: https://www.aclweb.org/anthology/D19-5022.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5022.jpg
  title: Sentence-Level Propaganda Detection in News Articles with Transfer Learning
    and BERT-BiLSTM-Capsule Model
  title_html: Sentence-Level Propaganda Detection in News Articles with Transfer Learning
    and <span class="acl-fixed-case">BERT</span>-<span class="acl-fixed-case">B</span>i<span
    class="acl-fixed-case">LSTM</span>-Capsule Model
  url: https://www.aclweb.org/anthology/D19-5022
  year: '2019'
D19-5023:
  abstract: This paper presents a method of detecting fine-grained categories of propaganda
    in text. Given a sentence, our method aims to identify a span of words and predict
    the type of propaganda used. To detect propaganda, we explore a method for extracting
    features of propaganda from contextualized embeddings without fine-tuning the
    large parameters of the base model. We show that by generating synthetic embeddings
    we can train a linear function with ReLU activation to extract useful labeled
    embeddings from an embedding space generated by a general-purpose language model.
    We also introduce an inference technique to detect continuous spans in sequences
    of propaganda tokens in sentences. A result of the ensemble model is submitted
    to the first shared task in fine-grained propaganda detection at NLP4IF as Team
    Stalin. In this paper, we provide additional analysis regarding our method of
    detecting spans of propaganda with synthetically generated representations.
  address: Hong Kong, China
  author:
  - first: Adam
    full: Adam Ek
    id: adam-ek
    last: Ek
  - first: Mehdi
    full: Mehdi Ghanimifard
    id: mehdi-ghanimifard
    last: Ghanimifard
  author_string: Adam Ek, Mehdi Ghanimifard
  bibkey: ek-ghanimifard-2019-synthetic
  bibtype: inproceedings
  booktitle: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  booktitle_html: 'Proceedings of the Second Workshop on Natural Language Processing
    for Internet Freedom: Censorship, Disinformation, and Propaganda'
  doi: 10.18653/v1/D19-5023
  month: November
  page_first: '155'
  page_last: '161'
  pages: "155\u2013161"
  paper_id: '23'
  parent_volume_id: D19-50
  pdf: https://www.aclweb.org/anthology/D19-5023.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5023.jpg
  title: Synthetic Propaganda Embeddings To Train A Linear Projection
  title_html: Synthetic Propaganda Embeddings To Train A Linear Projection
  url: https://www.aclweb.org/anthology/D19-5023
  year: '2019'
D19-5024:
  abstract: We present the shared task on Fine-Grained Propaganda Detection, which
    was organized as part of the NLP4IF workshop at EMNLP-IJCNLP 2019. There were
    two subtasks. FLC is a fragment-level task that asks for the identification of
    propagandist text fragments in a news article and also for the prediction of the
    specific propaganda technique used in each such fragment (18-way classification
    task). SLC is a sentence-level binary classification task asking to detect the
    sentences that contain propaganda. A total of 12 teams submitted systems for the
    FLC task, 25 teams did so for the SLC task, and 14 teams eventually submitted
    a system description paper. For both subtasks, most systems managed to beat the
    baseline by a sizable margin. The leaderboard and the data from the competition
    are available at http://propaganda.qcri.org/nlp4if-shared-task/.
  address: Hong Kong, China
  author:
  - first: Giovanni
    full: Giovanni Da San Martino
    id: giovanni-da-san-martino
    last: Da San Martino
  - first: Alberto
    full: "Alberto Barr\xF3n-Cede\xF1o"
    id: alberto-barron-cedeno
    last: "Barr\xF3n-Cede\xF1o"
  - first: Preslav
    full: Preslav Nakov
    id: preslav-nakov
    last: Nakov
  author_string: "Giovanni Da San Martino, Alberto Barr\xF3n-Cede\xF1o, Preslav Nakov"
  bibkey: da-san-martino-etal-2019-findings
  bibtype: inproceedings
  booktitle: 'Proceedings of the Second Workshop on Natural Language Processing for
    Internet Freedom: Censorship, Disinformation, and Propaganda'
  booktitle_html: 'Proceedings of the Second Workshop on Natural Language Processing
    for Internet Freedom: Censorship, Disinformation, and Propaganda'
  doi: 10.18653/v1/D19-5024
  month: November
  page_first: '162'
  page_last: '170'
  pages: "162\u2013170"
  paper_id: '24'
  parent_volume_id: D19-50
  pdf: https://www.aclweb.org/anthology/D19-5024.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5024.jpg
  title: Findings of the NLP4IF-2019 Shared Task on Fine-Grained Propaganda Detection
  title_html: Findings of the <span class="acl-fixed-case">NLP</span>4<span class="acl-fixed-case">IF</span>-2019
    Shared Task on Fine-Grained Propaganda Detection
  url: https://www.aclweb.org/anthology/D19-5024
  year: '2019'
D19-5100:
  address: Hong Kong
  author:
  - first: Udo
    full: Udo Hahn
    id: udo-hahn
    last: Hahn
  - first: "V\xE9ronique"
    full: "V\xE9ronique Hoste"
    id: veronique-hoste
    last: Hoste
  - first: Zhu
    full: Zhu Zhang
    id: zhu-zhang
    last: Zhang
  author_string: "Udo Hahn, V\xE9ronique Hoste, Zhu Zhang"
  bibkey: emnlp-2019-economics
  bibtype: proceedings
  booktitle: Proceedings of the Second Workshop on Economics and Natural Language
    Processing
  month: November
  paper_id: '0'
  parent_volume_id: D19-51
  pdf: https://www.aclweb.org/anthology/D19-5100.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5100.jpg
  title: Proceedings of the Second Workshop on Economics and Natural Language Processing
  title_html: Proceedings of the Second Workshop on Economics and Natural Language
    Processing
  url: https://www.aclweb.org/anthology/D19-5100
  year: '2019'
D19-5101:
  abstract: In order to automate banking processes (e.g. payments, money transfers,
    foreign trade), we need to extract banking transactions from different types of
    mediums such as faxes, e-mails, and scanners. Banking orders may be considered
    as complex documents since they contain quite complex relations compared to traditional
    datasets used in relation extraction research. In this paper, we present our method
    to extract intersentential, nested and complex relations from banking orders,
    and introduce a relation extraction method based on maximal clique factorization
    technique. We demonstrate 11% error reduction over previous methods.
  address: Hong Kong
  author:
  - first: Berke
    full: Berke Oral
    id: berke-oral
    last: Oral
  - first: Erdem
    full: Erdem Emekligil
    id: erdem-emekligil
    last: Emekligil
  - first: "Se\xE7il"
    full: "Se\xE7il Arslan"
    id: secil-arslan
    last: Arslan
  - first: "G\xFCl\u015Fen"
    full: "G\xFCl\u015Fen Eryi\u011Fit"
    id: gulsen-eryigit
    last: "Eryi\u011Fit"
  author_string: "Berke Oral, Erdem Emekligil, Se\xE7il Arslan, G\xFCl\u015Fen Eryi\u011F\
    it"
  bibkey: oral-etal-2019-extracting
  bibtype: inproceedings
  booktitle: Proceedings of the Second Workshop on Economics and Natural Language
    Processing
  booktitle_html: Proceedings of the Second Workshop on Economics and Natural Language
    Processing
  doi: 10.18653/v1/D19-5101
  month: November
  page_first: '1'
  page_last: '9'
  pages: "1\u20139"
  paper_id: '1'
  parent_volume_id: D19-51
  pdf: https://www.aclweb.org/anthology/D19-5101.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5101.jpg
  title: Extracting Complex Relations from Banking Documents
  title_html: Extracting Complex Relations from Banking Documents
  url: https://www.aclweb.org/anthology/D19-5101
  year: '2019'
D19-5102:
  abstract: Extraction of financial and economic events from text has previously been
    done mostly using rule-based methods, with more recent works employing machine
    learning techniques. This work is in line with this latter approach, leveraging
    relevant Wikipedia sections to extract weak labels for sentences describing economic
    events. Whereas previous weakly supervised approaches required a knowledge-base
    of such events, or corresponding financial figures, our approach requires no such
    additional data, and can be employed to extract economic events related to companies
    which are not even mentioned in the training data.
  address: Hong Kong
  author:
  - first: Liat
    full: Liat Ein-Dor
    id: liat-ein-dor
    last: Ein-Dor
  - first: Ariel
    full: Ariel Gera
    id: ariel-gera
    last: Gera
  - first: Orith
    full: Orith Toledo-Ronen
    id: orith-toledo-ronen
    last: Toledo-Ronen
  - first: Alon
    full: Alon Halfon
    id: alon-halfon
    last: Halfon
  - first: Benjamin
    full: Benjamin Sznajder
    id: benjamin-sznajder
    last: Sznajder
  - first: Lena
    full: Lena Dankin
    id: lena-dankin
    last: Dankin
  - first: Yonatan
    full: Yonatan Bilu
    id: yonatan-bilu
    last: Bilu
  - first: Yoav
    full: Yoav Katz
    id: yoav-katz
    last: Katz
  - first: Noam
    full: Noam Slonim
    id: noam-slonim
    last: Slonim
  author_string: Liat Ein-Dor, Ariel Gera, Orith Toledo-Ronen, Alon Halfon, Benjamin
    Sznajder, Lena Dankin, Yonatan Bilu, Yoav Katz, Noam Slonim
  bibkey: ein-dor-etal-2019-financial
  bibtype: inproceedings
  booktitle: Proceedings of the Second Workshop on Economics and Natural Language
    Processing
  booktitle_html: Proceedings of the Second Workshop on Economics and Natural Language
    Processing
  doi: 10.18653/v1/D19-5102
  month: November
  page_first: '10'
  page_last: '15'
  pages: "10\u201315"
  paper_id: '2'
  parent_volume_id: D19-51
  pdf: https://www.aclweb.org/anthology/D19-5102.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5102.jpg
  title: Financial Event Extraction Using Wikipedia-Based Weak Supervision
  title_html: Financial Event Extraction Using <span class="acl-fixed-case">W</span>ikipedia-Based
    Weak Supervision
  url: https://www.aclweb.org/anthology/D19-5102
  year: '2019'
D19-5103:
  abstract: "We examine the affective content of central bank press statements using\
    \ emotion analysis. Our focus is on two major international players, the European\
    \ Central Bank (ECB) and the US Federal Reserve Bank (Fed), covering a time span\
    \ from 1998 through 2019. We reveal characteristic patterns in the emotional dimensions\
    \ of valence, arousal, and dominance and find\u2014despite the commonly established\
    \ attitude that emotional wording in central bank communication should be avoided\u2014\
    a correlation between the state of the economy and particularly the dominance\
    \ dimension in the press releases under scrutiny and, overall, an impact of the\
    \ president in office."
  address: Hong Kong
  author:
  - first: Sven
    full: Sven Buechel
    id: sven-buechel
    last: Buechel
  - first: Simon
    full: Simon Junker
    id: simon-junker
    last: Junker
  - first: Thore
    full: Thore Schlaak
    id: thore-schlaak
    last: Schlaak
  - first: Claus
    full: Claus Michelsen
    id: claus-michelsen
    last: Michelsen
  - first: Udo
    full: Udo Hahn
    id: udo-hahn
    last: Hahn
  author_string: Sven Buechel, Simon Junker, Thore Schlaak, Claus Michelsen, Udo Hahn
  bibkey: buechel-etal-2019-time
  bibtype: inproceedings
  booktitle: Proceedings of the Second Workshop on Economics and Natural Language
    Processing
  booktitle_html: Proceedings of the Second Workshop on Economics and Natural Language
    Processing
  doi: 10.18653/v1/D19-5103
  month: November
  page_first: '16'
  page_last: '21'
  pages: "16\u201321"
  paper_id: '3'
  parent_volume_id: D19-51
  pdf: https://www.aclweb.org/anthology/D19-5103.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5103.jpg
  title: A Time Series Analysis of Emotional Loading in Central Bank Statements
  title_html: A Time Series Analysis of Emotional Loading in Central Bank Statements
  url: https://www.aclweb.org/anthology/D19-5103
  year: '2019'
D19-5104:
  abstract: "In this paper, we show deep learning models can be used to forecast firm\
    \ material event sequences based on the contents in the company\u2019s 8-K Current\
    \ Reports. Specifically, we exploit state-of-the-art neural architectures, including\
    \ sequence-to-sequence (Seq2Seq) architecture and attention mechanisms, in the\
    \ model. Our 8K-powered deep learning model demonstrates promising performance\
    \ in forecasting firm future event sequences. The model is poised to benefit various\
    \ stakeholders, including management and investors, by facilitating risk management\
    \ and decision making."
  address: Hong Kong
  author:
  - first: Shuang (Sophie)
    full: Shuang (Sophie) Zhai
    id: shuang-sophie-zhai
    last: Zhai
  - first: Zhu (Drew)
    full: Zhu (Drew) Zhang
    id: zhu-drew-zhang
    last: Zhang
  author_string: Shuang (Sophie) Zhai, Zhu (Drew) Zhang
  bibkey: zhai-zhang-2019-forecasting
  bibtype: inproceedings
  booktitle: Proceedings of the Second Workshop on Economics and Natural Language
    Processing
  booktitle_html: Proceedings of the Second Workshop on Economics and Natural Language
    Processing
  doi: 10.18653/v1/D19-5104
  month: November
  page_first: '22'
  page_last: '30'
  pages: "22\u201330"
  paper_id: '4'
  parent_volume_id: D19-51
  pdf: https://www.aclweb.org/anthology/D19-5104.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5104.jpg
  title: Forecasting Firm Material Events from 8-K Reports
  title_html: Forecasting Firm Material Events from 8-K Reports
  url: https://www.aclweb.org/anthology/D19-5104
  year: '2019'
D19-5105:
  abstract: Considering event structure information has proven helpful in text-based
    stock movement prediction. However, existing works mainly adopt the coarse-grained
    events, which loses the specific semantic information of diverse event types.
    In this work, we propose to incorporate the fine-grained events in stock movement
    prediction. Firstly, we propose a professional finance event dictionary built
    by domain experts and use it to extract fine-grained events automatically from
    finance news. Then we design a neural model to combine finance news with fine-grained
    event structure and stock trade data to predict the stock movement. Besides, in
    order to improve the generalizability of the proposed method, we design an advanced
    model that uses the extracted fine-grained events as the distant supervised label
    to train a multi-task framework of event extraction and stock prediction. The
    experimental results show that our method outperforms all the baselines and has
    good generalizability.
  address: Hong Kong
  attachment:
  - filename: D19-5105.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5105.Attachment.zip
  author:
  - first: Deli
    full: Deli Chen
    id: deli-chen
    last: Chen
  - first: Yanyan
    full: Yanyan Zou
    id: yanyan-zou
    last: Zou
  - first: Keiko
    full: Keiko Harimoto
    id: keiko-harimoto
    last: Harimoto
  - first: Ruihan
    full: Ruihan Bao
    id: ruihan-bao
    last: Bao
  - first: Xuancheng
    full: Xuancheng Ren
    id: xuancheng-ren
    last: Ren
  - first: Xu
    full: Xu Sun
    id: xu-sun
    last: Sun
  author_string: Deli Chen, Yanyan Zou, Keiko Harimoto, Ruihan Bao, Xuancheng Ren,
    Xu Sun
  bibkey: chen-etal-2019-incorporating
  bibtype: inproceedings
  booktitle: Proceedings of the Second Workshop on Economics and Natural Language
    Processing
  booktitle_html: Proceedings of the Second Workshop on Economics and Natural Language
    Processing
  doi: 10.18653/v1/D19-5105
  month: November
  page_first: '31'
  page_last: '40'
  pages: "31\u201340"
  paper_id: '5'
  parent_volume_id: D19-51
  pdf: https://www.aclweb.org/anthology/D19-5105.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5105.jpg
  title: Incorporating Fine-grained Events in Stock Movement Prediction
  title_html: Incorporating Fine-grained Events in Stock Movement Prediction
  url: https://www.aclweb.org/anthology/D19-5105
  year: '2019'
D19-5106:
  abstract: 'Incorporating related text information has proven successful in stock
    market prediction. However, it is a huge challenge to utilize texts in the enormous
    forex (foreign currency exchange) market because the associated texts are too
    redundant. In this work, we propose a BERT-based Hierarchical Aggregation Model
    to summarize a large amount of finance news to predict forex movement. We firstly
    group news from different aspects: time, topic and category. Then we extract the
    most crucial news in each group by the SOTA extractive summarization method. Finally,
    we conduct interaction between the news and the trade data with attention to predict
    the forex movement. The experimental results show that the category based method
    performs best among three grouping methods and outperforms all the baselines.
    Besides, we study the influence of essential news attributes (category and region)
    by statistical analysis and summarize the influence patterns for different currency
    pairs.'
  address: Hong Kong
  attachment:
  - filename: D19-5106.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5106.Attachment.zip
  author:
  - first: Deli
    full: Deli Chen
    id: deli-chen
    last: Chen
  - first: Shuming
    full: Shuming Ma
    id: shuming-ma
    last: Ma
  - first: Keiko
    full: Keiko Harimoto
    id: keiko-harimoto
    last: Harimoto
  - first: Ruihan
    full: Ruihan Bao
    id: ruihan-bao
    last: Bao
  - first: Qi
    full: Qi Su
    id: qi-su
    last: Su
  - first: Xu
    full: Xu Sun
    id: xu-sun
    last: Sun
  author_string: Deli Chen, Shuming Ma, Keiko Harimoto, Ruihan Bao, Qi Su, Xu Sun
  bibkey: chen-etal-2019-group
  bibtype: inproceedings
  booktitle: Proceedings of the Second Workshop on Economics and Natural Language
    Processing
  booktitle_html: Proceedings of the Second Workshop on Economics and Natural Language
    Processing
  doi: 10.18653/v1/D19-5106
  month: November
  page_first: '41'
  page_last: '50'
  pages: "41\u201350"
  paper_id: '6'
  parent_volume_id: D19-51
  pdf: https://www.aclweb.org/anthology/D19-5106.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5106.jpg
  title: 'Group, Extract and Aggregate: Summarizing a Large Amount of Finance News
    for Forex Movement Prediction'
  title_html: 'Group, Extract and Aggregate: Summarizing a Large Amount of Finance
    News for Forex Movement Prediction'
  url: https://www.aclweb.org/anthology/D19-5106
  year: '2019'
D19-5107:
  abstract: 'Governmental institutions are employing artificial intelligence techniques
    to deal with their specific problems and exploit their huge amounts of both structured
    and unstructured information. In particular, natural language processing and machine
    learning techniques are being used to process citizen feedback. In this paper,
    we report on the use of such techniques for analyzing and classifying complaints,
    in the context of the Portuguese Economic and Food Safety Authority. Grounded
    in its operational process, we address three different classification problems:
    target economic activity, implied infraction severity level, and institutional
    competence. We show promising results obtained using feature-based approaches
    and traditional classifiers, with accuracy scores above 70%, and analyze the shortcomings
    of our current results and avenues for further improvement, taking into account
    the intended use of our classifiers in helping human officers to cope with thousands
    of yearly complaints.'
  address: Hong Kong
  author:
  - first: "Jo\xE3o"
    full: "Jo\xE3o Filgueiras"
    id: joao-filgueiras
    last: Filgueiras
  - first: "Lu\xEDs"
    full: "Lu\xEDs Barbosa"
    id: luis-barbosa
    last: Barbosa
  - first: Gil
    full: Gil Rocha
    id: gil-rocha
    last: Rocha
  - first: Henrique
    full: Henrique Lopes Cardoso
    id: henrique-lopes-cardoso
    last: Lopes Cardoso
  - first: "Lu\xEDs Paulo"
    full: "Lu\xEDs Paulo Reis"
    id: luis-paulo-reis
    last: Reis
  - first: "Jo\xE3o Pedro"
    full: "Jo\xE3o Pedro Machado"
    id: joao-pedro-machado
    last: Machado
  - first: Ana Maria
    full: Ana Maria Oliveira
    id: ana-maria-oliveira
    last: Oliveira
  author_string: "Jo\xE3o Filgueiras, Lu\xEDs Barbosa, Gil Rocha, Henrique Lopes Cardoso,\
    \ Lu\xEDs Paulo Reis, Jo\xE3o Pedro Machado, Ana Maria Oliveira"
  bibkey: filgueiras-etal-2019-complaint
  bibtype: inproceedings
  booktitle: Proceedings of the Second Workshop on Economics and Natural Language
    Processing
  booktitle_html: Proceedings of the Second Workshop on Economics and Natural Language
    Processing
  doi: 10.18653/v1/D19-5107
  month: November
  page_first: '51'
  page_last: '60'
  pages: "51\u201360"
  paper_id: '7'
  parent_volume_id: D19-51
  pdf: https://www.aclweb.org/anthology/D19-5107.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5107.jpg
  title: Complaint Analysis and Classification for Economic and Food Safety
  title_html: Complaint Analysis and Classification for Economic and Food Safety
  url: https://www.aclweb.org/anthology/D19-5107
  year: '2019'
D19-5108:
  abstract: With conversational agents or chatbots making up in quantity of replies
    rather than quality, the need to identify user intent has become a main concern
    to improve these agents. Dialog act (DA) classification tackles this concern,
    and while existing studies have already addressed DA classification in general
    contexts, no training corpora in the context of e-commerce is available to the
    public. This research addressed the said insufficiency by building a text-based
    corpus of 7,265 posts from the question and answer section of products on Lazada
    Philippines. The SWBD-DAMSL tagset for DA classification was modified to 28 tags
    fitting the categories applicable to e-commerce conversations. The posts were
    annotated manually by three (3) human annotators and preprocessing techniques
    decreased the vocabulary size from 6,340 to 1,134. After analysis, the corpus
    was composed dominantly of single-label posts, with 34% of the corpus having multiple
    intent tags. The annotated corpus allowed insights toward the structure of posts
    created with single to multiple intents.
  address: Hong Kong
  attachment:
  - filename: D19-5108.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5108.Attachment.zip
  author:
  - first: Jared
    full: Jared Rivera
    id: jared-rivera
    last: Rivera
  - first: Jan Caleb Oliver
    full: Jan Caleb Oliver Pensica
    id: jan-caleb-oliver-pensica
    last: Pensica
  - first: Jolene
    full: Jolene Valenzuela
    id: jolene-valenzuela
    last: Valenzuela
  - first: Alfonso
    full: Alfonso Secuya
    id: alfonso-secuya
    last: Secuya
  - first: Charibeth
    full: Charibeth Cheng
    id: charibeth-cheng
    last: Cheng
  author_string: Jared Rivera, Jan Caleb Oliver Pensica, Jolene Valenzuela, Alfonso
    Secuya, Charibeth Cheng
  bibkey: rivera-etal-2019-annotation
  bibtype: inproceedings
  booktitle: Proceedings of the Second Workshop on Economics and Natural Language
    Processing
  booktitle_html: Proceedings of the Second Workshop on Economics and Natural Language
    Processing
  doi: 10.18653/v1/D19-5108
  month: November
  page_first: '61'
  page_last: '68'
  pages: "61\u201368"
  paper_id: '8'
  parent_volume_id: D19-51
  pdf: https://www.aclweb.org/anthology/D19-5108.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5108.jpg
  title: Annotation Process for the Dialog Act Classification of a Taglish E-commerce
    Q&A Corpus
  title_html: Annotation Process for the Dialog Act Classification of a Taglish E-commerce
    Q&amp;A Corpus
  url: https://www.aclweb.org/anthology/D19-5108
  year: '2019'
D19-5200:
  address: Hong Kong, China
  author:
  - first: Toshiaki
    full: Toshiaki Nakazawa
    id: toshiaki-nakazawa
    last: Nakazawa
  - first: Chenchen
    full: Chenchen Ding
    id: chenchen-ding
    last: Ding
  - first: Raj
    full: Raj Dabre
    id: raj-dabre
    last: Dabre
  - first: Anoop
    full: Anoop Kunchukuttan
    id: anoop-kunchukuttan
    last: Kunchukuttan
  - first: Nobushige
    full: Nobushige Doi
    id: nobushige-doi
    last: Doi
  - first: Yusuke
    full: Yusuke Oda
    id: yusuke-oda
    last: Oda
  - first: "Ond\u0159ej"
    full: "Ond\u0159ej Bojar"
    id: ondrej-bojar
    last: Bojar
  - first: Shantipriya
    full: Shantipriya Parida
    id: shantipriya-parida
    last: Parida
  - first: Isao
    full: Isao Goto
    id: isao-goto
    last: Goto
  - first: Hidaya
    full: Hidaya Mino
    id: hidaya-mino
    last: Mino
  author_string: "Toshiaki Nakazawa, Chenchen Ding, Raj Dabre, Anoop Kunchukuttan,\
    \ Nobushige Doi, Yusuke Oda, Ond\u0159ej Bojar, Shantipriya Parida, Isao Goto,\
    \ Hidaya Mino"
  bibkey: emnlp-2019-asian
  bibtype: proceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  month: November
  paper_id: '0'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5200.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5200.jpg
  title: Proceedings of the 6th Workshop on Asian Translation
  title_html: Proceedings of the 6th Workshop on Asian Translation
  url: https://www.aclweb.org/anthology/D19-5200
  year: '2019'
D19-5201:
  abstract: "This paper presents the results of the shared tasks from the 6th workshop\
    \ on Asian translation (WAT2019) including Ja\u2194En, Ja\u2194Zh scientific paper\
    \ translation subtasks, Ja\u2194En, Ja\u2194Ko, Ja\u2194En patent translation\
    \ subtasks, Hi\u2194En, My\u2194En, Km\u2194En, Ta\u2194En mixed domain subtasks\
    \ and Ru\u2194Ja news commentary translation task. For the WAT2019, 25 teams participated\
    \ in the shared tasks. We also received 10 research paper submissions out of which\
    \ 61 were accepted. About 400 translation results were submitted to the automatic\
    \ evaluation server, and selected submis- sions were manually evaluated."
  address: Hong Kong, China
  author:
  - first: Toshiaki
    full: Toshiaki Nakazawa
    id: toshiaki-nakazawa
    last: Nakazawa
  - first: Nobushige
    full: Nobushige Doi
    id: nobushige-doi
    last: Doi
  - first: Shohei
    full: Shohei Higashiyama
    id: shohei-higashiyama
    last: Higashiyama
  - first: Chenchen
    full: Chenchen Ding
    id: chenchen-ding
    last: Ding
  - first: Raj
    full: Raj Dabre
    id: raj-dabre
    last: Dabre
  - first: Hideya
    full: Hideya Mino
    id: hideya-mino
    last: Mino
  - first: Isao
    full: Isao Goto
    id: isao-goto
    last: Goto
  - first: Win Pa
    full: Win Pa Pa
    id: win-pa-pa
    last: Pa
  - first: Anoop
    full: Anoop Kunchukuttan
    id: anoop-kunchukuttan
    last: Kunchukuttan
  - first: Shantipriya
    full: Shantipriya Parida
    id: shantipriya-parida
    last: Parida
  - first: "Ond\u0159ej"
    full: "Ond\u0159ej Bojar"
    id: ondrej-bojar
    last: Bojar
  - first: Sadao
    full: Sadao Kurohashi
    id: sadao-kurohashi
    last: Kurohashi
  author_string: "Toshiaki Nakazawa, Nobushige Doi, Shohei Higashiyama, Chenchen Ding,\
    \ Raj Dabre, Hideya Mino, Isao Goto, Win Pa Pa, Anoop Kunchukuttan, Shantipriya\
    \ Parida, Ond\u0159ej Bojar, Sadao Kurohashi"
  bibkey: nakazawa-etal-2019-overview
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5201
  month: November
  page_first: '1'
  page_last: '35'
  pages: "1\u201335"
  paper_id: '1'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5201.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5201.jpg
  title: Overview of the 6th Workshop on Asian Translation
  title_html: Overview of the 6th Workshop on <span class="acl-fixed-case">A</span>sian
    Translation
  url: https://www.aclweb.org/anthology/D19-5201
  year: '2019'
D19-5202:
  abstract: Character-level translation has been proved to be able to achieve preferable
    translation quality without explicit segmentation, but training a character-level
    model needs a lot of hardware resources. In this paper, we introduced two character-level
    translation models which are mid-gated model and multi-attention model for Japanese-English
    translation. We showed that the mid-gated model achieved the better performance
    with respect to BLEU scores. We also showed that a relatively narrow beam of width
    4 or 5 was sufficient for the mid-gated model. As for unknown words, we showed
    that the mid-gated model could somehow translate the one containing Katakana by
    coining out a close word. We also showed that the model managed to produce tolerable
    results for heavily noised sentences, even though the model was trained with the
    dataset without noise.
  address: Hong Kong, China
  author:
  - first: Jinan
    full: Jinan Dai
    id: jinan-dai
    last: Dai
  - first: Kazunori
    full: Kazunori Yamaguchi
    id: kazunori-yamaguchi
    last: Yamaguchi
  author_string: Jinan Dai, Kazunori Yamaguchi
  bibkey: dai-yamaguchi-2019-compact
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5202
  month: November
  page_first: '36'
  page_last: '44'
  pages: "36\u201344"
  paper_id: '2'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5202.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5202.jpg
  title: Compact and Robust Models for Japanese-English Character-level Machine Translation
  title_html: Compact and Robust Models for <span class="acl-fixed-case">J</span>apanese-<span
    class="acl-fixed-case">E</span>nglish Character-level Machine Translation
  url: https://www.aclweb.org/anthology/D19-5202
  year: '2019'
D19-5203:
  abstract: In the Japanese language different levels of honorific speech are used
    to convey respect, deference, humility, formality and social distance. In this
    paper, we present a method for controlling the level of formality of Japanese
    output in English-to-Japanese neural machine translation (NMT). By using heuristics
    to identify honorific verb forms, we classify Japanese sentences as being one
    of three levels of informal, polite, or formal speech in parallel text. The English
    source side is marked with a feature that identifies the level of honorific speech
    present in the Japanese target side. We use this parallel text to train an English-Japanese
    NMT model capable of producing Japanese translations in different honorific speech
    styles for the same English input sentence.
  address: Hong Kong, China
  author:
  - first: Weston
    full: Weston Feely
    id: weston-feely
    last: Feely
  - first: Eva
    full: Eva Hasler
    id: eva-hasler
    last: Hasler
  - first: "Adri\xE0"
    full: "Adri\xE0 de Gispert"
    id: adria-de-gispert
    last: de Gispert
  author_string: "Weston Feely, Eva Hasler, Adri\xE0 de Gispert"
  bibkey: feely-etal-2019-controlling
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5203
  month: November
  page_first: '45'
  page_last: '53'
  pages: "45\u201353"
  paper_id: '3'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5203.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5203.jpg
  title: Controlling Japanese Honorifics in English-to-Japanese Neural Machine Translation
  title_html: Controlling <span class="acl-fixed-case">J</span>apanese Honorifics
    in <span class="acl-fixed-case">E</span>nglish-to-<span class="acl-fixed-case">J</span>apanese
    Neural Machine Translation
  url: https://www.aclweb.org/anthology/D19-5203
  year: '2019'
D19-5204:
  abstract: While the progress of machine translation of written text has come far
    in the past several years thanks to the increasing availability of parallel corpora
    and corpora-based training technologies, automatic translation of spoken text
    and dialogues remains challenging even for modern systems. In this paper, we aim
    to boost the machine translation quality of conversational texts by introducing
    a newly constructed Japanese-English business conversation parallel corpus. A
    detailed analysis of the corpus is provided along with challenging examples for
    automatic translation. We also experiment with adding the corpus in a machine
    translation training scenario and show how the resulting system benefits from
    its use.
  address: Hong Kong, China
  author:
  - first: "Mat\u012Bss"
    full: "Mat\u012Bss Rikters"
    id: matiss-rikters
    last: Rikters
  - first: Ryokan
    full: Ryokan Ri
    id: ryokan-ri
    last: Ri
  - first: Tong
    full: Tong Li
    id: tong-li
    last: Li
  - first: Toshiaki
    full: Toshiaki Nakazawa
    id: toshiaki-nakazawa
    last: Nakazawa
  author_string: "Mat\u012Bss Rikters, Ryokan Ri, Tong Li, Toshiaki Nakazawa"
  bibkey: rikters-etal-2019-designing
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5204
  month: November
  page_first: '54'
  page_last: '61'
  pages: "54\u201361"
  paper_id: '4'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5204.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5204.jpg
  title: Designing the Business Conversation Corpus
  title_html: Designing the Business Conversation Corpus
  url: https://www.aclweb.org/anthology/D19-5204
  year: '2019'
D19-5205:
  abstract: With the widespread use of Machine Trans-lation (MT) techniques, attempt
    to minimizecommunication gap among people from di-verse linguistic backgrounds.
    We have par-ticipated in Workshop on Asian Transla-tion 2019 (WAT2019) multi-modal
    translationtask. There are three types of submissiontrack namely, multi-modal
    translation, Hindi-only image captioning and text-only transla-tion for English
    to Hindi translation. The mainchallenge is to provide a precise MT output.The
    multi-modal concept incorporates textualand visual features in the translation
    task. Inthis work, multi-modal translation track re-lies on pre-trained convolutional
    neural net-works (CNN) with Visual Geometry Grouphaving 19 layered (VGG19) to
    extract imagefeatures and attention-based Neural MachineTranslation (NMT) system
    for translation.The merge-model of recurrent neural network(RNN) and CNN is used
    for the Hindi-onlyimage captioning. The text-only translationtrack is based on
    the transformer model of theNMT system. The official results evaluated atWAT2019
    translation task, which shows thatour multi-modal NMT system achieved Bilin-gual
    Evaluation Understudy (BLEU) score20.37, Rank-based Intuitive Bilingual Eval-uation
    Score (RIBES) 0.642838, Adequacy-Fluency Metrics (AMFM) score 0.668260 forchallenge
    test data and BLEU score 40.55,RIBES 0.760080, AMFM score 0.770860 forevaluation
    test data in English to Hindi multi-modal translation respectively.
  address: Hong Kong, China
  author:
  - first: Sahinur Rahman
    full: Sahinur Rahman Laskar
    id: sahinur-rahman-laskar
    last: Laskar
  - first: Rohit Pratap
    full: Rohit Pratap Singh
    id: rohit-pratap-singh
    last: Singh
  - first: Partha
    full: Partha Pakray
    id: partha-pakray
    last: Pakray
  - first: Sivaji
    full: Sivaji Bandyopadhyay
    id: sivaji-bandyopadhyay
    last: Bandyopadhyay
  author_string: Sahinur Rahman Laskar, Rohit Pratap Singh, Partha Pakray, Sivaji
    Bandyopadhyay
  bibkey: laskar-etal-2019-english
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5205
  month: November
  page_first: '62'
  page_last: '67'
  pages: "62\u201367"
  paper_id: '5'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5205.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5205.jpg
  title: English to Hindi Multi-modal Neural Machine Translation and Hindi Image Captioning
  title_html: <span class="acl-fixed-case">E</span>nglish to <span class="acl-fixed-case">H</span>indi
    Multi-modal Neural Machine Translation and <span class="acl-fixed-case">H</span>indi
    Image Captioning
  url: https://www.aclweb.org/anthology/D19-5205
  year: '2019'
D19-5206:
  abstract: "This paper presents the NICT\u2019s supervised and unsupervised machine\
    \ translation systems for the WAT2019 Myanmar-English and Khmer-English translation\
    \ tasks. For all the translation directions, we built state-of-the-art supervised\
    \ neural (NMT) and statistical (SMT) machine translation systems, using monolingual\
    \ data cleaned and normalized. Our combination of NMT and SMT performed among\
    \ the best systems for the four translation directions. We also investigated the\
    \ feasibility of unsupervised machine translation for low-resource and distant\
    \ language pairs and confirmed observations of previous work showing that unsupervised\
    \ MT is still largely unable to deal with them."
  address: Hong Kong, China
  author:
  - first: Benjamin
    full: Benjamin Marie
    id: benjamin-marie
    last: Marie
  - first: Hour
    full: Hour Kaing
    id: hour-kaing
    last: Kaing
  - first: Aye Myat
    full: Aye Myat Mon
    id: aye-myat-mon
    last: Mon
  - first: Chenchen
    full: Chenchen Ding
    id: chenchen-ding
    last: Ding
  - first: Atsushi
    full: Atsushi Fujita
    id: atsushi-fujita
    last: Fujita
  - first: Masao
    full: Masao Utiyama
    id: masao-utiyama
    last: Utiyama
  - first: Eiichiro
    full: Eiichiro Sumita
    id: eiichiro-sumita
    last: Sumita
  author_string: Benjamin Marie, Hour Kaing, Aye Myat Mon, Chenchen Ding, Atsushi
    Fujita, Masao Utiyama, Eiichiro Sumita
  bibkey: marie-etal-2019-supervised
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5206
  month: November
  page_first: '68'
  page_last: '75'
  pages: "68\u201375"
  paper_id: '6'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5206.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5206.jpg
  title: Supervised and Unsupervised Machine Translation for Myanmar-English and Khmer-English
  title_html: Supervised and Unsupervised Machine Translation for <span class="acl-fixed-case">M</span>yanmar-<span
    class="acl-fixed-case">E</span>nglish and <span class="acl-fixed-case">K</span>hmer-<span
    class="acl-fixed-case">E</span>nglish
  url: https://www.aclweb.org/anthology/D19-5206
  year: '2019'
D19-5207:
  abstract: "In this paper we describe our submissions to WAT 2019 for the following\
    \ tasks: English\u2013Tamil translation and Russian\u2013Japanese translation.\
    \ Our team,\u201CNICT-5\u201D, focused on multilingual domain adaptation and back-translation\
    \ for Russian\u2013Japanese translation and on simple fine-tuning for English\u2013\
    Tamil translation . We noted that multi-stage fine tuning is essential in leveraging\
    \ the power of multilingualism for an extremely low-resource language like Russian\u2013\
    Japanese. Furthermore, we can improve the performance of such a low-resource language\
    \ pair by exploiting a small but in-domain monolingual corpus via back-translation.\
    \ We managed to obtain second rank in both tasks for all translation directions."
  address: Hong Kong, China
  author:
  - first: Raj
    full: Raj Dabre
    id: raj-dabre
    last: Dabre
  - first: Eiichiro
    full: Eiichiro Sumita
    id: eiichiro-sumita
    last: Sumita
  author_string: Raj Dabre, Eiichiro Sumita
  bibkey: dabre-sumita-2019-nicts
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5207
  month: November
  page_first: '76'
  page_last: '80'
  pages: "76\u201380"
  paper_id: '7'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5207.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5207.jpg
  title: "NICT\u2019s participation to WAT 2019: Multilingualism and Multi-step Fine-Tuning\
    \ for Low Resource NMT"
  title_html: "<span class=\"acl-fixed-case\">NICT</span>\u2019s participation to\
    \ <span class=\"acl-fixed-case\">WAT</span> 2019: Multilingualism and Multi-step\
    \ Fine-Tuning for Low Resource <span class=\"acl-fixed-case\">NMT</span>"
  url: https://www.aclweb.org/anthology/D19-5207
  year: '2019'
D19-5208:
  abstract: 'In this paper, we describe the neural machine translation (NMT) system
    submitted by the Kangwon National University and HYUNDAI (KNU-HYUNDAI) team to
    the translation tasks of the 6th workshop on Asian Translation (WAT 2019). We
    participated in all tasks of ASPEC and JPC2, which included those of Chinese-Japanese,
    English-Japanese, and Korean->Japanese. We submitted our transformer-based NMT
    system with built using the following methods: a) relative positioning method
    for pairwise relationships between the input elements, b) back-translation and
    multi-source translation for data augmentation, c) right-to-left (r2l)-reranking
    model robust against error propagation in autoregressive architectures such as
    decoders, and d) checkpoint ensemble models, which selected the top three models
    with the best validation bilingual evaluation understudy (BLEU) . We have reported
    the translation results on the two aforementioned tasks. We performed well in
    both the tasks and were ranked first in terms of the BLEU scores in all the JPC2
    subtasks we participated in.'
  address: Hong Kong, China
  author:
  - first: Cheoneum
    full: Cheoneum Park
    id: cheoneum-park
    last: Park
  - first: Young-Jun
    full: Young-Jun Jung
    id: young-jun-jung
    last: Jung
  - first: Kihoon
    full: Kihoon Kim
    id: kihoon-kim
    last: Kim
  - first: Geonyeong
    full: Geonyeong Kim
    id: geonyeong-kim
    last: Kim
  - first: Jae-Won
    full: Jae-Won Jeon
    id: jae-won-jeon
    last: Jeon
  - first: Seongmin
    full: Seongmin Lee
    id: seongmin-lee
    last: Lee
  - first: Junseok
    full: Junseok Kim
    id: junseok-kim
    last: Kim
  - first: Changki
    full: Changki Lee
    id: changki-lee
    last: Lee
  author_string: Cheoneum Park, Young-Jun Jung, Kihoon Kim, Geonyeong Kim, Jae-Won
    Jeon, Seongmin Lee, Junseok Kim, Changki Lee
  bibkey: park-etal-2019-knu
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5208
  month: November
  page_first: '81'
  page_last: '89'
  pages: "81\u201389"
  paper_id: '8'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5208.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5208.jpg
  title: "KNU-HYUNDAI\u2019s NMT system for Scientific Paper and Patent Tasks onWAT\
    \ 2019"
  title_html: "<span class=\"acl-fixed-case\">KNU</span>-<span class=\"acl-fixed-case\"\
    >HYUNDAI</span>\u2019s <span class=\"acl-fixed-case\">NMT</span> system for Scientific\
    \ Paper and Patent Tasks on<span class=\"acl-fixed-case\">WAT</span> 2019"
  url: https://www.aclweb.org/anthology/D19-5208
  year: '2019'
D19-5209:
  abstract: "This paper presents the NICT\u2019s participation (team ID: NICT) in\
    \ the 6th Workshop on Asian Translation (WAT-2019) shared translation task, specifically\
    \ Myanmar (Burmese) - English task in both translation directions. We built neural\
    \ machine translation (NMT) systems for these tasks. Our NMT systems were trained\
    \ with language model pretraining. Back-translation technology is adopted to NMT.\
    \ Our NMT systems rank the third in English-to-Myanmar and the second in Myanmar-to-English\
    \ according to BLEU score."
  address: Hong Kong, China
  author:
  - first: Rui
    full: Rui Wang
    id: rui-wang
    last: Wang
  - first: Haipeng
    full: Haipeng Sun
    id: haipeng-sun
    last: Sun
  - first: Kehai
    full: Kehai Chen
    id: kehai-chen
    last: Chen
  - first: Chenchen
    full: Chenchen Ding
    id: chenchen-ding
    last: Ding
  - first: Masao
    full: Masao Utiyama
    id: masao-utiyama
    last: Utiyama
  - first: Eiichiro
    full: Eiichiro Sumita
    id: eiichiro-sumita
    last: Sumita
  author_string: Rui Wang, Haipeng Sun, Kehai Chen, Chenchen Ding, Masao Utiyama,
    Eiichiro Sumita
  bibkey: wang-etal-2019-english
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5209
  month: November
  page_first: '90'
  page_last: '93'
  pages: "90\u201393"
  paper_id: '9'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5209.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5209.jpg
  title: "English-Myanmar Supervised and Unsupervised NMT: NICT\u2019s Machine Translation\
    \ Systems at WAT-2019"
  title_html: "<span class=\"acl-fixed-case\">E</span>nglish-<span class=\"acl-fixed-case\"\
    >M</span>yanmar Supervised and Unsupervised <span class=\"acl-fixed-case\">NMT</span>:\
    \ <span class=\"acl-fixed-case\">NICT</span>\u2019s Machine Translation Systems\
    \ at <span class=\"acl-fixed-case\">WAT</span>-2019"
  url: https://www.aclweb.org/anthology/D19-5209
  year: '2019'
D19-5210:
  abstract: "This paper represents UCSMNLP\u2019s submission to the WAT 2019 Translation\
    \ Tasks focusing on the Myanmar-English translation. Phrase based statistical\
    \ machine translation (PBSMT) system is built by using other resources: Name Entity\
    \ Recognition (NER) corpus and bilingual dictionary which is created by Google\
    \ Translate (GT). This system is also adopted with listwise reranking process\
    \ in order to improve the quality of translation and tuning is done by changing\
    \ initial distortion weight. The experimental results show that PBSMT using other\
    \ resources with initial distortion weight (0.4) and listwise reranking function\
    \ outperforms the baseline system."
  address: Hong Kong, China
  author:
  - first: Aye
    full: Aye Thida
    id: aye-thida
    last: Thida
  - first: Nway Nway
    full: Nway Nway Han
    id: nway-nway-han1
    last: Han
  - first: Sheinn Thawtar
    full: Sheinn Thawtar Oo
    id: sheinn-thawtar-oo
    last: Oo
  - first: Khin Thet
    full: Khin Thet Htar
    id: khin-thet-htar
    last: Htar
  author_string: Aye Thida, Nway Nway Han, Sheinn Thawtar Oo, Khin Thet Htar
  bibkey: thida-etal-2019-ucsmnlp
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5210
  month: November
  page_first: '94'
  page_last: '98'
  pages: "94\u201398"
  paper_id: '10'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5210.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5210.jpg
  title: 'UCSMNLP: Statistical Machine Translation for WAT 2019'
  title_html: '<span class="acl-fixed-case">UCSMNLP</span>: Statistical Machine Translation
    for <span class="acl-fixed-case">WAT</span> 2019'
  url: https://www.aclweb.org/anthology/D19-5210
  year: '2019'
D19-5211:
  abstract: In this paper, we describe our systems that were submitted to the translation
    shared tasks at WAT 2019. This year, we participated in two distinct types of
    subtasks, a scientific paper subtask and a timely disclosure subtask, where we
    only considered English-to-Japanese and Japanese-to-English translation directions.
    We submitted two systems (En-Ja and Ja-En) for the scientific paper subtask and
    two systems (Ja-En, texts, items) for the timely disclosure subtask. Three of
    our four systems obtained the best human evaluation performances. We also confirmed
    that our new additional web-crawled parallel corpus improves the performance in
    unconstrained settings.
  address: Hong Kong, China
  author:
  - first: Makoto
    full: Makoto Morishita
    id: makoto-morishita
    last: Morishita
  - first: Jun
    full: Jun Suzuki
    id: jun-suzuki
    last: Suzuki
  - first: Masaaki
    full: Masaaki Nagata
    id: masaaki-nagata
    last: Nagata
  author_string: Makoto Morishita, Jun Suzuki, Masaaki Nagata
  bibkey: morishita-etal-2019-ntt
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5211
  month: November
  page_first: '99'
  page_last: '105'
  pages: "99\u2013105"
  paper_id: '11'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5211.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5211.jpg
  title: NTT Neural Machine Translation Systems at WAT 2019
  title_html: <span class="acl-fixed-case">NTT</span> Neural Machine Translation Systems
    at <span class="acl-fixed-case">WAT</span> 2019
  url: https://www.aclweb.org/anthology/D19-5211
  year: '2019'
D19-5212:
  abstract: "This paper describes NHK and NHK Engineering System (NHK-ES)\u2019s submission\
    \ to the newswire translation tasks of WAT 2019 in both directions of Japanese\u2192\
    English and English\u2192Japanese. In addition to the JIJI Corpus that was officially\
    \ provided by the task organizer, we developed a corpus of 0.22M sentence pairs\
    \ by manually, translating Japanese news sentences into English content- equivalently.\
    \ The content-equivalent corpus was effective for improving translation quality,\
    \ and our systems achieved the best human evaluation scores in the newswire translation\
    \ tasks at WAT 2019."
  address: Hong Kong, China
  author:
  - first: Hideya
    full: Hideya Mino
    id: hideya-mino
    last: Mino
  - first: Hitoshi
    full: Hitoshi Ito
    id: hitoshi-ito
    last: Ito
  - first: Isao
    full: Isao Goto
    id: isao-goto
    last: Goto
  - first: Ichiro
    full: Ichiro Yamada
    id: ichiro-yamada
    last: Yamada
  - first: Hideki
    full: Hideki Tanaka
    id: hideki-tanaka
    last: Tanaka
  - first: Takenobu
    full: Takenobu Tokunaga
    id: takenobu-tokunaga
    last: Tokunaga
  author_string: Hideya Mino, Hitoshi Ito, Isao Goto, Ichiro Yamada, Hideki Tanaka,
    Takenobu Tokunaga
  bibkey: mino-etal-2019-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5212
  month: November
  page_first: '106'
  page_last: '111'
  pages: "106\u2013111"
  paper_id: '12'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5212.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5212.jpg
  title: Neural Machine Translation System using a Content-equivalently Translated
    Parallel Corpus for the Newswire Translation Tasks at WAT 2019
  title_html: Neural Machine Translation System using a Content-equivalently Translated
    Parallel Corpus for the Newswire Translation Tasks at <span class="acl-fixed-case">WAT</span>
    2019
  url: https://www.aclweb.org/anthology/D19-5212
  year: '2019'
D19-5213:
  abstract: "This paper describes Facebook AI\u2019s submission to the WAT 2019 Myanmar-English\
    \ translation task. Our baseline systems are BPE-based transformer models. We\
    \ explore methods to leverage monolingual data to improve generalization, including\
    \ self-training, back-translation and their combination. We further improve results\
    \ by using noisy channel re-ranking and ensembling. We demonstrate that these\
    \ techniques can significantly improve not only a system trained with additional\
    \ monolingual data, but even the baseline system trained exclusively on the provided\
    \ small parallel dataset. Our system ranks first in both directions according\
    \ to human evaluation and BLEU, with a gain of over 8 BLEU points above the second\
    \ best system."
  address: Hong Kong, China
  author:
  - first: Peng-Jen
    full: Peng-Jen Chen
    id: peng-jen-chen
    last: Chen
  - first: Jiajun
    full: Jiajun Shen
    id: jiajun-shen
    last: Shen
  - first: Matthew
    full: Matthew Le
    id: matthew-le
    last: Le
  - first: Vishrav
    full: Vishrav Chaudhary
    id: vishrav-chaudhary
    last: Chaudhary
  - first: Ahmed
    full: Ahmed El-Kishky
    id: ahmed-el-kishky
    last: El-Kishky
  - first: Guillaume
    full: Guillaume Wenzek
    id: guillaume-wenzek
    last: Wenzek
  - first: Myle
    full: Myle Ott
    id: myle-ott
    last: Ott
  - first: "Marc\u2019Aurelio"
    full: "Marc\u2019Aurelio Ranzato"
    id: marcaurelio-ranzato
    last: Ranzato
  author_string: "Peng-Jen Chen, Jiajun Shen, Matthew Le, Vishrav Chaudhary, Ahmed\
    \ El-Kishky, Guillaume Wenzek, Myle Ott, Marc\u2019Aurelio Ranzato"
  bibkey: chen-etal-2019-facebook
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5213
  month: November
  page_first: '112'
  page_last: '122'
  pages: "112\u2013122"
  paper_id: '13'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5213.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5213.jpg
  title: "Facebook AI\u2019s WAT19 Myanmar-English Translation Task Submission"
  title_html: "<span class=\"acl-fixed-case\">F</span>acebook <span class=\"acl-fixed-case\"\
    >AI</span>\u2019s <span class=\"acl-fixed-case\">WAT</span>19 <span class=\"acl-fixed-case\"\
    >M</span>yanmar-<span class=\"acl-fixed-case\">E</span>nglish Translation Task\
    \ Submission"
  url: https://www.aclweb.org/anthology/D19-5213
  year: '2019'
D19-5214:
  abstract: "In this paper, we report our submission systems (geoduck) to the Timely\
    \ Disclosure task on the 6th Workshop on Asian Translation (WAT) (Nakazawa et\
    \ al., 2019). Our system employs a combined approach of translation memory and\
    \ Neural Machine Translation (NMT) models, where we can select final translation\
    \ outputs from either a translation memory or an NMT system, when the similarity\
    \ score of a test source sentence exceeds the predefined threshold. We observed\
    \ that this combination approach significantly improves the translation performance\
    \ on the Timely Disclosure corpus, as compared to a standalone NMT system. We\
    \ also conducted source-based direct assessment on the final output, and we discuss\
    \ the comparison between human references and each system\u2019s output."
  address: Hong Kong, China
  author:
  - first: Akiko
    full: Akiko Eriguchi
    id: akiko-eriguchi
    last: Eriguchi
  - first: Spencer
    full: Spencer Rarrick
    id: spencer-rarrick
    last: Rarrick
  - first: Hitokazu
    full: Hitokazu Matsushita
    id: hitokazu-matsushita
    last: Matsushita
  author_string: Akiko Eriguchi, Spencer Rarrick, Hitokazu Matsushita
  bibkey: eriguchi-etal-2019-combining
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5214
  month: November
  page_first: '123'
  page_last: '130'
  pages: "123\u2013130"
  paper_id: '14'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5214.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5214.jpg
  title: Combining Translation Memory with Neural Machine Translation
  title_html: Combining Translation Memory with Neural Machine Translation
  url: https://www.aclweb.org/anthology/D19-5214
  year: '2019'
D19-5215:
  abstract: This paper describes the Neural Machine Translation systems used by IIIT
    Hyderabad (CVIT-MT) for the translation tasks part of WAT-2019. We participated
    in tasks pertaining to Indian languages and submitted results for English-Hindi,
    Hindi-English, English-Tamil and Tamil-English language pairs. We employ Transformer
    architecture experimenting with multilingual models and methods for low-resource
    languages.
  address: Hong Kong, China
  author:
  - first: Jerin
    full: Jerin Philip
    id: jerin-philip
    last: Philip
  - first: Shashank
    full: Shashank Siripragada
    id: shashank-siripragada
    last: Siripragada
  - first: Upendra
    full: Upendra Kumar
    id: upendra-kumar
    last: Kumar
  - first: Vinay
    full: Vinay Namboodiri
    id: vinay-namboodiri
    last: Namboodiri
  - first: C V
    full: C V Jawahar
    id: c-v-jawahar1
    last: Jawahar
  author_string: Jerin Philip, Shashank Siripragada, Upendra Kumar, Vinay Namboodiri,
    C V Jawahar
  bibkey: philip-etal-2019-cvits
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5215
  month: November
  page_first: '131'
  page_last: '136'
  pages: "131\u2013136"
  paper_id: '15'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5215.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5215.jpg
  title: "CVIT\u2019s submissions to WAT-2019"
  title_html: "CVIT\u2019s submissions to WAT-2019"
  url: https://www.aclweb.org/anthology/D19-5215
  year: '2019'
D19-5216:
  abstract: This paper describes the Neural Machine Translation systems of IIIT-Hyderabad
    (LTRC-MT) for WAT 2019 Hindi-English shared task. We experimented with both Recurrent
    Neural Networks & Transformer architectures. We also show the results of our experiments
    of training NMT models using additional data via backtranslation.
  address: Hong Kong, China
  author:
  - first: Vikrant
    full: Vikrant Goyal
    id: vikrant-goyal
    last: Goyal
  - first: Dipti Misra
    full: Dipti Misra Sharma
    id: dipti-misra-sharma
    last: Sharma
  author_string: Vikrant Goyal, Dipti Misra Sharma
  bibkey: goyal-sharma-2019-ltrc
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5216
  month: November
  page_first: '137'
  page_last: '140'
  pages: "137\u2013140"
  paper_id: '16'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5216.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5216.jpg
  title: LTRC-MT Simple & Effective Hindi-English Neural Machine Translation Systems
    at WAT 2019
  title_html: <span class="acl-fixed-case">LTRC</span>-<span class="acl-fixed-case">MT</span>
    Simple &amp; Effective <span class="acl-fixed-case">H</span>indi-<span class="acl-fixed-case">E</span>nglish
    Neural Machine Translation Systems at <span class="acl-fixed-case">WAT</span>
    2019
  url: https://www.aclweb.org/anthology/D19-5216
  year: '2019'
D19-5217:
  abstract: "This paper describes the NICT-2 neural machine translation system at\
    \ the 6th Workshop on Asian Translation. This system employs the standard Transformer\
    \ model but features the following two characteristics. One is the long warm-up\
    \ strategy, which performs a longer warm-up of the learning rate at the start\
    \ of the training than conventional approaches. Another is that the system introduces\
    \ self-training approaches based on multiple back-translations generated by sampling.\
    \ We participated in three tasks\u2014ASPEC.en-ja, ASPEC.ja-en, and TDDC.ja-en\u2014\
    using this system."
  address: Hong Kong, China
  author:
  - first: Kenji
    full: Kenji Imamura
    id: kenji-imamura
    last: Imamura
  - first: Eiichiro
    full: Eiichiro Sumita
    id: eiichiro-sumita
    last: Sumita
  author_string: Kenji Imamura, Eiichiro Sumita
  bibkey: imamura-sumita-2019-long
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5217
  month: November
  page_first: '141'
  page_last: '146'
  pages: "141\u2013146"
  paper_id: '17'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5217.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5217.jpg
  title: 'Long Warm-up and Self-Training: Training Strategies of NICT-2 NMT System
    at WAT-2019'
  title_html: 'Long Warm-up and Self-Training: Training Strategies of <span class="acl-fixed-case">NICT</span>-2
    <span class="acl-fixed-case">NMT</span> System at <span class="acl-fixed-case">WAT</span>-2019'
  url: https://www.aclweb.org/anthology/D19-5217
  year: '2019'
D19-5218:
  abstract: This is the second time for SRCB to participate in WAT. This paper describes
    the neural machine translation systems for the shared translation tasks of WAT
    2019. We participated in ASPEC tasks and submitted results on English-Japanese,
    Japanese-English, Chinese-Japanese, and Japanese-Chinese four language pairs.
    We employed the Transformer model as the baseline and experimented relative position
    representation, data augmentation, deep layer model, ensemble. Experiments show
    that all these methods can yield substantial improvements.
  address: Hong Kong, China
  author:
  - first: Yixuan
    full: Yixuan Tong
    id: yixuan-tong
    last: Tong
  - first: Liang
    full: Liang Liang
    id: liang-liang
    last: Liang
  - first: Boyan
    full: Boyan Liu
    id: boyan-liu
    last: Liu
  - first: Shanshan
    full: Shanshan Jiang
    id: shanshan-jiang
    last: Jiang
  - first: Bin
    full: Bin Dong
    id: bin-dong
    last: Dong
  author_string: Yixuan Tong, Liang Liang, Boyan Liu, Shanshan Jiang, Bin Dong
  bibkey: tong-etal-2019-supervised
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5218
  month: November
  page_first: '147'
  page_last: '151'
  pages: "147\u2013151"
  paper_id: '18'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5218.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5218.jpg
  title: Supervised neural machine translation based on data augmentation and improved
    training & inference process
  title_html: Supervised neural machine translation based on data augmentation and
    improved training &amp; inference process
  url: https://www.aclweb.org/anthology/D19-5218
  year: '2019'
D19-5219:
  abstract: "This paper describes our MT systems\u2019 participation in the of WAT\
    \ 2019. We participated in the (i) Patent, (ii) Timely Disclosure, (iii) Newswire\
    \ and (iv) Mixed-domain tasks. Our main focus is to explore how similar Transformer\
    \ models perform on various tasks. We observed that for tasks with smaller datasets,\
    \ our best model setup are shallower models with lesser number of attention heads.\
    \ We investigated practical issues in NMT that often appear in production settings,\
    \ such as coping with multilinguality and simplifying pre- and post-processing\
    \ pipeline in deployment."
  address: Hong Kong, China
  author:
  - first: Raymond Hendy
    full: Raymond Hendy Susanto
    id: raymond-hendy-susanto
    last: Susanto
  - first: Ohnmar
    full: Ohnmar Htun
    id: ohnmar-htun
    last: Htun
  - first: Liling
    full: Liling Tan
    id: liling-tan
    last: Tan
  author_string: Raymond Hendy Susanto, Ohnmar Htun, Liling Tan
  bibkey: susanto-etal-2019-sarahs
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5219
  month: November
  page_first: '152'
  page_last: '158'
  pages: "152\u2013158"
  paper_id: '19'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5219.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5219.jpg
  title: "Sarah\u2019s Participation in WAT 2019"
  title_html: "Sarah\u2019s Participation in <span class=\"acl-fixed-case\">WAT</span>\
    \ 2019"
  url: https://www.aclweb.org/anthology/D19-5219
  year: '2019'
D19-5220:
  abstract: In this paper, we describe our Neural Machine Translation (NMT) systems
    for the WAT 2019 translation tasks we focus on. This year we participate in scientific
    paper tasks and focus on the language pair between English and Japanese. We use
    Transformer model through our work in this paper to explore and experience the
    powerful of the Transformer architecture relying on self-attention mechanism.
    We use different NMT toolkit/library as the implementation of training the Transformer
    model. For word segmentation, we use different subword segmentation strategies
    while using different toolkit/library. We not only give the translation accuracy
    obtained based on absolute position encodings that introduced in the Transformer
    model, but also report the the improvements in translation accuracy while replacing
    absolute position encodings with relative position representations. We also ensemble
    several independent trained Transformer models to further improve the translation
    accuracy.
  address: Hong Kong, China
  author:
  - first: Wei
    full: Wei Yang
    id: wei-yang
    last: Yang
  - first: Jun
    full: Jun Ogata
    id: jun-ogata
    last: Ogata
  author_string: Wei Yang, Jun Ogata
  bibkey: yang-ogata-2019-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5220
  month: November
  page_first: '159'
  page_last: '164'
  pages: "159\u2013164"
  paper_id: '20'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5220.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5220.jpg
  title: Our Neural Machine Translation Systems for WAT 2019
  title_html: Our Neural Machine Translation Systems for <span class="acl-fixed-case">WAT</span>
    2019
  url: https://www.aclweb.org/anthology/D19-5220
  year: '2019'
D19-5221:
  abstract: We introduce our system that is submitted to the News Commentary task
    (Japanese<->Russian) of the 6th Workshop on Asian Translation. The goal of this
    shared task is to study extremely low resource situations for distant language
    pairs. It is known that using parallel corpora of different language pair as training
    data is effective for multilingual neural machine translation model in extremely
    low resource scenarios. Therefore, to improve the translation quality of Japanese<->Russian
    language pair, our method leverages other in-domain Japanese-English and English-Russian
    parallel corpora as additional training data for our multilingual NMT model.
  address: Hong Kong, China
  author:
  - first: Aizhan
    full: Aizhan Imankulova
    id: aizhan-imankulova
    last: Imankulova
  - first: Masahiro
    full: Masahiro Kaneko
    id: masahiro-kaneko
    last: Kaneko
  - first: Mamoru
    full: Mamoru Komachi
    id: mamoru-komachi
    last: Komachi
  author_string: Aizhan Imankulova, Masahiro Kaneko, Mamoru Komachi
  bibkey: imankulova-etal-2019-japanese
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5221
  month: November
  page_first: '165'
  page_last: '170'
  pages: "165\u2013170"
  paper_id: '21'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5221.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5221.jpg
  title: Japanese-Russian TMU Neural Machine Translation System using Multilingual
    Model for WAT 2019
  title_html: <span class="acl-fixed-case">J</span>apanese-<span class="acl-fixed-case">R</span>ussian
    <span class="acl-fixed-case">TMU</span> Neural Machine Translation System using
    Multilingual Model for <span class="acl-fixed-case">WAT</span> 2019
  url: https://www.aclweb.org/anthology/D19-5221
  year: '2019'
D19-5222:
  abstract: This paper describes the Machine Translation system for Tamil-English
    Indic Task organized at WAT 2019. We use Transformer- based architecture for Neural
    Machine Translation.
  address: Hong Kong, China
  author:
  - first: Amit
    full: Amit Kumar
    id: amit-kumar
    last: Kumar
  - first: Anil Kumar
    full: Anil Kumar Singh
    id: anil-kumar-singh
    last: Singh
  author_string: Amit Kumar, Anil Kumar Singh
  bibkey: kumar-singh-2019-nlprl
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5222
  month: November
  page_first: '171'
  page_last: '174'
  pages: "171\u2013174"
  paper_id: '22'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5222.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5222.jpg
  title: "NLPRL at WAT2019: Transformer-based Tamil \u2013 English Indic Task Neural\
    \ Machine Translation System"
  title_html: "<span class=\"acl-fixed-case\">NLPRL</span> at <span class=\"acl-fixed-case\"\
    >WAT</span>2019: Transformer-based <span class=\"acl-fixed-case\">T</span>amil\
    \ \u2013 <span class=\"acl-fixed-case\">E</span>nglish Indic Task Neural Machine\
    \ Translation System"
  url: https://www.aclweb.org/anthology/D19-5222
  year: '2019'
D19-5223:
  abstract: "This paper describes the Idiap submission to WAT 2019 for the English-Hindi\
    \ Multi-Modal Translation Task. We have used the state-of-the-art Transformer\
    \ model and utilized the IITB English-Hindi parallel corpus as an additional data\
    \ source. Among the different tracks of the multi-modal task, we have participated\
    \ in the \u201CText-Only\u201D track for the evaluation and challenge test sets.\
    \ Our submission tops in its track among the competitors in terms of both automatic\
    \ and manual evaluation. Based on automatic scores, our text-only submission also\
    \ outperforms systems that consider visual information in the \u201Cmulti-modal\
    \ translation\u201D task."
  address: Hong Kong, China
  author:
  - first: Shantipriya
    full: Shantipriya Parida
    id: shantipriya-parida
    last: Parida
  - first: "Ond\u0159ej"
    full: "Ond\u0159ej Bojar"
    id: ondrej-bojar
    last: Bojar
  - first: Petr
    full: Petr Motlicek
    id: petr-motlicek
    last: Motlicek
  author_string: "Shantipriya Parida, Ond\u0159ej Bojar, Petr Motlicek"
  bibkey: parida-etal-2019-idiap
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5223
  month: November
  page_first: '175'
  page_last: '180'
  pages: "175\u2013180"
  paper_id: '23'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5223.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5223.jpg
  title: Idiap NMT System for WAT 2019 Multimodal Translation Task
  title_html: Idiap <span class="acl-fixed-case">NMT</span> System for <span class="acl-fixed-case">WAT</span>
    2019 Multimodal Translation Task
  url: https://www.aclweb.org/anthology/D19-5223
  year: '2019'
D19-5224:
  abstract: A multimodal translation is a task of translating a source language to
    a target language with the help of a parallel text corpus paired with images that
    represent the contextual details of the text. In this paper, we carried out an
    extensive comparison to evaluate the benefits of using a multimodal approach on
    translating text in English to a low resource language, Hindi as a part of WAT2019
    shared task. We carried out the translation of English to Hindi in three separate
    tasks with both the evaluation and challenge dataset. First, by using only the
    parallel text corpora, then through an image caption generation approach and,
    finally with the multimodal approach. Our experiment shows a significant improvement
    in the result with the multimodal approach than the other approach.
  address: Hong Kong, China
  author:
  - first: Loitongbam
    full: Loitongbam Sanayai Meetei
    id: loitongbam-sanayai-meetei
    last: Sanayai Meetei
  - first: Thoudam Doren
    full: Thoudam Doren Singh
    id: thoudam-doren-singh
    last: Singh
  - first: Sivaji
    full: Sivaji Bandyopadhyay
    id: sivaji-bandyopadhyay
    last: Bandyopadhyay
  author_string: Loitongbam Sanayai Meetei, Thoudam Doren Singh, Sivaji Bandyopadhyay
  bibkey: sanayai-meetei-etal-2019-wat2019
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5224
  month: November
  page_first: '181'
  page_last: '188'
  pages: "181\u2013188"
  paper_id: '24'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5224.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5224.jpg
  title: 'WAT2019: English-Hindi Translation on Hindi Visual Genome Dataset'
  title_html: '<span class="acl-fixed-case">WAT</span>2019: <span class="acl-fixed-case">E</span>nglish-<span
    class="acl-fixed-case">H</span>indi Translation on <span class="acl-fixed-case">H</span>indi
    Visual Genome Dataset'
  url: https://www.aclweb.org/anthology/D19-5224
  year: '2019'
D19-5225:
  abstract: "This paper describes Systran\u2019s submissions to WAT 2019 Russian-Japanese\
    \ News Commentary task. A challenging translation task due to the extremely low\
    \ resources available and the distance of the language pair. We have used the\
    \ neural Transformer architecture learned over the provided resources and we carried\
    \ out synthetic data generation experiments which aim at alleviating the data\
    \ scarcity problem. Results indicate the suitability of the data augmentation\
    \ experiments, enabling our systems to rank first according to automatic evaluations."
  address: Hong Kong, China
  author:
  - first: Jitao
    full: Jitao Xu
    id: jitao-xu
    last: Xu
  - first: TuAnh
    full: TuAnh Nguyen
    id: tuanh-nguyen
    last: Nguyen
  - first: MinhQuang
    full: MinhQuang Pham
    id: minh-quang-pham
    last: Pham
  - first: Josep
    full: Josep Crego
    id: josep-m-crego
    last: Crego
  - first: Jean
    full: Jean Senellart
    id: jean-senellart
    last: Senellart
  author_string: Jitao Xu, TuAnh Nguyen, MinhQuang Pham, Josep Crego, Jean Senellart
  bibkey: xu-etal-2019-systran
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5225
  month: November
  page_first: '189'
  page_last: '194'
  pages: "189\u2013194"
  paper_id: '25'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5225.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5225.jpg
  title: 'SYSTRAN @ WAT 2019: Russian-Japanese News Commentary task'
  title_html: '<span class="acl-fixed-case">SYSTRAN</span> @ <span class="acl-fixed-case">WAT</span>
    2019: <span class="acl-fixed-case">R</span>ussian-<span class="acl-fixed-case">J</span>apanese
    News Commentary task'
  url: https://www.aclweb.org/anthology/D19-5225
  year: '2019'
D19-5226:
  abstract: This paper describes the UCSYNLP-Lab submission to WAT 2019 for Myanmar-English
    translation tasks in both direction. We have used the neural machine translation
    systems with attention model and utilized the UCSY-corpus and ALT corpus. In NMT
    with attention model, we use the word segmentation level as well as syllable segmentation
    level. Especially, we made the UCSY-corpus to be cleaned in WAT 2019. Therefore,
    the UCSY corpus for WAT 2019 is not identical to those used in WAT 2018. Experiments
    show that the translation systems can produce the substantial improvements.
  address: Hong Kong, China
  author:
  - first: Yimon
    full: Yimon ShweSin
    id: yimon-shwesin
    last: ShweSin
  - first: Win Pa
    full: Win Pa Pa
    id: win-pa-pa
    last: Pa
  - first: KhinMar
    full: KhinMar Soe
    id: khinmar-soe
    last: Soe
  author_string: Yimon ShweSin, Win Pa Pa, KhinMar Soe
  bibkey: shwesin-etal-2019-ucsynlp
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5226
  month: November
  page_first: '195'
  page_last: '199'
  pages: "195\u2013199"
  paper_id: '26'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5226.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5226.jpg
  title: UCSYNLP-Lab Machine Translation Systems for WAT 2019
  title_html: <span class="acl-fixed-case">UCSYNLP</span>-Lab Machine Translation
    Systems for <span class="acl-fixed-case">WAT</span> 2019
  url: https://www.aclweb.org/anthology/D19-5226
  year: '2019'
D19-5227:
  abstract: Sentiment ambiguous lexicons refer to words where their polarity depends
    strongly on con- text. As such, when the context is absent, their translations
    or their embedded sentence ends up (incorrectly) being dependent on the training
    data. While neural machine translation (NMT) has achieved great progress in recent
    years, most systems aim to produce one single correct translation for a given
    source sentence. We investigate the translation variation in two sentiment scenarios.
    We perform experiments to study the preservation of sentiment during translation
    with three different methods that we propose. We conducted tests with both sentiment
    and non-sentiment bearing contexts to examine the effectiveness of our methods.
    We show that NMT can generate both positive- and negative-valent translations
    of a source sentence, based on a given input sentiment label. Empirical evaluations
    show that our valence-sensitive embedding (VSE) method significantly outperforms
    a sequence-to-sequence (seq2seq) baseline, both in terms of BLEU score and ambiguous
    word translation accuracy in test, given non-sentiment bearing contexts.
  address: Hong Kong, China
  author:
  - first: Chenglei
    full: Chenglei Si
    id: chenglei-si
    last: Si
  - first: Kui
    full: Kui Wu
    id: kui-wu
    last: Wu
  - first: Ai Ti
    full: Ai Ti Aw
    id: aiti-aw
    last: Aw
  - first: Min-Yen
    full: Min-Yen Kan
    id: min-yen-kan
    last: Kan
  author_string: Chenglei Si, Kui Wu, Ai Ti Aw, Min-Yen Kan
  bibkey: si-etal-2019-sentiment
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5227
  month: November
  page_first: '200'
  page_last: '206'
  pages: "200\u2013206"
  paper_id: '27'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5227.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5227.jpg
  title: Sentiment Aware Neural Machine Translation
  title_html: Sentiment Aware Neural Machine Translation
  url: https://www.aclweb.org/anthology/D19-5227
  year: '2019'
D19-5228:
  abstract: 'Among the six challenges of neural machine translation (NMT) coined by
    (Koehn and Knowles, 2017), rare-word problem is considered the most severe one,
    especially in translation of low-resource languages. In this paper, we propose
    three solutions to address the rare words in neural machine translation systems.
    First, we enhance source context to predict the target words by connecting directly
    the source embeddings to the output of the attention component in NMT. Second,
    we propose an algorithm to learn morphology of unknown words for English in supervised
    way in order to minimize the adverse effect of rare-word problem. Finally, we
    exploit synonymous relation from the WordNet to overcome out-of-vocabulary (OOV)
    problem of NMT. We evaluate our approaches on two low-resource language pairs:
    English-Vietnamese and Japanese-Vietnamese. In our experiments, we have achieved
    significant improvements of up to roughly +1.0 BLEU points in both language pairs.'
  address: Hong Kong, China
  author:
  - first: Thi-Vinh
    full: Thi-Vinh Ngo
    id: thi-vinh-ngo
    last: Ngo
  - first: Thanh-Le
    full: Thanh-Le Ha
    id: thanh-le-ha
    last: Ha
  - first: Phuong-Thai
    full: Phuong-Thai Nguyen
    id: phuong-thai-nguyen
    last: Nguyen
  - first: Le-Minh
    full: Le-Minh Nguyen
    id: minh-le-nguyen
    last: Nguyen
  author_string: Thi-Vinh Ngo, Thanh-Le Ha, Phuong-Thai Nguyen, Le-Minh Nguyen
  bibkey: ngo-etal-2019-overcoming
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5228
  month: November
  page_first: '207'
  page_last: '214'
  pages: "207\u2013214"
  paper_id: '28'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5228.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5228.jpg
  title: Overcoming the Rare Word Problem for low-resource language pairs in Neural
    Machine Translation
  title_html: Overcoming the Rare Word Problem for low-resource language pairs in
    Neural Machine Translation
  url: https://www.aclweb.org/anthology/D19-5228
  year: '2019'
D19-5229:
  abstract: In this work, we present several deep learning models for the automatic
    diacritization of Arabic text. Our models are built using two main approaches,
    viz. Feed-Forward Neural Network (FFNN) and Recurrent Neural Network (RNN), with
    several enhancements such as 100-hot encoding, embeddings, Conditional Random
    Field (CRF) and Block-Normalized Gradient (BNG). The models are tested on the
    only freely available benchmark dataset and the results show that our models are
    either better or on par with other models, which require language-dependent post-processing
    steps, unlike ours. Moreover, we show that diacritics in Arabic can be used to
    enhance the models of NLP tasks such as Machine Translation (MT) by proposing
    the Translation over Diacritization (ToD) approach.
  address: Hong Kong, China
  attachment:
  - filename: D19-5229.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5229.Attachment.zip
  author:
  - first: Ali
    full: Ali Fadel
    id: ali-fadel
    last: Fadel
  - first: Ibraheem
    full: Ibraheem Tuffaha
    id: ibraheem-tuffaha
    last: Tuffaha
  - first: "Bara\u2019"
    full: "Bara\u2019 Al-Jawarneh"
    id: bara-al-jawarneh
    last: Al-Jawarneh
  - first: Mahmoud
    full: Mahmoud Al-Ayyoub
    id: mahmoud-al-ayyoub
    last: Al-Ayyoub
  author_string: "Ali Fadel, Ibraheem Tuffaha, Bara\u2019 Al-Jawarneh, Mahmoud Al-Ayyoub"
  bibkey: fadel-etal-2019-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 6th Workshop on Asian Translation
  booktitle_html: Proceedings of the 6th Workshop on Asian Translation
  doi: 10.18653/v1/D19-5229
  month: November
  page_first: '215'
  page_last: '225'
  pages: "215\u2013225"
  paper_id: '29'
  parent_volume_id: D19-52
  pdf: https://www.aclweb.org/anthology/D19-5229.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5229.jpg
  title: 'Neural Arabic Text Diacritization: State of the Art Results and a Novel
    Approach for Machine Translation'
  title_html: 'Neural <span class="acl-fixed-case">A</span>rabic Text Diacritization:
    State of the Art Results and a Novel Approach for Machine Translation'
  url: https://www.aclweb.org/anthology/D19-5229
  year: '2019'
D19-5300:
  address: Hong Kong
  author:
  - first: Dmitry
    full: Dmitry Ustalov
    id: dmitry-ustalov
    last: Ustalov
  - first: Swapna
    full: Swapna Somasundaran
    id: swapna-somasundaran
    last: Somasundaran
  - first: Peter
    full: Peter Jansen
    id: peter-jansen
    last: Jansen
  - first: Goran
    full: "Goran Glava\u0161"
    id: goran-glavas
    last: "Glava\u0161"
  - first: Martin
    full: Martin Riedl
    id: martin-riedl
    last: Riedl
  - first: Mihai
    full: Mihai Surdeanu
    id: mihai-surdeanu
    last: Surdeanu
  - first: Michalis
    full: Michalis Vazirgiannis
    id: michalis-vazirgiannis
    last: Vazirgiannis
  author_string: "Dmitry Ustalov, Swapna Somasundaran, Peter Jansen, Goran Glava\u0161\
    , Martin Riedl, Mihai Surdeanu, Michalis Vazirgiannis"
  bibkey: emnlp-2019-graph
  bibtype: proceedings
  booktitle: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  month: November
  paper_id: '0'
  parent_volume_id: D19-53
  pdf: https://www.aclweb.org/anthology/D19-5300.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5300.jpg
  title: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  title_html: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  url: https://www.aclweb.org/anthology/D19-5300
  year: '2019'
D19-5301:
  abstract: Text adventure games, in which players must make sense of the world through
    text descriptions and declare actions through text descriptions, provide a stepping
    stone toward grounding action in language. Prior work has demonstrated that using
    a knowledge graph as a state representation and question-answering to pre-train
    a deep Q-network facilitates faster control policy learning. In this paper, we
    explore the use of knowledge graphs as a representation for domain knowledge transfer
    for training text-adventure playing reinforcement learning agents. Our methods
    are tested across multiple computer generated and human authored games, varying
    in domain and complexity, and demonstrate that our transfer learning methods let
    us learn a higher-quality control policy faster.
  address: Hong Kong
  author:
  - first: Prithviraj
    full: Prithviraj Ammanabrolu
    id: prithviraj-ammanabrolu
    last: Ammanabrolu
  - first: Mark
    full: Mark Riedl
    id: mark-riedl
    last: Riedl
  author_string: Prithviraj Ammanabrolu, Mark Riedl
  bibkey: ammanabrolu-riedl-2019-transfer
  bibtype: inproceedings
  booktitle: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  booktitle_html: Proceedings of the Thirteenth Workshop on Graph-Based Methods for
    Natural Language Processing (TextGraphs-13)
  doi: 10.18653/v1/D19-5301
  month: November
  page_first: '1'
  page_last: '10'
  pages: "1\u201310"
  paper_id: '1'
  parent_volume_id: D19-53
  pdf: https://www.aclweb.org/anthology/D19-5301.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5301.jpg
  title: Transfer in Deep Reinforcement Learning Using Knowledge Graphs
  title_html: Transfer in Deep Reinforcement Learning Using Knowledge Graphs
  url: https://www.aclweb.org/anthology/D19-5301
  year: '2019'
D19-5302:
  abstract: Knowledge graphs (KGs) are generally used for various NLP tasks. However,
    as KGs still miss some information, it is necessary to develop Knowledge Graph
    Completion (KGC) methods. Most KGC researches do not focus on the Out-of-KGs entities
    (Unseen-entities), we need a method that can predict the relation for the entity
    pairs containing Unseen-entities to automatically add new entities to the KGs.
    In this study, we focus on relation prediction and propose a method to learn entity
    representations via a graph structure that uses Seen-entities, Unseen-entities
    and words as nodes created from the descriptions of all entities. In the experiments,
    our method shows a significant improvement in the relation prediction for the
    entity pairs containing Unseen-entities.
  address: Hong Kong
  author:
  - first: Yuki
    full: Yuki Tagawa
    id: yuki-tagawa
    last: Tagawa
  - first: Motoki
    full: Motoki Taniguchi
    id: motoki-taniguchi
    last: Taniguchi
  - first: Yasuhide
    full: Yasuhide Miura
    id: yasuhide-miura
    last: Miura
  - first: Tomoki
    full: Tomoki Taniguchi
    id: tomoki-taniguchi
    last: Taniguchi
  - first: Tomoko
    full: Tomoko Ohkuma
    id: tomoko-ohkuma
    last: Ohkuma
  - first: Takayuki
    full: Takayuki Yamamoto
    id: takayuki-yamamoto
    last: Yamamoto
  - first: Keiichi
    full: Keiichi Nemoto
    id: keiichi-nemoto
    last: Nemoto
  author_string: Yuki Tagawa, Motoki Taniguchi, Yasuhide Miura, Tomoki Taniguchi,
    Tomoko Ohkuma, Takayuki Yamamoto, Keiichi Nemoto
  bibkey: tagawa-etal-2019-relation
  bibtype: inproceedings
  booktitle: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  booktitle_html: Proceedings of the Thirteenth Workshop on Graph-Based Methods for
    Natural Language Processing (TextGraphs-13)
  doi: 10.18653/v1/D19-5302
  month: November
  page_first: '11'
  page_last: '16'
  pages: "11\u201316"
  paper_id: '2'
  parent_volume_id: D19-53
  pdf: https://www.aclweb.org/anthology/D19-5302.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5302.jpg
  title: Relation Prediction for Unseen-Entities Using Entity-Word Graphs
  title_html: Relation Prediction for Unseen-Entities Using Entity-Word Graphs
  url: https://www.aclweb.org/anthology/D19-5302
  year: '2019'
D19-5303:
  abstract: In this paper, we consider the named entity linking (NEL) problem. We
    assume a set of queries, named entities, that have to be identified within a knowledge
    base. This knowledge base is represented by a text database paired with a semantic
    graph, endowed with a classification of entities (ontology). We present state-of-the-art
    methods in NEL, and propose a new method for individual identification requiring
    few annotated data samples. We demonstrate its scalability and performance over
    standard datasets, for several ontology configurations. Our approach is well-motivated
    for integration in real systems. Indeed, recent deep learning methods, despite
    their capacity to improve experimental precision, require lots of parameter tuning
    along with large volume of annotated data.
  address: Hong Kong
  author:
  - first: Sammy
    full: Sammy Khalife
    id: sammy-khalife
    last: Khalife
  - first: Michalis
    full: Michalis Vazirgiannis
    id: michalis-vazirgiannis
    last: Vazirgiannis
  author_string: Sammy Khalife, Michalis Vazirgiannis
  bibkey: khalife-vazirgiannis-2019-scalable
  bibtype: inproceedings
  booktitle: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  booktitle_html: Proceedings of the Thirteenth Workshop on Graph-Based Methods for
    Natural Language Processing (TextGraphs-13)
  doi: 10.18653/v1/D19-5303
  month: November
  page_first: '17'
  page_last: '25'
  pages: "17\u201325"
  paper_id: '3'
  parent_volume_id: D19-53
  pdf: https://www.aclweb.org/anthology/D19-5303.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5303.jpg
  title: Scalable graph-based method for individual named entity identification
  title_html: Scalable graph-based method for individual named entity identification
  url: https://www.aclweb.org/anthology/D19-5303
  year: '2019'
D19-5304:
  abstract: Speech translation systems usually follow a pipeline approach, using word
    lattices as an intermediate representation. However, previous work assume access
    to the original transcriptions used to train the ASR system, which can limit applicability
    in real scenarios. In this work we propose an approach for speech translation
    through lattice transformations and neural models based on graph networks. Experimental
    results show that our approach reaches competitive performance without relying
    on transcriptions, while also being orders of magnitude faster than previous work.
  address: Hong Kong
  attachment:
  - filename: D19-5304.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5304.Attachment.pdf
  author:
  - first: Daniel
    full: Daniel Beck
    id: daniel-beck
    last: Beck
  - first: Trevor
    full: Trevor Cohn
    id: trevor-cohn
    last: Cohn
  - first: Gholamreza
    full: Gholamreza Haffari
    id: gholamreza-haffari
    last: Haffari
  author_string: Daniel Beck, Trevor Cohn, Gholamreza Haffari
  bibkey: beck-etal-2019-neural
  bibtype: inproceedings
  booktitle: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  booktitle_html: Proceedings of the Thirteenth Workshop on Graph-Based Methods for
    Natural Language Processing (TextGraphs-13)
  doi: 10.18653/v1/D19-5304
  month: November
  page_first: '26'
  page_last: '31'
  pages: "26\u201331"
  paper_id: '4'
  parent_volume_id: D19-53
  pdf: https://www.aclweb.org/anthology/D19-5304.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5304.jpg
  title: Neural Speech Translation using Lattice Transformations and Graph Networks
  title_html: Neural Speech Translation using Lattice Transformations and Graph Networks
  url: https://www.aclweb.org/anthology/D19-5304
  year: '2019'
D19-5305:
  abstract: Word embedding algorithms have become a common tool in the field of natural
    language processing. While some, like Word2Vec, are based on sequential text input,
    others are utilizing a graph representation of text. In this paper, we introduce
    a new algorithm, named WordGraph2Vec, or in short WG2V, which combines the two
    approaches to gain the benefits of both. The algorithm uses a directed word graph
    to provide additional information for sequential text input algorithms. Our experiments
    on benchmark datasets show that text classification algorithms are nearly as accurate
    with WG2V as with other word embedding models while preserving more stable accuracy
    rankings.
  address: Hong Kong
  author:
  - first: Matan
    full: Matan Zuckerman
    id: matan-zuckerman
    last: Zuckerman
  - first: Mark
    full: Mark Last
    id: mark-last
    last: Last
  author_string: Matan Zuckerman, Mark Last
  bibkey: zuckerman-last-2019-using
  bibtype: inproceedings
  booktitle: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  booktitle_html: Proceedings of the Thirteenth Workshop on Graph-Based Methods for
    Natural Language Processing (TextGraphs-13)
  doi: 10.18653/v1/D19-5305
  month: November
  page_first: '32'
  page_last: '41'
  pages: "32\u201341"
  paper_id: '5'
  parent_volume_id: D19-53
  pdf: https://www.aclweb.org/anthology/D19-5305.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5305.jpg
  title: Using Graphs for Word Embedding with Enhanced Semantic Relations
  title_html: Using Graphs for Word Embedding with Enhanced Semantic Relations
  url: https://www.aclweb.org/anthology/D19-5305
  year: '2019'
D19-5306:
  abstract: Recent advances in reading comprehension have resulted in models that
    surpass human performance when the answer is contained in a single, continuous
    passage of text. However, complex Question Answering (QA) typically requires multi-hop
    reasoning - i.e. the integration of supporting facts from different sources, to
    infer the correct answer. This paper proposes Document Graph Network (DGN), a
    message passing architecture for the identification of supporting facts over a
    graph-structured representation of text. The evaluation on HotpotQA shows that
    DGN obtains competitive results when compared to a reading comprehension baseline
    operating on raw text, confirming the relevance of structured representations
    for supporting multi-hop reasoning.
  address: Hong Kong
  author:
  - first: Mokanarangan
    full: Mokanarangan Thayaparan
    id: mokanarangan-thayaparan
    last: Thayaparan
  - first: Marco
    full: Marco Valentino
    id: marco-valentino
    last: Valentino
  - first: Viktor
    full: Viktor Schlegel
    id: viktor-schlegel
    last: Schlegel
  - first: "Andr\xE9"
    full: "Andr\xE9 Freitas"
    id: andre-freitas
    last: Freitas
  author_string: "Mokanarangan Thayaparan, Marco Valentino, Viktor Schlegel, Andr\xE9\
    \ Freitas"
  bibkey: thayaparan-etal-2019-identifying
  bibtype: inproceedings
  booktitle: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  booktitle_html: Proceedings of the Thirteenth Workshop on Graph-Based Methods for
    Natural Language Processing (TextGraphs-13)
  doi: 10.18653/v1/D19-5306
  month: November
  page_first: '42'
  page_last: '51'
  pages: "42\u201351"
  paper_id: '6'
  parent_volume_id: D19-53
  pdf: https://www.aclweb.org/anthology/D19-5306.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5306.jpg
  title: Identifying Supporting Facts for Multi-hop Question Answering with Document
    Graph Networks
  title_html: Identifying Supporting Facts for Multi-hop Question Answering with Document
    Graph Networks
  url: https://www.aclweb.org/anthology/D19-5306
  year: '2019'
D19-5307:
  abstract: Paraphrases are important linguistic resources for a wide variety of NLP
    applications. Many techniques for automatic paraphrase mining from general corpora
    have been proposed. While these techniques are successful at discovering generic
    paraphrases, they often fail to identify domain-specific paraphrases (e.g., staff,
    concierge in the hospitality domain). This is because current techniques are often
    based on statistical methods, while domain-specific corpora are too small to fit
    statistical methods. In this paper, we present an unsupervised graph-based technique
    to mine paraphrases from a small set of sentences that roughly share the same
    topic or intent. Our system, Essentia, relies on word-alignment techniques to
    create a word-alignment graph that merges and organizes tokens from input sentences.
    The resulting graph is then used to generate candidate paraphrases. We demonstrate
    that our system obtains high quality paraphrases, as evaluated by crowd workers.
    We further show that the majority of the identified paraphrases are domain-specific
    and thus complement existing paraphrase databases.
  address: Hong Kong
  author:
  - first: Danni
    full: Danni Ma
    id: danni-ma
    last: Ma
  - first: Chen
    full: Chen Chen
    id: chen-chen
    last: Chen
  - first: Behzad
    full: Behzad Golshan
    id: behzad-golshan
    last: Golshan
  - first: Wang-Chiew
    full: Wang-Chiew Tan
    id: wang-chiew-tan
    last: Tan
  author_string: Danni Ma, Chen Chen, Behzad Golshan, Wang-Chiew Tan
  bibkey: ma-etal-2019-essentia
  bibtype: inproceedings
  booktitle: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  booktitle_html: Proceedings of the Thirteenth Workshop on Graph-Based Methods for
    Natural Language Processing (TextGraphs-13)
  doi: 10.18653/v1/D19-5307
  month: November
  page_first: '52'
  page_last: '57'
  pages: "52\u201357"
  paper_id: '7'
  parent_volume_id: D19-53
  pdf: https://www.aclweb.org/anthology/D19-5307.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5307.jpg
  title: 'Essentia: Mining Domain-specific Paraphrases with Word-Alignment Graphs'
  title_html: '<span class="acl-fixed-case">E</span>ssentia: Mining Domain-specific
    Paraphrases with Word-Alignment Graphs'
  url: https://www.aclweb.org/anthology/D19-5307
  year: '2019'
D19-5308:
  abstract: Representations in the hidden layers of Deep Neural Networks (DNN) are
    often hard to interpret since it is difficult to project them into an interpretable
    domain. Graph Convolutional Networks (GCN) allow this projection, but existing
    explainability methods do not exploit this fact, i.e. do not focus their explanations
    on intermediate states. In this work, we present a novel method that traces and
    visualizes features that contribute to a classification decision in the visible
    and hidden layers of a GCN. Our method exposes hidden cross-layer dynamics in
    the input graph structure. We experimentally demonstrate that it yields meaningful
    layerwise explanations for a GCN sentence classifier.
  address: Hong Kong
  attachment:
  - filename: D19-5308.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5308.Attachment.pdf
  author:
  - first: Robert
    full: Robert Schwarzenberg
    id: robert-schwarzenberg
    last: Schwarzenberg
  - first: Marc
    full: "Marc H\xFCbner"
    id: marc-hubner
    last: "H\xFCbner"
  - first: David
    full: David Harbecke
    id: david-harbecke
    last: Harbecke
  - first: Christoph
    full: Christoph Alt
    id: christoph-alt
    last: Alt
  - first: Leonhard
    full: Leonhard Hennig
    id: leonhard-hennig
    last: Hennig
  author_string: "Robert Schwarzenberg, Marc H\xFCbner, David Harbecke, Christoph\
    \ Alt, Leonhard Hennig"
  bibkey: schwarzenberg-etal-2019-layerwise
  bibtype: inproceedings
  booktitle: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  booktitle_html: Proceedings of the Thirteenth Workshop on Graph-Based Methods for
    Natural Language Processing (TextGraphs-13)
  doi: 10.18653/v1/D19-5308
  month: November
  page_first: '58'
  page_last: '62'
  pages: "58\u201362"
  paper_id: '8'
  parent_volume_id: D19-53
  pdf: https://www.aclweb.org/anthology/D19-5308.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5308.jpg
  title: Layerwise Relevance Visualization in Convolutional Text Graph Classifiers
  title_html: Layerwise Relevance Visualization in Convolutional Text Graph Classifiers
  url: https://www.aclweb.org/anthology/D19-5308
  year: '2019'
D19-5309:
  abstract: "While automated question answering systems are increasingly able to retrieve\
    \ answers to natural language questions, their ability to generate detailed human-readable\
    \ explanations for their answers is still quite limited. The Shared Task on Multi-Hop\
    \ Inference for Explanation Regeneration tasks participants with regenerating\
    \ detailed gold explanations for standardized elementary science exam questions\
    \ by selecting facts from a knowledge base of semi-structured tables. Each explanation\
    \ contains between 1 and 16 interconnected facts that form an \u201Cexplanation\
    \ graph\u201D spanning core scientific knowledge and detailed world knowledge.\
    \ It is expected that successfully combining these facts to generate detailed\
    \ explanations will require advancing methods in multi-hop inference and information\
    \ combination, and will make use of the supervised training data provided by the\
    \ WorldTree explanation corpus. The top-performing system achieved a mean average\
    \ precision (MAP) of 0.56, substantially advancing the state-of-the-art over a\
    \ baseline information retrieval model. Detailed extended analyses of all submitted\
    \ systems showed large relative improvements in accessing the most challenging\
    \ multi-hop inference problems, while absolute performance remains low, highlighting\
    \ the difficulty of generating detailed explanations through multi-hop reasoning."
  address: Hong Kong
  author:
  - first: Peter
    full: Peter Jansen
    id: peter-jansen
    last: Jansen
  - first: Dmitry
    full: Dmitry Ustalov
    id: dmitry-ustalov
    last: Ustalov
  author_string: Peter Jansen, Dmitry Ustalov
  bibkey: jansen-ustalov-2019-textgraphs
  bibtype: inproceedings
  booktitle: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  booktitle_html: Proceedings of the Thirteenth Workshop on Graph-Based Methods for
    Natural Language Processing (TextGraphs-13)
  doi: 10.18653/v1/D19-5309
  month: November
  page_first: '63'
  page_last: '77'
  pages: "63\u201377"
  paper_id: '9'
  parent_volume_id: D19-53
  pdf: https://www.aclweb.org/anthology/D19-5309.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5309.jpg
  title: TextGraphs 2019 Shared Task on Multi-Hop Inference for Explanation Regeneration
  title_html: <span class="acl-fixed-case">T</span>ext<span class="acl-fixed-case">G</span>raphs
    2019 Shared Task on Multi-Hop Inference for Explanation Regeneration
  url: https://www.aclweb.org/anthology/D19-5309
  year: '2019'
D19-5310:
  abstract: In this work we describe the system from Natural Language Processing group
    at Arizona State University for the TextGraphs 2019 Shared Task. The task focuses
    on Explanation Regeneration, an intermediate step towards general multi-hop inference
    on large graphs. Our approach consists of modeling the explanation regeneration
    task as a learning to rank problem, for which we use state-of-the-art language
    models and explore dataset preparation techniques. We utilize an iterative reranking
    based approach to further improve the rankings. Our system secured 2nd rank in
    the task with a mean average precision (MAP) of 41.3% on the test set.
  address: Hong Kong
  attachment:
  - filename: D19-5310.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5310.Attachment.zip
  author:
  - first: Pratyay
    full: Pratyay Banerjee
    id: pratyay-banerjee
    last: Banerjee
  author_string: Pratyay Banerjee
  bibkey: banerjee-2019-asu
  bibtype: inproceedings
  booktitle: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  booktitle_html: Proceedings of the Thirteenth Workshop on Graph-Based Methods for
    Natural Language Processing (TextGraphs-13)
  doi: 10.18653/v1/D19-5310
  month: November
  page_first: '78'
  page_last: '84'
  pages: "78\u201384"
  paper_id: '10'
  parent_volume_id: D19-53
  pdf: https://www.aclweb.org/anthology/D19-5310.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5310.jpg
  title: 'ASU at TextGraphs 2019 Shared Task: Explanation ReGeneration using Language
    Models and Iterative Re-Ranking'
  title_html: '<span class="acl-fixed-case">ASU</span> at <span class="acl-fixed-case">T</span>ext<span
    class="acl-fixed-case">G</span>raphs 2019 Shared Task: Explanation <span class="acl-fixed-case">R</span>e<span
    class="acl-fixed-case">G</span>eneration using Language Models and Iterative Re-Ranking'
  url: https://www.aclweb.org/anthology/D19-5310
  year: '2019'
D19-5311:
  abstract: "The TextGraphs-13 Shared Task on Explanation Regeneration (Jansen and\
    \ Ustalov, 2019) asked participants to develop methods to reconstruct gold explanations\
    \ for elementary science questions. Red Dragon AI\u2019s entries used the language\
    \ of the questions and explanation text directly, rather than a constructing a\
    \ separate graph-like representation. Our leaderboard submission placed us 3rd\
    \ in the competition, but we present here three methods of increasing sophistication,\
    \ each of which scored successively higher on the test set after the competition\
    \ close."
  address: Hong Kong
  author:
  - first: Yew Ken
    full: Yew Ken Chia
    id: yew-ken-chia
    last: Chia
  - first: Sam
    full: Sam Witteveen
    id: sam-witteveen
    last: Witteveen
  - first: Martin
    full: Martin Andrews
    id: martin-andrews
    last: Andrews
  author_string: Yew Ken Chia, Sam Witteveen, Martin Andrews
  bibkey: chia-etal-2019-red
  bibtype: inproceedings
  booktitle: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  booktitle_html: Proceedings of the Thirteenth Workshop on Graph-Based Methods for
    Natural Language Processing (TextGraphs-13)
  doi: 10.18653/v1/D19-5311
  month: November
  page_first: '85'
  page_last: '89'
  pages: "85\u201389"
  paper_id: '11'
  parent_volume_id: D19-53
  pdf: https://www.aclweb.org/anthology/D19-5311.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5311.jpg
  title: 'Red Dragon AI at TextGraphs 2019 Shared Task: Language Model Assisted Explanation
    Generation'
  title_html: 'Red Dragon <span class="acl-fixed-case">AI</span> at <span class="acl-fixed-case">T</span>ext<span
    class="acl-fixed-case">G</span>raphs 2019 Shared Task: Language Model Assisted
    Explanation Generation'
  url: https://www.aclweb.org/anthology/D19-5311
  year: '2019'
D19-5312:
  abstract: The TextGraphs 2019 Shared Task on Multi-Hop Inference for Explanation
    Regeneration (MIER-19) tackles explanation generation for answers to elementary
    science questions. It builds on the AI2 Reasoning Challenge 2018 (ARC-18) which
    was organized as an advanced question answering task on a dataset of elementary
    science questions. The ARC-18 questions were shown to be hard to answer with systems
    focusing on surface-level cues alone, instead requiring far more powerful knowledge
    and reasoning. To address MIER-19, we adopt a hybrid pipelined architecture comprising
    a featurerich learning-to-rank (LTR) machine learning model, followed by a rule-based
    system for reranking the LTR model predictions. Our system was ranked fourth in
    the official evaluation, scoring close to the second and third ranked teams, achieving
    39.4% MAP.
  address: Hong Kong
  author:
  - first: Jennifer
    full: "Jennifer D\u2019Souza"
    id: jennifer-dsouza
    last: "D\u2019Souza"
  - first: Isaiah Onando
    full: "Isaiah Onando Mulang\u2019"
    id: isaiah-onando-mulang
    last: "Mulang\u2019"
  - first: "S\xF6ren"
    full: "S\xF6ren Auer"
    id: soren-auer
    last: Auer
  author_string: "Jennifer D\u2019Souza, Isaiah Onando Mulang\u2019, S\xF6ren Auer"
  bibkey: dsouza-etal-2019-team
  bibtype: inproceedings
  booktitle: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  booktitle_html: Proceedings of the Thirteenth Workshop on Graph-Based Methods for
    Natural Language Processing (TextGraphs-13)
  doi: 10.18653/v1/D19-5312
  month: November
  page_first: '90'
  page_last: '100'
  pages: "90\u2013100"
  paper_id: '12'
  parent_volume_id: D19-53
  pdf: https://www.aclweb.org/anthology/D19-5312.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5312.jpg
  title: 'Team SVMrank: Leveraging Feature-rich Support Vector Machines for Ranking
    Explanations to Elementary Science Questions'
  title_html: 'Team <span class="acl-fixed-case">SVM</span>rank: Leveraging Feature-rich
    Support Vector Machines for Ranking Explanations to Elementary Science Questions'
  url: https://www.aclweb.org/anthology/D19-5312
  year: '2019'
D19-5313:
  abstract: "This paper describes our submission to the shared task on \u201CMulti-hop\
    \ Inference Explanation Regeneration\u201D in TextGraphs workshop at EMNLP 2019\
    \ (Jansen and Ustalov, 2019). Our system identifies chains of facts relevant to\
    \ explain an answer to an elementary science examination question. To counter\
    \ the problem of \u2018spurious chains\u2019 leading to \u2018semantic drifts\u2019\
    , we train a ranker that uses contextualized representation of facts to score\
    \ its relevance for explaining an answer to a question. Our system was ranked\
    \ first w.r.t the mean average precision (MAP) metric outperforming the second\
    \ best system by 14.95 points."
  address: Hong Kong
  author:
  - first: Rajarshi
    full: Rajarshi Das
    id: rajarshi-das
    last: Das
  - first: Ameya
    full: Ameya Godbole
    id: ameya-godbole
    last: Godbole
  - first: Manzil
    full: Manzil Zaheer
    id: manzil-zaheer
    last: Zaheer
  - first: Shehzaad
    full: Shehzaad Dhuliawala
    id: shehzaad-dhuliawala
    last: Dhuliawala
  - first: Andrew
    full: Andrew McCallum
    id: andrew-mccallum
    last: McCallum
  author_string: Rajarshi Das, Ameya Godbole, Manzil Zaheer, Shehzaad Dhuliawala,
    Andrew McCallum
  bibkey: das-etal-2019-chains
  bibtype: inproceedings
  booktitle: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  booktitle_html: Proceedings of the Thirteenth Workshop on Graph-Based Methods for
    Natural Language Processing (TextGraphs-13)
  doi: 10.18653/v1/D19-5313
  month: November
  page_first: '101'
  page_last: '117'
  pages: "101\u2013117"
  paper_id: '13'
  parent_volume_id: D19-53
  pdf: https://www.aclweb.org/anthology/D19-5313.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5313.jpg
  title: 'Chains-of-Reasoning at TextGraphs 2019 Shared Task: Reasoning over Chains
    of Facts for Explainable Multi-hop Inference'
  title_html: 'Chains-of-Reasoning at <span class="acl-fixed-case">T</span>ext<span
    class="acl-fixed-case">G</span>raphs 2019 Shared Task: Reasoning over Chains of
    Facts for Explainable Multi-hop Inference'
  url: https://www.aclweb.org/anthology/D19-5313
  year: '2019'
D19-5314:
  abstract: Word embeddings continue to be of great use for NLP researchers and practitioners
    due to their training speed and easiness of use and distribution. Prior work has
    shown that the representation of those words can be improved by the use of semantic
    knowledge-bases. In this paper we propose a novel way of combining those knowledge-bases
    while the lexical information of co-occurrences of words remains. It is conceptually
    clear, as it consists in mapping both distributional and semantic information
    into a multi-graph and modifying existing node embeddings techniques to compute
    word representations. Our experiments show improved results compared to vanilla
    word embeddings, retrofitting and concatenation techniques using the same information,
    on a variety of data-sets of word similarities.
  address: Hong Kong
  author:
  - first: Pierre
    full: Pierre Daix-Moreux
    id: pierre-daix-moreux
    last: Daix-Moreux
  - first: Matthias
    full: "Matthias Gall\xE9"
    id: matthias-galle
    last: "Gall\xE9"
  author_string: "Pierre Daix-Moreux, Matthias Gall\xE9"
  bibkey: daix-moreux-galle-2019-joint
  bibtype: inproceedings
  booktitle: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  booktitle_html: Proceedings of the Thirteenth Workshop on Graph-Based Methods for
    Natural Language Processing (TextGraphs-13)
  doi: 10.18653/v1/D19-5314
  month: November
  page_first: '118'
  page_last: '123'
  pages: "118\u2013123"
  paper_id: '14'
  parent_volume_id: D19-53
  pdf: https://www.aclweb.org/anthology/D19-5314.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5314.jpg
  title: Joint Semantic and Distributional Word Representations with Multi-Graph Embeddings
  title_html: Joint Semantic and Distributional Word Representations with Multi-Graph
    Embeddings
  url: https://www.aclweb.org/anthology/D19-5314
  year: '2019'
D19-5315:
  abstract: 'In this paper, we explore strategies to evaluate models for the task
    research paper novelty detection: Given all papers released at a given date, which
    of the papers discuss new ideas and influence future research? We find the novelty
    is not a singular concept, and thus inherently lacks of ground truth annotations
    with cross-annotator agreement, which is a major obstacle in evaluating these
    models. Test-of-time award is closest to such annotation, which can only be made
    retrospectively and is extremely scarce. We thus propose to compare and evaluate
    models using counterfactual simulations. First, we ask models if they can differentiate
    papers at time t and counterfactual paper from future time and counterfactual
    paper from future time t+d. Second, we ask models if they can predict test-of-time
    award at . Second, we ask models if they can predict test-of-time award at t+d.
    These are proxies that can be agreed by human annotators and easily augmented
    by correlated signals, using which evaluation can be done through four tasks:
    classification, ranking, correlation and feature selection. We show these proxy
    evaluation methods complement each other regarding error handling, coverage, interpretability,
    and scope, and thus altogether contribute to the observation of the relative strength
    of existing models.. These are proxies that can be agreed by human annotators
    and easily augmented by correlated signals, using which evaluation can be done
    through four tasks: classification, ranking, correlation and feature selection.
    We show these proxy evaluation methods complement each other regarding error handling,
    coverage, interpretability, and scope, and thus altogether contribute to the observation
    of the relative strength of existing models.'
  address: Hong Kong
  author:
  - first: Reinald Kim
    full: Reinald Kim Amplayo
    id: reinald-kim-amplayo
    last: Amplayo
  - first: Seung-won
    full: Seung-won Hwang
    id: seung-won-hwang
    last: Hwang
  - first: Min
    full: Min Song
    id: min-song
    last: Song
  author_string: Reinald Kim Amplayo, Seung-won Hwang, Min Song
  bibkey: amplayo-etal-2019-evaluating
  bibtype: inproceedings
  booktitle: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  booktitle_html: Proceedings of the Thirteenth Workshop on Graph-Based Methods for
    Natural Language Processing (TextGraphs-13)
  doi: 10.18653/v1/D19-5315
  month: November
  page_first: '124'
  page_last: '133'
  pages: "124\u2013133"
  paper_id: '15'
  parent_volume_id: D19-53
  pdf: https://www.aclweb.org/anthology/D19-5315.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5315.jpg
  title: 'Evaluating Research Novelty Detection: Counterfactual Approaches'
  title_html: 'Evaluating Research Novelty Detection: Counterfactual Approaches'
  url: https://www.aclweb.org/anthology/D19-5315
  year: '2019'
D19-5316:
  abstract: The rising growth of fake news and misleading information through online
    media outlets demands an automatic method for detecting such news articles. Of
    the few limited works which differentiate between trusted vs other types of news
    article (satire, propaganda, hoax), none of them model sentence interactions within
    a document. We observe an interesting pattern in the way sentences interact with
    each other across different kind of news articles. To capture this kind of information
    for long news articles, we propose a graph neural network-based model which does
    away with the need of feature engineering for fine grained fake news classification.
    Through experiments, we show that our proposed method beats strong neural baselines
    and achieves state-of-the-art accuracy on existing datasets. Moreover, we establish
    the generalizability of our model by evaluating its performance in out-of-domain
    scenarios. Code is available at https://github.com/MysteryVaibhav/fake_news_semantics.
  address: Hong Kong
  attachment:
  - filename: D19-5316.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5316.Attachment.pdf
  author:
  - first: Vaibhav
    full: Vaibhav Vaibhav
    id: vaibhav-vaibhav
    last: Vaibhav
  - first: Raghuram
    full: Raghuram Mandyam
    id: raghuram-mandyam
    last: Mandyam
  - first: Eduard
    full: Eduard Hovy
    id: eduard-hovy
    last: Hovy
  author_string: Vaibhav Vaibhav, Raghuram Mandyam, Eduard Hovy
  bibkey: vaibhav-etal-2019-sentence
  bibtype: inproceedings
  booktitle: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  booktitle_html: Proceedings of the Thirteenth Workshop on Graph-Based Methods for
    Natural Language Processing (TextGraphs-13)
  doi: 10.18653/v1/D19-5316
  month: November
  page_first: '134'
  page_last: '139'
  pages: "134\u2013139"
  paper_id: '16'
  parent_volume_id: D19-53
  pdf: https://www.aclweb.org/anthology/D19-5316.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5316.jpg
  title: Do Sentence Interactions Matter? Leveraging Sentence Level Representations
    for Fake News Classification
  title_html: Do Sentence Interactions Matter? Leveraging Sentence Level Representations
    for Fake News Classification
  url: https://www.aclweb.org/anthology/D19-5316
  year: '2019'
D19-5317:
  abstract: "On a scientific concept hierarchy, a parent concept may have a few attributes,\
    \ each of which has multiple values being a group of child concepts. We call these\
    \ attributes facets: classification has a few facets such as application (e.g.,\
    \ face recognition), model (e.g., svm, knn), and metric (e.g., precision). In\
    \ this work, we aim at building faceted concept hierarchies from scientific literature.\
    \ Hierarchy construction methods heavily rely on hypernym detection, however,\
    \ the faceted relations are parent-to-child links but the hypernym relation is\
    \ a multi-hop, i.e., ancestor-to-descendent link with a specific facet \u201C\
    type-of\u201D. We use information extraction techniques to find synonyms, sibling\
    \ concepts, and ancestor-descendent relations from a data science corpus. And\
    \ we propose a hierarchy growth algorithm to infer the parent-child links from\
    \ the three types of relationships. It resolves conflicts by maintaining the acyclic\
    \ structure of a hierarchy."
  address: Hong Kong
  author:
  - first: Qingkai
    full: Qingkai Zeng
    id: qingkai-zeng
    last: Zeng
  - first: Mengxia
    full: Mengxia Yu
    id: mengxia-yu
    last: Yu
  - first: Wenhao
    full: Wenhao Yu
    id: wenhao-yu
    last: Yu
  - first: JinJun
    full: JinJun Xiong
    id: jinjun-xiong
    last: Xiong
  - first: Yiyu
    full: Yiyu Shi
    id: yiyu-shi
    last: Shi
  - first: Meng
    full: Meng Jiang
    id: meng-jiang
    last: Jiang
  author_string: Qingkai Zeng, Mengxia Yu, Wenhao Yu, JinJun Xiong, Yiyu Shi, Meng
    Jiang
  bibkey: zeng-etal-2019-faceted
  bibtype: inproceedings
  booktitle: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  booktitle_html: Proceedings of the Thirteenth Workshop on Graph-Based Methods for
    Natural Language Processing (TextGraphs-13)
  doi: 10.18653/v1/D19-5317
  month: November
  page_first: '140'
  page_last: '150'
  pages: "140\u2013150"
  paper_id: '17'
  parent_volume_id: D19-53
  pdf: https://www.aclweb.org/anthology/D19-5317.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5317.jpg
  title: 'Faceted Hierarchy: A New Graph Type to Organize Scientific Concepts and
    a Construction Method'
  title_html: 'Faceted Hierarchy: A New Graph Type to Organize Scientific Concepts
    and a Construction Method'
  url: https://www.aclweb.org/anthology/D19-5317
  year: '2019'
D19-5318:
  abstract: "Semi-supervised learning is an efficient method to augment training data\
    \ automatically from unlabeled data. Development of many natural language understanding\
    \ (NLU) applications has a challenge where unlabeled data is relatively abundant\
    \ while labeled data is rather limited. In this work, we propose transductive\
    \ graph-based semi-supervised learning models as well as their inductive variants\
    \ for NLU. We evaluate the approach\u2019s applicability using publicly available\
    \ NLU data and models. In order to find similar utterances and construct a graph,\
    \ we use a paraphrase detection model. Results show that applying the inductive\
    \ graph-based semi-supervised learning can improve the error rate of the NLU model\
    \ by 5%."
  address: Hong Kong
  attachment:
  - filename: D19-5318.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5318.Attachment.zip
  author:
  - first: Zimeng
    full: Zimeng Qiu
    id: zimeng-qiu
    last: Qiu
  - first: Eunah
    full: Eunah Cho
    id: eunah-cho
    last: Cho
  - first: Xiaochun
    full: Xiaochun Ma
    id: xiaochun-ma
    last: Ma
  - first: William
    full: William Campbell
    id: william-campbell
    last: Campbell
  author_string: Zimeng Qiu, Eunah Cho, Xiaochun Ma, William Campbell
  bibkey: qiu-etal-2019-graph
  bibtype: inproceedings
  booktitle: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  booktitle_html: Proceedings of the Thirteenth Workshop on Graph-Based Methods for
    Natural Language Processing (TextGraphs-13)
  doi: 10.18653/v1/D19-5318
  month: November
  page_first: '151'
  page_last: '158'
  pages: "151\u2013158"
  paper_id: '18'
  parent_volume_id: D19-53
  pdf: https://www.aclweb.org/anthology/D19-5318.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5318.jpg
  title: Graph-Based Semi-Supervised Learning for Natural Language Understanding
  title_html: Graph-Based Semi-Supervised Learning for Natural Language Understanding
  url: https://www.aclweb.org/anthology/D19-5318
  year: '2019'
D19-5319:
  abstract: Semantic parsing is a fundamental problem in natural language understanding,
    as it involves the mapping of natural language to structured forms such as executable
    queries or logic-like knowledge representations. Existing deep learning approaches
    for semantic parsing have shown promise on a variety of benchmark data sets, particularly
    on text-to-SQL parsing. However, most text-to-SQL parsers do not generalize to
    unseen data sets in different domains. In this paper, we propose a new cross-domain
    learning scheme to perform text-to-SQL translation and demonstrate its use on
    Spider, a large-scale cross-domain text-to-SQL data set. We improve upon a state-of-the-art
    Spider model, SyntaxSQLNet, by constructing a graph of column names for all databases
    and using graph neural networks to compute their embeddings. The resulting embeddings
    offer better cross-domain representations and SQL queries, as evidenced by substantial
    improvement on the Spider data set compared to SyntaxSQLNet.
  address: Hong Kong
  author:
  - first: Siyu
    full: Siyu Huo
    id: siyu-huo
    last: Huo
  - first: Tengfei
    full: Tengfei Ma
    id: tengfei-ma
    last: Ma
  - first: Jie
    full: Jie Chen
    id: jie-chen
    last: Chen
  - first: Maria
    full: Maria Chang
    id: maria-chang
    last: Chang
  - first: Lingfei
    full: Lingfei Wu
    id: lingfei-wu
    last: Wu
  - first: Michael
    full: Michael Witbrock
    id: michael-j-witbrock
    last: Witbrock
  author_string: Siyu Huo, Tengfei Ma, Jie Chen, Maria Chang, Lingfei Wu, Michael
    Witbrock
  bibkey: huo-etal-2019-graph
  bibtype: inproceedings
  booktitle: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  booktitle_html: Proceedings of the Thirteenth Workshop on Graph-Based Methods for
    Natural Language Processing (TextGraphs-13)
  doi: 10.18653/v1/D19-5319
  month: November
  page_first: '159'
  page_last: '163'
  pages: "159\u2013163"
  paper_id: '19'
  parent_volume_id: D19-53
  pdf: https://www.aclweb.org/anthology/D19-5319.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5319.jpg
  title: Graph Enhanced Cross-Domain Text-to-SQL Generation
  title_html: Graph Enhanced Cross-Domain Text-to-<span class="acl-fixed-case">SQL</span>
    Generation
  url: https://www.aclweb.org/anthology/D19-5319
  year: '2019'
D19-5320:
  abstract: Reasoning over paths in large scale knowledge graphs is an important problem
    for many applications. In this paper we discuss a simple approach to automatically
    build and rank paths between a source and target entity pair with learned embeddings
    using a knowledge base completion model (KBC). We assembled a knowledge graph
    by mining the available biomedical scientific literature and extracted a set of
    high frequency paths to use for validation. We demonstrate that our method is
    able to effectively rank a list of known paths between a pair of entities and
    also come up with plausible paths that are not present in the knowledge graph.
    For a given entity pair we are able to reconstruct the highest ranking path 60%
    of the time within the top 10 ranked paths and achieve 49% mean average precision.
    Our approach is compositional since any KBC model that can produce vector representations
    of entities can be used.
  address: Hong Kong
  author:
  - first: Saatviga
    full: Saatviga Sudhahar
    id: saatviga-sudhahar
    last: Sudhahar
  - first: Andrea
    full: Andrea Pierleoni
    id: andrea-pierleoni
    last: Pierleoni
  - first: Ian
    full: Ian Roberts
    id: ian-roberts
    last: Roberts
  author_string: Saatviga Sudhahar, Andrea Pierleoni, Ian Roberts
  bibkey: sudhahar-etal-2019-reasoning
  bibtype: inproceedings
  booktitle: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  booktitle_html: Proceedings of the Thirteenth Workshop on Graph-Based Methods for
    Natural Language Processing (TextGraphs-13)
  doi: 10.18653/v1/D19-5320
  month: November
  page_first: '164'
  page_last: '171'
  pages: "164\u2013171"
  paper_id: '20'
  parent_volume_id: D19-53
  pdf: https://www.aclweb.org/anthology/D19-5320.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5320.jpg
  title: Reasoning Over Paths via Knowledge Base Completion
  title_html: Reasoning Over Paths via Knowledge Base Completion
  url: https://www.aclweb.org/anthology/D19-5320
  year: '2019'
D19-5321:
  abstract: Combining two graphs requires merging the nodes which are counterparts
    of each other. In this process errors occur, resulting in incorrect merging or
    incorrect failure to merge. We find a high prevalence of such errors when using
    AskNET, an algorithm for building Knowledge Graphs from text corpora. AskNET node
    matching method uses string similarity, which we propose to replace with vector
    embedding similarity. We explore graph-based and word-based embedding models and
    show an overall error reduction of from 56% to 23.6%, with a reduction of over
    a half in both types of incorrect node matching.
  address: Hong Kong
  author:
  - first: Ida
    full: Ida Szubert
    id: ida-szubert
    last: Szubert
  - first: Mark
    full: Mark Steedman
    id: mark-steedman
    last: Steedman
  author_string: Ida Szubert, Mark Steedman
  bibkey: szubert-steedman-2019-node
  bibtype: inproceedings
  booktitle: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  booktitle_html: Proceedings of the Thirteenth Workshop on Graph-Based Methods for
    Natural Language Processing (TextGraphs-13)
  doi: 10.18653/v1/D19-5321
  month: November
  page_first: '172'
  page_last: '176'
  pages: "172\u2013176"
  paper_id: '21'
  parent_volume_id: D19-53
  pdf: https://www.aclweb.org/anthology/D19-5321.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5321.jpg
  title: 'Node Embeddings for Graph Merging: Case of Knowledge Graph Construction'
  title_html: 'Node Embeddings for Graph Merging: Case of Knowledge Graph Construction'
  url: https://www.aclweb.org/anthology/D19-5321
  year: '2019'
D19-5322:
  abstract: This paper describes DBee, a database to support the construction of data-intensive
    AI applications. DBee provides a unique data model which operates jointly over
    large-scale knowledge graphs (KGs) and embedding vector spaces (VSs). This model
    supports queries which exploit the semantic properties of both types of representations
    (KGs and VSs). Additionally, DBee aims to facilitate the construction of KGs and
    VSs, by providing a library of generators, which can be used to create, integrate
    and transform data into KGs and VSs.
  address: Hong Kong
  author:
  - first: Viktor
    full: Viktor Schlegel
    id: viktor-schlegel
    last: Schlegel
  - first: "Andr\xE9"
    full: "Andr\xE9 Freitas"
    id: andre-freitas
    last: Freitas
  author_string: "Viktor Schlegel, Andr\xE9 Freitas"
  bibkey: schlegel-freitas-2019-dbee
  bibtype: inproceedings
  booktitle: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  booktitle_html: Proceedings of the Thirteenth Workshop on Graph-Based Methods for
    Natural Language Processing (TextGraphs-13)
  doi: 10.18653/v1/D19-5322
  month: November
  page_first: '177'
  page_last: '185'
  pages: "177\u2013185"
  paper_id: '22'
  parent_volume_id: D19-53
  pdf: https://www.aclweb.org/anthology/D19-5322.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5322.jpg
  title: 'DBee: A Database for Creating and Managing Knowledge Graphs and Embeddings'
  title_html: '<span class="acl-fixed-case">DB</span>ee: A Database for Creating and
    Managing Knowledge Graphs and Embeddings'
  url: https://www.aclweb.org/anthology/D19-5322
  year: '2019'
D19-5323:
  abstract: We present a simple, rule-based method for extracting entity networks
    from the abstracts of scientific literature. By taking advantage of selected syntactic
    features of constituent parsing trees, our method automatically extracts and constructs
    graphs in which nodes represent text-based entities (in this case, noun phrases)
    and their relationships (in this case, verb phrases or preposition phrases). We
    use two benchmark datasets for evaluation and compare with previously presented
    results for these data. Our evaluation results show that the proposed method leads
    to accuracy rates that are comparable to or exceed the results achieved with state-of-the-art,
    learning-based methods in several cases.
  address: Hong Kong
  author:
  - first: Ming
    full: Ming Jiang
    id: ming-jiang
    last: Jiang
  - first: Jana
    full: Jana Diesner
    id: jana-diesner
    last: Diesner
  author_string: Ming Jiang, Jana Diesner
  bibkey: jiang-diesner-2019-constituency
  bibtype: inproceedings
  booktitle: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural
    Language Processing (TextGraphs-13)
  booktitle_html: Proceedings of the Thirteenth Workshop on Graph-Based Methods for
    Natural Language Processing (TextGraphs-13)
  doi: 10.18653/v1/D19-5323
  month: November
  page_first: '186'
  page_last: '191'
  pages: "186\u2013191"
  paper_id: '23'
  parent_volume_id: D19-53
  pdf: https://www.aclweb.org/anthology/D19-5323.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5323.jpg
  title: A Constituency Parsing Tree based Method for Relation Extraction from Abstracts
    of Scholarly Publications
  title_html: A Constituency Parsing Tree based Method for Relation Extraction from
    Abstracts of Scholarly Publications
  url: https://www.aclweb.org/anthology/D19-5323
  year: '2019'
D19-5400:
  address: Hong Kong, China
  author:
  - first: Lu
    full: Lu Wang
    id: lu-wang
    last: Wang
  - first: Jackie Chi Kit
    full: Jackie Chi Kit Cheung
    id: jackie-chi-kit-cheung
    last: Cheung
  - first: Giuseppe
    full: Giuseppe Carenini
    id: giuseppe-carenini
    last: Carenini
  - first: Fei
    full: Fei Liu
    id: fei-liu-utdallas
    last: Liu
  author_string: Lu Wang, Jackie Chi Kit Cheung, Giuseppe Carenini, Fei Liu
  bibkey: emnlp-2019-new
  bibtype: proceedings
  booktitle: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  month: November
  paper_id: '0'
  parent_volume_id: D19-54
  pdf: https://www.aclweb.org/anthology/D19-5400.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5400.jpg
  title: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  title_html: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  url: https://www.aclweb.org/anthology/D19-5400
  year: '2019'
D19-5401:
  abstract: In recent years, the task of Question Answering over passages, also pitched
    as a reading comprehension, has evolved into a very active research area. A reading
    comprehension system extracts a span of text, comprising of named entities, dates,
    small phrases, etc., which serve as the answer to a given question. However, these
    spans of text would result in an unnatural reading experience in a conversational
    system. Usually, dialogue systems solve this issue by using template-based language
    generation. These systems, though adequate for a domain specific task, are too
    restrictive and predefined for a domain independent system. In order to present
    the user with a more conversational experience, we propose a pointer generator
    based full-length answer generator which can be used with most QA systems. Our
    system generates a full length answer given a question and the extracted factoid/span
    answer without relying on the passage from where the answer was extracted. We
    also present a dataset of 315000 question, factoid answer and full length answer
    triples. We have evaluated our system using ROUGE-1,2,L and BLEU and achieved
    74.05 BLEU score and 86.25 Rogue-L score.
  address: Hong Kong, China
  author:
  - first: Vaishali
    full: Vaishali Pal
    id: vaishali-pal
    last: Pal
  - first: Manish
    full: Manish Shrivastava
    id: manish-shrivastava
    last: Shrivastava
  - first: Irshad
    full: Irshad Bhat
    id: irshad-bhat
    last: Bhat
  author_string: Vaishali Pal, Manish Shrivastava, Irshad Bhat
  bibkey: pal-etal-2019-answering
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  booktitle_html: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  doi: 10.18653/v1/D19-5401
  month: November
  page_first: '1'
  page_last: '9'
  pages: "1\u20139"
  paper_id: '1'
  parent_volume_id: D19-54
  pdf: https://www.aclweb.org/anthology/D19-5401.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5401.jpg
  title: 'Answering Naturally: Factoid to Full length Answer Generation'
  title_html: 'Answering Naturally: Factoid to Full length Answer Generation'
  url: https://www.aclweb.org/anthology/D19-5401
  year: '2019'
D19-5402:
  abstract: As an attempt to combine extractive and abstractive summarization, Sentence
    Rewriting models adopt the strategy of extracting salient sentences from a document
    first and then paraphrasing the selected ones to generate a summary. However,
    the existing models in this framework mostly rely on sentence-level rewards or
    suboptimal labels, causing a mismatch between a training objective and evaluation
    metric. In this paper, we present a novel training signal that directly maximizes
    summary-level ROUGE scores through reinforcement learning. In addition, we incorporate
    BERT into our model, making good use of its ability on natural language understanding.
    In extensive experiments, we show that a combination of our proposed model and
    training procedure obtains new state-of-the-art performance on both CNN/Daily
    Mail and New York Times datasets. We also demonstrate that it generalizes better
    on DUC-2002 test set.
  address: Hong Kong, China
  attachment:
  - filename: D19-5402.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5402.Attachment.zip
  author:
  - first: Sanghwan
    full: Sanghwan Bae
    id: sanghwan-bae
    last: Bae
  - first: Taeuk
    full: Taeuk Kim
    id: taeuk-kim
    last: Kim
  - first: Jihoon
    full: Jihoon Kim
    id: jihoon-kim
    last: Kim
  - first: Sang-goo
    full: Sang-goo Lee
    id: sang-goo-lee
    last: Lee
  author_string: Sanghwan Bae, Taeuk Kim, Jihoon Kim, Sang-goo Lee
  bibkey: bae-etal-2019-summary
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  booktitle_html: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  doi: 10.18653/v1/D19-5402
  month: November
  page_first: '10'
  page_last: '20'
  pages: "10\u201320"
  paper_id: '2'
  parent_volume_id: D19-54
  pdf: https://www.aclweb.org/anthology/D19-5402.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5402.jpg
  title: Summary Level Training of Sentence Rewriting for Abstractive Summarization
  title_html: Summary Level Training of Sentence Rewriting for Abstractive Summarization
  url: https://www.aclweb.org/anthology/D19-5402
  year: '2019'
D19-5403:
  abstract: Timeline summarization (TLS) automatically identifies key dates of major
    events and provides short descriptions of what happened on these dates. Previous
    approaches to TLS have focused on extractive methods. In contrast, we suggest
    an abstractive timeline summarization system. Our system is entirely unsupervised,
    which makes it especially suited to TLS where there are very few gold summaries
    available for training of supervised systems. In addition, we present the first
    abstractive oracle experiments for TLS. Our system outperforms extractive competitors
    in terms of ROUGE when the number of input documents is high and the output requires
    strong compression. In these cases, our oracle experiments confirm that our approach
    also has a higher upper bound for ROUGE scores than extractive methods. A study
    with human judges shows that our abstractive system also produces output that
    is easy to read and understand.
  address: Hong Kong, China
  author:
  - first: Julius
    full: Julius Steen
    id: julius-steen
    last: Steen
  - first: Katja
    full: Katja Markert
    id: katja-markert
    last: Markert
  author_string: Julius Steen, Katja Markert
  bibkey: steen-markert-2019-abstractive
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  booktitle_html: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  doi: 10.18653/v1/D19-5403
  month: November
  page_first: '21'
  page_last: '31'
  pages: "21\u201331"
  paper_id: '3'
  parent_volume_id: D19-54
  pdf: https://www.aclweb.org/anthology/D19-5403.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5403.jpg
  title: Abstractive Timeline Summarization
  title_html: Abstractive Timeline Summarization
  url: https://www.aclweb.org/anthology/D19-5403
  year: '2019'
D19-5404:
  abstract: 'Linking facts across documents is a challenging task, as the language
    used to express the same information in a sentence can vary significantly, which
    complicates the task of multi-document summarization. Consequently, existing approaches
    heavily rely on hand-crafted features, which are domain-dependent and hard to
    craft, or additional annotated data, which is costly to gather. To overcome these
    limitations, we present a novel method, which makes use of two types of sentence
    embeddings: universal embeddings, which are trained on a large unrelated corpus,
    and domain-specific embeddings, which are learned during training. To this end,
    we develop SemSentSum, a fully data-driven model able to leverage both types of
    sentence embeddings by building a sentence semantic relation graph. SemSentSum
    achieves competitive results on two types of summary, consisting of 665 bytes
    and 100 words. Unlike other state-of-the-art models, neither hand-crafted features
    nor additional annotated data are necessary, and the method is easily adaptable
    for other tasks. To our knowledge, we are the first to use multiple sentence embeddings
    for the task of multi-document summarization.'
  address: Hong Kong, China
  author:
  - first: Diego
    full: Diego Antognini
    id: diego-antognini
    last: Antognini
  - first: Boi
    full: Boi Faltings
    id: boi-faltings
    last: Faltings
  author_string: Diego Antognini, Boi Faltings
  bibkey: antognini-faltings-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  booktitle_html: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  doi: 10.18653/v1/D19-5404
  month: November
  page_first: '32'
  page_last: '41'
  pages: "32\u201341"
  paper_id: '4'
  parent_volume_id: D19-54
  pdf: https://www.aclweb.org/anthology/D19-5404.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5404.jpg
  title: Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization
  title_html: Learning to Create Sentence Semantic Relation Graphs for Multi-Document
    Summarization
  url: https://www.aclweb.org/anthology/D19-5404
  year: '2019'
D19-5405:
  abstract: 'User-generated reviews of products or services provide valuable information
    to customers. However, it is often impossible to read each of the potentially
    thousands of reviews: it would therefore save valuable time to provide short summaries
    of their contents. We address opinion summarization, a multi-document summarization
    task, with an unsupervised abstractive summarization neural system. Our system
    is based on (i) a language model that is meant to encode reviews to a vector space,
    and to generate fluent sentences from the same vector space (ii) a clustering
    step that groups together reviews about the same aspects and allows the system
    to generate summary sentences focused on these aspects. Our experiments on the
    Oposum dataset empirically show the importance of the clustering step.'
  address: Hong Kong, China
  author:
  - first: Maximin
    full: Maximin Coavoux
    id: maximin-coavoux
    last: Coavoux
  - first: Hady
    full: Hady Elsahar
    id: hady-elsahar
    last: Elsahar
  - first: Matthias
    full: "Matthias Gall\xE9"
    id: matthias-galle
    last: "Gall\xE9"
  author_string: "Maximin Coavoux, Hady Elsahar, Matthias Gall\xE9"
  bibkey: coavoux-etal-2019-unsupervised
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  booktitle_html: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  doi: 10.18653/v1/D19-5405
  month: November
  page_first: '42'
  page_last: '47'
  pages: "42\u201347"
  paper_id: '5'
  parent_volume_id: D19-54
  pdf: https://www.aclweb.org/anthology/D19-5405.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5405.jpg
  title: Unsupervised Aspect-Based Multi-Document Abstractive Summarization
  title_html: Unsupervised Aspect-Based Multi-Document Abstractive Summarization
  url: https://www.aclweb.org/anthology/D19-5405
  year: '2019'
D19-5406:
  abstract: Automatic summarization methods have been studied on a variety of domains,
    including news and scientific articles. Yet, legislation has not previously been
    considered for this task, despite US Congress and state governments releasing
    tens of thousands of bills every year. In this paper, we introduce BillSum, the
    first dataset for summarization of US Congressional and California state bills.
    We explain the properties of the dataset that make it more challenging to process
    than other domains. Then, we benchmark extractive methods that consider neural
    sentence representations and traditional contextual features. Finally, we demonstrate
    that models built on Congressional bills can be used to summarize California billa,
    thus, showing that methods developed on this dataset can transfer to states without
    human-written summaries.
  address: Hong Kong, China
  author:
  - first: Anastassia
    full: Anastassia Kornilova
    id: anastassia-kornilova
    last: Kornilova
  - first: Vladimir
    full: Vladimir Eidelman
    id: vladimir-eidelman
    last: Eidelman
  author_string: Anastassia Kornilova, Vladimir Eidelman
  bibkey: kornilova-eidelman-2019-billsum
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  booktitle_html: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  doi: 10.18653/v1/D19-5406
  month: November
  page_first: '48'
  page_last: '56'
  pages: "48\u201356"
  paper_id: '6'
  parent_volume_id: D19-54
  pdf: https://www.aclweb.org/anthology/D19-5406.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5406.jpg
  title: 'BillSum: A Corpus for Automatic Summarization of US Legislation'
  title_html: '<span class="acl-fixed-case">B</span>ill<span class="acl-fixed-case">S</span>um:
    A Corpus for Automatic Summarization of <span class="acl-fixed-case">US</span>
    Legislation'
  url: https://www.aclweb.org/anthology/D19-5406
  year: '2019'
D19-5407:
  abstract: "We suggest a new idea of Editorial Network \u2013 a mixed extractive-abstractive\
    \ summarization approach, which is applied as a post-processing step over a given\
    \ sequence of extracted sentences. We further suggest an effective way for training\
    \ the \u201Ceditor\u201D based on a novel soft-labeling approach. Using the CNN/DailyMail\
    \ dataset we demonstrate the effectiveness of our approach compared to state-of-the-art\
    \ extractive-only or abstractive-only baselines."
  address: Hong Kong, China
  author:
  - first: Edward
    full: Edward Moroshko
    id: edward-moroshko
    last: Moroshko
  - first: Guy
    full: Guy Feigenblat
    id: guy-feigenblat
    last: Feigenblat
  - first: Haggai
    full: Haggai Roitman
    id: haggai-roitman
    last: Roitman
  - first: David
    full: David Konopnicki
    id: david-konopnicki
    last: Konopnicki
  author_string: Edward Moroshko, Guy Feigenblat, Haggai Roitman, David Konopnicki
  bibkey: moroshko-etal-2019-editorial
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  booktitle_html: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  doi: 10.18653/v1/D19-5407
  month: November
  page_first: '57'
  page_last: '63'
  pages: "57\u201363"
  paper_id: '7'
  parent_volume_id: D19-54
  pdf: https://www.aclweb.org/anthology/D19-5407.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5407.jpg
  title: An Editorial Network for Enhanced Document Summarization
  title_html: An Editorial Network for Enhanced Document Summarization
  url: https://www.aclweb.org/anthology/D19-5407
  year: '2019'
D19-5408:
  abstract: Highlighting is a powerful tool to pick out important content and emphasize.
    Creating summary highlights at the sub-sentence level is particularly desirable,
    because sub-sentences are more concise than whole sentences. They are also better
    suited than individual words and phrases that can potentially lead to disfluent,
    fragmented summaries. In this paper we seek to generate summary highlights by
    annotating summary-worthy sub-sentences and teaching classifiers to do the same.
    We frame the task as jointly selecting important sentences and identifying a single
    most informative textual unit from each sentence. This formulation dramatically
    reduces the task complexity involved in sentence compression. Our study provides
    new benchmarks and baselines for generating highlights at the sub-sentence level.
  address: Hong Kong, China
  author:
  - first: Kristjan
    full: Kristjan Arumae
    id: kristjan-arumae
    last: Arumae
  - first: Parminder
    full: Parminder Bhatia
    id: parminder-bhatia
    last: Bhatia
  - first: Fei
    full: Fei Liu
    id: fei-liu-utdallas
    last: Liu
  author_string: Kristjan Arumae, Parminder Bhatia, Fei Liu
  bibkey: arumae-etal-2019-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  booktitle_html: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  doi: 10.18653/v1/D19-5408
  month: November
  page_first: '64'
  page_last: '69'
  pages: "64\u201369"
  paper_id: '8'
  parent_volume_id: D19-54
  pdf: https://www.aclweb.org/anthology/D19-5408.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5408.jpg
  title: Towards Annotating and Creating Summary Highlights at Sub-sentence Level
  title_html: Towards Annotating and Creating Summary Highlights at Sub-sentence Level
  url: https://www.aclweb.org/anthology/D19-5408
  year: '2019'
D19-5409:
  abstract: "This paper introduces the SAMSum Corpus, a new dataset with abstractive\
    \ dialogue summaries. We investigate the challenges it poses for automated summarization\
    \ by testing several models and comparing their results with those obtained on\
    \ a corpus of news articles. We show that model-generated summaries of dialogues\
    \ achieve higher ROUGE scores than the model-generated summaries of news \u2013\
    \ in contrast with human evaluators\u2019 judgement. This suggests that a challenging\
    \ task of abstractive dialogue summarization requires dedicated models and non-standard\
    \ quality measures. To our knowledge, our study is the first attempt to introduce\
    \ a high-quality chat-dialogues corpus, manually annotated with abstractive summarizations,\
    \ which can be used by the research community for further studies."
  address: Hong Kong, China
  author:
  - first: Bogdan
    full: Bogdan Gliwa
    id: bogdan-gliwa
    last: Gliwa
  - first: Iwona
    full: Iwona Mochol
    id: iwona-mochol
    last: Mochol
  - first: Maciej
    full: Maciej Biesek
    id: maciej-biesek
    last: Biesek
  - first: Aleksander
    full: Aleksander Wawer
    id: aleksander-wawer
    last: Wawer
  author_string: Bogdan Gliwa, Iwona Mochol, Maciej Biesek, Aleksander Wawer
  bibkey: gliwa-etal-2019-samsum
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  booktitle_html: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  doi: 10.18653/v1/D19-5409
  month: November
  page_first: '70'
  page_last: '79'
  pages: "70\u201379"
  paper_id: '9'
  parent_volume_id: D19-54
  pdf: https://www.aclweb.org/anthology/D19-5409.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5409.jpg
  title: 'SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization'
  title_html: '<span class="acl-fixed-case">SAMS</span>um Corpus: A Human-annotated
    Dialogue Dataset for Abstractive Summarization'
  url: https://www.aclweb.org/anthology/D19-5409
  year: '2019'
D19-5410:
  abstract: In this paper, we take stock of the current state of summarization datasets
    and explore how different factors of datasets influence the generalization behaviour
    of neural extractive summarization models. Specifically, we first propose several
    properties of datasets, which matter for the generalization of summarization models.
    Then we build the connection between priors residing in datasets and model designs,
    analyzing how different properties of datasets influence the choices of model
    structure design and training methods. Finally, by taking a typical dataset as
    an example, we rethink the process of the model design based on the experience
    of the above analysis. We demonstrate that when we have a deep understanding of
    the characteristics of datasets, a simple approach can bring significant improvements
    to the existing state-of-the-art model.
  address: Hong Kong, China
  author:
  - first: Ming
    full: Ming Zhong
    id: ming-zhong
    last: Zhong
  - first: Danqing
    full: Danqing Wang
    id: danqing-wang
    last: Wang
  - first: Pengfei
    full: Pengfei Liu
    id: pengfei-liu
    last: Liu
  - first: Xipeng
    full: Xipeng Qiu
    id: xipeng-qiu
    last: Qiu
  - first: Xuanjing
    full: Xuanjing Huang
    id: xuan-jing-huang
    last: Huang
  author_string: Ming Zhong, Danqing Wang, Pengfei Liu, Xipeng Qiu, Xuanjing Huang
  bibkey: zhong-etal-2019-closer
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  booktitle_html: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  doi: 10.18653/v1/D19-5410
  month: November
  page_first: '80'
  page_last: '89'
  pages: "80\u201389"
  paper_id: '10'
  parent_volume_id: D19-54
  pdf: https://www.aclweb.org/anthology/D19-5410.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5410.jpg
  title: A Closer Look at Data Bias in Neural Extractive Summarization Models
  title_html: A Closer Look at Data Bias in Neural Extractive Summarization Models
  url: https://www.aclweb.org/anthology/D19-5410
  year: '2019'
D19-5411:
  abstract: We construct Global Voices, a multilingual dataset for evaluating cross-lingual
    summarization methods. We extract social-network descriptions of Global Voices
    news articles to cheaply collect evaluation data for into-English and from-English
    summarization in 15 languages. Especially, for the into-English summarization
    task, we crowd-source a high-quality evaluation dataset based on guidelines that
    emphasize accuracy, coverage, and understandability. To ensure the quality of
    this dataset, we collect human ratings to filter out bad summaries, and conduct
    a survey on humans, which shows that the remaining summaries are preferred over
    the social-network summaries. We study the effect of translation quality in cross-lingual
    summarization, comparing a translate-then-summarize approach with several baselines.
    Our results highlight the limitations of the ROUGE metric that are overlooked
    in monolingual summarization.
  address: Hong Kong, China
  author:
  - first: Khanh
    full: Khanh Nguyen
    id: khanh-nguyen
    last: Nguyen
  - first: Hal
    full: "Hal Daum\xE9 III"
    id: hal-daume-iii
    last: "Daum\xE9 III"
  author_string: "Khanh Nguyen, Hal Daum\xE9 III"
  bibkey: nguyen-daume-iii-2019-global
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  booktitle_html: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  doi: 10.18653/v1/D19-5411
  month: November
  page_first: '90'
  page_last: '97'
  pages: "90\u201397"
  paper_id: '11'
  parent_volume_id: D19-54
  pdf: https://www.aclweb.org/anthology/D19-5411.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5411.jpg
  title: 'Global Voices: Crossing Borders in Automatic News Summarization'
  title_html: 'Global Voices: Crossing Borders in Automatic News Summarization'
  url: https://www.aclweb.org/anthology/D19-5411
  year: '2019'
D19-5412:
  abstract: Emerged as one of the best performing techniques for extractive summarization,
    determinantal point processes select a most probable set of summary sentences
    according to a probabilistic measure defined by respectively modeling sentence
    prominence and pairwise repulsion. Traditionally, both aspects are modelled using
    shallow and linguistically informed features, but the rise of deep contextualized
    representations raises an interesting question. Whether, and to what extent, could
    contextualized sentence representations be used to improve the DPP framework?
    Our findings suggest that, despite the success of deep semantic representations,
    it remains necessary to combine them with surface indicators for effective identification
    of summary-worthy sentences.
  address: Hong Kong, China
  author:
  - first: Sangwoo
    full: Sangwoo Cho
    id: sangwoo-cho
    last: Cho
  - first: Chen
    full: Chen Li
    id: chen-li
    last: Li
  - first: Dong
    full: Dong Yu
    id: dong-yu
    last: Yu
  - first: Hassan
    full: Hassan Foroosh
    id: hassan-foroosh
    last: Foroosh
  - first: Fei
    full: Fei Liu
    id: fei-liu-utdallas
    last: Liu
  author_string: Sangwoo Cho, Chen Li, Dong Yu, Hassan Foroosh, Fei Liu
  bibkey: cho-etal-2019-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  booktitle_html: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  doi: 10.18653/v1/D19-5412
  month: November
  page_first: '98'
  page_last: '103'
  pages: "98\u2013103"
  paper_id: '12'
  parent_volume_id: D19-54
  pdf: https://www.aclweb.org/anthology/D19-5412.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5412.jpg
  title: Multi-Document Summarization with Determinantal Point Processes and Contextualized
    Representations
  title_html: Multi-Document Summarization with Determinantal Point Processes and
    Contextualized Representations
  url: https://www.aclweb.org/anthology/D19-5412
  year: '2019'
D19-5413:
  abstract: While recent work in abstractive summarization has resulted in higher
    scores in automatic metrics, there is little understanding on how these systems
    combine information taken from multiple document sentences. In this paper, we
    analyze the outputs of five state-of-the-art abstractive summarizers, focusing
    on summary sentences that are formed by sentence fusion. We ask assessors to judge
    the grammaticality, faithfulness, and method of fusion for summary sentences.
    Our analysis reveals that system sentences are mostly grammatical, but often fail
    to remain faithful to the original article.
  address: Hong Kong, China
  author:
  - first: Logan
    full: Logan Lebanoff
    id: logan-lebanoff
    last: Lebanoff
  - first: John
    full: John Muchovej
    id: john-muchovej
    last: Muchovej
  - first: Franck
    full: Franck Dernoncourt
    id: franck-dernoncourt
    last: Dernoncourt
  - first: Doo Soon
    full: Doo Soon Kim
    id: doo-soon-kim
    last: Kim
  - first: Seokhwan
    full: Seokhwan Kim
    id: seokhwan-kim
    last: Kim
  - first: Walter
    full: Walter Chang
    id: walter-chang
    last: Chang
  - first: Fei
    full: Fei Liu
    id: fei-liu-utdallas
    last: Liu
  author_string: Logan Lebanoff, John Muchovej, Franck Dernoncourt, Doo Soon Kim,
    Seokhwan Kim, Walter Chang, Fei Liu
  bibkey: lebanoff-etal-2019-analyzing
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  booktitle_html: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  doi: 10.18653/v1/D19-5413
  month: November
  page_first: '104'
  page_last: '110'
  pages: "104\u2013110"
  paper_id: '13'
  parent_volume_id: D19-54
  pdf: https://www.aclweb.org/anthology/D19-5413.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5413.jpg
  title: Analyzing Sentence Fusion in Abstractive Summarization
  title_html: Analyzing Sentence Fusion in Abstractive Summarization
  url: https://www.aclweb.org/anthology/D19-5413
  year: '2019'
D19-5414:
  abstract: 'Concept maps are visual summaries, structured as directed graphs: important
    concepts from a dataset are displayed as vertexes, and edges between vertexes
    show natural language descriptions of the relationships between the concepts on
    the map. Thus far, preliminary attempts at automatically creating concept maps
    have focused on building static summaries. However, in interactive settings, users
    will need to dynamically investigate particular relationships between pairs of
    concepts. For instance, a historian using a concept map browser might decide to
    investigate the relationship between two politicians in a news archive. We present
    a model which responds to such queries by returning one or more short, importance-ranked,
    natural language descriptions of the relationship between two requested concepts,
    for display in a visual interface. Our model is trained on a new public dataset,
    collected for this task.'
  address: Hong Kong, China
  author:
  - first: Abram
    full: Abram Handler
    id: abram-handler
    last: Handler
  - first: Premkumar
    full: Premkumar Ganeshkumar
    id: premkumar-ganeshkumar
    last: Ganeshkumar
  - first: Brendan
    full: "Brendan O\u2019Connor"
    id: brendan-oconnor
    last: "O\u2019Connor"
  - first: Mohamed
    full: Mohamed AlTantawy
    id: mohamed-altantawy
    last: AlTantawy
  author_string: "Abram Handler, Premkumar Ganeshkumar, Brendan O\u2019Connor, Mohamed\
    \ AlTantawy"
  bibkey: handler-etal-2019-summarizing
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  booktitle_html: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  doi: 10.18653/v1/D19-5414
  month: November
  page_first: '111'
  page_last: '115'
  pages: "111\u2013115"
  paper_id: '14'
  parent_volume_id: D19-54
  pdf: https://www.aclweb.org/anthology/D19-5414.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5414.jpg
  title: Summarizing Relationships for Interactive Concept Map Browsers
  title_html: Summarizing Relationships for Interactive Concept Map Browsers
  url: https://www.aclweb.org/anthology/D19-5414
  year: '2019'
D19-5415:
  abstract: Extractive summarization selects and concatenates the most essential text
    spans in a document. Most, if not all, neural approaches use sentences as the
    elementary unit to select content for summarization. However, semantic segments
    containing supplementary information or descriptive details are often nonessential
    in the generated summaries. In this work, we propose to exploit discourse-level
    segmentation as a finer-grained means to more precisely pinpoint the core content
    in a document. We investigate how the sub-sentential segmentation improves extractive
    summarization performance when content selection is modeled through two basic
    neural network architectures and a deep bi-directional transformer. Experiment
    results on the CNN/Daily Mail dataset show that discourse-level segmentation is
    effective in both cases. In particular, we achieve state-of-the-art performance
    when discourse-level segmentation is combined with our adapted contextual representation
    model.
  address: Hong Kong, China
  author:
  - first: Zhengyuan
    full: Zhengyuan Liu
    id: zhengyuan-liu
    last: Liu
  - first: Nancy
    full: Nancy Chen
    id: nancy-chen
    last: Chen
  author_string: Zhengyuan Liu, Nancy Chen
  bibkey: liu-chen-2019-exploiting
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  booktitle_html: Proceedings of the 2nd Workshop on New Frontiers in Summarization
  doi: 10.18653/v1/D19-5415
  month: November
  page_first: '116'
  page_last: '121'
  pages: "116\u2013121"
  paper_id: '15'
  parent_volume_id: D19-54
  pdf: https://www.aclweb.org/anthology/D19-5415.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5415.jpg
  title: Exploiting Discourse-Level Segmentation for Extractive Summarization
  title_html: Exploiting Discourse-Level Segmentation for Extractive Summarization
  url: https://www.aclweb.org/anthology/D19-5415
  year: '2019'
D19-5500:
  address: Hong Kong, China
  author:
  - first: Wei
    full: Wei Xu
    id: wei-xu
    last: Xu
  - first: Alan
    full: Alan Ritter
    id: alan-ritter
    last: Ritter
  - first: Tim
    full: Tim Baldwin
    id: timothy-baldwin
    last: Baldwin
  - first: Afshin
    full: Afshin Rahimi
    id: afshin-rahimi
    last: Rahimi
  author_string: Wei Xu, Alan Ritter, Tim Baldwin, Afshin Rahimi
  bibkey: emnlp-2019-noisy
  bibtype: proceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  month: November
  paper_id: '0'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5500.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5500.jpg
  title: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  title_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  url: https://www.aclweb.org/anthology/D19-5500
  year: '2019'
D19-5501:
  abstract: 'In many review classification applications, a fine-grained analysis of
    the reviews is desirable, because different segments (e.g., sentences) of a review
    may focus on different aspects of the entity in question. However, training supervised
    models for segment-level classification requires segment labels, which may be
    more difficult or expensive to obtain than review labels. In this paper, we employ
    Multiple Instance Learning (MIL) and use only weak supervision in the form of
    a single label per review. First, we show that when inappropriate MIL aggregation
    functions are used, then MIL-based networks are outperformed by simpler baselines.
    Second, we propose a new aggregation function based on the sigmoid attention mechanism
    and show that our proposed model outperforms the state-of-the-art models for segment-level
    sentiment classification (by up to 9.8% in F1). Finally, we highlight the importance
    of fine-grained predictions in an important public-health application: finding
    actionable reports of foodborne illness. We show that our model achieves 48.6%
    higher recall compared to previous models, thus increasing the chance of identifying
    previously unknown foodborne outbreaks.'
  address: Hong Kong, China
  attachment:
  - filename: D19-5501.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5501.Attachment.zip
  author:
  - first: Giannis
    full: Giannis Karamanolakis
    id: giannis-karamanolakis
    last: Karamanolakis
  - first: Daniel
    full: Daniel Hsu
    id: daniel-hsu
    last: Hsu
  - first: Luis
    full: Luis Gravano
    id: luis-gravano
    last: Gravano
  author_string: Giannis Karamanolakis, Daniel Hsu, Luis Gravano
  bibkey: karamanolakis-etal-2019-weakly
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5501
  month: November
  page_first: '1'
  page_last: '10'
  pages: "1\u201310"
  paper_id: '1'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5501.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5501.jpg
  title: Weakly Supervised Attention Networks for Fine-Grained Opinion Mining and
    Public Health
  title_html: Weakly Supervised Attention Networks for Fine-Grained Opinion Mining
    and Public Health
  url: https://www.aclweb.org/anthology/D19-5501
  year: '2019'
D19-5502:
  abstract: "Typical datasets used for style transfer in NLP contain aligned pairs\
    \ of two opposite extremes of a style. As each existing dataset is sourced from\
    \ a specific domain and context, most use cases will have a sizable mismatch from\
    \ the vocabulary and sentence structures of any dataset available. This reduces\
    \ the performance of the style transfer, and is particularly significant for noisy,\
    \ user-generated text. To solve this problem, we show a technique to derive a\
    \ dataset of aligned pairs (style-agnostic vs stylistic sentences) from an unlabeled\
    \ corpus by using an auxiliary dataset, allowing for in-domain training. We test\
    \ the technique with the Yahoo Formality Dataset and 6 novel datasets we produced,\
    \ which consist of scripts from 5 popular TV-shows (Friends, Futurama, Seinfeld,\
    \ Southpark, Stargate SG-1) and the Slate Star Codex online forum. We gather 1080\
    \ human evaluations, which show that our method produces a sizable change in formality\
    \ while maintaining fluency and context; and that it considerably outperforms\
    \ OpenNMT\u2019s Seq2Seq model directly trained on the Yahoo Formality Dataset.\
    \ Additionally, we publish the full pipeline code and our novel datasets."
  address: Hong Kong, China
  attachment:
  - filename: D19-5502.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5502.Attachment.zip
  author:
  - first: Isak
    full: Isak Czeresnia Etinger
    id: isak-czeresnia-etinger
    last: Czeresnia Etinger
  - first: Alan W
    full: Alan W Black
    id: alan-w-black
    last: Black
  author_string: Isak Czeresnia Etinger, Alan W Black
  bibkey: czeresnia-etinger-black-2019-formality
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5502
  month: November
  page_first: '11'
  page_last: '16'
  pages: "11\u201316"
  paper_id: '2'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5502.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5502.jpg
  title: 'Formality Style Transfer for Noisy, User-generated Conversations: Extracting
    Labeled, Parallel Data from Unlabeled Corpora'
  title_html: 'Formality Style Transfer for Noisy, User-generated Conversations: Extracting
    Labeled, Parallel Data from Unlabeled Corpora'
  url: https://www.aclweb.org/anthology/D19-5502
  year: '2019'
D19-5503:
  abstract: Naturally occurring paraphrase data, such as multiple news stories about
    the same event, is a useful but rare resource. This paper compares translation-based
    paraphrase gathering using human, automatic, or hybrid techniques to monolingual
    paraphrasing by experts and non-experts. We gather translations, paraphrases,
    and empirical human quality assessments of these approaches. Neural machine translation
    techniques, especially when pivoting through related languages, provide a relatively
    robust source of paraphrases with diversity comparable to expert human paraphrases.
    Surprisingly, human translators do not reliably outperform neural systems. The
    resulting data release will not only be a useful test set, but will also allow
    additional explorations in translation and paraphrase quality assessments and
    relationships.
  address: Hong Kong, China
  author:
  - first: Christian
    full: Christian Federmann
    id: christian-federmann
    last: Federmann
  - first: Oussama
    full: Oussama Elachqar
    id: oussama-elachqar
    last: Elachqar
  - first: Chris
    full: Chris Quirk
    id: chris-quirk
    last: Quirk
  author_string: Christian Federmann, Oussama Elachqar, Chris Quirk
  bibkey: federmann-etal-2019-multilingual
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5503
  month: November
  page_first: '17'
  page_last: '26'
  pages: "17\u201326"
  paper_id: '3'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5503.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5503.jpg
  title: 'Multilingual Whispers: Generating Paraphrases with Translation'
  title_html: 'Multilingual Whispers: Generating Paraphrases with Translation'
  url: https://www.aclweb.org/anthology/D19-5503
  year: '2019'
D19-5504:
  abstract: "Grammar error correction (GEC) systems have become ubiquitous in a variety\
    \ of software applications, and have started to approach human-level performance\
    \ for some datasets. However, very little is known about how to efficiently personalize\
    \ these systems to the user\u2019s characteristics, such as their proficiency\
    \ level and first language, or to emerging domains of text. We present the first\
    \ results on adapting a general purpose neural GEC system to both the proficiency\
    \ level and the first language of a writer, using only a few thousand annotated\
    \ sentences. Our study is the broadest of its kind, covering five proficiency\
    \ levels and twelve different languages, and comparing three different adaptation\
    \ scenarios: adapting to the proficiency level only, to the first language only,\
    \ or to both aspects simultaneously. We show that tailoring to both scenarios\
    \ achieves the largest performance improvement (3.6 F0.5) relative to a strong\
    \ baseline."
  address: Hong Kong, China
  attachment:
  - filename: D19-5504.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5504.Attachment.pdf
  author:
  - first: Maria
    full: Maria Nadejde
    id: maria-nadejde
    last: Nadejde
  - first: Joel
    full: Joel Tetreault
    id: joel-tetreault
    last: Tetreault
  author_string: Maria Nadejde, Joel Tetreault
  bibkey: nadejde-tetreault-2019-personalizing
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5504
  month: November
  page_first: '27'
  page_last: '33'
  pages: "27\u201333"
  paper_id: '4'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5504.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5504.jpg
  title: 'Personalizing Grammatical Error Correction: Adaptation to Proficiency Level
    and L1'
  title_html: 'Personalizing Grammatical Error Correction: Adaptation to Proficiency
    Level and <span class="acl-fixed-case">L</span>1'
  url: https://www.aclweb.org/anthology/D19-5504
  year: '2019'
D19-5505:
  abstract: In this paper, we investigate the modeling power of contextualized embeddings
    from pre-trained language models, e.g. BERT, on the E2E-ABSA task. Specifically,
    we build a series of simple yet insightful neural baselines to deal with E2E-ABSA.
    The experimental results show that even with a simple linear classification layer,
    our BERT-based architecture can outperform state-of-the-art works. Besides, we
    also standardize the comparative study by consistently utilizing a hold-out validation
    dataset for model selection, which is largely ignored by previous works. Therefore,
    our work can serve as a BERT-based benchmark for E2E-ABSA.
  address: Hong Kong, China
  author:
  - first: Xin
    full: Xin Li
    id: xin-li
    last: Li
  - first: Lidong
    full: Lidong Bing
    id: lidong-bing
    last: Bing
  - first: Wenxuan
    full: Wenxuan Zhang
    id: wenxuan-zhang
    last: Zhang
  - first: Wai
    full: Wai Lam
    id: wai-lam
    last: Lam
  author_string: Xin Li, Lidong Bing, Wenxuan Zhang, Wai Lam
  bibkey: li-etal-2019-exploiting
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5505
  month: November
  page_first: '34'
  page_last: '41'
  pages: "34\u201341"
  paper_id: '5'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5505.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5505.jpg
  title: Exploiting BERT for End-to-End Aspect-based Sentiment Analysis
  title_html: Exploiting <span class="acl-fixed-case">BERT</span> for End-to-End Aspect-based
    Sentiment Analysis
  url: https://www.aclweb.org/anthology/D19-5505
  year: '2019'
D19-5506:
  abstract: Contemporary machine translation systems achieve greater coverage by applying
    subword models such as BPE and character-level CNNs, but these methods are highly
    sensitive to orthographical variations such as spelling mistakes. We show how
    training on a mild amount of random synthetic noise can dramatically improve robustness
    to these variations, without diminishing performance on clean text. We focus on
    translation performance on natural typos, and show that robustness to such noise
    can be achieved using a balanced diet of simple synthetic noises at training time,
    without access to the natural noise data or distribution.
  address: Hong Kong, China
  author:
  - first: Vladimir
    full: Vladimir Karpukhin
    id: vladimir-karpukhin
    last: Karpukhin
  - first: Omer
    full: Omer Levy
    id: omer-levy
    last: Levy
  - first: Jacob
    full: Jacob Eisenstein
    id: jacob-eisenstein
    last: Eisenstein
  - first: Marjan
    full: Marjan Ghazvininejad
    id: marjan-ghazvininejad
    last: Ghazvininejad
  author_string: Vladimir Karpukhin, Omer Levy, Jacob Eisenstein, Marjan Ghazvininejad
  bibkey: karpukhin-etal-2019-training
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5506
  month: November
  page_first: '42'
  page_last: '47'
  pages: "42\u201347"
  paper_id: '6'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5506.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5506.jpg
  title: Training on Synthetic Noise Improves Robustness to Natural Noise in Machine
    Translation
  title_html: Training on Synthetic Noise Improves Robustness to Natural Noise in
    Machine Translation
  url: https://www.aclweb.org/anthology/D19-5506
  year: '2019'
D19-5507:
  abstract: "Illicit activity on the Web often uses noisy text to obscure information\
    \ between client and seller, such as the seller\u2019s phone number. This presents\
    \ an interesting challenge to language understanding systems; how do we model\
    \ adversarial noise in a text extraction system? This paper addresses the sex\
    \ trafficking domain, and proposes some of the first neural network architectures\
    \ to learn and extract phone numbers from noisy text. We create a new adversarial\
    \ advertisement dataset, propose several RNN-based models to solve the problem,\
    \ and most notably propose a visual character language model to interpret unseen\
    \ unicode characters. We train a CRF jointly with a CNN to improve number recognition\
    \ by 89% over just a CRF. Through data augmentation in this unique model, we present\
    \ the first results on characters never seen in training."
  address: Hong Kong, China
  author:
  - first: Nathanael
    full: Nathanael Chambers
    id: nathanael-chambers
    last: Chambers
  - first: Timothy
    full: Timothy Forman
    id: timothy-forman
    last: Forman
  - first: Catherine
    full: Catherine Griswold
    id: catherine-griswold
    last: Griswold
  - first: Kevin
    full: Kevin Lu
    id: kevin-lu
    last: Lu
  - first: Yogaish
    full: Yogaish Khastgir
    id: yogaish-khastgir
    last: Khastgir
  - first: Stephen
    full: Stephen Steckler
    id: stephen-steckler
    last: Steckler
  author_string: Nathanael Chambers, Timothy Forman, Catherine Griswold, Kevin Lu,
    Yogaish Khastgir, Stephen Steckler
  bibkey: chambers-etal-2019-character
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5507
  month: November
  page_first: '48'
  page_last: '56'
  pages: "48\u201356"
  paper_id: '7'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5507.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5507.jpg
  title: 'Character-Based Models for Adversarial Phone Extraction: Preventing Human
    Sex Trafficking'
  title_html: 'Character-Based Models for Adversarial Phone Extraction: Preventing
    Human Sex Trafficking'
  url: https://www.aclweb.org/anthology/D19-5507
  year: '2019'
D19-5508:
  abstract: Language is an important marker of a cultural group, large or small. One
    aspect of language variation between communities is the employment of highly specialized
    terms with unique significance to the group. We study these high affinity terms
    across a wide variety of communities by leveraging the rich diversity of Reddit.com.
    We provide a systematic exploration of high affinity terms, the often rapid semantic
    shifts they undergo, and their relationship to subreddit characteristics across
    2600 diverse subreddits. Our results show that high affinity terms are effective
    signals of loyal communities, they undergo more semantic shift than low affinity
    terms, and that they are partial barrier to entry for new users. We conclude that
    Reddit is a robust and valuable data source for testing further theories about
    high affinity terms across communities.
  address: Hong Kong, China
  author:
  - first: Abhinav
    full: Abhinav Bhandari
    id: abhinav-bhandari
    last: Bhandari
  - first: Caitrin
    full: Caitrin Armstrong
    id: caitrin-armstrong
    last: Armstrong
  author_string: Abhinav Bhandari, Caitrin Armstrong
  bibkey: bhandari-armstrong-2019-tkol
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5508
  month: November
  page_first: '57'
  page_last: '67'
  pages: "57\u201367"
  paper_id: '8'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5508.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5508.jpg
  title: 'Tkol, Httt, and r/radiohead: High Affinity Terms in Reddit Communities'
  title_html: 'Tkol, Httt, and r/radiohead: High Affinity Terms in <span class="acl-fixed-case">R</span>eddit
    Communities'
  url: https://www.aclweb.org/anthology/D19-5508
  year: '2019'
D19-5509:
  abstract: The goal of a Question Paraphrase Retrieval (QPR) system is to retrieve
    equivalent questions that result in the same answer as the original question.
    Such a system can be used to understand and answer rare and noisy reformulations
    of common questions by mapping them to a set of canonical forms. This has large-scale
    applications for community Question Answering (cQA) and open-domain spoken language
    question answering systems. In this paper we describe a new QPR system implemented
    as a Neural Information Retrieval (NIR) system consisting of a neural network
    sentence encoder and an approximate k-Nearest Neighbour index for efficient vector
    retrieval. We also describe our mechanism to generate an annotated dataset for
    question paraphrase retrieval experiments automatically from question-answer logs
    via distant supervision. We show that the standard loss function in NIR, triplet
    loss, does not perform well with noisy labels. We propose smoothed deep metric
    loss (SDML) and with our experiments on two QPR datasets we show that it significantly
    outperforms triplet loss in the noisy label setting.
  address: Hong Kong, China
  author:
  - first: Daniele
    full: Daniele Bonadiman
    id: daniele-bonadiman
    last: Bonadiman
  - first: Anjishnu
    full: Anjishnu Kumar
    id: anjishnu-kumar
    last: Kumar
  - first: Arpit
    full: Arpit Mittal
    id: arpit-mittal
    last: Mittal
  author_string: Daniele Bonadiman, Anjishnu Kumar, Arpit Mittal
  bibkey: bonadiman-etal-2019-large
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5509
  month: November
  page_first: '68'
  page_last: '75'
  pages: "68\u201375"
  paper_id: '9'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5509.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5509.jpg
  title: Large Scale Question Paraphrase Retrieval with Smoothed Deep Metric Learning
  title_html: Large Scale Question Paraphrase Retrieval with Smoothed Deep Metric
    Learning
  url: https://www.aclweb.org/anthology/D19-5509
  year: '2019'
D19-5510:
  abstract: "User reviews provide a significant source of information for companies\
    \ to understand their market and audience. In order to discover broad trends in\
    \ this source, researchers have typically used topic models such as Latent Dirichlet\
    \ Allocation (LDA). However, while there are metrics to choose the \u201Cbest\u201D\
    \ number of topics, it is not clear whether the resulting topics can also provide\
    \ in-depth, actionable product analysis. Our paper examines this issue by analyzing\
    \ user reviews from the Best Buy US website for smart speakers. Using coherence\
    \ scores to choose topics, we test whether the results help us to understand user\
    \ interests and concerns. We find that while coherence scores are a good starting\
    \ point to identify a number of topics, it still requires manual adaptation based\
    \ on domain knowledge to provide market insights. We show that the resulting dimensions\
    \ capture brand performance and differences, and differentiate the market into\
    \ two distinct groups with different properties."
  address: Hong Kong, China
  author:
  - first: Hanh
    full: Hanh Nguyen
    id: hanh-nguyen
    last: Nguyen
  - first: Dirk
    full: Dirk Hovy
    id: dirk-hovy
    last: Hovy
  author_string: Hanh Nguyen, Dirk Hovy
  bibkey: nguyen-hovy-2019-hey
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5510
  month: November
  page_first: '76'
  page_last: '83'
  pages: "76\u201383"
  paper_id: '10'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5510.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5510.jpg
  title: 'Hey Siri. Ok Google. Alexa: A topic modeling of user reviews for smart speakers'
  title_html: 'Hey <span class="acl-fixed-case">S</span>iri. Ok <span class="acl-fixed-case">G</span>oogle.
    <span class="acl-fixed-case">A</span>lexa: A topic modeling of user reviews for
    smart speakers'
  url: https://www.aclweb.org/anthology/D19-5510
  year: '2019'
D19-5511:
  abstract: We introduce the task of algorithm class prediction for programming word
    problems. A programming word problem is a problem written in natural language,
    which can be solved using an algorithm or a program. We define classes of various
    programming word problems which correspond to the class of algorithms required
    to solve the problem. We present four new datasets for this task, two multiclass
    datasets with 550 and 1159 problems each and two multilabel datasets having 3737
    and 3960 problems each. We pose the problem as a text classification problem and
    train neural network and non-neural network based models on this task. Our best
    performing classifier gets an accuracy of 62.7 percent for the multiclass case
    on the five class classification dataset, Codeforces Multiclass-5 (CFMC5). We
    also do some human-level analysis and compare human performance with that of our
    text classification models. Our best classifier has an accuracy only 9 percent
    lower than that of a human on this task. To the best of our knowledge, these are
    the first reported results on such a task. We make our code and datasets publicly
    available.
  address: Hong Kong, China
  author:
  - first: Vinayak
    full: Vinayak Athavale
    id: vinayak-athavale
    last: Athavale
  - first: Aayush
    full: Aayush Naik
    id: aayush-naik
    last: Naik
  - first: Rajas
    full: Rajas Vanjape
    id: rajas-vanjape
    last: Vanjape
  - first: Manish
    full: Manish Shrivastava
    id: manish-shrivastava
    last: Shrivastava
  author_string: Vinayak Athavale, Aayush Naik, Rajas Vanjape, Manish Shrivastava
  bibkey: athavale-etal-2019-predicting
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5511
  month: November
  page_first: '84'
  page_last: '93'
  pages: "84\u201393"
  paper_id: '11'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5511.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5511.jpg
  title: Predicting Algorithm Classes for Programming Word Problems
  title_html: Predicting Algorithm Classes for Programming Word Problems
  url: https://www.aclweb.org/anthology/D19-5511
  year: '2019'
D19-5512:
  abstract: "Psychologically motivated, lexicon-based text analysis methods such as\
    \ LIWC (Pennebaker et al., 2015) have been criticized by computational linguists\
    \ for their lack of adaptability, but they have not often been systematically\
    \ compared with either human evaluations or machine learning approaches. The goal\
    \ of the current study was to assess the effectiveness and predictive ability\
    \ of LIWC on a relationship goal classification task. In this paper, we compared\
    \ the outcomes of (1) LIWC, (2) machine learning, and (3) a human baseline. A\
    \ newly collected corpus of online dating profile texts (a genre not explored\
    \ before in the ACL anthology) was used, accompanied by the profile writers\u2019\
    \ self-selected relationship goal (long-term versus date). These three approaches\
    \ were tested by comparing their performance on identifying both the intended\
    \ relationship goal and content-related text labels. Results show that LIWC and\
    \ machine learning models correlate with human evaluations in terms of content-related\
    \ labels. LIWC\u2019s content-related labels corresponded more strongly to humans\
    \ than those of the classifier. Moreover, all approaches were similarly accurate\
    \ in predicting the relationship goal."
  address: Hong Kong, China
  attachment:
  - filename: D19-5512.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5512.Attachment.pdf
  author:
  - first: Chris
    full: Chris van der Lee
    id: chris-van-der-lee
    last: van der Lee
  - first: Tess
    full: Tess van der Zanden
    id: tess-van-der-zanden
    last: van der Zanden
  - first: Emiel
    full: Emiel Krahmer
    id: emiel-krahmer
    last: Krahmer
  - first: Maria
    full: Maria Mos
    id: maria-mos
    last: Mos
  - first: Alexander
    full: Alexander Schouten
    id: alexander-schouten
    last: Schouten
  author_string: Chris van der Lee, Tess van der Zanden, Emiel Krahmer, Maria Mos,
    Alexander Schouten
  bibkey: van-der-lee-etal-2019-automatic
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5512
  month: November
  page_first: '94'
  page_last: '100'
  pages: "94\u2013100"
  paper_id: '12'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5512.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5512.jpg
  title: "Automatic identification of writers\u2019 intentions: Comparing different\
    \ methods for predicting relationship goals in online dating profile texts"
  title_html: "Automatic identification of writers\u2019 intentions: Comparing different\
    \ methods for predicting relationship goals in online dating profile texts"
  url: https://www.aclweb.org/anthology/D19-5512
  year: '2019'
D19-5513:
  abstract: We describe a special type of deep contextualized word representation
    that is learned from distant supervision annotations and dedicated to named entity
    recognition. Our extensive experiments on 7 datasets show systematic gains across
    all domains over strong baselines, and demonstrate that our representation is
    complementary to previously proposed embeddings. We report new state-of-the-art
    results on CONLL and ONTONOTES datasets.
  address: Hong Kong, China
  author:
  - first: Abbas
    full: Abbas Ghaddar
    id: abbas-ghaddar
    last: Ghaddar
  - first: Phillippe
    full: Phillippe Langlais
    id: philippe-langlais
    last: Langlais
  author_string: Abbas Ghaddar, Phillippe Langlais
  bibkey: ghaddar-langlais-2019-contextualized
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5513
  month: November
  page_first: '101'
  page_last: '108'
  pages: "101\u2013108"
  paper_id: '13'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5513.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5513.jpg
  title: Contextualized Word Representations from Distant Supervision with and for
    NER
  title_html: Contextualized Word Representations from Distant Supervision with and
    for <span class="acl-fixed-case">NER</span>
  url: https://www.aclweb.org/anthology/D19-5513
  year: '2019'
D19-5514:
  abstract: Question paraphrasing aims to restate a given question with different
    expressions but keep the original meaning. Recent approaches are mostly based
    on neural networks following a sequence-to-sequence fashion, however, these models
    tend to generate unpredictable results. To overcome this drawback, we propose
    a pipeline model based on templates. It follows three steps, a) identifies template
    from the input question, b) retrieves candidate templates, c) fills candidate
    templates with original topic words. Experiment results on two self-constructed
    datasets show that our model outperforms the sequence-to-sequence model in a large
    margin and the advantage is more promising when the size of training sample is
    small.
  address: Hong Kong, China
  author:
  - first: Yunfan
    full: Yunfan Gu
    id: yunfan-gu
    last: Gu
  - first: Yang
    full: Yang Yuqiao
    id: yang-yuqiao
    last: Yuqiao
  - first: Zhongyu
    full: Zhongyu Wei
    id: zhongyu-wei
    last: Wei
  author_string: Yunfan Gu, Yang Yuqiao, Zhongyu Wei
  bibkey: gu-etal-2019-extract
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5514
  month: November
  page_first: '109'
  page_last: '114'
  pages: "109\u2013114"
  paper_id: '14'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5514.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5514.jpg
  title: 'Extract, Transform and Filling: A Pipeline Model for Question Paraphrasing
    based on Template'
  title_html: 'Extract, Transform and Filling: A Pipeline Model for Question Paraphrasing
    based on Template'
  url: https://www.aclweb.org/anthology/D19-5514
  year: '2019'
D19-5515:
  abstract: Existing natural language processing systems have often been designed
    with standard texts in mind. However, when these tools are used on the substantially
    different texts from social media, their performance drops dramatically. One solution
    is to translate social media data to standard language before processing, this
    is also called normalization. It is well-known that this improves performance
    for many natural language processing tasks on social media data. However, little
    is known about which types of normalization replacements have the most effect.
    Furthermore, it is unknown what the weaknesses of existing lexical normalization
    systems are in an extrinsic setting. In this paper, we analyze the effect of manual
    as well as automatic lexical normalization for dependency parsing. After our analysis,
    we conclude that for most categories, automatic normalization scores close to
    manually annotated normalization and that small annotation differences are important
    to take into consideration when exploiting normalization in a pipeline setup.
  address: Hong Kong, China
  attachment:
  - filename: D19-5515.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5515.Attachment.zip
  author:
  - first: Rob
    full: Rob van der Goot
    id: rob-van-der-goot
    last: van der Goot
  author_string: Rob van der Goot
  bibkey: van-der-goot-2019-depth
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5515
  month: November
  page_first: '115'
  page_last: '120'
  pages: "115\u2013120"
  paper_id: '15'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5515.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5515.jpg
  title: An In-depth Analysis of the Effect of Lexical Normalization on the Dependency
    Parsing of Social Media
  title_html: An In-depth Analysis of the Effect of Lexical Normalization on the Dependency
    Parsing of Social Media
  url: https://www.aclweb.org/anthology/D19-5515
  year: '2019'
D19-5516:
  abstract: "Modern e-commerce catalogs contain millions of references, associated\
    \ with textual and visual information that is of paramount importance for the\
    \ products to be found via search or browsing. Of particular significance is the\
    \ book category, where the author name(s) field poses a significant challenge.\
    \ Indeed, books written by a given author might be listed with different authors\u2019\
    \ names due to abbreviations, spelling variants and mistakes, among others. To\
    \ solve this problem at scale, we design a composite system involving open data\
    \ sources for books, as well as deep learning components, such as approximate\
    \ match with Siamese networks and name correction with sequence-to-sequence networks.\
    \ We evaluate this approach on product data from the e-commerce website Rakuten\
    \ France, and find that the top proposal of the system is the normalized author\
    \ name with 72% accuracy."
  address: Hong Kong, China
  author:
  - first: "B\xE9ranger"
    full: "B\xE9ranger Dumont"
    id: beranger-dumont
    last: Dumont
  - first: Simona
    full: Simona Maggio
    id: simona-maggio
    last: Maggio
  - first: Ghiles
    full: Ghiles Sidi Said
    id: ghiles-sidi-said
    last: Sidi Said
  - first: Quoc-Tien
    full: Quoc-Tien Au
    id: quoc-tien-au
    last: Au
  author_string: "B\xE9ranger Dumont, Simona Maggio, Ghiles Sidi Said, Quoc-Tien Au"
  bibkey: dumont-etal-2019-wrote
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5516
  month: November
  page_first: '121'
  page_last: '125'
  pages: "121\u2013125"
  paper_id: '16'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5516.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5516.jpg
  title: Who wrote this book? A challenge for e-commerce
  title_html: Who wrote this book? A challenge for e-commerce
  url: https://www.aclweb.org/anthology/D19-5516
  year: '2019'
D19-5517:
  abstract: The automatic analysis of expressions of opinion has been well studied
    in the opinion mining area, but a remaining problem is robustness for user-generated
    texts. Although consumer-generated texts are valuable since they contain a great
    number and wide variety of user evaluations, spelling inconsistency and the variety
    of expressions make analysis difficult. In order to tackle such situations, we
    applied a model that is reported to handle context in many natural language processing
    areas, to the problem of extracting references to the opinion target from text.
    Experiments on tweets that refer to television programs show that the model can
    extract such references with more than 90% accuracy.
  address: Hong Kong, China
  author:
  - first: Takeshi
    full: Takeshi Kobayakawa
    id: takeshi-kobayakawa
    last: Kobayakawa
  - first: Taro
    full: Taro Miyazaki
    id: taro-miyazaki
    last: Miyazaki
  - first: Hiroki
    full: Hiroki Okamoto
    id: hiroki-okamoto
    last: Okamoto
  - first: Simon
    full: Simon Clippingdale
    id: simon-clippingdale
    last: Clippingdale
  author_string: Takeshi Kobayakawa, Taro Miyazaki, Hiroki Okamoto, Simon Clippingdale
  bibkey: kobayakawa-etal-2019-mining
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5517
  month: November
  page_first: '126'
  page_last: '130'
  pages: "126\u2013130"
  paper_id: '17'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5517.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5517.jpg
  title: Mining Tweets that refer to TV programs with Deep Neural Networks
  title_html: Mining Tweets that refer to <span class="acl-fixed-case">TV</span> programs
    with Deep Neural Networks
  url: https://www.aclweb.org/anthology/D19-5517
  year: '2019'
D19-5518:
  abstract: We work with Algerian, an under-resourced non-standardised Arabic variety,
    for which we compile a new parallel corpus consisting of user-generated textual
    data matched with normalised and corrected human annotations following data-driven
    and our linguistically motivated standard. We use an end-to-end deep neural model
    designed to deal with context-dependent spelling correction and normalisation.
    Results indicate that a model with two CNN sub-network encoders and an LSTM decoder
    performs the best, and that word context matters. Additionally, pre-processing
    data token-by-token with an edit-distance based aligner significantly improves
    the performance. We get promising results for the spelling correction and normalisation,
    as a pre-processing step for downstream tasks, on detecting binary Semantic Textual
    Similarity.
  address: Hong Kong, China
  author:
  - first: Wafia
    full: Wafia Adouane
    id: wafia-adouane
    last: Adouane
  - first: Jean-Philippe
    full: Jean-Philippe Bernardy
    id: jean-philippe-bernardy
    last: Bernardy
  - first: Simon
    full: Simon Dobnik
    id: simon-dobnik
    last: Dobnik
  author_string: Wafia Adouane, Jean-Philippe Bernardy, Simon Dobnik
  bibkey: adouane-etal-2019-normalising
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5518
  month: November
  page_first: '131'
  page_last: '140'
  pages: "131\u2013140"
  paper_id: '18'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5518.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5518.jpg
  title: Normalising Non-standardised Orthography in Algerian Code-switched User-generated
    Data
  title_html: Normalising Non-standardised Orthography in <span class="acl-fixed-case">A</span>lgerian
    Code-switched User-generated Data
  url: https://www.aclweb.org/anthology/D19-5518
  year: '2019'
D19-5519:
  abstract: We compare different LSTMs and transformer models in terms of their effectiveness
    in normalizing dialectal Finnish into the normative standard Finnish. As dialect
    is the common way of communication for people online in Finnish, such a normalization
    is a necessary step to improve the accuracy of the existing Finnish NLP tools
    that are tailored for normative Finnish text. We work on a corpus consisting of
    dialectal data of 23 distinct Finnish dialects. The best functioning BRNN approach
    lowers the initial word error rate of the corpus from 52.89 to 5.73.
  address: Hong Kong, China
  author:
  - first: Niko
    full: Niko Partanen
    id: niko-partanen
    last: Partanen
  - first: Mika
    full: "Mika H\xE4m\xE4l\xE4inen"
    id: mika-hamalainen
    last: "H\xE4m\xE4l\xE4inen"
  - first: Khalid
    full: Khalid Alnajjar
    id: khalid-alnajjar
    last: Alnajjar
  author_string: "Niko Partanen, Mika H\xE4m\xE4l\xE4inen, Khalid Alnajjar"
  bibkey: partanen-etal-2019-dialect
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5519
  month: November
  page_first: '141'
  page_last: '146'
  pages: "141\u2013146"
  paper_id: '19'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5519.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5519.jpg
  title: Dialect Text Normalization to Normative Standard Finnish
  title_html: Dialect Text Normalization to Normative Standard <span class="acl-fixed-case">F</span>innish
  url: https://www.aclweb.org/anthology/D19-5519
  year: '2019'
D19-5520:
  abstract: In relevance classification, we hope to judge whether some utterances
    expressed on a topic are relevant or not. A usual method is to train a specific
    classifier respectively for each topic. However, in that way, it easily causes
    an underfitting problem in supervised learning model, since annotated data can
    be insufficient for every single topic. In this paper, we explore the common features
    beyond different topics and propose our cross-topic relevance embedding aggregation
    methodology (CREAM) that can expand the range of training data and apply what
    has been learned from source topics to a target topic. In our experiment, we show
    that our proposal could capture common features within a small amount of annotated
    data and improve the performance of relevance classification compared with other
    baselines.
  address: Hong Kong, China
  author:
  - first: Jiawei
    full: Jiawei Yong
    id: jiawei-yong
    last: Yong
  author_string: Jiawei Yong
  bibkey: yong-2019-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5520
  month: November
  page_first: '147'
  page_last: '152'
  pages: "147\u2013152"
  paper_id: '20'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5520.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5520.jpg
  title: A Cross-Topic Method for Supervised Relevance Classification
  title_html: A Cross-Topic Method for Supervised Relevance Classification
  url: https://www.aclweb.org/anthology/D19-5520
  year: '2019'
D19-5521:
  abstract: We study methods for learning sentence embeddings with syntactic structure.
    We focus on methods of learning syntactic sentence-embeddings by using a multilingual
    parallel-corpus augmented by Universal Parts-of-Speech tags. We evaluate the quality
    of the learned embeddings by examining sentence-level nearest neighbours and functional
    dissimilarity in the embedding space. We also evaluate the ability of the method
    to learn syntactic sentence-embeddings for low-resource languages and demonstrate
    strong evidence for transfer learning. Our results show that syntactic sentence-embeddings
    can be learned while using less training data, fewer model parameters, and resulting
    in better evaluation metrics than state-of-the-art language models.
  address: Hong Kong, China
  author:
  - first: Chen
    full: Chen Liu
    id: chen-liu
    last: Liu
  - first: Anderson
    full: Anderson De Andrade
    id: anderson-de-andrade
    last: De Andrade
  - first: Muhammad
    full: Muhammad Osama
    id: muhammad-osama
    last: Osama
  author_string: Chen Liu, Anderson De Andrade, Muhammad Osama
  bibkey: liu-etal-2019-exploring
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5521
  month: November
  page_first: '153'
  page_last: '159'
  pages: "153\u2013159"
  paper_id: '21'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5521.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5521.jpg
  title: Exploring Multilingual Syntactic Sentence Representations
  title_html: Exploring Multilingual Syntactic Sentence Representations
  url: https://www.aclweb.org/anthology/D19-5521
  year: '2019'
D19-5522:
  abstract: "We propose a Chinese spell checker \u2013 FASPell based on a new paradigm\
    \ which consists of a denoising autoencoder (DAE) and a decoder. In comparison\
    \ with previous state-of-the-art models, the new paradigm allows our spell checker\
    \ to be Faster in computation, readily Adaptable to both simplified and traditional\
    \ Chinese texts produced by either humans or machines, and to require much Simpler\
    \ structure to be as much Powerful in both error detection and correction. These\
    \ four achievements are made possible because the new paradigm circumvents two\
    \ bottlenecks. First, the DAE curtails the amount of Chinese spell checking data\
    \ needed for supervised learning (to <10k sentences) by leveraging the power of\
    \ unsupervisedly pre-trained masked language model as in BERT, XLNet, MASS etc.\
    \ Second, the decoder helps to eliminate the use of confusion set that is deficient\
    \ in flexibility and sufficiency of utilizing the salient feature of Chinese character\
    \ similarity."
  address: Hong Kong, China
  attachment:
  - filename: D19-5522.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5522.Attachment.zip
  author:
  - first: Yuzhong
    full: Yuzhong Hong
    id: yuzhong-hong
    last: Hong
  - first: Xianguo
    full: Xianguo Yu
    id: xianguo-yu
    last: Yu
  - first: Neng
    full: Neng He
    id: neng-he
    last: He
  - first: Nan
    full: Nan Liu
    id: nan-liu
    last: Liu
  - first: Junhui
    full: Junhui Liu
    id: junhui-liu
    last: Liu
  author_string: Yuzhong Hong, Xianguo Yu, Neng He, Nan Liu, Junhui Liu
  bibkey: hong-etal-2019-faspell
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5522
  month: November
  page_first: '160'
  page_last: '169'
  pages: "160\u2013169"
  paper_id: '22'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5522.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5522.jpg
  title: 'FASPell: A Fast, Adaptable, Simple, Powerful Chinese Spell Checker Based
    On DAE-Decoder Paradigm'
  title_html: '<span class="acl-fixed-case">FASP</span>ell: A Fast, Adaptable, Simple,
    Powerful <span class="acl-fixed-case">C</span>hinese Spell Checker Based On <span
    class="acl-fixed-case">DAE</span>-Decoder Paradigm'
  url: https://www.aclweb.org/anthology/D19-5522
  year: '2019'
D19-5523:
  abstract: The Princeton WordNet is a powerful tool for studying language and developing
    natural language processing algorithms. With significant work developing it further,
    one line considers its extension through aligning its expert-annotated structure
    with other lexical resources. In contrast, this work explores a completely data-driven
    approach to network construction, forming a wordnet using the entirety of the
    open-source, noisy, user-annotated dictionary, Wiktionary. Comparing baselines
    to WordNet, we find compelling evidence that our network induction process constructs
    a network with useful semantic structure. With thousands of semantically-linked
    examples that demonstrate sense usage from basic lemmas to multiword expressions
    (MWEs), we believe this work motivates future research.
  address: Hong Kong, China
  author:
  - first: Hunter
    full: Hunter Heidenreich
    id: hunter-heidenreich
    last: Heidenreich
  - first: Jake
    full: Jake Williams
    id: jake-williams
    last: Williams
  author_string: Hunter Heidenreich, Jake Williams
  bibkey: heidenreich-williams-2019-latent
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5523
  month: November
  page_first: '170'
  page_last: '180'
  pages: "170\u2013180"
  paper_id: '23'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5523.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5523.jpg
  title: Latent semantic network induction in the context of linked example senses
  title_html: Latent semantic network induction in the context of linked example senses
  url: https://www.aclweb.org/anthology/D19-5523
  year: '2019'
D19-5524:
  abstract: Contemporary datasets on tobacco consumption focus on one of two topics,
    either public health mentions and disease surveillance, or sentiment analysis
    on topical tobacco products and services. However, two primary considerations
    are not accounted for, the language of the demographic affected and a combination
    of the topics mentioned above in a fine-grained classification mechanism. In this
    paper, we create a dataset of 3144 tweets, which are selected based on the presence
    of colloquial slang related to smoking and analyze it based on the semantics of
    the tweet. Each class is created and annotated based on the content of the tweets
    such that further hierarchical methods can be easily applied. Further, we prove
    the efficacy of standard text classification methods on this dataset, by designing
    experiments which do both binary as well as multi-class classification. Our experiments
    tackle the identification of either a specific topic (such as tobacco product
    promotion), a general mention (cigarettes and related products) or a more fine-grained
    classification. This methodology paves the way for further analysis, such as understanding
    sentiment or style, which makes this dataset a vital contribution to both disease
    surveillance and tobacco use research.
  address: Hong Kong, China
  author:
  - first: Kartikey
    full: Kartikey Pant
    id: kartikey-pant
    last: Pant
  - first: Venkata Himakar
    full: Venkata Himakar Yanamandra
    id: venkata-himakar-yanamandra
    last: Yanamandra
  - first: Alok
    full: Alok Debnath
    id: alok-debnath
    last: Debnath
  - first: Radhika
    full: Radhika Mamidi
    id: radhika-mamidi
    last: Mamidi
  author_string: Kartikey Pant, Venkata Himakar Yanamandra, Alok Debnath, Radhika
    Mamidi
  bibkey: pant-etal-2019-smokeng
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5524
  month: November
  page_first: '181'
  page_last: '190'
  pages: "181\u2013190"
  paper_id: '24'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5524.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5524.jpg
  title: 'SmokEng: Towards Fine-grained Classification of Tobacco-related Social Media
    Text'
  title_html: '<span class="acl-fixed-case">S</span>mok<span class="acl-fixed-case">E</span>ng:
    Towards Fine-grained Classification of Tobacco-related Social Media Text'
  url: https://www.aclweb.org/anthology/D19-5524
  year: '2019'
D19-5525:
  abstract: "In the context of document quality assessment, previous work has mainly\
    \ focused on predicting the quality of a document relative to a putative gold\
    \ standard, without paying attention to the subjectivity of this task. To imitate\
    \ people\u2019s disagreement over inherently subjective tasks such as rating the\
    \ quality of a Wikipedia article, a document quality assessment system should\
    \ provide not only a prediction of the article quality but also the uncertainty\
    \ over its predictions. This motivates us to measure the uncertainty in document\
    \ quality predictions, in addition to making the label prediction. Experimental\
    \ results show that both Gaussian processes (GPs) and random forests (RFs) can\
    \ yield competitive results in predicting the quality of Wikipedia articles, while\
    \ providing an estimate of uncertainty when there is inconsistency in the quality\
    \ labels from the Wikipedia contributors. We additionally evaluate our methods\
    \ in the context of a semi-automated document quality class assignment decision-making\
    \ process, where there is asymmetric risk associated with overestimates and underestimates\
    \ of document quality. Our experiments suggest that GPs provide more reliable\
    \ estimates in this context."
  address: Hong Kong, China
  author:
  - first: Aili
    full: Aili Shen
    id: aili-shen
    last: Shen
  - first: Daniel
    full: Daniel Beck
    id: daniel-beck
    last: Beck
  - first: Bahar
    full: Bahar Salehi
    id: bahar-salehi
    last: Salehi
  - first: Jianzhong
    full: Jianzhong Qi
    id: jianzhong-qi
    last: Qi
  - first: Timothy
    full: Timothy Baldwin
    id: timothy-baldwin
    last: Baldwin
  author_string: Aili Shen, Daniel Beck, Bahar Salehi, Jianzhong Qi, Timothy Baldwin
  bibkey: shen-etal-2019-modelling
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5525
  month: November
  page_first: '191'
  page_last: '201'
  pages: "191\u2013201"
  paper_id: '25'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5525.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5525.jpg
  title: Modelling Uncertainty in Collaborative Document Quality Assessment
  title_html: Modelling Uncertainty in Collaborative Document Quality Assessment
  url: https://www.aclweb.org/anthology/D19-5525
  year: '2019'
D19-5526:
  abstract: "Approaches to knowledge extraction (KE) in the health domain often start\
    \ by annotating text to indicate the knowledge to be extracted, and then use the\
    \ annotated text to train systems to perform the KE. This may work for annotat-\
    \ ing named entities or other contiguous noun phrases (drugs, some drug effects),\
    \ but be- comes increasingly difficult when items tend to be expressed across\
    \ multiple, possibly non- contiguous, syntactic constituents (e.g. most descriptions\
    \ of drug effects in user-generated text). Other issues include that it is not\
    \ al- ways clear how annotations map to actionable insights, or how they scale\
    \ up to, or can form part of, more complex KE tasks. This paper reports our efforts\
    \ in developing an approach to extracting knowledge about drug nonadher- ence\
    \ from health forums which led us to con- clude that development cannot proceed\
    \ in sep- arate steps but that all aspects\u2014from concep- tualisation to annotation\
    \ scheme development, annotation, KE system training and knowl- edge graph instantiation\u2014\
    are interdependent and need to be co-developed. Our aim in this paper is two-fold:\
    \ we describe a generally ap- plicable framework for developing a KE ap- proach,\
    \ and present a specific KE approach, developed with the framework, for the task\
    \ of gathering information about antidepressant drug nonadherence. We report the\
    \ conceptual- isation, the annotation scheme, the annotated corpus, and an analysis\
    \ of annotated texts."
  address: Hong Kong, China
  author:
  - first: Anja
    full: Anja Belz
    id: anja-belz
    last: Belz
  - first: Richard
    full: Richard Hoile
    id: richard-hoile
    last: Hoile
  - first: Elizabeth
    full: Elizabeth Ford
    id: elizabeth-ford
    last: Ford
  - first: Azam
    full: Azam Mullick
    id: azam-mullick
    last: Mullick
  author_string: Anja Belz, Richard Hoile, Elizabeth Ford, Azam Mullick
  bibkey: belz-etal-2019-conceptualisation
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5526
  month: November
  page_first: '202'
  page_last: '211'
  pages: "202\u2013211"
  paper_id: '26'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5526.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5526.jpg
  title: Conceptualisation and Annotation of Drug Nonadherence Information for Knowledge
    Extraction from Patient-Generated Texts
  title_html: Conceptualisation and Annotation of Drug Nonadherence Information for
    Knowledge Extraction from Patient-Generated Texts
  url: https://www.aclweb.org/anthology/D19-5526
  year: '2019'
D19-5527:
  abstract: Irony detection is an important task with applications in identification
    of online abuse and harassment. With the ubiquitous use of non-verbal cues such
    as emojis in social media, in this work we aim to study the role of these structures
    in irony detection. Since the existing irony detection datasets have <10% ironic
    tweets with emoji, classifiers trained on them are insensitive to emojis. We propose
    an automated pipeline for creating a more balanced dataset.
  address: Hong Kong, China
  author:
  - first: Shirley Anugrah
    full: Shirley Anugrah Hayati
    id: shirley-anugrah-hayati
    last: Hayati
  - first: Aditi
    full: Aditi Chaudhary
    id: aditi-chaudhary
    last: Chaudhary
  - first: Naoki
    full: Naoki Otani
    id: naoki-otani
    last: Otani
  - first: Alan W
    full: Alan W Black
    id: alan-w-black
    last: Black
  author_string: Shirley Anugrah Hayati, Aditi Chaudhary, Naoki Otani, Alan W Black
  bibkey: hayati-etal-2019-sunny
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5527
  month: November
  page_first: '212'
  page_last: '216'
  pages: "212\u2013216"
  paper_id: '27'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5527.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5527.jpg
  title: "What A Sunny Day \u2614: Toward Emoji-Sensitive Irony Detection"
  title_html: "What A Sunny Day \u2614: Toward Emoji-Sensitive Irony Detection"
  url: https://www.aclweb.org/anthology/D19-5527
  year: '2019'
D19-5528:
  abstract: Geolocation, predicting the location of a post based on text and other
    information, has a huge potential for several social media applications. Typically,
    the problem is modeled as either multi-class classification or regression. In
    the first case, the classes are geographic areas previously identified; in the
    second, the models directly predict geographic coordinates. The former requires
    discretization of the coordinates, but yields better performance. The latter is
    potentially more precise and true to the nature of the problem, but often results
    in worse performance. We propose to combine the two approaches in an attentionbased
    multitask convolutional neural network that jointly predicts both discrete locations
    and continuous geographic coordinates. We evaluate the multi-task (MTL) model
    against singletask models and prior work. We find that MTL significantly improves
    performance, reporting large gains on one data set, but also note that the correlation
    between labels and coordinates has a marked impact on the effectiveness of including
    a regression task.
  address: Hong Kong, China
  author:
  - first: Tommaso
    full: Tommaso Fornaciari
    id: tommaso-fornaciari
    last: Fornaciari
  - first: Dirk
    full: Dirk Hovy
    id: dirk-hovy
    last: Hovy
  author_string: Tommaso Fornaciari, Dirk Hovy
  bibkey: fornaciari-hovy-2019-geolocation
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5528
  month: November
  page_first: '217'
  page_last: '223'
  pages: "217\u2013223"
  paper_id: '28'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5528.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5528.jpg
  title: Geolocation with Attention-Based Multitask Learning Models
  title_html: Geolocation with Attention-Based Multitask Learning Models
  url: https://www.aclweb.org/anthology/D19-5528
  year: '2019'
D19-5529:
  abstract: Prior research has shown that geolocation can be substantially improved
    by including user network information. While effective, it suffers from the curse
    of dimensionality, since networks are usually represented as sparse adjacency
    matrices of connections, which grow exponentially with the number of users. In
    order to incorporate this information, we therefore need to limit the network
    size, in turn limiting performance and risking sample bias. In this paper, we
    address these limitations by instead using dense network representations. We explore
    two methods to learn continuous node representations from either 1) the network
    structure with node2vec (Grover and Leskovec, 2016), or 2) textual user mentions
    via doc2vec (Le and Mikolov, 2014). We combine both methods with input from social
    media posts in an attention-based convolutional neural network and evaluate the
    contribution of each component on geolocation performance. Our method enables
    us to incorporate arbitrarily large networks in a fixed-length vector, without
    limiting the network size. Our models achieve competitive results with similar
    state-of-the-art methods, but with much fewer model parameters, while being applicable
    to networks of virtually any size.
  address: Hong Kong, China
  author:
  - first: Tommaso
    full: Tommaso Fornaciari
    id: tommaso-fornaciari
    last: Fornaciari
  - first: Dirk
    full: Dirk Hovy
    id: dirk-hovy
    last: Hovy
  author_string: Tommaso Fornaciari, Dirk Hovy
  bibkey: fornaciari-hovy-2019-dense
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5529
  month: November
  page_first: '224'
  page_last: '230'
  pages: "224\u2013230"
  paper_id: '29'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5529.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5529.jpg
  title: Dense Node Representation for Geolocation
  title_html: Dense Node Representation for Geolocation
  url: https://www.aclweb.org/anthology/D19-5529
  year: '2019'
D19-5530:
  abstract: Geolocating social media posts relies on the assumption that language
    carries sufficient geographic information. However, locations are usually given
    as continuous latitude/longitude tuples, so we first need to define discrete geographic
    regions that can serve as labels. Most studies use some form of clustering to
    discretize the continuous coordinates (Han et al., 2016). However, the resulting
    regions do not always correspond to existing linguistic areas. Consequently, accuracy
    at 100 miles tends to be good, but degrades for finer-grained distinctions, when
    different linguistic regions get lumped together. We describe a new algorithm,
    Point-to-City (P2C), an iterative k-d tree-based method for clustering geographic
    coordinates and associating them with towns. We create three sets of labels at
    different levels of granularity, and compare performance of a state-of-the-art
    geolocation model trained and tested with P2C labels to one with regular k-d tree
    labels. Even though P2C results in substantially more labels than the baseline,
    model accuracy increases significantly over using traditional labels at the fine-grained
    level, while staying comparable at 100 miles. The results suggest that identifying
    meaningful linguistic areas is crucial for improving geolocation at a fine-grained
    level.
  address: Hong Kong, China
  author:
  - first: Tommaso
    full: Tommaso Fornaciari
    id: tommaso-fornaciari
    last: Fornaciari
  - first: Dirk
    full: Dirk Hovy
    id: dirk-hovy
    last: Hovy
  author_string: Tommaso Fornaciari, Dirk Hovy
  bibkey: fornaciari-hovy-2019-identifying
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5530
  month: November
  page_first: '231'
  page_last: '236'
  pages: "231\u2013236"
  paper_id: '30'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5530.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5530.jpg
  title: Identifying Linguistic Areas for Geolocation
  title_html: Identifying Linguistic Areas for Geolocation
  url: https://www.aclweb.org/anthology/D19-5530
  year: '2019'
D19-5531:
  abstract: Robustness to capitalization errors is a highly desirable characteristic
    of named entity recognizers, yet we find standard models for the task are surprisingly
    brittle to such noise.Existing methods to improve robustness to the noise completely
    discard given orthographic information, which significantly degrades their performance
    on well-formed text. We propose a simple alternative approach based on data augmentation,
    which allows the model to learn to utilize or ignore orthographic information
    depending on its usefulness in the context. It achieves competitive robustness
    to capitalization errors while making negligible compromise to its performance
    on well-formed text and significantly improving generalization power on noisy
    user-generated text. Our experiments clearly and consistently validate our claim
    across different types of machine learning models, languages, and dataset sizes.
  address: Hong Kong, China
  author:
  - first: Sravan
    full: Sravan Bodapati
    id: sravan-bodapati
    last: Bodapati
  - first: Hyokun
    full: Hyokun Yun
    id: hyokun-yun
    last: Yun
  - first: Yaser
    full: Yaser Al-Onaizan
    id: yaser-al-onaizan
    last: Al-Onaizan
  author_string: Sravan Bodapati, Hyokun Yun, Yaser Al-Onaizan
  bibkey: bodapati-etal-2019-robustness
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5531
  month: November
  page_first: '237'
  page_last: '242'
  pages: "237\u2013242"
  paper_id: '31'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5531.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5531.jpg
  title: Robustness to Capitalization Errors in Named Entity Recognition
  title_html: Robustness to Capitalization Errors in Named Entity Recognition
  url: https://www.aclweb.org/anthology/D19-5531
  year: '2019'
D19-5532:
  abstract: Traditional event detection classifies a word or a phrase in a given sentence
    for a set of prede- fined event types. The limitation of such pre- defined set
    is that it prevents the adaptation of the event detection models to new event
    types. We study a novel formulation of event detec- tion that describes types
    via several keywords to match the contexts in documents. This fa- cilitates the
    operation of the models to new types. We introduce a novel feature-based attention
    mechanism for convolutional neural networks for event detection in the new for-
    mulation. Our extensive experiments demon- strate the benefits of the new formulation
    for new type extension for event detection as well as the proposed attention mechanism
    for this problem
  address: Hong Kong, China
  author:
  - first: Viet Dac
    full: Viet Dac Lai
    id: viet-dac-lai
    last: Lai
  - first: Thien
    full: Thien Nguyen
    id: thien-nguyen
    last: Nguyen
  author_string: Viet Dac Lai, Thien Nguyen
  bibkey: lai-nguyen-2019-extending
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5532
  month: November
  page_first: '243'
  page_last: '248'
  pages: "243\u2013248"
  paper_id: '32'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5532.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5532.jpg
  title: Extending Event Detection to New Types with Learning from Keywords
  title_html: Extending Event Detection to New Types with Learning from Keywords
  url: https://www.aclweb.org/anthology/D19-5532
  year: '2019'
D19-5533:
  abstract: Distant supervised relation extraction is an efficient and effective strategy
    to find relations between entities in texts. However, it inevitably suffers from
    mislabeling problem and the noisy data will hinder the performance. In this paper,
    we propose the Separate Head-Tail Convolution Neural Network (SHTCNN), a novel
    neural relation extraction framework to alleviate this issue. In this method,
    we apply separate convolution and pooling to the head and tail entity respectively
    for extracting better semantic features of sentences, and coarse-to-fine strategy
    to filter out instances which do not have actual relations in order to alleviate
    noisy data issues. Experiments on a widely used dataset show that our model achieves
    significant and consistent improvements in relation extraction compared to statistical
    and vanilla CNN-based methods.
  address: Hong Kong, China
  author:
  - first: Rui
    full: Rui Xing
    id: rui-xing
    last: Xing
  - first: Jie
    full: Jie Luo
    id: jie-luo
    last: Luo
  author_string: Rui Xing, Jie Luo
  bibkey: xing-luo-2019-distant
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5533
  month: November
  page_first: '249'
  page_last: '258'
  pages: "249\u2013258"
  paper_id: '33'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5533.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5533.jpg
  title: Distant Supervised Relation Extraction with Separate Head-Tail CNN
  title_html: Distant Supervised Relation Extraction with Separate Head-Tail <span
    class="acl-fixed-case">CNN</span>
  url: https://www.aclweb.org/anthology/D19-5533
  year: '2019'
D19-5534:
  abstract: In this work, we revisit the functions of language proposed by linguist
    Roman Jakobson and we highlight their potential in analyzing online forum conversations.
    We investigate the relationship between functions and other properties of comments,
    such as controversiality. We propose and evaluate a semi-supervised framework
    for predicting the functions of Reddit comments. To accommodate further research,
    we release a corpus of 165K comments annotated with their functions of language.
  address: Hong Kong, China
  attachment:
  - filename: D19-5534.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5534.Attachment.zip
  author:
  - first: Youmna
    full: Youmna Ismaeil
    id: youmna-ismaeil
    last: Ismaeil
  - first: Oana
    full: Oana Balalau
    id: oana-balalau
    last: Balalau
  - first: Paramita
    full: Paramita Mirza
    id: paramita-mirza
    last: Mirza
  author_string: Youmna Ismaeil, Oana Balalau, Paramita Mirza
  bibkey: ismaeil-etal-2019-discovering
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5534
  month: November
  page_first: '259'
  page_last: '264'
  pages: "259\u2013264"
  paper_id: '34'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5534.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5534.jpg
  title: Discovering the Functions of Language in Online Forums
  title_html: Discovering the Functions of Language in Online Forums
  url: https://www.aclweb.org/anthology/D19-5534
  year: '2019'
D19-5535:
  abstract: The state-of-the-art neural network architectures make it possible to
    create spoken language understanding systems with high quality and fast processing
    time. One major challenge for real-world applications is the high latency of these
    systems caused by triggered actions with high executions times. If an action can
    be separated into subactions, the reaction time of the systems can be improved
    through incremental processing of the user utterance and starting subactions while
    the utterance is still being uttered. In this work, we present a model-agnostic
    method to achieve high quality in processing incrementally produced partial utterances.
    Based on clean and noisy versions of the ATIS dataset, we show how to create datasets
    with our method to create low-latency natural language understanding components.
    We get improvements of up to 47.91 absolute percentage points in the metric F1-score.
  address: Hong Kong, China
  attachment:
  - filename: D19-5535.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5535.Attachment.zip
  author:
  - first: Stefan
    full: Stefan Constantin
    id: stefan-constantin
    last: Constantin
  - first: Jan
    full: Jan Niehues
    id: jan-niehues
    last: Niehues
  - first: Alex
    full: Alex Waibel
    id: alex-waibel
    last: Waibel
  author_string: Stefan Constantin, Jan Niehues, Alex Waibel
  bibkey: constantin-etal-2019-incremental
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5535
  month: November
  page_first: '265'
  page_last: '274'
  pages: "265\u2013274"
  paper_id: '35'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5535.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5535.jpg
  title: Incremental processing of noisy user utterances in the spoken language understanding
    task
  title_html: Incremental processing of noisy user utterances in the spoken language
    understanding task
  url: https://www.aclweb.org/anthology/D19-5535
  year: '2019'
D19-5536:
  abstract: One of the most persistent characteristics of written user-generated content
    (UGC) is the use of non-standard words. This characteristic contributes to an
    increased difficulty to automatically process and analyze UGC. Text normalization
    is the task of transforming lexical variants to their canonical forms and is often
    used as a pre-processing step for conventional NLP tasks in order to overcome
    the performance drop that NLP systems experience when applied to UGC. In this
    work, we follow a Neural Machine Translation approach to text normalization. To
    train such an encoder-decoder model, large parallel training corpora of sentence
    pairs are required. However, obtaining large data sets with UGC and their normalized
    version is not trivial, especially for languages other than English. In this paper,
    we explore how to overcome this data bottleneck for Dutch, a low-resource language.
    We start off with a small publicly available parallel Dutch data set comprising
    three UGC genres and compare two different approaches. The first is to manually
    normalize and add training data, a money and time-consuming task. The second approach
    is a set of data augmentation techniques which increase data size by converting
    existing resources into synthesized non-standard forms. Our results reveal that,
    while the different approaches yield similar results regarding the normalization
    issues in the test set, they also introduce a large amount of over-normalizations.
  address: Hong Kong, China
  author:
  - first: Claudia
    full: Claudia Matos Veliz
    id: claudia-matos-veliz
    last: Matos Veliz
  - first: Orphee
    full: Orphee De Clercq
    id: orphee-de-clercq
    last: De Clercq
  - first: Veronique
    full: Veronique Hoste
    id: veronique-hoste
    last: Hoste
  author_string: Claudia Matos Veliz, Orphee De Clercq, Veronique Hoste
  bibkey: matos-veliz-etal-2019-benefits
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5536
  month: November
  page_first: '275'
  page_last: '285'
  pages: "275\u2013285"
  paper_id: '36'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5536.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5536.jpg
  title: Benefits of Data Augmentation for NMT-based Text Normalization of User-Generated
    Content
  title_html: Benefits of Data Augmentation for <span class="acl-fixed-case">NMT</span>-based
    Text Normalization of User-Generated Content
  url: https://www.aclweb.org/anthology/D19-5536
  year: '2019'
D19-5537:
  abstract: Recently, with the help of deep learning models, significant advances
    have been made in different Natural Language Processing (NLP) tasks. Unfortunately,
    state-of-the-art models are vulnerable to noisy texts. We propose a new contextual
    text denoising algorithm based on the ready-to-use masked language model. The
    proposed algorithm does not require retraining of the model and can be integrated
    into any NLP system without additional training on paired cleaning training data.
    We evaluate our method under synthetic noise and natural noise and show that the
    proposed algorithm can use context information to correct noise text and improve
    the performance of noisy inputs in several downstream tasks.
  address: Hong Kong, China
  author:
  - first: Yifu
    full: Yifu Sun
    id: yifu-sun
    last: Sun
  - first: Haoming
    full: Haoming Jiang
    id: haoming-jiang
    last: Jiang
  author_string: Yifu Sun, Haoming Jiang
  bibkey: sun-jiang-2019-contextual
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5537
  month: November
  page_first: '286'
  page_last: '290'
  pages: "286\u2013290"
  paper_id: '37'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5537.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5537.jpg
  title: Contextual Text Denoising with Masked Language Model
  title_html: Contextual Text Denoising with Masked Language Model
  url: https://www.aclweb.org/anthology/D19-5537
  year: '2019'
D19-5538:
  abstract: We present a system for automating Semantic Role Labelling of Hindi-English
    code-mixed tweets. We explore the issues posed by noisy, user generated code-mixed
    social media data. We also compare the individual effect of various linguistic
    features used in our system. Our proposed model is a 2-step system for automated
    labelling which gives an overall accuracy of 84% for Argument Classification,
    marking a 10% increase over the existing rule-based baseline model. This is the
    first attempt at building a statistical Semantic Role Labeller for Hindi-English
    code-mixed data, to the best of our knowledge.
  address: Hong Kong, China
  author:
  - first: Riya
    full: Riya Pal
    id: riya-pal
    last: Pal
  - first: Dipti
    full: Dipti Sharma
    id: dipti-misra-sharma
    last: Sharma
  author_string: Riya Pal, Dipti Sharma
  bibkey: pal-sharma-2019-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5538
  month: November
  page_first: '291'
  page_last: '296'
  pages: "291\u2013296"
  paper_id: '38'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5538.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5538.jpg
  title: Towards Automated Semantic Role Labelling of Hindi-English Code-Mixed Tweets
  title_html: Towards Automated Semantic Role Labelling of <span class="acl-fixed-case">H</span>indi-<span
    class="acl-fixed-case">E</span>nglish Code-Mixed Tweets
  url: https://www.aclweb.org/anthology/D19-5538
  year: '2019'
D19-5539:
  abstract: 'Language model-based pre-trained representations have become ubiquitous
    in natural language processing. They have been shown to significantly improve
    the performance of neural models on a great variety of tasks. However, it remains
    unclear how useful those general models can be in handling non-canonical text.
    In this article, focusing on User Generated Content (UGC), we study the ability
    of BERT to perform lexical normalisation. Our contribution is simple: by framing
    lexical normalisation as a token prediction task, by enhancing its architecture
    and by carefully fine-tuning it, we show that BERT can be a competitive lexical
    normalisation model without the need of any UGC resources aside from 3,000 training
    sentences. To the best of our knowledge, it is the first work done in adapting
    and analysing the ability of this model to handle noisy UGC data.'
  address: Hong Kong, China
  author:
  - first: Benjamin
    full: Benjamin Muller
    id: benjamin-muller
    last: Muller
  - first: Benoit
    full: Benoit Sagot
    id: benoit-sagot
    last: Sagot
  - first: "Djam\xE9"
    full: "Djam\xE9 Seddah"
    id: djame-seddah
    last: Seddah
  author_string: "Benjamin Muller, Benoit Sagot, Djam\xE9 Seddah"
  bibkey: muller-etal-2019-enhancing
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5539
  month: November
  page_first: '297'
  page_last: '306'
  pages: "297\u2013306"
  paper_id: '39'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5539.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5539.jpg
  title: Enhancing BERT for Lexical Normalization
  title_html: Enhancing <span class="acl-fixed-case">BERT</span> for Lexical Normalization
  url: https://www.aclweb.org/anthology/D19-5539
  year: '2019'
D19-5540:
  abstract: "We present a probabilistic clustering algorithm that can help Reddit\
    \ users to find posts that discuss experiences similar to their own. This model\
    \ is built upon the BERT Next Sentence Prediction model and reduces the time complexity\
    \ for clustering all posts in a corpus from O(n\u02C62) to O(n) with respect to\
    \ the number of posts. We demonstrate that such probabilistic clustering can yield\
    \ a performance better than baseline clustering methods based on Latent Dirichlet\
    \ Allocation (Blei et al., 2003) and Word2Vec (Mikolov et al., 2013). Furthermore,\
    \ there is a high degree of coherence between our probabilistic clustering and\
    \ the exhaustive comparison O(n\u02C62) algorithm in which the similarity between\
    \ every pair of posts is found. This makes the use of the BERT Next Sentence Prediction\
    \ model more practical for unsupervised clustering tasks due to the high runtime\
    \ overhead of each BERT computation."
  address: Hong Kong, China
  author:
  - first: Zhilin
    full: Zhilin Wang
    id: zhilin-wang
    last: Wang
  - first: Elena
    full: Elena Rastorgueva
    id: elena-rastorgueva
    last: Rastorgueva
  - first: Weizhe
    full: Weizhe Lin
    id: weizhe-lin
    last: Lin
  - first: Xiaodong
    full: Xiaodong Wu
    id: xiaodong-wu
    last: Wu
  author_string: Zhilin Wang, Elena Rastorgueva, Weizhe Lin, Xiaodong Wu
  bibkey: wang-etal-2019-youre
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5540
  month: November
  page_first: '307'
  page_last: '315'
  pages: "307\u2013315"
  paper_id: '40'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5540.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5540.jpg
  title: "No, you\u2019re not alone: A better way to find people with similar experiences\
    \ on Reddit"
  title_html: "No, you\u2019re not alone: A better way to find people with similar\
    \ experiences on <span class=\"acl-fixed-case\">R</span>eddit"
  url: https://www.aclweb.org/anthology/D19-5540
  year: '2019'
D19-5541:
  abstract: Deep learning based general language models have achieved state-of-the-art
    results in many popular tasks such as sentiment analysis and QA tasks. Text in
    domains like social media has its own salient characteristics. Domain knowledge
    should be helpful in domain relevant tasks. In this work, we devise a simple method
    to obtain domain knowledge and further propose a method to integrate domain knowledge
    with general knowledge based on deep language models to improve performance of
    emotion classification. Experiments on Twitter data show that even though a deep
    language model fine-tuned by a target domain data has attained comparable results
    to that of previous state-of-the-art models, this fine-tuned model can still benefit
    from our extracted domain knowledge to obtain more improvement. This highlights
    the importance of making use of domain knowledge in domain-specific applications.
  address: Hong Kong, China
  author:
  - first: Wenhao
    full: Wenhao Ying
    id: wenhao-ying
    last: Ying
  - first: Rong
    full: Rong Xiang
    id: rong-xiang
    last: Xiang
  - first: Qin
    full: Qin Lu
    id: qin-lu
    last: Lu
  author_string: Wenhao Ying, Rong Xiang, Qin Lu
  bibkey: ying-etal-2019-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5541
  month: November
  page_first: '316'
  page_last: '321'
  pages: "316\u2013321"
  paper_id: '41'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5541.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5541.jpg
  title: Improving Multi-label Emotion Classification by Integrating both General
    and Domain-specific Knowledge
  title_html: Improving Multi-label Emotion Classification by Integrating both General
    and Domain-specific Knowledge
  url: https://www.aclweb.org/anthology/D19-5541
  year: '2019'
D19-5542:
  abstract: "Mental health poses a significant challenge for an individual\u2019s\
    \ well-being. Text analysis of rich resources, like social media, can contribute\
    \ to deeper understanding of illnesses and provide means for their early detection.\
    \ We tackle a challenge of detecting social media users\u2019 mental status through\
    \ deep learning-based models, moving away from traditional approaches to the task.\
    \ In a binary classification task on predicting if a user suffers from one of\
    \ nine different disorders, a hierarchical attention network outperforms previously\
    \ set benchmarks for four of the disorders. Furthermore, we explore the limitations\
    \ of our model and analyze phrases relevant for classification by inspecting the\
    \ model\u2019s word-level attention weights."
  address: Hong Kong, China
  author:
  - first: Ivan
    full: Ivan Sekulic
    id: ivan-sekulic
    last: Sekulic
  - first: Michael
    full: Michael Strube
    id: michael-strube
    last: Strube
  author_string: Ivan Sekulic, Michael Strube
  bibkey: sekulic-strube-2019-adapting
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5542
  month: November
  page_first: '322'
  page_last: '327'
  pages: "322\u2013327"
  paper_id: '42'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5542.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5542.jpg
  title: Adapting Deep Learning Methods for Mental Health Prediction on Social Media
  title_html: Adapting Deep Learning Methods for Mental Health Prediction on Social
    Media
  url: https://www.aclweb.org/anthology/D19-5542
  year: '2019'
D19-5543:
  abstract: "Neural Machine Translation (NMT) models have been proved strong when\
    \ translating clean texts, but they are very sensitive to noise in the input.\
    \ Improving NMT models robustness can be seen as a form of \u201Cdomain\u201D\
    \ adaption to noise. The recently created Machine Translation on Noisy Text task\
    \ corpus provides noisy-clean parallel data for a few language pairs, but this\
    \ data is very limited in size and diversity. The state-of-the-art approaches\
    \ are heavily dependent on large volumes of back-translated data. This paper has\
    \ two main contributions: Firstly, we propose new data augmentation methods to\
    \ extend limited noisy data and further improve NMT robustness to noise while\
    \ keeping the models small. Secondly, we explore the effect of utilizing noise\
    \ from external data in the form of speech transcripts and show that it could\
    \ help robustness."
  address: Hong Kong, China
  author:
  - first: Zhenhao
    full: Zhenhao Li
    id: zhenhao-li
    last: Li
  - first: Lucia
    full: Lucia Specia
    id: lucia-specia
    last: Specia
  author_string: Zhenhao Li, Lucia Specia
  bibkey: li-specia-2019-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5543
  month: November
  page_first: '328'
  page_last: '336'
  pages: "328\u2013336"
  paper_id: '43'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5543.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5543.jpg
  title: 'Improving Neural Machine Translation Robustness via Data Augmentation: Beyond
    Back-Translation'
  title_html: 'Improving Neural Machine Translation Robustness via Data Augmentation:
    Beyond Back-Translation'
  url: https://www.aclweb.org/anthology/D19-5543
  year: '2019'
D19-5544:
  abstract: Due to the nature of online user reviews, sentiment analysis on such data
    requires a deep semantic understanding of the text. Many online reviews are sarcastic,
    humorous, or hateful. Signals from such language nuances may reinforce or completely
    alter the sentiment of a review as predicted by a machine learning model that
    attempts to detect sentiment alone. Thus, having a model that is explicitly aware
    of these features should help it perform better on reviews that are characterized
    by them. We propose a composite two-step model that extracts features pertaining
    to sarcasm, humour, hate speech, as well as sentiment, in the first step, feeding
    them in conjunction to inform sentiment classification in the second step. We
    show that this multi-step approach leads to a better empirical performance for
    sentiment classification than a model that predicts sentiment alone. A qualitative
    analysis reveals that the conjunctive approach can better capture the nuances
    of sentiment as expressed in online reviews.
  address: Hong Kong, China
  attachment:
  - filename: D19-5544.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5544.Attachment.zip
  author:
  - first: Rohan
    full: Rohan Badlani
    id: rohan-badlani
    last: Badlani
  - first: Nishit
    full: Nishit Asnani
    id: nishit-asnani
    last: Asnani
  - first: Manan
    full: Manan Rai
    id: manan-rai
    last: Rai
  author_string: Rohan Badlani, Nishit Asnani, Manan Rai
  bibkey: badlani-etal-2019-ensemble
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5544
  month: November
  page_first: '337'
  page_last: '345'
  pages: "337\u2013345"
  paper_id: '44'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5544.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5544.jpg
  title: An Ensemble of Humour, Sarcasm, and Hate Speechfor Sentiment Classification
    in Online Reviews
  title_html: An Ensemble of Humour, Sarcasm, and Hate Speechfor Sentiment Classification
    in Online Reviews
  url: https://www.aclweb.org/anthology/D19-5544
  year: '2019'
D19-5545:
  abstract: Grammatical error correction in English is a long studied problem with
    many existing systems and datasets. However, there has been only a limited research
    on error correction of other languages. In this paper, we present a new dataset
    AKCES-GEC on grammatical error correction for Czech. We then make experiments
    on Czech, German and Russian and show that when utilizing synthetic parallel corpus,
    Transformer neural machine translation model can reach new state-of-the-art results
    on these datasets. AKCES-GEC is published under CC BY-NC-SA 4.0 license at http://hdl.handle.net/11234/1-3057,
    and the source code of the GEC model is available at https://github.com/ufal/low-resource-gec-wnut2019.
  address: Hong Kong, China
  author:
  - first: Jakub
    full: "Jakub N\xE1plava"
    id: jakub-naplava
    last: "N\xE1plava"
  - first: Milan
    full: Milan Straka
    id: milan-straka
    last: Straka
  author_string: "Jakub N\xE1plava, Milan Straka"
  bibkey: naplava-straka-2019-grammatical
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5545
  month: November
  page_first: '346'
  page_last: '356'
  pages: "346\u2013356"
  paper_id: '45'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5545.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5545.jpg
  title: Grammatical Error Correction in Low-Resource Scenarios
  title_html: Grammatical Error Correction in Low-Resource Scenarios
  url: https://www.aclweb.org/anthology/D19-5545
  year: '2019'
D19-5546:
  abstract: There has been an increased interest in low-resource approaches to automatic
    grammatical error correction. We introduce Minimally-Augmented Grammatical Error
    Correction (MAGEC) that does not require any error-labelled data. Our unsupervised
    approach is based on a simple but effective synthetic error generation method
    based on confusion sets from inverted spell-checkers. In low-resource settings,
    we outperform the current state-of-the-art results for German and Russian GEC
    tasks by a large margin without using any real error-annotated training data.
    When combined with labelled data, our method can serve as an efficient pre-training
    technique
  address: Hong Kong, China
  author:
  - first: Roman
    full: Roman Grundkiewicz
    id: roman-grundkiewicz
    last: Grundkiewicz
  - first: Marcin
    full: Marcin Junczys-Dowmunt
    id: marcin-junczys-dowmunt
    last: Junczys-Dowmunt
  author_string: Roman Grundkiewicz, Marcin Junczys-Dowmunt
  bibkey: grundkiewicz-junczys-dowmunt-2019-minimally
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5546
  month: November
  page_first: '357'
  page_last: '363'
  pages: "357\u2013363"
  paper_id: '46'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5546.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5546.jpg
  title: Minimally-Augmented Grammatical Error Correction
  title_html: Minimally-Augmented Grammatical Error Correction
  url: https://www.aclweb.org/anthology/D19-5546
  year: '2019'
D19-5547:
  abstract: We present a gold standard of annotated social opinion for the Malta Government
    Budget 2018. It consists of over 500 online posts in English and/or the Maltese
    less-resourced language, gathered from social media platforms, specifically, social
    networking services and newswires, which have been annotated with information
    about opinions expressed by the general public and other entities, in terms of
    sentiment polarity, emotion, sarcasm/irony, and negation. This dataset is a resource
    for opinion mining based on social data, within the context of politics. It is
    the first opinion annotated social dataset from Malta, which has very limited
    language resources available.
  address: Hong Kong, China
  author:
  - first: Keith
    full: Keith Cortis
    id: keith-cortis
    last: Cortis
  - first: Brian
    full: Brian Davis
    id: brian-davis
    last: Davis
  author_string: Keith Cortis, Brian Davis
  bibkey: cortis-davis-2019-social
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5547
  month: November
  page_first: '364'
  page_last: '369'
  pages: "364\u2013369"
  paper_id: '47'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5547.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5547.jpg
  title: A Social Opinion Gold Standard for the Malta Government Budget 2018
  title_html: A Social Opinion Gold Standard for the Malta Government Budget 2018
  url: https://www.aclweb.org/anthology/D19-5547
  year: '2019'
D19-5548:
  abstract: This study analyzes the political slants of user comments on Korean partisan
    media. We built a BERT-based classifier to detect political leaning of short comments
    via the use of semi-unsupervised deep learning methods that produced an F1 score
    of 0.83. As a result of classifying 21.6K comments, we found the high presence
    of conservative bias on both conservative and liberal news outlets. Moreover,
    this study discloses an asymmetry across the partisan spectrum in that more liberals
    (48.0%) than conservatives (23.6%) comment not only on news stories resonating
    with their political perspectives but also on those challenging their viewpoints.
    These findings advance the current understanding of online echo chambers.
  address: Hong Kong, China
  author:
  - first: Jiyoung
    full: Jiyoung Han
    id: jiyoung-han
    last: Han
  - first: Youngin
    full: Youngin Lee
    id: youngin-lee
    last: Lee
  - first: Junbum
    full: Junbum Lee
    id: junbum-lee
    last: Lee
  - first: Meeyoung
    full: Meeyoung Cha
    id: meeyoung-cha
    last: Cha
  author_string: Jiyoung Han, Youngin Lee, Junbum Lee, Meeyoung Cha
  bibkey: han-etal-2019-fallacy
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5548
  month: November
  page_first: '370'
  page_last: '374'
  pages: "370\u2013374"
  paper_id: '48'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5548.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5548.jpg
  title: 'The Fallacy of Echo Chambers: Analyzing the Political Slants of User-Generated
    News Comments in Korean Media'
  title_html: 'The Fallacy of Echo Chambers: Analyzing the Political Slants of User-Generated
    News Comments in <span class="acl-fixed-case">K</span>orean Media'
  url: https://www.aclweb.org/anthology/D19-5548
  year: '2019'
D19-5549:
  abstract: "Distinguishing between singular and plural \u201Cyou\u201D in English\
    \ is a challenging task which has potential for downstream applications, such\
    \ as machine translation or coreference resolution. While formal written English\
    \ does not distinguish between these cases, other languages (such as Spanish),\
    \ as well as other dialects of English (via phrases such as \u201Cy\u2019all\u201D\
    ), do make this distinction. We make use of this to obtain distantly-supervised\
    \ labels for the task on a large-scale in two domains. Following, we train a model\
    \ to distinguish between the single/plural \u2018you\u2019, finding that although\
    \ in-domain training achieves reasonable accuracy (\u2265 77%), there is still\
    \ a lot of room for improvement, especially in the domain-transfer scenario, which\
    \ proves extremely challenging. Our code and data are publicly available."
  address: Hong Kong, China
  author:
  - first: Gabriel
    full: Gabriel Stanovsky
    id: gabriel-stanovsky
    last: Stanovsky
  - first: Ronen
    full: Ronen Tamari
    id: ronen-tamari
    last: Tamari
  author_string: Gabriel Stanovsky, Ronen Tamari
  bibkey: stanovsky-tamari-2019-yall
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5549
  month: November
  page_first: '375'
  page_last: '380'
  pages: "375\u2013380"
  paper_id: '49'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5549.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5549.jpg
  title: "Y\u2019all should read this! Identifying Plurality in Second-Person Personal\
    \ Pronouns in English Texts"
  title_html: "Y\u2019all should read this! Identifying Plurality in Second-Person\
    \ Personal Pronouns in <span class=\"acl-fixed-case\">E</span>nglish Texts"
  url: https://www.aclweb.org/anthology/D19-5549
  year: '2019'
D19-5550:
  abstract: We propose an edit-centric approach to assess Wikipedia article quality
    as a complementary alternative to current full document-based techniques. Our
    model consists of a main classifier equipped with an auxiliary generative module
    which, for a given edit, jointly provides an estimation of its quality and generates
    a description in natural language. We performed an empirical study to assess the
    feasibility of the proposed model and its cost-effectiveness in terms of data
    and quality requirements.
  address: Hong Kong, China
  attachment:
  - filename: D19-5550.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5550.Attachment.pdf
  author:
  - first: Edison
    full: Edison Marrese-Taylor
    id: edison-marrese-taylor
    last: Marrese-Taylor
  - first: Pablo
    full: Pablo Loyola
    id: pablo-loyola
    last: Loyola
  - first: Yutaka
    full: Yutaka Matsuo
    id: yutaka-matsuo
    last: Matsuo
  author_string: Edison Marrese-Taylor, Pablo Loyola, Yutaka Matsuo
  bibkey: marrese-taylor-etal-2019-edit
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5550
  month: November
  page_first: '381'
  page_last: '386'
  pages: "381\u2013386"
  paper_id: '50'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5550.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5550.jpg
  title: An Edit-centric Approach for Wikipedia Article Quality Assessment
  title_html: An Edit-centric Approach for <span class="acl-fixed-case">W</span>ikipedia
    Article Quality Assessment
  url: https://www.aclweb.org/anthology/D19-5550
  year: '2019'
D19-5551:
  abstract: "Additive compositionality of word embedding models has been studied from\
    \ empirical and theoretical perspectives. Existing research on justifying additive\
    \ compositionality of existing word embedding models requires a rather strong\
    \ assumption of uniform word distribution. In this paper, we relax that assumption\
    \ and propose more realistic conditions for proving additive compositionality,\
    \ and we develop a novel word and sub-word embedding model that satisfies additive\
    \ compositionality under those conditions. We then empirically show our model\u2019\
    s improved semantic representation performance on word similarity and noisy sentence\
    \ similarity."
  address: Hong Kong, China
  author:
  - first: Yeon
    full: Yeon Seonwoo
    id: yeon-seonwoo
    last: Seonwoo
  - first: Sungjoon
    full: Sungjoon Park
    id: sungjoon-park
    last: Park
  - first: Dongkwan
    full: Dongkwan Kim
    id: dongkwan-kim
    last: Kim
  - first: Alice
    full: Alice Oh
    id: alice-oh
    last: Oh
  author_string: Yeon Seonwoo, Sungjoon Park, Dongkwan Kim, Alice Oh
  bibkey: seonwoo-etal-2019-additive
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5551
  month: November
  page_first: '387'
  page_last: '396'
  pages: "387\u2013396"
  paper_id: '51'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5551.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5551.jpg
  title: Additive Compositionality of Word Vectors
  title_html: Additive Compositionality of Word Vectors
  url: https://www.aclweb.org/anthology/D19-5551
  year: '2019'
D19-5552:
  abstract: 'Lexical substitution ranks substitution candidates from the viewpoint
    of paraphrasability for a target word in a given sentence. There are two major
    approaches for lexical substitution: (1) generating contextualized word embeddings
    by assigning multiple embeddings to one word and (2) generating context embeddings
    using the sentence. Herein we propose a method that combines these two approaches
    to contextualize word embeddings for lexical substitution. Experiments demonstrate
    that our method outperforms the current state-of-the-art method. We also create
    CEFR-LP, a new evaluation dataset for the lexical substitution task. It has a
    wider coverage of substitution candidates than previous datasets and assigns English
    proficiency levels to all target words and substitution candidates.'
  address: Hong Kong, China
  author:
  - first: Kazuki
    full: Kazuki Ashihara
    id: kazuki-ashihara
    last: Ashihara
  - first: Tomoyuki
    full: Tomoyuki Kajiwara
    id: tomoyuki-kajiwara
    last: Kajiwara
  - first: Yuki
    full: Yuki Arase
    id: yuki-arase
    last: Arase
  - first: Satoru
    full: Satoru Uchida
    id: satoru-uchida
    last: Uchida
  author_string: Kazuki Ashihara, Tomoyuki Kajiwara, Yuki Arase, Satoru Uchida
  bibkey: ashihara-etal-2019-contextualized
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5552
  month: November
  page_first: '397'
  page_last: '406'
  pages: "397\u2013406"
  paper_id: '52'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5552.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5552.jpg
  title: Contextualized context2vec
  title_html: Contextualized context2vec
  url: https://www.aclweb.org/anthology/D19-5552
  year: '2019'
D19-5553:
  abstract: We present an approach to correct noisy User Generated Content (UGC) in
    French aiming to produce a pretreatement pipeline to improve Machine Translation
    for this kind of non-canonical corpora. In order to do so, we have implemented
    a character-based neural model phonetizer to produce IPA pronunciations of words.
    In this way, we intend to correct grammar, vocabulary and accentuation errors
    often present in noisy UGC corpora. Our method leverages on the fact that some
    errors are due to confusion induced by words with similar pronunciation which
    can be corrected using a phonetic look-up table to produce normalization candidates.
    These potential corrections are then encoded in a lattice and ranked using a language
    model to output the most probable corrected phrase. Compare to using other phonetizers,
    our method boosts a transformer-based machine translation system on UGC.
  address: Hong Kong, China
  author:
  - first: "Jos\xE9 Carlos"
    full: "Jos\xE9 Carlos Rosales N\xFA\xF1ez"
    id: jose-carlos-rosales-nunez
    last: "Rosales N\xFA\xF1ez"
  - first: "Djam\xE9"
    full: "Djam\xE9 Seddah"
    id: djame-seddah
    last: Seddah
  - first: Guillaume
    full: Guillaume Wisniewski
    id: guillaume-wisniewski
    last: Wisniewski
  author_string: "Jos\xE9 Carlos Rosales N\xFA\xF1ez, Djam\xE9 Seddah, Guillaume Wisniewski"
  bibkey: rosales-nunez-etal-2019-phonetic
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5553
  month: November
  page_first: '407'
  page_last: '416'
  pages: "407\u2013416"
  paper_id: '53'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5553.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5553.jpg
  title: Phonetic Normalization for Machine Translation of User Generated Content
  title_html: Phonetic Normalization for Machine Translation of User Generated Content
  url: https://www.aclweb.org/anthology/D19-5553
  year: '2019'
D19-5554:
  abstract: 'Twitter is an excellent source of data for NLP researches as it offers
    tremendous amount of textual data. However, processing tweet to extract meaningful
    information is very challenging, at least for two reasons: (i) using nonstandard
    words as well as informal writing manner, and (ii) code-mixing issues, which is
    combining multiple languages in single tweet conversation. Most of the previous
    works have addressed both issues in isolated different task. In this study, we
    work on normalization task in code-mixed Twitter data, more specifically in Indonesian-English
    language. We propose a pipeline that consists of four modules, i.e tokenization,
    language identification, lexical normalization, and translation. Another contribution
    is to provide a gold standard of Indonesian-English code-mixed data for each module.'
  address: Hong Kong, China
  author:
  - first: Anab Maulana
    full: Anab Maulana Barik
    id: anab-maulana-barik
    last: Barik
  - first: Rahmad
    full: Rahmad Mahendra
    id: rahmad-mahendra
    last: Mahendra
  - first: Mirna
    full: Mirna Adriani
    id: mirna-adriani
    last: Adriani
  author_string: Anab Maulana Barik, Rahmad Mahendra, Mirna Adriani
  bibkey: barik-etal-2019-normalization
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5554
  month: November
  page_first: '417'
  page_last: '424'
  pages: "417\u2013424"
  paper_id: '54'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5554.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5554.jpg
  title: Normalization of Indonesian-English Code-Mixed Twitter Data
  title_html: Normalization of <span class="acl-fixed-case">I</span>ndonesian-<span
    class="acl-fixed-case">E</span>nglish Code-Mixed Twitter Data
  url: https://www.aclweb.org/anthology/D19-5554
  year: '2019'
D19-5555:
  abstract: This paper presents an approach for detecting and normalizing neologisms
    in social media content. Neologisms refer to recent expressions that are specific
    to certain entities or events and are being increasingly used by the public, but
    have not yet been accepted in mainstream language. Automated methods for handling
    neologisms are important for natural language understanding and normalization,
    especially for informal genres with user generated content. We present an unsupervised
    approach for detecting neologisms and then normalizing them to canonical words
    without relying on parallel training data. Our approach builds on the text normalization
    literature and introduces adaptations to fit the specificities of this task, including
    phonetic and etymological considerations. We evaluate the proposed techniques
    on a dataset of Reddit comments, with detected neologisms and corresponding normalizations.
  address: Hong Kong, China
  author:
  - first: Nasser
    full: Nasser Zalmout
    id: nasser-zalmout
    last: Zalmout
  - first: Kapil
    full: Kapil Thadani
    id: kapil-thadani
    last: Thadani
  - first: Aasish
    full: Aasish Pappu
    id: aasish-pappu
    last: Pappu
  author_string: Nasser Zalmout, Kapil Thadani, Aasish Pappu
  bibkey: zalmout-etal-2019-unsupervised
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5555
  month: November
  page_first: '425'
  page_last: '430'
  pages: "425\u2013430"
  paper_id: '55'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5555.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5555.jpg
  title: Unsupervised Neologism Normalization Using Embedding Space Mapping
  title_html: Unsupervised Neologism Normalization Using Embedding Space Mapping
  url: https://www.aclweb.org/anthology/D19-5555
  year: '2019'
D19-5556:
  abstract: 'Understanding the vulnerability of linguistic features extracted from
    noisy text is important for both developing better health text classification
    models and for interpreting vulnerabilities of natural language models. In this
    paper, we investigate how generic language characteristics, such as syntax or
    the lexicon, are impacted by artificial text alterations. The vulnerability of
    features is analysed from two perspectives: (1) the level of feature value change,
    and (2) the level of change of feature predictive power as a result of text modifications.
    We show that lexical features are more sensitive to text modifications than syntactic
    ones. However, we also demonstrate that these smaller changes of syntactic features
    have a stronger influence on classification performance downstream, compared to
    the impact of changes to lexical features. Results are validated across three
    datasets representing different text-classification tasks, with different levels
    of lexical and syntactic complexity of both conversational and written language.'
  address: Hong Kong, China
  author:
  - first: Jekaterina
    full: Jekaterina Novikova
    id: jekaterina-novikova
    last: Novikova
  - first: Aparna
    full: Aparna Balagopalan
    id: aparna-balagopalan
    last: Balagopalan
  - first: Ksenia
    full: Ksenia Shkaruta
    id: ksenia-shkaruta
    last: Shkaruta
  - first: Frank
    full: Frank Rudzicz
    id: frank-rudzicz
    last: Rudzicz
  author_string: Jekaterina Novikova, Aparna Balagopalan, Ksenia Shkaruta, Frank Rudzicz
  bibkey: novikova-etal-2019-lexical
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5556
  month: November
  page_first: '431'
  page_last: '443'
  pages: "431\u2013443"
  paper_id: '56'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5556.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5556.jpg
  title: Lexical Features Are More Vulnerable, Syntactic Features Have More Predictive
    Power
  title_html: Lexical Features Are More Vulnerable, Syntactic Features Have More Predictive
    Power
  url: https://www.aclweb.org/anthology/D19-5556
  year: '2019'
D19-5557:
  abstract: 'Regarding the problem of automatically generating paraphrases with modified
    styles or attributes, the difficulty lies in the lack of parallel corpora. Numerous
    advances have been proposed for the generation. However, significant problems
    remain with the auto-evaluation of style transfer tasks. Based on the summary
    of Pang and Gimpel (2018) and Mir et al. (2019), style transfer evaluations rely
    on three metrics: post-transfer style classification accuracy, content or semantic
    similarity, and naturalness or fluency. We elucidate the dangerous current state
    of style transfer auto-evaluation research. Moreover, we propose ways to aggregate
    the three metrics into one evaluator. This abstract aims to bring researchers
    to think about the future of style transfer and style transfer evaluation research.'
  address: Hong Kong, China
  author:
  - first: Richard Yuanzhe
    full: Richard Yuanzhe Pang
    id: richard-yuanzhe-pang
    last: Pang
  author_string: Richard Yuanzhe Pang
  bibkey: pang-2019-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5557
  month: November
  page_first: '444'
  page_last: '445'
  pages: "444\u2013445"
  paper_id: '57'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5557.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5557.jpg
  title: Towards Actual (Not Operational) Textual Style Transfer Auto-Evaluation
  title_html: Towards Actual (Not Operational) Textual Style Transfer Auto-Evaluation
  url: https://www.aclweb.org/anthology/D19-5557
  year: '2019'
D19-5558:
  abstract: In contrast to many decades of research on oral code-switching, the study
    of written multilingual productions has only recently enjoyed a surge of interest.
    Many open questions remain regarding the sociolinguistic underpinnings of written
    code-switching, and progress has been limited by a lack of suitable resources.
    We introduce a novel, large, and diverse dataset of written code-switched productions,
    curated from topical threads of multiple bilingual communities on the Reddit discussion
    platform, and explore questions that were mainly addressed in the context of spoken
    language thus far. We investigate whether findings in oral code-switching concerning
    content and style, as well as speaker proficiency, are carried over into written
    code-switching in discussion forums. The released dataset can further facilitate
    a range of research and practical activities.
  address: Hong Kong, China
  author:
  - first: Ella
    full: Ella Rabinovich
    id: ella-rabinovich
    last: Rabinovich
  - first: Masih
    full: Masih Sultani
    id: masih-sultani
    last: Sultani
  - first: Suzanne
    full: Suzanne Stevenson
    id: suzanne-stevenson
    last: Stevenson
  author_string: Ella Rabinovich, Masih Sultani, Suzanne Stevenson
  bibkey: rabinovich-etal-2019-codeswitch-reddit
  bibtype: inproceedings
  booktitle: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)
  booktitle_html: Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT
    2019)
  doi: 10.18653/v1/D19-5558
  month: November
  pages: '446'
  paper_id: '58'
  parent_volume_id: D19-55
  pdf: https://www.aclweb.org/anthology/D19-5558.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5558.jpg
  title: 'CodeSwitch-Reddit: Exploration of Written Multilingual Discourse in Online
    Discussion Forums'
  title_html: '<span class="acl-fixed-case">C</span>ode<span class="acl-fixed-case">S</span>witch-<span
    class="acl-fixed-case">R</span>eddit: Exploration of Written Multilingual Discourse
    in Online Discussion Forums'
  url: https://www.aclweb.org/anthology/D19-5558
  year: '2019'
D19-5600:
  address: Hong Kong
  author:
  - first: Alexandra
    full: Alexandra Birch
    id: alexandra-birch
    last: Birch
  - first: Andrew
    full: Andrew Finch
    id: andrew-finch
    last: Finch
  - first: Hiroaki
    full: Hiroaki Hayashi
    id: hiroaki-hayashi
    last: Hayashi
  - first: Ioannis
    full: Ioannis Konstas
    id: ioannis-konstas
    last: Konstas
  - first: Thang
    full: Thang Luong
    id: minh-thang-luong
    last: Luong
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  - first: Yusuke
    full: Yusuke Oda
    id: yusuke-oda
    last: Oda
  - first: Katsuhito
    full: Katsuhito Sudoh
    id: katsuhito-sudoh
    last: Sudoh
  author_string: Alexandra Birch, Andrew Finch, Hiroaki Hayashi, Ioannis Konstas,
    Thang Luong, Graham Neubig, Yusuke Oda, Katsuhito Sudoh
  bibkey: emnlp-2019-neural
  bibtype: proceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  month: November
  paper_id: '0'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5600.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5600.jpg
  title: Proceedings of the 3rd Workshop on Neural Generation and Translation
  title_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  url: https://www.aclweb.org/anthology/D19-5600
  year: '2019'
D19-5601:
  abstract: This document describes the findings of the Third Workshop on Neural Generation
    and Translation, held in concert with the annual conference of the Empirical Methods
    in Natural Language Processing (EMNLP 2019). First, we summarize the research
    trends of papers presented in the proceedings. Second, we describe the results
    of the two shared tasks 1) efficient neural machine translation (NMT) where participants
    were tasked with creating NMT systems that are both accurate and efficient, and
    2) document generation and translation (DGT) where participants were tasked with
    developing systems that generate summaries from structured data, potentially with
    assistance from text in another language.
  address: Hong Kong
  author:
  - first: Hiroaki
    full: Hiroaki Hayashi
    id: hiroaki-hayashi
    last: Hayashi
  - first: Yusuke
    full: Yusuke Oda
    id: yusuke-oda
    last: Oda
  - first: Alexandra
    full: Alexandra Birch
    id: alexandra-birch
    last: Birch
  - first: Ioannis
    full: Ioannis Konstas
    id: ioannis-konstas
    last: Konstas
  - first: Andrew
    full: Andrew Finch
    id: andrew-finch
    last: Finch
  - first: Minh-Thang
    full: Minh-Thang Luong
    id: minh-thang-luong
    last: Luong
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  - first: Katsuhito
    full: Katsuhito Sudoh
    id: katsuhito-sudoh
    last: Sudoh
  author_string: Hiroaki Hayashi, Yusuke Oda, Alexandra Birch, Ioannis Konstas, Andrew
    Finch, Minh-Thang Luong, Graham Neubig, Katsuhito Sudoh
  bibkey: hayashi-etal-2019-findings
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5601
  month: November
  page_first: '1'
  page_last: '14'
  pages: "1\u201314"
  paper_id: '1'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5601.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5601.jpg
  title: Findings of the Third Workshop on Neural Generation and Translation
  title_html: Findings of the Third Workshop on Neural Generation and Translation
  url: https://www.aclweb.org/anthology/D19-5601
  year: '2019'
D19-5602:
  abstract: 'Data scarcity is a long-standing and crucial challenge that hinders quick
    development of task-oriented dialogue systems across multiple domains: task-oriented
    dialogue models are expected to learn grammar, syntax, dialogue reasoning, decision
    making, and language generation from absurdly small amounts of task-specific data.
    In this paper, we demonstrate that recent progress in language modeling pre-training
    and transfer learning shows promise to overcome this problem. We propose a task-oriented
    dialogue model that operates solely on text input: it effectively bypasses explicit
    policy and language generation modules. Building on top of the TransferTransfo
    framework (Wolf et al., 2019) and generative model pre-training (Radford et al.,
    2019), we validate the approach on complex multi-domain task-oriented dialogues
    from the MultiWOZ dataset. Our automatic and human evaluations show that the proposed
    model is on par with a strong task-specific neural baseline. In the long run,
    our approach holds promise to mitigate the data scarcity problem, and to support
    the construction of more engaging and more eloquent task-oriented conversational
    agents.'
  address: Hong Kong
  author:
  - first: "Pawe\u0142"
    full: "Pawe\u0142 Budzianowski"
    id: pawel-budzianowski
    last: Budzianowski
  - first: Ivan
    full: "Ivan Vuli\u0107"
    id: ivan-vulic
    last: "Vuli\u0107"
  author_string: "Pawe\u0142 Budzianowski, Ivan Vuli\u0107"
  bibkey: budzianowski-vulic-2019-hello
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5602
  month: November
  page_first: '15'
  page_last: '22'
  pages: "15\u201322"
  paper_id: '2'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5602.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5602.jpg
  title: "Hello, It\u2019s GPT-2 - How Can I Help You? Towards the Use of Pretrained\
    \ Language Models for Task-Oriented Dialogue Systems"
  title_html: "Hello, It\u2019s <span class=\"acl-fixed-case\">GPT</span>-2 - How\
    \ Can <span class=\"acl-fixed-case\">I</span> Help You? Towards the Use of Pretrained\
    \ Language Models for Task-Oriented Dialogue Systems"
  url: https://www.aclweb.org/anthology/D19-5602
  year: '2019'
D19-5603:
  abstract: In this paper, a pre-trained Bidirectional Encoder Representations from
    Transformers (BERT) model is applied to Transformer-based neural machine translation
    (NMT). In contrast to monolingual tasks, the number of unlearned model parameters
    in an NMT decoder is as huge as the number of learned parameters in the BERT model.
    To train all the models appropriately, we employ two-stage optimization, which
    first trains only the unlearned parameters by freezing the BERT model, and then
    fine-tunes all the sub-models. In our experiments, stable two-stage optimization
    was achieved, in contrast the BLEU scores of direct fine-tuning were extremely
    low. Consequently, the BLEU scores of the proposed method were better than those
    of the Transformer base model and the same model without pre-training. Additionally,
    we confirmed that NMT with the BERT encoder is more effective in low-resource
    settings.
  address: Hong Kong
  author:
  - first: Kenji
    full: Kenji Imamura
    id: kenji-imamura
    last: Imamura
  - first: Eiichiro
    full: Eiichiro Sumita
    id: eiichiro-sumita
    last: Sumita
  author_string: Kenji Imamura, Eiichiro Sumita
  bibkey: imamura-sumita-2019-recycling
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5603
  month: November
  page_first: '23'
  page_last: '31'
  pages: "23\u201331"
  paper_id: '3'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5603.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5603.jpg
  title: Recycling a Pre-trained BERT Encoder for Neural Machine Translation
  title_html: Recycling a Pre-trained <span class="acl-fixed-case">BERT</span> Encoder
    for Neural Machine Translation
  url: https://www.aclweb.org/anthology/D19-5603
  year: '2019'
D19-5604:
  abstract: Ambiguous user queries in search engines result in the retrieval of documents
    that often span multiple topics. One potential solution is for the search engine
    to generate multiple refined queries, each of which relates to a subset of the
    documents spanning the same topic. A preliminary step towards this goal is to
    generate a question that captures common concepts of multiple documents. We propose
    a new task of generating common question from multiple documents and present simple
    variant of an existing multi-source encoder-decoder framework, called the Multi-Source
    Question Generator (MSQG). We first train an RNN-based single encoder-decoder
    generator from (single document, question) pairs. At test time, given multiple
    documents, the Distribute step of our MSQG model predicts target word distributions
    for each document using the trained model. The Aggregate step aggregates these
    distributions to generate a common question. This simple yet effective strategy
    significantly outperforms several existing baseline models applied to the new
    task when evaluated using automated metrics and human judgments on the MS-MARCO-QA
    dataset.
  address: Hong Kong
  author:
  - first: Woon Sang
    full: Woon Sang Cho
    id: woon-sang-cho
    last: Cho
  - first: Yizhe
    full: Yizhe Zhang
    id: yizhe-zhang
    last: Zhang
  - first: Sudha
    full: Sudha Rao
    id: sudha-rao
    last: Rao
  - first: Chris
    full: Chris Brockett
    id: chris-brockett
    last: Brockett
  - first: Sungjin
    full: Sungjin Lee
    id: sungjin-lee
    last: Lee
  author_string: Woon Sang Cho, Yizhe Zhang, Sudha Rao, Chris Brockett, Sungjin Lee
  bibkey: cho-etal-2019-generating
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5604
  month: November
  page_first: '32'
  page_last: '43'
  pages: "32\u201343"
  paper_id: '4'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5604.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5604.jpg
  title: Generating a Common Question from Multiple Documents using Multi-source Encoder-Decoder
    Models
  title_html: Generating a Common Question from Multiple Documents using Multi-source
    Encoder-Decoder Models
  url: https://www.aclweb.org/anthology/D19-5604
  year: '2019'
D19-5605:
  abstract: 'We propose a simple and effective modeling framework for controlled generation
    of multiple, diverse outputs. We focus on the setting of generating the next sentence
    of a story given its context. As controllable dimensions, we consider several
    sentence attributes, including sentiment, length, predicates, frames, and automatically-induced
    clusters. Our empirical results demonstrate: (1) our framework is accurate in
    terms of generating outputs that match the target control values; (2) our model
    yields increased maximum metric scores compared to standard n-best list generation
    via beam search; (3) controlling generation with semantic frames leads to a stronger
    combination of diversity and quality than other control variables as measured
    by automatic metrics. We also conduct a human evaluation to assess the utility
    of providing multiple suggestions for creative writing, demonstrating promising
    results for the potential of controllable, diverse generation in a collaborative
    writing system.'
  address: Hong Kong
  author:
  - first: Lifu
    full: Lifu Tu
    id: lifu-tu
    last: Tu
  - first: Xiaoan
    full: Xiaoan Ding
    id: xiaoan-ding
    last: Ding
  - first: Dong
    full: Dong Yu
    id: dong-yu
    last: Yu
  - first: Kevin
    full: Kevin Gimpel
    id: kevin-gimpel
    last: Gimpel
  author_string: Lifu Tu, Xiaoan Ding, Dong Yu, Kevin Gimpel
  bibkey: tu-etal-2019-generating
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5605
  month: November
  page_first: '44'
  page_last: '58'
  pages: "44\u201358"
  paper_id: '5'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5605.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5605.jpg
  title: Generating Diverse Story Continuations with Controllable Semantics
  title_html: Generating Diverse Story Continuations with Controllable Semantics
  url: https://www.aclweb.org/anthology/D19-5605
  year: '2019'
D19-5606:
  abstract: Neural networks are known to be data hungry and domain sensitive, but
    it is nearly impossible to obtain large quantities of labeled data for every domain
    we are interested in. This necessitates the use of domain adaptation strategies.
    One common strategy encourages generalization by aligning the global distribution
    statistics between source and target domains, but one drawback is that the statistics
    of different domains or tasks are inherently divergent, and smoothing over these
    differences can lead to sub-optimal performance. In this paper, we propose the
    framework of Domain Differential Adaptation (DDA), where instead of smoothing
    over these differences we embrace them, directly modeling the difference between
    domains using models in a related task. We then use these learned domain differentials
    to adapt models for the target task accordingly. Experimental results on domain
    adaptation for neural machine translation demonstrate the effectiveness of this
    strategy, achieving consistent improvements over other alternative adaptation
    strategies in multiple experimental settings.
  address: Hong Kong
  author:
  - first: Zi-Yi
    full: Zi-Yi Dou
    id: zi-yi-dou
    last: Dou
  - first: Xinyi
    full: Xinyi Wang
    id: xinyi-wang
    last: Wang
  - first: Junjie
    full: Junjie Hu
    id: junjie-hu
    last: Hu
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  author_string: Zi-Yi Dou, Xinyi Wang, Junjie Hu, Graham Neubig
  bibkey: dou-etal-2019-domain
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5606
  month: November
  page_first: '59'
  page_last: '69'
  pages: "59\u201369"
  paper_id: '6'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5606.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5606.jpg
  title: Domain Differential Adaptation for Neural Machine Translation
  title_html: Domain Differential Adaptation for Neural Machine Translation
  url: https://www.aclweb.org/anthology/D19-5606
  year: '2019'
D19-5607:
  abstract: "We propose a system that improves performance on single document summarization\
    \ task using the CNN/DailyMail and Newsroom datasets. It follows the popular encoder-decoder\
    \ paradigm, but with an extra focus on the encoder. The intuition is that the\
    \ probability of correctly decoding an information significantly lies in the pattern\
    \ and correctness of the encoder. Hence we introduce, encode \u2013encode \u2013\
    \ decode. A framework that encodes the source text first with a transformer, then\
    \ a sequence-to-sequence (seq2seq) model. We find that the transformer and seq2seq\
    \ model complement themselves adequately, making for a richer encoded vector representation.\
    \ We also find that paying more attention to the vocabulary of target words during\
    \ abstraction improves performance. We experiment our hypothesis and framework\
    \ on the task of extractive and abstractive single document summarization and\
    \ evaluate using the standard CNN/DailyMail dataset and the recently released\
    \ Newsroom dataset."
  address: Hong Kong
  author:
  - first: Elozino
    full: Elozino Egonmwan
    id: elozino-egonmwan
    last: Egonmwan
  - first: Yllias
    full: Yllias Chali
    id: yllias-chali
    last: Chali
  author_string: Elozino Egonmwan, Yllias Chali
  bibkey: egonmwan-chali-2019-transformer
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5607
  month: November
  page_first: '70'
  page_last: '79'
  pages: "70\u201379"
  paper_id: '7'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5607.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5607.jpg
  title: Transformer-based Model for Single Documents Neural Summarization
  title_html: Transformer-based Model for Single Documents Neural Summarization
  url: https://www.aclweb.org/anthology/D19-5607
  year: '2019'
D19-5608:
  abstract: Asynchronous stochastic gradient descent (SGD) converges poorly for Transformer
    models, so synchronous SGD has become the norm for Transformer training. This
    is unfortunate because asynchronous SGD is faster at raw training speed since
    it avoids waiting for synchronization. Moreover, the Transformer model is the
    basis for state-of-the-art models for several tasks, including machine translation,
    so training speed matters. To understand why asynchronous SGD under-performs,
    we blur the lines between asynchronous and synchronous methods. We find that summing
    several asynchronous updates, rather than applying them immediately, restores
    convergence behavior. With this method, the Transformer attains the same BLEU
    score 1.36 times as fast.
  address: Hong Kong
  author:
  - first: Alham Fikri
    full: Alham Fikri Aji
    id: alham-fikri-aji
    last: Aji
  - first: Kenneth
    full: Kenneth Heafield
    id: kenneth-heafield
    last: Heafield
  author_string: Alham Fikri Aji, Kenneth Heafield
  bibkey: aji-heafield-2019-making
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5608
  month: November
  page_first: '80'
  page_last: '89'
  pages: "80\u201389"
  paper_id: '8'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5608.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5608.jpg
  title: Making Asynchronous Stochastic Gradient Descent Work for Transformers
  title_html: Making Asynchronous Stochastic Gradient Descent Work for Transformers
  url: https://www.aclweb.org/anthology/D19-5608
  year: '2019'
D19-5609:
  abstract: Data availability is a bottleneck during early stages of development of
    new capabilities for intelligent artificial agents. We investigate the use of
    text generation techniques to augment the training data of a popular commercial
    artificial agent across categories of functionality, with the goal of faster development
    of new functionality. We explore a variety of encoder-decoder generative models
    for synthetic training data generation and propose using conditional variational
    auto-encoders. Our approach requires only direct optimization, works well with
    limited data and significantly outperforms the previous controlled text generation
    techniques. Further, the generated data are used as additional training samples
    in an extrinsic intent classification task, leading to improved performance by
    up to 5% absolute f-score in low-resource cases, validating the usefulness of
    our approach.
  address: Hong Kong
  author:
  - first: Nikolaos
    full: Nikolaos Malandrakis
    id: nikolaos-malandrakis
    last: Malandrakis
  - first: Minmin
    full: Minmin Shen
    id: minmin-shen
    last: Shen
  - first: Anuj
    full: Anuj Goyal
    id: anuj-goyal
    last: Goyal
  - first: Shuyang
    full: Shuyang Gao
    id: shuyang-gao
    last: Gao
  - first: Abhishek
    full: Abhishek Sethi
    id: abhishek-sethi
    last: Sethi
  - first: Angeliki
    full: Angeliki Metallinou
    id: angeliki-metallinou
    last: Metallinou
  author_string: Nikolaos Malandrakis, Minmin Shen, Anuj Goyal, Shuyang Gao, Abhishek
    Sethi, Angeliki Metallinou
  bibkey: malandrakis-etal-2019-controlled
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5609
  month: November
  page_first: '90'
  page_last: '98'
  pages: "90\u201398"
  paper_id: '9'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5609.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5609.jpg
  title: Controlled Text Generation for Data Augmentation in Intelligent Artificial
    Agents
  title_html: Controlled Text Generation for Data Augmentation in Intelligent Artificial
    Agents
  url: https://www.aclweb.org/anthology/D19-5609
  year: '2019'
D19-5610:
  abstract: Zero-shot neural machine translation (NMT) is a framework that uses source-pivot
    and target-pivot parallel data to train a source-target NMT system. An extension
    to zero-shot NMT is zero-resource NMT, which generates pseudo-parallel corpora
    using a zero-shot system and further trains the zero-shot system on that data.
    In this paper, we expand on zero-resource NMT by incorporating monolingual data
    in the pivot language into training; since the pivot language is usually the highest-resource
    language of the three, we expect monolingual pivot-language data to be most abundant.
    We propose methods for generating pseudo-parallel corpora using pivot-language
    monolingual data and for leveraging the pseudo-parallel corpora to improve the
    zero-shot NMT system. We evaluate these methods for a high-resource language pair
    (German-Russian) using English as the pivot. We show that our proposed methods
    yield consistent improvements over strong zero-shot and zero-resource baselines
    and even catch up to pivot-based models in BLEU (while not requiring the two-pass
    inference that pivot models require).
  address: Hong Kong
  author:
  - first: Anna
    full: Anna Currey
    id: anna-currey
    last: Currey
  - first: Kenneth
    full: Kenneth Heafield
    id: kenneth-heafield
    last: Heafield
  author_string: Anna Currey, Kenneth Heafield
  bibkey: currey-heafield-2019-zero
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5610
  month: November
  page_first: '99'
  page_last: '107'
  pages: "99\u2013107"
  paper_id: '10'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5610.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5610.jpg
  title: Zero-Resource Neural Machine Translation with Monolingual Pivot Data
  title_html: Zero-Resource Neural Machine Translation with Monolingual Pivot Data
  url: https://www.aclweb.org/anthology/D19-5610
  year: '2019'
D19-5611:
  abstract: Exploiting large pretrained models for various NMT tasks have gained a
    lot of visibility recently. In this work we study how BERT pretrained models could
    be exploited for supervised Neural Machine Translation. We compare various ways
    to integrate pretrained BERT model with NMT model and study the impact of the
    monolingual data used for BERT training on the final translation quality. We use
    WMT-14 English-German, IWSLT15 English-German and IWSLT14 English-Russian datasets
    for these experiments. In addition to standard task test set evaluation, we perform
    evaluation on out-of-domain test sets and noise injected test sets, in order to
    assess how BERT pretrained representations affect model robustness.
  address: Hong Kong
  author:
  - first: Stephane
    full: Stephane Clinchant
    id: stephane-clinchant1
    last: Clinchant
  - first: Kweon Woo
    full: Kweon Woo Jung
    id: kweon-woo-jung
    last: Jung
  - first: Vassilina
    full: Vassilina Nikoulina
    id: vassilina-nikoulina
    last: Nikoulina
  author_string: Stephane Clinchant, Kweon Woo Jung, Vassilina Nikoulina
  bibkey: clinchant-etal-2019-use
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5611
  month: November
  page_first: '108'
  page_last: '117'
  pages: "108\u2013117"
  paper_id: '11'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5611.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5611.jpg
  title: On the use of BERT for Neural Machine Translation
  title_html: On the use of <span class="acl-fixed-case">BERT</span> for Neural Machine
    Translation
  url: https://www.aclweb.org/anthology/D19-5611
  year: '2019'
D19-5612:
  abstract: Variational Autoencoders (VAEs) are known to suffer from learning uninformative
    latent representation of the input due to issues such as approximated posterior
    collapse, or entanglement of the latent space. We impose an explicit constraint
    on the Kullback-Leibler (KL) divergence term inside the VAE objective function.
    While the explicit constraint naturally avoids posterior collapse, we use it to
    further understand the significance of the KL term in controlling the information
    transmitted through the VAE channel. Within this framework, we explore different
    properties of the estimated posterior distribution, and highlight the trade-off
    between the amount of information encoded in a latent code during training, and
    the generative capacity of the model.
  address: Hong Kong
  author:
  - first: Victor
    full: Victor Prokhorov
    id: victor-prokhorov
    last: Prokhorov
  - first: Ehsan
    full: Ehsan Shareghi
    id: ehsan-shareghi
    last: Shareghi
  - first: Yingzhen
    full: Yingzhen Li
    id: yingzhen-li
    last: Li
  - first: Mohammad Taher
    full: Mohammad Taher Pilehvar
    id: mohammad-taher-pilehvar
    last: Pilehvar
  - first: Nigel
    full: Nigel Collier
    id: nigel-collier
    last: Collier
  author_string: Victor Prokhorov, Ehsan Shareghi, Yingzhen Li, Mohammad Taher Pilehvar,
    Nigel Collier
  bibkey: prokhorov-etal-2019-importance
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5612
  month: November
  page_first: '118'
  page_last: '127'
  pages: "118\u2013127"
  paper_id: '12'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5612.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5612.jpg
  title: On the Importance of the Kullback-Leibler Divergence Term in Variational
    Autoencoders for Text Generation
  title_html: On the Importance of the <span class="acl-fixed-case">K</span>ullback-<span
    class="acl-fixed-case">L</span>eibler Divergence Term in Variational Autoencoders
    for Text Generation
  url: https://www.aclweb.org/anthology/D19-5612
  year: '2019'
D19-5613:
  abstract: This paper focuses on latent representations that could effectively decompose
    different aspects of textual information. Using a framework of style transfer
    for texts, we propose several empirical methods to assess information decomposition
    quality. We validate these methods with several state-of-the-art textual style
    transfer methods. Higher quality of information decomposition corresponds to higher
    performance in terms of bilingual evaluation understudy (BLEU) between output
    and human-written reformulations.
  address: Hong Kong
  author:
  - first: Ivan P.
    full: Ivan P. Yamshchikov
    id: ivan-p-yamshchikov
    last: Yamshchikov
  - first: Viacheslav
    full: Viacheslav Shibaev
    id: viacheslav-shibaev
    last: Shibaev
  - first: Aleksander
    full: Aleksander Nagaev
    id: aleksander-nagaev
    last: Nagaev
  - first: "J\xFCrgen"
    full: "J\xFCrgen Jost"
    id: jurgen-jost
    last: Jost
  - first: Alexey
    full: Alexey Tikhonov
    id: alexey-tikhonov
    last: Tikhonov
  author_string: "Ivan P. Yamshchikov, Viacheslav Shibaev, Aleksander Nagaev, J\xFC\
    rgen Jost, Alexey Tikhonov"
  bibkey: yamshchikov-etal-2019-decomposing
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5613
  month: November
  page_first: '128'
  page_last: '137'
  pages: "128\u2013137"
  paper_id: '13'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5613.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5613.jpg
  title: Decomposing Textual Information For Style Transfer
  title_html: Decomposing Textual Information For Style Transfer
  url: https://www.aclweb.org/anthology/D19-5613
  year: '2019'
D19-5614:
  abstract: We consider the problem of automatically generating textual paraphrases
    with modified attributes or properties, focusing on the setting without parallel
    data (Hu et al., 2017; Shen et al., 2017). This setting poses challenges for evaluation.
    We show that the metric of post-transfer classification accuracy is insufficient
    on its own, and propose additional metrics based on semantic preservation and
    fluency as well as a way to combine them into a single overall score. We contribute
    new loss functions and training strategies to address the different metrics. Semantic
    preservation is addressed by adding a cyclic consistency loss and a loss based
    on paraphrase pairs, while fluency is improved by integrating losses based on
    style-specific language models. We experiment with a Yelp sentiment dataset and
    a new literature dataset that we propose, using multiple models that extend prior
    work (Shen et al., 2017). We demonstrate that our metrics correlate well with
    human judgments, at both the sentence-level and system-level. Automatic and manual
    evaluation also show large improvements over the baseline method of Shen et al.
    (2017). We hope that our proposed metrics can speed up system development for
    new textual transfer tasks while also encouraging the community to address our
    three complementary aspects of transfer quality.
  address: Hong Kong
  attachment:
  - filename: D19-5614.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5614.Attachment.pdf
  author:
  - first: Richard Yuanzhe
    full: Richard Yuanzhe Pang
    id: richard-yuanzhe-pang
    last: Pang
  - first: Kevin
    full: Kevin Gimpel
    id: kevin-gimpel
    last: Gimpel
  author_string: Richard Yuanzhe Pang, Kevin Gimpel
  bibkey: pang-gimpel-2019-unsupervised
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5614
  month: November
  page_first: '138'
  page_last: '147'
  pages: "138\u2013147"
  paper_id: '14'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5614.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5614.jpg
  title: Unsupervised Evaluation Metrics and Learning Criteria for Non-Parallel Textual
    Transfer
  title_html: Unsupervised Evaluation Metrics and Learning Criteria for Non-Parallel
    Textual Transfer
  url: https://www.aclweb.org/anthology/D19-5614
  year: '2019'
D19-5615:
  abstract: 'Neural models have recently shown significant progress on data-to-text
    generation tasks in which descriptive texts are generated conditioned on database
    records. In this work, we present a new Transformer-based data-to-text generation
    model which learns content selection and summary generation in an end-to-end fashion.
    We introduce two extensions to the baseline transformer model: First, we modify
    the latent representation of the input, which helps to significantly improve the
    content correctness of the output summary; Second, we include an additional learning
    objective that accounts for content selection modelling. In addition, we propose
    two data augmentation methods that succeed to further improve performance of the
    resulting generation models. Evaluation experiments show that our final model
    outperforms current state-of-the-art systems as measured by different metrics:
    BLEU, content selection precision and content ordering. We made publicly available
    the transformer extension presented in this paper.'
  address: Hong Kong
  author:
  - first: Li
    full: Li Gong
    id: li-gong
    last: Gong
  - first: Josep
    full: Josep Crego
    id: josep-m-crego
    last: Crego
  - first: Jean
    full: Jean Senellart
    id: jean-senellart
    last: Senellart
  author_string: Li Gong, Josep Crego, Jean Senellart
  bibkey: gong-etal-2019-enhanced
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5615
  month: November
  page_first: '148'
  page_last: '156'
  pages: "148\u2013156"
  paper_id: '15'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5615.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5615.jpg
  title: Enhanced Transformer Model for Data-to-Text Generation
  title_html: Enhanced Transformer Model for Data-to-Text Generation
  url: https://www.aclweb.org/anthology/D19-5615
  year: '2019'
D19-5616:
  abstract: "Exposure bias refers to the train-test discrepancy that seemingly arises\
    \ when an autoregressive generative model uses only ground-truth contexts at training\
    \ time but generated ones at test time. We separate the contribution of the learning\
    \ framework and the model to clarify the debate on consequences and review proposed\
    \ counter-measures. In this light, we argue that generalization is the underlying\
    \ property to address and propose unconditional generation as its fundamental\
    \ benchmark. Finally, we combine latent variable modeling with a recent formulation\
    \ of exploration in reinforcement learning to obtain a rigorous handling of true\
    \ and generated contexts. Results on language modeling and variational sentence\
    \ auto-encoding confirm the model\u2019s generalization capability."
  address: Hong Kong
  attachment:
  - filename: D19-5616.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5616.Attachment.pdf
  author:
  - first: Florian
    full: Florian Schmidt
    id: florian-schmidt
    last: Schmidt
  author_string: Florian Schmidt
  bibkey: schmidt-2019-generalization
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5616
  month: November
  page_first: '157'
  page_last: '167'
  pages: "157\u2013167"
  paper_id: '16'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5616.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5616.jpg
  title: 'Generalization in Generation: A closer look at Exposure Bias'
  title_html: 'Generalization in Generation: A closer look at Exposure Bias'
  url: https://www.aclweb.org/anthology/D19-5616
  year: '2019'
D19-5617:
  abstract: We share a French-English parallel corpus of Foursquare restaurant reviews,
    and define a new task to encourage research on Neural Machine Translation robustness
    and domain adaptation, in a real-world scenario where better-quality MT would
    be greatly beneficial. We discuss the challenges of such user-generated content,
    and train good baseline models that build upon the latest techniques for MT robustness.
    We also perform an extensive evaluation (automatic and human) that shows significant
    improvements over existing online systems. Finally, we propose task-specific metrics
    based on sentiment analysis or translation accuracy of domain-specific polysemous
    words.
  address: Hong Kong
  attachment:
  - filename: D19-5617.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5617.Attachment.zip
  author:
  - first: Alexandre
    full: Alexandre Berard
    id: alexandre-berard1
    last: Berard
  - first: Ioan
    full: Ioan Calapodescu
    id: ioan-calapodescu
    last: Calapodescu
  - first: Marc
    full: Marc Dymetman
    id: marc-dymetman
    last: Dymetman
  - first: Claude
    full: Claude Roux
    id: claude-roux
    last: Roux
  - first: Jean-Luc
    full: Jean-Luc Meunier
    id: jean-luc-meunier
    last: Meunier
  - first: Vassilina
    full: Vassilina Nikoulina
    id: vassilina-nikoulina
    last: Nikoulina
  author_string: Alexandre Berard, Ioan Calapodescu, Marc Dymetman, Claude Roux, Jean-Luc
    Meunier, Vassilina Nikoulina
  bibkey: berard-etal-2019-machine
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5617
  month: November
  page_first: '168'
  page_last: '176'
  pages: "168\u2013176"
  paper_id: '17'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5617.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5617.jpg
  title: 'Machine Translation of Restaurant Reviews: New Corpus for Domain Adaptation
    and Robustness'
  title_html: 'Machine Translation of Restaurant Reviews: New Corpus for Domain Adaptation
    and Robustness'
  url: https://www.aclweb.org/anthology/D19-5617
  year: '2019'
D19-5618:
  abstract: 'Neural Machine Translation (NMT), a data-hungry technology, suffers from
    the lack of bilingual data in low-resource scenarios. Multitask learning (MTL)
    can alleviate this issue by injecting inductive biases into NMT, using auxiliary
    syntactic and semantic tasks. However, an effective training schedule is required
    to balance the importance of tasks to get the best use of the training signal.
    The role of training schedule becomes even more crucial in biased-MTL where the
    goal is to improve one (or a subset) of tasks the most, e.g. translation quality.
    Current approaches for biased-MTL are based on brittle hand-engineered heuristics
    that require trial and error, and should be (re-)designed for each learning scenario.
    To the best of our knowledge, ours is the first work on adaptively and dynamically
    changing the training schedule in biased-MTL. We propose a rigorous approach for
    automatically reweighing the training data of the main and auxiliary tasks throughout
    the training process based on their contributions to the generalisability of the
    main NMT task. Our experiments on translating from English to Vietnamese/Turkish/Spanish
    show improvements of up to +1.2 BLEU points, compared to strong baselines. Additionally,
    our analyses shed light on the dynamic of needs throughout the training of NMT:
    from syntax to semantic.'
  address: Hong Kong
  author:
  - first: Poorya
    full: Poorya Zaremoodi
    id: poorya-zaremoodi
    last: Zaremoodi
  - first: Gholamreza
    full: Gholamreza Haffari
    id: gholamreza-haffari
    last: Haffari
  author_string: Poorya Zaremoodi, Gholamreza Haffari
  bibkey: zaremoodi-haffari-2019-adaptively
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5618
  month: November
  page_first: '177'
  page_last: '186'
  pages: "177\u2013186"
  paper_id: '18'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5618.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5618.jpg
  title: 'Adaptively Scheduled Multitask Learning: The Case of Low-Resource Neural
    Machine Translation'
  title_html: 'Adaptively Scheduled Multitask Learning: The Case of Low-Resource Neural
    Machine Translation'
  url: https://www.aclweb.org/anthology/D19-5618
  year: '2019'
D19-5619:
  abstract: Neural Machine Translation (NMT) models generally perform translation
    using a fixed-size lexical vocabulary, which is an important bottleneck on their
    generalization capability and overall translation quality. The standard approach
    to overcome this limitation is to segment words into subword units, typically
    using some external tools with arbitrary heuristics, resulting in vocabulary units
    not optimized for the translation task. Recent studies have shown that the same
    approach can be extended to perform NMT directly at the level of characters, which
    can deliver translation accuracy on-par with subword-based models, on the other
    hand, this requires relatively deeper networks. In this paper, we propose a more
    computationally-efficient solution for character-level NMT which implements a
    hierarchical decoding architecture where translations are subsequently generated
    at the level of words and characters. We evaluate different methods for open-vocabulary
    NMT in the machine translation task from English into five languages with distinct
    morphological typology, and show that the hierarchical decoding model can reach
    higher translation accuracy than the subword-level NMT model using significantly
    fewer parameters, while demonstrating better capacity in learning longer-distance
    contextual and grammatical dependencies than the standard character-level NMT
    model.
  address: Hong Kong
  attachment:
  - filename: D19-5619.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5619.Attachment.zip
  author:
  - first: Duygu
    full: Duygu Ataman
    id: duygu-ataman
    last: Ataman
  - first: Orhan
    full: Orhan Firat
    id: orhan-firat
    last: Firat
  - first: Mattia A.
    full: Mattia A. Di Gangi
    id: mattia-a-di-gangi
    last: Di Gangi
  - first: Marcello
    full: Marcello Federico
    id: marcello-federico
    last: Federico
  - first: Alexandra
    full: Alexandra Birch
    id: alexandra-birch
    last: Birch
  author_string: Duygu Ataman, Orhan Firat, Mattia A. Di Gangi, Marcello Federico,
    Alexandra Birch
  bibkey: ataman-etal-2019-importance
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5619
  month: November
  page_first: '187'
  page_last: '193'
  pages: "187\u2013193"
  paper_id: '19'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5619.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5619.jpg
  title: On the Importance of Word Boundaries in Character-level Neural Machine Translation
  title_html: On the Importance of Word Boundaries in Character-level Neural Machine
    Translation
  url: https://www.aclweb.org/anthology/D19-5619
  year: '2019'
D19-5620:
  abstract: "The Insertion Transformer is well suited for long form text generation\
    \ due to its parallel generation capabilities, requiring O(log2 n) generation\
    \ steps to generate generation steps to generate n tokens. However, modeling long\
    \ sequences is difficult, as there is more ambiguity captured in the attention\
    \ mechanism. This work proposes the Big Bidirectional Insertion Representations\
    \ for Documents (Big BIRD), an insertion-based model for document-level translation\
    \ tasks. We scale up the insertion-based models to long form documents. Our key\
    \ contribution is introducing sentence alignment via sentence-positional embeddings\
    \ between the source and target document. We show an improvement of +4.3 BLEU\
    \ on the WMT\u201919 English->German document-level translation task compared\
    \ with the Insertion Transformer baseline. tokens. However, modeling long sequences\
    \ is difficult, as there is more ambiguity captured in the attention mechanism.\
    \ This work proposes the Big Bidirectional Insertion Representations for Documents\
    \ (Big BIRD), an insertion-based model for document-level translation tasks. We\
    \ scale up the insertion-based models to long form documents. Our key contribution\
    \ is introducing sentence alignment via sentence-positional embeddings between\
    \ the source and target document. We show an improvement of +4.3 BLEU on the WMT\u2019\
    19 English->German document-level translation task compared with the Insertion\
    \ Transformer baseline."
  address: Hong Kong
  author:
  - first: Lala
    full: Lala Li
    id: lala-li
    last: Li
  - first: William
    full: William Chan
    id: william-chan
    last: Chan
  author_string: Lala Li, William Chan
  bibkey: li-chan-2019-big
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5620
  month: November
  page_first: '194'
  page_last: '198'
  pages: "194\u2013198"
  paper_id: '20'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5620.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5620.jpg
  title: Big Bidirectional Insertion Representations for Documents
  title_html: Big Bidirectional Insertion Representations for Documents
  url: https://www.aclweb.org/anthology/D19-5620
  year: '2019'
D19-5621:
  abstract: Neural models that eliminate the softmax bottleneck by generating word
    embeddings (rather than multinomial distributions over a vocabulary) attain faster
    training with fewer learnable parameters. These models are currently trained by
    maximizing densities of pretrained target embeddings under von Mises-Fisher distributions
    parameterized by corresponding model-predicted embeddings. This work explores
    the utility of margin-based loss functions in optimizing such models. We present
    syn-margin loss, a novel margin-based loss that uses a synthetic negative sample
    constructed from only the predicted and target embeddings at every step. The loss
    is efficient to compute, and we use a geometric analysis to argue that it is more
    consistent and interpretable than other margin-based losses. Empirically, we find
    that syn-margin provides small but significant improvements over both vMF and
    standard margin-based losses in continuous-output neural machine translation.
  address: Hong Kong
  author:
  - first: Gayatri
    full: Gayatri Bhat
    id: gayatri-bhat
    last: Bhat
  - first: Sachin
    full: Sachin Kumar
    id: sachin-kumar
    last: Kumar
  - first: Yulia
    full: Yulia Tsvetkov
    id: yulia-tsvetkov
    last: Tsvetkov
  author_string: Gayatri Bhat, Sachin Kumar, Yulia Tsvetkov
  bibkey: bhat-etal-2019-margin
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5621
  month: November
  page_first: '199'
  page_last: '205'
  pages: "199\u2013205"
  paper_id: '21'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5621.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5621.jpg
  title: A Margin-based Loss with Synthetic Negative Samples for Continuous-output
    Machine Translation
  title_html: A Margin-based Loss with Synthetic Negative Samples for Continuous-output
    Machine Translation
  url: https://www.aclweb.org/anthology/D19-5621
  year: '2019'
D19-5622:
  abstract: "Recently, the Transformer becomes a state-of-the-art architecture in\
    \ the filed of neural machine translation (NMT). A key point of its high-performance\
    \ is the multi-head self-attention which is supposed to allow the model to independently\
    \ attend to information from different representation subspaces. However, there\
    \ is no explicit mechanism to ensure that different attention heads indeed capture\
    \ different features, and in practice, redundancy has occurred in multiple heads.\
    \ In this paper, we argue that using the same global attention in multiple heads\
    \ limits multi-head self-attention\u2019s capacity for learning distinct features.\
    \ In order to improve the expressiveness of multi-head self-attention, we propose\
    \ a novel Mixed Multi-Head Self-Attention (MMA) which models not only global and\
    \ local attention but also forward and backward attention in different attention\
    \ heads. This enables the model to learn distinct representations explicitly among\
    \ multiple heads. In our experiments on both WAT17 English-Japanese as well as\
    \ IWSLT14 German-English translation task, we show that, without increasing the\
    \ number of parameters, our models yield consistent and significant improvements\
    \ (0.9 BLEU scores on average) over the strong Transformer baseline."
  address: Hong Kong
  author:
  - first: Hongyi
    full: Hongyi Cui
    id: hongyi-cui
    last: Cui
  - first: Shohei
    full: Shohei Iida
    id: shohei-iida
    last: Iida
  - first: Po-Hsuan
    full: Po-Hsuan Hung
    id: po-hsuan-hung
    last: Hung
  - first: Takehito
    full: Takehito Utsuro
    id: takehito-utsuro
    last: Utsuro
  - first: Masaaki
    full: Masaaki Nagata
    id: masaaki-nagata
    last: Nagata
  author_string: Hongyi Cui, Shohei Iida, Po-Hsuan Hung, Takehito Utsuro, Masaaki
    Nagata
  bibkey: cui-etal-2019-mixed
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5622
  month: November
  page_first: '206'
  page_last: '214'
  pages: "206\u2013214"
  paper_id: '22'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5622.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5622.jpg
  title: Mixed Multi-Head Self-Attention for Neural Machine Translation
  title_html: Mixed Multi-Head Self-Attention for Neural Machine Translation
  url: https://www.aclweb.org/anthology/D19-5622
  year: '2019'
D19-5623:
  abstract: Recently, large language models such as GPT-2 have shown themselves to
    be extremely adept at text generation and have also been able to achieve high-quality
    results in many downstream NLP tasks such as text classification, sentiment analysis
    and question answering with the aid of fine-tuning. We present a useful technique
    for using a large language model to perform the task of paraphrasing on a variety
    of texts and subjects. Our approach is demonstrated to be capable of generating
    paraphrases not only at a sentence level but also for longer spans of text such
    as paragraphs without needing to break the text into smaller chunks.
  address: Hong Kong
  attachment:
  - filename: D19-5623.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5623.Attachment.pdf
  author:
  - first: Sam
    full: Sam Witteveen
    id: sam-witteveen
    last: Witteveen
  - first: Martin
    full: Martin Andrews
    id: martin-andrews
    last: Andrews
  author_string: Sam Witteveen, Martin Andrews
  bibkey: witteveen-andrews-2019-paraphrasing
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5623
  month: November
  page_first: '215'
  page_last: '220'
  pages: "215\u2013220"
  paper_id: '23'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5623.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5623.jpg
  title: Paraphrasing with Large Language Models
  title_html: Paraphrasing with Large Language Models
  url: https://www.aclweb.org/anthology/D19-5623
  year: '2019'
D19-5624:
  abstract: "Attention models have become a crucial component in neural machine translation\
    \ (NMT). They are often implicitly or explicitly used to justify the model\u2019\
    s decision in generating a specific token but it has not yet been rigorously established\
    \ to what extent attention is a reliable source of information in NMT. To evaluate\
    \ the explanatory power of attention for NMT, we examine the possibility of yielding\
    \ the same prediction but with counterfactual attention models that modify crucial\
    \ aspects of the trained attention model. Using these counterfactual attention\
    \ mechanisms we assess the extent to which they still preserve the generation\
    \ of function and content words in the translation process. Compared to a state\
    \ of the art attention model, our counterfactual attention models produce 68%\
    \ of function words and 21% of content words in our German-English dataset. Our\
    \ experiments demonstrate that attention models by themselves cannot reliably\
    \ explain the decisions made by a NMT model."
  address: Hong Kong
  author:
  - first: Pooya
    full: Pooya Moradi
    id: pooya-moradi
    last: Moradi
  - first: Nishant
    full: Nishant Kambhatla
    id: nishant-kambhatla
    last: Kambhatla
  - first: Anoop
    full: Anoop Sarkar
    id: anoop-sarkar
    last: Sarkar
  author_string: Pooya Moradi, Nishant Kambhatla, Anoop Sarkar
  bibkey: moradi-etal-2019-interrogating
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5624
  month: November
  page_first: '221'
  page_last: '230'
  pages: "221\u2013230"
  paper_id: '24'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5624.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5624.jpg
  title: Interrogating the Explanatory Power of Attention in Neural Machine Translation
  title_html: Interrogating the Explanatory Power of Attention in Neural Machine Translation
  url: https://www.aclweb.org/anthology/D19-5624
  year: '2019'
D19-5625:
  abstract: Neural sequence-to-sequence models, particularly the Transformer, are
    the state of the art in machine translation. Yet these neural networks are very
    sensitive to architecture and hyperparameter settings. Optimizing these settings
    by grid or random search is computationally expensive because it requires many
    training runs. In this paper, we incorporate architecture search into a single
    training run through auto-sizing, which uses regularization to delete neurons
    in a network over the course of training. On very low-resource language pairs,
    we show that auto-sizing can improve BLEU scores by up to 3.9 points while removing
    one-third of the parameters from the model.
  address: Hong Kong
  author:
  - first: Kenton
    full: Kenton Murray
    id: kenton-murray
    last: Murray
  - first: Jeffery
    full: Jeffery Kinnison
    id: jeffery-kinnison
    last: Kinnison
  - first: Toan Q.
    full: Toan Q. Nguyen
    id: toan-q-nguyen
    last: Nguyen
  - first: Walter
    full: Walter Scheirer
    id: walter-scheirer
    last: Scheirer
  - first: David
    full: David Chiang
    id: david-chiang
    last: Chiang
  author_string: Kenton Murray, Jeffery Kinnison, Toan Q. Nguyen, Walter Scheirer,
    David Chiang
  bibkey: murray-etal-2019-auto
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5625
  month: November
  page_first: '231'
  page_last: '240'
  pages: "231\u2013240"
  paper_id: '25'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5625.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5625.jpg
  title: 'Auto-Sizing the Transformer Network: Improving Speed, Efficiency, and Performance
    for Low-Resource Machine Translation'
  title_html: 'Auto-Sizing the Transformer Network: Improving Speed, Efficiency, and
    Performance for Low-Resource Machine Translation'
  url: https://www.aclweb.org/anthology/D19-5625
  year: '2019'
D19-5626:
  abstract: Neural machine translation (NMT) often fails in one-to-many translation,
    e.g., in the translation of multi-word expressions, compounds, and collocations.
    To improve the translation of phrases, phrase-based NMT systems have been proposed;
    these typically combine word-based NMT with external phrase dictionaries or with
    phrase tables from phrase-based statistical MT systems. These solutions introduce
    a significant overhead of additional resources and computational costs. In this
    paper, we introduce a phrase-based NMT model built upon continuous-output NMT,
    in which the decoder generates embeddings of words or phrases. The model uses
    a fertility module, which guides the decoder to generate embeddings of sequences
    of varying lengths. We show that our model learns to translate phrases better,
    performing on par with state of the art phrase-based NMT. Since our model does
    not resort to softmax computation over a huge vocabulary of phrases, its training
    time is about 112x faster than the baseline.
  address: Hong Kong
  author:
  - first: Chan Young
    full: Chan Young Park
    id: chan-young-park
    last: Park
  - first: Yulia
    full: Yulia Tsvetkov
    id: yulia-tsvetkov
    last: Tsvetkov
  author_string: Chan Young Park, Yulia Tsvetkov
  bibkey: park-tsvetkov-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5626
  month: November
  page_first: '241'
  page_last: '248'
  pages: "241\u2013248"
  paper_id: '26'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5626.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5626.jpg
  title: Learning to Generate Word- and Phrase-Embeddings for Efficient Phrase-Based
    Neural Machine Translation
  title_html: Learning to Generate Word- and Phrase-Embeddings for Efficient Phrase-Based
    Neural Machine Translation
  url: https://www.aclweb.org/anthology/D19-5626
  year: '2019'
D19-5627:
  abstract: "Paraphrase generation aims to improve the clarity of a sentence by using\
    \ different wording that convey similar meaning. For better quality of generated\
    \ paraphrases, we propose a framework that combines the effectiveness of two models\
    \ \u2013 transformer and sequence-to-sequence (seq2seq). We design a two-layer\
    \ stack of encoders. The first layer is a transformer model containing 6 stacked\
    \ identical layers with multi-head self attention, while the second-layer is a\
    \ seq2seq model with gated recurrent units (GRU-RNN). The transformer encoder\
    \ layer learns to capture long-term dependencies, together with syntactic and\
    \ semantic properties of the input sentence. This rich vector representation learned\
    \ by the transformer serves as input to the GRU-RNN encoder responsible for producing\
    \ the state vector for decoding. Experimental results on two datasets-QUORA and\
    \ MSCOCO using our framework, produces a new benchmark for paraphrase generation."
  address: Hong Kong
  author:
  - first: Elozino
    full: Elozino Egonmwan
    id: elozino-egonmwan
    last: Egonmwan
  - first: Yllias
    full: Yllias Chali
    id: yllias-chali
    last: Chali
  author_string: Elozino Egonmwan, Yllias Chali
  bibkey: egonmwan-chali-2019-transformer-seq2seq
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5627
  month: November
  page_first: '249'
  page_last: '255'
  pages: "249\u2013255"
  paper_id: '27'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5627.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5627.jpg
  title: Transformer and seq2seq model for Paraphrase Generation
  title_html: Transformer and seq2seq model for Paraphrase Generation
  url: https://www.aclweb.org/anthology/D19-5627
  year: '2019'
D19-5628:
  abstract: We describe the work of Monash University for the shared task of Rotowire
    document translation organised by the 3rd Workshop on Neural Generation and Translation
    (WNGT 2019). We submitted systems for both directions of the English-German language
    pair. Our main focus is on employing an established document-level neural machine
    translation model for this task. We achieve a BLEU score of 39.83 (41.46 BLEU
    per WNGT evaluation) for En-De and 45.06 (47.39 BLEU per WNGT evaluation) for
    De-En translation directions on the Rotowire test set. All experiments conducted
    in the process are also described.
  address: Hong Kong
  author:
  - first: Sameen
    full: Sameen Maruf
    id: sameen-maruf
    last: Maruf
  - first: Gholamreza
    full: Gholamreza Haffari
    id: gholamreza-haffari
    last: Haffari
  author_string: Sameen Maruf, Gholamreza Haffari
  bibkey: maruf-haffari-2019-monash
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5628
  month: November
  page_first: '256'
  page_last: '261'
  pages: "256\u2013261"
  paper_id: '28'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5628.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5628.jpg
  title: "Monash University\u2019s Submissions to the WNGT 2019 Document Translation\
    \ Task"
  title_html: "Monash University\u2019s Submissions to the <span class=\"acl-fixed-case\"\
    >WNGT</span> 2019 Document Translation Task"
  url: https://www.aclweb.org/anthology/D19-5628
  year: '2019'
D19-5629:
  abstract: This paper describes SYSTRAN participation to the Document-level Generation
    and Trans- lation (DGT) Shared Task of the 3rd Workshop on Neural Generation and
    Translation (WNGT 2019). We participate for the first time using a Transformer
    network enhanced with modified input embeddings and optimising an additional objective
    function that considers content selection. The network takes in structured data
    of basketball games and outputs a summary of the game in natural language.
  address: Hong Kong
  author:
  - first: Li
    full: Li Gong
    id: li-gong
    last: Gong
  - first: Josep
    full: Josep Crego
    id: josep-m-crego
    last: Crego
  - first: Jean
    full: Jean Senellart
    id: jean-senellart
    last: Senellart
  author_string: Li Gong, Josep Crego, Jean Senellart
  bibkey: gong-etal-2019-systran
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5629
  month: November
  page_first: '262'
  page_last: '267'
  pages: "262\u2013267"
  paper_id: '29'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5629.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5629.jpg
  title: 'SYSTRAN @ WNGT 2019: DGT Task'
  title_html: 'SYSTRAN @ WNGT 2019: DGT Task'
  url: https://www.aclweb.org/anthology/D19-5629
  year: '2019'
D19-5630:
  abstract: 'The University of Edinburgh participated in all six tracks: NLG, MT,
    and MT+NLG with both English and German as targeted languages. For the NLG track,
    we submitted a multilingual system based on the Content Selection and Planning
    model of Puduppully et al (2019). For the MT track, we submitted Transformer-based
    Neural Machine Translation models, where out-of-domain parallel data was augmented
    with in-domain data extracted from monolingual corpora. Our MT+NLG systems disregard
    the structured input data and instead rely exclusively on the source summaries.'
  address: Hong Kong
  author:
  - first: Ratish
    full: Ratish Puduppully
    id: ratish-puduppully
    last: Puduppully
  - first: Jonathan
    full: Jonathan Mallinson
    id: jonathan-mallinson
    last: Mallinson
  - first: Mirella
    full: Mirella Lapata
    id: mirella-lapata
    last: Lapata
  author_string: Ratish Puduppully, Jonathan Mallinson, Mirella Lapata
  bibkey: puduppully-etal-2019-university
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5630
  month: November
  page_first: '268'
  page_last: '272'
  pages: "268\u2013272"
  paper_id: '30'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5630.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5630.jpg
  title: "University of Edinburgh\u2019s submission to the Document-level Generation\
    \ and Translation Shared Task"
  title_html: "University of <span class=\"acl-fixed-case\">E</span>dinburgh\u2019\
    s submission to the Document-level Generation and Translation Shared Task"
  url: https://www.aclweb.org/anthology/D19-5630
  year: '2019'
D19-5631:
  abstract: "Recently, neural models led to significant improvements in both machine\
    \ translation (MT) and natural language generation tasks (NLG). However, generation\
    \ of long descriptive summaries conditioned on structured data remains an open\
    \ challenge. Likewise, MT that goes beyond sentence-level context is still an\
    \ open issue (e.g., document-level MT or MT with metadata). To address these challenges,\
    \ we propose to leverage data from both tasks and do transfer learning between\
    \ MT, NLG, and MT with source-side metadata (MT+NLG). First, we train document-based\
    \ MT systems with large amounts of parallel data. Then, we adapt these models\
    \ to pure NLG and MT+NLG tasks by fine-tuning with smaller amounts of domain-specific\
    \ data. This end-to-end NLG approach, without data selection and planning, outperforms\
    \ the previous state of the art on the Rotowire NLG task. We participated to the\
    \ \u201CDocument Generation and Translation\u201D task at WNGT 2019, and ranked\
    \ first in all tracks."
  address: Hong Kong
  author:
  - first: Fahimeh
    full: Fahimeh Saleh
    id: fahimeh-saleh
    last: Saleh
  - first: Alexandre
    full: Alexandre Berard
    id: alexandre-berard1
    last: Berard
  - first: Ioan
    full: Ioan Calapodescu
    id: ioan-calapodescu
    last: Calapodescu
  - first: Laurent
    full: Laurent Besacier
    id: laurent-besacier
    last: Besacier
  author_string: Fahimeh Saleh, Alexandre Berard, Ioan Calapodescu, Laurent Besacier
  bibkey: saleh-etal-2019-naver
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5631
  month: November
  page_first: '273'
  page_last: '279'
  pages: "273\u2013279"
  paper_id: '31'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5631.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5631.jpg
  title: "Naver Labs Europe\u2019s Systems for the Document-Level Generation and Translation\
    \ Task at WNGT 2019"
  title_html: "Naver Labs <span class=\"acl-fixed-case\">E</span>urope\u2019s Systems\
    \ for the Document-Level Generation and Translation Task at <span class=\"acl-fixed-case\"\
    >WNGT</span> 2019"
  url: https://www.aclweb.org/anthology/D19-5631
  year: '2019'
D19-5632:
  abstract: "This paper describes the submissions of the \u201CMarian\u201D team to\
    \ the WNGT 2019 efficiency shared task. Taking our dominating submissions to the\
    \ previous edition of the shared task as a starting point, we develop improved\
    \ teacher-student training via multi-agent dual-learning and noisy backward-forward\
    \ translation for Transformer-based student models. For efficient CPU-based decoding,\
    \ we propose pre-packed 8-bit matrix products, improved batched decoding, cache-friendly\
    \ student architectures with parameter sharing and light-weight RNN-based decoder\
    \ architectures. GPU-based decoding benefits from the same architecture changes,\
    \ from pervasive 16-bit inference and concurrent streams. These modifications\
    \ together with profiler-based C++ code optimization allow us to push the Pareto\
    \ frontier established during the 2018 edition towards 24x (CPU) and 14x (GPU)\
    \ faster models at comparable or higher BLEU values. Our fastest CPU model is\
    \ more than 4x faster than last year\u2019s fastest submission at more than 3\
    \ points higher BLEU. Our fastest GPU model at 1.5 seconds translation time is\
    \ slightly faster than last year\u2019s fastest RNN-based submissions, but outperforms\
    \ them by more than 4 BLEU and 10 BLEU points respectively."
  address: Hong Kong
  author:
  - first: Young Jin
    full: Young Jin Kim
    id: young-jin-kim
    last: Kim
  - first: Marcin
    full: Marcin Junczys-Dowmunt
    id: marcin-junczys-dowmunt
    last: Junczys-Dowmunt
  - first: Hany
    full: Hany Hassan
    id: hany-hassan
    last: Hassan
  - first: Alham
    full: Alham Fikri Aji
    id: alham-fikri-aji1
    last: Fikri Aji
  - first: Kenneth
    full: Kenneth Heafield
    id: kenneth-heafield
    last: Heafield
  - first: Roman
    full: Roman Grundkiewicz
    id: roman-grundkiewicz
    last: Grundkiewicz
  - first: Nikolay
    full: Nikolay Bogoychev
    id: nikolay-bogoychev
    last: Bogoychev
  author_string: Young Jin Kim, Marcin Junczys-Dowmunt, Hany Hassan, Alham Fikri Aji,
    Kenneth Heafield, Roman Grundkiewicz, Nikolay Bogoychev
  bibkey: kim-etal-2019-research
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5632
  month: November
  page_first: '280'
  page_last: '288'
  pages: "280\u2013288"
  paper_id: '32'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5632.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5632.jpg
  title: 'From Research to Production and Back: Ludicrously Fast Neural Machine Translation'
  title_html: 'From Research to Production and Back: Ludicrously Fast Neural Machine
    Translation'
  url: https://www.aclweb.org/anthology/D19-5632
  year: '2019'
D19-5633:
  abstract: 'In this paper, we report our system submissions to all 6 tracks of the
    WNGT 2019 shared task on Document-Level Generation and Translation. The objective
    is to generate a textual document from either structured data: generation task,
    or a document in a different language: translation task. For the translation task,
    we focused on adapting a large scale system trained on WMT data by fine tuning
    it on the RotoWire data. For the generation task, we participated with two systems
    based on a selection and planning model followed by (a) a simple language model
    generation, and (b) a GPT-2 pre-trained language model approach. The selection
    and planning module chooses a subset of table records in order, and the language
    models produce text given such a subset.'
  address: Hong Kong
  author:
  - first: Lesly
    full: Lesly Miculicich
    id: lesly-miculicich-werlen
    last: Miculicich
  - first: Marc
    full: Marc Marone
    id: marc-marone
    last: Marone
  - first: Hany
    full: Hany Hassan
    id: hany-hassan
    last: Hassan
  author_string: Lesly Miculicich, Marc Marone, Hany Hassan
  bibkey: miculicich-etal-2019-selecting
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5633
  month: November
  page_first: '289'
  page_last: '296'
  pages: "289\u2013296"
  paper_id: '33'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5633.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5633.jpg
  title: 'Selecting, Planning, and Rewriting: A Modular Approach for Data-to-Document
    Generation and Translation'
  title_html: 'Selecting, Planning, and Rewriting: A Modular Approach for Data-to-Document
    Generation and Translation'
  url: https://www.aclweb.org/anthology/D19-5633
  year: '2019'
D19-5634:
  abstract: "This paper describes the Notre Dame Natural Language Processing Group\u2019\
    s (NDNLP) submission to the WNGT 2019 shared task (Hayashi et al., 2019). We investigated\
    \ the impact of auto-sizing (Murray and Chiang, 2015; Murray et al., 2019) to\
    \ the Transformer network (Vaswani et al., 2017) with the goal of substantially\
    \ reducing the number of parameters in the model. Our method was able to eliminate\
    \ more than 25% of the model\u2019s parameters while suffering a decrease of only\
    \ 1.1 BLEU."
  address: Hong Kong
  author:
  - first: Kenton
    full: Kenton Murray
    id: kenton-murray
    last: Murray
  - first: Brian
    full: Brian DuSell
    id: brian-dusell
    last: DuSell
  - first: David
    full: David Chiang
    id: david-chiang
    last: Chiang
  author_string: Kenton Murray, Brian DuSell, David Chiang
  bibkey: murray-etal-2019-efficiency
  bibtype: inproceedings
  booktitle: Proceedings of the 3rd Workshop on Neural Generation and Translation
  booktitle_html: Proceedings of the 3rd Workshop on Neural Generation and Translation
  doi: 10.18653/v1/D19-5634
  month: November
  page_first: '297'
  page_last: '301'
  pages: "297\u2013301"
  paper_id: '34'
  parent_volume_id: D19-56
  pdf: https://www.aclweb.org/anthology/D19-5634.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5634.jpg
  title: "Efficiency through Auto-Sizing: Notre Dame NLP\u2019s Submission to the\
    \ WNGT 2019 Efficiency Task"
  title_html: "Efficiency through Auto-Sizing: <span class=\"acl-fixed-case\">N</span>otre\
    \ <span class=\"acl-fixed-case\">D</span>ame <span class=\"acl-fixed-case\">NLP</span>\u2019\
    s Submission to the <span class=\"acl-fixed-case\">WNGT</span> 2019 Efficiency\
    \ Task"
  url: https://www.aclweb.org/anthology/D19-5634
  year: '2019'
D19-5700:
  address: Hong Kong, China
  author:
  - first: Kim
    full: Kim Jin-Dong
    id: kim-jin-dong
    last: Jin-Dong
  - first: "N\xE9dellec"
    full: "N\xE9dellec Claire"
    id: nedellec-claire
    last: Claire
  - first: Bossy
    full: Bossy Robert
    id: bossy-robert
    last: Robert
  - first: "Del\xE9ger"
    full: "Del\xE9ger Louise"
    id: deleger-louise
    last: Louise
  author_string: "Kim Jin-Dong, N\xE9dellec Claire, Bossy Robert, Del\xE9ger Louise"
  bibkey: emnlp-2019-bionlp
  bibtype: proceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  month: November
  paper_id: '0'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5700.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5700.jpg
  title: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  title_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  url: https://www.aclweb.org/anthology/D19-5700
  year: '2019'
D19-5701:
  abstract: One of the biomedical entity types of relevance for medicine or biosciences
    are chemical compounds and drugs. The correct detection these entities is critical
    for other text mining applications building on them, such as adverse drug-reaction
    detection, medication-related fake news or drug-target extraction. Although a
    significant effort was made to detect mentions of drugs/chemicals in English texts,
    so far only very limited attempts were made to recognize them in medical documents
    in other languages. Taking into account the growing amount of medical publications
    and clinical records written in Spanish, we have organized the first shared task
    on detecting drug and chemical entities in Spanish medical documents. Additionally,
    we included a clinical concept-indexing sub-track asking teams to return SNOMED-CT
    identifiers related to drugs/chemicals for a collection of documents. For this
    task, named PharmaCoNER, we generated annotation guidelines together with a corpus
    of 1,000 manually annotated clinical case studies. A total of 22 teams participated
    in the sub-track 1, (77 system runs), and 7 teams in the sub-track 2 (19 system
    runs). Top scoring teams used sophisticated deep learning approaches yielding
    very competitive results with F-measures above 0.91. These results indicate that
    there is a real interest in promoting biomedical text mining efforts beyond English.
    We foresee that the PharmaCoNER annotation guidelines, corpus and participant
    systems will foster the development of new resources for clinical and biomedical
    text mining systems of Spanish medical data.
  address: Hong Kong, China
  author:
  - first: Aitor
    full: Aitor Gonzalez-Agirre
    id: aitor-gonzalez-agirre
    last: Gonzalez-Agirre
  - first: Montserrat
    full: Montserrat Marimon
    id: montserrat-marimon
    last: Marimon
  - first: Ander
    full: Ander Intxaurrondo
    id: ander-intxaurrondo
    last: Intxaurrondo
  - first: Obdulia
    full: Obdulia Rabal
    id: obdulia-rabal
    last: Rabal
  - first: Marta
    full: Marta Villegas
    id: marta-villegas
    last: Villegas
  - first: Martin
    full: Martin Krallinger
    id: martin-krallinger
    last: Krallinger
  author_string: Aitor Gonzalez-Agirre, Montserrat Marimon, Ander Intxaurrondo, Obdulia
    Rabal, Marta Villegas, Martin Krallinger
  bibkey: gonzalez-agirre-etal-2019-pharmaconer
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5701
  month: November
  page_first: '1'
  page_last: '10'
  pages: "1\u201310"
  paper_id: '1'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5701.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5701.jpg
  title: 'PharmaCoNER: Pharmacological Substances, Compounds and proteins Named Entity
    Recognition track'
  title_html: '<span class="acl-fixed-case">P</span>harma<span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NER</span>: Pharmacological Substances, Compounds and proteins
    Named Entity Recognition track'
  url: https://www.aclweb.org/anthology/D19-5701
  year: '2019'
D19-5702:
  abstract: The recognition of pharmacological substances, compounds and proteins
    is an essential preliminary work for the recognition of relations between chemicals
    and other biomedically relevant units. In this paper, we describe an approach
    to Task 1 of the PharmaCoNER Challenge, which involves the recognition of mentions
    of chemicals and drugs in Spanish medical texts. We train a state-of-the-art BiLSTM-CRF
    sequence tagger with stacked Pooled Contextualized Embeddings, word and sub-word
    embeddings using the open-source framework FLAIR. We present a new corpus composed
    of articles and papers from Spanish health science journals, termed the Spanish
    Health Corpus, and use it to train domain-specific embeddings which we incorporate
    in our model training. We achieve a result of 89.76% F1-score using pre-trained
    embeddings and are able to improve these results to 90.52% F1-score using specialized
    embeddings.
  address: Hong Kong, China
  author:
  - first: Manuel
    full: Manuel Stoeckel
    id: manuel-stoeckel
    last: Stoeckel
  - first: Wahed
    full: Wahed Hemati
    id: wahed-hemati
    last: Hemati
  - first: Alexander
    full: Alexander Mehler
    id: alexander-mehler
    last: Mehler
  author_string: Manuel Stoeckel, Wahed Hemati, Alexander Mehler
  bibkey: stoeckel-etal-2019-specialization
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5702
  month: November
  page_first: '11'
  page_last: '15'
  pages: "11\u201315"
  paper_id: '2'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5702.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5702.jpg
  title: 'When Specialization Helps: Using Pooled Contextualized Embeddings to Detect
    Chemical and Biomedical Entities in Spanish'
  title_html: 'When Specialization Helps: Using Pooled Contextualized Embeddings to
    Detect Chemical and Biomedical Entities in <span class="acl-fixed-case">S</span>panish'
  url: https://www.aclweb.org/anthology/D19-5702
  year: '2019'
D19-5703:
  abstract: This paper presents the participation of the VSP team for the PharmaCoNER
    Tracks from the BioNLP Open Shared Task 2019. The system consists of a neural
    model for the Named Entity Recognition of drugs, medications and chemical entities
    in Spanish and the use of the Spanish Edition of SNOMED CT term search engine
    for the concept normalization of the recognized mentions. The neural network is
    implemented with two bidirectional Recurrent Neural Networks with LSTM cells that
    creates a feature vector for each word of the sentences in order to classify the
    entities. The first layer uses the characters of each word and the resulting vector
    is aggregated to the second layer together with its word embedding in order to
    create the feature vector of the word. Besides, a Conditional Random Field layer
    classifies the vector representation of each word in one of the mention types.
    The system obtains a performance of 76.29%, and 60.34% in F1 for the classification
    of the Named Entity Recognition task and the Concept indexing task, respectively.
    This method presents good results with a basic approach without using pretrained
    word embeddings or any hand-crafted features.
  address: Hong Kong, China
  author:
  - first: "V\xEDctor"
    full: "V\xEDctor Su\xE1rez-Paniagua"
    id: victor-suarez-paniagua
    last: "Su\xE1rez-Paniagua"
  author_string: "V\xEDctor Su\xE1rez-Paniagua"
  bibkey: suarez-paniagua-2019-vsp
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5703
  month: November
  page_first: '16'
  page_last: '20'
  pages: "16\u201320"
  paper_id: '3'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5703.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5703.jpg
  title: 'VSP at PharmaCoNER 2019: Recognition of Pharmacological Substances, Compounds
    and Proteins with Recurrent Neural Networks in Spanish Clinical Cases'
  title_html: '<span class="acl-fixed-case">VSP</span> at <span class="acl-fixed-case">P</span>harma<span
    class="acl-fixed-case">C</span>o<span class="acl-fixed-case">NER</span> 2019:
    Recognition of Pharmacological Substances, Compounds and Proteins with Recurrent
    Neural Networks in <span class="acl-fixed-case">S</span>panish Clinical Cases'
  url: https://www.aclweb.org/anthology/D19-5703
  year: '2019'
D19-5704:
  abstract: 'The aim of this paper is to present our approach (IxaMed) in the PharmacoNER
    2019 task. The task consists of identifying chemical, drug, and gene/protein mentions
    from clinical case studies written in Spanish. The evaluation of the task is divided
    in two scenarios: one corresponding to the detection of named entities and one
    corresponding to the indexation of named entities that have been previously identified.
    In order to identify named entities we have made use of a Bi-LSTM with a CRF on
    top in combination with different types of word embeddings. We have achieved our
    best result (86.81 F-Score) combining pretrained word embeddings of Wikipedia
    and Electronic Health Records (50M words) with contextual string embeddings of
    Wikipedia and Electronic Health Records. On the other hand, for the indexation
    of the named entities we have used the Levenshtein distance obtaining a 85.34
    F-Score as our best result.'
  address: Hong Kong, China
  author:
  - first: Xabier
    full: Xabier Lahuerta
    id: xabier-lahuerta
    last: Lahuerta
  - first: Iakes
    full: Iakes Goenaga
    id: iakes-goenaga
    last: Goenaga
  - first: Koldo
    full: Koldo Gojenola
    id: koldo-gojenola
    last: Gojenola
  - first: Aitziber
    full: Aitziber Atutxa Salazar
    id: aitziber-atutxa-salazar
    last: Atutxa Salazar
  - first: Maite
    full: Maite Oronoz
    id: maite-oronoz
    last: Oronoz
  author_string: Xabier Lahuerta, Iakes Goenaga, Koldo Gojenola, Aitziber Atutxa Salazar,
    Maite Oronoz
  bibkey: lahuerta-etal-2019-ixamed
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5704
  month: November
  page_first: '21'
  page_last: '25'
  pages: "21\u201325"
  paper_id: '4'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5704.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5704.jpg
  title: IxaMed at PharmacoNER Challenge 2019
  title_html: <span class="acl-fixed-case">I</span>xa<span class="acl-fixed-case">M</span>ed
    at <span class="acl-fixed-case">P</span>harmaco<span class="acl-fixed-case">NER</span>
    Challenge 2019
  url: https://www.aclweb.org/anthology/D19-5704
  year: '2019'
D19-5705:
  abstract: "Named entity recognition has been extensively studied on English news\
    \ texts. However, the transfer to other domains and languages is still a challenging\
    \ problem. In this paper, we describe the system with which we participated in\
    \ the first subtrack of the PharmaCoNER competition of the BioNLP Open Shared\
    \ Tasks 2019. Aiming at pharmacological entity detection in Spanish texts, the\
    \ task provides a non-standard domain and language setting. However, we propose\
    \ an architecture that requires neither language nor domain expertise. We treat\
    \ the task as a sequence labeling task and experiment with attention-based embedding\
    \ selection and the training on automatically annotated data to further improve\
    \ our system\u2019s performance. Our system achieves promising results, especially\
    \ by combining the different techniques, and reaches up to 88.6% F1 in the competition."
  address: Hong Kong, China
  author:
  - first: Lukas
    full: Lukas Lange
    id: lukas-lange
    last: Lange
  - first: Heike
    full: Heike Adel
    id: heike-adel
    last: Adel
  - first: Jannik
    full: "Jannik Str\xF6tgen"
    id: jannik-strotgen
    last: "Str\xF6tgen"
  author_string: "Lukas Lange, Heike Adel, Jannik Str\xF6tgen"
  bibkey: lange-etal-2019-nlnde
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5705
  month: November
  page_first: '26'
  page_last: '32'
  pages: "26\u201332"
  paper_id: '5'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5705.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5705.jpg
  title: 'NLNDE: Enhancing Neural Sequence Taggers with Attention and Noisy Channel
    for Robust Pharmacological Entity Detection'
  title_html: '<span class="acl-fixed-case">NLNDE</span>: Enhancing Neural Sequence
    Taggers with Attention and Noisy Channel for Robust Pharmacological Entity Detection'
  url: https://www.aclweb.org/anthology/D19-5705
  year: '2019'
D19-5706:
  abstract: 'The Biological Text Mining Unit at BSC and CNIO organized the first shared
    task on chemical & drug mention recognition from Spanish medical texts called
    PharmaCoNER (Pharmacological Substances, Compounds and proteins and Named Entity
    Recognition track) in 2019, which includes two tracks: one for NER offset and
    entity classification (track 1) and the other one for concept indexing (track
    2). We developed a pipeline system based on deep learning methods for this shared
    task, specifically, a subsystem based on BERT (Bidirectional Encoder Representations
    from Transformers) for NER offset and entity classification and a subsystem based
    on Bpool (Bi-LSTM with max/mean pooling) for concept indexing. Evaluation conducted
    on the shared task data showed that our system achieves a micro-average F1-score
    of 0.9105 on track 1 and a micro-average F1-score of 0.8391 on track 2.'
  address: Hong Kong, China
  author:
  - first: Ying
    full: Ying Xiong
    id: ying-xiong
    last: Xiong
  - first: Yedan
    full: Yedan Shen
    id: yedan-shen
    last: Shen
  - first: Yuanhang
    full: Yuanhang Huang
    id: yuanhang-huang
    last: Huang
  - first: Shuai
    full: Shuai Chen
    id: shuai-chen
    last: Chen
  - first: Buzhou
    full: Buzhou Tang
    id: buzhou-tang
    last: Tang
  - first: Xiaolong
    full: Xiaolong Wang
    id: xiaolong-wang
    last: Wang
  - first: Qingcai
    full: Qingcai Chen
    id: qingcai-chen
    last: Chen
  - first: Jun
    full: Jun Yan
    id: jun-yan
    last: Yan
  - first: Yi
    full: Yi Zhou
    id: yi-zhou
    last: Zhou
  author_string: Ying Xiong, Yedan Shen, Yuanhang Huang, Shuai Chen, Buzhou Tang,
    Xiaolong Wang, Qingcai Chen, Jun Yan, Yi Zhou
  bibkey: xiong-etal-2019-deep
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5706
  month: November
  page_first: '33'
  page_last: '37'
  pages: "33\u201337"
  paper_id: '6'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5706.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5706.jpg
  title: A Deep Learning-Based System for PharmaCoNER
  title_html: A Deep Learning-Based System for <span class="acl-fixed-case">P</span>harma<span
    class="acl-fixed-case">C</span>o<span class="acl-fixed-case">NER</span>
  url: https://www.aclweb.org/anthology/D19-5706
  year: '2019'
D19-5707:
  abstract: In this work, we introduce a Deep Learning architecture for pharmaceutical
    and chemical Named Entity Recognition in Spanish clinical cases texts. We propose
    a hybrid model approach based on two Bidirectional Long Short-Term Memory (Bi-LSTM)
    network and Conditional Random Field (CRF) network using character, word, concept
    and sense embeddings to deal with the extraction of semantic, syntactic and morphological
    features. The approach was evaluated on the PharmaCoNER Corpus obtaining an F-measure
    of 85.24% for subtask 1 and 49.36% for subtask2. These results prove that deep
    learning methods with specific domain embedding representations can outperform
    the state-of-the-art approaches.
  address: Hong Kong, China
  author:
  - first: Renzo
    full: Renzo Rivera
    id: renzo-rivera
    last: Rivera
  - first: Paloma
    full: "Paloma Mart\xEDnez"
    id: paloma-martinez
    last: "Mart\xEDnez"
  author_string: "Renzo Rivera, Paloma Mart\xEDnez"
  bibkey: rivera-martinez-2019-deep
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5707
  month: November
  page_first: '38'
  page_last: '46'
  pages: "38\u201346"
  paper_id: '7'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5707.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5707.jpg
  title: Deep neural model with enhanced embeddings for pharmaceutical and chemical
    entities recognition in Spanish clinical text
  title_html: Deep neural model with enhanced embeddings for pharmaceutical and chemical
    entities recognition in <span class="acl-fixed-case">S</span>panish clinical text
  url: https://www.aclweb.org/anthology/D19-5707
  year: '2019'
D19-5708:
  abstract: We present a neural pipeline approach that performs named entity recognition
    (NER) and concept indexing (CI), which links them to concept unique identifiers
    (CUIs) in a knowledge base, for the PharmaCoNER shared task on pharmaceutical
    drugs and chemical entities. We proposed a neural NER model that captures the
    surrounding semantic information of a given sequence by capturing the forward-
    and backward-context of bidirectional LSTM (Bi-LSTM) output of a target span using
    contextual span representation-based exhaustive approach. The NER model enumerates
    all possible spans as potential entity mentions and classify them into entity
    types or no entity with deep neural networks. For representing span, we compare
    several different neural network architectures and their ensembling for the NER
    model. We then perform dictionary matching for CI and, if there is no matching,
    we further compute similarity scores between a mention and CUIs using entity embeddings
    to assign the CUI with the highest score to the mention. We evaluate our approach
    on the two sub-tasks in the shared task. Among the five submitted runs, the best
    run for each sub-task achieved the F-score of 86.76% on Sub-task 1 (NER) and the
    F-score of 79.97% (strict) on Sub-task 2 (CI).
  address: Hong Kong, China
  author:
  - first: Mohammad Golam
    full: Mohammad Golam Sohrab
    id: mohammad-golam-sohrab
    last: Sohrab
  - first: Minh Thang
    full: Minh Thang Pham
    id: minh-thang-pham
    last: Pham
  - first: Makoto
    full: Makoto Miwa
    id: makoto-miwa
    last: Miwa
  - first: Hiroya
    full: Hiroya Takamura
    id: hiroya-takamura
    last: Takamura
  author_string: Mohammad Golam Sohrab, Minh Thang Pham, Makoto Miwa, Hiroya Takamura
  bibkey: sohrab-etal-2019-neural
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5708
  month: November
  page_first: '47'
  page_last: '55'
  pages: "47\u201355"
  paper_id: '8'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5708.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5708.jpg
  title: A Neural Pipeline Approach for the PharmaCoNER Shared Task using Contextual
    Exhaustive Models
  title_html: A Neural Pipeline Approach for the <span class="acl-fixed-case">P</span>harma<span
    class="acl-fixed-case">C</span>o<span class="acl-fixed-case">NER</span> Shared
    Task using Contextual Exhaustive Models
  url: https://www.aclweb.org/anthology/D19-5708
  year: '2019'
D19-5709:
  abstract: 'We present the approach of the Turku NLP group to the PharmaCoNER task
    on Spanish biomedical named entity recognition. We apply a CRF-based baseline
    approach and multilingual BERT to the task, achieving an F-score of 88% on the
    development data and 87% on the test set with BERT. Our approach reflects a straightforward
    application of a state-of-the-art multilingual model that is not specifically
    tailored to either the language nor the application domain. The source code is
    available at: https://github.com/chaanim/pharmaconer'
  address: Hong Kong, China
  author:
  - first: Kai
    full: Kai Hakala
    id: kai-hakala
    last: Hakala
  - first: Sampo
    full: Sampo Pyysalo
    id: sampo-pyysalo
    last: Pyysalo
  author_string: Kai Hakala, Sampo Pyysalo
  bibkey: hakala-pyysalo-2019-biomedical
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5709
  month: November
  page_first: '56'
  page_last: '61'
  pages: "56\u201361"
  paper_id: '9'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5709.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5709.jpg
  title: Biomedical Named Entity Recognition with Multilingual BERT
  title_html: Biomedical Named Entity Recognition with Multilingual <span class="acl-fixed-case">BERT</span>
  url: https://www.aclweb.org/anthology/D19-5709
  year: '2019'
D19-5710:
  abstract: "The active gene annotation corpus (AGAC) was developed to support knowledge\
    \ discovery for drug repurposing. Based on the corpus, the AGAC track of the BioNLP\
    \ Open Shared Tasks 2019 was organized, to facilitate cross-disciplinary collaboration\
    \ across BioNLP and Pharmacoinformatics communities, for drug repurposing. The\
    \ AGAC track consists of three subtasks: 1) named entity recognition, 2) thematic\
    \ relation extraction, and 3) loss of function (LOF) / gain of function (GOF)\
    \ topic classification. The AGAC track was participated by five teams, of which\
    \ the performance are compared and analyzed. The the results revealed a substantial\
    \ room for improvement in the design of the task, which we analyzed in terms of\
    \ \u201Cimbalanced data\u201D, \u201Cselective annotation\u201D and \u201Clatent\
    \ topic annotation\u201D."
  address: Hong Kong, China
  author:
  - first: Yuxing
    full: Yuxing Wang
    id: yuxing-wang
    last: Wang
  - first: Kaiyin
    full: Kaiyin Zhou
    id: kaiyin-zhou
    last: Zhou
  - first: Mina
    full: Mina Gachloo
    id: mina-gachloo
    last: Gachloo
  - first: Jingbo
    full: Jingbo Xia
    id: jingbo-xia
    last: Xia
  author_string: Yuxing Wang, Kaiyin Zhou, Mina Gachloo, Jingbo Xia
  bibkey: wang-etal-2019-overview
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5710
  month: November
  page_first: '62'
  page_last: '71'
  pages: "62\u201371"
  paper_id: '10'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5710.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5710.jpg
  title: An Overview of the Active Gene Annotation Corpus and the BioNLP OST 2019
    AGAC Track Tasks
  title_html: An Overview of the Active Gene Annotation Corpus and the <span class="acl-fixed-case">B</span>io<span
    class="acl-fixed-case">NLP</span> <span class="acl-fixed-case">OST</span> 2019
    <span class="acl-fixed-case">AGAC</span> Track Tasks
  url: https://www.aclweb.org/anthology/D19-5710
  year: '2019'
D19-5711:
  abstract: The prediction of the relationship between the disease with genes and
    its mutations is a very important knowledge extraction task that can potentially
    help drug discovery. In this paper, we present our approaches for trigger word
    detection (task 1) and the identification of its thematic role (task 2) in AGAC
    track of BioNLP Open Shared Task 2019. Task 1 can be regarded as the traditional
    name entity recognition (NER), which cultivates molecular phenomena related to
    gene mutation. Task 2 can be regarded as relation extraction which captures the
    thematic roles between entities. For two tasks, we exploit the pre-trained biomedical
    language representation model (i.e., BERT) in the pipe of information extraction
    for the collection of mutation-disease knowledge from PubMed. And also, we design
    a fine-tuning technique and extra features by using multi-task learning. The experiment
    results show that our proposed approaches achieve 0.60 (ranks 1) and 0.25 (ranks
    2) on task 1 and task 2 respectively in terms of F1 metric. metric.
  address: Hong Kong, China
  author:
  - first: Dongfang
    full: Dongfang Li
    id: dongfang-li
    last: Li
  - first: Ying
    full: Ying Xiong
    id: ying-xiong
    last: Xiong
  - first: Baotian
    full: Baotian Hu
    id: baotian-hu
    last: Hu
  - first: Hanyang
    full: Hanyang Du
    id: hanyang-du
    last: Du
  - first: Buzhou
    full: Buzhou Tang
    id: buzhou-tang
    last: Tang
  - first: Qingcai
    full: Qingcai Chen
    id: qingcai-chen
    last: Chen
  author_string: Dongfang Li, Ying Xiong, Baotian Hu, Hanyang Du, Buzhou Tang, Qingcai
    Chen
  bibkey: li-etal-2019-trigger
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5711
  month: November
  page_first: '72'
  page_last: '76'
  pages: "72\u201376"
  paper_id: '11'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5711.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5711.jpg
  title: Trigger Word Detection and Thematic Role Identification via BERT and Multitask
    Learning
  title_html: Trigger Word Detection and Thematic Role Identification via <span class="acl-fixed-case">BERT</span>
    and Multitask Learning
  url: https://www.aclweb.org/anthology/D19-5711
  year: '2019'
D19-5712:
  abstract: Understanding the pathogenesis of genetic diseases through different gene
    activities and their relations to relevant diseases is important for new drug
    discovery and drug repositioning. In this paper, we present a joint deep learning
    model in a multi-task learning paradigm for gene mutation-disease knowledge extraction,
    DeepGeneMD, which adapts the state-of-the-art hierarchical multi-task learning
    framework for joint inference on named entity recognition (NER) and relation extraction
    (RE) in the context of the AGAC (Active Gene Annotation Corpus) track at 2019
    BioNLP Open Shared Tasks (BioNLP-OST). It simultaneously extracts gene mutation
    related activities, diseases, and their relations from the published scientific
    literature. In DeepGeneMD, we explore the task decomposition to create auxiliary
    subtasks so that more interactions between different learning subtasks can be
    leveraged in model training. Our model achieves the average F1 score of 0.45 on
    recognizing gene activities and disease entities, ranking 2nd in the AGAC NER
    task; and the average F1 score of 0.35 on extracting relations, ranking 1st in
    the AGAC RE task.
  address: Hong Kong, China
  author:
  - first: Feifan
    full: Feifan Liu
    id: feifan-liu
    last: Liu
  - first: Xiaoyu
    full: Xiaoyu Zheng
    id: xiaoyu-zheng
    last: Zheng
  - first: Bo
    full: Bo Wang
    id: bo-wang
    last: Wang
  - first: Catarina
    full: Catarina Kiefe
    id: catarina-kiefe
    last: Kiefe
  author_string: Feifan Liu, Xiaoyu Zheng, Bo Wang, Catarina Kiefe
  bibkey: liu-etal-2019-deepgenemd
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5712
  month: November
  page_first: '77'
  page_last: '83'
  pages: "77\u201383"
  paper_id: '12'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5712.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5712.jpg
  title: 'DeepGeneMD: A Joint Deep Learning Model for Extracting Gene Mutation-Disease
    Knowledge from PubMed Literature'
  title_html: '<span class="acl-fixed-case">D</span>eep<span class="acl-fixed-case">G</span>ene<span
    class="acl-fixed-case">MD</span>: A Joint Deep Learning Model for Extracting Gene
    Mutation-Disease Knowledge from <span class="acl-fixed-case">P</span>ub<span class="acl-fixed-case">M</span>ed
    Literature'
  url: https://www.aclweb.org/anthology/D19-5712
  year: '2019'
D19-5713:
  abstract: "This paper presents our participation in the AGAC Track from the 2019\
    \ BioNLP Open Shared Tasks. We provide a solution for Task 3, which aims to extract\
    \ \u201Cgene - function change - disease\u201D triples, where \u201Cgene\u201D\
    \ and \u201Cdisease\u201D are mentions of particular genes and diseases respectively\
    \ and \u201Cfunction change\u201D is one of four pre-defined relationship types.\
    \ Our system extends BERT (Devlin et al., 2018), a state-of-the-art language model,\
    \ which learns contextual language representations from a large unlabelled corpus\
    \ and whose parameters can be fine-tuned to solve specific tasks with minimal\
    \ additional architecture. We encode the pair of mentions and their textual context\
    \ as two consecutive sequences in BERT, separated by a special symbol. We then\
    \ use a single linear layer to classify their relationship into five classes (four\
    \ pre-defined, as well as \u2018no relation\u2019). Despite considerable class\
    \ imbalance, our system significantly outperforms a random baseline while relying\
    \ on an extremely simple setup with no specially engineered features."
  address: Hong Kong, China
  author:
  - first: Ashok
    full: Ashok Thillaisundaram
    id: ashok-thillaisundaram
    last: Thillaisundaram
  - first: Theodosia
    full: Theodosia Togia
    id: theodosia-togia
    last: Togia
  author_string: Ashok Thillaisundaram, Theodosia Togia
  bibkey: thillaisundaram-togia-2019-biomedical
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5713
  month: November
  page_first: '84'
  page_last: '89'
  pages: "84\u201389"
  paper_id: '13'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5713.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5713.jpg
  title: Biomedical relation extraction with pre-trained language representations
    and minimal task-specific architecture
  title_html: Biomedical relation extraction with pre-trained language representations
    and minimal task-specific architecture
  url: https://www.aclweb.org/anthology/D19-5713
  year: '2019'
D19-5714:
  abstract: "This paper describes the Named Entity Recognition system of the Institute\
    \ for Artificial Intelligence \u201CMihai Dr\u0103g\u0103nescu\u201D of the Romanian\
    \ Academy (RACAI for short). Our best F1 score of 0.84984 was achieved using an\
    \ ensemble of two systems: a gazetteer-based baseline and a RNN-based NER system,\
    \ developed specially for PharmaCoNER 2019. We will describe the individual systems\
    \ and the ensemble algorithm, compare the final system to the current state of\
    \ the art, as well as discuss our results with respect to the quality of the training\
    \ data and its annotation strategy. The resulting NER system is language independent,\
    \ provided that language-dependent resources and preprocessing tools exist, such\
    \ as tokenizers and POS taggers."
  address: Hong Kong, China
  author:
  - first: Radu
    full: Radu Ion
    id: radu-ion
    last: Ion
  - first: Vasile Florian
    full: "Vasile Florian P\u0103i\u0219"
    id: vasile-florian-pais
    last: "P\u0103i\u0219"
  - first: Maria
    full: Maria Mitrofan
    id: maria-mitrofan
    last: Mitrofan
  author_string: "Radu Ion, Vasile Florian P\u0103i\u0219, Maria Mitrofan"
  bibkey: ion-etal-2019-racais
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5714
  month: November
  page_first: '90'
  page_last: '99'
  pages: "90\u201399"
  paper_id: '14'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5714.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5714.jpg
  title: "RACAI\u2019s System at PharmaCoNER 2019"
  title_html: "<span class=\"acl-fixed-case\">RACAI</span>\u2019s System at <span\
    \ class=\"acl-fixed-case\">P</span>harma<span class=\"acl-fixed-case\">C</span>o<span\
    \ class=\"acl-fixed-case\">NER</span> 2019"
  url: https://www.aclweb.org/anthology/D19-5714
  year: '2019'
D19-5715:
  abstract: To date, a large amount of biomedical content has been published in non-English
    texts, especially for clinical documents. Therefore, it is of considerable significance
    to conduct Natural Language Processing (NLP) research in non-English literature.
    PharmaCoNER is the first Named Entity Recognition (NER) task to recognize chemical
    and protein entities from Spanish biomedical texts. Since there have been abundant
    resources in the NLP field, how to exploit these existing resources to a new task
    to obtain competitive performance is a meaningful study. Inspired by the success
    of transfer learning with language models, we introduce the BERT benchmark to
    facilitate the research of PharmaCoNER task. In this paper, we evaluate two baselines
    based on Multilingual BERT and BioBERT on the PharmaCoNER corpus. Experimental
    results show that transferring the knowledge learned from source large-scale datasets
    to the target domain offers an effective solution for the PharmaCoNER task.
  address: Hong Kong, China
  author:
  - first: Cong
    full: Cong Sun
    id: cong-sun
    last: Sun
  - first: Zhihao
    full: Zhihao Yang
    id: zhihao-yang
    last: Yang
  author_string: Cong Sun, Zhihao Yang
  bibkey: sun-yang-2019-transfer
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5715
  month: November
  page_first: '100'
  page_last: '104'
  pages: "100\u2013104"
  paper_id: '15'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5715.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5715.jpg
  title: 'Transfer Learning in Biomedical Named Entity Recognition: An Evaluation
    of BERT in the PharmaCoNER task'
  title_html: 'Transfer Learning in Biomedical Named Entity Recognition: An Evaluation
    of <span class="acl-fixed-case">BERT</span> in the <span class="acl-fixed-case">P</span>harma<span
    class="acl-fixed-case">C</span>o<span class="acl-fixed-case">NER</span> task'
  url: https://www.aclweb.org/anthology/D19-5715
  year: '2019'
D19-5716:
  abstract: This paper presents a novel transfer multi-task learning method for Bacteria
    Biotope rel+ner task at BioNLP-OST 2019. To alleviate the data deficiency problem
    in domain-specific information extraction, we use BERT(Bidirectional Encoder Representations
    from Transformers) and pre-train it using mask language models and next sentence
    prediction on both general corpus and medical corpus like PubMed. In fine-tuning
    stage, we fine-tune the relation extraction layer and mention recognition layer
    designed by us on the top of BERT to extract mentions and relations simultaneously.
    The evaluation results show that our method achieves the best performance on all
    metrics (including slot error rate, precision and recall) in the Bacteria Biotope
    rel+ner subtask.
  address: Hong Kong, China
  author:
  - first: Qi
    full: Qi Zhang
    id: qi-zhang
    last: Zhang
  - first: Chao
    full: Chao Liu
    id: chao-liu
    last: Liu
  - first: Ying
    full: Ying Chi
    id: ying-chi
    last: Chi
  - first: Xuansong
    full: Xuansong Xie
    id: xuansong-xie
    last: Xie
  - first: Xiansheng
    full: Xiansheng Hua
    id: xiansheng-hua
    last: Hua
  author_string: Qi Zhang, Chao Liu, Ying Chi, Xuansong Xie, Xiansheng Hua
  bibkey: zhang-etal-2019-multi
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5716
  month: November
  page_first: '105'
  page_last: '109'
  pages: "105\u2013109"
  paper_id: '16'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5716.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5716.jpg
  title: A Multi-Task Learning Framework for Extracting Bacteria Biotope Information
  title_html: A Multi-Task Learning Framework for Extracting Bacteria Biotope Information
  url: https://www.aclweb.org/anthology/D19-5716
  year: '2019'
D19-5717:
  abstract: 'We participated in the BioNLP 2019 Open Shared Tasks: binary relation
    extraction of SeeDev task. The model was constructed us- ing convolutional neural
    networks (CNN) and long short term memory networks (LSTM). The full text information
    and context information were collected using the advantages of CNN and LSTM. The
    model consisted of two main modules: distributed semantic representation construction,
    such as word embedding, distance embedding and entity type embed- ding; and CNN-LSTM
    model. The F1 value of our participated task on the test data set of all types
    was 0.342. We achieved the second highest in the task. The results showed that
    our proposed method performed effectively in the binary relation extraction.'
  address: Hong Kong, China
  author:
  - first: Junyi
    full: Junyi Li
    id: junyi-li
    last: Li
  - first: Xiaobing
    full: Xiaobing Zhou
    id: xiaobing-zhou
    last: Zhou
  - first: Yuhang
    full: Yuhang Wu
    id: yuhang-wu
    last: Wu
  - first: Bin
    full: Bin Wang
    id: bin-wang
    last: Wang
  author_string: Junyi Li, Xiaobing Zhou, Yuhang Wu, Bin Wang
  bibkey: li-etal-2019-ynu
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5717
  month: November
  page_first: '110'
  page_last: '114'
  pages: "110\u2013114"
  paper_id: '17'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5717.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5717.jpg
  title: 'YNU-junyi in BioNLP-OST 2019: Using CNN-LSTM Model with Embeddings for SeeDev
    Binary Event Extraction'
  title_html: '<span class="acl-fixed-case">YNU</span>-junyi in <span class="acl-fixed-case">B</span>io<span
    class="acl-fixed-case">NLP</span>-<span class="acl-fixed-case">OST</span> 2019:
    Using <span class="acl-fixed-case">CNN</span>-<span class="acl-fixed-case">LSTM</span>
    Model with Embeddings for <span class="acl-fixed-case">S</span>ee<span class="acl-fixed-case">D</span>ev
    Binary Event Extraction'
  url: https://www.aclweb.org/anthology/D19-5717
  year: '2019'
D19-5718:
  abstract: In this paper we describe a new named entity extraction system. Our work
    proposes a system for the identification and annotation of drug names in Spanish
    biomedical texts based on machine learning and deep learning models. Subsequently,
    a standardized code using Snomed is assigned to these drugs, for this purpose,
    Natural Language Processing tools and techniques have been used, and a dictionary
    of different sources of information has been built. The results are promising,
    we obtain 78% in F1 score on the first sub-track and in the second task we map
    with Snomed correctly 72% of the found entities.
  address: Hong Kong, China
  author:
  - first: Pilar
    full: "Pilar L\xF3pez \xDAbeda"
    id: pilar-lopez-ubeda
    last: "L\xF3pez \xDAbeda"
  - first: Manuel Carlos
    full: "Manuel Carlos D\xEDaz Galiano"
    id: manuel-carlos-diaz-galiano
    last: "D\xEDaz Galiano"
  - first: L. Alfonso
    full: L. Alfonso Urena Lopez
    id: l-alfonso-urena-lopez
    last: Urena Lopez
  - first: Maite
    full: Maite Martin
    id: m-teresa-martin-valdivia
    last: Martin
  author_string: "Pilar L\xF3pez \xDAbeda, Manuel Carlos D\xEDaz Galiano, L. Alfonso\
    \ Urena Lopez, Maite Martin"
  bibkey: lopez-ubeda-etal-2019-using
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5718
  month: November
  page_first: '115'
  page_last: '120'
  pages: "115\u2013120"
  paper_id: '18'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5718.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5718.jpg
  title: Using Snomed to recognize and index chemical and drug mentions.
  title_html: Using Snomed to recognize and index chemical and drug mentions.
  url: https://www.aclweb.org/anthology/D19-5718
  year: '2019'
D19-5719:
  abstract: This paper presents the fourth edition of the Bacteria Biotope task at
    BioNLP Open Shared Tasks 2019. The task focuses on the extraction of the locations
    and phenotypes of microorganisms from PubMed abstracts and full-text excerpts,
    and the characterization of these entities with respect to reference knowledge
    sources (NCBI taxonomy, OntoBiotope ontology). The task is motivated by the importance
    of the knowledge on biodiversity for fundamental research and applications in
    microbiology. The paper describes the different proposed subtasks, the corpus
    characteristics, and the challenge organization. We also provide an analysis of
    the results obtained by participants, and inspect the evolution of the results
    since the last edition in 2016.
  address: Hong Kong, China
  author:
  - first: Robert
    full: Robert Bossy
    id: robert-bossy
    last: Bossy
  - first: Louise
    full: "Louise Del\xE9ger"
    id: louise-deleger
    last: "Del\xE9ger"
  - first: Estelle
    full: Estelle Chaix
    id: estelle-chaix
    last: Chaix
  - first: Mouhamadou
    full: Mouhamadou Ba
    id: mouhamadou-ba
    last: Ba
  - first: Claire
    full: "Claire N\xE9dellec"
    id: claire-nedellec
    last: "N\xE9dellec"
  author_string: "Robert Bossy, Louise Del\xE9ger, Estelle Chaix, Mouhamadou Ba, Claire\
    \ N\xE9dellec"
  bibkey: bossy-etal-2019-bacteria
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5719
  month: November
  page_first: '121'
  page_last: '131'
  pages: "121\u2013131"
  paper_id: '19'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5719.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5719.jpg
  title: Bacteria Biotope at BioNLP Open Shared Tasks 2019
  title_html: Bacteria Biotope at <span class="acl-fixed-case">B</span>io<span class="acl-fixed-case">NLP</span>
    Open Shared Tasks 2019
  url: https://www.aclweb.org/anthology/D19-5719
  year: '2019'
D19-5720:
  abstract: "Named Entity Recognition (NER) and Relation Extraction (RE) are essential\
    \ tools in distilling knowledge from biomedical literature. This paper presents\
    \ our findings from participating in BioNLP Shared Tasks 2019. We addressed Named\
    \ Entity Recognition including nested entities extraction, Entity Normalization\
    \ and Relation Extraction. Our proposed approach of Named Entities can be generalized\
    \ to different languages and we have shown it\u2019s effectiveness for English\
    \ and Spanish text. We investigated linguistic features, hybrid loss including\
    \ ranking and Conditional Random Fields (CRF), multi-task objective and token\
    \ level ensembling strategy to improve NER. We employed dictionary based fuzzy\
    \ and semantic search to perform Entity Normalization. Finally, our RE system\
    \ employed Support Vector Machine (SVM) with linguistic features. Our NER submission\
    \ (team:MIC-CIS) ranked first in BB-2019 norm+NER task with standard error rate\
    \ (SER) of 0.7159 and showed competitive performance on PharmaCo NER task with\
    \ F1-score of 0.8662. Our RE system ranked first in the SeeDev-binary Relation\
    \ Extraction Task with F1-score of 0.3738."
  address: Hong Kong, China
  author:
  - first: Pankaj
    full: Pankaj Gupta
    id: pankaj-gupta
    last: Gupta
  - first: Usama
    full: Usama Yaseen
    id: usama-yaseen
    last: Yaseen
  - first: Hinrich
    full: "Hinrich Sch\xFCtze"
    id: hinrich-schutze
    last: "Sch\xFCtze"
  author_string: "Pankaj Gupta, Usama Yaseen, Hinrich Sch\xFCtze"
  bibkey: gupta-etal-2019-linguistically
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5720
  month: November
  page_first: '132'
  page_last: '142'
  pages: "132\u2013142"
  paper_id: '20'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5720.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5720.jpg
  title: Linguistically Informed Relation Extraction and Neural Architectures for
    Nested Named Entity Recognition in BioNLP-OST 2019
  title_html: Linguistically Informed Relation Extraction and Neural Architectures
    for Nested Named Entity Recognition in <span class="acl-fixed-case">B</span>io<span
    class="acl-fixed-case">NLP</span>-<span class="acl-fixed-case">OST</span> 2019
  url: https://www.aclweb.org/anthology/D19-5720
  year: '2019'
D19-5721:
  abstract: Different representations of the same concept could often be seen in scientific
    reports and publications. Entity normalization (or entity linking) is the task
    to match the different representations to their standard concepts. In this paper,
    we present a two-step ensemble CNN method that normalizes microbiology-related
    entities in free text to concepts in standard dictionaries. The method is capable
    of linking entities when only a small microbiology-related biomedical corpus is
    available for training, and achieved reasonable performance in the online test
    of the BioNLP-OST19 shared task Bacteria Biotope.
  address: Hong Kong, China
  author:
  - first: Pan
    full: Pan Deng
    id: pan-deng
    last: Deng
  - first: Haipeng
    full: Haipeng Chen
    id: haipeng-chen
    last: Chen
  - first: Mengyao
    full: Mengyao Huang
    id: mengyao-huang
    last: Huang
  - first: Xiaowen
    full: Xiaowen Ruan
    id: xiaowen-ruan
    last: Ruan
  - first: Liang
    full: Liang Xu
    id: liang-xu
    last: Xu
  author_string: Pan Deng, Haipeng Chen, Mengyao Huang, Xiaowen Ruan, Liang Xu
  bibkey: deng-etal-2019-ensemble
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5721
  month: November
  page_first: '143'
  page_last: '149'
  pages: "143\u2013149"
  paper_id: '21'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5721.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5721.jpg
  title: An ensemble CNN method for biomedical entity normalization
  title_html: An ensemble <span class="acl-fixed-case">CNN</span> method for biomedical
    entity normalization
  url: https://www.aclweb.org/anthology/D19-5721
  year: '2019'
D19-5722:
  abstract: 'This paper presents our participation to the Bacteria Biotope Task of
    the BioNLP Shared Task 2019. Our participation includes two systems for the two
    subtasks of the Bacteria Biotope Task: the normalization of entities (BB-norm)
    and the identification of the relations between the entities given a biomedical
    text (BB-rel). For the normalization of entities, we utilized word embeddings
    and syntactic re-ranking. For the relation extraction task, pre-defined rules
    are used. Although both approaches are unsupervised, in the sense that they do
    not need any labeled data, they achieved promising results. Especially, for the
    BB-norm task, the results have shown that the proposed method performs as good
    as deep learning based methods, which require labeled data.'
  address: Hong Kong, China
  author:
  - first: "\u0130lknur"
    full: "\u0130lknur Karadeniz"
    id: ilknur-karadeniz
    last: Karadeniz
  - first: "\xD6mer Faruk"
    full: "\xD6mer Faruk Tuna"
    id: omer-faruk-tuna
    last: Tuna
  - first: Arzucan
    full: "Arzucan \xD6zg\xFCr"
    id: arzucan-ozgur
    last: "\xD6zg\xFCr"
  author_string: "\u0130lknur Karadeniz, \xD6mer Faruk Tuna, Arzucan \xD6zg\xFCr"
  bibkey: karadeniz-etal-2019-boun
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5722
  month: November
  page_first: '150'
  page_last: '157'
  pages: "150\u2013157"
  paper_id: '22'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5722.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5722.jpg
  title: 'BOUN-ISIK Participation: An Unsupervised Approach for the Named Entity Normalization
    and Relation Extraction of Bacteria Biotopes'
  title_html: '<span class="acl-fixed-case">BOUN</span>-<span class="acl-fixed-case">ISIK</span>
    Participation: An Unsupervised Approach for the Named Entity Normalization and
    Relation Extraction of Bacteria Biotopes'
  url: https://www.aclweb.org/anthology/D19-5722
  year: '2019'
D19-5723:
  abstract: abstract In this article, we describe our approach for the Bacteria Biotopes
    relation extraction (BB-rel) subtask in the BioNLP Shared Task 2019. This task
    aims to promote the development of text mining systems that extract relationships
    between Microorganism, Habitat and Phenotype entities. In this paper, we propose
    a novel approach for dependency graph construction based on lexical chains, so
    one dependency graph can represent one or multiple sentences. After that, we propose
    a neural network model which consists of the bidirectional long short-term memories
    and an attention graph convolution neural network to learn relation extraction
    features from the graph. Our approach is able to extract both intra- and inter-sentence
    relations, and meanwhile utilize syntax information. The results show that our
    approach achieved the best F1 (66.3%) in the official evaluation participated
    by 7 teams.
  address: Hong Kong, China
  author:
  - first: Wuti
    full: Wuti Xiong
    id: wuti-xiong
    last: Xiong
  - first: Fei
    full: Fei Li
    id: fei-li
    last: Li
  - first: Ming
    full: Ming Cheng
    id: ming-cheng
    last: Cheng
  - first: Hong
    full: Hong Yu
    id: hong-yu
    last: Yu
  - first: Donghong
    full: Donghong Ji
    id: donghong-ji
    last: Ji
  author_string: Wuti Xiong, Fei Li, Ming Cheng, Hong Yu, Donghong Ji
  bibkey: xiong-etal-2019-bacteria
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5723
  month: November
  page_first: '158'
  page_last: '167'
  pages: "158\u2013167"
  paper_id: '23'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5723.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5723.jpg
  title: Bacteria Biotope Relation Extraction via Lexical Chains and Dependency Graphs
  title_html: Bacteria Biotope Relation Extraction via Lexical Chains and Dependency
    Graphs
  url: https://www.aclweb.org/anthology/D19-5723
  year: '2019'
D19-5724:
  abstract: In this paper, we present our participation in the Bacteria Biotope (BB)
    task at BioNLP-OST 2019. Our system utilizes fine-tuned language representation
    models and machine learning approaches based on word embedding and lexical features
    for entities recognition, normalization and relation extraction. It achieves the
    state-of-the-art performance and is among the top two systems in five of all six
    subtasks.
  address: Hong Kong, China
  author:
  - first: Jihang
    full: Jihang Mao
    id: jihang-mao
    last: Mao
  - first: Wanli
    full: Wanli Liu
    id: wanli-liu
    last: Liu
  author_string: Jihang Mao, Wanli Liu
  bibkey: mao-liu-2019-integration
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5724
  month: November
  page_first: '168'
  page_last: '173'
  pages: "168\u2013173"
  paper_id: '24'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5724.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5724.jpg
  title: Integration of Deep Learning and Traditional Machine Learning for Knowledge
    Extraction from Biomedical Literature
  title_html: Integration of Deep Learning and Traditional Machine Learning for Knowledge
    Extraction from Biomedical Literature
  url: https://www.aclweb.org/anthology/D19-5724
  year: '2019'
D19-5725:
  abstract: "As part of the BioNLP Open Shared Tasks 2019, the CRAFT Shared Tasks\
    \ 2019 provides a platform to gauge the state of the art for three fundamental\
    \ language processing tasks \u2014 dependency parse construction, coreference\
    \ resolution, and ontology concept identification \u2014 over full-text biomedical\
    \ articles. The structural annotation task requires the automatic generation of\
    \ dependency parses for each sentence of an article given only the article text.\
    \ The coreference resolution task focuses on linking coreferring base noun phrase\
    \ mentions into chains using the symmetrical and transitive identity relation.\
    \ The ontology concept annotation task involves the identification of concept\
    \ mentions within text using the classes of ten distinct ontologies in the biomedical\
    \ domain, both unmodified and augmented with extension classes. This paper provides\
    \ an overview of each task, including descriptions of the data provided to participants\
    \ and the evaluation metrics used, and discusses participant results relative\
    \ to baseline performances for each of the three tasks."
  address: Hong Kong, China
  author:
  - first: William
    full: William Baumgartner
    id: william-a-baumgartner-jr
    last: Baumgartner
  - first: Michael
    full: Michael Bada
    id: michael-bada
    last: Bada
  - first: Sampo
    full: Sampo Pyysalo
    id: sampo-pyysalo
    last: Pyysalo
  - first: Manuel R.
    full: Manuel R. Ciosici
    id: manuel-r-ciosici
    last: Ciosici
  - first: Negacy
    full: Negacy Hailu
    id: negacy-hailu
    last: Hailu
  - first: Harrison
    full: Harrison Pielke-Lombardo
    id: harrison-pielke-lombardo
    last: Pielke-Lombardo
  - first: Michael
    full: Michael Regan
    id: michael-regan
    last: Regan
  - first: Lawrence
    full: Lawrence Hunter
    id: lawrence-hunter
    last: Hunter
  author_string: William Baumgartner, Michael Bada, Sampo Pyysalo, Manuel R. Ciosici,
    Negacy Hailu, Harrison Pielke-Lombardo, Michael Regan, Lawrence Hunter
  bibkey: baumgartner-etal-2019-craft
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5725
  month: November
  page_first: '174'
  page_last: '184'
  pages: "174\u2013184"
  paper_id: '25'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5725.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5725.jpg
  title: "CRAFT Shared Tasks 2019 Overview \u2014 Integrated Structure, Semantics,\
    \ and Coreference"
  title_html: "<span class=\"acl-fixed-case\">CRAFT</span> Shared Tasks 2019 Overview\
    \ \u2014 Integrated Structure, Semantics, and Coreference"
  url: https://www.aclweb.org/anthology/D19-5725
  year: '2019'
D19-5726:
  abstract: As our submission to the CRAFT shared task 2019, we present two neural
    approaches to concept recognition. We propose two different systems for joint
    named entity recognition (NER) and normalization (NEN), both of which model the
    task as a sequence labeling problem. Our first system is a BiLSTM network with
    two separate outputs for NER and NEN trained from scratch, whereas the second
    system is an instance of BioBERT fine-tuned on the concept-recognition task. We
    exploit two strategies for extending concept coverage, ontology pretraining and
    backoff with a dictionary lookup. Our results show that the backoff strategy effectively
    tackles the problem of unseen concepts, addressing a major limitation of the chosen
    design. In the cross-system comparison, BioBERT proves to be a strong basis for
    creating a concept-recognition system, although some entity types are predicted
    more accurately by the BiLSTM-based system.
  address: Hong Kong, China
  author:
  - first: Lenz
    full: Lenz Furrer
    id: lenz-furrer
    last: Furrer
  - first: Joseph
    full: Joseph Cornelius
    id: joseph-cornelius
    last: Cornelius
  - first: Fabio
    full: Fabio Rinaldi
    id: fabio-rinaldi
    last: Rinaldi
  author_string: Lenz Furrer, Joseph Cornelius, Fabio Rinaldi
  bibkey: furrer-etal-2019-uzh
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5726
  month: November
  page_first: '185'
  page_last: '195'
  pages: "185\u2013195"
  paper_id: '26'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5726.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5726.jpg
  title: 'UZH@CRAFT-ST: a Sequence-labeling Approach to Concept Recognition'
  title_html: '<span class="acl-fixed-case">UZH</span>@<span class="acl-fixed-case">CRAFT</span>-<span
    class="acl-fixed-case">ST</span>: a Sequence-labeling Approach to Concept Recognition'
  url: https://www.aclweb.org/anthology/D19-5726
  year: '2019'
D19-5727:
  abstract: "This paper describes our system developed for the coreference resolution\
    \ task of the CRAFT Shared Tasks 2019. The CRAFT corpus is more challenging than\
    \ other existing corpora because it contains full text articles. We have employed\
    \ an existing span-based state-of-theart neural coreference resolution system\
    \ as a baseline system. We enhance the system with two different techniques to\
    \ capture longdistance coreferent pairs. Firstly, we filter noisy mentions based\
    \ on parse trees with increasing the number of antecedent candidates. Secondly,\
    \ instead of relying on the LSTMs, we integrate the highly expressive language\
    \ model\u2013BERT into our model. Experimental results show that our proposed\
    \ systems significantly outperform the baseline. The best performing system obtained\
    \ F-scores of 44%, 48%, 39%, 49%, 40%, and 57% on the test set with B3, BLANC,\
    \ CEAFE, CEAFM, LEA, and MUC metrics, respectively. Additionally, the proposed\
    \ model is able to detect coreferent pairs in long distances, even with a distance\
    \ of more than 200 sentences."
  address: Hong Kong, China
  author:
  - first: Hai-Long
    full: Hai-Long Trieu
    id: hai-long-trieu
    last: Trieu
  - first: Anh-Khoa
    full: Anh-Khoa Duong Nguyen
    id: anh-khoa-duong-nguyen
    last: Duong Nguyen
  - first: Nhung
    full: Nhung Nguyen
    id: nhung-nguyen
    last: Nguyen
  - first: Makoto
    full: Makoto Miwa
    id: makoto-miwa
    last: Miwa
  - first: Hiroya
    full: Hiroya Takamura
    id: hiroya-takamura
    last: Takamura
  - first: Sophia
    full: Sophia Ananiadou
    id: sophia-ananiadou
    last: Ananiadou
  author_string: Hai-Long Trieu, Anh-Khoa Duong Nguyen, Nhung Nguyen, Makoto Miwa,
    Hiroya Takamura, Sophia Ananiadou
  bibkey: trieu-etal-2019-coreference
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5727
  month: November
  page_first: '196'
  page_last: '205'
  pages: "196\u2013205"
  paper_id: '27'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5727.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5727.jpg
  title: Coreference Resolution in Full Text Articles with BERT and Syntax-based Mention
    Filtering
  title_html: Coreference Resolution in Full Text Articles with <span class="acl-fixed-case">BERT</span>
    and Syntax-based Mention Filtering
  url: https://www.aclweb.org/anthology/D19-5727
  year: '2019'
D19-5728:
  abstract: We present the approach taken by the TurkuNLP group in the CRAFT Structural
    Annotation task, a shared task on dependency parsing. Our approach builds primarily
    on the Turku neural parser, a native dependency parser that ranked among the best
    in the recent CoNLL tasks on parsing Universal Dependencies. To adapt the parser
    to the biomedical domain, we considered and evaluated a number of approaches,
    including the generation of custom word embeddings, combination with other in-domain
    resources, and the incorporation of information from named entity recognition.
    We achieved a labeled attachment score of 89.7%, the best result among task participants.
  address: Hong Kong, China
  author:
  - first: Thang Minh
    full: Thang Minh Ngo
    id: thang-minh-ngo
    last: Ngo
  - first: Jenna
    full: Jenna Kanerva
    id: jenna-kanerva
    last: Kanerva
  - first: Filip
    full: Filip Ginter
    id: filip-ginter
    last: Ginter
  - first: Sampo
    full: Sampo Pyysalo
    id: sampo-pyysalo
    last: Pyysalo
  author_string: Thang Minh Ngo, Jenna Kanerva, Filip Ginter, Sampo Pyysalo
  bibkey: ngo-etal-2019-neural
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5728
  month: November
  page_first: '206'
  page_last: '215'
  pages: "206\u2013215"
  paper_id: '28'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5728.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5728.jpg
  title: 'Neural Dependency Parsing of Biomedical Text: TurkuNLP entry in the CRAFT
    Structural Annotation Task'
  title_html: 'Neural Dependency Parsing of Biomedical Text: <span class="acl-fixed-case">T</span>urku<span
    class="acl-fixed-case">NLP</span> entry in the <span class="acl-fixed-case">CRAFT</span>
    Structural Annotation Task'
  url: https://www.aclweb.org/anthology/D19-5728
  year: '2019'
D19-5729:
  abstract: "BioNLP Open Shared Tasks (BioNLP-OST) is an international competition\
    \ organized to facilitate development and sharing of computational tasks of biomedical\
    \ text mining and solutions to them. For BioNLP-OST 2019, we introduced a new\
    \ mental health informatics task called \u201CRDoC Task\u201D, which is composed\
    \ of two subtasks: information retrieval and sentence extraction through National\
    \ Institutes of Mental Health\u2019s Research Domain Criteria framework. Five\
    \ and four teams around the world participated in the two tasks, respectively.\
    \ According to the performance on the two tasks, we observe that there is room\
    \ for improvement for text mining on brain research and mental illness."
  address: Hong Kong, China
  author:
  - first: Mohammad
    full: Mohammad Anani
    id: mohammad-anani
    last: Anani
  - first: Nazmul
    full: Nazmul Kazi
    id: nazmul-kazi
    last: Kazi
  - first: Matthew
    full: Matthew Kuntz
    id: matthew-kuntz
    last: Kuntz
  - first: Indika
    full: Indika Kahanda
    id: indika-kahanda
    last: Kahanda
  author_string: Mohammad Anani, Nazmul Kazi, Matthew Kuntz, Indika Kahanda
  bibkey: anani-etal-2019-rdoc
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5729
  month: November
  page_first: '216'
  page_last: '226'
  pages: "216\u2013226"
  paper_id: '29'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5729.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5729.jpg
  title: RDoC Task at BioNLP-OST 2019
  title_html: <span class="acl-fixed-case">RD</span>o<span class="acl-fixed-case">C</span>
    Task at <span class="acl-fixed-case">B</span>io<span class="acl-fixed-case">NLP</span>-<span
    class="acl-fixed-case">OST</span> 2019
  url: https://www.aclweb.org/anthology/D19-5729
  year: '2019'
D19-5730:
  abstract: This paper presents our system details and results of participation in
    the RDoC Tasks of BioNLP-OST 2019. Research Domain Criteria (RDoC) construct is
    a multi-dimensional and broad framework to describe mental health disorders by
    combining knowledge from genomics to behaviour. Non-availability of RDoC labelled
    dataset and tedious labelling process hinders the use of RDoC framework to reach
    its full potential in Biomedical research community and Healthcare industry. Therefore,
    Task-1 aims at retrieval and ranking of PubMed abstracts relevant to a given RDoC
    construct and Task-2 aims at extraction of the most relevant sentence from a given
    PubMed abstract. We investigate (1) attention based supervised neural topic model
    and SVM for retrieval and ranking of PubMed abstracts and, further utilize BM25
    and other relevance measures for re-ranking, (2) supervised and unsupervised sentence
    ranking models utilizing multi-view representations comprising of query-aware
    attention-based sentence representation (QAR), bag-of-words (BoW) and TF-IDF.
    Our best systems achieved 1st rank and scored 0.86 mAP and 0.58 macro average
    accuracy in Task-1 and Task-2 respectively.
  address: Hong Kong, China
  author:
  - first: Pankaj
    full: Pankaj Gupta
    id: pankaj-gupta
    last: Gupta
  - first: Yatin
    full: Yatin Chaudhary
    id: yatin-chaudhary
    last: Chaudhary
  - first: Hinrich
    full: "Hinrich Sch\xFCtze"
    id: hinrich-schutze
    last: "Sch\xFCtze"
  author_string: "Pankaj Gupta, Yatin Chaudhary, Hinrich Sch\xFCtze"
  bibkey: gupta-etal-2019-bionlp
  bibtype: inproceedings
  booktitle: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  booktitle_html: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks
  doi: 10.18653/v1/D19-5730
  month: November
  page_first: '227'
  page_last: '236'
  pages: "227\u2013236"
  paper_id: '30'
  parent_volume_id: D19-57
  pdf: https://www.aclweb.org/anthology/D19-5730.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5730.jpg
  title: 'BioNLP-OST 2019 RDoC Tasks: Multi-grain Neural Relevance Ranking Using Topics
    and Attention Based Query-Document-Sentence Interactions'
  title_html: '<span class="acl-fixed-case">B</span>io<span class="acl-fixed-case">NLP</span>-<span
    class="acl-fixed-case">OST</span> 2019 <span class="acl-fixed-case">RD</span>o<span
    class="acl-fixed-case">C</span> Tasks: Multi-grain Neural Relevance Ranking Using
    Topics and Attention Based Query-Document-Sentence Interactions'
  url: https://www.aclweb.org/anthology/D19-5730
  year: '2019'
D19-5800:
  address: Hong Kong, China
  author:
  - first: Adam
    full: Adam Fisch
    id: adam-fisch
    last: Fisch
  - first: Alon
    full: Alon Talmor
    id: alon-talmor
    last: Talmor
  - first: Robin
    full: Robin Jia
    id: robin-jia
    last: Jia
  - first: Minjoon
    full: Minjoon Seo
    id: minjoon-seo
    last: Seo
  - first: Eunsol
    full: Eunsol Choi
    id: eunsol-choi
    last: Choi
  - first: Danqi
    full: Danqi Chen
    id: danqi-chen
    last: Chen
  author_string: Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, Danqi
    Chen
  bibkey: emnlp-2019-machine
  bibtype: proceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  month: November
  paper_id: '0'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5800.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5800.jpg
  title: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  title_html: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  url: https://www.aclweb.org/anthology/D19-5800
  year: '2019'
D19-5801:
  abstract: We present the results of the Machine Reading for Question Answering (MRQA)
    2019 shared task on evaluating the generalization capabilities of reading comprehension
    systems. In this task, we adapted and unified 18 distinct question answering datasets
    into the same format. Among them, six datasets were made available for training,
    six datasets were made available for development, and the rest were hidden for
    final evaluation. Ten teams submitted systems, which explored various ideas including
    data sampling, multi-task learning, adversarial training and ensembling. The best
    system achieved an average F1 score of 72.5 on the 12 held-out datasets, 10.7
    absolute points higher than our initial baseline based on BERT.
  address: Hong Kong, China
  author:
  - first: Adam
    full: Adam Fisch
    id: adam-fisch
    last: Fisch
  - first: Alon
    full: Alon Talmor
    id: alon-talmor
    last: Talmor
  - first: Robin
    full: Robin Jia
    id: robin-jia
    last: Jia
  - first: Minjoon
    full: Minjoon Seo
    id: minjoon-seo
    last: Seo
  - first: Eunsol
    full: Eunsol Choi
    id: eunsol-choi
    last: Choi
  - first: Danqi
    full: Danqi Chen
    id: danqi-chen
    last: Chen
  author_string: Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, Danqi
    Chen
  bibkey: fisch-etal-2019-mrqa
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5801
  month: November
  page_first: '1'
  page_last: '13'
  pages: "1\u201313"
  paper_id: '1'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5801.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5801.jpg
  title: 'MRQA 2019 Shared Task: Evaluating Generalization in Reading Comprehension'
  title_html: '<span class="acl-fixed-case">MRQA</span> 2019 Shared Task: Evaluating
    Generalization in Reading Comprehension'
  url: https://www.aclweb.org/anthology/D19-5801
  year: '2019'
D19-5802:
  abstract: 'Most machine reading comprehension (MRC) models separately handle encoding
    and matching with different network architectures. In contrast, pretrained language
    models with Transformer layers, such as GPT (Radford et al., 2018) and BERT (Devlin
    et al., 2018), have achieved competitive performance on MRC. A research question
    that naturally arises is: apart from the benefits of pre-training, how many performance
    gain comes from the unified network architecture. In this work, we evaluate and
    analyze unifying encoding and matching components with Transformer for the MRC
    task. Experimental results on SQuAD show that the unified model outperforms previous
    networks that separately treat encoding and matching. We also introduce a metric
    to inspect whether a Transformer layer tends to perform encoding or matching.
    The analysis results show that the unified model learns different modeling strategies
    compared with previous manually-designed models.'
  address: Hong Kong, China
  author:
  - first: Hangbo
    full: Hangbo Bao
    id: hangbo-bao
    last: Bao
  - first: Li
    full: Li Dong
    id: li-dong
    last: Dong
  - first: Furu
    full: Furu Wei
    id: furu-wei
    last: Wei
  - first: Wenhui
    full: Wenhui Wang
    id: wenhui-wang
    last: Wang
  - first: Nan
    full: Nan Yang
    id: nan-yang
    last: Yang
  - first: Lei
    full: Lei Cui
    id: lei-cui
    last: Cui
  - first: Songhao
    full: Songhao Piao
    id: songhao-piao
    last: Piao
  - first: Ming
    full: Ming Zhou
    id: ming-zhou
    last: Zhou
  author_string: Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Lei Cui, Songhao
    Piao, Ming Zhou
  bibkey: bao-etal-2019-inspecting
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5802
  month: November
  page_first: '14'
  page_last: '18'
  pages: "14\u201318"
  paper_id: '2'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5802.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5802.jpg
  title: 'Inspecting Unification of Encoding and Matching with Transformer: A Case
    Study of Machine Reading Comprehension'
  title_html: 'Inspecting Unification of Encoding and Matching with Transformer: A
    Case Study of Machine Reading Comprehension'
  url: https://www.aclweb.org/anthology/D19-5802
  year: '2019'
D19-5803:
  abstract: Machine reading comprehension is a task related to Question-Answering
    where questions are not generic in scope but are related to a particular document.
    Recently very large corpora (SQuAD, MS MARCO) containing triplets (document, question,
    answer) were made available to the scientific community to develop supervised
    methods based on deep neural networks with promising results. These methods need
    very large training corpus to be efficient, however such kind of data only exists
    for English and Chinese at the moment. The aim of this study is the development
    of such resources for other languages by proposing to generate in a semi-automatic
    way questions from the semantic Frame analysis of large corpora. The collect of
    natural questions is reduced to a validation/test set. We applied this method
    on the CALOR-Frame French corpus to develop the CALOR-QUEST resource presented
    in this paper.
  address: Hong Kong, China
  author:
  - first: Frederic
    full: Frederic Bechet
    id: frederic-bechet
    last: Bechet
  - first: Cindy
    full: Cindy Aloui
    id: cindy-aloui
    last: Aloui
  - first: Delphine
    full: Delphine Charlet
    id: delphine-charlet
    last: Charlet
  - first: Geraldine
    full: Geraldine Damnati
    id: geraldine-damnati
    last: Damnati
  - first: Johannes
    full: Johannes Heinecke
    id: johannes-heinecke
    last: Heinecke
  - first: Alexis
    full: Alexis Nasr
    id: alexis-nasr
    last: Nasr
  - first: Frederic
    full: Frederic Herledan
    id: frederic-herledan
    last: Herledan
  author_string: Frederic Bechet, Cindy Aloui, Delphine Charlet, Geraldine Damnati,
    Johannes Heinecke, Alexis Nasr, Frederic Herledan
  bibkey: bechet-etal-2019-calor
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5803
  month: November
  page_first: '19'
  page_last: '26'
  pages: "19\u201326"
  paper_id: '3'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5803.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5803.jpg
  title: 'CALOR-QUEST : generating a training corpus for Machine Reading Comprehension
    models from shallow semantic annotations'
  title_html: '<span class="acl-fixed-case">CALOR</span>-<span class="acl-fixed-case">QUEST</span>
    : generating a training corpus for Machine Reading Comprehension models from shallow
    semantic annotations'
  url: https://www.aclweb.org/anthology/D19-5803
  year: '2019'
D19-5804:
  abstract: 'We focus on multiple-choice question answering (QA) tasks in subject
    areas such as science, where we require both broad background knowledge and the
    facts from the given subject-area reference corpus. In this work, we explore simple
    yet effective methods for exploiting two sources of external knowledge for subject-area
    QA. The first enriches the original subject-area reference corpus with relevant
    text snippets extracted from an open-domain resource (i.e., Wikipedia) that cover
    potentially ambiguous concepts in the question and answer options. As in other
    QA research, the second method simply increases the amount of training data by
    appending additional in-domain subject-area instances. Experiments on three challenging
    multiple-choice science QA tasks (i.e., ARC-Easy, ARC-Challenge, and OpenBookQA)
    demonstrate the effectiveness of our methods: in comparison to the previous state-of-the-art,
    we obtain absolute gains in accuracy of up to 8.1%, 13.0%, and 12.8%, respectively.
    While we observe consistent gains when we introduce knowledge from Wikipedia,
    we find that employing additional QA training instances is not uniformly helpful:
    performance degrades when the added instances exhibit a higher level of difficulty
    than the original training data. As one of the first studies on exploiting unstructured
    external knowledge for subject-area QA, we hope our methods, observations, and
    discussion of the exposed limitations may shed light on further developments in
    the area.'
  address: Hong Kong, China
  author:
  - first: Xiaoman
    full: Xiaoman Pan
    id: xiaoman-pan
    last: Pan
  - first: Kai
    full: Kai Sun
    id: kai-sun
    last: Sun
  - first: Dian
    full: Dian Yu
    id: dian-yu
    last: Yu
  - first: Jianshu
    full: Jianshu Chen
    id: jianshu-chen
    last: Chen
  - first: Heng
    full: Heng Ji
    id: heng-ji
    last: Ji
  - first: Claire
    full: Claire Cardie
    id: claire-cardie
    last: Cardie
  - first: Dong
    full: Dong Yu
    id: dong-yu
    last: Yu
  author_string: Xiaoman Pan, Kai Sun, Dian Yu, Jianshu Chen, Heng Ji, Claire Cardie,
    Dong Yu
  bibkey: pan-etal-2019-improving-question
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5804
  month: November
  page_first: '27'
  page_last: '37'
  pages: "27\u201337"
  paper_id: '4'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5804.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5804.jpg
  title: Improving Question Answering with External Knowledge
  title_html: Improving Question Answering with External Knowledge
  url: https://www.aclweb.org/anthology/D19-5804
  year: '2019'
D19-5805:
  abstract: In conversational machine comprehension, it has become one of the research
    hotspots integrating conversational history information through question reformulation
    for obtaining better answers. However, the existing question reformulation models
    are trained only using supervised question labels annotated by annotators without
    considering any feedback information from answers. In this paper, we propose a
    novel Answer-Supervised Question Reformulation (ASQR) model for enhancing conversational
    machine comprehension with reinforcement learning technology. ASQR utilizes a
    pointer-copy-based question reformulation model as an agent, takes an action to
    predict the next word, and observes a reward for the whole sentence state after
    generating the end-of-sequence token. The experimental results on QuAC dataset
    prove that our ASQR model is more effective in conversational machine comprehension.
    Moreover, pretraining is essential in reinforcement learning models, so we provide
    a high-quality annotated dataset for question reformulation by sampling a part
    of QuAC dataset.
  address: Hong Kong, China
  attachment:
  - filename: D19-5805.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5805.Attachment.zip
  author:
  - first: Qian
    full: Qian Li
    id: qian-li
    last: Li
  - first: Hui
    full: Hui Su
    id: hui-su
    last: Su
  - first: Cheng
    full: Cheng Niu
    id: cheng-niu
    last: Niu
  - first: Daling
    full: Daling Wang
    id: daling-wang
    last: Wang
  - first: Zekang
    full: Zekang Li
    id: zekang-li
    last: Li
  - first: Shi
    full: Shi Feng
    id: shi-feng
    last: Feng
  - first: Yifei
    full: Yifei Zhang
    id: yifei-zhang
    last: Zhang
  author_string: Qian Li, Hui Su, Cheng Niu, Daling Wang, Zekang Li, Shi Feng, Yifei
    Zhang
  bibkey: li-etal-2019-answer
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5805
  month: November
  page_first: '38'
  page_last: '47'
  pages: "38\u201347"
  paper_id: '5'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5805.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5805.jpg
  title: Answer-Supervised Question Reformulation for Enhancing Conversational Machine
    Comprehension
  title_html: Answer-Supervised Question Reformulation for Enhancing Conversational
    Machine Comprehension
  url: https://www.aclweb.org/anthology/D19-5805
  year: '2019'
D19-5806:
  abstract: A key challenge of multi-hop question answering (QA) in the open-domain
    setting is to accurately retrieve the supporting passages from a large corpus.
    Existing work on open-domain QA typically relies on off-the-shelf information
    retrieval (IR) techniques to retrieve answer passages, i.e., the passages containing
    the groundtruth answers. However, IR-based approaches are insufficient for multi-hop
    questions, as the topic of the second or further hops is not explicitly covered
    by the question. To resolve this issue, we introduce a new subproblem of open-domain
    multi-hop QA, which aims to recognize the bridge (i.e., the anchor that links
    to the answer passage) from the context of a set of start passages with a reading
    comprehension model. This model, the bridge reasoner, is trained with a weakly
    supervised signal and produces the candidate answer passages for the passage reader
    to extract the answer. On the full-wiki HotpotQA benchmark, we significantly improve
    the baseline method by 14 point F1. Without using any memory inefficient contextual
    embeddings, our result is also competitive with the state-of-the-art that applies
    BERT in multiple modules.
  address: Hong Kong, China
  author:
  - first: Wenhan
    full: Wenhan Xiong
    id: wenhan-xiong
    last: Xiong
  - first: Mo
    full: Mo Yu
    id: mo-yu
    last: Yu
  - first: Xiaoxiao
    full: Xiaoxiao Guo
    id: xiaoxiao-guo
    last: Guo
  - first: Hong
    full: Hong Wang
    id: hong-wang
    last: Wang
  - first: Shiyu
    full: Shiyu Chang
    id: shiyu-chang
    last: Chang
  - first: Murray
    full: Murray Campbell
    id: murray-campbell
    last: Campbell
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  author_string: Wenhan Xiong, Mo Yu, Xiaoxiao Guo, Hong Wang, Shiyu Chang, Murray
    Campbell, William Yang Wang
  bibkey: xiong-etal-2019-simple
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5806
  month: November
  page_first: '48'
  page_last: '52'
  pages: "48\u201352"
  paper_id: '6'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5806.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5806.jpg
  title: Simple yet Effective Bridge Reasoning for Open-Domain Multi-Hop Question
    Answering
  title_html: Simple yet Effective Bridge Reasoning for Open-Domain Multi-Hop Question
    Answering
  url: https://www.aclweb.org/anthology/D19-5806
  year: '2019'
D19-5807:
  abstract: Despite the remarkable progress on Machine Reading Comprehension (MRC)
    with the help of open-source datasets, recent studies indicate that most of the
    current MRC systems unfortunately suffer from weak robustness against adversarial
    samples. To address this issue, we attempt to take sentence syntax as the leverage
    in the answer predicting process which previously only takes account of phrase-level
    semantics. Furthermore, to better utilize the sentence syntax and improve the
    robustness, we propose a Syntactic Leveraging Network, which is designed to deal
    with adversarial samples by exploiting the syntactic elements of a question. The
    experiment results indicate that our method is promising for improving the generalization
    and robustness of MRC models against the influence of adversarial samples, with
    performance well-maintained.
  address: Hong Kong, China
  author:
  - first: Bowen
    full: Bowen Wu
    id: bowen-wu
    last: Wu
  - first: Haoyang
    full: Haoyang Huang
    id: haoyang-huang
    last: Huang
  - first: Zongsheng
    full: Zongsheng Wang
    id: zongsheng-wang
    last: Wang
  - first: Qihang
    full: Qihang Feng
    id: qihang-feng
    last: Feng
  - first: Jingsong
    full: Jingsong Yu
    id: jingsong-yu
    last: Yu
  - first: Baoxun
    full: Baoxun Wang
    id: baoxun-wang
    last: Wang
  author_string: Bowen Wu, Haoyang Huang, Zongsheng Wang, Qihang Feng, Jingsong Yu,
    Baoxun Wang
  bibkey: wu-etal-2019-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5807
  month: November
  page_first: '53'
  page_last: '57'
  pages: "53\u201357"
  paper_id: '7'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5807.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5807.jpg
  title: Improving the Robustness of Deep Reading Comprehension Models by Leveraging
    Syntax Prior
  title_html: Improving the Robustness of Deep Reading Comprehension Models by Leveraging
    Syntax Prior
  url: https://www.aclweb.org/anthology/D19-5807
  year: '2019'
D19-5808:
  abstract: "A key component of successfully reading a passage of text is the ability\
    \ to apply knowledge gained from the passage to a new situation. In order to facilitate\
    \ progress on this kind of reading, we present ROPES, a challenging benchmark\
    \ for reading comprehension targeting Reasoning Over Paragraph Effects in Situations.\
    \ We target expository language describing causes and effects (e.g., \u201Canimal\
    \ pollinators increase efficiency of fertilization in flowers\u201D), as they\
    \ have clear implications for new situations. A system is presented a background\
    \ passage containing at least one of these relations, a novel situation that uses\
    \ this background, and questions that require reasoning about effects of the relationships\
    \ in the background passage in the context of the situation. We collect background\
    \ passages from science textbooks and Wikipedia that contain such phenomena, and\
    \ ask crowd workers to author situations, questions, and answers, resulting in\
    \ a 14,322 question dataset. We analyze the challenges of this task and evaluate\
    \ the performance of state-of-the-art reading comprehension models. The best model\
    \ performs only slightly better than randomly guessing an answer of the correct\
    \ type, at 61.6% F1, well below the human performance of 89.0%."
  address: Hong Kong, China
  attachment:
  - filename: D19-5808.Attachment.tgz
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5808.Attachment.tgz
  author:
  - first: Kevin
    full: Kevin Lin
    id: kevin-lin
    last: Lin
  - first: Oyvind
    full: Oyvind Tafjord
    id: oyvind-tafjord
    last: Tafjord
  - first: Peter
    full: Peter Clark
    id: peter-clark
    last: Clark
  - first: Matt
    full: Matt Gardner
    id: matt-gardner
    last: Gardner
  author_string: Kevin Lin, Oyvind Tafjord, Peter Clark, Matt Gardner
  bibkey: lin-etal-2019-reasoning
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5808
  month: November
  page_first: '58'
  page_last: '62'
  pages: "58\u201362"
  paper_id: '8'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5808.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5808.jpg
  title: Reasoning Over Paragraph Effects in Situations
  title_html: Reasoning Over Paragraph Effects in Situations
  url: https://www.aclweb.org/anthology/D19-5808
  year: '2019'
D19-5809:
  abstract: Conversational question generation is a novel area of NLP research which
    has a range of potential applications. This paper is first to presents a framework
    for conversational question generation that is unaware of the corresponding answers.
    To properly generate a question coherent to the grounding text and the current
    conversation history, the proposed framework first locates the focus of a question
    in the text passage, and then identifies the question pattern that leads the sequential
    generation of the words in a question. The experiments using the CoQA dataset
    demonstrate that the quality of generated questions greatly improves if the question
    foci and the question patterns are correctly identified. In addition, it was shown
    that the question foci, even estimated with a reasonable accuracy, could contribute
    to the quality improvement. These results established that our research direction
    may be promising, but at the same time revealed that the identification of question
    patterns is a challenging issue, and it has to be largely refined to achieve a
    better quality in the end-to-end automatic question generation.
  address: Hong Kong, China
  author:
  - first: Mao
    full: Mao Nakanishi
    id: mao-nakanishi
    last: Nakanishi
  - first: Tetsunori
    full: Tetsunori Kobayashi
    id: tetsunori-kobayashi
    last: Kobayashi
  - first: Yoshihiko
    full: Yoshihiko Hayashi
    id: yoshihiko-hayashi
    last: Hayashi
  author_string: Mao Nakanishi, Tetsunori Kobayashi, Yoshihiko Hayashi
  bibkey: nakanishi-etal-2019-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5809
  month: November
  page_first: '63'
  page_last: '71'
  pages: "63\u201371"
  paper_id: '9'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5809.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5809.jpg
  title: Towards Answer-unaware Conversational Question Generation
  title_html: Towards Answer-unaware Conversational Question Generation
  url: https://www.aclweb.org/anthology/D19-5809
  year: '2019'
D19-5810:
  abstract: 'We demonstrate the viability of knowledge transfer between two related
    tasks: machine reading comprehension (MRC) and query-based text summarization.
    Using an MRC model trained on the SQuAD1.1 dataset as a core system component,
    we first build an extractive query-based summarizer. For better precision, this
    summarizer also compresses the output of the MRC model using a novel sentence
    compression technique. We further leverage pre-trained machine translation systems
    to abstract our extracted summaries. Our models achieve state-of-the-art results
    on the publicly available CNN/Daily Mail and Debatepedia datasets, and can serve
    as simple yet powerful baselines for future systems. We also hope that these results
    will encourage research on transfer learning from large MRC corpora to query-based
    summarization.'
  address: Hong Kong, China
  author:
  - first: Elozino
    full: Elozino Egonmwan
    id: elozino-egonmwan
    last: Egonmwan
  - first: Vittorio
    full: Vittorio Castelli
    id: vittorio-castelli
    last: Castelli
  - first: Md Arafat
    full: Md Arafat Sultan
    id: md-arafat-sultan
    last: Sultan
  author_string: Elozino Egonmwan, Vittorio Castelli, Md Arafat Sultan
  bibkey: egonmwan-etal-2019-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5810
  month: November
  page_first: '72'
  page_last: '77'
  pages: "72\u201377"
  paper_id: '10'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5810.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5810.jpg
  title: Cross-Task Knowledge Transfer for Query-Based Text Summarization
  title_html: Cross-Task Knowledge Transfer for Query-Based Text Summarization
  url: https://www.aclweb.org/anthology/D19-5810
  year: '2019'
D19-5811:
  abstract: We present a system for answering questions based on the full text of
    books (BookQA), which first selects book passages given a question at hand, and
    then uses a memory network to reason and predict an answer. To improve generalization,
    we pretrain our memory network using artificial questions generated from book
    sentences. We experiment with the recently published NarrativeQA corpus, on the
    subset of Who questions, which expect book characters as answers. We experimentally
    show that BERT-based retrieval and pretraining improve over baseline results significantly.
    At the same time, we confirm that NarrativeQA is a highly challenging data set,
    and that there is need for novel research in order to achieve high-precision BookQA
    results. We analyze some of the bottlenecks of the current approach, and we argue
    that more research is needed on text representation, retrieval of relevant passages,
    and reasoning, including commonsense knowledge.
  address: Hong Kong, China
  author:
  - first: Stefanos
    full: Stefanos Angelidis
    id: stefanos-angelidis
    last: Angelidis
  - first: Lea
    full: Lea Frermann
    id: lea-frermann
    last: Frermann
  - first: Diego
    full: Diego Marcheggiani
    id: diego-marcheggiani
    last: Marcheggiani
  - first: Roi
    full: Roi Blanco
    id: roi-blanco
    last: Blanco
  - first: "Llu\xEDs"
    full: "Llu\xEDs M\xE0rquez"
    id: lluis-marquez
    last: "M\xE0rquez"
  author_string: "Stefanos Angelidis, Lea Frermann, Diego Marcheggiani, Roi Blanco,\
    \ Llu\xEDs M\xE0rquez"
  bibkey: angelidis-etal-2019-book
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5811
  month: November
  page_first: '78'
  page_last: '85'
  pages: "78\u201385"
  paper_id: '11'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5811.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5811.jpg
  title: 'Book QA: Stories of Challenges and Opportunities'
  title_html: 'Book <span class="acl-fixed-case">QA</span>: Stories of Challenges
    and Opportunities'
  url: https://www.aclweb.org/anthology/D19-5811
  year: '2019'
D19-5812:
  abstract: Conversational machine comprehension requires deep understanding of the
    dialogue flow, and the prior work proposed FlowQA to implicitly model the context
    representations in reasoning for better understanding. This paper proposes to
    explicitly model the information gain through the dialogue reasoning in order
    to allow the model to focus on more informative cues. The proposed model achieves
    the state-of-the-art performance in a conversational QA dataset QuAC and sequential
    instruction understanding dataset SCONE, which shows the effectiveness of the
    proposed mechanism and demonstrate its capability of generalization to different
    QA models and tasks.
  address: Hong Kong, China
  attachment:
  - filename: D19-5812.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5812.Attachment.pdf
  author:
  - first: Yi-Ting
    full: Yi-Ting Yeh
    id: yi-ting-yeh
    last: Yeh
  - first: Yun-Nung
    full: Yun-Nung Chen
    id: yun-nung-chen
    last: Chen
  author_string: Yi-Ting Yeh, Yun-Nung Chen
  bibkey: yeh-chen-2019-flowdelta
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5812
  month: November
  page_first: '86'
  page_last: '90'
  pages: "86\u201390"
  paper_id: '12'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5812.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5812.jpg
  title: 'FlowDelta: Modeling Flow Information Gain in Reasoning for Conversational
    Machine Comprehension'
  title_html: '<span class="acl-fixed-case">F</span>low<span class="acl-fixed-case">D</span>elta:
    Modeling Flow Information Gain in Reasoning for Conversational Machine Comprehension'
  url: https://www.aclweb.org/anthology/D19-5812
  year: '2019'
D19-5813:
  abstract: General Question Answering (QA) systems over texts require the multi-hop
    reasoning capability, i.e. the ability to reason with information collected from
    multiple passages to derive the answer. In this paper we conduct a systematic
    analysis to assess such an ability of various existing models proposed for multi-hop
    QA tasks. Specifically, our analysis investigates that whether providing the full
    reasoning chain of multiple passages, instead of just one final passage where
    the answer appears, could improve the performance of the existing QA models. Surprisingly,
    when using the additional evidence passages, the improvements of all the existing
    multi-hop reading approaches are rather limited, with the highest error reduction
    of 5.8% on F1 (corresponding to 1.3% improvement) from the BERT model. To better
    understand whether the reasoning chains indeed could help find the correct answers,
    we further develop a co-matching-based method that leads to 13.1% error reduction
    with passage chains when applied to two of our base readers (including BERT).
    Our results demonstrate the existence of the potential improvement using explicit
    multi-hop reasoning and the necessity to develop models with better reasoning
    abilities.
  address: Hong Kong, China
  author:
  - first: Haoyu
    full: Haoyu Wang
    id: haoyu-wang
    last: Wang
  - first: Mo
    full: Mo Yu
    id: mo-yu
    last: Yu
  - first: Xiaoxiao
    full: Xiaoxiao Guo
    id: xiaoxiao-guo
    last: Guo
  - first: Rajarshi
    full: Rajarshi Das
    id: rajarshi-das
    last: Das
  - first: Wenhan
    full: Wenhan Xiong
    id: wenhan-xiong
    last: Xiong
  - first: Tian
    full: Tian Gao
    id: tian-gao
    last: Gao
  author_string: Haoyu Wang, Mo Yu, Xiaoxiao Guo, Rajarshi Das, Wenhan Xiong, Tian
    Gao
  bibkey: wang-etal-2019-multi-hop
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5813
  month: November
  page_first: '91'
  page_last: '97'
  pages: "91\u201397"
  paper_id: '13'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5813.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5813.jpg
  title: Do Multi-hop Readers Dream of Reasoning Chains?
  title_html: Do Multi-hop Readers Dream of Reasoning Chains?
  url: https://www.aclweb.org/anthology/D19-5813
  year: '2019'
D19-5814:
  abstract: 'To improve the accuracy of predicate-argument structure (PAS) analysis,
    large-scale training data and knowledge for PAS analysis are indispensable. We
    focus on a specific domain, specifically Japanese blogs on driving, and construct
    two wide-coverage datasets as a form of QA using crowdsourcing: a PAS-QA dataset
    and a reading comprehension QA (RC-QA) dataset. We train a machine comprehension
    (MC) model based on these datasets to perform PAS analysis. Our experiments show
    that a stepwise training method is the most effective, which pre-trains an MC
    model based on the RC-QA dataset to acquire domain knowledge and then fine-tunes
    based on the PAS-QA dataset.'
  address: Hong Kong, China
  author:
  - first: Norio
    full: Norio Takahashi
    id: norio-takahashi
    last: Takahashi
  - first: Tomohide
    full: Tomohide Shibata
    id: tomohide-shibata
    last: Shibata
  - first: Daisuke
    full: Daisuke Kawahara
    id: daisuke-kawahara
    last: Kawahara
  - first: Sadao
    full: Sadao Kurohashi
    id: sadao-kurohashi
    last: Kurohashi
  author_string: Norio Takahashi, Tomohide Shibata, Daisuke Kawahara, Sadao Kurohashi
  bibkey: takahashi-etal-2019-machine
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5814
  month: November
  page_first: '98'
  page_last: '104'
  pages: "98\u2013104"
  paper_id: '14'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5814.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5814.jpg
  title: Machine Comprehension Improves Domain-Specific Japanese Predicate-Argument
    Structure Analysis
  title_html: Machine Comprehension Improves Domain-Specific <span class="acl-fixed-case">J</span>apanese
    Predicate-Argument Structure Analysis
  url: https://www.aclweb.org/anthology/D19-5814
  year: '2019'
D19-5815:
  abstract: "Machine reading comprehension, the task of evaluating a machine\u2019\
    s ability to comprehend a passage of text, has seen a surge in popularity in recent\
    \ years. There are many datasets that are targeted at reading comprehension, and\
    \ many systems that perform as well as humans on some of these datasets. Despite\
    \ all of this interest, there is no work that systematically defines what reading\
    \ comprehension is. In this work, we justify a question answering approach to\
    \ reading comprehension and describe the various kinds of questions one might\
    \ use to more fully test a system\u2019s comprehension of a passage, moving beyond\
    \ questions that only probe local predicate-argument structures. The main pitfall\
    \ of this approach is that questions can easily have surface cues or other biases\
    \ that allow a model to shortcut the intended reasoning process. We discuss ways\
    \ proposed in current literature to mitigate these shortcuts, and we conclude\
    \ with recommendations for future dataset collection efforts."
  address: Hong Kong, China
  author:
  - first: Matt
    full: Matt Gardner
    id: matt-gardner
    last: Gardner
  - first: Jonathan
    full: Jonathan Berant
    id: jonathan-berant
    last: Berant
  - first: Hannaneh
    full: Hannaneh Hajishirzi
    id: hannaneh-hajishirzi
    last: Hajishirzi
  - first: Alon
    full: Alon Talmor
    id: alon-talmor
    last: Talmor
  - first: Sewon
    full: Sewon Min
    id: sewon-min
    last: Min
  author_string: Matt Gardner, Jonathan Berant, Hannaneh Hajishirzi, Alon Talmor,
    Sewon Min
  bibkey: gardner-etal-2019-making
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5815
  month: November
  page_first: '105'
  page_last: '112'
  pages: "105\u2013112"
  paper_id: '15'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5815.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5815.jpg
  title: On Making Reading Comprehension More Comprehensive
  title_html: On Making Reading Comprehension More Comprehensive
  url: https://www.aclweb.org/anthology/D19-5815
  year: '2019'
D19-5816:
  abstract: "Multi-hop question answering (QA) requires an information retrieval (IR)\
    \ system that can find multiple supporting evidence needed to answer the question,\
    \ making the retrieval process very challenging. This paper introduces an IR technique\
    \ that uses information of entities present in the initially retrieved evidence\
    \ to learn to \u2018hop\u2019 to other relevant evidence. In a setting, with more\
    \ than 5 million Wikipedia paragraphs, our approach leads to significant boost\
    \ in retrieval performance. The retrieved evidence also increased the performance\
    \ of an existing QA model (without any training) on the benchmark by 10.59 F1."
  address: Hong Kong, China
  author:
  - first: Rajarshi
    full: Rajarshi Das
    id: rajarshi-das
    last: Das
  - first: Ameya
    full: Ameya Godbole
    id: ameya-godbole
    last: Godbole
  - first: Dilip
    full: Dilip Kavarthapu
    id: dilip-kavarthapu
    last: Kavarthapu
  - first: Zhiyu
    full: Zhiyu Gong
    id: zhiyu-gong
    last: Gong
  - first: Abhishek
    full: Abhishek Singhal
    id: abhishek-singhal
    last: Singhal
  - first: Mo
    full: Mo Yu
    id: mo-yu
    last: Yu
  - first: Xiaoxiao
    full: Xiaoxiao Guo
    id: xiaoxiao-guo
    last: Guo
  - first: Tian
    full: Tian Gao
    id: tian-gao
    last: Gao
  - first: Hamed
    full: Hamed Zamani
    id: hamed-zamani
    last: Zamani
  - first: Manzil
    full: Manzil Zaheer
    id: manzil-zaheer
    last: Zaheer
  - first: Andrew
    full: Andrew McCallum
    id: andrew-mccallum
    last: McCallum
  author_string: Rajarshi Das, Ameya Godbole, Dilip Kavarthapu, Zhiyu Gong, Abhishek
    Singhal, Mo Yu, Xiaoxiao Guo, Tian Gao, Hamed Zamani, Manzil Zaheer, Andrew McCallum
  bibkey: das-etal-2019-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5816
  month: November
  page_first: '113'
  page_last: '118'
  pages: "113\u2013118"
  paper_id: '16'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5816.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5816.jpg
  title: Multi-step Entity-centric Information Retrieval for Multi-Hop Question Answering
  title_html: Multi-step Entity-centric Information Retrieval for Multi-Hop Question
    Answering
  url: https://www.aclweb.org/anthology/D19-5816
  year: '2019'
D19-5817:
  abstract: As the complexity of question answering (QA) datasets evolve, moving away
    from restricted formats like span extraction and multiple-choice (MC) to free-form
    answer generation, it is imperative to understand how well current metrics perform
    in evaluating QA. This is especially important as existing metrics (BLEU, ROUGE,
    METEOR, and F1) are computed using n-gram similarity and have a number of well-known
    drawbacks. In this work, we study the suitability of existing metrics in QA. For
    generative QA, we show that while current metrics do well on existing datasets,
    converting multiple-choice datasets into free-response datasets is challenging
    for current metrics. We also look at span-based QA, where F1 is a reasonable metric.
    We show that F1 may not be suitable for all extractive QA tasks depending on the
    answer types. Our study suggests that while current metrics may be suitable for
    existing QA datasets, they limit the complexity of QA datasets that can be created.
    This is especially true in the context of free-form QA, where we would like our
    models to be able to generate more complex and abstractive answers, thus necessitating
    new metrics that go beyond n-gram based matching. As a step towards a better QA
    metric, we explore using BERTScore, a recently proposed metric for evaluating
    translation, for QA. We find that although it fails to provide stronger correlation
    with human judgements, future work focused on tailoring a BERT-based metric to
    QA evaluation may prove fruitful.
  address: Hong Kong, China
  author:
  - first: Anthony
    full: Anthony Chen
    id: anthony-chen
    last: Chen
  - first: Gabriel
    full: Gabriel Stanovsky
    id: gabriel-stanovsky
    last: Stanovsky
  - first: Sameer
    full: Sameer Singh
    id: sameer-singh
    last: Singh
  - first: Matt
    full: Matt Gardner
    id: matt-gardner
    last: Gardner
  author_string: Anthony Chen, Gabriel Stanovsky, Sameer Singh, Matt Gardner
  bibkey: chen-etal-2019-evaluating
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5817
  month: November
  page_first: '119'
  page_last: '124'
  pages: "119\u2013124"
  paper_id: '17'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5817.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5817.jpg
  title: Evaluating Question Answering Evaluation
  title_html: Evaluating Question Answering Evaluation
  url: https://www.aclweb.org/anthology/D19-5817
  year: '2019'
D19-5818:
  abstract: The field of question answering (QA) has seen rapid growth in new tasks
    and modeling approaches in recent years. Large scale datasets and focus on challenging
    linguistic phenomena have driven development in neural models, some of which have
    achieved parity with human performance in limited cases. However, an examination
    of state-of-the-art model output reveals that a gap remains in reasoning ability
    compared to a human, and performance tends to degrade when models are exposed
    to less-constrained tasks. We are interested in more clearly defining the strengths
    and limitations of leading models across diverse QA challenges, intending to help
    future researchers with identifying pathways to generalizable performance. We
    conduct extensive qualitative and quantitative analyses on the results of four
    models across four datasets and relate common errors to model capabilities. We
    also illustrate limitations in the datasets we examine and discuss a way forward
    for achieving generalizable models and datasets that broadly test QA capabilities.
  address: Hong Kong, China
  attachment:
  - filename: D19-5818.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5818.Attachment.pdf
  author:
  - first: Hemant
    full: Hemant Pugaliya
    id: hemant-pugaliya
    last: Pugaliya
  - first: James
    full: James Route
    id: james-route
    last: Route
  - first: Kaixin
    full: Kaixin Ma
    id: kaixin-ma
    last: Ma
  - first: Yixuan
    full: Yixuan Geng
    id: yixuan-geng
    last: Geng
  - first: Eric
    full: Eric Nyberg
    id: eric-nyberg
    last: Nyberg
  author_string: Hemant Pugaliya, James Route, Kaixin Ma, Yixuan Geng, Eric Nyberg
  bibkey: pugaliya-etal-2019-bend
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5818
  month: November
  page_first: '125'
  page_last: '136'
  pages: "125\u2013136"
  paper_id: '18'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5818.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5818.jpg
  title: "Bend but Don\u2019t Break? Multi-Challenge Stress Test for QA Models"
  title_html: "Bend but Don\u2019t Break? Multi-Challenge Stress Test for <span class=\"\
    acl-fixed-case\">QA</span> Models"
  url: https://www.aclweb.org/anthology/D19-5818
  year: '2019'
D19-5819:
  abstract: Popular QA benchmarks like SQuAD have driven progress on the task of identifying
    answer spans within a specific passage, with models now surpassing human performance.
    However, retrieving relevant answers from a huge corpus of documents is still
    a challenging problem, and places different requirements on the model architecture.
    There is growing interest in developing scalable answer retrieval models trained
    end-to-end, bypassing the typical document retrieval step. In this paper, we introduce
    Retrieval Question-Answering (ReQA), a benchmark for evaluating large-scale sentence-level
    answer retrieval models. We establish baselines using both neural encoding models
    as well as classical information retrieval techniques. We release our evaluation
    code to encourage further work on this challenging task.
  address: Hong Kong, China
  author:
  - first: Amin
    full: Amin Ahmad
    id: amin-ahmad
    last: Ahmad
  - first: Noah
    full: Noah Constant
    id: noah-constant
    last: Constant
  - first: Yinfei
    full: Yinfei Yang
    id: yinfei-yang
    last: Yang
  - first: Daniel
    full: Daniel Cer
    id: daniel-cer
    last: Cer
  author_string: Amin Ahmad, Noah Constant, Yinfei Yang, Daniel Cer
  bibkey: ahmad-etal-2019-reqa
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5819
  month: November
  page_first: '137'
  page_last: '146'
  pages: "137\u2013146"
  paper_id: '19'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5819.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5819.jpg
  title: 'ReQA: An Evaluation for End-to-End Answer Retrieval Models'
  title_html: '<span class="acl-fixed-case">R</span>e<span class="acl-fixed-case">QA</span>:
    An Evaluation for End-to-End Answer Retrieval Models'
  url: https://www.aclweb.org/anthology/D19-5819
  year: '2019'
D19-5820:
  abstract: "Reading comprehension is one of the crucial tasks for furthering research\
    \ in natural language understanding. A lot of diverse reading comprehension datasets\
    \ have recently been introduced to study various phenomena in natural language,\
    \ ranging from simple paraphrase matching and entity typing to entity tracking\
    \ and understanding the implications of the context. Given the availability of\
    \ many such datasets, comprehensive and reliable evaluation is tedious and time-consuming\
    \ for researchers working on this problem. We present an evaluation server, ORB,\
    \ that reports performance on seven diverse reading comprehension datasets, encouraging\
    \ and facilitating testing a single model\u2019s capability in understanding a\
    \ wide variety of reading phenomena. The evaluation server places no restrictions\
    \ on how models are trained, so it is a suitable test bed for exploring training\
    \ paradigms and representation learning for general reading facility. As more\
    \ suitable datasets are released, they will be added to the evaluation server.\
    \ We also collect and include synthetic augmentations for these datasets, testing\
    \ how well models can handle out-of-domain questions."
  address: Hong Kong, China
  author:
  - first: Dheeru
    full: Dheeru Dua
    id: dheeru-dua
    last: Dua
  - first: Ananth
    full: Ananth Gottumukkala
    id: ananth-gottumukkala
    last: Gottumukkala
  - first: Alon
    full: Alon Talmor
    id: alon-talmor
    last: Talmor
  - first: Sameer
    full: Sameer Singh
    id: sameer-singh
    last: Singh
  - first: Matt
    full: Matt Gardner
    id: matt-gardner
    last: Gardner
  author_string: Dheeru Dua, Ananth Gottumukkala, Alon Talmor, Sameer Singh, Matt
    Gardner
  bibkey: dua-etal-2019-comprehensive
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5820
  month: November
  page_first: '147'
  page_last: '153'
  pages: "147\u2013153"
  paper_id: '20'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5820.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5820.jpg
  title: Comprehensive Multi-Dataset Evaluation of Reading Comprehension
  title_html: Comprehensive Multi-Dataset Evaluation of Reading Comprehension
  url: https://www.aclweb.org/anthology/D19-5820
  year: '2019'
D19-5821:
  abstract: In this study, we investigate the employment of the pre-trained BERT language
    model to tackle question generation tasks. We introduce three neural architectures
    built on top of BERT for question generation tasks. The first one is a straightforward
    BERT employment, which reveals the defects of directly using BERT for text generation.
    Accordingly, we propose another two models by restructuring our BERT employment
    into a sequential manner for taking information from previous decoded results.
    Our models are trained and evaluated on the recent question-answering dataset
    SQuAD. Experiment results show that our best model yields state-of-the-art performance
    which advances the BLEU 4 score of the existing best models from 16.85 to 22.17.
  address: Hong Kong, China
  author:
  - first: Ying-Hong
    full: Ying-Hong Chan
    id: ying-hong-chan
    last: Chan
  - first: Yao-Chung
    full: Yao-Chung Fan
    id: yao-chung-fan
    last: Fan
  author_string: Ying-Hong Chan, Yao-Chung Fan
  bibkey: chan-fan-2019-recurrent
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5821
  month: November
  page_first: '154'
  page_last: '162'
  pages: "154\u2013162"
  paper_id: '21'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5821.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5821.jpg
  title: A Recurrent BERT-based Model for Question Generation
  title_html: A Recurrent <span class="acl-fixed-case">BERT</span>-based Model for
    Question Generation
  url: https://www.aclweb.org/anthology/D19-5821
  year: '2019'
D19-5822:
  abstract: 'Question Generation (QG) is a Natural Language Processing (NLP) task
    that aids advances in Question Answering (QA) and conversational assistants. Existing
    models focus on generating a question based on a text and possibly the answer
    to the generated question. They need to determine the type of interrogative word
    to be generated while having to pay attention to the grammar and vocabulary of
    the question. In this work, we propose Interrogative-Word-Aware Question Generation
    (IWAQG), a pipelined system composed of two modules: an interrogative word classifier
    and a QG model. The first module predicts the interrogative word that is provided
    to the second module to create the question. Owing to an increased recall of deciding
    the interrogative words to be used for the generated questions, the proposed model
    achieves new state-of-the-art results on the task of QG in SQuAD, improving from
    46.58 to 47.69 in BLEU-1, 17.55 to 18.53 in BLEU-4, 21.24 to 22.33 in METEOR,
    and from 44.53 to 46.94 in ROUGE-L.'
  address: Hong Kong, China
  author:
  - first: Junmo
    full: Junmo Kang
    id: junmo-kang
    last: Kang
  - first: Haritz
    full: Haritz Puerto San Roman
    id: haritz-puerto-san-roman
    last: Puerto San Roman
  - first: Sung-Hyon
    full: Sung-Hyon Myaeng
    id: sung-hyon-myaeng
    last: Myaeng
  author_string: Junmo Kang, Haritz Puerto San Roman, Sung-Hyon Myaeng
  bibkey: kang-etal-2019-know
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5822
  month: November
  page_first: '163'
  page_last: '171'
  pages: "163\u2013171"
  paper_id: '22'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5822.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5822.jpg
  title: 'Let Me Know What to Ask: Interrogative-Word-Aware Question Generation'
  title_html: 'Let Me Know What to Ask: Interrogative-Word-Aware Question Generation'
  url: https://www.aclweb.org/anthology/D19-5822
  year: '2019'
D19-5823:
  abstract: Although advances in neural architectures for NLP problems as well as
    unsupervised pre-training have led to substantial improvements on question answering
    and natural language inference, understanding of and reasoning over long texts
    still poses a substantial challenge. Here, we consider the task of question answering
    from full narratives (e.g., books or movie scripts), or their summaries, tackling
    the NarrativeQA challenge (NQA; Kocisky et al. (2018)). We introduce a heuristic
    extractive version of the data set, which allows us to approach the more feasible
    problem of answer extraction (rather than generation). We train systems for passage
    retrieval as well as answer span prediction using this data set. We use pre-trained
    BERT embeddings for injecting prior knowledge into our system. We show that our
    setup leads to state of the art performance on summary-level QA. On QA from full
    narratives, our model outperforms previous models on the METEOR metric. We analyze
    the relative contributions of pre-trained embeddings and the extractive training
    paradigm, and provide a detailed error analysis.
  address: Hong Kong, China
  author:
  - first: Lea
    full: Lea Frermann
    id: lea-frermann
    last: Frermann
  author_string: Lea Frermann
  bibkey: frermann-2019-extractive
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5823
  month: November
  page_first: '172'
  page_last: '182'
  pages: "172\u2013182"
  paper_id: '23'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5823.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5823.jpg
  title: Extractive NarrativeQA with Heuristic Pre-Training
  title_html: Extractive <span class="acl-fixed-case">N</span>arrative<span class="acl-fixed-case">QA</span>
    with Heuristic Pre-Training
  url: https://www.aclweb.org/anthology/D19-5823
  year: '2019'
D19-5824:
  abstract: "This paper describes our model for the reading comprehension task of\
    \ the MRQA shared task. We propose CLER, which stands for Cross-task Learning\
    \ with Expert Representation for the generalization of reading and understanding.\
    \ To generalize its capabilities, the proposed model is composed of three key\
    \ ideas: multi-task learning, mixture of experts, and ensemble. In-domain datasets\
    \ are used to train and validate our model, and other out-of-domain datasets are\
    \ used to validate the generalization of our model\u2019s performances. In a submission\
    \ run result, the proposed model achieved an average F1 score of 66.1 % in the\
    \ out-of-domain setting, which is a 4.3 percentage point improvement over the\
    \ official BERT baseline model."
  address: Hong Kong, China
  author:
  - first: Takumi
    full: Takumi Takahashi
    id: takumi-takahashi
    last: Takahashi
  - first: Motoki
    full: Motoki Taniguchi
    id: motoki-taniguchi
    last: Taniguchi
  - first: Tomoki
    full: Tomoki Taniguchi
    id: tomoki-taniguchi
    last: Taniguchi
  - first: Tomoko
    full: Tomoko Ohkuma
    id: tomoko-ohkuma
    last: Ohkuma
  author_string: Takumi Takahashi, Motoki Taniguchi, Tomoki Taniguchi, Tomoko Ohkuma
  bibkey: takahashi-etal-2019-cler
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5824
  month: November
  page_first: '183'
  page_last: '190'
  pages: "183\u2013190"
  paper_id: '24'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5824.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5824.jpg
  title: 'CLER: Cross-task Learning with Expert Representation to Generalize Reading
    and Understanding'
  title_html: '<span class="acl-fixed-case">CLER</span>: Cross-task Learning with
    Expert Representation to Generalize Reading and Understanding'
  url: https://www.aclweb.org/anthology/D19-5824
  year: '2019'
D19-5825:
  abstract: The model submitted works as follows. When supplied a question and a passage
    it makes use of the BERT embedding along with the hierarchical attention model
    which consists of 2 parts, the co-attention and the self-attention, to locate
    a continuous span of the passage that is the answer to the question.
  address: Hong Kong, China
  author:
  - first: Reham
    full: Reham Osama
    id: reham-osama
    last: Osama
  - first: Nagwa
    full: Nagwa El-Makky
    id: nagwa-m-el-makky
    last: El-Makky
  - first: Marwan
    full: Marwan Torki
    id: marwan-torki
    last: Torki
  author_string: Reham Osama, Nagwa El-Makky, Marwan Torki
  bibkey: osama-etal-2019-question
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5825
  month: November
  page_first: '191'
  page_last: '195'
  pages: "191\u2013195"
  paper_id: '25'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5825.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5825.jpg
  title: Question Answering Using Hierarchical Attention on Top of BERT Features
  title_html: Question Answering Using Hierarchical Attention on Top of <span class="acl-fixed-case">BERT</span>
    Features
  url: https://www.aclweb.org/anthology/D19-5825
  year: '2019'
D19-5826:
  abstract: Adapting models to new domain without finetuning is a challenging problem
    in deep learning. In this paper, we utilize an adversarial training framework
    for domain generalization in Question Answering (QA) task. Our model consists
    of a conventional QA model and a discriminator. The training is performed in the
    adversarial manner, where the two models constantly compete, so that QA model
    can learn domain-invariant features. We apply this approach in MRQA Shared Task
    2019 and show better performance compared to the baseline model.
  address: Hong Kong, China
  author:
  - first: Seanie
    full: Seanie Lee
    id: seanie-lee
    last: Lee
  - first: Donggyu
    full: Donggyu Kim
    id: donggyu-kim
    last: Kim
  - first: Jangwon
    full: Jangwon Park
    id: jangwon-park
    last: Park
  author_string: Seanie Lee, Donggyu Kim, Jangwon Park
  bibkey: lee-etal-2019-domain
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5826
  month: November
  page_first: '196'
  page_last: '202'
  pages: "196\u2013202"
  paper_id: '26'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5826.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5826.jpg
  title: Domain-agnostic Question-Answering with Adversarial Training
  title_html: Domain-agnostic Question-Answering with Adversarial Training
  url: https://www.aclweb.org/anthology/D19-5826
  year: '2019'
D19-5827:
  abstract: With a large number of datasets being released and new techniques being
    proposed, Question answering (QA) systems have witnessed great breakthroughs in
    reading comprehension (RC)tasks. However, most existing methods focus on improving
    in-domain performance, leaving open the research question of how these mod-els
    and techniques can generalize to out-of-domain and unseen RC tasks. To enhance
    the generalization ability, we propose a multi-task learning framework that learns
    the shared representation across different tasks. Our model is built on top of
    a large pre-trained language model, such as XLNet, and then fine-tuned on multiple
    RC datasets. Experimental results show the effectiveness of our methods, with
    an average Exact Match score of 56.59 and an average F1 score of 68.98, which
    significantly improves the BERT-Large baseline by8.39 and 7.22, respectively
  address: Hong Kong, China
  author:
  - first: Dan
    full: Dan Su
    id: dan-su
    last: Su
  - first: Yan
    full: Yan Xu
    id: yan-xu
    last: Xu
  - first: Genta Indra
    full: Genta Indra Winata
    id: genta-indra-winata
    last: Winata
  - first: Peng
    full: Peng Xu
    id: peng-xu
    last: Xu
  - first: Hyeondey
    full: Hyeondey Kim
    id: hyeondey-kim
    last: Kim
  - first: Zihan
    full: Zihan Liu
    id: zihan-liu
    last: Liu
  - first: Pascale
    full: Pascale Fung
    id: pascale-fung
    last: Fung
  author_string: Dan Su, Yan Xu, Genta Indra Winata, Peng Xu, Hyeondey Kim, Zihan
    Liu, Pascale Fung
  bibkey: su-etal-2019-generalizing
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5827
  month: November
  page_first: '203'
  page_last: '211'
  pages: "203\u2013211"
  paper_id: '27'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5827.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5827.jpg
  title: Generalizing Question Answering System with Pre-trained Language Model Fine-tuning
  title_html: Generalizing Question Answering System with Pre-trained Language Model
    Fine-tuning
  url: https://www.aclweb.org/anthology/D19-5827
  year: '2019'
D19-5828:
  abstract: In this paper, we introduce a simple system Baidu submitted for MRQA (Machine
    Reading for Question Answering) 2019 Shared Task that focused on generalization
    of machine reading comprehension (MRC) models. Our system is built on a framework
    of pretraining and fine-tuning, namely D-NET. The techniques of pre-trained language
    models and multi-task learning are explored to improve the generalization of MRC
    models and we conduct experiments to examine the effectiveness of these strategies.
    Our system is ranked at top 1 of all the participants in terms of averaged F1
    score. Our codes and models will be released at PaddleNLP.
  address: Hong Kong, China
  author:
  - first: Hongyu
    full: Hongyu Li
    id: hongyu-li
    last: Li
  - first: Xiyuan
    full: Xiyuan Zhang
    id: xiyuan-zhang
    last: Zhang
  - first: Yibing
    full: Yibing Liu
    id: yibing-liu
    last: Liu
  - first: Yiming
    full: Yiming Zhang
    id: yiming-zhang
    last: Zhang
  - first: Quan
    full: Quan Wang
    id: quan-wang
    last: Wang
  - first: Xiangyang
    full: Xiangyang Zhou
    id: xiangyang-zhou
    last: Zhou
  - first: Jing
    full: Jing Liu
    id: jing-liu
    last: Liu
  - first: Hua
    full: Hua Wu
    id: hua-wu
    last: Wu
  - first: Haifeng
    full: Haifeng Wang
    id: haifeng-wang
    last: Wang
  author_string: Hongyu Li, Xiyuan Zhang, Yibing Liu, Yiming Zhang, Quan Wang, Xiangyang
    Zhou, Jing Liu, Hua Wu, Haifeng Wang
  bibkey: li-etal-2019-net
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5828
  month: November
  page_first: '212'
  page_last: '219'
  pages: "212\u2013219"
  paper_id: '28'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5828.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5828.jpg
  title: 'D-NET: A Pre-Training and Fine-Tuning Framework for Improving the Generalization
    of Machine Reading Comprehension'
  title_html: 'D-<span class="acl-fixed-case">NET</span>: A Pre-Training and Fine-Tuning
    Framework for Improving the Generalization of Machine Reading Comprehension'
  url: https://www.aclweb.org/anthology/D19-5828
  year: '2019'
D19-5829:
  abstract: To produce a domain-agnostic question answering model for the Machine
    Reading Question Answering (MRQA) 2019 Shared Task, we investigate the relative
    benefits of large pre-trained language models, various data sampling strategies,
    as well as query and context paraphrases generated by back-translation. We find
    a simple negative sampling technique to be particularly effective, even though
    it is typically used for datasets that include unanswerable questions, such as
    SQuAD 2.0. When applied in conjunction with per-domain sampling, our XLNet (Yang
    et al., 2019)-based submission achieved the second best Exact Match and F1 in
    the MRQA leaderboard competition.
  address: Hong Kong, China
  author:
  - first: Shayne
    full: Shayne Longpre
    id: shayne-longpre
    last: Longpre
  - first: Yi
    full: Yi Lu
    id: yi-lu
    last: Lu
  - first: Zhucheng
    full: Zhucheng Tu
    id: zhucheng-tu
    last: Tu
  - first: Chris
    full: Chris DuBois
    id: chris-dubois
    last: DuBois
  author_string: Shayne Longpre, Yi Lu, Zhucheng Tu, Chris DuBois
  bibkey: longpre-etal-2019-exploration
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  booktitle_html: Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering
  doi: 10.18653/v1/D19-5829
  month: November
  page_first: '220'
  page_last: '227'
  pages: "220\u2013227"
  paper_id: '29'
  parent_volume_id: D19-58
  pdf: https://www.aclweb.org/anthology/D19-5829.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5829.jpg
  title: An Exploration of Data Augmentation and Sampling Techniques for Domain-Agnostic
    Question Answering
  title_html: An Exploration of Data Augmentation and Sampling Techniques for Domain-Agnostic
    Question Answering
  url: https://www.aclweb.org/anthology/D19-5829
  year: '2019'
D19-5900:
  address: Hong Kong
  author:
  - first: Silviu
    full: Silviu Paun
    id: silviu-paun
    last: Paun
  - first: Dirk
    full: Dirk Hovy
    id: dirk-hovy
    last: Hovy
  author_string: Silviu Paun, Dirk Hovy
  bibkey: emnlp-2019-aggregating
  bibtype: proceedings
  booktitle: Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced
    Annotations for NLP
  month: November
  paper_id: '0'
  parent_volume_id: D19-59
  pdf: https://www.aclweb.org/anthology/D19-5900.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5900.jpg
  title: Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced
    Annotations for NLP
  title_html: Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced
    Annotations for NLP
  url: https://www.aclweb.org/anthology/D19-5900
  year: '2019'
D19-5901:
  abstract: Crowdsourcing is frequently employed to quickly and inexpensively obtain
    valuable linguistic annotations but is rarely used for parsing, likely due to
    the perceived difficulty of the task and the limited training of the available
    workers. This paper presents what is, to the best of our knowledge, the first
    published use of Mechanical Turk (or similar platform) to crowdsource parse trees.
    We pay Turkers to construct unlabeled dependency trees for 500 English sentences
    using an interactive graphical dependency tree editor, collecting 10 annotations
    per sentence. Despite not requiring any training, several of the more prolific
    workers meet or exceed 90% attachment agreement with the Penn Treebank (PTB) portion
    of our data, and, furthermore, for 72% of these PTB sentences, at least one Turker
    produces a perfect parse. Thus, we find that, supported with a simple graphical
    interface, people with presumably no prior experience can achieve surprisingly
    high degrees of accuracy on this task. To facilitate research into aggregation
    techniques for complex crowdsourced annotations, we publicly release our annotated
    corpus.
  address: Hong Kong
  author:
  - first: Stephen
    full: Stephen Tratz
    id: stephen-tratz
    last: Tratz
  author_string: Stephen Tratz
  bibkey: tratz-2019-dependency
  bibtype: inproceedings
  booktitle: Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced
    Annotations for NLP
  booktitle_html: Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced
    Annotations for NLP
  doi: 10.18653/v1/D19-5901
  month: November
  page_first: '1'
  page_last: '5'
  pages: "1\u20135"
  paper_id: '1'
  parent_volume_id: D19-59
  pdf: https://www.aclweb.org/anthology/D19-5901.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5901.jpg
  title: Dependency Tree Annotation with Mechanical Turk
  title_html: Dependency Tree Annotation with Mechanical Turk
  url: https://www.aclweb.org/anthology/D19-5901
  year: '2019'
D19-5902:
  abstract: "This paper presents research on word familiarity rate estimation using\
    \ the \u2018Word List by Semantic Principles\u2019. We collected rating information\
    \ on 96,557 words in the \u2018Word List by Semantic Principles\u2019 via Yahoo!\
    \ crowdsourcing. We asked 3,392 subject participants to use their introspection\
    \ to rate the familiarity of words based on the five perspectives of \u2018KNOW\u2019\
    , \u2018WRITE\u2019, \u2018READ\u2019, \u2018SPEAK\u2019, and \u2018LISTEN\u2019\
    , and each word was rated by at least 16 subject participants. We used Bayesian\
    \ linear mixed models to estimate the word familiarity rates. We also explored\
    \ the ratings with the semantic labels used in the \u2018Word List by Semantic\
    \ Principles\u2019."
  address: Hong Kong
  author:
  - first: Masayuki
    full: Masayuki Asahara
    id: masayuki-asahara
    last: Asahara
  author_string: Masayuki Asahara
  bibkey: asahara-2019-word
  bibtype: inproceedings
  booktitle: Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced
    Annotations for NLP
  booktitle_html: Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced
    Annotations for NLP
  doi: 10.18653/v1/D19-5902
  month: November
  page_first: '6'
  page_last: '14'
  pages: "6\u201314"
  paper_id: '2'
  parent_volume_id: D19-59
  pdf: https://www.aclweb.org/anthology/D19-5902.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5902.jpg
  title: Word Familiarity Rate Estimation Using a Bayesian Linear Mixed Model
  title_html: Word Familiarity Rate Estimation Using a <span class="acl-fixed-case">B</span>ayesian
    Linear Mixed Model
  url: https://www.aclweb.org/anthology/D19-5902
  year: '2019'
D19-5903:
  abstract: Detecting event mentions is the first step in event extraction from text
    and annotating them is a notoriously difficult task. Evaluating annotator consistency
    is crucial when building datasets for mention detection. When event mentions are
    allowed to cover many tokens, annotators may disagree on their span, which means
    that overlapping annotations may then refer to the same event or to different
    events. This paper explores different fuzzy-matching functions which aim to resolve
    this ambiguity. The functions extract the sets of syntactic heads present in the
    annotations, use the Dice coefficient to measure the similarity between sets and
    return a judgment based on a given threshold. The functions are tested against
    the judgment of a human evaluator and a comparison is made between sets of tokens
    and sets of syntactic heads. The best-performing function is a head-based function
    that is found to agree with the human evaluator in 89% of cases.
  address: Hong Kong
  author:
  - first: Camiel
    full: Camiel Colruyt
    id: camiel-colruyt
    last: Colruyt
  - first: "Orph\xE9e"
    full: "Orph\xE9e De Clercq"
    id: orphee-de-clercq
    last: De Clercq
  - first: "V\xE9ronique"
    full: "V\xE9ronique Hoste"
    id: veronique-hoste
    last: Hoste
  author_string: "Camiel Colruyt, Orph\xE9e De Clercq, V\xE9ronique Hoste"
  bibkey: colruyt-etal-2019-leveraging
  bibtype: inproceedings
  booktitle: Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced
    Annotations for NLP
  booktitle_html: Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced
    Annotations for NLP
  doi: 10.18653/v1/D19-5903
  month: November
  page_first: '15'
  page_last: '23'
  pages: "15\u201323"
  paper_id: '3'
  parent_volume_id: D19-59
  pdf: https://www.aclweb.org/anthology/D19-5903.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5903.jpg
  title: Leveraging syntactic parsing to improve event annotation matching
  title_html: Leveraging syntactic parsing to improve event annotation matching
  url: https://www.aclweb.org/anthology/D19-5903
  year: '2019'
D19-5904:
  abstract: The target outputs of many NLP tasks are word sequences. To collect the
    data for training and evaluating models, the crowd is a cheaper and easier to
    access than the oracle. To ensure the quality of the crowdsourced data, people
    can assign multiple workers to one question and then aggregate the multiple answers
    with diverse quality into a golden one. How to aggregate multiple crowdsourced
    word sequences with diverse quality is a curious and challenging problem. People
    need a dataset for addressing this problem. We thus create a dataset (CrowdWSA2019)
    which contains the translated sentences generated from multiple workers. We provide
    three approaches as the baselines on the task of extractive word sequence aggregation.
    Specially, one of them is an original one we propose which models the reliability
    of workers. We also discuss some issues on ground truth creation of word sequences
    which can be addressed based on this dataset.
  address: Hong Kong
  author:
  - first: Jiyi
    full: Jiyi Li
    id: jiyi-li
    last: Li
  - first: Fumiyo
    full: Fumiyo Fukumoto
    id: fumiyo-fukumoto
    last: Fukumoto
  author_string: Jiyi Li, Fumiyo Fukumoto
  bibkey: li-fukumoto-2019-dataset
  bibtype: inproceedings
  booktitle: Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced
    Annotations for NLP
  booktitle_html: Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced
    Annotations for NLP
  doi: 10.18653/v1/D19-5904
  month: November
  page_first: '24'
  page_last: '28'
  pages: "24\u201328"
  paper_id: '4'
  parent_volume_id: D19-59
  pdf: https://www.aclweb.org/anthology/D19-5904.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5904.jpg
  title: 'A Dataset of Crowdsourced Word Sequences: Collections and Answer Aggregation
    for Ground Truth Creation'
  title_html: 'A Dataset of Crowdsourced Word Sequences: Collections and Answer Aggregation
    for Ground Truth Creation'
  url: https://www.aclweb.org/anthology/D19-5904
  year: '2019'
D19-5905:
  abstract: Recent advancements in machine reading and listening comprehension involve
    the annotation of long texts. Such tasks are typically time consuming, making
    crowd-annotations an attractive solution, yet their complexity often makes such
    a solution unfeasible. In particular, a major concern is that crowd annotators
    may be tempted to skim through long texts, and answer questions without reading
    thoroughly. We present a case study of adapting this type of task to the crowd.
    The task is to identify claims in a several minute long debate speech. We show
    that sentence-by-sentence annotation does not scale and that labeling only a subset
    of sentences is insufficient. Instead, we propose a scheme for effectively performing
    the full, complex task with crowd annotators, allowing the collection of large
    scale annotated datasets. We believe that the encountered challenges and pitfalls,
    as well as lessons learned, are relevant in general when collecting data for large
    scale natural language understanding (NLU) tasks.
  address: Hong Kong
  attachment:
  - filename: D19-5905.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5905.Attachment.pdf
  author:
  - first: Tamar
    full: Tamar Lavee
    id: tamar-lavee
    last: Lavee
  - first: Lili
    full: Lili Kotlerman
    id: lili-kotlerman
    last: Kotlerman
  - first: Matan
    full: Matan Orbach
    id: matan-orbach
    last: Orbach
  - first: Yonatan
    full: Yonatan Bilu
    id: yonatan-bilu
    last: Bilu
  - first: Michal
    full: Michal Jacovi
    id: michal-jacovi
    last: Jacovi
  - first: Ranit
    full: Ranit Aharonov
    id: ranit-aharonov
    last: Aharonov
  - first: Noam
    full: Noam Slonim
    id: noam-slonim
    last: Slonim
  author_string: Tamar Lavee, Lili Kotlerman, Matan Orbach, Yonatan Bilu, Michal Jacovi,
    Ranit Aharonov, Noam Slonim
  bibkey: lavee-etal-2019-crowd
  bibtype: inproceedings
  booktitle: Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced
    Annotations for NLP
  booktitle_html: Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced
    Annotations for NLP
  doi: 10.18653/v1/D19-5905
  month: November
  page_first: '29'
  page_last: '38'
  pages: "29\u201338"
  paper_id: '5'
  parent_volume_id: D19-59
  pdf: https://www.aclweb.org/anthology/D19-5905.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5905.jpg
  title: 'Crowd-sourcing annotation of complex NLU tasks: A case study of argumentative
    content annotation'
  title_html: 'Crowd-sourcing annotation of complex <span class="acl-fixed-case">NLU</span>
    tasks: A case study of argumentative content annotation'
  url: https://www.aclweb.org/anthology/D19-5905
  year: '2019'
D19-5906:
  abstract: We propose a method of machine-assisted annotation for the identification
    of tension development, annotating whether the tension is increasing, decreasing,
    or staying unchanged. We use a neural network based prediction model, whose predicted
    results are given to the annotators as initial values for the options that they
    are asked to choose. By presenting such initial values to the annotators, the
    annotation task becomes an evaluation task where the annotators inspect whether
    or not the predicted results are correct. To demonstrate the effectiveness of
    our method, we performed the annotation task in both in-house and crowdsourced
    environments. For the crowdsourced environment, we compared the annotation results
    with and without our method of machine-assisted annotation. We find that the results
    with our method showed a higher agreement to the gold standard than those without,
    though our method had little effect at reducing the time for annotation. Our codes
    for the experiment are made publicly available.
  address: Hong Kong
  author:
  - first: Seungwon
    full: Seungwon Yoon
    id: seungwon-yoon
    last: Yoon
  - first: Wonsuk
    full: Wonsuk Yang
    id: wonsuk-yang
    last: Yang
  - first: Jong
    full: Jong Park
    id: jong-c-park
    last: Park
  author_string: Seungwon Yoon, Wonsuk Yang, Jong Park
  bibkey: yoon-etal-2019-computer
  bibtype: inproceedings
  booktitle: Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced
    Annotations for NLP
  booktitle_html: Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced
    Annotations for NLP
  doi: 10.18653/v1/D19-5906
  month: November
  page_first: '39'
  page_last: '47'
  pages: "39\u201347"
  paper_id: '6'
  parent_volume_id: D19-59
  pdf: https://www.aclweb.org/anthology/D19-5906.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5906.jpg
  title: Computer Assisted Annotation of Tension Development in TED Talks through
    Crowdsourcing
  title_html: Computer Assisted Annotation of Tension Development in <span class="acl-fixed-case">TED</span>
    Talks through Crowdsourcing
  url: https://www.aclweb.org/anthology/D19-5906
  year: '2019'
D19-5907:
  abstract: Code-switching refers to the alternation of two or more languages in a
    conversation or utterance and is common in multilingual communities across the
    world. Building code-switched speech and natural language processing systems are
    challenging due to the lack of annotated speech and text data. We present a speech
    annotation interface CoSSAT, which helps annotators transcribe code-switched speech
    faster, more easily and more accurately than a traditional interface, by displaying
    candidate words from monolingual speech recognizers. We conduct a user study on
    the transcription of Hindi-English code-switched speech with 10 annotators and
    describe quantitative and qualitative results.
  address: Hong Kong
  attachment:
  - filename: D19-5907.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-5907.Attachment.pdf
  author:
  - first: Sanket
    full: Sanket Shah
    id: sanket-shah
    last: Shah
  - first: Pratik
    full: Pratik Joshi
    id: pratik-joshi
    last: Joshi
  - first: Sebastin
    full: Sebastin Santy
    id: sebastin-santy
    last: Santy
  - first: Sunayana
    full: Sunayana Sitaram
    id: sunayana-sitaram
    last: Sitaram
  author_string: Sanket Shah, Pratik Joshi, Sebastin Santy, Sunayana Sitaram
  bibkey: shah-etal-2019-cossat
  bibtype: inproceedings
  booktitle: Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced
    Annotations for NLP
  booktitle_html: Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced
    Annotations for NLP
  doi: 10.18653/v1/D19-5907
  month: November
  page_first: '48'
  page_last: '52'
  pages: "48\u201352"
  paper_id: '7'
  parent_volume_id: D19-59
  pdf: https://www.aclweb.org/anthology/D19-5907.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-5907.jpg
  title: 'CoSSAT: Code-Switched Speech Annotation Tool'
  title_html: '<span class="acl-fixed-case">C</span>o<span class="acl-fixed-case">SSAT</span>:
    Code-Switched Speech Annotation Tool'
  url: https://www.aclweb.org/anthology/D19-5907
  year: '2019'
D19-6000:
  address: Hong Kong, China
  author:
  - first: Simon
    full: Simon Ostermann
    id: simon-ostermann
    last: Ostermann
  - first: Sheng
    full: Sheng Zhang
    id: sheng-zhang
    last: Zhang
  - first: Michael
    full: Michael Roth
    id: michael-roth
    last: Roth
  - first: Peter
    full: Peter Clark
    id: peter-clark
    last: Clark
  author_string: Simon Ostermann, Sheng Zhang, Michael Roth, Peter Clark
  bibkey: emnlp-2019-commonsense
  bibtype: proceedings
  booktitle: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  month: November
  paper_id: '0'
  parent_volume_id: D19-60
  pdf: https://www.aclweb.org/anthology/D19-6000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6000.jpg
  title: Proceedings of the First Workshop on Commonsense Inference in Natural Language
    Processing
  title_html: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  url: https://www.aclweb.org/anthology/D19-6000
  year: '2019'
D19-6001:
  abstract: "Pretrained deep contextual representations have advanced the state-of-the-art\
    \ on various commonsense NLP tasks, but we lack a concrete understanding of the\
    \ capability of these models. Thus, we investigate and challenge several aspects\
    \ of BERT\u2019s commonsense representation abilities. First, we probe BERT\u2019\
    s ability to classify various object attributes, demonstrating that BERT shows\
    \ a strong ability in encoding various commonsense features in its embedding space,\
    \ but is still deficient in many areas. Next, we show that, by augmenting BERT\u2019\
    s pretraining data with additional data related to the deficient attributes, we\
    \ are able to improve performance on a downstream commonsense reasoning task while\
    \ using a minimal amount of data. Finally, we develop a method of fine-tuning\
    \ knowledge graphs embeddings alongside BERT and show the continued importance\
    \ of explicit knowledge graphs."
  address: Hong Kong, China
  author:
  - first: Jeff
    full: Jeff Da
    id: jeff-da
    last: Da
  - first: Jungo
    full: Jungo Kasai
    id: jungo-kasai
    last: Kasai
  author_string: Jeff Da, Jungo Kasai
  bibkey: da-kasai-2019-cracking
  bibtype: inproceedings
  booktitle: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  booktitle_html: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  doi: 10.18653/v1/D19-6001
  month: November
  page_first: '1'
  page_last: '12'
  pages: "1\u201312"
  paper_id: '1'
  parent_volume_id: D19-60
  pdf: https://www.aclweb.org/anthology/D19-6001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6001.jpg
  title: 'Cracking the Contextual Commonsense Code: Understanding Commonsense Reasoning
    Aptitude of Deep Contextual Representations'
  title_html: 'Cracking the Contextual Commonsense Code: Understanding Commonsense
    Reasoning Aptitude of Deep Contextual Representations'
  url: https://www.aclweb.org/anthology/D19-6001
  year: '2019'
D19-6002:
  abstract: 'This paper proposes a hybrid neural network(HNN) model for commonsense
    reasoning. An HNN consists of two component models, a masked language model and
    a semantic similarity model, which share a BERTbased contextual encoder but use
    different model-specific input and output layers. HNN obtains new state-of-the-art
    results on three classic commonsense reasoning tasks, pushing the WNLI benchmark
    to 89%, the Winograd Schema Challenge (WSC) benchmark to 75.1%, and the PDP60
    benchmark to 90.0%. An ablation study shows that language models and semantic
    similarity models are complementary approaches to commonsense reasoning, and HNN
    effectively combines the strengths of both. The code and pre-trained models will
    be publicly available at https: //github.com/namisan/mt-dnn.'
  address: Hong Kong, China
  author:
  - first: Pengcheng
    full: Pengcheng He
    id: pengcheng-he
    last: He
  - first: Xiaodong
    full: Xiaodong Liu
    id: xiaodong-liu
    last: Liu
  - first: Weizhu
    full: Weizhu Chen
    id: weizhu-chen
    last: Chen
  - first: Jianfeng
    full: Jianfeng Gao
    id: jianfeng-gao
    last: Gao
  author_string: Pengcheng He, Xiaodong Liu, Weizhu Chen, Jianfeng Gao
  bibkey: he-etal-2019-hybrid
  bibtype: inproceedings
  booktitle: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  booktitle_html: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  doi: 10.18653/v1/D19-6002
  month: November
  page_first: '13'
  page_last: '21'
  pages: "13\u201321"
  paper_id: '2'
  parent_volume_id: D19-60
  pdf: https://www.aclweb.org/anthology/D19-6002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6002.jpg
  title: A Hybrid Neural Network Model for Commonsense Reasoning
  title_html: A Hybrid Neural Network Model for Commonsense Reasoning
  url: https://www.aclweb.org/anthology/D19-6002
  year: '2019'
D19-6003:
  abstract: Non-extractive commonsense QA remains a challenging AI task, as it requires
    systems to reason about, synthesize, and gather disparate pieces of information,
    in order to generate responses to queries. Recent approaches on such tasks show
    increased performance, only when models are either pre-trained with additional
    information or when domain-specific heuristics are used, without any special consideration
    regarding the knowledge resource type. In this paper, we perform a survey of recent
    commonsense QA methods and we provide a systematic analysis of popular knowledge
    resources and knowledge-integration methods, across benchmarks from multiple commonsense
    datasets. Our results and analysis show that attention-based injection seems to
    be a preferable choice for knowledge integration and that the degree of domain
    overlap, between knowledge bases and datasets, plays a crucial role in determining
    model success.
  address: Hong Kong, China
  author:
  - first: Kaixin
    full: Kaixin Ma
    id: kaixin-ma
    last: Ma
  - first: Jonathan
    full: Jonathan Francis
    id: jonathan-francis
    last: Francis
  - first: Quanyang
    full: Quanyang Lu
    id: quanyang-lu
    last: Lu
  - first: Eric
    full: Eric Nyberg
    id: eric-nyberg
    last: Nyberg
  - first: Alessandro
    full: Alessandro Oltramari
    id: alessandro-oltramari
    last: Oltramari
  author_string: Kaixin Ma, Jonathan Francis, Quanyang Lu, Eric Nyberg, Alessandro
    Oltramari
  bibkey: ma-etal-2019-towards
  bibtype: inproceedings
  booktitle: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  booktitle_html: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  doi: 10.18653/v1/D19-6003
  month: November
  page_first: '22'
  page_last: '32'
  pages: "22\u201332"
  paper_id: '3'
  parent_volume_id: D19-60
  pdf: https://www.aclweb.org/anthology/D19-6003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6003.jpg
  title: Towards Generalizable Neuro-Symbolic Systems for Commonsense Question Answering
  title_html: Towards Generalizable Neuro-Symbolic Systems for Commonsense Question
    Answering
  url: https://www.aclweb.org/anthology/D19-6003
  year: '2019'
D19-6004:
  abstract: "Pretrained language models, such as BERT and RoBERTa, have shown large\
    \ improvements in the commonsense reasoning benchmark COPA. However, recent work\
    \ found that many improvements in benchmarks of natural language understanding\
    \ are not due to models learning the task, but due to their increasing ability\
    \ to exploit superficial cues, such as tokens that occur more often in the correct\
    \ answer than the wrong one. Are BERT\u2019s and RoBERTa\u2019s good performance\
    \ on COPA also caused by this? We find superficial cues in COPA, as well as evidence\
    \ that BERT exploits these cues.To remedy this problem, we introduce Balanced\
    \ COPA, an extension of COPA that does not suffer from easy-to-exploit single\
    \ token cues. We analyze BERT\u2019s and RoBERTa\u2019s performance on original\
    \ and Balanced COPA, finding that BERT relies on superficial cues when they are\
    \ present, but still achieves comparable performance once they are made ineffective,\
    \ suggesting that BERT learns the task to a certain degree when forced to. In\
    \ contrast, RoBERTa does not appear to rely on superficial cues."
  address: Hong Kong, China
  author:
  - first: Pride
    full: Pride Kavumba
    id: pride-kavumba
    last: Kavumba
  - first: Naoya
    full: Naoya Inoue
    id: naoya-inoue
    last: Inoue
  - first: Benjamin
    full: Benjamin Heinzerling
    id: benjamin-heinzerling
    last: Heinzerling
  - first: Keshav
    full: Keshav Singh
    id: keshav-singh
    last: Singh
  - first: Paul
    full: Paul Reisert
    id: paul-reisert
    last: Reisert
  - first: Kentaro
    full: Kentaro Inui
    id: kentaro-inui
    last: Inui
  author_string: Pride Kavumba, Naoya Inoue, Benjamin Heinzerling, Keshav Singh, Paul
    Reisert, Kentaro Inui
  bibkey: kavumba-etal-2019-choosing
  bibtype: inproceedings
  booktitle: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  booktitle_html: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  doi: 10.18653/v1/D19-6004
  month: November
  page_first: '33'
  page_last: '42'
  pages: "33\u201342"
  paper_id: '4'
  parent_volume_id: D19-60
  pdf: https://www.aclweb.org/anthology/D19-6004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6004.jpg
  title: When Choosing Plausible Alternatives, Clever Hans can be Clever
  title_html: When Choosing Plausible Alternatives, Clever Hans can be Clever
  url: https://www.aclweb.org/anthology/D19-6004
  year: '2019'
D19-6005:
  abstract: We consider the problem of extracting from text commonsense knowledge
    pertaining to human senses such as sound and smell. First, we consider the problem
    of recognizing mentions of human senses in text. Our contribution is a method
    for acquiring labeled data. Experiments show the effectiveness of our proposed
    data labeling approach when used with standard machine learning models on the
    task of sense recognition in text. Second, we propose to extract novel, common
    sense relationships pertaining to sense perception concepts. Our contribution
    is a process for generating labeled data by leveraging large corpora and crowdsourcing
    questionnaires.
  address: Hong Kong, China
  author:
  - first: Ndapa
    full: Ndapa Nakashole
    id: ndapandula-nakashole
    last: Nakashole
  author_string: Ndapa Nakashole
  bibkey: nakashole-2019-commonsense
  bibtype: inproceedings
  booktitle: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  booktitle_html: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  doi: 10.18653/v1/D19-6005
  month: November
  page_first: '43'
  page_last: '52'
  pages: "43\u201352"
  paper_id: '5'
  parent_volume_id: D19-60
  pdf: https://www.aclweb.org/anthology/D19-6005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6005.jpg
  title: 'Commonsense about Human Senses: Labeled Data Collection Processes'
  title_html: 'Commonsense about Human Senses: Labeled Data Collection Processes'
  url: https://www.aclweb.org/anthology/D19-6005
  year: '2019'
D19-6006:
  abstract: "Complex questions often require combining multiple facts to correctly\
    \ answer, particularly when generating detailed explanations for why those answers\
    \ are correct. Combining multiple facts to answer questions is often modeled as\
    \ a \u201Cmulti-hop\u201D graph traversal problem, where a given solver must find\
    \ a series of interconnected facts in a knowledge graph that, taken together,\
    \ answer the question and explain the reasoning behind that answer. Multi-hop\
    \ inference currently suffers from semantic drift, or the tendency for chains\
    \ of reasoning to \u201Cdrift\u201D\u2019 to unrelated topics, and this semantic\
    \ drift greatly limits the number of facts that can be combined in both free text\
    \ or knowledge base inference. In this work we present our effort to mitigate\
    \ semantic drift by extracting large high-confidence multi-hop inference patterns,\
    \ generated by abstracting large-scale explanatory structure from a corpus of\
    \ detailed explanations. We represent these inference patterns as sets of generalized\
    \ constraints over sentences represented as rows in a knowledge base of semi-structured\
    \ tables. We present a prototype tool for identifying common inference patterns\
    \ from corpora of semi-structured explanations, and use it to successfully extract\
    \ 67 inference patterns from a \u201Cmatter\u201D subset of standardized elementary\
    \ science exam questions that span scientific and world knowledge."
  address: Hong Kong, China
  author:
  - first: Sebastian
    full: Sebastian Thiem
    id: sebastian-thiem
    last: Thiem
  - first: Peter
    full: Peter Jansen
    id: peter-jansen
    last: Jansen
  author_string: Sebastian Thiem, Peter Jansen
  bibkey: thiem-jansen-2019-extracting
  bibtype: inproceedings
  booktitle: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  booktitle_html: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  doi: 10.18653/v1/D19-6006
  month: November
  page_first: '53'
  page_last: '65'
  pages: "53\u201365"
  paper_id: '6'
  parent_volume_id: D19-60
  pdf: https://www.aclweb.org/anthology/D19-6006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6006.jpg
  title: Extracting Common Inference Patterns from Semi-Structured Explanations
  title_html: Extracting Common Inference Patterns from Semi-Structured Explanations
  url: https://www.aclweb.org/anthology/D19-6006
  year: '2019'
D19-6007:
  abstract: "This paper reports on the results of the shared tasks of the COIN workshop\
    \ at EMNLP-IJCNLP 2019. The tasks consisted of two machine comprehension evaluations,\
    \ each of which tested a system\u2019s ability to answer questions/queries about\
    \ a text. Both evaluations were designed such that systems need to exploit commonsense\
    \ knowledge, for example, in the form of inferences over information that is available\
    \ in the common ground but not necessarily mentioned in the text. A total of five\
    \ participating teams submitted systems for the shared tasks, with the best submitted\
    \ system achieving 90.6% accuracy and 83.7% F1-score on task 1 and task 2, respectively."
  address: Hong Kong, China
  author:
  - first: Simon
    full: Simon Ostermann
    id: simon-ostermann
    last: Ostermann
  - first: Sheng
    full: Sheng Zhang
    id: sheng-zhang
    last: Zhang
  - first: Michael
    full: Michael Roth
    id: michael-roth
    last: Roth
  - first: Peter
    full: Peter Clark
    id: peter-clark
    last: Clark
  author_string: Simon Ostermann, Sheng Zhang, Michael Roth, Peter Clark
  bibkey: ostermann-etal-2019-commonsense
  bibtype: inproceedings
  booktitle: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  booktitle_html: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  doi: 10.18653/v1/D19-6007
  month: November
  page_first: '66'
  page_last: '74'
  pages: "66\u201374"
  paper_id: '7'
  parent_volume_id: D19-60
  pdf: https://www.aclweb.org/anthology/D19-6007.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6007.jpg
  title: Commonsense Inference in Natural Language Processing (COIN) - Shared Task
    Report
  title_html: Commonsense Inference in Natural Language Processing (<span class="acl-fixed-case">COIN</span>)
    - Shared Task Report
  url: https://www.aclweb.org/anthology/D19-6007
  year: '2019'
D19-6008:
  abstract: 'This paper describes our model for COmmonsense INference in Natural Language
    Processing (COIN) shared task 1: Commonsense Inference in Everyday Narrations.
    This paper explores the use of Bidirectional Encoder Representations from Transformers(BERT)
    along with external relational knowledge from ConceptNet to tackle the problem
    of commonsense inference. The input passage, question, and answer are augmented
    with relational knowledge from ConceptNet. Using this technique we are able to
    achieve an accuracy of 73.3 % on the official test data.'
  address: Hong Kong, China
  author:
  - first: Yash
    full: Yash Jain
    id: yash-jain
    last: Jain
  - first: Chinmay
    full: Chinmay Singh
    id: chinmay-singh
    last: Singh
  author_string: Yash Jain, Chinmay Singh
  bibkey: jain-singh-2019-karna
  bibtype: inproceedings
  booktitle: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  booktitle_html: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  doi: 10.18653/v1/D19-6008
  month: November
  page_first: '75'
  page_last: '79'
  pages: "75\u201379"
  paper_id: '8'
  parent_volume_id: D19-60
  pdf: https://www.aclweb.org/anthology/D19-6008.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6008.jpg
  title: 'KARNA at COIN Shared Task 1: Bidirectional Encoder Representations from
    Transformers with relational knowledge for machine comprehension with common sense'
  title_html: '<span class="acl-fixed-case">KARNA</span> at <span class="acl-fixed-case">COIN</span>
    Shared Task 1: Bidirectional Encoder Representations from Transformers with relational
    knowledge for machine comprehension with common sense'
  url: https://www.aclweb.org/anthology/D19-6008
  year: '2019'
D19-6009:
  abstract: 'In this paper, we describe our system for COIN 2019 Shared Task 1: Commonsense
    Inference in Everyday Narrations. We show the power of leveraging state-of-the-art
    pre-trained language models such as BERT(Bidirectional Encoder Representations
    from Transformers) and XLNet over other Commonsense Knowledge Base Resources such
    as ConceptNet and NELL for modeling machine comprehension. We used an ensemble
    of BERT-Large and XLNet-Large. Experimental results show that our model give substantial
    improvements over the baseline and other systems incorporating knowledge bases.
    We bagged 2nd position on the final test set leaderboard with an accuracy of 90.5%'
  address: Hong Kong, China
  author:
  - first: Prakhar
    full: Prakhar Sharma
    id: prakhar-sharma
    last: Sharma
  - first: Sumegh
    full: Sumegh Roychowdhury
    id: sumegh-roychowdhury
    last: Roychowdhury
  author_string: Prakhar Sharma, Sumegh Roychowdhury
  bibkey: sharma-roychowdhury-2019-iit
  bibtype: inproceedings
  booktitle: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  booktitle_html: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  doi: 10.18653/v1/D19-6009
  month: November
  page_first: '80'
  page_last: '84'
  pages: "80\u201384"
  paper_id: '9'
  parent_volume_id: D19-60
  pdf: https://www.aclweb.org/anthology/D19-6009.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6009.jpg
  title: 'IIT-KGP at COIN 2019: Using pre-trained Language Models for modeling Machine
    Comprehension'
  title_html: '<span class="acl-fixed-case">IIT</span>-<span class="acl-fixed-case">KGP</span>
    at <span class="acl-fixed-case">COIN</span> 2019: Using pre-trained Language Models
    for modeling Machine Comprehension'
  url: https://www.aclweb.org/anthology/D19-6009
  year: '2019'
D19-6010:
  abstract: 'We introduce a simple yet effective method of integrating contextual
    embeddings with commonsense graph embeddings, dubbed BERT Infused Graphs: Matching
    Over Other embeDdings. First, we introduce a preprocessing method to improve the
    speed of querying knowledge bases. Then, we develop a method of creating knowledge
    embeddings from each knowledge base. We introduce a method of aligning tokens
    between two misaligned tokenization methods. Finally, we contribute a method of
    contextualizing BERT after combining with knowledge base embeddings. We also show
    BERTs tendency to correct lower accuracy question types. Our model achieves a
    higher accuracy than BERT, and we score fifth on the official leaderboard of the
    shared task and score the highest without any additional language model pretraining.'
  address: Hong Kong, China
  author:
  - first: Jeff
    full: Jeff Da
    id: jeff-da
    last: Da
  author_string: Jeff Da
  bibkey: da-2019-jeff
  bibtype: inproceedings
  booktitle: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  booktitle_html: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  doi: 10.18653/v1/D19-6010
  month: November
  page_first: '85'
  page_last: '92'
  pages: "85\u201392"
  paper_id: '10'
  parent_volume_id: D19-60
  pdf: https://www.aclweb.org/anthology/D19-6010.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6010.jpg
  title: 'Jeff Da at COIN - Shared Task: BIG MOOD: Relating Transformers to Explicit
    Commonsense Knowledge'
  title_html: 'Jeff Da at <span class="acl-fixed-case">COIN</span> - Shared Task:
    <span class="acl-fixed-case">BIG</span> <span class="acl-fixed-case">MOOD</span>:
    Relating Transformers to Explicit Commonsense Knowledge'
  url: https://www.aclweb.org/anthology/D19-6010
  year: '2019'
D19-6011:
  abstract: "To solve the shared tasks of COIN: COmmonsense INference in Natural Language\
    \ Processing) Workshop in , we need explore the impact of knowledge representation\
    \ in modeling commonsense knowledge to boost performance of machine reading comprehension\
    \ beyond simple text matching. There are two approaches to represent knowledge\
    \ in the low-dimensional space. The first is to leverage large-scale unsupervised\
    \ text corpus to train fixed or contextual language representations. The second\
    \ approach is to explicitly express knowledge into a knowledge graph (KG), and\
    \ then fit a model to represent the facts in the KG. We have experimented both\
    \ (a) improving the fine-tuning of pre-trained language models on a task with\
    \ a small dataset size, by leveraging datasets of similar tasks; and (b) incorporating\
    \ the distributional representations of a KG onto the representations of pre-trained\
    \ language models, via simply concatenation or multi-head attention. We find out\
    \ that: (a) for task 1, first fine-tuning on larger datasets like RACE (Lai et\
    \ al., 2017) and SWAG (Zellersetal.,2018), and then fine-tuning on the target\
    \ task improve the performance significantly; (b) for task 2, we find out the\
    \ incorporating a KG of commonsense knowledge, WordNet (Miller, 1995) into the\
    \ Bert model (Devlin et al., 2018) is helpful, however, it will hurts the performace\
    \ of XLNET (Yangetal.,2019), a more powerful pre-trained model. Our approaches\
    \ achieve the state-of-the-art results on both shared task\u2019s official test\
    \ data, outperforming all the other submissions."
  address: Hong Kong, China
  author:
  - first: Xiepeng
    full: Xiepeng Li
    id: xiepeng-li
    last: Li
  - first: Zhexi
    full: Zhexi Zhang
    id: zhexi-zhang
    last: Zhang
  - first: Wei
    full: Wei Zhu
    id: wei-zhu
    last: Zhu
  - first: Zheng
    full: Zheng Li
    id: zheng-li
    last: Li
  - first: Yuan
    full: Yuan Ni
    id: yuan-ni
    last: Ni
  - first: Peng
    full: Peng Gao
    id: peng-gao
    last: Gao
  - first: Junchi
    full: Junchi Yan
    id: junchi-yan
    last: Yan
  - first: Guotong
    full: Guotong Xie
    id: guotong-xie
    last: Xie
  author_string: Xiepeng Li, Zhexi Zhang, Wei Zhu, Zheng Li, Yuan Ni, Peng Gao, Junchi
    Yan, Guotong Xie
  bibkey: li-etal-2019-pingan
  bibtype: inproceedings
  booktitle: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  booktitle_html: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  doi: 10.18653/v1/D19-6011
  month: November
  page_first: '93'
  page_last: '98'
  pages: "93\u201398"
  paper_id: '11'
  parent_volume_id: D19-60
  pdf: https://www.aclweb.org/anthology/D19-6011.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6011.jpg
  title: 'Pingan Smart Health and SJTU at COIN - Shared Task: utilizing Pre-trained
    Language Models and Common-sense Knowledge in Machine Reading Tasks'
  title_html: 'Pingan Smart Health and <span class="acl-fixed-case">SJTU</span> at
    <span class="acl-fixed-case">COIN</span> - Shared Task: utilizing Pre-trained
    Language Models and Common-sense Knowledge in Machine Reading Tasks'
  url: https://www.aclweb.org/anthology/D19-6011
  year: '2019'
D19-6012:
  abstract: 'This paper describes our system for COIN Shared Task 1: Commonsense Inference
    in Everyday Narrations. To inject more external knowledge to better reason over
    the narrative passage, question and answer, the system adopts a stagewise fine-tuning
    method based on pre-trained BERT model. More specifically, the first stage is
    to fine-tune on addi- tional machine reading comprehension dataset to learn more
    commonsense knowledge. The second stage is to fine-tune on target-task (MCScript2.0)
    with MCScript (2018) dataset assisted. Experimental results show that our system
    achieves significant improvements over the baseline systems with 84.2% accuracy
    on the official test dataset.'
  address: Hong Kong, China
  author:
  - first: Chunhua
    full: Chunhua Liu
    id: chunhua-liu
    last: Liu
  - first: Dong
    full: Dong Yu
    id: dong-yu
    last: Yu
  author_string: Chunhua Liu, Dong Yu
  bibkey: liu-yu-2019-blcu
  bibtype: inproceedings
  booktitle: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  booktitle_html: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  doi: 10.18653/v1/D19-6012
  month: November
  page_first: '99'
  page_last: '103'
  pages: "99\u2013103"
  paper_id: '12'
  parent_volume_id: D19-60
  pdf: https://www.aclweb.org/anthology/D19-6012.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6012.jpg
  title: 'BLCU-NLP at COIN-Shared Task1: Stagewise Fine-tuning BERT for Commonsense
    Inference in Everyday Narrations'
  title_html: '<span class="acl-fixed-case">BLCU</span>-<span class="acl-fixed-case">NLP</span>
    at <span class="acl-fixed-case">COIN</span>-Shared Task1: Stagewise Fine-tuning
    <span class="acl-fixed-case">BERT</span> for Commonsense Inference in Everyday
    Narrations'
  url: https://www.aclweb.org/anthology/D19-6012
  year: '2019'
D19-6013:
  abstract: Natural language communication between machines and humans are still constrained.
    The article addresses a gap in natural language understanding about actions, specifically
    that of understanding commands. We propose a new method for commonsense inference
    (grounding) of high-level natural language commands into specific action commands
    for further execution by a robotic system. The method allows to build a knowledge
    base that consists of a large set of commonsense inferences. The preliminary results
    have been presented.
  address: Hong Kong, China
  author:
  - first: Aliaksandr
    full: Aliaksandr Huminski
    id: aliaksandr-huminski
    last: Huminski
  - first: Yan Bin
    full: Yan Bin Ng
    id: yan-bin-ng
    last: Ng
  - first: Kenneth
    full: Kenneth Kwok
    id: kenneth-kwok
    last: Kwok
  - first: Francis
    full: Francis Bond
    id: francis-bond
    last: Bond
  author_string: Aliaksandr Huminski, Yan Bin Ng, Kenneth Kwok, Francis Bond
  bibkey: huminski-etal-2019-commonsense
  bibtype: inproceedings
  booktitle: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  booktitle_html: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  doi: 10.18653/v1/D19-6013
  month: November
  page_first: '104'
  page_last: '112'
  pages: "104\u2013112"
  paper_id: '13'
  parent_volume_id: D19-60
  pdf: https://www.aclweb.org/anthology/D19-6013.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6013.jpg
  title: Commonsense inference in human-robot communication
  title_html: Commonsense inference in human-robot communication
  url: https://www.aclweb.org/anthology/D19-6013
  year: '2019'
D19-6014:
  abstract: Typical event sequences are an important class of commonsense knowledge.
    Formalizing the task as the generation of a next event conditioned on a current
    event, previous work in event prediction employs sequence-to-sequence (seq2seq)
    models. However, what can happen after a given event is usually diverse, a fact
    that can hardly be captured by deterministic models. In this paper, we propose
    to incorporate a conditional variational autoencoder (CVAE) into seq2seq for its
    ability to represent diverse next events as a probabilistic distribution. We further
    extend the CVAE-based seq2seq with a reconstruction mechanism to prevent the model
    from concentrating on highly typical events. To facilitate fair and systematic
    evaluation of the diversity-aware models, we also extend existing evaluation datasets
    by tying each current event to multiple next events. Experiments show that the
    CVAE-based models drastically outperform deterministic models in terms of precision
    and that the reconstruction mechanism improves the recall of CVAE-based models
    without sacrificing precision.
  address: Hong Kong, China
  author:
  - first: Hirokazu
    full: Hirokazu Kiyomaru
    id: hirokazu-kiyomaru
    last: Kiyomaru
  - first: Kazumasa
    full: Kazumasa Omura
    id: kazumasa-omura
    last: Omura
  - first: Yugo
    full: Yugo Murawaki
    id: yugo-murawaki
    last: Murawaki
  - first: Daisuke
    full: Daisuke Kawahara
    id: daisuke-kawahara
    last: Kawahara
  - first: Sadao
    full: Sadao Kurohashi
    id: sadao-kurohashi
    last: Kurohashi
  author_string: Hirokazu Kiyomaru, Kazumasa Omura, Yugo Murawaki, Daisuke Kawahara,
    Sadao Kurohashi
  bibkey: kiyomaru-etal-2019-diversity
  bibtype: inproceedings
  booktitle: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  booktitle_html: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  doi: 10.18653/v1/D19-6014
  month: November
  page_first: '113'
  page_last: '122'
  pages: "113\u2013122"
  paper_id: '14'
  parent_volume_id: D19-60
  pdf: https://www.aclweb.org/anthology/D19-6014.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6014.jpg
  title: Diversity-aware Event Prediction based on a Conditional Variational Autoencoder
    with Reconstruction
  title_html: Diversity-aware Event Prediction based on a Conditional Variational
    Autoencoder with Reconstruction
  url: https://www.aclweb.org/anthology/D19-6014
  year: '2019'
D19-6015:
  abstract: Modeling semantic plausibility requires commonsense knowledge about the
    world and has been used as a testbed for exploring various knowledge representations.
    Previous work has focused specifically on modeling physical plausibility and shown
    that distributional methods fail when tested in a supervised setting. At the same
    time, distributional models, namely large pretrained language models, have led
    to improved results for many natural language understanding tasks. In this work,
    we show that these pretrained language models are in fact effective at modeling
    physical plausibility in the supervised setting. We therefore present the more
    difficult problem of learning to model physical plausibility directly from text.
    We create a training set by extracting attested events from a large corpus, and
    we provide a baseline for training on these attested events in a self-supervised
    manner and testing on a physical plausibility task. We believe results could be
    further improved by injecting explicit commonsense knowledge into a distributional
    model.
  address: Hong Kong, China
  author:
  - first: Ian
    full: Ian Porada
    id: ian-porada
    last: Porada
  - first: Kaheer
    full: Kaheer Suleman
    id: kaheer-suleman
    last: Suleman
  - first: Jackie Chi Kit
    full: Jackie Chi Kit Cheung
    id: jackie-chi-kit-cheung
    last: Cheung
  author_string: Ian Porada, Kaheer Suleman, Jackie Chi Kit Cheung
  bibkey: porada-etal-2019-gorilla
  bibtype: inproceedings
  booktitle: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  booktitle_html: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  doi: 10.18653/v1/D19-6015
  month: November
  page_first: '123'
  page_last: '129'
  pages: "123\u2013129"
  paper_id: '15'
  parent_volume_id: D19-60
  pdf: https://www.aclweb.org/anthology/D19-6015.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6015.jpg
  title: Can a Gorilla Ride a Camel? Learning Semantic Plausibility from Text
  title_html: Can a Gorilla Ride a Camel? Learning Semantic Plausibility from Text
  url: https://www.aclweb.org/anthology/D19-6015
  year: '2019'
D19-6016:
  abstract: "Understanding common sense is important for effective natural language\
    \ reasoning. One type of common sense is how two objects compare on physical properties\
    \ such as size and weight: e.g., \u2018is a house bigger than a person?\u2019\
    . We probe whether pre-trained representations capture comparisons and find they,\
    \ in fact, have higher accuracy than previous approaches. They also generalize\
    \ to comparisons involving objects not seen during training. We investigate how\
    \ such comparisons are made: models learn a consistent ordering over all the objects\
    \ in the comparisons. Probing models have significantly higher accuracy than those\
    \ baseline models which use dataset artifacts: e.g., memorizing some words are\
    \ larger than any other word."
  address: Hong Kong, China
  attachment:
  - filename: D19-6016.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-6016.Attachment.zip
  author:
  - first: Pranav
    full: Pranav Goel
    id: pranav-goel
    last: Goel
  - first: Shi
    full: Shi Feng
    id: shi-feng
    last: Feng
  - first: Jordan
    full: Jordan Boyd-Graber
    id: jordan-boyd-graber
    last: Boyd-Graber
  author_string: Pranav Goel, Shi Feng, Jordan Boyd-Graber
  bibkey: goel-etal-2019-pre
  bibtype: inproceedings
  booktitle: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  booktitle_html: Proceedings of the First Workshop on Commonsense Inference in Natural
    Language Processing
  doi: 10.18653/v1/D19-6016
  month: November
  page_first: '130'
  page_last: '135'
  pages: "130\u2013135"
  paper_id: '16'
  parent_volume_id: D19-60
  pdf: https://www.aclweb.org/anthology/D19-6016.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6016.jpg
  title: How Pre-trained Word Representations Capture Commonsense Physical Comparisons
  title_html: How Pre-trained Word Representations Capture Commonsense Physical Comparisons
  url: https://www.aclweb.org/anthology/D19-6016
  year: '2019'
D19-6100:
  address: Hong Kong, China
  author:
  - first: Colin
    full: Colin Cherry
    id: colin-cherry
    last: Cherry
  - first: Greg
    full: Greg Durrett
    id: greg-durrett
    last: Durrett
  - first: George
    full: George Foster
    id: george-foster
    last: Foster
  - first: Reza
    full: Reza Haffari
    id: reza-haffari
    last: Haffari
  - first: Shahram
    full: Shahram Khadivi
    id: shahram-khadivi
    last: Khadivi
  - first: Nanyun
    full: Nanyun Peng
    id: nanyun-peng
    last: Peng
  - first: Xiang
    full: Xiang Ren
    id: xiang-ren
    last: Ren
  - first: Swabha
    full: Swabha Swayamdipta
    id: swabha-swayamdipta
    last: Swayamdipta
  author_string: Colin Cherry, Greg Durrett, George Foster, Reza Haffari, Shahram
    Khadivi, Nanyun Peng, Xiang Ren, Swabha Swayamdipta
  bibkey: emnlp-2019-deep
  bibtype: proceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  month: November
  paper_id: '0'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6100.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6100.jpg
  title: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  title_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  url: https://www.aclweb.org/anthology/D19-6100
  year: '2019'
D19-6101:
  abstract: New conversation topics and functionalities are constantly being added
    to conversational AI agents like Amazon Alexa and Apple Siri. As data collection
    and annotation is not scalable and is often costly, only a handful of examples
    for the new functionalities are available, which results in poor generalization
    performance. We formulate it as a Few-Shot Integration (FSI) problem where a few
    examples are used to introduce a new intent. In this paper, we study six feature
    space data augmentation methods to improve classification performance in FSI setting
    in combination with both supervised and unsupervised representation learning methods
    such as BERT. Through realistic experiments on two public conversational datasets,
    SNIPS, and the Facebook Dialog corpus, we show that data augmentation in feature
    space provides an effective way to improve intent classification performance in
    few-shot setting beyond traditional transfer learning approaches. In particular,
    we show that (a) upsampling in latent space is a competitive baseline for feature
    space augmentation (b) adding the difference between two examples to a new example
    is a simple yet effective data augmentation method.
  address: Hong Kong, China
  attachment:
  - filename: D19-6101.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-6101.Attachment.zip
  author:
  - first: Varun
    full: Varun Kumar
    id: varun-kumar
    last: Kumar
  - first: Hadrien
    full: Hadrien Glaude
    id: hadrien-glaude
    last: Glaude
  - first: Cyprien
    full: Cyprien de Lichy
    id: cyprien-de-lichy
    last: de Lichy
  - first: Wlliam
    full: Wlliam Campbell
    id: wlliam-campbell
    last: Campbell
  author_string: Varun Kumar, Hadrien Glaude, Cyprien de Lichy, Wlliam Campbell
  bibkey: kumar-etal-2019-closer
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6101
  month: November
  page_first: '1'
  page_last: '10'
  pages: "1\u201310"
  paper_id: '1'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6101.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6101.jpg
  title: A Closer Look At Feature Space Data Augmentation For Few-Shot Intent Classification
  title_html: A Closer Look At Feature Space Data Augmentation For Few-Shot Intent
    Classification
  url: https://www.aclweb.org/anthology/D19-6101
  year: '2019'
D19-6102:
  abstract: 'To overcome the lack of annotated resources in less-resourced languages,
    recent approaches have been proposed to perform unsupervised language adaptation.
    In this paper, we explore three recent proposals: Adversarial Training, Sentence
    Encoder Alignment and Shared-Private Architecture. We highlight the differences
    of these approaches in terms of unlabeled data requirements and capability to
    overcome additional domain shift in the data. A comparative analysis in two different
    tasks is conducted, namely on Sentiment Classification and Natural Language Inference.
    We show that adversarial training methods are more suitable when the source and
    target language datasets contain other variations in content besides the language
    shift. Otherwise, sentence encoder alignment methods are very effective and can
    yield scores on the target language that are close to the source language scores.'
  address: Hong Kong, China
  author:
  - first: Gil
    full: Gil Rocha
    id: gil-rocha
    last: Rocha
  - first: Henrique
    full: Henrique Lopes Cardoso
    id: henrique-lopes-cardoso
    last: Lopes Cardoso
  author_string: Gil Rocha, Henrique Lopes Cardoso
  bibkey: rocha-lopes-cardoso-2019-comparative
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6102
  month: November
  page_first: '11'
  page_last: '21'
  pages: "11\u201321"
  paper_id: '2'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6102.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6102.jpg
  title: A Comparative Analysis of Unsupervised Language Adaptation Methods
  title_html: A Comparative Analysis of Unsupervised Language Adaptation Methods
  url: https://www.aclweb.org/anthology/D19-6102
  year: '2019'
D19-6103:
  abstract: 'At present, different deep learning models are presenting high accuracy
    on popular inference datasets such as SNLI, MNLI, and SciTail. However, there
    are different indicators that those datasets can be exploited by using some simple
    linguistic patterns. This fact poses difficulties to our understanding of the
    actual capacity of machine learning models to solve the complex task of textual
    inference. We propose a new set of syntactic tasks focused on contradiction detection
    that require specific capacities over linguistic logical forms such as: Boolean
    coordination, quantifiers, definite description, and counting operators. We evaluate
    two kinds of deep learning models that implicitly exploit language structure:
    recurrent models and the Transformer network BERT. We show that although BERT
    is clearly more efficient to generalize over most logical forms, there is space
    for improvement when dealing with counting operators. Since the syntactic tasks
    can be implemented in different languages, we show a successful case of cross-lingual
    transfer learning between English and Portuguese.'
  address: Hong Kong, China
  author:
  - first: Felipe
    full: Felipe Salvatore
    id: felipe-salvatore
    last: Salvatore
  - first: Marcelo
    full: Marcelo Finger
    id: marcelo-finger
    last: Finger
  - first: Roberto
    full: Roberto Hirata Jr
    id: roberto-hirata-jr
    last: Hirata Jr
  author_string: Felipe Salvatore, Marcelo Finger, Roberto Hirata Jr
  bibkey: salvatore-etal-2019-logical
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6103
  month: November
  page_first: '22'
  page_last: '30'
  pages: "22\u201330"
  paper_id: '3'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6103.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6103.jpg
  title: A logical-based corpus for cross-lingual evaluation
  title_html: A logical-based corpus for cross-lingual evaluation
  url: https://www.aclweb.org/anthology/D19-6103
  year: '2019'
D19-6104:
  abstract: Word embeddings are an essential component in a wide range of natural
    language processing applications. However, distributional semantic models are
    known to struggle when only a small number of context sentences are available.
    Several methods have been proposed to obtain higher-quality vectors for these
    words, leveraging both this context information and sometimes the word forms themselves
    through a hybrid approach. We show that the current tasks do not suffice to evaluate
    models that use word-form information, as such models can easily leverage word
    forms in the training data that are related to word forms in the test data. We
    introduce 3 new tasks, allowing for a more balanced comparison between models.
    Furthermore, we show that hyperparameters that have largely been ignored in previous
    work can consistently improve the performance of both baseline and advanced models,
    achieving a new state of the art on 4 out of 6 tasks.
  address: Hong Kong, China
  author:
  - first: Jeroen
    full: Jeroen Van Hautte
    id: jeroen-van-hautte
    last: Van Hautte
  - first: Guy
    full: Guy Emerson
    id: guy-emerson
    last: Emerson
  - first: Marek
    full: Marek Rei
    id: marek-rei
    last: Rei
  author_string: Jeroen Van Hautte, Guy Emerson, Marek Rei
  bibkey: van-hautte-etal-2019-bad
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6104
  month: November
  page_first: '31'
  page_last: '39'
  pages: "31\u201339"
  paper_id: '4'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6104.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6104.jpg
  title: 'Bad Form: Comparing Context-Based and Form-Based Few-Shot Learning in Distributional
    Semantic Models'
  title_html: 'Bad Form: Comparing Context-Based and Form-Based Few-Shot Learning
    in Distributional Semantic Models'
  url: https://www.aclweb.org/anthology/D19-6104
  year: '2019'
D19-6105:
  abstract: 'Many architectures for multi-task learning (MTL) have been proposed to
    take advantage of transfer among tasks, often involving complex models and training
    procedures. In this paper, we ask if the sentence-level representations learned
    in previous approaches provide significant benefit beyond that provided by simply
    improving word-based representations. To investigate this question, we consider
    three techniques that ignore sequence information: a syntactically-oblivious pooling
    encoder, pre-trained non-contextual word embeddings, and unigram generative regularization.
    Compared to a state-of-the-art MTL approach to textual inference, the simple techniques
    we use yield similar performance on a universe of task combinations while reducing
    training time and model size.'
  address: Hong Kong, China
  author:
  - first: Seth
    full: Seth Ebner
    id: seth-ebner
    last: Ebner
  - first: Felicity
    full: Felicity Wang
    id: felicity-wang
    last: Wang
  - first: Benjamin
    full: Benjamin Van Durme
    id: benjamin-van-durme
    last: Van Durme
  author_string: Seth Ebner, Felicity Wang, Benjamin Van Durme
  bibkey: ebner-etal-2019-bag
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6105
  month: November
  page_first: '40'
  page_last: '46'
  pages: "40\u201346"
  paper_id: '5'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6105.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6105.jpg
  title: 'Bag-of-Words Transfer: Non-Contextual Techniques for Multi-Task Learning'
  title_html: 'Bag-of-Words Transfer: Non-Contextual Techniques for Multi-Task Learning'
  url: https://www.aclweb.org/anthology/D19-6105
  year: '2019'
D19-6106:
  abstract: Multilingual transfer learning can benefit both high- and low-resource
    languages, but the source of these improvements is not well understood. Cananical
    Correlation Analysis (CCA) of the internal representations of a pre- trained,
    multilingual BERT model reveals that the model partitions representations for
    each language rather than using a common, shared, interlingual space. This effect
    is magnified at deeper layers, suggesting that the model does not progressively
    abstract semantic con- tent while disregarding languages. Hierarchical clustering
    based on the CCA similarity scores between languages reveals a tree structure
    that mirrors the phylogenetic trees hand- designed by linguists. The subword tokenization
    employed by BERT provides a stronger bias towards such structure than character-
    and word-level tokenizations. We release a subset of the XNLI dataset translated
    into an additional 14 languages at https://www.github.com/salesforce/xnli_extension
    to assist further research into multilingual representations.
  address: Hong Kong, China
  author:
  - first: Jasdeep
    full: Jasdeep Singh
    id: jasdeep-singh
    last: Singh
  - first: Bryan
    full: Bryan McCann
    id: bryan-mccann
    last: McCann
  - first: Richard
    full: Richard Socher
    id: richard-socher
    last: Socher
  - first: Caiming
    full: Caiming Xiong
    id: caiming-xiong
    last: Xiong
  author_string: Jasdeep Singh, Bryan McCann, Richard Socher, Caiming Xiong
  bibkey: singh-etal-2019-bert
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6106
  month: November
  page_first: '47'
  page_last: '55'
  pages: "47\u201355"
  paper_id: '6'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6106.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6106.jpg
  title: BERT is Not an Interlingua and the Bias of Tokenization
  title_html: <span class="acl-fixed-case">BERT</span> is Not an Interlingua and the
    Bias of Tokenization
  url: https://www.aclweb.org/anthology/D19-6106
  year: '2019'
D19-6107:
  abstract: 'Entities, which refer to distinct objects in the real world, can be viewed
    as language universals and used as effective signals to generate less ambiguous
    semantic representations and align multiple languages. We propose a novel method,
    CLEW, to generate cross-lingual data that is a mix of entities and contextual
    words based on Wikipedia. We replace each anchor link in the source language with
    its corresponding entity title in the target language if it exists, or in the
    source language otherwise. A cross-lingual joint entity and word embedding learned
    from this kind of data not only can disambiguate linkable entities but can also
    effectively represent unlinkable entities. Because this multilingual common space
    directly relates the semantics of contextual words in the source language to that
    of entities in the target language, we leverage it for unsupervised cross-lingual
    entity linking. Experimental results show that CLEW significantly advances the
    state-of-the-art: up to 3.1% absolute F-score gain for unsupervised cross-lingual
    entity linking. Moreover, it provides reliable alignment on both the word/entity
    level and the sentence level, and thus we use it to mine parallel sentences for
    all (302, 2) language pairs in Wikipedia.'
  address: Hong Kong, China
  author:
  - first: Xiaoman
    full: Xiaoman Pan
    id: xiaoman-pan
    last: Pan
  - first: Thamme
    full: Thamme Gowda
    id: thamme-gowda
    last: Gowda
  - first: Heng
    full: Heng Ji
    id: heng-ji
    last: Ji
  - first: Jonathan
    full: Jonathan May
    id: jonathan-may
    last: May
  - first: Scott
    full: Scott Miller
    id: scott-miller
    last: Miller
  author_string: Xiaoman Pan, Thamme Gowda, Heng Ji, Jonathan May, Scott Miller
  bibkey: pan-etal-2019-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6107
  month: November
  page_first: '56'
  page_last: '66'
  pages: "56\u201366"
  paper_id: '7'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6107.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6107.jpg
  title: Cross-lingual Joint Entity and Word Embedding to Improve Entity Linking and
    Parallel Sentence Mining
  title_html: Cross-lingual Joint Entity and Word Embedding to Improve Entity Linking
    and Parallel Sentence Mining
  url: https://www.aclweb.org/anthology/D19-6107
  year: '2019'
D19-6108:
  abstract: We present a novel framework to deal with relation extraction tasks in
    cases where there is complete lack of supervision, either in the form of gold
    annotations, or relations from a knowledge base. Our approach leverages syntactic
    parsing and pre-trained word embeddings to extract few but precise relations,
    which are then used to annotate a larger corpus, in a manner identical to distant
    supervision. The resulting data set is employed to fine tune a pre-trained BERT
    model in order to perform relation extraction. Empirical evaluation on four data
    sets from the biomedical domain shows that our method significantly outperforms
    two simple baselines for unsupervised relation extraction and, even if not using
    any supervision at all, achieves slightly worse results than the state-of-the-art
    in three out of four data sets. Importantly, we show that it is possible to successfully
    fine tune a large pretrained language model with noisy data, as opposed to previous
    works that rely on gold data for fine tuning.
  address: Hong Kong, China
  author:
  - first: Yannis
    full: Yannis Papanikolaou
    id: yannis-papanikolaou
    last: Papanikolaou
  - first: Ian
    full: Ian Roberts
    id: ian-roberts
    last: Roberts
  - first: Andrea
    full: Andrea Pierleoni
    id: andrea-pierleoni
    last: Pierleoni
  author_string: Yannis Papanikolaou, Ian Roberts, Andrea Pierleoni
  bibkey: papanikolaou-etal-2019-deep
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6108
  month: November
  page_first: '67'
  page_last: '75'
  pages: "67\u201375"
  paper_id: '8'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6108.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6108.jpg
  title: Deep Bidirectional Transformers for Relation Extraction without Supervision
  title_html: Deep Bidirectional Transformers for Relation Extraction without Supervision
  url: https://www.aclweb.org/anthology/D19-6108
  year: '2019'
D19-6109:
  abstract: The performance of deep neural models can deteriorate substantially when
    there is a domain shift between training and test data. For example, the pre-trained
    BERT model can be easily fine-tuned with just one additional output layer to create
    a state-of-the-art model for a wide range of tasks. However, the fine-tuned BERT
    model suffers considerably at zero-shot when applied to a different domain. In
    this paper, we present a novel two-step domain adaptation framework based on curriculum
    learning and domain-discriminative data selection. The domain adaptation is conducted
    in a mostly unsupervised manner using a small target domain validation set for
    hyper-parameter tuning. We tested the framework on four large public datasets
    with different domain similarities and task types. Our framework outperforms a
    popular discrepancy-based domain adaptation method on most transfer tasks while
    consuming only a fraction of the training budget.
  address: Hong Kong, China
  author:
  - first: Xiaofei
    full: Xiaofei Ma
    id: xiaofei-ma
    last: Ma
  - first: Peng
    full: Peng Xu
    id: peng-xu
    last: Xu
  - first: Zhiguo
    full: Zhiguo Wang
    id: zhiguo-wang
    last: Wang
  - first: Ramesh
    full: Ramesh Nallapati
    id: ramesh-nallapati
    last: Nallapati
  - first: Bing
    full: Bing Xiang
    id: bing-xiang
    last: Xiang
  author_string: Xiaofei Ma, Peng Xu, Zhiguo Wang, Ramesh Nallapati, Bing Xiang
  bibkey: ma-etal-2019-domain
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6109
  month: November
  page_first: '76'
  page_last: '83'
  pages: "76\u201383"
  paper_id: '9'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6109.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6109.jpg
  title: Domain Adaptation with BERT-based Domain Classification and Data Selection
  title_html: Domain Adaptation with <span class="acl-fixed-case">BERT</span>-based
    Domain Classification and Data Selection
  url: https://www.aclweb.org/anthology/D19-6109
  year: '2019'
D19-6110:
  abstract: "Active learning (AL) for machine translation (MT) has been well-studied\
    \ for the phrase-based MT paradigm. Several AL algorithms for data sampling have\
    \ been proposed over the years. However, given the rapid advancement in neural\
    \ methods, these algorithms have not been thoroughly investigated in the context\
    \ of neural MT (NMT). In this work, we address this missing aspect by conducting\
    \ a systematic comparison of different AL methods in a simulated AL framework.\
    \ Our experimental setup to compare different AL methods uses: i) State-of-the-art\
    \ NMT architecture to achieve realistic results; and ii) the same dataset (WMT\u2019\
    13 English-Spanish) to have fair comparison across different methods. We then\
    \ demonstrate how recent advancements in unsupervised pre-training and paraphrastic\
    \ embedding can be used to improve existing AL methods. Finally, we propose a\
    \ neural extension for an AL sampling method used in the context of phrase-based\
    \ MT - Round Trip Translation Likelihood (RTTL). RTTL uses a bidirectional translation\
    \ model to estimate the loss of information during translation and outperforms\
    \ previous methods."
  address: Hong Kong, China
  author:
  - first: Xiangkai
    full: Xiangkai Zeng
    id: xiangkai-zeng
    last: Zeng
  - first: Sarthak
    full: Sarthak Garg
    id: sarthak-garg
    last: Garg
  - first: Rajen
    full: Rajen Chatterjee
    id: rajen-chatterjee
    last: Chatterjee
  - first: Udhyakumar
    full: Udhyakumar Nallasamy
    id: udhyakumar-nallasamy
    last: Nallasamy
  - first: Matthias
    full: Matthias Paulik
    id: matthias-paulik
    last: Paulik
  author_string: Xiangkai Zeng, Sarthak Garg, Rajen Chatterjee, Udhyakumar Nallasamy,
    Matthias Paulik
  bibkey: zeng-etal-2019-empirical
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6110
  month: November
  page_first: '84'
  page_last: '93'
  pages: "84\u201393"
  paper_id: '10'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6110.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6110.jpg
  title: Empirical Evaluation of Active Learning Techniques for Neural MT
  title_html: Empirical Evaluation of Active Learning Techniques for Neural <span
    class="acl-fixed-case">MT</span>
  url: https://www.aclweb.org/anthology/D19-6110
  year: '2019'
D19-6111:
  abstract: "Semantic parsers are used to convert user\u2019s natural language commands\
    \ to executable logical form in intelligent personal agents. Labeled datasets\
    \ required to train such parsers are expensive to collect, and are never comprehensive.\
    \ As a result, for effective post-deployment domain adaptation and personalization,\
    \ semantic parsers are continuously retrained to learn new user vocabulary and\
    \ paraphrase variety. However, state-of-the art attention based neural parsers\
    \ are slow to retrain which inhibits real time domain adaptation. Secondly, these\
    \ parsers do not leverage numerous paraphrases already present in the training\
    \ dataset. Designing parsers which can simultaneously maintain high accuracy and\
    \ fast retraining time is challenging. In this paper, we present novel paraphrase\
    \ attention based sequence-to-sequence/tree parsers which support fast near real\
    \ time retraining. In addition, our parsers often boost accuracy by jointly modeling\
    \ the semantic dependencies of paraphrases. We evaluate our model on benchmark\
    \ datasets to demonstrate upto 9X speedup in retraining time compared to existing\
    \ parsers, as well as achieving state-of-the-art accuracy."
  address: Hong Kong, China
  author:
  - first: Avik
    full: Avik Ray
    id: avik-ray
    last: Ray
  - first: Yilin
    full: Yilin Shen
    id: yilin-shen
    last: Shen
  - first: Hongxia
    full: Hongxia Jin
    id: hongxia-jin
    last: Jin
  author_string: Avik Ray, Yilin Shen, Hongxia Jin
  bibkey: ray-etal-2019-fast
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6111
  month: November
  page_first: '94'
  page_last: '103'
  pages: "94\u2013103"
  paper_id: '11'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6111.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6111.jpg
  title: Fast Domain Adaptation of Semantic Parsers via Paraphrase Attention
  title_html: Fast Domain Adaptation of Semantic Parsers via Paraphrase Attention
  url: https://www.aclweb.org/anthology/D19-6111
  year: '2019'
D19-6112:
  abstract: Historical text normalization often relies on small training datasets.
    Recent work has shown that multi-task learning can lead to significant improvements
    by exploiting synergies with related datasets, but there has been no systematic
    study of different multi-task learning architectures. This paper evaluates 63
    multi-task learning configurations for sequence-to-sequence-based historical text
    normalization across ten datasets from eight languages, using autoencoding, grapheme-to-phoneme
    mapping, and lemmatization as auxiliary tasks. We observe consistent, significant
    improvements across languages when training data for the target task is limited,
    but minimal or no improvements when training data is abundant. We also show that
    zero-shot learning outperforms the simple, but relatively strong, identity baseline.
  address: Hong Kong, China
  author:
  - first: Marcel
    full: Marcel Bollmann
    id: marcel-bollmann
    last: Bollmann
  - first: Natalia
    full: Natalia Korchagina
    id: natalia-korchagina
    last: Korchagina
  - first: Anders
    full: "Anders S\xF8gaard"
    id: anders-sogaard
    last: "S\xF8gaard"
  author_string: "Marcel Bollmann, Natalia Korchagina, Anders S\xF8gaard"
  bibkey: bollmann-etal-2019-shot
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6112
  month: November
  page_first: '104'
  page_last: '114'
  pages: "104\u2013114"
  paper_id: '12'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6112.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6112.jpg
  title: Few-Shot and Zero-Shot Learning for Historical Text Normalization
  title_html: Few-Shot and Zero-Shot Learning for Historical Text Normalization
  url: https://www.aclweb.org/anthology/D19-6112
  year: '2019'
D19-6113:
  abstract: "Recent research on cross-lingual transfer show state-of-the-art results\
    \ on benchmark datasets using pre-trained language representation models (PLRM)\
    \ like BERT. These results are achieved with the traditional training approaches,\
    \ such as Zero-shot with no data, Translate-train or Translate-test with machine\
    \ translated data. In this work, we propose an approach of \u201CMultilingual\
    \ Co-training\u201D (MCT) where we augment the expert annotated dataset in the\
    \ source language (English) with the corresponding machine translations in the\
    \ target languages (e.g. Arabic, Spanish) and fine-tune the PLRM jointly. We observe\
    \ that the proposed approach provides consistent gains in the performance of BERT\
    \ for multiple benchmark datasets (e.g. 1.0% gain on MLDocs, and 1.2% gain on\
    \ XNLI over translate-train with BERT), while requiring a single model for multiple\
    \ languages. We further consider a FAQ dataset where the available English test\
    \ dataset is translated by experts into Arabic and Spanish. On such a dataset,\
    \ we observe an average gain of 4.9% over all other cross-lingual transfer protocols\
    \ with BERT. We further observe that domain-specific joint pre-training of the\
    \ PLRM using HR policy documents in English along with the machine translations\
    \ in the target languages, followed by the joint finetuning, provides a further\
    \ improvement of 2.8% in average accuracy."
  address: Hong Kong, China
  author:
  - first: Mayur
    full: Mayur Patidar
    id: mayur-patidar
    last: Patidar
  - first: Surabhi
    full: Surabhi Kumari
    id: surabhi-kumari
    last: Kumari
  - first: Manasi
    full: Manasi Patwardhan
    id: manasi-patwardhan
    last: Patwardhan
  - first: Shirish
    full: Shirish Karande
    id: shirish-karande
    last: Karande
  - first: Puneet
    full: Puneet Agarwal
    id: puneet-agarwal
    last: Agarwal
  - first: Lovekesh
    full: Lovekesh Vig
    id: lovekesh-vig
    last: Vig
  - first: Gautam
    full: Gautam Shroff
    id: gautam-shroff
    last: Shroff
  author_string: Mayur Patidar, Surabhi Kumari, Manasi Patwardhan, Shirish Karande,
    Puneet Agarwal, Lovekesh Vig, Gautam Shroff
  bibkey: patidar-etal-2019-monolingual
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6113
  month: November
  page_first: '115'
  page_last: '123'
  pages: "115\u2013123"
  paper_id: '13'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6113.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6113.jpg
  title: From Monolingual to Multilingual FAQ Assistant using Multilingual Co-training
  title_html: From Monolingual to Multilingual <span class="acl-fixed-case">FAQ</span>
    Assistant using Multilingual Co-training
  url: https://www.aclweb.org/anthology/D19-6113
  year: '2019'
D19-6114:
  abstract: 'Over the past year, the emergence of transfer learning with large-scale
    language models (LM) has led to dramatic performance improvements across a broad
    range of natural language understanding tasks. However, the size and memory footprint
    of these large LMs often makes them difficult to deploy in many scenarios (e.g.
    on mobile phones). Recent research points to knowledge distillation as a potential
    solution, showing that when training data for a given task is abundant, it is
    possible to distill a large (teacher) LM into a small task-specific (student)
    network with minimal loss of performance. However, when such data is scarce, there
    remains a significant performance gap between large pretrained LMs and smaller
    task-specific models, even when training via distillation. In this paper, we bridge
    this gap with a novel training approach, called generation-distillation, that
    leverages large finetuned LMs in two ways: (1) to generate new (unlabeled) training
    examples, and (2) to distill their knowledge into a small network using these
    examples. Across three low-resource text classification datsets, we achieve comparable
    performance to BERT while using 300 times fewer parameters, and we outperform
    prior approaches to distillation for text classification while using 3 times fewer
    parameters.'
  address: Hong Kong, China
  author:
  - first: Luke
    full: Luke Melas-Kyriazi
    id: luke-melas-kyriazi
    last: Melas-Kyriazi
  - first: George
    full: George Han
    id: george-han
    last: Han
  - first: Celine
    full: Celine Liang
    id: celine-liang
    last: Liang
  author_string: Luke Melas-Kyriazi, George Han, Celine Liang
  bibkey: melas-kyriazi-etal-2019-generation
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6114
  month: November
  page_first: '124'
  page_last: '131'
  pages: "124\u2013131"
  paper_id: '14'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6114.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6114.jpg
  title: Generation-Distillation for Efficient Natural Language Understanding in Low-Data
    Settings
  title_html: Generation-Distillation for Efficient Natural Language Understanding
    in Low-Data Settings
  url: https://www.aclweb.org/anthology/D19-6114
  year: '2019'
D19-6115:
  abstract: "Statistical natural language inference (NLI) models are susceptible to\
    \ learning dataset bias: superficial cues that happen to associate with the label\
    \ on a particular dataset, but are not useful in general, e.g., negation words\
    \ indicate contradiction. As exposed by several recent challenge datasets, these\
    \ models perform poorly when such association is absent, e.g., predicting that\
    \ \u201CI love dogs.\u201D contradicts \u201CI don\u2019t love cats.\u201D. Our\
    \ goal is to design learning algorithms that guard against known dataset bias.\
    \ We formalize the concept of dataset bias under the framework of distribution\
    \ shift and present a simple debiasing algorithm based on residual fitting, which\
    \ we call DRiFt. We first learn a biased model that only uses features that are\
    \ known to relate to dataset bias. Then, we train a debiased model that fits to\
    \ the residual of the biased model, focusing on examples that cannot be predicted\
    \ well by biased features only. We use DRiFt to train three high-performing NLI\
    \ models on two benchmark datasets, SNLI and MNLI. Our debiased models achieve\
    \ significant gains over baseline models on two challenge test sets, while maintaining\
    \ reasonable performance on the original test sets."
  address: Hong Kong, China
  attachment:
  - filename: D19-6115.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-6115.Attachment.pdf
  author:
  - first: He
    full: He He
    id: he-he
    last: He
  - first: Sheng
    full: Sheng Zha
    id: sheng-zha
    last: Zha
  - first: Haohan
    full: Haohan Wang
    id: haohan-wang
    last: Wang
  author_string: He He, Sheng Zha, Haohan Wang
  bibkey: he-etal-2019-unlearn
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6115
  month: November
  page_first: '132'
  page_last: '142'
  pages: "132\u2013142"
  paper_id: '15'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6115.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6115.jpg
  title: Unlearn Dataset Bias in Natural Language Inference by Fitting the Residual
  title_html: Unlearn Dataset Bias in Natural Language Inference by Fitting the Residual
  url: https://www.aclweb.org/anthology/D19-6115
  year: '2019'
D19-6116:
  abstract: Traditional text classifiers are limited to predicting over a fixed set
    of labels. However, in many real-world applications the label set is frequently
    changing. For example, in intent classification, new intents may be added over
    time while others are removed. We propose to address the problem of dynamic text
    classification by replacing the traditional, fixed-size output layer with a learned,
    semantically meaningful metric space. Here the distances between textual inputs
    are optimized to perform nearest-neighbor classification across overlapping label
    sets. Changing the label set does not involve removing parameters, but rather
    simply adding or removing support points in the metric space. Then the learned
    metric can be fine-tuned with only a few additional training examples. We demonstrate
    that this simple strategy is robust to changes in the label space. Furthermore,
    our results show that learning a non-Euclidean metric can improve performance
    in the low data regime, suggesting that further work on metric spaces may benefit
    low-resource research.
  address: Hong Kong, China
  author:
  - first: Jeremy
    full: Jeremy Wohlwend
    id: jeremy-wohlwend
    last: Wohlwend
  - first: Ethan R.
    full: Ethan R. Elenberg
    id: ethan-r-elenberg
    last: Elenberg
  - first: Sam
    full: Sam Altschul
    id: sam-altschul
    last: Altschul
  - first: Shawn
    full: Shawn Henry
    id: shawn-henry
    last: Henry
  - first: Tao
    full: Tao Lei
    id: tao-lei
    last: Lei
  author_string: Jeremy Wohlwend, Ethan R. Elenberg, Sam Altschul, Shawn Henry, Tao
    Lei
  bibkey: wohlwend-etal-2019-metric
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6116
  month: November
  page_first: '143'
  page_last: '152'
  pages: "143\u2013152"
  paper_id: '16'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6116.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6116.jpg
  title: Metric Learning for Dynamic Text Classification
  title_html: Metric Learning for Dynamic Text Classification
  url: https://www.aclweb.org/anthology/D19-6116
  year: '2019'
D19-6117:
  abstract: The Lottery Ticket Hypothesis suggests large, over-parameterized neural
    networks consist of small, sparse subnetworks that can be trained in isolation
    to reach a similar (or better) test accuracy. However, the initialization and
    generalizability of the obtained sparse subnetworks have been recently called
    into question. Our work focuses on evaluating the initialization of sparse subnetworks
    under distributional shifts. Specifically, we investigate the extent to which
    a sparse subnetwork obtained in a source domain can be re-trained in isolation
    in a dissimilar, target domain. In addition, we examine the effects of different
    initialization strategies at transfer-time. Our experiments show that sparse subnetworks
    obtained through lottery ticket training do not simply overfit to particular domains,
    but rather reflect an inductive bias of deep neural networks that can be exploited
    in multiple domains.
  address: Hong Kong, China
  author:
  - first: Shrey
    full: Shrey Desai
    id: shrey-desai
    last: Desai
  - first: Hongyuan
    full: Hongyuan Zhan
    id: hongyuan-zhan
    last: Zhan
  - first: Ahmed
    full: Ahmed Aly
    id: ahmed-aly
    last: Aly
  author_string: Shrey Desai, Hongyuan Zhan, Ahmed Aly
  bibkey: desai-etal-2019-evaluating
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6117
  month: November
  page_first: '153'
  page_last: '162'
  pages: "153\u2013162"
  paper_id: '17'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6117.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6117.jpg
  title: Evaluating Lottery Tickets Under Distributional Shifts
  title_html: Evaluating Lottery Tickets Under Distributional Shifts
  url: https://www.aclweb.org/anthology/D19-6117
  year: '2019'
D19-6118:
  abstract: 'Cross-lingual dependency parsing involves transferring syntactic knowledge
    from one language to another. It is a crucial component for inducing dependency
    parsers in low-resource scenarios where no training data for a language exists.
    Using Faroese as the target language, we compare two approaches using annotation
    projection: first, projecting from multiple monolingual source models; second,
    projecting from a single polyglot model which is trained on the combination of
    all source languages. Furthermore, we reproduce multi-source projection (Tyers
    et al., 2018), in which dependency trees of multiple sources are combined. Finally,
    we apply multi-treebank modelling to the projected treebanks, in addition to or
    alternatively to polyglot modelling on the source side. We find that polyglot
    training on the source languages produces an overall trend of better results on
    the target language but the single best result for the target language is obtained
    by projecting from monolingual source parsing models and then training multi-treebank
    POS tagging and parsing models on the target side.'
  address: Hong Kong, China
  author:
  - first: James
    full: James Barry
    id: james-barry
    last: Barry
  - first: Joachim
    full: Joachim Wagner
    id: joachim-wagner
    last: Wagner
  - first: Jennifer
    full: Jennifer Foster
    id: jennifer-foster
    last: Foster
  author_string: James Barry, Joachim Wagner, Jennifer Foster
  bibkey: barry-etal-2019-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6118
  month: November
  page_first: '163'
  page_last: '174'
  pages: "163\u2013174"
  paper_id: '18'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6118.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6118.jpg
  title: 'Cross-lingual Parsing with Polyglot Training and Multi-treebank Learning:
    A Faroese Case Study'
  title_html: 'Cross-lingual Parsing with Polyglot Training and Multi-treebank Learning:
    A Faroese Case Study'
  url: https://www.aclweb.org/anthology/D19-6118
  year: '2019'
D19-6119:
  abstract: "Short Answer Grading (SAG) is a task of scoring students\u2019 answers\
    \ in examinations. Most existing SAG systems predict scores based only on the\
    \ answers, including the model used as base line in this paper, which gives the-state-of-the-art\
    \ performance. But they ignore important evaluation criteria such as rubrics,\
    \ which play a crucial role for evaluating answers in real-world situations. In\
    \ this paper, we present a method to inject information from rubrics into SAG\
    \ systems. We implement our approach on top of word-level attention mechanism\
    \ to introduce the rubric information, in order to locate information in each\
    \ answer that are highly related to the score. Our experimental results demonstrate\
    \ that injecting rubric information effectively contributes to the performance\
    \ improvement and that our proposed model outperforms the state-of-the-art SAG\
    \ model on the widely used ASAP-SAS dataset under low-resource settings."
  address: Hong Kong, China
  author:
  - first: Tianqi
    full: Tianqi Wang
    id: tianqi-wang
    last: Wang
  - first: Naoya
    full: Naoya Inoue
    id: naoya-inoue
    last: Inoue
  - first: Hiroki
    full: Hiroki Ouchi
    id: hiroki-ouchi
    last: Ouchi
  - first: Tomoya
    full: Tomoya Mizumoto
    id: tomoya-mizumoto
    last: Mizumoto
  - first: Kentaro
    full: Kentaro Inui
    id: kentaro-inui
    last: Inui
  author_string: Tianqi Wang, Naoya Inoue, Hiroki Ouchi, Tomoya Mizumoto, Kentaro
    Inui
  bibkey: wang-etal-2019-inject
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6119
  month: November
  page_first: '175'
  page_last: '182'
  pages: "175\u2013182"
  paper_id: '19'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6119.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6119.jpg
  title: Inject Rubrics into Short Answer Grading System
  title_html: Inject Rubrics into Short Answer Grading System
  url: https://www.aclweb.org/anthology/D19-6119
  year: '2019'
D19-6120:
  abstract: Supervised learning models are typically trained on a single dataset and
    the performance of these models rely heavily on the size of the dataset i.e.,
    the amount of data available with ground truth. Learning algorithms try to generalize
    solely based on the data that it is presented with during the training. In this
    work, we propose an inductive transfer learning method that can augment learning
    models by infusing similar instances from different learning tasks in Natural
    Language Processing (NLP) domain. We propose to use instance representations from
    a source dataset, without inheriting anything else from the source learning model.
    Representations of the instances of source and target datasets are learned, retrieval
    of relevant source instances is performed using soft-attention mechanism and locality
    sensitive hashing and then augmented into the model during training on the target
    dataset. Therefore, while learning from a training data, we also simultaneously
    exploit and infuse relevant local instance-level information from an external
    data. Using this approach we have shown significant improvements over the baseline
    for three major news classification datasets. Experimental evaluations also show
    that the proposed approach reduces dependency on labeled data by a significant
    margin for comparable performance. With our proposed cross dataset learning procedure
    we show that one can achieve competitive/better performance than learning from
    a single dataset.
  address: Hong Kong, China
  author:
  - first: Somnath
    full: Somnath Basu Roy Chowdhury
    id: somnath-basu-roy-chowdhury
    last: Basu Roy Chowdhury
  - first: Annervaz
    full: Annervaz M
    id: annervaz-m
    last: M
  - first: Ambedkar
    full: Ambedkar Dukkipati
    id: ambedkar-dukkipati
    last: Dukkipati
  author_string: Somnath Basu Roy Chowdhury, Annervaz M, Ambedkar Dukkipati
  bibkey: basu-roy-chowdhury-etal-2019-instance
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6120
  month: November
  page_first: '183'
  page_last: '191'
  pages: "183\u2013191"
  paper_id: '20'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6120.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6120.jpg
  title: Instance-based Inductive Deep Transfer Learning by Cross-Dataset Querying
    with Locality Sensitive Hashing
  title_html: Instance-based Inductive Deep Transfer Learning by Cross-Dataset Querying
    with Locality Sensitive Hashing
  url: https://www.aclweb.org/anthology/D19-6120
  year: '2019'
D19-6121:
  abstract: "Grapheme-to-phoneme conversion (g2p) is the task of predicting the pronunciation\
    \ of words from their orthographic representation. His- torically, g2p systems\
    \ were transition- or rule- based, making generalization beyond a mono- lingual\
    \ (high resource) domain impractical. Recently, neural architectures have enabled\
    \ multilingual systems to generalize widely; however, all systems to date have\
    \ been trained only on spelling-pronunciation pairs. We hy- pothesize that the\
    \ sequences of IPA characters used to represent pronunciation do not capture its\
    \ full nuance, especially when cleaned to fa- cilitate machine learning. We leverage\
    \ audio data as an auxiliary modality in a multi-task training process to learn\
    \ a more optimal inter- mediate representation of source graphemes; this is the\
    \ first multimodal model proposed for multilingual g2p. Our approach is highly\
    \ ef- fective: on our in-domain test set, our mul- timodal model reduces phoneme\
    \ error rate to 2.46%, a more than 65% decrease compared to our implementation\
    \ of a unimodal spelling- pronunciation model\u2014which itself achieves state-of-the-art\
    \ results on the Wiktionary test set. The advantages of the multimodal model generalize\
    \ to wholly unseen languages, reduc- ing phoneme error rate on our out-of-domain\
    \ test set to 6.39% from the unimodal 8.21%, a more than 20% relative decrease.\
    \ Further- more, our training and test sets are composed primarily of low-resource\
    \ languages, demon- strating that our multimodal approach remains useful when\
    \ training data are constrained."
  address: Hong Kong, China
  author:
  - first: James
    full: James Route
    id: james-route
    last: Route
  - first: Steven
    full: Steven Hillis
    id: steven-hillis
    last: Hillis
  - first: Isak
    full: Isak Czeresnia Etinger
    id: isak-czeresnia-etinger
    last: Czeresnia Etinger
  - first: Han
    full: Han Zhang
    id: han-zhang
    last: Zhang
  - first: Alan W
    full: Alan W Black
    id: alan-w-black
    last: Black
  author_string: James Route, Steven Hillis, Isak Czeresnia Etinger, Han Zhang, Alan
    W Black
  bibkey: route-etal-2019-multimodal
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6121
  month: November
  page_first: '192'
  page_last: '201'
  pages: "192\u2013201"
  paper_id: '21'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6121.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6121.jpg
  title: Multimodal, Multilingual Grapheme-to-Phoneme Conversion for Low-Resource
    Languages
  title_html: Multimodal, Multilingual Grapheme-to-Phoneme Conversion for Low-Resource
    Languages
  url: https://www.aclweb.org/anthology/D19-6121
  year: '2019'
D19-6122:
  abstract: Knowledge distillation can effectively transfer knowledge from BERT, a
    deep language representation model, to traditional, shallow word embedding-based
    neural networks, helping them approach or exceed the quality of other heavyweight
    language representation models. As shown in previous work, critical to this distillation
    procedure is the construction of an unlabeled transfer dataset, which enables
    effective knowledge transfer. To create transfer set examples, we propose to sample
    from pretrained language models fine-tuned on task-specific text. Unlike previous
    techniques, this directly captures the purpose of the transfer set. We hypothesize
    that this principled, general approach outperforms rule-based techniques. On four
    datasets in sentiment classification, sentence similarity, and linguistic acceptability,
    we show that our approach improves upon previous methods. We outperform OpenAI
    GPT, a deep pretrained transformer, on three of the datasets, while using a single-layer
    bidirectional LSTM that runs at least ten times faster.
  address: Hong Kong, China
  author:
  - first: Raphael
    full: Raphael Tang
    id: raphael-tang
    last: Tang
  - first: Yao
    full: Yao Lu
    id: yao-lu
    last: Lu
  - first: Jimmy
    full: Jimmy Lin
    id: jimmy-lin
    last: Lin
  author_string: Raphael Tang, Yao Lu, Jimmy Lin
  bibkey: tang-etal-2019-natural
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6122
  month: November
  page_first: '202'
  page_last: '208'
  pages: "202\u2013208"
  paper_id: '22'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6122.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6122.jpg
  title: Natural Language Generation for Effective Knowledge Distillation
  title_html: Natural Language Generation for Effective Knowledge Distillation
  url: https://www.aclweb.org/anthology/D19-6122
  year: '2019'
D19-6123:
  abstract: "Recently, neural network models which automatically infer syntactic structure\
    \ from raw text have started to achieve promising results. However, earlier work\
    \ on unsupervised parsing shows large performance differences between non-neural\
    \ models trained on corpora in different languages, even for comparable amounts\
    \ of data. With that in mind, we train instances of the PRPN architecture (Shen\
    \ et al., 2018)\u2014one of these unsupervised neural network parsers\u2014for\
    \ Arabic, Chinese, English, and German. We find that (i) the model strongly outperforms\
    \ trivial baselines and, thus, acquires at least some parsing ability for all\
    \ languages; (ii) good hyperparameter values seem to be universal; (iii) how the\
    \ model benefits from larger training set sizes depends on the corpus, with the\
    \ model achieving the largest performance gains when increasing the number of\
    \ sentences from 2,500 to 12,500 for English. In addition, we show that, by sharing\
    \ parameters between the related languages German and English, we can improve\
    \ the model\u2019s unsupervised parsing F1 score by up to 4% in the low-resource\
    \ setting."
  address: Hong Kong, China
  author:
  - first: Katharina
    full: Katharina Kann
    id: katharina-kann
    last: Kann
  - first: Anhad
    full: Anhad Mohananey
    id: anhad-mohananey
    last: Mohananey
  - first: Samuel R.
    full: Samuel R. Bowman
    id: samuel-bowman
    last: Bowman
  - first: Kyunghyun
    full: Kyunghyun Cho
    id: kyunghyun-cho
    last: Cho
  author_string: Katharina Kann, Anhad Mohananey, Samuel R. Bowman, Kyunghyun Cho
  bibkey: kann-etal-2019-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6123
  month: November
  page_first: '209'
  page_last: '218'
  pages: "209\u2013218"
  paper_id: '23'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6123.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6123.jpg
  title: Neural Unsupervised Parsing Beyond English
  title_html: Neural Unsupervised Parsing Beyond <span class="acl-fixed-case">E</span>nglish
  url: https://www.aclweb.org/anthology/D19-6123
  year: '2019'
D19-6124:
  abstract: Argument component extraction is a challenging and complex high-level
    semantic extraction task. As such, it is both expensive to annotate (meaning training
    data is limited and low-resource by nature), and hard for current-generation deep
    learning methods to model. In this paper, we reevaluate the performance of state-of-the-art
    approaches in both single- and multi-task learning settings using combinations
    of character-level, GloVe, ELMo, and BERT encodings using standard BiLSTM-CRF
    encoders. We use evaluation metrics that are more consistent with evaluation practice
    in named entity recognition to understand how well current baselines address this
    challenge and compare their performance to lower-level semantic tasks such as
    CoNLL named entity recognition. We find that performance utilizing various pre-trained
    representations and training methodologies often leaves a lot to be desired as
    it currently stands, and suggest future pathways for improvement.
  address: Hong Kong, China
  author:
  - first: Anirudh
    full: Anirudh Joshi
    id: anirudh-joshi
    last: Joshi
  - first: Timothy
    full: Timothy Baldwin
    id: timothy-baldwin
    last: Baldwin
  - first: Richard
    full: Richard Sinnott
    id: richard-sinnott
    last: Sinnott
  - first: Cecile
    full: Cecile Paris
    id: cecile-paris
    last: Paris
  author_string: Anirudh Joshi, Timothy Baldwin, Richard Sinnott, Cecile Paris
  bibkey: joshi-etal-2019-reevaluating
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6124
  month: November
  page_first: '219'
  page_last: '224'
  pages: "219\u2013224"
  paper_id: '24'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6124.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6124.jpg
  title: Reevaluating Argument Component Extraction in Low Resource Settings
  title_html: Reevaluating Argument Component Extraction in Low Resource Settings
  url: https://www.aclweb.org/anthology/D19-6124
  year: '2019'
D19-6125:
  abstract: Existing named entity recognition (NER) systems rely on large amounts
    of human-labeled data for supervision. However, obtaining large-scale annotated
    data is challenging particularly in specific domains like health-care, e-commerce
    and so on. Given the availability of domain specific knowledge resources, (e.g.,
    ontologies, dictionaries), distant supervision is a solution to generate automatically
    labeled training data to reduce human effort. The outcome of distant supervision
    for NER, however, is often noisy. False positive and false negative instances
    are the main issues that reduce performance on this kind of auto-generated data.
    In this paper, we explore distant supervision in a supervised setup. We adopt
    a technique of partial annotation to address false negative cases and implement
    a reinforcement learning strategy with a neural network policy to identify false
    positive instances. Our results establish a new state-of-the-art on four benchmark
    datasets taken from different domains and different languages. We then go on to
    show that our model reduces the amount of manually annotated data required to
    perform NER in a new domain.
  address: Hong Kong, China
  author:
  - first: Farhad
    full: Farhad Nooralahzadeh
    id: farhad-nooralahzadeh
    last: Nooralahzadeh
  - first: Jan Tore
    full: "Jan Tore L\xF8nning"
    id: jan-tore-lonning
    last: "L\xF8nning"
  - first: Lilja
    full: "Lilja \xD8vrelid"
    id: lilja-ovrelid
    last: "\xD8vrelid"
  author_string: "Farhad Nooralahzadeh, Jan Tore L\xF8nning, Lilja \xD8vrelid"
  bibkey: nooralahzadeh-etal-2019-reinforcement
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6125
  month: November
  page_first: '225'
  page_last: '233'
  pages: "225\u2013233"
  paper_id: '25'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6125.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6125.jpg
  title: Reinforcement-based denoising of distantly supervised NER with partial annotation
  title_html: Reinforcement-based denoising of distantly supervised <span class="acl-fixed-case">NER</span>
    with partial annotation
  url: https://www.aclweb.org/anthology/D19-6125
  year: '2019'
D19-6126:
  abstract: In this paper, a dialogue system for Hospital domain in Telugu, which
    is a resource-poor Dravidian language, has been built. It handles various hospital
    and doctor related queries. The main aim of this paper is to present an approach
    for modelling a dialogue system in a resource-poor language by combining linguistic
    and domain knowledge. Focusing on the question answering aspect of the dialogue
    system, we identified Question Classification and Query Processing as the two
    most important parts of the dialogue system. Our method combines deep learning
    techniques for question classification and computational rule-based analysis for
    query processing. Human evaluation of the system has been performed as there is
    no automated evaluation tool for dialogue systems in Telugu. Our system achieves
    a high overall rating along with a significantly accurate context-capturing method
    as shown in the results.
  address: Hong Kong, China
  author:
  - first: Suma Reddy
    full: Suma Reddy Duggenpudi
    id: suma-reddy-duggenpudi
    last: Duggenpudi
  - first: Kusampudi
    full: Kusampudi Siva Subrahamanyam Varma
    id: kusampudi-siva-subrahamanyam-varma
    last: Siva Subrahamanyam Varma
  - first: Radhika
    full: Radhika Mamidi
    id: radhika-mamidi
    last: Mamidi
  author_string: Suma Reddy Duggenpudi, Kusampudi Siva Subrahamanyam Varma, Radhika
    Mamidi
  bibkey: duggenpudi-etal-2019-samvaadhana
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6126
  month: November
  page_first: '234'
  page_last: '242'
  pages: "234\u2013242"
  paper_id: '26'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6126.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6126.jpg
  title: 'Samvaadhana: A Telugu Dialogue System in Hospital Domain'
  title_html: '<span class="acl-fixed-case">S</span>amvaadhana: A <span class="acl-fixed-case">T</span>elugu
    Dialogue System in Hospital Domain'
  url: https://www.aclweb.org/anthology/D19-6126
  year: '2019'
D19-6127:
  abstract: Cross-lingual entity linking (XEL) grounds named entities in a source
    language to an English Knowledge Base (KB), such as Wikipedia. XEL is challenging
    for most languages because of limited availability of requisite resources. However,
    many works on XEL have been on simulated settings that actually use significant
    resources (e.g. source language Wikipedia, bilingual entity maps, multilingual
    embeddings) that are not available in truly low-resource languages. In this work,
    we first examine the effect of these resource assumptions and quantify how much
    the availability of these resource affects overall quality of existing XEL systems.
    We next propose three improvements to both entity candidate generation and disambiguation
    that make better use of the limited resources we do have in resource-scarce scenarios.
    With experiments on four extremely low-resource languages, we show that our model
    results in gains of 6-20% end-to-end linking accuracy.
  address: Hong Kong, China
  author:
  - first: Shuyan
    full: Shuyan Zhou
    id: shuyan-zhou
    last: Zhou
  - first: Shruti
    full: Shruti Rijhwani
    id: shruti-rijhwani
    last: Rijhwani
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  author_string: Shuyan Zhou, Shruti Rijhwani, Graham Neubig
  bibkey: zhou-etal-2019-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6127
  month: November
  page_first: '243'
  page_last: '252'
  pages: "243\u2013252"
  paper_id: '27'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6127.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6127.jpg
  title: Towards Zero-resource Cross-lingual Entity Linking
  title_html: Towards Zero-resource Cross-lingual Entity Linking
  url: https://www.aclweb.org/anthology/D19-6127
  year: '2019'
D19-6128:
  abstract: "Multi-task learning and self-training are two common ways to improve\
    \ a machine learning model\u2019s performance in settings with limited training\
    \ data. Drawing heavily on ideas from those two approaches, we suggest transductive\
    \ auxiliary task self-training: training a multi-task model on (i) a combination\
    \ of main and auxiliary task training data, and (ii) test instances with auxiliary\
    \ task labels which a single-task version of the model has previously generated.\
    \ We perform extensive experiments on 86 combinations of languages and tasks.\
    \ Our results are that, on average, transductive auxiliary task self-training\
    \ improves absolute accuracy by up to 9.56% over the pure multi-task model for\
    \ dependency relation tagging and by up to 13.03% for semantic tagging."
  address: Hong Kong, China
  author:
  - first: Johannes
    full: Johannes Bjerva
    id: johannes-bjerva
    last: Bjerva
  - first: Katharina
    full: Katharina Kann
    id: katharina-kann
    last: Kann
  - first: Isabelle
    full: Isabelle Augenstein
    id: isabelle-augenstein
    last: Augenstein
  author_string: Johannes Bjerva, Katharina Kann, Isabelle Augenstein
  bibkey: bjerva-etal-2019-transductive
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6128
  month: November
  page_first: '253'
  page_last: '258'
  pages: "253\u2013258"
  paper_id: '28'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6128.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6128.jpg
  title: Transductive Auxiliary Task Self-Training for Neural Multi-Task Models
  title_html: Transductive Auxiliary Task Self-Training for Neural Multi-Task Models
  url: https://www.aclweb.org/anthology/D19-6128
  year: '2019'
D19-6129:
  abstract: 'We propose a weakly supervised neural model for Ad-hoc Cross-lingual
    Information Retrieval (CLIR) from low-resource languages. Low resource languages
    often lack relevance annotations for CLIR, and when available the training data
    usually has limited coverage for possible queries. In this paper, we design a
    model which does not require relevance annotations, instead it is trained on samples
    extracted from translation corpora as weak supervision. This model relies on an
    attention mechanism to learn spans in the foreign sentence that are relevant to
    the query. We report experiments on two low resource languages: Swahili and Tagalog,
    trained on less that 100k parallel sentences each. The proposed model achieves
    19 MAP points improvement compared to using CNNs for feature extraction, 12 points
    improvement from machine translation-based CLIR, and up to 6 points improvement
    compared to probabilistic CLIR models.'
  address: Hong Kong, China
  author:
  - first: Lingjun
    full: Lingjun Zhao
    id: lingjun-zhao
    last: Zhao
  - first: Rabih
    full: Rabih Zbib
    id: rabih-zbib
    last: Zbib
  - first: Zhuolin
    full: Zhuolin Jiang
    id: zhuolin-jiang
    last: Jiang
  - first: Damianos
    full: Damianos Karakos
    id: damianos-karakos
    last: Karakos
  - first: Zhongqiang
    full: Zhongqiang Huang
    id: zhongqiang-huang
    last: Huang
  author_string: Lingjun Zhao, Rabih Zbib, Zhuolin Jiang, Damianos Karakos, Zhongqiang
    Huang
  bibkey: zhao-etal-2019-weakly
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6129
  month: November
  page_first: '259'
  page_last: '264'
  pages: "259\u2013264"
  paper_id: '29'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6129.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6129.jpg
  title: Weakly Supervised Attentional Model for Low Resource Ad-hoc Cross-lingual
    Information Retrieval
  title_html: Weakly Supervised Attentional Model for Low Resource Ad-hoc Cross-lingual
    Information Retrieval
  url: https://www.aclweb.org/anthology/D19-6129
  year: '2019'
D19-6130:
  abstract: Although the vast majority of knowledge bases (KBs) are heavily biased
    towards English, Wikipedias do cover very different topics in different languages.
    Exploiting this, we introduce a new multilingual dataset (X-WikiRE), framing relation
    extraction as a multilingual machine reading problem. We show that by leveraging
    this resource it is possible to robustly transfer models cross-lingually and that
    multilingual support significantly improves (zero-shot) relation extraction, enabling
    the population of low-resourced KBs from their well-populated counterparts.
  address: Hong Kong, China
  author:
  - first: Mostafa
    full: Mostafa Abdou
    id: mostafa-abdou
    last: Abdou
  - first: Cezar
    full: Cezar Sas
    id: cezar-sas
    last: Sas
  - first: Rahul
    full: Rahul Aralikatte
    id: rahul-aralikatte
    last: Aralikatte
  - first: Isabelle
    full: Isabelle Augenstein
    id: isabelle-augenstein
    last: Augenstein
  - first: Anders
    full: "Anders S\xF8gaard"
    id: anders-sogaard
    last: "S\xF8gaard"
  author_string: "Mostafa Abdou, Cezar Sas, Rahul Aralikatte, Isabelle Augenstein,\
    \ Anders S\xF8gaard"
  bibkey: abdou-etal-2019-x
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6130
  month: November
  page_first: '265'
  page_last: '274'
  pages: "265\u2013274"
  paper_id: '30'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6130.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6130.jpg
  title: 'X-WikiRE: A Large, Multilingual Resource for Relation Extraction as Machine
    Comprehension'
  title_html: 'X-<span class="acl-fixed-case">W</span>iki<span class="acl-fixed-case">RE</span>:
    A Large, Multilingual Resource for Relation Extraction as Machine Comprehension'
  url: https://www.aclweb.org/anthology/D19-6130
  year: '2019'
D19-6131:
  abstract: 'In this paper we address a challenging cross-lingual name retrieval task.
    Given an English named entity query, we aim to find all name mentions in documents
    in low-resource languages. We present a novel method which relies on zero annotation
    or resources from the target language. By leveraging freely available, cross-lingual
    resources and a small amount of training data from another language, we are able
    to perform name retrieval on a new language without any additional training data.
    Our method proceeds in a multi-step process: first, we pre-train a language-independent
    orthographic encoder using Wikipedia inter-lingual links from dozens of languages.
    Next, we gather user expectations about important entities in an English comparable
    document and compare those expected entities with actual spans of the target language
    text in order to perform name finding. Our method shows 11.6% absolute F-score
    improvement over state-of-the-art methods.'
  address: Hong Kong, China
  author:
  - first: Kevin
    full: Kevin Blissett
    id: kevin-blissett
    last: Blissett
  - first: Heng
    full: Heng Ji
    id: heng-ji
    last: Ji
  author_string: Kevin Blissett, Heng Ji
  bibkey: blissett-ji-2019-zero
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6131
  month: November
  page_first: '275'
  page_last: '280'
  pages: "275\u2013280"
  paper_id: '31'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6131.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6131.jpg
  title: Zero-Shot Cross-lingual Name Retrieval for Low-Resource Languages
  title_html: Zero-Shot Cross-lingual Name Retrieval for Low-Resource Languages
  url: https://www.aclweb.org/anthology/D19-6131
  year: '2019'
D19-6132:
  abstract: "We investigate whether off-the-shelf deep bidirectional sentence representations\
    \ (Devlin et al., 2019) trained on a massively multilingual corpus (multilingual\
    \ BERT) enable the development of an unsupervised universal dependency parser.\
    \ This approach only leverages a mix of monolingual corpora in many languages\
    \ and does not require any translation data making it applicable to low-resource\
    \ languages. In our experiments we outperform the best CoNLL 2018 language-specific\
    \ systems in all of the shared task\u2019s six truly low-resource languages while\
    \ using a single system. However, we also find that (i) parsing accuracy still\
    \ varies dramatically when changing the training languages and (ii) in some target\
    \ languages zero-shot transfer fails under all tested conditions, raising concerns\
    \ on the \u2018universality\u2019 of the whole approach."
  address: Hong Kong, China
  author:
  - first: Ke
    full: Ke Tran
    id: ke-m-tran
    last: Tran
  - first: Arianna
    full: Arianna Bisazza
    id: arianna-bisazza
    last: Bisazza
  author_string: Ke Tran, Arianna Bisazza
  bibkey: tran-bisazza-2019-zero
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
    NLP (DeepLo 2019)
  booktitle_html: Proceedings of the 2nd Workshop on Deep Learning Approaches for
    Low-Resource NLP (DeepLo 2019)
  doi: 10.18653/v1/D19-6132
  month: November
  page_first: '281'
  page_last: '288'
  pages: "281\u2013288"
  paper_id: '32'
  parent_volume_id: D19-61
  pdf: https://www.aclweb.org/anthology/D19-6132.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6132.jpg
  title: Zero-shot Dependency Parsing with Pre-trained Multilingual Sentence Representations
  title_html: Zero-shot Dependency Parsing with Pre-trained Multilingual Sentence
    Representations
  url: https://www.aclweb.org/anthology/D19-6132
  year: '2019'
D19-6200:
  address: Hong Kong
  author:
  - first: Eben
    full: Eben Holderness
    id: eben-holderness
    last: Holderness
  - first: Antonio
    full: Antonio Jimeno Yepes
    id: antonio-jimeno-yepes
    last: Jimeno Yepes
  - first: Alberto
    full: Alberto Lavelli
    id: alberto-lavelli
    last: Lavelli
  - first: Anne-Lyse
    full: Anne-Lyse Minard
    id: anne-lyse-minard
    last: Minard
  - first: James
    full: James Pustejovsky
    id: james-pustejovsky
    last: Pustejovsky
  - first: Fabio
    full: Fabio Rinaldi
    id: fabio-rinaldi
    last: Rinaldi
  author_string: Eben Holderness, Antonio Jimeno Yepes, Alberto Lavelli, Anne-Lyse
    Minard, James Pustejovsky, Fabio Rinaldi
  bibkey: emnlp-2019-international
  bibtype: proceedings
  booktitle: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  month: November
  paper_id: '0'
  parent_volume_id: D19-62
  pdf: https://www.aclweb.org/anthology/D19-6200.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6200.jpg
  title: Proceedings of the Tenth International Workshop on Health Text Mining and
    Information Analysis (LOUHI 2019)
  title_html: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  url: https://www.aclweb.org/anthology/D19-6200
  year: '2019'
D19-6201:
  abstract: This paper discusses a cross-document coreference annotation schema that
    was developed to further automatic extraction of timelines in the clinical domain.
    Lexical senses and coreference choices are determined largely by context, but
    cross-document work requires reasoning across contexts that are not necessarily
    coherent. We found that an annotation approach that relies less on context-guided
    annotator intuitions and more on schematic rules was most effective in creating
    meaningful and consistent cross-document relations.
  address: Hong Kong
  author:
  - first: Kristin
    full: Kristin Wright-Bettner
    id: kristin-wright-bettner
    last: Wright-Bettner
  - first: Martha
    full: Martha Palmer
    id: martha-palmer
    last: Palmer
  - first: Guergana
    full: Guergana Savova
    id: guergana-savova
    last: Savova
  - first: Piet
    full: Piet de Groen
    id: piet-de-groen
    last: de Groen
  - first: Timothy
    full: Timothy Miller
    id: timothy-miller
    last: Miller
  author_string: Kristin Wright-Bettner, Martha Palmer, Guergana Savova, Piet de Groen,
    Timothy Miller
  bibkey: wright-bettner-etal-2019-cross
  bibtype: inproceedings
  booktitle: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  booktitle_html: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  doi: 10.18653/v1/D19-6201
  month: November
  page_first: '1'
  page_last: '10'
  pages: "1\u201310"
  paper_id: '1'
  parent_volume_id: D19-62
  pdf: https://www.aclweb.org/anthology/D19-6201.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6201.jpg
  title: 'Cross-document coreference: An approach to capturing coreference without
    context'
  title_html: 'Cross-document coreference: An approach to capturing coreference without
    context'
  url: https://www.aclweb.org/anthology/D19-6201
  year: '2019'
D19-6202:
  abstract: Pre-trained word embeddings are becoming increasingly popular for natural
    language processing tasks. This includes medical applications, where embeddings
    are trained for clinical concepts using specific medical data. Recent work continues
    to improve on these embeddings. However, no one has yet sought to determine whether
    these embeddings work as well for one field of medicine as they do in others.
    In this work, we use intrinsic methods to evaluate embeddings from the various
    fields of medicine as defined by their ICD-9 systems. We find significant differences
    between fields, and motivate future work to investigate whether extrinsic tasks
    will follow a similar pattern.
  address: Hong Kong
  author:
  - first: John-Jose
    full: John-Jose Nunez
    id: john-jose-nunez
    last: Nunez
  - first: Giuseppe
    full: Giuseppe Carenini
    id: giuseppe-carenini
    last: Carenini
  author_string: John-Jose Nunez, Giuseppe Carenini
  bibkey: nunez-carenini-2019-comparing
  bibtype: inproceedings
  booktitle: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  booktitle_html: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  doi: 10.18653/v1/D19-6202
  month: November
  page_first: '11'
  page_last: '17'
  pages: "11\u201317"
  paper_id: '2'
  parent_volume_id: D19-62
  pdf: https://www.aclweb.org/anthology/D19-6202.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6202.jpg
  title: Comparing the Intrinsic Performance of Clinical Concept Embeddings by Their
    Field of Medicine
  title_html: Comparing the Intrinsic Performance of Clinical Concept Embeddings by
    Their Field of Medicine
  url: https://www.aclweb.org/anthology/D19-6202
  year: '2019'
D19-6203:
  abstract: Deep learning models have achieved state-of-the-art performances on many
    relation extraction datasets. A common element in these deep learning models involves
    the pooling mechanisms where a sequence of hidden vectors is aggregated to generate
    a single representation vector, serving as the features to perform prediction
    for RE. Unfortunately, the models in the literature tend to employ different strategies
    to perform pooling for RE, leading to the challenge to determine the best pooling
    mechanism for this problem, especially in the biomedical domain. In order to answer
    this question, in this work, we conduct a comprehensive study to evaluate the
    effectiveness of different pooling mechanisms for the deep learning models in
    biomedical RE. The experimental results suggest that dependency-based pooling
    is the best pooling strategy for RE in the biomedical domain, yielding the state-of-the-art
    performance on two benchmark datasets for this problem.
  address: Hong Kong
  author:
  - first: Tuan Ngo
    full: Tuan Ngo Nguyen
    id: tuan-ngo-nguyen
    last: Nguyen
  - first: Franck
    full: Franck Dernoncourt
    id: franck-dernoncourt
    last: Dernoncourt
  - first: Thien Huu
    full: Thien Huu Nguyen
    id: thien-huu-nguyen
    last: Nguyen
  author_string: Tuan Ngo Nguyen, Franck Dernoncourt, Thien Huu Nguyen
  bibkey: nguyen-etal-2019-effectiveness
  bibtype: inproceedings
  booktitle: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  booktitle_html: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  doi: 10.18653/v1/D19-6203
  month: November
  page_first: '18'
  page_last: '27'
  pages: "18\u201327"
  paper_id: '3'
  parent_volume_id: D19-62
  pdf: https://www.aclweb.org/anthology/D19-6203.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6203.jpg
  title: On the Effectiveness of the Pooling Methods for Biomedical Relation Extraction
    with Deep Learning
  title_html: On the Effectiveness of the Pooling Methods for Biomedical Relation
    Extraction with Deep Learning
  url: https://www.aclweb.org/anthology/D19-6203
  year: '2019'
D19-6204:
  abstract: In this paper we tackle two unique challenges in biomedical relation extraction.
    The first challenge is that the contextual information between two entity mentions
    often involves sophisticated syntactic structures. We propose a novel graph convolutional
    networks model that incorporates dependency parsing and contextualized embedding
    to effectively capture comprehensive contextual information. The second challenge
    is that most of the benchmark data sets for this task are quite imbalanced because
    more than 80% mention pairs are negative instances (i.e., no relations). We propose
    a multi-task learning framework to jointly model relation identification and classification
    tasks to propagate supervision signals from each other and apply a focal loss
    to focus training on ambiguous mention pairs. By applying these two strategies,
    experiments show that our model achieves state-of-the-art F-score on the 2013
    drug-drug interaction extraction task.
  address: Hong Kong
  author:
  - first: Diya
    full: Diya Li
    id: diya-li
    last: Li
  - first: Heng
    full: Heng Ji
    id: heng-ji
    last: Ji
  author_string: Diya Li, Heng Ji
  bibkey: li-ji-2019-syntax
  bibtype: inproceedings
  booktitle: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  booktitle_html: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  doi: 10.18653/v1/D19-6204
  month: November
  page_first: '28'
  page_last: '33'
  pages: "28\u201333"
  paper_id: '4'
  parent_volume_id: D19-62
  pdf: https://www.aclweb.org/anthology/D19-6204.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6204.jpg
  title: Syntax-aware Multi-task Graph Convolutional Networks for Biomedical Relation
    Extraction
  title_html: Syntax-aware Multi-task Graph Convolutional Networks for Biomedical
    Relation Extraction
  url: https://www.aclweb.org/anthology/D19-6204
  year: '2019'
D19-6205:
  abstract: Word embeddings, in their different shapes and iterations, have changed
    the natural language processing research landscape in the last years. The biomedical
    text processing field is no stranger to this revolution; however, scholars in
    the field largely trained their embeddings on scientific documents only, even
    when working on user-generated data. In this paper we show how training embeddings
    from a corpus collected from user-generated text from medical forums heavily influences
    the performance on downstream tasks, outperforming embeddings trained both on
    general purpose data or on scientific papers when applied on user-generated content.
  address: Hong Kong
  author:
  - first: Marco
    full: Marco Basaldella
    id: marco-basaldella
    last: Basaldella
  - first: Nigel
    full: Nigel Collier
    id: nigel-collier
    last: Collier
  author_string: Marco Basaldella, Nigel Collier
  bibkey: basaldella-collier-2019-bioreddit
  bibtype: inproceedings
  booktitle: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  booktitle_html: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  doi: 10.18653/v1/D19-6205
  month: November
  page_first: '34'
  page_last: '38'
  pages: "34\u201338"
  paper_id: '5'
  parent_volume_id: D19-62
  pdf: https://www.aclweb.org/anthology/D19-6205.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6205.jpg
  title: 'BioReddit: Word Embeddings for User-Generated Biomedical NLP'
  title_html: '<span class="acl-fixed-case">B</span>io<span class="acl-fixed-case">R</span>eddit:
    Word Embeddings for User-Generated Biomedical <span class="acl-fixed-case">NLP</span>'
  url: https://www.aclweb.org/anthology/D19-6205
  year: '2019'
D19-6206:
  abstract: "Clinical notes are essential medical documents to record each patient\u2019\
    s symptoms. Each record is typically annotated with medical diagnostic codes,\
    \ which means diagnosis and treatment. This paper focuses on predicting diagnostic\
    \ codes given the descriptive present illness in electronic health records by\
    \ leveraging domain knowledge. We investigate various losses in a convolutional\
    \ model to utilize hierarchical category knowledge of diagnostic codes in order\
    \ to allow the model to share semantics across different labels under the same\
    \ category. The proposed model not only considers the external domain knowledge\
    \ but also addresses the issue about data imbalance. The MIMIC3 benchmark experiments\
    \ show that the proposed methods can effectively utilize category knowledge and\
    \ provide informative cues to improve the performance in terms of the top-ranked\
    \ diagnostic codes which is better than the prior state-of-the-art. The investigation\
    \ and discussion express the potential of integrating the domain knowledge in\
    \ the current machine learning based models and guiding future research directions."
  address: Hong Kong
  attachment:
  - filename: D19-6206.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-6206.Attachment.pdf
  author:
  - first: Shang-Chi
    full: Shang-Chi Tsai
    id: shang-chi-tsai
    last: Tsai
  - first: Ting-Yun
    full: Ting-Yun Chang
    id: ting-yun-chang
    last: Chang
  - first: Yun-Nung
    full: Yun-Nung Chen
    id: yun-nung-chen
    last: Chen
  author_string: Shang-Chi Tsai, Ting-Yun Chang, Yun-Nung Chen
  bibkey: tsai-etal-2019-leveraging
  bibtype: inproceedings
  booktitle: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  booktitle_html: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  doi: 10.18653/v1/D19-6206
  month: November
  page_first: '39'
  page_last: '43'
  pages: "39\u201343"
  paper_id: '6'
  parent_volume_id: D19-62
  pdf: https://www.aclweb.org/anthology/D19-6206.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6206.jpg
  title: Leveraging Hierarchical Category Knowledge for Data-Imbalanced Multi-Label
    Diagnostic Text Understanding
  title_html: Leveraging Hierarchical Category Knowledge for Data-Imbalanced Multi-Label
    Diagnostic Text Understanding
  url: https://www.aclweb.org/anthology/D19-6206
  year: '2019'
D19-6207:
  abstract: 'The paper addresses experiments to expand ad hoc ambiguous abbreviations
    in medical notes on the basis of morphologically annotated texts, without using
    additional domain resources. We work on Polish data but the described approaches
    can be used for other languages too. We test two methods to select candidates
    for word abbreviation expansions. The first one automatically selects all words
    in text which might be an expansion of an abbreviation according to the language
    rules. The second method uses clustering of abbreviation occurrences to select
    representative elements which are manually annotated to determine lists of potential
    expansions. We then train a classifier to assign expansions to abbreviations based
    on three training sets: automatically obtained, consisting of manual annotation,
    and concatenation of the two previous ones. The results obtained for the manually
    annotated training data significantly outperform automatically obtained training
    data. Adding the automatically obtained training data to the manually annotated
    data improves the results, in particular for less frequent abbreviations. In this
    context the proposed a priori data driven selection of possible extensions turned
    out to be crucial.'
  address: Hong Kong
  author:
  - first: Agnieszka
    full: Agnieszka Mykowiecka
    id: agnieszka-mykowiecka
    last: Mykowiecka
  - first: Malgorzata
    full: Malgorzata Marciniak
    id: malgorzata-marciniak
    last: Marciniak
  author_string: Agnieszka Mykowiecka, Malgorzata Marciniak
  bibkey: mykowiecka-marciniak-2019-experiments
  bibtype: inproceedings
  booktitle: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  booktitle_html: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  doi: 10.18653/v1/D19-6207
  month: November
  page_first: '44'
  page_last: '53'
  pages: "44\u201353"
  paper_id: '7'
  parent_volume_id: D19-62
  pdf: https://www.aclweb.org/anthology/D19-6207.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6207.jpg
  title: Experiments with ad hoc ambiguous abbreviation expansion
  title_html: Experiments with ad hoc ambiguous abbreviation expansion
  url: https://www.aclweb.org/anthology/D19-6207
  year: '2019'
D19-6208:
  abstract: We investigate the impact of using emotional patterns identified by the
    clinical practitioners and computational linguists to enhance the prediction capabilities
    of a mental illness detection (in our case depression and post-traumatic stress
    disorder) model built using a deep neural network architecture. Over the years,
    deep learning methods have been successfully used in natural language processing
    tasks, including a few in the domain of mental illness and suicide ideation detection.
    We illustrate the effectiveness of using multi-task learning with a multi-channel
    convolutional neural network as the shared representation and use additional inputs
    identified by researchers as indicatives in detecting mental disorders to enhance
    the model predictability. Given the limited amount of unstructured data available
    for training, we managed to obtain a task-specific AUC higher than 0.90. In comparison
    to methods such as multi-class classification, we identified multi-task learning
    with multi-channel convolution neural network and multiple-inputs to be effective
    in detecting mental disorders.
  address: Hong Kong
  author:
  - first: Prasadith
    full: Prasadith Kirinde Gamaarachchige
    id: prasadith-kirinde-gamaarachchige
    last: Kirinde Gamaarachchige
  - first: Diana
    full: Diana Inkpen
    id: diana-inkpen
    last: Inkpen
  author_string: Prasadith Kirinde Gamaarachchige, Diana Inkpen
  bibkey: kirinde-gamaarachchige-inkpen-2019-multi
  bibtype: inproceedings
  booktitle: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  booktitle_html: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  doi: 10.18653/v1/D19-6208
  month: November
  page_first: '54'
  page_last: '64'
  pages: "54\u201364"
  paper_id: '8'
  parent_volume_id: D19-62
  pdf: https://www.aclweb.org/anthology/D19-6208.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6208.jpg
  title: Multi-Task, Multi-Channel, Multi-Input Learning for Mental Illness Detection
    using Social Media Text
  title_html: Multi-Task, Multi-Channel, Multi-Input Learning for Mental Illness Detection
    using Social Media Text
  url: https://www.aclweb.org/anthology/D19-6208
  year: '2019'
D19-6209:
  abstract: We present a system for automatically extracting pertinent medical information
    from dialogues between clinicians and patients. The system parses each dialogue
    and extracts entities such as medications and symptoms, using context to predict
    which entities are relevant. We also classify the primary diagnosis for each conversation.
    In addition, we extract topic information and identify relevant utterances. This
    serves as a baseline for a system that extracts information from dialogues and
    automatically generates a patient note, which can be reviewed and edited by the
    clinician.
  address: Hong Kong
  author:
  - first: Serena
    full: Serena Jeblee
    id: serena-jeblee
    last: Jeblee
  - first: Faiza
    full: Faiza Khan Khattak
    id: faiza-khan-khattak
    last: Khan Khattak
  - first: Noah
    full: Noah Crampton
    id: noah-crampton
    last: Crampton
  - first: Muhammad
    full: Muhammad Mamdani
    id: muhammad-mamdani
    last: Mamdani
  - first: Frank
    full: Frank Rudzicz
    id: frank-rudzicz
    last: Rudzicz
  author_string: Serena Jeblee, Faiza Khan Khattak, Noah Crampton, Muhammad Mamdani,
    Frank Rudzicz
  bibkey: jeblee-etal-2019-extracting
  bibtype: inproceedings
  booktitle: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  booktitle_html: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  doi: 10.18653/v1/D19-6209
  month: November
  page_first: '65'
  page_last: '74'
  pages: "65\u201374"
  paper_id: '9'
  parent_volume_id: D19-62
  pdf: https://www.aclweb.org/anthology/D19-6209.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6209.jpg
  title: Extracting relevant information from physician-patient dialogues for automated
    clinical note taking
  title_html: Extracting relevant information from physician-patient dialogues for
    automated clinical note taking
  url: https://www.aclweb.org/anthology/D19-6209
  year: '2019'
D19-6210:
  abstract: Relation classification is crucial for inferring semantic relatedness
    between entities in a piece of text. These systems can be trained given labelled
    data. However, relation classification is very domain-specific and it takes a
    lot of effort to label data for a new domain. In this paper, we explore domain
    adaptation techniques for this task. While past works have focused on single source
    domain adaptation for bio-medical relation classification, we classify relations
    in an unlabeled target domain by transferring useful knowledge from one or more
    related source domains. Our experiments with the model have shown to improve state-of-the-art
    F1 score on 3 benchmark biomedical corpora for single domain and on 2 out of 3
    for multi-domain scenarios. When used with contextualized embeddings, there is
    further boost in performance outperforming neural-network based domain adaptation
    baselines for both the cases.
  address: Hong Kong
  author:
  - first: Sinchani
    full: Sinchani Chakraborty
    id: sinchani-chakraborty
    last: Chakraborty
  - first: Sudeshna
    full: Sudeshna Sarkar
    id: sudeshna-sarkar
    last: Sarkar
  - first: Pawan
    full: Pawan Goyal
    id: pawan-goyal
    last: Goyal
  - first: Mahanandeeshwar
    full: Mahanandeeshwar Gattu
    id: mahanandeeshwar-gattu
    last: Gattu
  author_string: Sinchani Chakraborty, Sudeshna Sarkar, Pawan Goyal, Mahanandeeshwar
    Gattu
  bibkey: chakraborty-etal-2019-biomedical
  bibtype: inproceedings
  booktitle: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  booktitle_html: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  doi: 10.18653/v1/D19-6210
  month: November
  page_first: '75'
  page_last: '80'
  pages: "75\u201380"
  paper_id: '10'
  parent_volume_id: D19-62
  pdf: https://www.aclweb.org/anthology/D19-6210.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6210.jpg
  title: Biomedical Relation Classification by single and multiple source domain adaptation
  title_html: Biomedical Relation Classification by single and multiple source domain
    adaptation
  url: https://www.aclweb.org/anthology/D19-6210
  year: '2019'
D19-6211:
  abstract: Predicting which patients are more likely to be readmitted to a hospital
    within 30 days after discharge is a valuable piece of information in clinical
    decision-making. Building a successful readmission risk classifier based on the
    content of Electronic Health Records (EHRs) has proved, however, to be a challenging
    task. Previously explored features include mainly structured information, such
    as sociodemographic data, comorbidity codes and physiological variables. In this
    paper we assess incorporating additional clinically interpretable NLP-based features
    such as topic extraction and clinical sentiment analysis to predict early readmission
    risk in psychiatry patients.
  address: Hong Kong
  author:
  - first: Elena
    full: Elena Alvarez-Mellado
    id: elena-alvarez-mellado
    last: Alvarez-Mellado
  - first: Eben
    full: Eben Holderness
    id: eben-holderness
    last: Holderness
  - first: Nicholas
    full: Nicholas Miller
    id: nicholas-miller
    last: Miller
  - first: Fyonn
    full: Fyonn Dhang
    id: fyonn-dhang
    last: Dhang
  - first: Philip
    full: Philip Cawkwell
    id: philip-cawkwell
    last: Cawkwell
  - first: Kirsten
    full: Kirsten Bolton
    id: kirsten-bolton
    last: Bolton
  - first: James
    full: James Pustejovsky
    id: james-pustejovsky
    last: Pustejovsky
  - first: Mei-Hua
    full: Mei-Hua Hall
    id: mei-hua-hall
    last: Hall
  author_string: Elena Alvarez-Mellado, Eben Holderness, Nicholas Miller, Fyonn Dhang,
    Philip Cawkwell, Kirsten Bolton, James Pustejovsky, Mei-Hua Hall
  bibkey: alvarez-mellado-etal-2019-assessing
  bibtype: inproceedings
  booktitle: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  booktitle_html: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  doi: 10.18653/v1/D19-6211
  month: November
  page_first: '81'
  page_last: '86'
  pages: "81\u201386"
  paper_id: '11'
  parent_volume_id: D19-62
  pdf: https://www.aclweb.org/anthology/D19-6211.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6211.jpg
  title: Assessing the Efficacy of Clinical Sentiment Analysis and Topic Extraction
    in Psychiatric Readmission Risk Prediction
  title_html: Assessing the Efficacy of Clinical Sentiment Analysis and Topic Extraction
    in Psychiatric Readmission Risk Prediction
  url: https://www.aclweb.org/anthology/D19-6211
  year: '2019'
D19-6212:
  abstract: "In this work we investigate the signal contained in the language of food\
    \ on social media. We experiment with a dataset of 24 million food-related tweets,\
    \ and make several observations. First,thelanguageoffoodhaspredictive power. We\
    \ are able to predict if states in the United States (US) are above the medianratesfortype2diabetesmellitus(T2DM),\
    \ income, poverty, and education \u2013 outperforming previous work by 4\u2013\
    18%. Second, we investigate the effect of socioeconomic factors (income, poverty,\
    \ and education) on predicting state-level T2DM rates. Socioeconomic factors do\
    \ improve T2DM prediction, with the greatestimprovementcomingfrompovertyinformation(6%),but,importantly,thelanguage\
    \ of food adds distinct information that is not captured by socioeconomics. Third,\
    \ we analyze how the language of food has changed over a five-year period (2013\
    \ \u2013 2017), which is indicative of the shift in eating habits in the US during\
    \ that period. We find several food trends, and that the language of food is used\
    \ differently by different groups such as differentgenders. Last,weprovideanonlinevisualization\
    \ tool for real-time queries and semantic analysis."
  address: Hong Kong
  author:
  - first: Hoang
    full: Hoang Van
    id: hoang-van
    last: Van
  - first: Ahmad
    full: Ahmad Musa
    id: ahmad-musa
    last: Musa
  - first: Hang
    full: Hang Chen
    id: hang-chen
    last: Chen
  - first: Stephen
    full: Stephen Kobourov
    id: stephen-kobourov
    last: Kobourov
  - first: Mihai
    full: Mihai Surdeanu
    id: mihai-surdeanu
    last: Surdeanu
  author_string: Hoang Van, Ahmad Musa, Hang Chen, Stephen Kobourov, Mihai Surdeanu
  bibkey: van-etal-2019-language
  bibtype: inproceedings
  booktitle: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  booktitle_html: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  doi: 10.18653/v1/D19-6212
  month: November
  page_first: '87'
  page_last: '96'
  pages: "87\u201396"
  paper_id: '12'
  parent_volume_id: D19-62
  pdf: https://www.aclweb.org/anthology/D19-6212.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6212.jpg
  title: What does the language of foods say about us?
  title_html: What does the language of foods say about us?
  url: https://www.aclweb.org/anthology/D19-6212
  year: '2019'
D19-6213:
  abstract: Stress is a nigh-universal human experience, particularly in the online
    world. While stress can be a motivator, too much stress is associated with many
    negative health outcomes, making its identification useful across a range of domains.
    However, existing computational research typically only studies stress in domains
    such as speech, or in short genres such as Twitter. We present Dreaddit, a new
    text corpus of lengthy multi-domain social media data for the identification of
    stress. Our dataset consists of 190K posts from five different categories of Reddit
    communities; we additionally label 3.5K total segments taken from 3K posts using
    Amazon Mechanical Turk. We present preliminary supervised learning methods for
    identifying stress, both neural and traditional, and analyze the complexity and
    diversity of the data and characteristics of each category.
  address: Hong Kong
  attachment:
  - filename: D19-6213.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-6213.Attachment.zip
  author:
  - first: Elsbeth
    full: Elsbeth Turcan
    id: elsbeth-turcan
    last: Turcan
  - first: Kathy
    full: Kathy McKeown
    id: kathleen-mckeown
    last: McKeown
  author_string: Elsbeth Turcan, Kathy McKeown
  bibkey: turcan-mckeown-2019-dreaddit
  bibtype: inproceedings
  booktitle: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  booktitle_html: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  doi: 10.18653/v1/D19-6213
  month: November
  page_first: '97'
  page_last: '107'
  pages: "97\u2013107"
  paper_id: '13'
  parent_volume_id: D19-62
  pdf: https://www.aclweb.org/anthology/D19-6213.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6213.jpg
  title: 'Dreaddit: A Reddit Dataset for Stress Analysis in Social Media'
  title_html: '<span class="acl-fixed-case">D</span>readdit: A <span class="acl-fixed-case">R</span>eddit
    Dataset for Stress Analysis in Social Media'
  url: https://www.aclweb.org/anthology/D19-6213
  year: '2019'
D19-6214:
  abstract: Randomized controlled trials (RCTs) represent the paramount evidence of
    clinical medicine. Using machines to interpret the massive amount of RCTs has
    the potential of aiding clinical decision-making. We propose a RCT conclusion
    generation task from the PubMed 200k RCT sentence classification dataset to examine
    the effectiveness of sequence-to-sequence models on understanding RCTs. We first
    build a pointer-generator baseline model for conclusion generation. Then we fine-tune
    the state-of-the-art GPT-2 language model, which is pre-trained with general domain
    data, for this new medical domain task. Both automatic and human evaluation show
    that our GPT-2 fine-tuned models achieve improved quality and correctness in the
    generated conclusions compared to the baseline pointer-generator model. Further
    inspection points out the limitations of this current approach and future directions
    to explore.
  address: Hong Kong
  author:
  - first: Alexander Te-Wei
    full: Alexander Te-Wei Shieh
    id: alexander-te-wei-shieh
    last: Shieh
  - first: Yung-Sung
    full: Yung-Sung Chuang
    id: yung-sung-chuang
    last: Chuang
  - first: Shang-Yu
    full: Shang-Yu Su
    id: shang-yu-su
    last: Su
  - first: Yun-Nung
    full: Yun-Nung Chen
    id: yun-nung-chen
    last: Chen
  author_string: Alexander Te-Wei Shieh, Yung-Sung Chuang, Shang-Yu Su, Yun-Nung Chen
  bibkey: shieh-etal-2019-towards
  bibtype: inproceedings
  booktitle: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  booktitle_html: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  doi: 10.18653/v1/D19-6214
  month: November
  page_first: '108'
  page_last: '117'
  pages: "108\u2013117"
  paper_id: '14'
  parent_volume_id: D19-62
  pdf: https://www.aclweb.org/anthology/D19-6214.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6214.jpg
  title: Towards Understanding of Medical Randomized Controlled Trials by Conclusion
    Generation
  title_html: Towards Understanding of Medical Randomized Controlled Trials by Conclusion
    Generation
  url: https://www.aclweb.org/anthology/D19-6214
  year: '2019'
D19-6215:
  abstract: This article presents experiments with pseudonymised Swedish clinical
    text used as training data to de-identify real clinical text with the future aim
    to transfer non-sensitive training data to other hospitals. Conditional Random
    Fields (CFR) and Long Short-Term Memory (LSTM) machine learning algorithms were
    used to train de-identification models. The two models were trained on pseudonymised
    data and evaluated on real data. For benchmarking, models were also trained on
    real data, and evaluated on real data as well as trained on pseudonymised data
    and evaluated on pseudonymised data. CRF showed better performance for some PHI
    information like Date Part, First Name and Last Name; consistent with some reports
    in the literature. In contrast, poor performances on Location and Health Care
    Unit information were noted, partially due to the constrained vocabulary in the
    pseudonymised training data. It is concluded that it is possible to train transferable
    models based on pseudonymised Swedish clinical data, but even small narrative
    and distributional variation could negatively impact performance.
  address: Hong Kong
  author:
  - first: Hanna
    full: Hanna Berg
    id: hanna-berg
    last: Berg
  - first: Taridzo
    full: Taridzo Chomutare
    id: taridzo-chomutare
    last: Chomutare
  - first: Hercules
    full: Hercules Dalianis
    id: hercules-dalianis
    last: Dalianis
  author_string: Hanna Berg, Taridzo Chomutare, Hercules Dalianis
  bibkey: berg-etal-2019-building
  bibtype: inproceedings
  booktitle: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  booktitle_html: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  doi: 10.18653/v1/D19-6215
  month: November
  page_first: '118'
  page_last: '125'
  pages: "118\u2013125"
  paper_id: '15'
  parent_volume_id: D19-62
  pdf: https://www.aclweb.org/anthology/D19-6215.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6215.jpg
  title: Building a De-identification System for Real Swedish Clinical Text Using
    Pseudonymised Clinical Text
  title_html: Building a De-identification System for Real <span class="acl-fixed-case">S</span>wedish
    Clinical Text Using Pseudonymised Clinical Text
  url: https://www.aclweb.org/anthology/D19-6215
  year: '2019'
D19-6216:
  abstract: Clinical notes provide important documentation critical to medical care,
    as well as billing and legal needs. Too little information degrades quality of
    care; too much information impedes care. Training for clinical note documentation
    is highly variable, depending on institutions and programs. In this work, we introduce
    the problem of automatic evaluation of note creation through rubric-based content
    grading, which has the potential for accelerating and regularizing clinical note
    documentation training. To this end, we describe our corpus creation methods as
    well as provide simple feature-based and neural network baseline systems. We further
    provide tagset and scaling experiments to inform readers of plausible expected
    performances. Our baselines show promising results with content point accuracy
    and kappa values at 0.86 and 0.71 on the test set.
  address: Hong Kong
  author:
  - first: Wen-wai
    full: Wen-wai Yim
    id: wen-wai-yim
    last: Yim
  - first: Ashley
    full: Ashley Mills
    id: ashley-mills
    last: Mills
  - first: Harold
    full: Harold Chun
    id: harold-chun
    last: Chun
  - first: Teresa
    full: Teresa Hashiguchi
    id: teresa-hashiguchi
    last: Hashiguchi
  - first: Justin
    full: Justin Yew
    id: justin-yew
    last: Yew
  - first: Bryan
    full: Bryan Lu
    id: bryan-lu
    last: Lu
  author_string: Wen-wai Yim, Ashley Mills, Harold Chun, Teresa Hashiguchi, Justin
    Yew, Bryan Lu
  bibkey: yim-etal-2019-automatic
  bibtype: inproceedings
  booktitle: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  booktitle_html: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  doi: 10.18653/v1/D19-6216
  month: November
  page_first: '126'
  page_last: '135'
  pages: "126\u2013135"
  paper_id: '16'
  parent_volume_id: D19-62
  pdf: https://www.aclweb.org/anthology/D19-6216.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6216.jpg
  title: Automatic rubric-based content grading for clinical notes
  title_html: Automatic rubric-based content grading for clinical notes
  url: https://www.aclweb.org/anthology/D19-6216
  year: '2019'
D19-6217:
  abstract: In this paper we present a dilated LSTM with attention mechanism for document-level
    classification of suicide notes, last statements and depressed notes. We achieve
    an accuracy of 87.34% compared to competitive baselines of 80.35% (Logistic Model
    Tree) and 82.27% (Bi-directional LSTM with Attention). Furthermore, we provide
    an analysis of both the grammatical and thematic content of suicide notes, last
    statements and depressed notes. We find that the use of personal pronouns, cognitive
    processes and references to loved ones are most important. Finally, we show through
    visualisations of attention weights that the Dilated LSTM with attention is able
    to identify the same distinguishing features across documents as the linguistic
    analysis.
  address: Hong Kong
  author:
  - first: Annika M
    full: Annika M Schoene
    id: annika-m-schoene
    last: Schoene
  - first: George
    full: George Lacey
    id: george-lacey
    last: Lacey
  - first: Alexander P
    full: Alexander P Turner
    id: alexander-p-turner
    last: Turner
  - first: Nina
    full: Nina Dethlefs
    id: nina-dethlefs
    last: Dethlefs
  author_string: Annika M Schoene, George Lacey, Alexander P Turner, Nina Dethlefs
  bibkey: schoene-etal-2019-dilated
  bibtype: inproceedings
  booktitle: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  booktitle_html: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  doi: 10.18653/v1/D19-6217
  month: November
  page_first: '136'
  page_last: '145'
  pages: "136\u2013145"
  paper_id: '17'
  parent_volume_id: D19-62
  pdf: https://www.aclweb.org/anthology/D19-6217.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6217.jpg
  title: Dilated LSTM with attention for Classification of Suicide Notes
  title_html: Dilated <span class="acl-fixed-case">LSTM</span> with attention for
    Classification of Suicide Notes
  url: https://www.aclweb.org/anthology/D19-6217
  year: '2019'
D19-6218:
  abstract: Natural language processing techniques are being applied to increasingly
    diverse types of electronic health records, and can benefit from in-depth understanding
    of the distinguishing characteristics of medical document types. We present a
    method for characterizing the usage patterns of clinical concepts among different
    document types, in order to capture semantic differences beyond the lexical level.
    By training concept embeddings on clinical documents of different types and measuring
    the differences in their nearest neighborhood structures, we are able to measure
    divergences in concept usage while correcting for noise in embedding learning.
    Experiments on the MIMIC-III corpus demonstrate that our approach captures clinically-relevant
    differences in concept usage and provides an intuitive way to explore semantic
    characteristics of clinical document collections.
  address: Hong Kong
  author:
  - first: Denis
    full: Denis Newman-Griffis
    id: denis-newman-griffis
    last: Newman-Griffis
  - first: Eric
    full: Eric Fosler-Lussier
    id: eric-fosler-lussier
    last: Fosler-Lussier
  author_string: Denis Newman-Griffis, Eric Fosler-Lussier
  bibkey: newman-griffis-fosler-lussier-2019-writing
  bibtype: inproceedings
  booktitle: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  booktitle_html: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  doi: 10.18653/v1/D19-6218
  month: November
  page_first: '146'
  page_last: '156'
  pages: "146\u2013156"
  paper_id: '18'
  parent_volume_id: D19-62
  pdf: https://www.aclweb.org/anthology/D19-6218.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6218.jpg
  title: 'Writing habits and telltale neighbors: analyzing clinical concept usage
    patterns with sublanguage embeddings'
  title_html: 'Writing habits and telltale neighbors: analyzing clinical concept usage
    patterns with sublanguage embeddings'
  url: https://www.aclweb.org/anthology/D19-6218
  year: '2019'
D19-6219:
  abstract: Entity recognition is a critical first step to a number of clinical NLP
    applications, such as entity linking and relation extraction. We present the first
    attempt to apply state-of-the-art entity recognition approaches on a newly released
    dataset, MedMentions. This dataset contains over 4000 biomedical abstracts, annotated
    for UMLS semantic types. In comparison to existing datasets, MedMentions contains
    a far greater number of entity types, and thus represents a more challenging but
    realistic scenario in a real-world setting. We explore a number of relevant dimensions,
    including the use of contextual versus non-contextual word embeddings, general
    versus domain-specific unsupervised pre-training, and different deep learning
    architectures. We contrast our results against the well-known i2b2 2010 entity
    recognition dataset, and propose a new method to combine general and domain-specific
    information. While producing a state-of-the-art result for the i2b2 2010 task
    (F1 = 0.90), our results on MedMentions are significantly lower (F1 = 0.63), suggesting
    there is still plenty of opportunity for improvement on this new data.
  address: Hong Kong
  author:
  - first: Isar
    full: Isar Nejadgholi
    id: isar-nejadgholi
    last: Nejadgholi
  - first: Kathleen C.
    full: Kathleen C. Fraser
    id: kathleen-c-fraser
    last: Fraser
  - first: Berry
    full: Berry De Bruijn
    id: berry-de-bruijn
    last: De Bruijn
  - first: Muqun
    full: Muqun Li
    id: muqun-li
    last: Li
  - first: Astha
    full: Astha LaPlante
    id: astha-laplante
    last: LaPlante
  - first: Khaldoun
    full: Khaldoun Zine El Abidine
    id: khaldoun-zine-el-abidine
    last: Zine El Abidine
  author_string: Isar Nejadgholi, Kathleen C. Fraser, Berry De Bruijn, Muqun Li, Astha
    LaPlante, Khaldoun Zine El Abidine
  bibkey: nejadgholi-etal-2019-recognizing
  bibtype: inproceedings
  booktitle: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  booktitle_html: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  doi: 10.18653/v1/D19-6219
  month: November
  page_first: '157'
  page_last: '167'
  pages: "157\u2013167"
  paper_id: '19'
  parent_volume_id: D19-62
  pdf: https://www.aclweb.org/anthology/D19-6219.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6219.jpg
  title: Recognizing UMLS Semantic Types with Deep Learning
  title_html: Recognizing <span class="acl-fixed-case">UMLS</span> Semantic Types
    with Deep Learning
  url: https://www.aclweb.org/anthology/D19-6219
  year: '2019'
D19-6220:
  abstract: We present a semantically interpretable system for automated ICD coding
    of clinical text documents. Our contribution is an ontological attention mechanism
    which matches the structure of the ICD ontology, in which shared attention vectors
    are learned at each level of the hierarchy, and combined into label-dependent
    ensembles. Analysis of the attention heads shows that shared concepts are learned
    by the lowest common denominator node. This allows child nodes to focus on the
    differentiating concepts, leading to efficient learning and memory usage. Visualisation
    of the multi-level attention on the original text allows explanation of the code
    predictions according to the semantics of the ICD ontology. On the MIMIC-III dataset
    we achieve a 2.7% absolute (11% relative) improvement from 0.218 to 0.245 macro-F1
    score compared to the previous state of the art across 3,912 codes. Finally, we
    analyse the labelling inconsistencies arising from different coding practices
    which limit performance on this task.
  address: Hong Kong
  author:
  - first: Matus
    full: Matus Falis
    id: matus-falis
    last: Falis
  - first: Maciej
    full: Maciej Pajak
    id: maciej-pajak
    last: Pajak
  - first: Aneta
    full: Aneta Lisowska
    id: aneta-lisowska
    last: Lisowska
  - first: Patrick
    full: Patrick Schrempf
    id: patrick-schrempf
    last: Schrempf
  - first: Lucas
    full: Lucas Deckers
    id: lucas-deckers
    last: Deckers
  - first: Shadia
    full: Shadia Mikhael
    id: shadia-mikhael
    last: Mikhael
  - first: Sotirios
    full: Sotirios Tsaftaris
    id: sotirios-tsaftaris
    last: Tsaftaris
  - first: Alison
    full: "Alison O\u2019Neil"
    id: alison-oneil
    last: "O\u2019Neil"
  author_string: "Matus Falis, Maciej Pajak, Aneta Lisowska, Patrick Schrempf, Lucas\
    \ Deckers, Shadia Mikhael, Sotirios Tsaftaris, Alison O\u2019Neil"
  bibkey: falis-etal-2019-ontological
  bibtype: inproceedings
  booktitle: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  booktitle_html: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  doi: 10.18653/v1/D19-6220
  month: November
  page_first: '168'
  page_last: '177'
  pages: "168\u2013177"
  paper_id: '20'
  parent_volume_id: D19-62
  pdf: https://www.aclweb.org/anthology/D19-6220.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6220.jpg
  title: Ontological attention ensembles for capturing semantic concepts in ICD code
    prediction from clinical text
  title_html: Ontological attention ensembles for capturing semantic concepts in <span
    class="acl-fixed-case">ICD</span> code prediction from clinical text
  url: https://www.aclweb.org/anthology/D19-6220
  year: '2019'
D19-6221:
  abstract: Since the introduction of context-aware token representation techniques
    such as Embeddings from Language Models (ELMo) and Bidirectional Encoder Representations
    from Transformers (BERT), there has been numerous reports on improved performance
    on a variety of natural language tasks. Nevertheless, the degree to which the
    resulting context-aware representations encode information about morpho-syntactic
    properties of the word/token in a sentence remains unclear. In this paper, we
    investigate the application and impact of state-of-the-art neural token representations
    for automatic cue-conditional speculation and negation scope detection coupled
    with the independently computed morpho-syntactic information. Through this work,
    We establish a new state-of-the-art for the BioScope and NegPar corpus. More importantly,
    we provide a thorough analysis of neural representations and additional features
    interactions, cue-representation for conditioning, discuss model behavior on different
    datasets and address the annotation-induced biases in the learned representations.
  address: Hong Kong
  author:
  - first: Elena
    full: Elena Sergeeva
    id: elena-sergeeva
    last: Sergeeva
  - first: Henghui
    full: Henghui Zhu
    id: henghui-zhu
    last: Zhu
  - first: Amir
    full: Amir Tahmasebi
    id: amir-tahmasebi
    last: Tahmasebi
  - first: Peter
    full: Peter Szolovits
    id: peter-szolovits
    last: Szolovits
  author_string: Elena Sergeeva, Henghui Zhu, Amir Tahmasebi, Peter Szolovits
  bibkey: sergeeva-etal-2019-neural
  bibtype: inproceedings
  booktitle: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  booktitle_html: Proceedings of the Tenth International Workshop on Health Text Mining
    and Information Analysis (LOUHI 2019)
  doi: 10.18653/v1/D19-6221
  month: November
  page_first: '178'
  page_last: '187'
  pages: "178\u2013187"
  paper_id: '21'
  parent_volume_id: D19-62
  pdf: https://www.aclweb.org/anthology/D19-6221.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6221.jpg
  title: Neural Token Representations and Negation and Speculation Scope Detection
    in Biomedical and General Domain Text
  title_html: Neural Token Representations and Negation and Speculation Scope Detection
    in Biomedical and General Domain Text
  url: https://www.aclweb.org/anthology/D19-6221
  year: '2019'
D19-6300:
  address: Hong Kong, China
  author:
  - first: Simon
    full: Simon Mille
    id: simon-mille
    last: Mille
  - first: Anja
    full: Anja Belz
    id: anja-belz
    last: Belz
  - first: Bernd
    full: Bernd Bohnet
    id: bernd-bohnet
    last: Bohnet
  - first: Yvette
    full: Yvette Graham
    id: yvette-graham
    last: Graham
  - first: Leo
    full: Leo Wanner
    id: leo-wanner
    last: Wanner
  author_string: Simon Mille, Anja Belz, Bernd Bohnet, Yvette Graham, Leo Wanner
  bibkey: emnlp-2019-multilingual
  bibtype: proceedings
  booktitle: Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR
    2019)
  month: November
  paper_id: '0'
  parent_volume_id: D19-63
  pdf: https://www.aclweb.org/anthology/D19-6300.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6300.jpg
  title: Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR
    2019)
  title_html: Proceedings of the 2nd Workshop on Multilingual Surface Realisation
    (MSR 2019)
  url: https://www.aclweb.org/anthology/D19-6300
  year: '2019'
D19-6301:
  abstract: "We report results from the SR\u201919 Shared Task, the second edition\
    \ of a multilingual surface realisation task organised as part of the EMNLP\u2019\
    19 Workshop on Multilingual Surface Realisation. As in SR\u201918, the shared\
    \ task comprised two tracks with different levels of complexity: (a) a shallow\
    \ track where the inputs were full UD structures with word order information removed\
    \ and tokens lemmatised; and (b) a deep track where additionally, functional words\
    \ and morphological information were removed. The shallow track was offered in\
    \ eleven, and the deep track in three languages. Systems were evaluated (a) automatically,\
    \ using a range of intrinsic metrics, and (b) by human judges in terms of readability\
    \ and meaning similarity. This report presents the evaluation results, along with\
    \ descriptions of the SR\u201919 tracks, data and evaluation methods. For full\
    \ descriptions of the participating systems, please see the separate system reports\
    \ elsewhere in this volume."
  address: Hong Kong, China
  author:
  - first: Simon
    full: Simon Mille
    id: simon-mille
    last: Mille
  - first: Anja
    full: Anja Belz
    id: anja-belz
    last: Belz
  - first: Bernd
    full: Bernd Bohnet
    id: bernd-bohnet
    last: Bohnet
  - first: Yvette
    full: Yvette Graham
    id: yvette-graham
    last: Graham
  - first: Leo
    full: Leo Wanner
    id: leo-wanner
    last: Wanner
  author_string: Simon Mille, Anja Belz, Bernd Bohnet, Yvette Graham, Leo Wanner
  bibkey: mille-etal-2019-second
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR
    2019)
  booktitle_html: Proceedings of the 2nd Workshop on Multilingual Surface Realisation
    (MSR 2019)
  doi: 10.18653/v1/D19-6301
  month: November
  page_first: '1'
  page_last: '17'
  pages: "1\u201317"
  paper_id: '1'
  parent_volume_id: D19-63
  pdf: https://www.aclweb.org/anthology/D19-6301.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6301.jpg
  title: "The Second Multilingual Surface Realisation Shared Task (SR\u201919): Overview\
    \ and Evaluation Results"
  title_html: "The Second Multilingual Surface Realisation Shared Task (<span class=\"\
    acl-fixed-case\">SR</span>\u201919): Overview and Evaluation Results"
  url: https://www.aclweb.org/anthology/D19-6301
  year: '2019'
D19-6302:
  abstract: "Recent advances in deep learning have shown promises in solving complex\
    \ combinatorial optimization problems, such as sorting variable-sized sequences.\
    \ In this work, we take a step further and tackle the problem of ordering the\
    \ elements of sequences that come with graph structures. Our solution adopts an\
    \ encoder-decoder framework, in which the encoder is a graph neural network that\
    \ learns the representation for each element, and the decoder predicts the ordering\
    \ of each local neighborhood of the graph in turn. We apply our framework to multilingual\
    \ surface realization, which is the task of ordering and completing sentences\
    \ with their dependency parses given but without the ordering of words. Experiments\
    \ show that our approach is much better for this task than prior works that do\
    \ not consider graph structures. We participated in 2019 Surface Realization Shared\
    \ Task (SR\u201919), and we ranked second out of 14 teams while outperforming\
    \ those teams below by a large margin."
  address: Hong Kong, China
  author:
  - first: Wenchao
    full: Wenchao Du
    id: wenchao-du
    last: Du
  - first: Alan W
    full: Alan W Black
    id: alan-w-black
    last: Black
  author_string: Wenchao Du, Alan W Black
  bibkey: du-black-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR
    2019)
  booktitle_html: Proceedings of the 2nd Workshop on Multilingual Surface Realisation
    (MSR 2019)
  doi: 10.18653/v1/D19-6302
  month: November
  page_first: '18'
  page_last: '24'
  pages: "18\u201324"
  paper_id: '2'
  parent_volume_id: D19-63
  pdf: https://www.aclweb.org/anthology/D19-6302.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6302.jpg
  title: Learning to Order Graph Elements with Application to Multilingual Surface
    Realization
  title_html: Learning to Order Graph Elements with Application to Multilingual Surface
    Realization
  url: https://www.aclweb.org/anthology/D19-6302
  year: '2019'
D19-6303:
  abstract: "This paper describes a method of inflecting and linearizing a lemmatized\
    \ dependency tree by: (1) determining a regular expression and substitution to\
    \ describe each productive wordform rule; (2) learning the dependency distance\
    \ tolerance for each head-dependent pair, resulting in an edge-weighted directed\
    \ acyclic graph (DAG); and (3) topologically sorting the DAG into a surface realization\
    \ based on edge weight. The method\u2019s output for 11 languages across 18 treebanks\
    \ is competitive with the other submissions to the Second Multilingual Surface\
    \ Realization Shared Task (SR \u201819)."
  address: Hong Kong, China
  author:
  - first: William
    full: William Dyer
    id: william-dyer
    last: Dyer
  author_string: William Dyer
  bibkey: dyer-2019-depdist
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR
    2019)
  booktitle_html: Proceedings of the 2nd Workshop on Multilingual Surface Realisation
    (MSR 2019)
  doi: 10.18653/v1/D19-6303
  month: November
  page_first: '25'
  page_last: '34'
  pages: "25\u201334"
  paper_id: '3'
  parent_volume_id: D19-63
  pdf: https://www.aclweb.org/anthology/D19-6303.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6303.jpg
  title: 'DepDist: Surface realization via regex and learned dependency-distance tolerance'
  title_html: '<span class="acl-fixed-case">D</span>ep<span class="acl-fixed-case">D</span>ist:
    Surface realization via regex and learned dependency-distance tolerance'
  url: https://www.aclweb.org/anthology/D19-6303
  year: '2019'
D19-6304:
  abstract: The Surface Realization Shared Task involves mapping Universal Dependency
    graphs to raw text, i.e. restoring word order and inflection from a graph of typed,
    directed dependencies between lemmas. Interpreted Regular Tree Grammars (IRTGs)
    encode the correspondence between generations in multiple algebras, and have previously
    been used for semantic parsing from raw text. Our system induces an IRTG for simultaneously
    building pairs of surface forms and UD graphs in the SRST training data, then
    prunes this grammar for each UD graph in the test data for efficient parsing and
    generation of the surface ordering of lemmas. For the inflection step we use a
    standard sequence-to-sequence model with a biLSTM encoder and an LSTM decoder
    with attention. Both components of our system are available on GitHub under an
    MIT license.
  address: Hong Kong, China
  author:
  - first: "\xC1d\xE1m"
    full: "\xC1d\xE1m Kov\xE1cs"
    id: adam-kovacs
    last: "Kov\xE1cs"
  - first: Evelin
    full: "Evelin \xC1cs"
    id: evelin-acs
    last: "\xC1cs"
  - first: Judit
    full: "Judit \xC1cs"
    id: judit-acs
    last: "\xC1cs"
  - first: Andras
    full: Andras Kornai
    id: andras-kornai1
    last: Kornai
  - first: "G\xE1bor"
    full: "G\xE1bor Recski"
    id: gabor-recski
    last: Recski
  author_string: "\xC1d\xE1m Kov\xE1cs, Evelin \xC1cs, Judit \xC1cs, Andras Kornai,\
    \ G\xE1bor Recski"
  bibkey: kovacs-etal-2019-bme
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR
    2019)
  booktitle_html: Proceedings of the 2nd Workshop on Multilingual Surface Realisation
    (MSR 2019)
  doi: 10.18653/v1/D19-6304
  month: November
  page_first: '35'
  page_last: '40'
  pages: "35\u201340"
  paper_id: '4'
  parent_volume_id: D19-63
  pdf: https://www.aclweb.org/anthology/D19-6304.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6304.jpg
  title: 'BME-UW at SRST-2019: Surface realization with Interpreted Regular Tree Grammars'
  title_html: '<span class="acl-fixed-case">BME</span>-<span class="acl-fixed-case">UW</span>
    at <span class="acl-fixed-case">SRST</span>-2019: Surface realization with Interpreted
    Regular Tree Grammars'
  url: https://www.aclweb.org/anthology/D19-6304
  year: '2019'
D19-6305:
  abstract: "We first describe a surface realizer forUniversal Dependencies (UD) structures.\
    \ The system uses a symbolic approach to transform the dependency tree into a\
    \ tree of constituents that is transformed into an English sentence by an existing\
    \ realizer. This approach was then adapted for the two shared tasks of SR\u2019\
    19. The system is quite fast and showed competitive results for English sentences\
    \ using automatic and manual evaluation measures."
  address: Hong Kong, China
  author:
  - first: Guy
    full: Guy Lapalme
    id: guy-lapalme
    last: Lapalme
  author_string: Guy Lapalme
  bibkey: lapalme-2019-realizing
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR
    2019)
  booktitle_html: Proceedings of the 2nd Workshop on Multilingual Surface Realisation
    (MSR 2019)
  doi: 10.18653/v1/D19-6305
  month: November
  page_first: '41'
  page_last: '49'
  pages: "41\u201349"
  paper_id: '5'
  parent_volume_id: D19-63
  pdf: https://www.aclweb.org/anthology/D19-6305.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6305.jpg
  title: Realizing Universal Dependencies Structures
  title_html: Realizing Universal Dependencies Structures
  url: https://www.aclweb.org/anthology/D19-6305
  year: '2019'
D19-6306:
  abstract: 'We introduce the IMS contribution to the Surface Realization Shared Task
    2019. Our submission achieves the state-of-the-art performance without using any
    external resources. The system takes a pipeline approach consisting of five steps:
    linearization, completion, inflection, contraction, and detokenization. We compare
    the performance of our linearization algorithm with two external baselines and
    report results for each step in the pipeline. Furthermore, we perform detailed
    error analysis revealing correlation between word order freedom and difficulty
    of the linearization task.'
  address: Hong Kong, China
  author:
  - first: Xiang
    full: Xiang Yu
    id: xiang-yu
    last: Yu
  - first: Agnieszka
    full: Agnieszka Falenska
    id: agnieszka-falenska
    last: Falenska
  - first: Marina
    full: Marina Haid
    id: marina-haid
    last: Haid
  - first: Ngoc Thang
    full: Ngoc Thang Vu
    id: ngoc-thang-vu
    last: Vu
  - first: Jonas
    full: Jonas Kuhn
    id: jonas-kuhn
    last: Kuhn
  author_string: Xiang Yu, Agnieszka Falenska, Marina Haid, Ngoc Thang Vu, Jonas Kuhn
  bibkey: yu-etal-2019-imsurreal
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR
    2019)
  booktitle_html: Proceedings of the 2nd Workshop on Multilingual Surface Realisation
    (MSR 2019)
  doi: 10.18653/v1/D19-6306
  month: November
  page_first: '50'
  page_last: '58'
  pages: "50\u201358"
  paper_id: '6'
  parent_volume_id: D19-63
  pdf: https://www.aclweb.org/anthology/D19-6306.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6306.jpg
  title: 'IMSurReal: IMS at the Surface Realization Shared Task 2019'
  title_html: '<span class="acl-fixed-case">IMS</span>ur<span class="acl-fixed-case">R</span>eal:
    <span class="acl-fixed-case">IMS</span> at the Surface Realization Shared Task
    2019'
  url: https://www.aclweb.org/anthology/D19-6306
  year: '2019'
D19-6307:
  abstract: "This study describes the approach developed by the Tilburg University\
    \ team to the shallow track of the Multilingual Surface Realization Shared Task\
    \ 2019 (SR\u201919) (Mille et al., 2019). Based on Ferreira et al. (2017) and\
    \ on our 2018 submission Ferreira et al. (2018), the approach generates texts\
    \ by first preprocessing an input dependency tree into an ordered linearized string,\
    \ which is then realized using a rule-based and a statistical machine translation\
    \ (SMT) model. This year our submission is able to realize texts in the 11 languages\
    \ proposed for the task, different from our last year submission, which covered\
    \ only 6 Indo-European languages. The model is publicly available."
  address: Hong Kong, China
  author:
  - first: Thiago
    full: Thiago Castro Ferreira
    id: thiago-castro-ferreira
    last: Castro Ferreira
  - first: Emiel
    full: Emiel Krahmer
    id: emiel-krahmer
    last: Krahmer
  author_string: Thiago Castro Ferreira, Emiel Krahmer
  bibkey: castro-ferreira-krahmer-2019-surface
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR
    2019)
  booktitle_html: Proceedings of the 2nd Workshop on Multilingual Surface Realisation
    (MSR 2019)
  doi: 10.18653/v1/D19-6307
  month: November
  page_first: '59'
  page_last: '62'
  pages: "59\u201362"
  paper_id: '7'
  parent_volume_id: D19-63
  pdf: https://www.aclweb.org/anthology/D19-6307.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6307.jpg
  title: 'Surface Realization Shared Task 2019 (MSR19): The Team 6 Approach'
  title_html: 'Surface Realization Shared Task 2019 (<span class="acl-fixed-case">MSR</span>19):
    The Team 6 Approach'
  url: https://www.aclweb.org/anthology/D19-6307
  year: '2019'
D19-6308:
  abstract: 'This paper presents the model we developed for the shallow track of the
    2019 NLG Surface Realization Shared Task. The model reconstructs sentences whose
    word order and word inflections were removed. We divided the problem into two
    sub-problems: reordering and inflecting. For the purpose of reordering, we used
    a pointer network integrated with a transformer model as its encoder-decoder modules.
    In order to generate the inflected forms of tokens, a Feed Forward Neural Network
    was employed.'
  address: Hong Kong, China
  author:
  - first: Farhood
    full: Farhood Farahnak
    id: farhood-farahnak
    last: Farahnak
  - first: Laya
    full: Laya Rafiee
    id: laya-rafiee
    last: Rafiee
  - first: Leila
    full: Leila Kosseim
    id: leila-kosseim
    last: Kosseim
  - first: Thomas
    full: Thomas Fevens
    id: thomas-fevens
    last: Fevens
  author_string: Farhood Farahnak, Laya Rafiee, Leila Kosseim, Thomas Fevens
  bibkey: farahnak-etal-2019-concordia
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR
    2019)
  booktitle_html: Proceedings of the 2nd Workshop on Multilingual Surface Realisation
    (MSR 2019)
  doi: 10.18653/v1/D19-6308
  month: November
  page_first: '63'
  page_last: '67'
  pages: "63\u201367"
  paper_id: '8'
  parent_volume_id: D19-63
  pdf: https://www.aclweb.org/anthology/D19-6308.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6308.jpg
  title: The Concordia NLG Surface Realizer at SRST 2019
  title_html: The Concordia <span class="acl-fixed-case">NLG</span> Surface Realizer
    at <span class="acl-fixed-case">SRST</span> 2019
  url: https://www.aclweb.org/anthology/D19-6308
  year: '2019'
D19-6309:
  abstract: We describe our exploratory system for the shallow surface realization
    task, which combines morphological inflection using character sequence-to-sequence
    models with a baseline linearizer that implements a tree-to-tree model using sequence-to-sequence
    models on serialized trees. Results for morphological inflection were competitive
    across languages. Due to time constraints, we could only submit complete results
    (including linearization) for English. Preliminary linearization results were
    decent, with a small benefit from reranking to prefer valid output trees, but
    inadequate control over the words in the output led to poor quality on longer
    sentences.
  address: Hong Kong, China
  author:
  - first: Kartikeya
    full: Kartikeya Upasani
    id: kartikeya-upasani
    last: Upasani
  - first: David
    full: David King
    id: david-king
    last: King
  - first: Jinfeng
    full: Jinfeng Rao
    id: jinfeng-rao
    last: Rao
  - first: Anusha
    full: Anusha Balakrishnan
    id: anusha-balakrishnan
    last: Balakrishnan
  - first: Michael
    full: Michael White
    id: michael-white
    last: White
  author_string: Kartikeya Upasani, David King, Jinfeng Rao, Anusha Balakrishnan,
    Michael White
  bibkey: upasani-etal-2019-osu
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR
    2019)
  booktitle_html: Proceedings of the 2nd Workshop on Multilingual Surface Realisation
    (MSR 2019)
  doi: 10.18653/v1/D19-6309
  month: November
  page_first: '68'
  page_last: '74'
  pages: "68\u201374"
  paper_id: '9'
  parent_volume_id: D19-63
  pdf: https://www.aclweb.org/anthology/D19-6309.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6309.jpg
  title: 'The OSU/Facebook Realizer for SRST 2019: Seq2Seq Inflection and Serialized
    Tree2Tree Linearization'
  title_html: 'The <span class="acl-fixed-case">OSU</span>/<span class="acl-fixed-case">F</span>acebook
    Realizer for <span class="acl-fixed-case">SRST</span> 2019: <span class="acl-fixed-case">S</span>eq2<span
    class="acl-fixed-case">S</span>eq Inflection and Serialized <span class="acl-fixed-case">T</span>ree2<span
    class="acl-fixed-case">T</span>ree Linearization'
  url: https://www.aclweb.org/anthology/D19-6309
  year: '2019'
D19-6310:
  abstract: The Multilingual Surface Realization Shared Task 2019 focuses on generating
    sentences from lemmatized sets of universal dependency parses with rich features.
    This paper describes the results of our participation in the deep track. The core
    innovation in our approach is to use a graph convolutional network to encode the
    dependency trees given as input. Upon adding morphological features, our system
    achieves the third rank without using data augmentation techniques or additional
    components (such as a re-ranker).
  address: Hong Kong, China
  author:
  - first: Xudong
    full: Xudong Hong
    id: xudong-hong
    last: Hong
  - first: Ernie
    full: Ernie Chang
    id: ernie-chang
    last: Chang
  - first: Vera
    full: Vera Demberg
    id: vera-demberg
    last: Demberg
  author_string: Xudong Hong, Ernie Chang, Vera Demberg
  bibkey: hong-etal-2019-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR
    2019)
  booktitle_html: Proceedings of the 2nd Workshop on Multilingual Surface Realisation
    (MSR 2019)
  doi: 10.18653/v1/D19-6310
  month: November
  page_first: '75'
  page_last: '80'
  pages: "75\u201380"
  paper_id: '10'
  parent_volume_id: D19-63
  pdf: https://www.aclweb.org/anthology/D19-6310.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6310.jpg
  title: Improving Language Generation from Feature-Rich Tree-Structured Data with
    Relational Graph Convolutional Encoders
  title_html: Improving Language Generation from Feature-Rich Tree-Structured Data
    with Relational Graph Convolutional Encoders
  url: https://www.aclweb.org/anthology/D19-6310
  year: '2019'
D19-6311:
  abstract: "We describe the system presented at the SR\u201919 shared task by the\
    \ DipInfoUnito team. Our approach is based on supervised machine learning. In\
    \ particular, we divide the SR task into two independent subtasks, namely word\
    \ order prediction and morphology inflection prediction. Two neural networks with\
    \ different architectures run on the same input structure, each producing a partial\
    \ output which is recombined in the final step in order to produce the predicted\
    \ surface form. This work is a direct successor of the architecture presented\
    \ at SR\u201919."
  address: Hong Kong, China
  author:
  - first: Alessandro
    full: Alessandro Mazzei
    id: alessandro-mazzei
    last: Mazzei
  - first: Valerio
    full: Valerio Basile
    id: valerio-basile
    last: Basile
  author_string: Alessandro Mazzei, Valerio Basile
  bibkey: mazzei-basile-2019-dipinfounito
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR
    2019)
  booktitle_html: Proceedings of the 2nd Workshop on Multilingual Surface Realisation
    (MSR 2019)
  doi: 10.18653/v1/D19-6311
  month: November
  page_first: '81'
  page_last: '87'
  pages: "81\u201387"
  paper_id: '11'
  parent_volume_id: D19-63
  pdf: https://www.aclweb.org/anthology/D19-6311.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6311.jpg
  title: "The DipInfoUniTo Realizer at SRST\u201919: Learning to Rank and Deep Morphology\
    \ Prediction for Multilingual Surface Realization"
  title_html: "The <span class=\"acl-fixed-case\">D</span>ip<span class=\"acl-fixed-case\"\
    >I</span>nfo<span class=\"acl-fixed-case\">U</span>ni<span class=\"acl-fixed-case\"\
    >T</span>o Realizer at <span class=\"acl-fixed-case\">SRST</span>\u201919: Learning\
    \ to Rank and Deep Morphology Prediction for Multilingual Surface Realization"
  url: https://www.aclweb.org/anthology/D19-6311
  year: '2019'
D19-6312:
  abstract: This paper presents the LORIA / Lorraine University submission at the
    Multilingual Surface Realisation shared task 2019 for the shallow track. We outline
    our approach and evaluate it on 11 languages covered by the shared task. We provide
    a separate evaluation of each component of our pipeline, concluding on some difficulties
    and suggesting directions for future work.
  address: Hong Kong, China
  author:
  - first: Anastasia
    full: Anastasia Shimorina
    id: anastasia-shimorina
    last: Shimorina
  - first: Claire
    full: Claire Gardent
    id: claire-gardent
    last: Gardent
  author_string: Anastasia Shimorina, Claire Gardent
  bibkey: shimorina-gardent-2019-loria
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR
    2019)
  booktitle_html: Proceedings of the 2nd Workshop on Multilingual Surface Realisation
    (MSR 2019)
  doi: 10.18653/v1/D19-6312
  month: November
  page_first: '88'
  page_last: '93'
  pages: "88\u201393"
  paper_id: '12'
  parent_volume_id: D19-63
  pdf: https://www.aclweb.org/anthology/D19-6312.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6312.jpg
  title: LORIA / Lorraine University at Multilingual Surface Realisation 2019
  title_html: <span class="acl-fixed-case">LORIA</span> / Lorraine University at Multilingual
    Surface Realisation 2019
  url: https://www.aclweb.org/anthology/D19-6312
  year: '2019'
D19-6313:
  abstract: This paper presents an exploratory study that aims to evaluate the usefulness
    of back-translation in Natural Language Generation (NLG) from semantic representations
    for non-English languages. Specifically, Abstract Meaning Representation and Brazilian
    Portuguese (BP) are chosen as semantic representation and language, respectively.
    Two methods (focused on Statistical and Neural Machine Translation) are evaluated
    on two datasets (one automatically generated and another one human-generated)
    to compare the performance in a real context. Also, several cuts according to
    quality measures are performed to evaluate the importance (or not) of the data
    quality in NLG. Results show that there are still many improvements to be made
    but this is a promising approach.
  address: Hong Kong, China
  author:
  - first: Marco Antonio
    full: Marco Antonio Sobrevilla Cabezudo
    id: marco-antonio-sobrevilla-cabezudo
    last: Sobrevilla Cabezudo
  - first: Simon
    full: Simon Mille
    id: simon-mille
    last: Mille
  - first: Thiago
    full: Thiago Pardo
    id: thiago-pardo
    last: Pardo
  author_string: Marco Antonio Sobrevilla Cabezudo, Simon Mille, Thiago Pardo
  bibkey: sobrevilla-cabezudo-etal-2019-back
  bibtype: inproceedings
  booktitle: Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR
    2019)
  booktitle_html: Proceedings of the 2nd Workshop on Multilingual Surface Realisation
    (MSR 2019)
  doi: 10.18653/v1/D19-6313
  month: November
  page_first: '94'
  page_last: '103'
  pages: "94\u2013103"
  paper_id: '13'
  parent_volume_id: D19-63
  pdf: https://www.aclweb.org/anthology/D19-6313.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6313.jpg
  title: Back-Translation as Strategy to Tackle the Lack of Corpus in Natural Language
    Generation from Semantic Representations
  title_html: Back-Translation as Strategy to Tackle the Lack of Corpus in Natural
    Language Generation from Semantic Representations
  url: https://www.aclweb.org/anthology/D19-6313
  year: '2019'
D19-6400:
  address: Hong Kong, China
  author:
  - first: Aditya
    full: Aditya Mogadala
    id: aditya-mogadala
    last: Mogadala
  - first: Dietrich
    full: Dietrich Klakow
    id: dietrich-klakow
    last: Klakow
  - first: Sandro
    full: Sandro Pezzelle
    id: sandro-pezzelle
    last: Pezzelle
  - first: Marie-Francine
    full: Marie-Francine Moens
    id: marie-francine-moens
    last: Moens
  author_string: Aditya Mogadala, Dietrich Klakow, Sandro Pezzelle, Marie-Francine
    Moens
  bibkey: emnlp-2019-beyond
  bibtype: proceedings
  booktitle: 'Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world
    kNowledge (LANTERN)'
  month: November
  paper_id: '0'
  parent_volume_id: D19-64
  pdf: https://www.aclweb.org/anthology/D19-6400.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6400.jpg
  title: 'Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world kNowledge
    (LANTERN)'
  title_html: 'Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world
    kNowledge (LANTERN)'
  url: https://www.aclweb.org/anthology/D19-6400
  year: '2019'
D19-6401:
  abstract: Neural Module Networks, originally proposed for the task of visual question
    answering, are a class of neural network architectures that involve human-specified
    neural modules, each designed for a specific form of reasoning. In current formulations
    of such networks only the parameters of the neural modules and/or the order of
    their execution is learned. In this work, we further expand this approach and
    also learn the underlying internal structure of modules in terms of the ordering
    and combination of simple and elementary arithmetic operators. We utilize a minimum
    amount of prior knowledge from the human-specified neural modules in the form
    of different input types and arithmetic operators used in these modules. Our results
    show that one is indeed able to simultaneously learn both internal module structure
    and module sequencing without extra supervisory signals for module execution sequencing.
    With this approach, we report performance comparable to models using hand-designed
    modules. In addition, we do a analysis of sensitivity of the learned modules w.r.t.
    the arithmetic operations and infer the analytical expressions of the learned
    modules.
  address: Hong Kong, China
  attachment:
  - filename: D19-6401.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-6401.Attachment.zip
  author:
  - first: Vardaan
    full: Vardaan Pahuja
    id: vardaan-pahuja
    last: Pahuja
  - first: Jie
    full: Jie Fu
    id: jie-fu
    last: Fu
  - first: Sarath
    full: Sarath Chandar
    id: sarath-chandar
    last: Chandar
  - first: Christopher
    full: Christopher Pal
    id: christopher-pal
    last: Pal
  author_string: Vardaan Pahuja, Jie Fu, Sarath Chandar, Christopher Pal
  bibkey: pahuja-etal-2019-structure
  bibtype: inproceedings
  booktitle: 'Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world
    kNowledge (LANTERN)'
  booktitle_html: 'Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world
    kNowledge (LANTERN)'
  doi: 10.18653/v1/D19-6401
  month: November
  page_first: '1'
  page_last: '10'
  pages: "1\u201310"
  paper_id: '1'
  parent_volume_id: D19-64
  pdf: https://www.aclweb.org/anthology/D19-6401.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6401.jpg
  title: Structure Learning for Neural Module Networks
  title_html: Structure Learning for Neural Module Networks
  url: https://www.aclweb.org/anthology/D19-6401
  year: '2019'
D19-6402:
  abstract: 'In this paper, we propose a new approach to learn multimodal multilingual
    embeddings for matching images and their relevant captions in two languages. We
    combine two existing objective functions to make images and captions close in
    a joint embedding space while adapting the alignment of word embeddings between
    existing languages in our model. We show that our approach enables better generalization,
    achieving state-of-the-art performance in text-to-image and image-to-text retrieval
    task, and caption-caption similarity task. Two multimodal multilingual datasets
    are used for evaluation: Multi30k with German and English captions and Microsoft-COCO
    with English and Japanese captions.'
  address: Hong Kong, China
  attachment:
  - filename: D19-6402.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-6402.Attachment.zip
  author:
  - first: Alireza
    full: Alireza Mohammadshahi
    id: alireza-mohammadshahi
    last: Mohammadshahi
  - first: "R\xE9mi"
    full: "R\xE9mi Lebret"
    id: remi-lebret
    last: Lebret
  - first: Karl
    full: Karl Aberer
    id: karl-aberer
    last: Aberer
  author_string: "Alireza Mohammadshahi, R\xE9mi Lebret, Karl Aberer"
  bibkey: mohammadshahi-etal-2019-aligning
  bibtype: inproceedings
  booktitle: 'Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world
    kNowledge (LANTERN)'
  booktitle_html: 'Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world
    kNowledge (LANTERN)'
  doi: 10.18653/v1/D19-6402
  month: November
  page_first: '11'
  page_last: '17'
  pages: "11\u201317"
  paper_id: '2'
  parent_volume_id: D19-64
  pdf: https://www.aclweb.org/anthology/D19-6402.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6402.jpg
  title: Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task
  title_html: Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task
  url: https://www.aclweb.org/anthology/D19-6402
  year: '2019'
D19-6403:
  abstract: "In this paper, we experiment with a recently proposed visual reasoning\
    \ task dealing with quantities \u2013 modeling the multimodal, contextually-dependent\
    \ meaning of size adjectives (\u2018big\u2019, \u2018small\u2019) \u2013 and explore\
    \ the impact of varying the training data on the learning behavior of a state-of-art\
    \ system. In previous work, models have been shown to fail in generalizing to\
    \ unseen adjective-noun combinations. Here, we investigate whether, and to what\
    \ extent, seeing some of these cases during training helps a model understand\
    \ the rule subtending the task, i.e., that being big implies being not small,\
    \ and vice versa. We show that relatively few examples are enough to understand\
    \ this relationship, and that developing a specific, mutually exclusive representation\
    \ of size adjectives is beneficial to the task."
  address: Hong Kong, China
  author:
  - first: Sandro
    full: Sandro Pezzelle
    id: sandro-pezzelle
    last: Pezzelle
  - first: Raquel
    full: "Raquel Fern\xE1ndez"
    id: raquel-fernandez
    last: "Fern\xE1ndez"
  author_string: "Sandro Pezzelle, Raquel Fern\xE1ndez"
  bibkey: pezzelle-fernandez-2019-big
  bibtype: inproceedings
  booktitle: 'Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world
    kNowledge (LANTERN)'
  booktitle_html: 'Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world
    kNowledge (LANTERN)'
  doi: 10.18653/v1/D19-6403
  month: November
  page_first: '18'
  page_last: '23'
  pages: "18\u201323"
  paper_id: '3'
  parent_volume_id: D19-64
  pdf: https://www.aclweb.org/anthology/D19-6403.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6403.jpg
  title: 'Big Generalizations with Small Data: Exploring the Role of Training Samples
    in Learning Adjectives of Size'
  title_html: 'Big Generalizations with Small Data: Exploring the Role of Training
    Samples in Learning Adjectives of Size'
  url: https://www.aclweb.org/anthology/D19-6403
  year: '2019'
D19-6404:
  abstract: Chinese characters are unique in its logographic nature, which inherently
    encodes world knowledge through thousands of years evolution. This paper proposes
    an embedding approach, namely eigencharacter (EC) space, which helps NLP application
    easily access the knowledge encoded in Chinese orthography. These EC representations
    are automatically extracted, encode both structural and radical information, and
    easily integrate with other computational models. We built EC representations
    of 5,000 Chinese characters, investigated orthography knowledge encoded in ECs,
    and demonstrated how these ECs identified visually similar characters with both
    structural and radical information.
  address: Hong Kong, China
  author:
  - first: Yu-Hsiang
    full: Yu-Hsiang Tseng
    id: yu-hsiang-tseng
    last: Tseng
  - first: Shu-Kai
    full: Shu-Kai Hsieh
    id: shu-kai-hsieh
    last: Hsieh
  author_string: Yu-Hsiang Tseng, Shu-Kai Hsieh
  bibkey: tseng-hsieh-2019-eigencharacter
  bibtype: inproceedings
  booktitle: 'Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world
    kNowledge (LANTERN)'
  booktitle_html: 'Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world
    kNowledge (LANTERN)'
  doi: 10.18653/v1/D19-6404
  month: November
  page_first: '24'
  page_last: '28'
  pages: "24\u201328"
  paper_id: '4'
  parent_volume_id: D19-64
  pdf: https://www.aclweb.org/anthology/D19-6404.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6404.jpg
  title: 'Eigencharacter: An Embedding of Chinese Character Orthography'
  title_html: '<span class="acl-fixed-case">E</span>igencharacter: An Embedding of
    <span class="acl-fixed-case">C</span>hinese Character Orthography'
  url: https://www.aclweb.org/anthology/D19-6404
  year: '2019'
D19-6405:
  abstract: Scene graphs represent semantic information in images, which can help
    image captioning system to produce more descriptive outputs versus using only
    the image as context. Recent captioning approaches rely on ad-hoc approaches to
    obtain graphs for images. However, those graphs introduce noise and it is unclear
    the effect of parser errors on captioning accuracy. In this work, we investigate
    to what extent scene graphs can help image captioning. Our results show that a
    state-of-the-art scene graph parser can boost performance almost as much as the
    ground truth graphs, showing that the bottleneck currently resides more on the
    captioning models than on the performance of the scene graph parser.
  address: Hong Kong, China
  author:
  - first: Dalin
    full: Dalin Wang
    id: dalin-wang
    last: Wang
  - first: Daniel
    full: Daniel Beck
    id: daniel-beck
    last: Beck
  - first: Trevor
    full: Trevor Cohn
    id: trevor-cohn
    last: Cohn
  author_string: Dalin Wang, Daniel Beck, Trevor Cohn
  bibkey: wang-etal-2019-role
  bibtype: inproceedings
  booktitle: 'Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world
    kNowledge (LANTERN)'
  booktitle_html: 'Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world
    kNowledge (LANTERN)'
  doi: 10.18653/v1/D19-6405
  month: November
  page_first: '29'
  page_last: '34'
  pages: "29\u201334"
  paper_id: '5'
  parent_volume_id: D19-64
  pdf: https://www.aclweb.org/anthology/D19-6405.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6405.jpg
  title: On the Role of Scene Graphs in Image Captioning
  title_html: On the Role of Scene Graphs in Image Captioning
  url: https://www.aclweb.org/anthology/D19-6405
  year: '2019'
D19-6406:
  abstract: It is assumed that multimodal machine translation systems are better than
    text-only systems at translating phrases that have a direct correspondence in
    the image. This assumption has been challenged in experiments demonstrating that
    state-of-the-art multimodal systems perform equally well in the presence of randomly
    selected images, but, more recently, it has been shown that masking entities from
    the source language sentence during training can help to overcome this problem.
    In this paper, we conduct experiments with both visual and textual adversaries
    in order to understand the role of incorrect textual inputs to such systems. Our
    results show that when the source language sentence contains mistakes, multimodal
    translation systems do not leverage the additional visual signal to produce the
    correct translation. We also find that the degradation of translation performance
    caused by textual adversaries is significantly higher than by visual adversaries.
  address: Hong Kong, China
  author:
  - first: Koel
    full: Koel Dutta Chowdhury
    id: koel-dutta-chowdhury
    last: Dutta Chowdhury
  - first: Desmond
    full: Desmond Elliott
    id: desmond-elliott
    last: Elliott
  author_string: Koel Dutta Chowdhury, Desmond Elliott
  bibkey: dutta-chowdhury-elliott-2019-understanding
  bibtype: inproceedings
  booktitle: 'Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world
    kNowledge (LANTERN)'
  booktitle_html: 'Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world
    kNowledge (LANTERN)'
  doi: 10.18653/v1/D19-6406
  month: November
  page_first: '35'
  page_last: '40'
  pages: "35\u201340"
  paper_id: '6'
  parent_volume_id: D19-64
  pdf: https://www.aclweb.org/anthology/D19-6406.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6406.jpg
  title: Understanding the Effect of Textual Adversaries in Multimodal Machine Translation
  title_html: Understanding the Effect of Textual Adversaries in Multimodal Machine
    Translation
  url: https://www.aclweb.org/anthology/D19-6406
  year: '2019'
D19-6407:
  abstract: "Previous research into agent communication has shown that a pre-trained\
    \ guide can speed up the learning process of an imitation learning agent. The\
    \ guide achieves this by providing the agent with discrete messages in an emerged\
    \ language about how to solve the task. We extend this one-directional communication\
    \ by a one-bit communication channel from the learner back to the guide: It is\
    \ able to ask the guide for help, and we limit the guidance by penalizing the\
    \ learner for these requests. During training, the agent learns to control this\
    \ gate based on its current observation. We find that the amount of requested\
    \ guidance decreases over time and guidance is requested in situations of high\
    \ uncertainty. We investigate the agent\u2019s performance in cases of open and\
    \ closed gates and discuss potential motives for the observed gating behavior."
  address: Hong Kong, China
  attachment:
  - filename: D19-6407.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-6407.Attachment.zip
  author:
  - first: Benjamin
    full: Benjamin Kolb
    id: benjamin-kolb
    last: Kolb
  - first: Leon
    full: Leon Lang
    id: leon-lang
    last: Lang
  - first: Henning
    full: Henning Bartsch
    id: henning-bartsch
    last: Bartsch
  - first: Arwin
    full: Arwin Gansekoele
    id: arwin-gansekoele
    last: Gansekoele
  - first: Raymond
    full: Raymond Koopmanschap
    id: raymond-koopmanschap
    last: Koopmanschap
  - first: Leonardo
    full: Leonardo Romor
    id: leonardo-romor
    last: Romor
  - first: David
    full: David Speck
    id: david-speck
    last: Speck
  - first: Mathijs
    full: Mathijs Mul
    id: mathijs-mul
    last: Mul
  - first: Elia
    full: Elia Bruni
    id: elia-bruni
    last: Bruni
  author_string: Benjamin Kolb, Leon Lang, Henning Bartsch, Arwin Gansekoele, Raymond
    Koopmanschap, Leonardo Romor, David Speck, Mathijs Mul, Elia Bruni
  bibkey: kolb-etal-2019-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world
    kNowledge (LANTERN)'
  booktitle_html: 'Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world
    kNowledge (LANTERN)'
  doi: 10.18653/v1/D19-6407
  month: November
  page_first: '41'
  page_last: '50'
  pages: "41\u201350"
  paper_id: '7'
  parent_volume_id: D19-64
  pdf: https://www.aclweb.org/anthology/D19-6407.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6407.jpg
  title: Learning to request guidance in emergent language
  title_html: Learning to request guidance in emergent language
  url: https://www.aclweb.org/anthology/D19-6407
  year: '2019'
D19-6408:
  abstract: "Readers\u2019 eye movements used as part of the training signal have\
    \ been shown to improve performance in a wide range of Natural Language Processing\
    \ (NLP) tasks. Previous work uses gaze data either at the type level or at the\
    \ token level and mostly from a single eye-tracking corpus. In this paper, we\
    \ analyze type vs token-level integration options with eye tracking data from\
    \ two corpora to inform two syntactic sequence labeling problems: binary phrase\
    \ chunking and part-of-speech tagging. We show that using globally-aggregated\
    \ measures that capture the central tendency or variability of gaze data is more\
    \ beneficial than proposed local views which retain individual participant information.\
    \ While gaze data is informative for supervised POS tagging, which complements\
    \ previous findings on unsupervised POS induction, almost no improvement is obtained\
    \ for binary phrase chunking, except for a single specific setup. Hence, caution\
    \ is warranted when using gaze data as signal for NLP, as no single view is robust\
    \ over tasks, modeling choice and gaze corpus."
  address: Hong Kong, China
  author:
  - first: Sigrid
    full: Sigrid Klerke
    id: sigrid-klerke
    last: Klerke
  - first: Barbara
    full: Barbara Plank
    id: barbara-plank
    last: Plank
  author_string: Sigrid Klerke, Barbara Plank
  bibkey: klerke-plank-2019-glance
  bibtype: inproceedings
  booktitle: 'Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world
    kNowledge (LANTERN)'
  booktitle_html: 'Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world
    kNowledge (LANTERN)'
  doi: 10.18653/v1/D19-6408
  month: November
  page_first: '51'
  page_last: '61'
  pages: "51\u201361"
  paper_id: '8'
  parent_volume_id: D19-64
  pdf: https://www.aclweb.org/anthology/D19-6408.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6408.jpg
  title: 'At a Glance: The Impact of Gaze Aggregation Views on Syntactic Tagging'
  title_html: 'At a Glance: The Impact of Gaze Aggregation Views on Syntactic Tagging'
  url: https://www.aclweb.org/anthology/D19-6408
  year: '2019'
D19-6409:
  abstract: "How can we teach artificial agents to use human language flexibly to\
    \ solve problems in real-world environments? We have an example of this in nature:\
    \ human babies eventually learn to use human language to solve problems, and they\
    \ are taught with an adult human-in-the-loop. Unfortunately, current machine learning\
    \ methods (e.g. from deep reinforcement learning) are too data inefficient to\
    \ learn language in this way. An outstanding goal is finding an algorithm with\
    \ a suitable \u2018language learning prior\u2019 that allows it to learn human\
    \ language, while minimizing the number of on-policy human interactions. In this\
    \ paper, we propose to learn such a prior in simulation using an approach we call,\
    \ Learning to Learn to Communicate (L2C). Specifically, in L2C we train a meta-learning\
    \ agent in simulation to interact with populations of pre-trained agents, each\
    \ with their own distinct communication protocol. Once the meta-learning agent\
    \ is able to quickly adapt to each population of agents, it can be deployed in\
    \ new populations, including populations speaking human language. Our key insight\
    \ is that such populations can be obtained via self-play, after pre-training agents\
    \ with imitation learning on a small amount of off-policy human language data.\
    \ We call this latter technique Seeded Self-Play (S2P). Our preliminary experiments\
    \ show that agents trained with L2C and S2P need fewer on-policy samples to learn\
    \ a compositional language in a Lewis signaling game."
  address: Hong Kong, China
  author:
  - first: Abhinav
    full: Abhinav Gupta
    id: abhinav-gupta
    last: Gupta
  - first: Ryan
    full: Ryan Lowe
    id: ryan-lowe
    last: Lowe
  - first: Jakob
    full: Jakob Foerster
    id: jakob-foerster
    last: Foerster
  - first: Douwe
    full: Douwe Kiela
    id: douwe-kiela
    last: Kiela
  - first: Joelle
    full: Joelle Pineau
    id: joelle-pineau
    last: Pineau
  author_string: Abhinav Gupta, Ryan Lowe, Jakob Foerster, Douwe Kiela, Joelle Pineau
  bibkey: gupta-etal-2019-seeded
  bibtype: inproceedings
  booktitle: 'Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world
    kNowledge (LANTERN)'
  booktitle_html: 'Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world
    kNowledge (LANTERN)'
  doi: 10.18653/v1/D19-6409
  month: November
  page_first: '62'
  page_last: '66'
  pages: "62\u201366"
  paper_id: '9'
  parent_volume_id: D19-64
  pdf: https://www.aclweb.org/anthology/D19-6409.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6409.jpg
  title: Seeded self-play for language learning
  title_html: Seeded self-play for language learning
  url: https://www.aclweb.org/anthology/D19-6409
  year: '2019'
D19-6500:
  address: Hong Kong, China
  author:
  - first: Andrei
    full: Andrei Popescu-Belis
    id: andrei-popescu-belis
    last: Popescu-Belis
  - first: Sharid
    full: "Sharid Lo\xE1iciga"
    id: sharid-loaiciga
    last: "Lo\xE1iciga"
  - first: Christian
    full: Christian Hardmeier
    id: christian-hardmeier
    last: Hardmeier
  - first: Deyi
    full: Deyi Xiong
    id: deyi-xiong
    last: Xiong
  author_string: "Andrei Popescu-Belis, Sharid Lo\xE1iciga, Christian Hardmeier, Deyi\
    \ Xiong"
  bibkey: emnlp-2019-discourse
  bibtype: proceedings
  booktitle: Proceedings of the Fourth Workshop on Discourse in Machine Translation
    (DiscoMT 2019)
  month: November
  paper_id: '0'
  parent_volume_id: D19-65
  pdf: https://www.aclweb.org/anthology/D19-6500.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6500.jpg
  title: Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT
    2019)
  title_html: Proceedings of the Fourth Workshop on Discourse in Machine Translation
    (DiscoMT 2019)
  url: https://www.aclweb.org/anthology/D19-6500
  year: '2019'
D19-6501:
  abstract: 'We analyse coreference phenomena in three neural machine translation
    systems trained with different data settings with or without access to explicit
    intra- and cross-sentential anaphoric information. We compare system performance
    on two different genres: news and TED talks. To do this, we manually annotate
    (the possibly incorrect) coreference chains in the MT outputs and evaluate the
    coreference chain translations. We define an error typology that aims to go further
    than pronoun translation adequacy and includes types such as incorrect word selection
    or missing words. The features of coreference chains in automatic translations
    are also compared to those of the source texts and human translations. The analysis
    shows stronger potential translationese effects in machine translated outputs
    than in human translations.'
  address: Hong Kong, China
  author:
  - first: Ekaterina
    full: Ekaterina Lapshinova-Koltunski
    id: ekaterina-lapshinova-koltunski
    last: Lapshinova-Koltunski
  - first: Cristina
    full: "Cristina Espa\xF1a-Bonet"
    id: cristina-espana-bonet
    last: "Espa\xF1a-Bonet"
  - first: Josef
    full: Josef van Genabith
    id: josef-van-genabith
    last: van Genabith
  author_string: "Ekaterina Lapshinova-Koltunski, Cristina Espa\xF1a-Bonet, Josef\
    \ van Genabith"
  bibkey: lapshinova-koltunski-etal-2019-analysing
  bibtype: inproceedings
  booktitle: Proceedings of the Fourth Workshop on Discourse in Machine Translation
    (DiscoMT 2019)
  booktitle_html: Proceedings of the Fourth Workshop on Discourse in Machine Translation
    (DiscoMT 2019)
  doi: 10.18653/v1/D19-6501
  month: November
  page_first: '1'
  page_last: '12'
  pages: "1\u201312"
  paper_id: '1'
  parent_volume_id: D19-65
  pdf: https://www.aclweb.org/anthology/D19-6501.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6501.jpg
  title: Analysing Coreference in Transformer Outputs
  title_html: Analysing Coreference in Transformer Outputs
  url: https://www.aclweb.org/anthology/D19-6501
  year: '2019'
D19-6502:
  abstract: "This work presents a decoding architecture that fuses the information\
    \ from a neural translation model and the context semantics enclosed in a semantic\
    \ space language model based on word embeddings. The method extends the beam search\
    \ decoding process and therefore can be applied to any neural machine translation\
    \ framework. With this, we sidestep two drawbacks of current document-level systems:\
    \ (i) we do not modify the training process so there is no increment in training\
    \ time, and (ii) we do not require document-level an-notated data. We analyze\
    \ the impact of the fusion system approach and its parameters on the final translation\
    \ quality for English\u2013Spanish. We obtain consistent and statistically significant\
    \ improvements in terms of BLEU and METEOR and we observe how the fused systems\
    \ are able to handle synonyms to propose more adequate translations as well as\
    \ help the system to disambiguate among several translation candidates for a word."
  address: Hong Kong, China
  author:
  - first: Eva
    full: "Eva Mart\xEDnez Garcia"
    id: eva-martinez-garcia
    last: "Mart\xEDnez Garcia"
  - first: Carles
    full: Carles Creus
    id: carles-creus
    last: Creus
  - first: Cristina
    full: "Cristina Espa\xF1a-Bonet"
    id: cristina-espana-bonet
    last: "Espa\xF1a-Bonet"
  author_string: "Eva Mart\xEDnez Garcia, Carles Creus, Cristina Espa\xF1a-Bonet"
  bibkey: martinez-garcia-etal-2019-context
  bibtype: inproceedings
  booktitle: Proceedings of the Fourth Workshop on Discourse in Machine Translation
    (DiscoMT 2019)
  booktitle_html: Proceedings of the Fourth Workshop on Discourse in Machine Translation
    (DiscoMT 2019)
  doi: 10.18653/v1/D19-6502
  month: November
  page_first: '13'
  page_last: '23'
  pages: "13\u201323"
  paper_id: '2'
  parent_volume_id: D19-65
  pdf: https://www.aclweb.org/anthology/D19-6502.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6502.jpg
  title: Context-Aware Neural Machine Translation Decoding
  title_html: Context-Aware Neural Machine Translation Decoding
  url: https://www.aclweb.org/anthology/D19-6502
  year: '2019'
D19-6503:
  abstract: Document-level context has received lots of attention for compensating
    neural machine translation (NMT) of isolated sentences. However, recent advances
    in document-level NMT focus on sophisticated integration of the context, explaining
    its improvement with only a few selected examples or targeted test sets. We extensively
    quantify the causes of improvements by a document-level model in general test
    sets, clarifying the limit of the usefulness of document-level context in NMT.
    We show that most of the improvements are not interpretable as utilizing the context.
    We also show that a minimal encoding is sufficient for the context modeling and
    very long context is not helpful for NMT.
  address: Hong Kong, China
  attachment:
  - filename: D19-6503.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-6503.Attachment.pdf
  author:
  - first: Yunsu
    full: Yunsu Kim
    id: yunsu-kim
    last: Kim
  - first: Duc Thanh
    full: Duc Thanh Tran
    id: duc-thanh-tran
    last: Tran
  - first: Hermann
    full: Hermann Ney
    id: hermann-ney
    last: Ney
  author_string: Yunsu Kim, Duc Thanh Tran, Hermann Ney
  bibkey: kim-etal-2019-document
  bibtype: inproceedings
  booktitle: Proceedings of the Fourth Workshop on Discourse in Machine Translation
    (DiscoMT 2019)
  booktitle_html: Proceedings of the Fourth Workshop on Discourse in Machine Translation
    (DiscoMT 2019)
  doi: 10.18653/v1/D19-6503
  month: November
  page_first: '24'
  page_last: '34'
  pages: "24\u201334"
  paper_id: '3'
  parent_volume_id: D19-65
  pdf: https://www.aclweb.org/anthology/D19-6503.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6503.jpg
  title: When and Why is Document-level Context Useful in Neural Machine Translation?
  title_html: When and Why is Document-level Context Useful in Neural Machine Translation?
  url: https://www.aclweb.org/anthology/D19-6503
  year: '2019'
D19-6504:
  abstract: A single sentence does not always convey information that is enough to
    translate it into other languages. Some target languages need to add or specialize
    words that are omitted or ambiguous in the source languages (e.g, zero pronouns
    in translating Japanese to English or epicene pronouns in translating English
    to French). To translate such ambiguous sentences, we need contexts beyond a single
    sentence, and have so far explored context-aware neural machine translation (NMT).
    However, a large amount of parallel corpora is not easily available to train accurate
    context-aware NMT models. In this study, we first obtain large-scale pseudo parallel
    corpora by back-translating monolingual data, and then investigate its impact
    on the translation accuracy of context-aware NMT models. We evaluated context-aware
    NMT models trained with small parallel corpora and the large-scale pseudo parallel
    corpora on English-Japanese and English-French datasets to demonstrate the large
    impact of the data augmentation for context-aware NMT models.
  address: Hong Kong, China
  author:
  - first: Amane
    full: Amane Sugiyama
    id: amane-sugiyama
    last: Sugiyama
  - first: Naoki
    full: Naoki Yoshinaga
    id: naoki-yoshinaga
    last: Yoshinaga
  author_string: Amane Sugiyama, Naoki Yoshinaga
  bibkey: sugiyama-yoshinaga-2019-data
  bibtype: inproceedings
  booktitle: Proceedings of the Fourth Workshop on Discourse in Machine Translation
    (DiscoMT 2019)
  booktitle_html: Proceedings of the Fourth Workshop on Discourse in Machine Translation
    (DiscoMT 2019)
  doi: 10.18653/v1/D19-6504
  month: November
  page_first: '35'
  page_last: '44'
  pages: "35\u201344"
  paper_id: '4'
  parent_volume_id: D19-65
  pdf: https://www.aclweb.org/anthology/D19-6504.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6504.jpg
  title: Data augmentation using back-translation for context-aware neural machine
    translation
  title_html: Data augmentation using back-translation for context-aware neural machine
    translation
  url: https://www.aclweb.org/anthology/D19-6504
  year: '2019'
D19-6505:
  abstract: We present neural machine translation models for translating a sentence
    in a text by using a graph-based encoder which can consider coreference relations
    provided within the text explicitly. The graph-based encoder can dynamically encode
    the source text without attending to all tokens in the text. In experiments, our
    proposed models provide statistically significant improvement to the previous
    approach of at most 0.9 points in the BLEU score on the OpenSubtitle2018 English-to-Japanese
    data set. Experimental results also show that the graph-based encoder can handle
    a longer text well, compared with the previous approach.
  address: Hong Kong, China
  author:
  - first: Takumi
    full: Takumi Ohtani
    id: takumi-ohtani
    last: Ohtani
  - first: Hidetaka
    full: Hidetaka Kamigaito
    id: hidetaka-kamigaito
    last: Kamigaito
  - first: Masaaki
    full: Masaaki Nagata
    id: masaaki-nagata
    last: Nagata
  - first: Manabu
    full: Manabu Okumura
    id: manabu-okumura
    last: Okumura
  author_string: Takumi Ohtani, Hidetaka Kamigaito, Masaaki Nagata, Manabu Okumura
  bibkey: ohtani-etal-2019-context
  bibtype: inproceedings
  booktitle: Proceedings of the Fourth Workshop on Discourse in Machine Translation
    (DiscoMT 2019)
  booktitle_html: Proceedings of the Fourth Workshop on Discourse in Machine Translation
    (DiscoMT 2019)
  doi: 10.18653/v1/D19-6505
  month: November
  page_first: '45'
  page_last: '50'
  pages: "45\u201350"
  paper_id: '5'
  parent_volume_id: D19-65
  pdf: https://www.aclweb.org/anthology/D19-6505.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6505.jpg
  title: Context-aware Neural Machine Translation with Coreference Information
  title_html: Context-aware Neural Machine Translation with Coreference Information
  url: https://www.aclweb.org/anthology/D19-6505
  year: '2019'
D19-6506:
  abstract: In this paper, we investigate how different aspects of discourse context
    affect the performance of recent neural MT systems. We describe two popular datasets
    covering news and movie subtitles and we provide a thorough analysis of the distribution
    of various document-level features in their domains. Furthermore, we train a set
    of context-aware MT models on both datasets and propose a comparative evaluation
    scheme that contrasts coherent context with artificially scrambled documents and
    absent context, arguing that the impact of discourse-aware MT models will become
    visible in this way. Our results show that the models are indeed affected by the
    manipulation of the test data, providing a different view on document-level translation
    quality than absolute sentence-level scores.
  address: Hong Kong, China
  author:
  - first: Yves
    full: Yves Scherrer
    id: yves-scherrer
    last: Scherrer
  - first: "J\xF6rg"
    full: "J\xF6rg Tiedemann"
    id: jorg-tiedemann
    last: Tiedemann
  - first: Sharid
    full: "Sharid Lo\xE1iciga"
    id: sharid-loaiciga
    last: "Lo\xE1iciga"
  author_string: "Yves Scherrer, J\xF6rg Tiedemann, Sharid Lo\xE1iciga"
  bibkey: scherrer-etal-2019-analysing
  bibtype: inproceedings
  booktitle: Proceedings of the Fourth Workshop on Discourse in Machine Translation
    (DiscoMT 2019)
  booktitle_html: Proceedings of the Fourth Workshop on Discourse in Machine Translation
    (DiscoMT 2019)
  doi: 10.18653/v1/D19-6506
  month: November
  page_first: '51'
  page_last: '61'
  pages: "51\u201361"
  paper_id: '6'
  parent_volume_id: D19-65
  pdf: https://www.aclweb.org/anthology/D19-6506.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6506.jpg
  title: Analysing concatenation approaches to document-level NMT in two different
    domains
  title_html: Analysing concatenation approaches to document-level <span class="acl-fixed-case">NMT</span>
    in two different domains
  url: https://www.aclweb.org/anthology/D19-6506
  year: '2019'
D19-6600:
  address: Hong Kong, China
  author:
  - first: James
    full: James Thorne
    id: james-thorne
    last: Thorne
  - first: Andreas
    full: Andreas Vlachos
    id: andreas-vlachos
    last: Vlachos
  - first: Oana
    full: Oana Cocarascu
    id: oana-cocarascu
    last: Cocarascu
  - first: Christos
    full: Christos Christodoulopoulos
    id: christos-christodoulopoulos
    last: Christodoulopoulos
  - first: Arpit
    full: Arpit Mittal
    id: arpit-mittal
    last: Mittal
  author_string: James Thorne, Andreas Vlachos, Oana Cocarascu, Christos Christodoulopoulos,
    Arpit Mittal
  bibkey: emnlp-2019-fact
  bibtype: proceedings
  booktitle: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  month: November
  paper_id: '0'
  parent_volume_id: D19-66
  pdf: https://www.aclweb.org/anthology/D19-6600.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6600.jpg
  title: Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER)
  title_html: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  url: https://www.aclweb.org/anthology/D19-6600
  year: '2019'
D19-6601:
  abstract: "We present the results of the second Fact Extraction and VERification\
    \ (FEVER2.0) Shared Task. The task challenged participants to both build systems\
    \ to verify factoid claims using evidence retrieved from Wikipedia and to generate\
    \ adversarial attacks against other participant\u2019s systems. The shared task\
    \ had three phases: building, breaking and fixing. There were 8 systems in the\
    \ builder\u2019s round, three of which were new qualifying submissions for this\
    \ shared task, and 5 adversaries generated instances designed to induce classification\
    \ errors and one builder submitted a fixed system which had higher FEVER score\
    \ and resilience than their first submission. All but one newly submitted systems\
    \ attained FEVER scores higher than the best performing system from the first\
    \ shared task and under adversarial evaluation, all systems exhibited losses in\
    \ FEVER score. There was a great variety in adversarial attack types as well as\
    \ the techniques used to generate the attacks, In this paper, we present the results\
    \ of the shared task and a summary of the systems, highlighting commonalities\
    \ and innovations among participating systems."
  address: Hong Kong, China
  author:
  - first: James
    full: James Thorne
    id: james-thorne
    last: Thorne
  - first: Andreas
    full: Andreas Vlachos
    id: andreas-vlachos
    last: Vlachos
  - first: Oana
    full: Oana Cocarascu
    id: oana-cocarascu
    last: Cocarascu
  - first: Christos
    full: Christos Christodoulopoulos
    id: christos-christodoulopoulos
    last: Christodoulopoulos
  - first: Arpit
    full: Arpit Mittal
    id: arpit-mittal
    last: Mittal
  author_string: James Thorne, Andreas Vlachos, Oana Cocarascu, Christos Christodoulopoulos,
    Arpit Mittal
  bibkey: thorne-etal-2019-fever2
  bibtype: inproceedings
  booktitle: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  booktitle_html: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  doi: 10.18653/v1/D19-6601
  month: November
  page_first: '1'
  page_last: '6'
  pages: "1\u20136"
  paper_id: '1'
  parent_volume_id: D19-66
  pdf: https://www.aclweb.org/anthology/D19-6601.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6601.jpg
  title: The FEVER2.0 Shared Task
  title_html: The <span class="acl-fixed-case">FEVER</span>2.0 Shared Task
  url: https://www.aclweb.org/anthology/D19-6601
  year: '2019'
D19-6602:
  abstract: The goal of our paper is to compare psycholinguistic text features with
    fact checking approaches to distinguish lies from true statements. We examine
    both methods using data from a large ongoing study on deception and deception
    detection covering a mixture of factual and opinionated topics that polarize public
    opinion. We conclude that fact checking approaches based on Wikipedia are too
    limited for this task, as only a few percent of sentences from our study has enough
    evidence to become supported or refuted. Psycholinguistic features turn out to
    outperform both fact checking and human baselines, but the accuracy is not high.
    Overall, it appears that deception detection applicable to less-than-obvious topics
    is a difficult task and a problem to be solved.
  address: Hong Kong, China
  author:
  - first: Aleksander
    full: Aleksander Wawer
    id: aleksander-wawer
    last: Wawer
  - first: Grzegorz
    full: Grzegorz Wojdyga
    id: grzegorz-wojdyga
    last: Wojdyga
  - first: Justyna
    full: "Justyna Sarzy\u0144ska-Wawer"
    id: justyna-sarzynska-wawer
    last: "Sarzy\u0144ska-Wawer"
  author_string: "Aleksander Wawer, Grzegorz Wojdyga, Justyna Sarzy\u0144ska-Wawer"
  bibkey: wawer-etal-2019-fact
  bibtype: inproceedings
  booktitle: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  booktitle_html: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  doi: 10.18653/v1/D19-6602
  month: November
  page_first: '7'
  page_last: '12'
  pages: "7\u201312"
  paper_id: '2'
  parent_volume_id: D19-66
  pdf: https://www.aclweb.org/anthology/D19-6602.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6602.jpg
  title: 'Fact Checking or Psycholinguistics: How to Distinguish Fake and True Claims?'
  title_html: 'Fact Checking or Psycholinguistics: How to Distinguish Fake and True
    Claims?'
  url: https://www.aclweb.org/anthology/D19-6602
  year: '2019'
D19-6603:
  abstract: We present a multi-task learning model that leverages large amount of
    textual information from existing datasets to improve stance prediction. In particular,
    we utilize multiple NLP tasks under both unsupervised and supervised settings
    for the target stance prediction task. Our model obtains state-of-the-art performance
    on a public benchmark dataset, Fake News Challenge, outperforming current approaches
    by a wide margin.
  address: Hong Kong, China
  author:
  - first: Wei
    full: Wei Fang
    id: wei-fang
    last: Fang
  - first: Moin
    full: Moin Nadeem
    id: moin-nadeem
    last: Nadeem
  - first: Mitra
    full: Mitra Mohtarami
    id: mitra-mohtarami
    last: Mohtarami
  - first: James
    full: James Glass
    id: james-glass
    last: Glass
  author_string: Wei Fang, Moin Nadeem, Mitra Mohtarami, James Glass
  bibkey: fang-etal-2019-neural
  bibtype: inproceedings
  booktitle: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  booktitle_html: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  doi: 10.18653/v1/D19-6603
  month: November
  page_first: '13'
  page_last: '19'
  pages: "13\u201319"
  paper_id: '3'
  parent_volume_id: D19-66
  pdf: https://www.aclweb.org/anthology/D19-6603.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6603.jpg
  title: Neural Multi-Task Learning for Stance Prediction
  title_html: Neural Multi-Task Learning for Stance Prediction
  url: https://www.aclweb.org/anthology/D19-6603
  year: '2019'
D19-6604:
  abstract: We present our Generative Enhanced Model (GEM) that we used to create
    samples awarded the first prize on the FEVER 2.0 Breakers Task. GEM is the extended
    language model developed upon GPT-2 architecture. The addition of novel target
    vocabulary input to the already existing context input enabled controlled text
    generation. The training procedure resulted in creating a model that inherited
    the knowledge of pretrained GPT-2, and therefore was ready to generate natural-like
    English sentences in the task domain with some additional control. As a result,
    GEM generated malicious claims that mixed facts from various articles, so it became
    difficult to classify their truthfulness.
  address: Hong Kong, China
  attachment:
  - filename: D19-6604.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-6604.Attachment.pdf
  author:
  - first: Piotr
    full: Piotr Niewinski
    id: piotr-niewinski
    last: Niewinski
  - first: Maria
    full: Maria Pszona
    id: maria-pszona
    last: Pszona
  - first: Maria
    full: Maria Janicka
    id: maria-janicka
    last: Janicka
  author_string: Piotr Niewinski, Maria Pszona, Maria Janicka
  bibkey: niewinski-etal-2019-gem
  bibtype: inproceedings
  booktitle: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  booktitle_html: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  doi: 10.18653/v1/D19-6604
  month: November
  page_first: '20'
  page_last: '26'
  pages: "20\u201326"
  paper_id: '4'
  parent_volume_id: D19-66
  pdf: https://www.aclweb.org/anthology/D19-6604.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6604.jpg
  title: 'GEM: Generative Enhanced Model for adversarial attacks'
  title_html: '<span class="acl-fixed-case">GEM</span>: Generative Enhanced Model
    for adversarial attacks'
  url: https://www.aclweb.org/anthology/D19-6604
  year: '2019'
D19-6605:
  abstract: 'In this paper, we propose a new approach to learn multimodal multilingual
    embeddings for matching images and their relevant captions in two languages. We
    combine two existing objective functions to make images and captions close in
    a joint embedding space while adapting the alignment of word embeddings between
    existing languages in our model. We show that our approach enables better generalization,
    achieving state-of-the-art performance in text-to-image and image-to-text retrieval
    task, and caption-caption similarity task. Two multimodal multilingual datasets
    are used for evaluation: Multi30k with German and English captions and Microsoft-COCO
    with English and Japanese captions.'
  address: Hong Kong, China
  attachment:
  - filename: D19-6605.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-6605.Attachment.zip
  author:
  - first: Alireza
    full: Alireza Mohammadshahi
    id: alireza-mohammadshahi
    last: Mohammadshahi
  - first: "R\xE9mi"
    full: "R\xE9mi Lebret"
    id: remi-lebret
    last: Lebret
  - first: Karl
    full: Karl Aberer
    id: karl-aberer
    last: Aberer
  author_string: "Alireza Mohammadshahi, R\xE9mi Lebret, Karl Aberer"
  bibkey: mohammadshahi-etal-2019-aligning-multilingual
  bibtype: inproceedings
  booktitle: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  booktitle_html: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  doi: 10.18653/v1/D19-6605
  month: November
  page_first: '27'
  page_last: '33'
  pages: "27\u201333"
  paper_id: '5'
  parent_volume_id: D19-66
  pdf: https://www.aclweb.org/anthology/D19-6605.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6605.jpg
  title: Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task
  title_html: Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task
  url: https://www.aclweb.org/anthology/D19-6605
  year: '2019'
D19-6606:
  abstract: "The recent demonstration of the power of huge language models such as\
    \ GPT-2 to memorise the answers to factoid questions raises questions about the\
    \ extent to which knowledge is being embedded directly within these large models.\
    \ This short paper describes an architecture through which much smaller models\
    \ can also answer such questions - by making use of \u2018raw\u2019 external knowledge.\
    \ The contribution of this work is that the methods presented here rely on unsupervised\
    \ learning techniques, complementing the unsupervised training of the Language\
    \ Model. The goal of this line of research is to be able to add knowledge explicitly,\
    \ without extensive training."
  address: Hong Kong, China
  author:
  - first: Martin
    full: Martin Andrews
    id: martin-andrews
    last: Andrews
  - first: Sam
    full: Sam Witteveen
    id: sam-witteveen
    last: Witteveen
  author_string: Martin Andrews, Sam Witteveen
  bibkey: andrews-witteveen-2019-unsupervised
  bibtype: inproceedings
  booktitle: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  booktitle_html: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  doi: 10.18653/v1/D19-6606
  month: November
  page_first: '34'
  page_last: '38'
  pages: "34\u201338"
  paper_id: '6'
  parent_volume_id: D19-66
  pdf: https://www.aclweb.org/anthology/D19-6606.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6606.jpg
  title: Unsupervised Natural Question Answering with a Small Model
  title_html: Unsupervised Natural Question Answering with a Small Model
  url: https://www.aclweb.org/anthology/D19-6606
  year: '2019'
D19-6607:
  abstract: "We present a scalable, open-source platform that \u201Cdistills\u201D\
    \ a potentially large text collection into a knowledge graph. Our platform takes\
    \ documents stored in Apache Solr and scales out the Stanford CoreNLP toolkit\
    \ via Apache Spark integration to extract mentions and relations that are then\
    \ ingested into the Neo4j graph database. The raw knowledge graph is then enriched\
    \ with facts extracted from an external knowledge graph. The complete product\
    \ can be manipulated by various applications using Neo4j\u2019s native Cypher\
    \ query language: We present a subgraph-matching approach to align extracted relations\
    \ with external facts and show that fact verification, locating textual support\
    \ for asserted facts, detecting inconsistent and missing facts, and extracting\
    \ distantly-supervised training data can all be performed within the same framework."
  address: Hong Kong, China
  author:
  - first: Ryan
    full: Ryan Clancy
    id: ryan-clancy
    last: Clancy
  - first: Ihab F.
    full: Ihab F. Ilyas
    id: ihab-f-ilyas
    last: Ilyas
  - first: Jimmy
    full: Jimmy Lin
    id: jimmy-lin
    last: Lin
  author_string: Ryan Clancy, Ihab F. Ilyas, Jimmy Lin
  bibkey: clancy-etal-2019-scalable
  bibtype: inproceedings
  booktitle: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  booktitle_html: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  doi: 10.18653/v1/D19-6607
  month: November
  page_first: '39'
  page_last: '46'
  pages: "39\u201346"
  paper_id: '7'
  parent_volume_id: D19-66
  pdf: https://www.aclweb.org/anthology/D19-6607.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6607.jpg
  title: Scalable Knowledge Graph Construction from Text Collections
  title_html: Scalable Knowledge Graph Construction from Text Collections
  url: https://www.aclweb.org/anthology/D19-6607
  year: '2019'
D19-6608:
  abstract: Many previous studies on relation extrac-tion have been focused on finding
    only one relation between two entities in a single sentence. However, we can easily
    find the fact that multiple entities exist in a single sentence and the entities
    form multiple relations. To resolve this prob-lem, we propose a relation extraction
    model based on a dual pointer network with a multi-head attention mechanism. The
    proposed model finds n-to-1 subject-object relations by using a forward de-coder
    called an object decoder. Then, it finds 1-to-n subject-object relations by using
    a backward decoder called a sub-ject decoder. In the experiments with the ACE-05
    dataset and the NYT dataset, the proposed model achieved the state-of-the-art
    performances (F1-score of 80.5% in the ACE-05 dataset, F1-score of 78.3% in the
    NYT dataset)
  address: Hong Kong, China
  author:
  - first: Seong Sik
    full: Seong Sik Park
    id: seong-sik-park
    last: Park
  - first: Harksoo
    full: Harksoo Kim
    id: harksoo-kim
    last: Kim
  author_string: Seong Sik Park, Harksoo Kim
  bibkey: park-kim-2019-relation
  bibtype: inproceedings
  booktitle: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  booktitle_html: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  doi: 10.18653/v1/D19-6608
  month: November
  page_first: '47'
  page_last: '51'
  pages: "47\u201351"
  paper_id: '8'
  parent_volume_id: D19-66
  pdf: https://www.aclweb.org/anthology/D19-6608.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6608.jpg
  title: Relation Extraction among Multiple Entities Using a Dual Pointer Network
    with a Multi-Head Attention Mechanism
  title_html: Relation Extraction among Multiple Entities Using a Dual Pointer Network
    with a Multi-Head Attention Mechanism
  url: https://www.aclweb.org/anthology/D19-6608
  year: '2019'
D19-6609:
  abstract: "Recent Deep Learning (DL) models have succeeded in achieving human-level\
    \ accuracy on various natural language tasks such as question-answering, natural\
    \ language inference (NLI), and textual entailment. These tasks not only require\
    \ the contextual knowledge but also the reasoning abilities to be solved efficiently.\
    \ In this paper, we propose an unsupervised question-answering based approach\
    \ for a similar task, fact-checking. We transform the FEVER dataset into a Cloze-task\
    \ by masking named entities provided in the claims. To predict the answer token,\
    \ we utilize pre-trained Bidirectional Encoder Representations from Transformers\
    \ (BERT). The classifier computes label based on the correctly answered questions\
    \ and a threshold. Currently, the classifier is able to classify the claims as\
    \ \u201CSUPPORTS\u201D and \u201CMANUAL_REVIEW\u201D. This approach achieves a\
    \ label accuracy of 80.2% on the development set and 80.25% on the test set of\
    \ the transformed dataset."
  address: Hong Kong, China
  author:
  - first: Mayank
    full: Mayank Jobanputra
    id: mayank-jobanputra
    last: Jobanputra
  author_string: Mayank Jobanputra
  bibkey: jobanputra-2019-unsupervised
  bibtype: inproceedings
  booktitle: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  booktitle_html: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  doi: 10.18653/v1/D19-6609
  month: November
  page_first: '52'
  page_last: '56'
  pages: "52\u201356"
  paper_id: '9'
  parent_volume_id: D19-66
  pdf: https://www.aclweb.org/anthology/D19-6609.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6609.jpg
  title: Unsupervised Question Answering for Fact-Checking
  title_html: Unsupervised Question Answering for Fact-Checking
  url: https://www.aclweb.org/anthology/D19-6609
  year: '2019'
D19-6610:
  abstract: Recognizing the implicit link between a claim and a piece of evidence
    (i.e. warrant) is the key to improving the performance of evidence detection.
    In this work, we explore the effectiveness of automatically extracted warrants
    for evidence detection. Given a claim and candidate evidence, our proposed method
    extracts multiple warrants via similarity search from an existing, structured
    corpus of arguments. We then attentively aggregate the extracted warrants, considering
    the consistency between the given argument and the acquired warrants. Although
    a qualitative analysis on the warrants shows that the extraction method needs
    to be improved, our results indicate that our method can still improve the performance
    of evidence detection.
  address: Hong Kong, China
  author:
  - first: Keshav
    full: Keshav Singh
    id: keshav-singh
    last: Singh
  - first: Paul
    full: Paul Reisert
    id: paul-reisert
    last: Reisert
  - first: Naoya
    full: Naoya Inoue
    id: naoya-inoue
    last: Inoue
  - first: Pride
    full: Pride Kavumba
    id: pride-kavumba
    last: Kavumba
  - first: Kentaro
    full: Kentaro Inui
    id: kentaro-inui
    last: Inui
  author_string: Keshav Singh, Paul Reisert, Naoya Inoue, Pride Kavumba, Kentaro Inui
  bibkey: singh-etal-2019-improving
  bibtype: inproceedings
  booktitle: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  booktitle_html: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  doi: 10.18653/v1/D19-6610
  month: November
  page_first: '57'
  page_last: '62'
  pages: "57\u201362"
  paper_id: '10'
  parent_volume_id: D19-66
  pdf: https://www.aclweb.org/anthology/D19-6610.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6610.jpg
  title: Improving Evidence Detection by Leveraging Warrants
  title_html: Improving Evidence Detection by Leveraging Warrants
  url: https://www.aclweb.org/anthology/D19-6610
  year: '2019'
D19-6611:
  abstract: One of the important tasks in opinion mining is to extract aspects of
    the opinion target. Aspects are features or characteristics of the opinion target
    that are being reviewed, which can be categorised into explicit and implicit aspects.
    Extracting aspects from opinions is essential in order to ensure accurate information
    about certain attributes of an opinion target is retrieved. For instance, a professional
    camera receives a positive feedback in terms of its functionalities in a review,
    but its overly high price receives negative feedback. Most of the existing solutions
    focus on explicit aspects. However, sentences in reviews normally do not state
    the aspects explicitly. In this research, two hybrid models are proposed to identify
    and extract both explicit and implicit aspects, namely TDM-DC and TDM-TED. The
    proposed models combine topic modelling and dictionary-based approach. The models
    are unsupervised as they do not require any labelled dataset. The experimental
    results show that TDM-DC achieves F1-measure of 58.70%, where it outperforms both
    the baseline topic model and dictionary-based approach. In comparison to other
    existing unsupervised techniques, the proposed models are able to achieve higher
    F1-measure by approximately 3%. Although the supervised techniques perform slightly
    better, the proposed models are domain-independent, and hence more versatile.
  address: Hong Kong, China
  author:
  - first: Wai-Howe
    full: Wai-Howe Khong
    id: wai-howe-khong
    last: Khong
  - first: Lay-Ki
    full: Lay-Ki Soon
    id: lay-ki-soon
    last: Soon
  - first: Hui-Ngo
    full: Hui-Ngo Goh
    id: hui-ngo-goh
    last: Goh
  author_string: Wai-Howe Khong, Lay-Ki Soon, Hui-Ngo Goh
  bibkey: khong-etal-2019-hybrid
  bibtype: inproceedings
  booktitle: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  booktitle_html: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  doi: 10.18653/v1/D19-6611
  month: November
  page_first: '63'
  page_last: '68'
  pages: "63\u201368"
  paper_id: '11'
  parent_volume_id: D19-66
  pdf: https://www.aclweb.org/anthology/D19-6611.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6611.jpg
  title: Hybrid Models for Aspects Extraction without Labelled Dataset
  title_html: Hybrid Models for Aspects Extraction without Labelled Dataset
  url: https://www.aclweb.org/anthology/D19-6611
  year: '2019'
D19-6612:
  abstract: Triggered by Internet development, a large amount of information is published
    in online sources. However, it is a well-known fact that publications are inundated
    with inaccurate data. That is why fact-checking has become a significant topic
    in the last 5 years. It is widely accepted that factual data verification is a
    challenge even for the experts. This paper presents a domain-independent fact
    checking system. It can solve the fact verification problem entirely or at the
    individual stages. The proposed model combines various advanced methods of text
    data analysis, such as BERT and Infersent. The theoretical and empirical study
    of the system features is carried out. Based on FEVER and Fact Checking Challenge
    test-collections, experimental results demonstrate that our model can achieve
    the score on a par with state-of-the-art models designed by the specificity of
    particular datasets.
  address: Hong Kong, China
  author:
  - first: Anton
    full: Anton Chernyavskiy
    id: anton-chernyavskiy
    last: Chernyavskiy
  - first: Dmitry
    full: Dmitry Ilvovsky
    id: dmitry-ilvovsky
    last: Ilvovsky
  author_string: Anton Chernyavskiy, Dmitry Ilvovsky
  bibkey: chernyavskiy-ilvovsky-2019-extract
  bibtype: inproceedings
  booktitle: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  booktitle_html: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  doi: 10.18653/v1/D19-6612
  month: November
  page_first: '69'
  page_last: '78'
  pages: "69\u201378"
  paper_id: '12'
  parent_volume_id: D19-66
  pdf: https://www.aclweb.org/anthology/D19-6612.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6612.jpg
  title: 'Extract and Aggregate: A Novel Domain-Independent Approach to Factual Data
    Verification'
  title_html: 'Extract and Aggregate: A Novel Domain-Independent Approach to Factual
    Data Verification'
  url: https://www.aclweb.org/anthology/D19-6612
  year: '2019'
D19-6613:
  abstract: Finding evidence is of vital importance in research as well as fact checking
    and an evidence detection method would be useful in speeding up this process.
    However, when addressing a new topic there is no training data and there are two
    approaches to get started. One could use large amounts of out-of-domain data to
    train a state-of-the-art method, or to use the small data that a person creates
    while working on the topic. In this paper, we address this problem in two steps.
    First, by simulating users who read source documents and label sentences they
    can use as evidence, thereby creating small amounts of training data for an interactively
    trained evidence detection model; and second, by comparing such an interactively
    trained model against a pre-trained model that has been trained on large out-of-domain
    data. We found that an interactively trained model not only often out-performs
    a state-of-the-art model but also requires significantly lower amounts of computational
    resources. Therefore, especially when computational resources are scarce, e.g.
    no GPU available, training a smaller model on the fly is preferable to training
    a well generalising but resource hungry out-of-domain model.
  address: Hong Kong, China
  attachment:
  - filename: D19-6613.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D19-6613.Attachment.pdf
  author:
  - first: Chris
    full: Chris Stahlhut
    id: chris-stahlhut
    last: Stahlhut
  author_string: Chris Stahlhut
  bibkey: stahlhut-2019-interactive
  bibtype: inproceedings
  booktitle: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  booktitle_html: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  doi: 10.18653/v1/D19-6613
  month: November
  page_first: '79'
  page_last: '89'
  pages: "79\u201389"
  paper_id: '13'
  parent_volume_id: D19-66
  pdf: https://www.aclweb.org/anthology/D19-6613.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6613.jpg
  title: 'Interactive Evidence Detection: train state-of-the-art model out-of-domain
    or simple model interactively?'
  title_html: 'Interactive Evidence Detection: train state-of-the-art model out-of-domain
    or simple model interactively?'
  url: https://www.aclweb.org/anthology/D19-6613
  year: '2019'
D19-6614:
  abstract: Defined as the intentional or unintentionalspread of false information
    (K et al., 2019)through context and/or content manipulation,fake news has become
    one of the most seriousproblems associated with online information(Waldrop, 2017).
    Consequently, it comes asno surprise that Fake News Detection hasbecome one of
    the major foci of variousfields of machine learning and while machinelearning
    models have allowed individualsand companies to automate decision-basedprocesses
    that were once thought to be onlydoable by humans, it is no secret that thereal-life
    applications of such models are notviable without the existence of an adequatetraining
    dataset. In this paper we describethe Veritas Annotator, a web application formanually
    identifying the origin of a rumour.These rumours, often referred as claims,were
    previously checked for validity byFact-Checking Agencies.
  address: Hong Kong, China
  author:
  - first: Lucas
    full: Lucas Azevedo
    id: lucas-azevedo
    last: Azevedo
  - first: Mohamed
    full: Mohamed Moustafa
    id: mohamed-moustafa
    last: Moustafa
  author_string: Lucas Azevedo, Mohamed Moustafa
  bibkey: azevedo-moustafa-2019-veritas
  bibtype: inproceedings
  booktitle: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  booktitle_html: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  doi: 10.18653/v1/D19-6614
  month: November
  page_first: '90'
  page_last: '98'
  pages: "90\u201398"
  paper_id: '14'
  parent_volume_id: D19-66
  pdf: https://www.aclweb.org/anthology/D19-6614.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6614.jpg
  title: 'Veritas Annotator: Discovering the Origin of a Rumour'
  title_html: 'Veritas Annotator: Discovering the Origin of a Rumour'
  url: https://www.aclweb.org/anthology/D19-6614
  year: '2019'
D19-6615:
  abstract: "We describe our submission for the Breaker phase of the second Fact Extraction\
    \ and VERification (FEVER) Shared Task. Our adversarial data can be explained\
    \ by two perspectives. First, we aimed at testing model\u2019s ability to retrieve\
    \ evidence, when appropriate query terms could not be easily generated from the\
    \ claim. Second, we test model\u2019s ability to precisely understand the implications\
    \ of the texts, which we expect to be rare in FEVER 1.0 dataset. Overall, we suggested\
    \ six types of adversarial attacks. The evaluation on the submitted systems showed\
    \ that the systems were only able get both the evidence and label correct in 20%\
    \ of the data. We also demonstrate our adversarial run analysis in the data development\
    \ process."
  address: Hong Kong, China
  author:
  - first: Youngwoo
    full: Youngwoo Kim
    id: youngwoo-kim
    last: Kim
  - first: James
    full: James Allan
    id: james-allan
    last: Allan
  author_string: Youngwoo Kim, James Allan
  bibkey: kim-allan-2019-fever
  bibtype: inproceedings
  booktitle: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  booktitle_html: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  doi: 10.18653/v1/D19-6615
  month: November
  page_first: '99'
  page_last: '104'
  pages: "99\u2013104"
  paper_id: '15'
  parent_volume_id: D19-66
  pdf: https://www.aclweb.org/anthology/D19-6615.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6615.jpg
  title: "FEVER Breaker\u2019s Run of Team NbAuzDrLqg"
  title_html: "<span class=\"acl-fixed-case\">FEVER</span> Breaker\u2019s Run of Team\
    \ <span class=\"acl-fixed-case\">N</span>b<span class=\"acl-fixed-case\">A</span>uz<span\
    \ class=\"acl-fixed-case\">D</span>r<span class=\"acl-fixed-case\">L</span>qg"
  url: https://www.aclweb.org/anthology/D19-6615
  year: '2019'
D19-6616:
  abstract: This paper contains our system description for the second Fact Extraction
    and VERification (FEVER) challenge. We propose a two-staged sentence selection
    strategy to account for examples in the dataset where evidence is not only conditioned
    on the claim, but also on previously retrieved evidence. We use a publicly available
    document retrieval module and have fine-tuned BERT checkpoints for sentence se-
    lection and as the entailment classifier. We report a FEVER score of 68.46% on
    the blind testset.
  address: Hong Kong, China
  author:
  - first: Dominik
    full: Dominik Stammbach
    id: dominik-stammbach
    last: Stammbach
  - first: Guenter
    full: Guenter Neumann
    id: gunter-neumann
    last: Neumann
  author_string: Dominik Stammbach, Guenter Neumann
  bibkey: stammbach-neumann-2019-team
  bibtype: inproceedings
  booktitle: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  booktitle_html: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  doi: 10.18653/v1/D19-6616
  month: November
  page_first: '105'
  page_last: '109'
  pages: "105\u2013109"
  paper_id: '16'
  parent_volume_id: D19-66
  pdf: https://www.aclweb.org/anthology/D19-6616.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6616.jpg
  title: 'Team DOMLIN: Exploiting Evidence Enhancement for the FEVER Shared Task'
  title_html: 'Team <span class="acl-fixed-case">DOMLIN</span>: Exploiting Evidence
    Enhancement for the <span class="acl-fixed-case">FEVER</span> Shared Task'
  url: https://www.aclweb.org/anthology/D19-6616
  year: '2019'
D19-6617:
  abstract: Fever Shared 2.0 Task is a challenge meant for developing automated fact
    checking systems. Our approach for the Fever 2.0 is based on a previous proposal
    developed by Team Athene UKP TU Darmstadt. Our proposal modifies the sentence
    retrieval phase, using statement extraction and representation in the form of
    triplets (subject, object, action). Triplets are extracted from the claim and
    compare to triplets extracted from Wikipedia articles using semantic similarity.
    Our results are satisfactory but there is room for improvement.
  address: Hong Kong, China
  author:
  - first: "Aim\xE9e"
    full: "Aim\xE9e Alonso-Reina"
    id: aimee-alonso-reina
    last: Alonso-Reina
  - first: Robiert
    full: "Robiert Sep\xFAlveda-Torres"
    id: robiert-sepulveda-torres
    last: "Sep\xFAlveda-Torres"
  - first: Estela
    full: Estela Saquete
    id: estela-saquete
    last: Saquete
  - first: Manuel
    full: Manuel Palomar
    id: manuel-palomar
    last: Palomar
  author_string: "Aim\xE9e Alonso-Reina, Robiert Sep\xFAlveda-Torres, Estela Saquete,\
    \ Manuel Palomar"
  bibkey: alonso-reina-etal-2019-team
  bibtype: inproceedings
  booktitle: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  booktitle_html: Proceedings of the Second Workshop on Fact Extraction and VERification
    (FEVER)
  doi: 10.18653/v1/D19-6617
  month: November
  page_first: '110'
  page_last: '114'
  pages: "110\u2013114"
  paper_id: '17'
  parent_volume_id: D19-66
  pdf: https://www.aclweb.org/anthology/D19-6617.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D19-6617.jpg
  title: Team GPLSI. Approach for automated fact checking
  title_html: Team <span class="acl-fixed-case">GPLSI</span>. Approach for automated
    fact checking
  url: https://www.aclweb.org/anthology/D19-6617
  year: '2019'
