C18-1000:
  address: Santa Fe, New Mexico, USA
  author:
  - first: Emily M.
    full: Emily M. Bender
    id: emily-m-bender
    last: Bender
  - first: Leon
    full: Leon Derczynski
    id: leon-derczynski
    last: Derczynski
  - first: Pierre
    full: Pierre Isabelle
    id: pierre-isabelle
    last: Isabelle
  author_string: Emily M. Bender, Leon Derczynski, Pierre Isabelle
  bibkey: coling-2018-international
  bibtype: proceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  month: August
  paper_id: '0'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1000.jpg
  title: Proceedings of the 27th International Conference on Computational Linguistics
  title_html: Proceedings of the 27th International Conference on Computational Linguistics
  url: https://www.aclweb.org/anthology/C18-1000
  year: '2018'
C18-1001:
  abstract: Animacy is a necessary property for a referent to be an agent, and thus
    animacy detection is useful for a variety of natural language processing tasks,
    including word sense disambiguation, co-reference resolution, semantic role labeling,
    and others. Prior work treated animacy as a word-level property, and has developed
    statistical classifiers to classify words as either animate or inanimate. We discuss
    why this approach to the problem is ill-posed, and present a new approach based
    on classifying the animacy of co-reference chains. We show that simple voting
    approaches to inferring the animacy of a chain from its constituent words perform
    relatively poorly, and then present a hybrid system merging supervised machine
    learning (ML) and a small number of hand-built rules to compute the animacy of
    referring expressions and co-reference chains. This method achieves state of the
    art performance. The supervised ML component leverages features such as word embeddings
    over referring expressions, parts of speech, and grammatical and semantic roles.
    The rules take into consideration parts of speech and the hypernymy structure
    encoded in WordNet. The system achieves an F1 of 0.88 for classifying the animacy
    of referring expressions, which is comparable to state of the art results for
    classifying the animacy of words, and achieves an F1 of 0.75 for classifying the
    animacy of coreference chains themselves. We release our training and test dataset,
    which includes 142 texts (all narratives) comprising 156,154 words, 34,698 referring
    expressions, and 10,941 co-reference chains. We test the method on a subset of
    the OntoNotes dataset, showing using manual sampling that animacy classification
    is 90% +/- 2% accurate for coreference chains, and 92% +/- 1% for referring expressions.
    The data also contains 46 folktales, which present an interesting challenge because
    they often involve characters who are members of traditionally inanimate classes
    (e.g., stoves that walk, trees that talk). We show that our system is able to
    detect the animacy of these unusual referents with an F1 of 0.95.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Labiba
    full: Labiba Jahan
    id: labiba-jahan
    last: Jahan
  - first: Geeticka
    full: Geeticka Chauhan
    id: geeticka-chauhan
    last: Chauhan
  - first: Mark
    full: Mark Finlayson
    id: mark-finlayson
    last: Finlayson
  author_string: Labiba Jahan, Geeticka Chauhan, Mark Finlayson
  bibkey: jahan-etal-2018-new
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1'
  page_last: '12'
  pages: "1\u201312"
  paper_id: '1'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1001.jpg
  title: A New Approach to Animacy Detection
  title_html: A New Approach to <span class="acl-fixed-case">A</span>nimacy Detection
  url: https://www.aclweb.org/anthology/C18-1001
  year: '2018'
C18-1002:
  abstract: 'Recent neural network methods for zero pronoun resolution explore multiple
    models for generating representation vectors for zero pronouns and their candidate
    antecedents. Typically, contextual information is utilized to encode the zero
    pronouns since they are simply gaps that contain no actual content. To better
    utilize contexts of the zero pronouns, we here introduce the self-attention mechanism
    for encoding zero pronouns. With the help of the multiple hops of attention, our
    model is able to focus on some informative parts of the associated texts and therefore
    produces an efficient way of encoding the zero pronouns. In addition, an attention-based
    recurrent neural network is proposed for encoding candidate antecedents by their
    contents. Experiment results are encouraging: our proposed attention-based model
    gains the best performance on the Chinese portion of the OntoNotes corpus, substantially
    surpasses existing Chinese zero pronoun resolution baseline systems.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Qingyu
    full: Qingyu Yin
    id: qingyu-yin
    last: Yin
  - first: Yu
    full: Yu Zhang
    id: yu-zhang
    last: Zhang
  - first: Weinan
    full: Weinan Zhang
    id: weinan-zhang
    last: Zhang
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  author_string: Qingyu Yin, Yu Zhang, Weinan Zhang, Ting Liu, William Yang Wang
  bibkey: yin-etal-2018-zero
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '13'
  page_last: '23'
  pages: "13\u201323"
  paper_id: '2'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1002.jpg
  title: Zero Pronoun Resolution with Attention-based Neural Network
  title_html: Zero Pronoun Resolution with Attention-based Neural Network
  url: https://www.aclweb.org/anthology/C18-1002
  year: '2018'
C18-1003:
  abstract: This paper analyzes arguably the most challenging yet under-explored aspect
    of resolution tasks such as coreference resolution and entity linking, that is
    the resolution of plural mentions. Unlike singular mentions each of which represents
    one entity, plural mentions stand for multiple entities. To tackle this aspect,
    we take the character identification corpus from the SemEval 2018 shared task
    that consists of entity annotation for singular mentions, and expand it by adding
    annotation for plural mentions. We then introduce a novel coreference resolution
    algorithm that selectively creates clusters to handle both singular and plural
    mentions, and also a deep learning-based entity linking model that jointly handles
    both types of mentions through multi-task learning. Adjusted evaluation metrics
    are proposed for these tasks as well to handle the uniqueness of plural mentions.
    Our experiments show that the new coreference resolution and entity linking models
    significantly outperform traditional models designed only for singular mentions.
    To the best of our knowledge, this is the first time that plural mentions are
    thoroughly analyzed for these two resolution tasks.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Ethan
    full: Ethan Zhou
    id: ethan-zhou
    last: Zhou
  - first: Jinho D.
    full: Jinho D. Choi
    id: jinho-d-choi
    last: Choi
  author_string: Ethan Zhou, Jinho D. Choi
  bibkey: zhou-choi-2018-exist
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '24'
  page_last: '34'
  pages: "24\u201334"
  paper_id: '3'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1003.jpg
  title: They Exist! Introducing Plural Mentions to Coreference Resolution and Entity
    Linking
  title_html: They Exist! Introducing Plural Mentions to Coreference Resolution and
    Entity Linking
  url: https://www.aclweb.org/anthology/C18-1003
  year: '2018'
C18-1004:
  abstract: We propose a triad-based neural network system that generates affinity
    scores between entity mentions for coreference resolution. The system simultaneously
    accepts three mentions as input, taking mutual dependency and logical constraints
    of all three mentions into account, and thus makes more accurate predictions than
    the traditional pairwise approach. Depending on system choices, the affinity scores
    can be further used in clustering or mention ranking. Our experiments show that
    a standard hierarchical clustering using the scores produces state-of-art results
    with MUC and B 3 metrics on the English portion of CoNLL 2012 Shared Task. The
    model does not rely on many handcrafted features and is easy to train and use.
    The triads can also be easily extended to polyads of higher orders. To our knowledge,
    this is the first neural network system to model mutual dependency of more than
    two members at mention level.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yuanliang
    full: Yuanliang Meng
    id: yuanliang-meng
    last: Meng
  - first: Anna
    full: Anna Rumshisky
    id: anna-rumshisky
    last: Rumshisky
  author_string: Yuanliang Meng, Anna Rumshisky
  bibkey: meng-rumshisky-2018-triad
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '35'
  page_last: '43'
  pages: "35\u201343"
  paper_id: '4'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1004.jpg
  title: Triad-based Neural Network for Coreference Resolution
  title_html: Triad-based Neural Network for Coreference Resolution
  url: https://www.aclweb.org/anthology/C18-1004
  year: '2018'
C18-1005:
  abstract: This paper describes an unsupervised model for morphological segmentation
    that exploits the notion of paradigms, which are sets of morphological categories
    (e.g., suffixes) that can be applied to a homogeneous set of words (e.g., nouns
    or verbs). Our algorithm identifies statistically reliable paradigms from the
    morphological segmentation result of a probabilistic model, and chooses reliable
    suffixes from them. The new suffixes can be fed back iteratively to improve the
    accuracy of the probabilistic model. Finally, the unreliable paradigms are subjected
    to pruning to eliminate unreliable morphological relations between words. The
    paradigm-based algorithm significantly improves segmentation accuracy. Our method
    achieves start-of-the-art results on experiments using the Morpho-Challenge data,
    including English, Turkish, and Finnish.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Hongzhi
    full: Hongzhi Xu
    id: hongzhi-xu
    last: Xu
  - first: Mitchell
    full: Mitchell Marcus
    id: mitch-marcus
    last: Marcus
  - first: Charles
    full: Charles Yang
    id: charles-yang
    last: Yang
  - first: Lyle
    full: Lyle Ungar
    id: lyle-ungar
    last: Ungar
  author_string: Hongzhi Xu, Mitchell Marcus, Charles Yang, Lyle Ungar
  bibkey: xu-etal-2018-unsupervised
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '44'
  page_last: '54'
  pages: "44\u201354"
  paper_id: '5'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1005.jpg
  title: Unsupervised Morphology Learning with Statistical Paradigms
  title_html: Unsupervised Morphology Learning with Statistical Paradigms
  url: https://www.aclweb.org/anthology/C18-1005
  year: '2018'
C18-1006:
  abstract: Indigenous languages of the American continent are highly diverse. However,
    they have received little attention from the technological perspective. In this
    paper, we review the research, the digital resources and the available NLP systems
    that focus on these languages. We present the main challenges and research questions
    that arise when distant languages and low-resource scenarios are faced. We would
    like to encourage NLP research in linguistically rich and diverse areas like the
    Americas.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Manuel
    full: Manuel Mager
    id: manuel-mager
    last: Mager
  - first: Ximena
    full: Ximena Gutierrez-Vasques
    id: ximena-gutierrez-vasques
    last: Gutierrez-Vasques
  - first: Gerardo
    full: Gerardo Sierra
    id: gerardo-sierra
    last: Sierra
  - first: Ivan
    full: Ivan Meza-Ruiz
    id: ivan-meza-ruiz
    last: Meza-Ruiz
  author_string: Manuel Mager, Ximena Gutierrez-Vasques, Gerardo Sierra, Ivan Meza-Ruiz
  bibkey: mager-etal-2018-challenges
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '55'
  page_last: '69'
  pages: "55\u201369"
  paper_id: '6'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1006.jpg
  title: Challenges of language technologies for the indigenous languages of the Americas
  title_html: Challenges of language technologies for the indigenous languages of
    the <span class="acl-fixed-case">A</span>mericas
  url: https://www.aclweb.org/anthology/C18-1006
  year: '2018'
C18-1007:
  abstract: "The use of machine learning for NLP generally requires resources for\
    \ training. Tasks performed in a low-resource language usually rely on labeled\
    \ data in another, typically resource-rich, language. However, there might not\
    \ be enough labeled data even in a resource-rich language such as English. In\
    \ such cases, one approach is to use a hand-crafted approach that utilizes only\
    \ a small bilingual dictionary with minimal manual verification to create distantly\
    \ supervised data. Another is to explore typical machine learning techniques,\
    \ for example adversarial training of bilingual word representations. We find\
    \ that in event-type detection task\u2014the task to classify [parts of] documents\
    \ into a fixed set of labels\u2014they give about the same performance. We explore\
    \ ways in which the two methods can be complementary and also see how to best\
    \ utilize a limited budget for manual annotation to maximize performance gain."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Aldrian Obaja
    full: Aldrian Obaja Muis
    id: aldrian-obaja-muis
    last: Muis
  - first: Naoki
    full: Naoki Otani
    id: naoki-otani
    last: Otani
  - first: Nidhi
    full: Nidhi Vyas
    id: nidhi-vyas
    last: Vyas
  - first: Ruochen
    full: Ruochen Xu
    id: ruochen-xu
    last: Xu
  - first: Yiming
    full: Yiming Yang
    id: yiming-yang
    last: Yang
  - first: Teruko
    full: Teruko Mitamura
    id: teruko-mitamura
    last: Mitamura
  - first: Eduard
    full: Eduard Hovy
    id: eduard-hovy
    last: Hovy
  author_string: Aldrian Obaja Muis, Naoki Otani, Nidhi Vyas, Ruochen Xu, Yiming Yang,
    Teruko Mitamura, Eduard Hovy
  bibkey: muis-etal-2018-low
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '70'
  page_last: '82'
  pages: "70\u201382"
  paper_id: '7'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1007.pdf
  publisher: Association for Computational Linguistics
  revision:
  - id: '1'
    url: https://www.aclweb.org/anthology/C18-1007v1.pdf
    value: C18-1007v1
  - explanation: No description of the changes were recorded.
    id: '2'
    url: https://www.aclweb.org/anthology/C18-1007v2.pdf
    value: C18-1007v2
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1007.jpg
  title: Low-resource Cross-lingual Event Type Detection via Distant Supervision with
    Minimal Effort
  title_html: Low-resource Cross-lingual Event Type Detection via Distant Supervision
    with Minimal Effort
  url: https://www.aclweb.org/anthology/C18-1007
  year: '2018'
C18-1008:
  abstract: We present a neural transition-based model that uses a simple set of edit
    actions (copy, delete, insert) for morphological transduction tasks such as inflection
    generation, lemmatization, and reinflection. In a large-scale evaluation on four
    datasets and dozens of languages, our approach consistently outperforms state-of-the-art
    systems on low and medium training-set sizes and is competitive in the high-resource
    setting. Learning to apply a generic copy action enables our approach to generalize
    quickly from a few data points. We successfully leverage minimum risk training
    to compensate for the weaknesses of MLE parameter learning and neutralize the
    negative effects of training a pipeline with a separate character aligner.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Peter
    full: Peter Makarov
    id: peter-makarov
    last: Makarov
  - first: Simon
    full: Simon Clematide
    id: simon-clematide
    last: Clematide
  author_string: Peter Makarov, Simon Clematide
  bibkey: makarov-clematide-2018-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '83'
  page_last: '93'
  pages: "83\u201393"
  paper_id: '8'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1008.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1008.jpg
  title: Neural Transition-based String Transduction for Limited-Resource Setting
    in Morphology
  title_html: Neural Transition-based String Transduction for Limited-Resource Setting
    in Morphology
  url: https://www.aclweb.org/anthology/C18-1008
  year: '2018'
C18-1009:
  abstract: Capturing interactions among multiple predicate-argument structures (PASs)
    is a crucial issue in the task of analyzing PAS in Japanese. In this paper, we
    propose new Japanese PAS analysis models that integrate the label prediction information
    of arguments in multiple PASs by extending the input and last layers of a standard
    deep bidirectional recurrent neural network (bi-RNN) model. In these models, using
    the mechanisms of pooling and attention, we aim to directly capture the potential
    interactions among multiple PASs, without being disturbed by the word order and
    distance. Our experiments show that the proposed models improve the prediction
    accuracy specifically for cases where the predicate and argument are in an indirect
    dependency relation and achieve a new state of the art in the overall F1 on a
    standard benchmark corpus. on a standard benchmark corpus.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yuichiroh
    full: Yuichiroh Matsubayashi
    id: yuichiroh-matsubayashi
    last: Matsubayashi
  - first: Kentaro
    full: Kentaro Inui
    id: kentaro-inui
    last: Inui
  author_string: Yuichiroh Matsubayashi, Kentaro Inui
  bibkey: matsubayashi-inui-2018-distance
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '94'
  page_last: '106'
  pages: "94\u2013106"
  paper_id: '9'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1009.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1009.jpg
  title: Distance-Free Modeling of Multi-Predicate Interactions in End-to-End Japanese
    Predicate-Argument Structure Analysis
  title_html: Distance-Free Modeling of Multi-Predicate Interactions in End-to-End
    <span class="acl-fixed-case">J</span>apanese Predicate-Argument Structure Analysis
  url: https://www.aclweb.org/anthology/C18-1009
  year: '2018'
C18-1010:
  abstract: 'We present a method for detecting annotation errors in manually and automatically
    annotated dependency parse trees, based on ensemble parsing in combination with
    Bayesian inference, guided by active learning. We evaluate our method in different
    scenarios: (i) for error detection in dependency treebanks and (ii) for improving
    parsing accuracy on in- and out-of-domain data.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Ines
    full: Ines Rehbein
    id: ines-rehbein
    last: Rehbein
  - first: Josef
    full: Josef Ruppenhofer
    id: josef-ruppenhofer
    last: Ruppenhofer
  author_string: Ines Rehbein, Josef Ruppenhofer
  bibkey: rehbein-ruppenhofer-2018-sprucing
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '107'
  page_last: '118'
  pages: "107\u2013118"
  paper_id: '10'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1010.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1010.jpg
  title: "Sprucing up the trees \u2013 Error detection in treebanks"
  title_html: "Sprucing up the trees \u2013 Error detection in treebanks"
  url: https://www.aclweb.org/anthology/C18-1010
  year: '2018'
C18-1011:
  abstract: Non-local features have been exploited by syntactic parsers for capturing
    dependencies between sub output structures. Such features have been a key to the
    success of state-of-the-art statistical parsers. With the rise of deep learning,
    however, it has been shown that local output decisions can give highly competitive
    accuracies, thanks to the power of dense neural input representations that embody
    global syntactic information. We investigate two conceptually simple local neural
    models for constituent parsing, which make local decisions to constituent spans
    and CFG rules, respectively. Consistent with previous findings along the line,
    our best model gives highly competitive results, achieving the labeled bracketing
    F1 scores of 92.4% on PTB and 87.3% on CTB 5.1.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Zhiyang
    full: Zhiyang Teng
    id: zhiyang-teng
    last: Teng
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  author_string: Zhiyang Teng, Yue Zhang
  bibkey: teng-zhang-2018-two
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '119'
  page_last: '132'
  pages: "119\u2013132"
  paper_id: '11'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1011.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1011.jpg
  title: Two Local Models for Neural Constituent Parsing
  title_html: Two Local Models for Neural Constituent Parsing
  url: https://www.aclweb.org/anthology/C18-1011
  year: '2018'
C18-1012:
  abstract: The paper explores the ability of LSTM networks trained on a language
    modeling task to detect linguistic structures which are ungrammatical due to extraction
    violations (extra arguments and subject-relative clause island violations), and
    considers its implications for the debate on language innatism. The results show
    that the current RNN model can correctly classify (un)grammatical sentences, in
    certain conditions, but it is sensitive to linguistic processing factors and probably
    ultimately unable to induce a more abstract notion of grammaticality, at least
    in the domain we tested.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Shammur Absar
    full: Shammur Absar Chowdhury
    id: shammur-absar-chowdhury
    last: Chowdhury
  - first: Roberto
    full: Roberto Zamparelli
    id: roberto-zamparelli
    last: Zamparelli
  author_string: Shammur Absar Chowdhury, Roberto Zamparelli
  bibkey: chowdhury-zamparelli-2018-rnn
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '133'
  page_last: '144'
  pages: "133\u2013144"
  paper_id: '12'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1012.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1012.jpg
  title: RNN Simulations of Grammaticality Judgments on Long-distance Dependencies
  title_html: <span class="acl-fixed-case">RNN</span> Simulations of Grammaticality
    Judgments on Long-distance Dependencies
  url: https://www.aclweb.org/anthology/C18-1012
  year: '2018'
C18-1013:
  abstract: Modeling U.S. Congressional legislation and roll-call votes has received
    significant attention in previous literature, and while legislators across 50
    state governments and D.C. propose over 100,000 bills each year, enacting over
    30% of them on average, state level analysis has received relatively less attention
    due in part to the difficulty in obtaining the necessary data. Since each state
    legislature is guided by their own procedures, politics and issues, however, it
    is difficult to qualitatively asses the factors that affect the likelihood of
    a legislative initiative succeeding. We present several methods for modeling the
    likelihood of a bill receiving floor action across all 50 states and D.C. We utilize
    the lexical content of over 1 million bills, along with contextual legislature
    and legislator derived features to build our predictive models, allowing a comparison
    of what factors are important to the lawmaking process. Furthermore, we show that
    these signals hold complementary predictive power, together achieving an average
    improvement in accuracy of 18% over state specific baselines.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Vladimir
    full: Vladimir Eidelman
    id: vladimir-eidelman
    last: Eidelman
  - first: Anastassia
    full: Anastassia Kornilova
    id: anastassia-kornilova
    last: Kornilova
  - first: Daniel
    full: Daniel Argyle
    id: daniel-argyle
    last: Argyle
  author_string: Vladimir Eidelman, Anastassia Kornilova, Daniel Argyle
  bibkey: eidelman-etal-2018-predictable
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '145'
  page_last: '160'
  pages: "145\u2013160"
  paper_id: '13'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1013.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1013.jpg
  title: How Predictable is Your State? Leveraging Lexical and Contextual Information
    for Predicting Legislative Floor Action at the State Level
  title_html: How Predictable is Your State? Leveraging Lexical and Contextual Information
    for Predicting Legislative Floor Action at the State Level
  url: https://www.aclweb.org/anthology/C18-1013
  year: '2018'
C18-1014:
  abstract: Reading comprehension models are based on recurrent neural networks that
    sequentially process the document tokens. As interest turns to answering more
    complex questions over longer documents, sequential reading of large portions
    of text becomes a substantial bottleneck. Inspired by how humans use document
    structure, we propose a novel framework for reading comprehension. We represent
    documents as trees, and model an agent that learns to interleave quick navigation
    through the document tree with more expensive answer extraction. To encourage
    exploration of the document tree, we propose a new algorithm, based on Deep Q-Network
    (DQN), which strategically samples tree nodes at training time. Empirically we
    find our algorithm improves question answering performance compared to DQN and
    a strong information-retrieval (IR) baseline, and that ensembling our model with
    the IR baseline results in further gains in performance.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Mor
    full: Mor Geva
    id: mor-geva
    last: Geva
  - first: Jonathan
    full: Jonathan Berant
    id: jonathan-berant
    last: Berant
  author_string: Mor Geva, Jonathan Berant
  bibkey: geva-berant-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '161'
  page_last: '176'
  pages: "161\u2013176"
  paper_id: '14'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1014.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1014.jpg
  title: Learning to Search in Long Documents Using Document Structure
  title_html: Learning to Search in Long Documents Using Document Structure
  url: https://www.aclweb.org/anthology/C18-1014
  year: '2018'
C18-1015:
  abstract: "Event relation recognition is a challenging language processing task.\
    \ It is required to determine the relation class of a pair of query events, such\
    \ as causality, under the condition that there isn\u2019t any reliable clue for\
    \ use. We follow the traditional statistical approach in this paper, speculating\
    \ the relation class of the target events based on the relation-class distributions\
    \ on the similar events. There is minimal supervision used during the speculation\
    \ process. In particular, we incorporate image processing into the acquisition\
    \ of similar event instances, including the utilization of images for visually\
    \ representing event scenes, and the use of the neural network based image matching\
    \ for approximate calculation between events. We test our method on the ACE-R2\
    \ corpus and compared our model with the fully-supervised neural network models.\
    \ Experimental results show that we achieve a comparable performance to CNN while\
    \ slightly better than LSTM."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yu
    full: Yu Hong
    id: yu-hong
    last: Hong
  - first: Yang
    full: Yang Xu
    id: yang-xu
    last: Xu
  - first: Huibin
    full: Huibin Ruan
    id: huibin-ruan
    last: Ruan
  - first: Bowei
    full: Bowei Zou
    id: bowei-zou
    last: Zou
  - first: Jianmin
    full: Jianmin Yao
    id: jianmin-yao
    last: Yao
  - first: Guodong
    full: Guodong Zhou
    id: guodong-zhou
    last: Zhou
  author_string: Yu Hong, Yang Xu, Huibin Ruan, Bowei Zou, Jianmin Yao, Guodong Zhou
  bibkey: hong-etal-2018-incorporating
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '177'
  page_last: '189'
  pages: "177\u2013189"
  paper_id: '15'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1015.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1015.jpg
  title: Incorporating Image Matching Into Knowledge Acquisition for Event-Oriented
    Relation Recognition
  title_html: Incorporating Image Matching Into Knowledge Acquisition for Event-Oriented
    Relation Recognition
  url: https://www.aclweb.org/anthology/C18-1015
  year: '2018'
C18-1016:
  abstract: In this paper, we describe TextEnt, a neural network model that learns
    distributed representations of entities and documents directly from a knowledge
    base (KB). Given a document in a KB consisting of words and entity annotations,
    we train our model to predict the entity that the document describes and map the
    document and its target entity close to each other in a continuous vector space.
    Our model is trained using a large number of documents extracted from Wikipedia.
    The performance of the proposed model is evaluated using two tasks, namely fine-grained
    entity typing and multiclass text classification. The results demonstrate that
    our model achieves state-of-the-art performance on both tasks. The code and the
    trained representations are made available online for further academic research.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Ikuya
    full: Ikuya Yamada
    id: ikuya-yamada
    last: Yamada
  - first: Hiroyuki
    full: Hiroyuki Shindo
    id: hiroyuki-shindo
    last: Shindo
  - first: Yoshiyasu
    full: Yoshiyasu Takefuji
    id: yoshiyasu-takefuji
    last: Takefuji
  author_string: Ikuya Yamada, Hiroyuki Shindo, Yoshiyasu Takefuji
  bibkey: yamada-etal-2018-representation
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '190'
  page_last: '201'
  pages: "190\u2013201"
  paper_id: '16'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1016.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1016.jpg
  title: Representation Learning of Entities and Documents from Knowledge Base Descriptions
  title_html: Representation Learning of Entities and Documents from Knowledge Base
    Descriptions
  url: https://www.aclweb.org/anthology/C18-1016
  year: '2018'
C18-1017:
  abstract: We present domain independent models to date documents based only on neologism
    usage patterns. Our models capture patterns of neologism usage over time to date
    texts, provide insights into temporal locality of word usage over a span of 150
    years, and generalize to various domains like News, Fiction, and Non-Fiction with
    competitive performance. Quite intriguingly, we show that by modeling only the
    distribution of usage counts over neologisms (the model being agnostic of the
    particular words themselves), we achieve competitive performance using several
    orders of magnitude fewer features (only 200 input features) compared to state
    of the art models some of which use 200K features.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Vivek
    full: Vivek Kulkarni
    id: vivek-kulkarni
    last: Kulkarni
  - first: Yingtao
    full: Yingtao Tian
    id: yingtao-tian
    last: Tian
  - first: Parth
    full: Parth Dandiwala
    id: parth-dandiwala
    last: Dandiwala
  - first: Steve
    full: Steve Skiena
    id: steven-skiena
    last: Skiena
  author_string: Vivek Kulkarni, Yingtao Tian, Parth Dandiwala, Steve Skiena
  bibkey: kulkarni-etal-2018-simple
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '202'
  page_last: '212'
  pages: "202\u2013212"
  paper_id: '17'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1017.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1017.jpg
  title: Simple Neologism Based Domain Independent Models to Predict Year of Authorship
  title_html: Simple Neologism Based Domain Independent Models to Predict Year of
    Authorship
  url: https://www.aclweb.org/anthology/C18-1017
  year: '2018'
C18-1018:
  abstract: "Sequence-to-sequence model has been applied to solve math word problems.\
    \ The model takes math problem descriptions as input and generates equations as\
    \ output. The advantage of sequence-to-sequence model requires no feature engineering\
    \ and can generate equations that do not exist in training data. However, our\
    \ experimental analysis reveals that this model suffers from two shortcomings:\
    \ (1) generate spurious numbers; (2) generate numbers at wrong positions. In this\
    \ paper, we propose incorporating copy and alignment mechanism to the sequence-to-sequence\
    \ model (namely CASS) to address these shortcomings. To train our model, we apply\
    \ reinforcement learning to directly optimize the solution accuracy. It overcomes\
    \ the \u201Ctrain-test discrepancy\u201D issue of maximum likelihood estimation,\
    \ which uses the surrogate objective of maximizing equation likelihood during\
    \ training while the evaluation metric is solution accuracy (non-differentiable)\
    \ at test time. Furthermore, to explore the effectiveness of our neural model,\
    \ we use our model output as a feature and incorporate it into the feature-based\
    \ model. Experimental results show that (1) The copy and alignment mechanism is\
    \ effective to address the two issues; (2) Reinforcement learning leads to better\
    \ performance than maximum likelihood on this task; (3) Our neural model is complementary\
    \ to the feature-based model and their combination significantly outperforms the\
    \ state-of-the-art results."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Danqing
    full: Danqing Huang
    id: danqing-huang
    last: Huang
  - first: Jing
    full: Jing Liu
    id: jing-liu
    last: Liu
  - first: Chin-Yew
    full: Chin-Yew Lin
    id: chin-yew-lin
    last: Lin
  - first: Jian
    full: Jian Yin
    id: jian-yin
    last: Yin
  author_string: Danqing Huang, Jing Liu, Chin-Yew Lin, Jian Yin
  bibkey: huang-etal-2018-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '213'
  page_last: '223'
  pages: "213\u2013223"
  paper_id: '18'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1018.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1018.jpg
  title: Neural Math Word Problem Solver with Reinforcement Learning
  title_html: Neural Math Word Problem Solver with Reinforcement Learning
  url: https://www.aclweb.org/anthology/C18-1018
  year: '2018'
C18-1019:
  abstract: "A lexical simplification (LS) system aims to substitute complex words\
    \ with simple words in a text, while preserving its meaning and grammaticality.\
    \ Despite individual users\u2019 differences in vocabulary knowledge, current\
    \ systems do not consider these variations; rather, they are trained to find one\
    \ optimal substitution or ranked list of substitutions for all users. We evaluate\
    \ the performance of a state-of-the-art LS system on individual learners of English\
    \ at different proficiency levels, and measure the benefits of using complex word\
    \ identification (CWI) models to personalize the system. Experimental results\
    \ show that even a simple personalized CWI model, based on graded vocabulary lists,\
    \ can help the system avoid some unnecessary simplifications and produce more\
    \ readable output."
  address: Santa Fe, New Mexico, USA
  author:
  - first: John
    full: John Lee
    id: john-s-y-lee
    last: Lee
  - first: Chak Yan
    full: Chak Yan Yeung
    id: chak-yan-yeung
    last: Yeung
  author_string: John Lee, Chak Yan Yeung
  bibkey: lee-yeung-2018-personalizing
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '224'
  page_last: '232'
  pages: "224\u2013232"
  paper_id: '19'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1019.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1019.jpg
  title: Personalizing Lexical Simplification
  title_html: Personalizing Lexical Simplification
  url: https://www.aclweb.org/anthology/C18-1019
  year: '2018'
C18-1020:
  abstract: Distributional word representations (often referred to as word embeddings)
    are omnipresent in modern NLP. Early work has focused on building representations
    for word types, and recent studies show that lemmatization and part of speech
    (POS) disambiguation of targets in isolation improve the performance of word embeddings
    on a range of downstream tasks. However, the reasons behind these improvements,
    the qualitative effects of these operations and the combined performance of lemmatized
    and POS disambiguated targets are less studied. This work aims to close this gap
    and puts previous findings into a general perspective. We examine the effect of
    lemmatization and POS typing on word embedding performance in a novel resource-based
    evaluation scenario, as well as on standard similarity benchmarks. We show that
    these two operations have complimentary qualitative and vocabulary-level effects
    and are best used in combination. We find that the improvement is more pronounced
    for verbs and show how lemmatization and POS typing implicitly target some of
    the verb-specific issues. We claim that the observed improvement is a result of
    better conceptual alignment between word embeddings and lexical resources, stressing
    the need for conceptually plausible modeling of word embedding targets.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Ilia
    full: Ilia Kuznetsov
    id: ilia-kuznetsov
    last: Kuznetsov
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Ilia Kuznetsov, Iryna Gurevych
  bibkey: kuznetsov-gurevych-2018-text
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '233'
  page_last: '244'
  pages: "233\u2013244"
  paper_id: '20'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1020.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1020.jpg
  title: 'From Text to Lexicon: Bridging the Gap between Word Embeddings and Lexical
    Resources'
  title_html: 'From Text to Lexicon: Bridging the Gap between Word Embeddings and
    Lexical Resources'
  url: https://www.aclweb.org/anthology/C18-1020
  year: '2018'
C18-1021:
  abstract: Most previous research in text simplification has aimed to develop generic
    solutions, assuming very homogeneous target audiences with consistent intra-group
    simplification needs. We argue that this assumption does not hold, and that instead
    we need to develop simplification systems that adapt to the individual needs of
    specific users. As a first step towards personalized simplification, we propose
    a framework for adaptive lexical simplification and introduce Lexi, a free open-source
    and easily extensible tool for adaptive, personalized text simplification. Lexi
    is easily installed as a browser extension, enabling easy access to the service
    for its users.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Joachim
    full: Joachim Bingel
    id: joachim-bingel
    last: Bingel
  - first: Gustavo
    full: Gustavo Paetzold
    id: gustavo-paetzold
    last: Paetzold
  - first: Anders
    full: "Anders S\xF8gaard"
    id: anders-sogaard
    last: "S\xF8gaard"
  author_string: "Joachim Bingel, Gustavo Paetzold, Anders S\xF8gaard"
  bibkey: bingel-etal-2018-lexi
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '245'
  page_last: '258'
  pages: "245\u2013258"
  paper_id: '21'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1021.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1021.jpg
  title: 'Lexi: A tool for adaptive, personalized text simplification'
  title_html: '<span class="acl-fixed-case">L</span>exi: A tool for adaptive, personalized
    text simplification'
  url: https://www.aclweb.org/anthology/C18-1021
  year: '2018'
C18-1022:
  abstract: Identifying emergent research trends is a key issue for both primary researchers
    as well as secondary research managers. Such processes can uncover the historical
    development of an area, and yield insight on developing topics. We propose an
    embedded trend detection framework for this task which incorporates our bijunctive
    hypothesis that important phrases are written by important authors within a field
    and vice versa. By ranking both author and phrase information in a multigraph,
    our method jointly determines key phrases and authoritative authors. We represent
    this intermediate output as phrasal embeddings, and feed this to a recurrent neural
    network (RNN) to compute trend scores that identify research trends. Over two
    large datasets of scientific articles, we demonstrate that our approach successfully
    detects past trends from the field, outperforming baselines based solely on text
    centrality or citation.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Shenhao
    full: Shenhao Jiang
    id: shenhao-jiang
    last: Jiang
  - first: Animesh
    full: Animesh Prasad
    id: animesh-prasad
    last: Prasad
  - first: Min-Yen
    full: Min-Yen Kan
    id: min-yen-kan
    last: Kan
  - first: Kazunari
    full: Kazunari Sugiyama
    id: kazunari-sugiyama
    last: Sugiyama
  author_string: Shenhao Jiang, Animesh Prasad, Min-Yen Kan, Kazunari Sugiyama
  bibkey: jiang-etal-2018-identifying
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '259'
  page_last: '269'
  pages: "259\u2013269"
  paper_id: '22'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1022.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1022.jpg
  title: Identifying Emergent Research Trends by Key Authors and Phrases
  title_html: Identifying Emergent Research Trends by Key Authors and Phrases
  url: https://www.aclweb.org/anthology/C18-1022
  year: '2018'
C18-1023:
  abstract: "In this paper, we study how we can improve a deep learning approach to\
    \ textual entailment by incorporating lexical entailment relations from WordNet.\
    \ Our idea is to embed the lexical entailment knowledge contained in WordNet in\
    \ specially-learned word vectors, which we call \u201Centailment vectors.\u201D\
    \ We present a standard neural network model and a novel set-theoretic model to\
    \ learn these entailment vectors from word pairs with known lexical entailment\
    \ relations derived from WordNet. We further incorporate these entailment vectors\
    \ into a decomposable attention model for textual entailment and evaluate the\
    \ model on the SICK and the SNLI dataset. We find that using these special entailment\
    \ word vectors, we can significantly improve the performance of textual entailment\
    \ compared with a baseline that uses only standard word2vec vectors. The final\
    \ performance of our model is close to or above the state of the art, but our\
    \ method does not rely on any manually-crafted rules or extensive syntactic features."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yunshi
    full: Yunshi Lan
    id: yunshi-lan
    last: Lan
  - first: Jing
    full: Jing Jiang
    id: jing-jiang
    last: Jiang
  author_string: Yunshi Lan, Jing Jiang
  bibkey: lan-jiang-2018-embedding
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '270'
  page_last: '281'
  pages: "270\u2013281"
  paper_id: '23'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1023.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1023.jpg
  title: Embedding WordNet Knowledge for Textual Entailment
  title_html: Embedding <span class="acl-fixed-case">W</span>ord<span class="acl-fixed-case">N</span>et
    Knowledge for Textual Entailment
  url: https://www.aclweb.org/anthology/C18-1023
  year: '2018'
C18-1024:
  abstract: Fine-grained entity typing aims at identifying the semantic type of an
    entity in KB. Type information is very important in knowledge bases, but are unfortunately
    incomplete even in some large knowledge bases. Limitations of existing methods
    are either ignoring the structure and type information in KB or requiring large
    scale annotated corpus. To address these issues, we propose an attributed and
    predictive entity embedding method, which can fully utilize various kinds of information
    comprehensively. Extensive experiments on two real DBpedia datasets show that
    our proposed method significantly outperforms 8 state-of-the-art methods, with
    4.0% and 5.2% improvement in Mi-F1 and Ma-F1, respectively.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Hailong
    full: Hailong Jin
    id: hailong-jin
    last: Jin
  - first: Lei
    full: Lei Hou
    id: lei-hou
    last: Hou
  - first: Juanzi
    full: Juanzi Li
    id: juanzi-li
    last: Li
  - first: Tiansi
    full: Tiansi Dong
    id: tiansi-dong
    last: Dong
  author_string: Hailong Jin, Lei Hou, Juanzi Li, Tiansi Dong
  bibkey: jin-etal-2018-attributed
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '282'
  page_last: '292'
  pages: "282\u2013292"
  paper_id: '24'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1024.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1024.jpg
  title: Attributed and Predictive Entity Embedding for Fine-Grained Entity Typing
    in Knowledge Bases
  title_html: Attributed and Predictive Entity Embedding for Fine-Grained Entity Typing
    in Knowledge Bases
  url: https://www.aclweb.org/anthology/C18-1024
  year: '2018'
C18-1025:
  abstract: Recently, a significant number of studies have focused on neural information
    retrieval (IR) models. One category of works use unlabeled data to train general
    word embeddings based on term proximity, which can be integrated into traditional
    IR models. The other category employs labeled data (e.g. click-through data) to
    train end-to-end neural IR models consisting of layers for target-specific representation
    learning. The latter idea accounts better for the IR task and is favored by recent
    research works, which is the one we will follow in this paper. We hypothesize
    that general semantics learned from unlabeled data can complement task-specific
    representation learned from labeled data of limited quality, and that a combination
    of the two is favorable. To this end, we propose a learning framework which can
    benefit from both labeled and more abundant unlabeled data for representation
    learning in the context of IR. Through a joint learning fashion in a single neural
    framework, the learned representation is optimized to minimize both the supervised
    loss on query-document matching and the unsupervised loss on text reconstruction.
    Standard retrieval experiments on TREC collections indicate that the joint learning
    methodology leads to significant better performance of retrieval over several
    strong baselines for IR.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Bo
    full: Bo Li
    id: bo-li
    last: Li
  - first: Ping
    full: Ping Cheng
    id: ping-cheng
    last: Cheng
  - first: Le
    full: Le Jia
    id: le-jia
    last: Jia
  author_string: Bo Li, Ping Cheng, Le Jia
  bibkey: li-etal-2018-joint-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '293'
  page_last: '302'
  pages: "293\u2013302"
  paper_id: '25'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1025.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1025.jpg
  title: Joint Learning from Labeled and Unlabeled Data for Information Retrieval
  title_html: Joint Learning from Labeled and Unlabeled Data for Information Retrieval
  url: https://www.aclweb.org/anthology/C18-1025
  year: '2018'
C18-1026:
  abstract: "We analyze two novel data sets of German educational media texts targeting\
    \ adults and children. The analysis is based on 400 automatically extracted measures\
    \ of linguistic complexity from a wide range of linguistic domains. We show that\
    \ both data sets exhibit broad linguistic adaptation to the target audience, which\
    \ generalizes across both data sets. Our most successful binary classification\
    \ model for German readability robustly shows high accuracy between 89.4%\u2013\
    98.9% for both data sets. To our knowledge, this comprehensive German readability\
    \ model is the first for which robust cross-corpus performance has been shown.\
    \ The research also contributes resources for German readability assessment that\
    \ are externally validated as successful for different target audiences: we compiled\
    \ a new corpus of German news broadcast subtitles, the Tagesschau/Logo corpus,\
    \ and crawled a GEO/GEOlino corpus substantially enlarging the data compiled by\
    \ Hancke et al. 2012."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Zarah
    full: "Zarah Wei\xDF"
    id: zarah-weiss
    last: "Wei\xDF"
  - first: Detmar
    full: Detmar Meurers
    id: detmar-meurers
    last: Meurers
  author_string: "Zarah Wei\xDF, Detmar Meurers"
  bibkey: weiss-meurers-2018-modeling
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '303'
  page_last: '317'
  pages: "303\u2013317"
  paper_id: '26'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1026.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1026.jpg
  title: 'Modeling the Readability of German Targeting Adults and Children: An empirically
    broad analysis and its cross-corpus validation'
  title_html: 'Modeling the Readability of <span class="acl-fixed-case">G</span>erman
    Targeting Adults and Children: An empirically broad analysis and its cross-corpus
    validation'
  url: https://www.aclweb.org/anthology/C18-1026
  year: '2018'
C18-1027:
  abstract: Complexity of texts is usually assessed only at the lexical and syntactic
    levels. Although it is known that conceptual complexity plays a significant role
    in text understanding, no attempts have been made at assessing it automatically.
    We propose to automatically estimate the conceptual complexity of texts by exploiting
    a number of graph-based measures on a large knowledge base. By using a high-quality
    language learners corpus for English, we show that graph-based measures of individual
    text concepts, as well as the way they relate to each other in the knowledge graph,
    have a high discriminative power when distinguishing between two versions of the
    same text. Furthermore, when used as features in a binary classification task
    aiming to choose the simpler of two versions of the same text, our measures achieve
    high performance even in a default setup.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Sanja
    full: "Sanja \u0160tajner"
    id: sanja-stajner
    last: "\u0160tajner"
  - first: Ioana
    full: "Ioana Hulpu\u015F"
    id: ioana-hulpus
    last: "Hulpu\u015F"
  author_string: "Sanja \u0160tajner, Ioana Hulpu\u015F"
  bibkey: stajner-hulpus-2018-automatic
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '318'
  page_last: '330'
  pages: "318\u2013330"
  paper_id: '27'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1027.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1027.jpg
  title: Automatic Assessment of Conceptual Text Complexity Using Knowledge Graphs
  title_html: Automatic Assessment of Conceptual Text Complexity Using Knowledge Graphs
  url: https://www.aclweb.org/anthology/C18-1027
  year: '2018'
C18-1028:
  abstract: Learning from a real-world data stream and continuously updating the model
    without explicit supervision is a new challenge for NLP applications with machine
    learning components. In this work, we have developed an adaptive learning system
    for text simplification, which improves the underlying learning-to-rank model
    from usage data, i.e. how users have employed the system for the task of simplification.
    Our experimental result shows that, over a period of time, the performance of
    the embedded paraphrase ranking model increases steadily improving from a score
    of 62.88% up to 75.70% based on the NDCG@10 evaluation metrics. To our knowledge,
    this is the first study where an NLP component is adaptively improved through
    usage.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Seid Muhie
    full: Seid Muhie Yimam
    id: seid-muhie-yimam
    last: Yimam
  - first: Chris
    full: Chris Biemann
    id: chris-biemann
    last: Biemann
  author_string: Seid Muhie Yimam, Chris Biemann
  bibkey: yimam-biemann-2018-par4sim
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '331'
  page_last: '342'
  pages: "331\u2013342"
  paper_id: '28'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1028.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1028.jpg
  title: "Par4Sim \u2013 Adaptive Paraphrasing for Text Simplification"
  title_html: "<span class=\"acl-fixed-case\">P</span>ar4<span class=\"acl-fixed-case\"\
    >S</span>im \u2013 Adaptive Paraphrasing for Text Simplification"
  url: https://www.aclweb.org/anthology/C18-1028
  year: '2018'
C18-1029:
  abstract: "Approaches to authorship attribution, the task of identifying the author\
    \ of a document, are based on analysis of individuals\u2019 writing style and/or\
    \ preferred topics. Although the problem has been widely explored, no previous\
    \ studies have analysed the relationship between dataset characteristics and effectiveness\
    \ of different types of features. This study carries out an analysis of four widely\
    \ used datasets to explore how different types of features affect authorship attribution\
    \ accuracy under varying conditions. The results of the analysis are applied to\
    \ authorship attribution models based on both discrete and continuous representations.\
    \ We apply the conclusions from our analysis to an extension of an existing approach\
    \ to authorship attribution and outperform the prior state-of-the-art on two out\
    \ of the four datasets used."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yunita
    full: Yunita Sari
    id: yunita-sari
    last: Sari
  - first: Mark
    full: Mark Stevenson
    id: mark-stevenson
    last: Stevenson
  - first: Andreas
    full: Andreas Vlachos
    id: andreas-vlachos
    last: Vlachos
  author_string: Yunita Sari, Mark Stevenson, Andreas Vlachos
  bibkey: sari-etal-2018-topic
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '343'
  page_last: '353'
  pages: "343\u2013353"
  paper_id: '29'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1029.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1029.jpg
  title: Topic or Style? Exploring the Most Useful Features for Authorship Attribution
  title_html: Topic or Style? Exploring the Most Useful Features for Authorship Attribution
  url: https://www.aclweb.org/anthology/C18-1029
  year: '2018'
C18-1030:
  abstract: LSTM-based language models have been shown effective in Word Sense Disambiguation
    (WSD). In particular, the technique proposed by Yuan et al. (2016) returned state-of-the-art
    performance in several benchmarks, but neither the training data nor the source
    code was released. This paper presents the results of a reproduction study and
    analysis of this technique using only openly available datasets (GigaWord, SemCor,
    OMSTI) and software (TensorFlow). Our study showed that similar results can be
    obtained with much less data than hinted at by Yuan et al. (2016). Detailed analyses
    shed light on the strengths and weaknesses of this method. First, adding more
    unannotated training data is useful, but is subject to diminishing returns. Second,
    the model can correctly identify both popular and unpopular meanings. Finally,
    the limited sense coverage in the annotated datasets is a major limitation. All
    code and trained models are made freely available.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Minh
    full: Minh Le
    id: minh-le1
    last: Le
  - first: Marten
    full: Marten Postma
    id: marten-postma
    last: Postma
  - first: Jacopo
    full: Jacopo Urbani
    id: jacopo-urbani
    last: Urbani
  - first: Piek
    full: Piek Vossen
    id: piek-vossen
    last: Vossen
  author_string: Minh Le, Marten Postma, Jacopo Urbani, Piek Vossen
  bibkey: le-etal-2018-deep
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '354'
  page_last: '365'
  pages: "354\u2013365"
  paper_id: '30'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1030.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1030.jpg
  title: A Deep Dive into Word Sense Disambiguation with LSTM
  title_html: A Deep Dive into Word Sense Disambiguation with <span class="acl-fixed-case">LSTM</span>
  url: https://www.aclweb.org/anthology/C18-1030
  year: '2018'
C18-1031:
  abstract: In this paper, we present a method which learns the word embedding for
    readability assessment. For the existing word embedding models, they typically
    focus on the syntactic or semantic relations of words, while ignoring the reading
    difficulty, thus they may not be suitable for readability assessment. Hence, we
    provide the knowledge-enriched word embedding (KEWE), which encodes the knowledge
    on reading difficulty into the representation of words. Specifically, we extract
    the knowledge on word-level difficulty from three perspectives to construct a
    knowledge graph, and develop two word embedding models to incorporate the difficulty
    context derived from the knowledge graph to define the loss functions. Experiments
    are designed to apply KEWE for readability assessment on both English and Chinese
    datasets, and the results demonstrate both effectiveness and potential of KEWE.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Zhiwei
    full: Zhiwei Jiang
    id: zhiwei-jiang
    last: Jiang
  - first: Qing
    full: Qing Gu
    id: qing-gu
    last: Gu
  - first: Yafeng
    full: Yafeng Yin
    id: yafeng-yin
    last: Yin
  - first: Daoxu
    full: Daoxu Chen
    id: daoxu-chen
    last: Chen
  author_string: Zhiwei Jiang, Qing Gu, Yafeng Yin, Daoxu Chen
  bibkey: jiang-etal-2018-enriching
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '366'
  page_last: '378'
  pages: "366\u2013378"
  paper_id: '31'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1031.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1031.jpg
  title: Enriching Word Embeddings with Domain Knowledge for Readability Assessment
  title_html: Enriching Word Embeddings with Domain Knowledge for Readability Assessment
  url: https://www.aclweb.org/anthology/C18-1031
  year: '2018'
C18-1032:
  abstract: "The exponential increase in the usage of Wikipedia as a key source of\
    \ scientific knowledge among the researchers is making it absolutely necessary\
    \ to metamorphose this knowledge repository into an integral and self-contained\
    \ source of information for direct utilization. Unfortunately, the references\
    \ which support the content of each Wikipedia entity page, are far from complete.\
    \ Why are the reference section ill-formed for most Wikipedia pages? Is this section\
    \ edited as frequently as the other sections of a page? Can there be appropriate\
    \ surrogates that can automatically enhance the reference section? In this paper,\
    \ we propose a novel two step approach \u2013 WikiRef \u2013 that (i) leverages\
    \ the wikilinks present in a scientific Wikipedia target page and, thereby, (ii)\
    \ recommends highly relevant references to be included in that target page appropriately\
    \ and automatically borrowed from the reference section of the wikilinks. In the\
    \ first step, we build a classifier to ascertain whether a wikilink is a potential\
    \ source of reference or not. In the following step, we recommend references to\
    \ the target page from the reference section of the wikilinks that are classified\
    \ as potential sources of references in the first step. We perform an extensive\
    \ evaluation of our approach on datasets from two different domains \u2013 Computer\
    \ Science and Physics. For Computer Science we achieve a notably good performance\
    \ with a precision@1 of 0.44 for reference recommendation as opposed to 0.38 obtained\
    \ from the most competitive baseline. For the Physics dataset, we obtain a similar\
    \ performance boost of 10% with respect to the most competitive baseline."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Abhik
    full: Abhik Jana
    id: abhik-jana
    last: Jana
  - first: Pranjal
    full: Pranjal Kanojiya
    id: pranjal-kanojiya
    last: Kanojiya
  - first: Pawan
    full: Pawan Goyal
    id: pawan-goyal
    last: Goyal
  - first: Animesh
    full: Animesh Mukherjee
    id: animesh-mukherjee
    last: Mukherjee
  author_string: Abhik Jana, Pranjal Kanojiya, Pawan Goyal, Animesh Mukherjee
  bibkey: jana-etal-2018-wikiref
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '379'
  page_last: '389'
  pages: "379\u2013389"
  paper_id: '32'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1032.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1032.jpg
  title: 'WikiRef: Wikilinks as a route to recommending appropriate references for
    scientific Wikipedia pages'
  title_html: '<span class="acl-fixed-case">W</span>iki<span class="acl-fixed-case">R</span>ef:
    Wikilinks as a route to recommending appropriate references for scientific <span
    class="acl-fixed-case">W</span>ikipedia pages'
  url: https://www.aclweb.org/anthology/C18-1032
  year: '2018'
C18-1033:
  abstract: "Book recommender systems can help promote the practice of reading for\
    \ pleasure, which has been declining in recent years. One factor that influences\
    \ reading preferences is writing style. We propose a system that recommends books\
    \ after learning their authors\u2019 style. To our knowledge, this is the first\
    \ work that applies the information learned by an author-identification model\
    \ to book recommendations. We evaluated the system according to a top-k recommendation\
    \ scenario. Our system gives better accuracy when compared with many state-of-the-art\
    \ methods. We also conducted a qualitative analysis by checking if similar books/authors\
    \ were annotated similarly by experts."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Haifa
    full: Haifa Alharthi
    id: haifa-alharthi
    last: Alharthi
  - first: Diana
    full: Diana Inkpen
    id: diana-inkpen
    last: Inkpen
  - first: Stan
    full: Stan Szpakowicz
    id: stan-szpakowicz
    last: Szpakowicz
  author_string: Haifa Alharthi, Diana Inkpen, Stan Szpakowicz
  bibkey: alharthi-etal-2018-authorship
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '390'
  page_last: '400'
  pages: "390\u2013400"
  paper_id: '33'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1033.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1033.jpg
  title: Authorship Identification for Literary Book Recommendations
  title_html: Authorship Identification for Literary Book Recommendations
  url: https://www.aclweb.org/anthology/C18-1033
  year: '2018'
C18-1034:
  abstract: Effective textual communication depends on readers being proficient enough
    to comprehend texts, and texts being clear enough to be understood by the intended
    audience, in a reading task. When the meaning of textual information and instructions
    is not well conveyed, many losses and damages may occur. Among the solutions to
    alleviate this problem is the automatic evaluation of sentence readability, task
    which has been receiving a lot of attention due to its large applicability. However,
    a shortage of resources, such as corpora for training and evaluation, hinders
    the full development of this task. In this paper, we generate a nontrivial sentence
    corpus in Portuguese. We evaluate three scenarios for building it, taking advantage
    of a parallel corpus of simplification, in which each sentence triplet is aligned
    and has simplification operations annotated, being ideal for justifying possible
    mistakes of future methods. The best scenario of our corpus PorSimplesSent is
    composed of 4,888 pairs, which is bigger than a similar corpus for English; all
    the three versions of it are publicly available. We created four baselines for
    PorSimplesSent and made available a pairwise ranking method, using 17 linguistic
    and psycholinguistic features, which correctly identifies the ranking of sentence
    pairs with an accuracy of 74.2%.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Sidney Evaldo
    full: Sidney Evaldo Leal
    id: sidney-evaldo-leal
    last: Leal
  - first: Magali Sanches
    full: Magali Sanches Duran
    id: magali-sanches-duran
    last: Duran
  - first: Sandra Maria
    full: "Sandra Maria Alu\xEDsio"
    id: sandra-aluisio
    last: "Alu\xEDsio"
  author_string: "Sidney Evaldo Leal, Magali Sanches Duran, Sandra Maria Alu\xEDsio"
  bibkey: leal-etal-2018-nontrivial
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '401'
  page_last: '413'
  pages: "401\u2013413"
  paper_id: '34'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1034.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1034.jpg
  title: A Nontrivial Sentence Corpus for the Task of Sentence Readability Assessment
    in Portuguese
  title_html: A Nontrivial Sentence Corpus for the Task of Sentence Readability Assessment
    in <span class="acl-fixed-case">P</span>ortuguese
  url: https://www.aclweb.org/anthology/C18-1034
  year: '2018'
C18-1035:
  abstract: This paper proposes to perform natural language inference with Word-Pair-Dependency-Triplets.
    Most previous DNN-based approaches either ignore syntactic dependency among words,
    or directly use tree-LSTM to generate sentence representation with irrelevant
    information. To overcome the problems mentioned above, we adopt Word-Pair-Dependency-Triplets
    to improve alignment and inference judgment. To be specific, instead of comparing
    each triplet from one passage with the merged information of another passage,
    we first propose to perform comparison directly between the triplets of the given
    passage-pair to make the judgement more interpretable. Experimental results show
    that the performance of our approach is better than most of the approaches that
    use tree structures, and is comparable to other state-of-the-art approaches.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Qianlong
    full: Qianlong Du
    id: qianlong-du
    last: Du
  - first: Chengqing
    full: Chengqing Zong
    id: chengqing-zong
    last: Zong
  - first: Keh-Yih
    full: Keh-Yih Su
    id: keh-yih-su
    last: Su
  author_string: Qianlong Du, Chengqing Zong, Keh-Yih Su
  bibkey: du-etal-2018-adopting
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '414'
  page_last: '425'
  pages: "414\u2013425"
  paper_id: '35'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1035.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1035.jpg
  title: Adopting the Word-Pair-Dependency-Triplets with Individual Comparison for
    Natural Language Inference
  title_html: Adopting the Word-Pair-Dependency-Triplets with Individual Comparison
    for Natural Language Inference
  url: https://www.aclweb.org/anthology/C18-1035
  year: '2018'
C18-1036:
  abstract: Distantly supervised relation extraction greatly reduces human efforts
    in extracting relational facts from unstructured texts. However, it suffers from
    noisy labeling problem, which can degrade its performance. Meanwhile, the useful
    information expressed in knowledge graph is still underutilized in the state-of-the-art
    methods for distantly supervised relation extraction. In the light of these challenges,
    we propose CORD, a novelCOopeRativeDenoising framework, which consists two base
    networks leveraging text corpus and knowledge graph respectively, and a cooperative
    module involving their mutual learning by the adaptive bi-directional knowledge
    distillation and dynamic ensemble with noisy-varying instances. Experimental results
    on a real-world dataset demonstrate that the proposed method reduces the noisy
    labels and achieves substantial improvement over the state-of-the-art methods.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Kai
    full: Kai Lei
    id: kai-lei
    last: Lei
  - first: Daoyuan
    full: Daoyuan Chen
    id: daoyuan-chen
    last: Chen
  - first: Yaliang
    full: Yaliang Li
    id: yaliang-li
    last: Li
  - first: Nan
    full: Nan Du
    id: nan-du
    last: Du
  - first: Min
    full: Min Yang
    id: min-yang
    last: Yang
  - first: Wei
    full: Wei Fan
    id: wei-fan
    last: Fan
  - first: Ying
    full: Ying Shen
    id: ying-shen
    last: Shen
  author_string: Kai Lei, Daoyuan Chen, Yaliang Li, Nan Du, Min Yang, Wei Fan, Ying
    Shen
  bibkey: lei-etal-2018-cooperative
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '426'
  page_last: '436'
  pages: "426\u2013436"
  paper_id: '36'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1036.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1036.jpg
  title: Cooperative Denoising for Distantly Supervised Relation Extraction
  title_html: Cooperative Denoising for Distantly Supervised Relation Extraction
  url: https://www.aclweb.org/anthology/C18-1036
  year: '2018'
C18-1037:
  abstract: Relation Classification aims to classify the semantic relationship between
    two marked entities in a given sentence. It plays a vital role in a variety of
    natural language processing applications. Most existing methods focus on exploiting
    mono-lingual data, e.g., in English, due to the lack of annotated data in other
    languages. In this paper, we come up with a feature adaptation approach for cross-lingual
    relation classification, which employs a generative adversarial network (GAN)
    to transfer feature representations from one language with rich annotated data
    to another language with scarce annotated data. Such a feature adaptation approach
    enables feature imitation via the competition between a relation classification
    network and a rival discriminator. Experimental results on the ACE 2005 multilingual
    training corpus, treating English as the source language and Chinese the target,
    demonstrate the effectiveness of our proposed approach, yielding an improvement
    of 5.7% over the state-of-the-art.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Bowei
    full: Bowei Zou
    id: bowei-zou
    last: Zou
  - first: Zengzhuang
    full: Zengzhuang Xu
    id: zengzhuang-xu
    last: Xu
  - first: Yu
    full: Yu Hong
    id: yu-hong
    last: Hong
  - first: Guodong
    full: Guodong Zhou
    id: guodong-zhou
    last: Zhou
  author_string: Bowei Zou, Zengzhuang Xu, Yu Hong, Guodong Zhou
  bibkey: zou-etal-2018-adversarial
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '437'
  page_last: '448'
  pages: "437\u2013448"
  paper_id: '37'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1037.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1037.jpg
  title: Adversarial Feature Adaptation for Cross-lingual Relation Classification
  title_html: Adversarial Feature Adaptation for Cross-lingual Relation Classification
  url: https://www.aclweb.org/anthology/C18-1037
  year: '2018'
C18-1038:
  abstract: Answering questions from university admission exams (Gaokao in Chinese)
    is a challenging AI task since it requires effective representation to capture
    complicated semantic relations between questions and answers. In this work, we
    propose a hybrid neural model for deep question-answering task from history examinations.
    Our model employs a cooperative gated neural network to retrieve answers with
    the assistance of extra labels given by a neural turing machine labeler. Empirical
    study shows that the labeler works well with only a small training dataset and
    the gated mechanism is good at fetching the semantic representation of lengthy
    answers. Experiments on question answering demonstrate the proposed model obtains
    substantial performance gains over various neural model baselines in terms of
    multiple evaluation metrics.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Zhuosheng
    full: Zhuosheng Zhang
    id: zhuosheng-zhang
    last: Zhang
  - first: Hai
    full: Hai Zhao
    id: hai-zhao
    last: Zhao
  author_string: Zhuosheng Zhang, Hai Zhao
  bibkey: zhang-zhao-2018-one
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '449'
  page_last: '461'
  pages: "449\u2013461"
  paper_id: '38'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1038.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1038.jpg
  title: One-shot Learning for Question-Answering in Gaokao History Challenge
  title_html: One-shot Learning for Question-Answering in <span class="acl-fixed-case">G</span>aokao
    History Challenge
  url: https://www.aclweb.org/anthology/C18-1038
  year: '2018'
C18-1039:
  abstract: "Sentence simplification aims to improve readability and understandability,\
    \ based on several operations such as splitting, deletion, and paraphrasing. However,\
    \ a valid simplified sentence should also be logically entailed by its input sentence.\
    \ In this work, we first present a strong pointer-copy mechanism based sequence-to-sequence\
    \ sentence simplification model, and then improve its entailment and paraphrasing\
    \ capabilities via multi-task learning with related auxiliary tasks of entailment\
    \ and paraphrase generation. Moreover, we propose a novel \u2018multi-level\u2019\
    \ layered soft sharing approach where each auxiliary task shares different (higher\
    \ versus lower) level layers of the sentence simplification model, depending on\
    \ the task\u2019s semantic versus lexico-syntactic nature. We also introduce a\
    \ novel multi-armed bandit based training approach that dynamically learns how\
    \ to effectively switch across tasks during multi-task learning. Experiments on\
    \ multiple popular datasets demonstrate that our model outperforms competitive\
    \ simplification systems in SARI and FKGL automatic metrics, and human evaluation.\
    \ Further, we present several ablation analyses on alternative layer sharing methods,\
    \ soft versus hard sharing, dynamic multi-armed bandit sampling approaches, and\
    \ our model\u2019s learned entailment and paraphrasing skills."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Han
    full: Han Guo
    id: han-guo
    last: Guo
  - first: Ramakanth
    full: Ramakanth Pasunuru
    id: ramakanth-pasunuru
    last: Pasunuru
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  author_string: Han Guo, Ramakanth Pasunuru, Mohit Bansal
  bibkey: guo-etal-2018-dynamic
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '462'
  page_last: '476'
  pages: "462\u2013476"
  paper_id: '39'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1039.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1039.jpg
  title: Dynamic Multi-Level Multi-Task Learning for Sentence Simplification
  title_html: Dynamic Multi-Level Multi-Task Learning for Sentence Simplification
  url: https://www.aclweb.org/anthology/C18-1039
  year: '2018'
C18-1040:
  abstract: 'Targeting the database search dialogue, we propose to utilise information
    in the user utterances that do not directly mention the database (DB) field of
    the backend database system but are useful for constructing database queries.
    We call this kind of information implicit conditions. Interpreting the implicit
    conditions enables the dialogue system more natural and efficient in communicating
    with humans. We formalised the interpretation of the implicit conditions as classifying
    user utterances into the related DB field while identifying the evidence for that
    classification at the same time. Introducing this new task is one of the contributions
    of this paper. We implemented two models for this task: an SVM-based model and
    an RCNN-based model. Through the evaluation using a corpus of simulated dialogues
    between a real estate agent and a customer, we found that the SVM-based model
    showed better performance than the RCNN-based model.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Shunya
    full: Shunya Fukunaga
    id: shun-ya-fukunaga
    last: Fukunaga
  - first: Hitoshi
    full: Hitoshi Nishikawa
    id: hitoshi-nishikawa
    last: Nishikawa
  - first: Takenobu
    full: Takenobu Tokunaga
    id: takenobu-tokunaga
    last: Tokunaga
  - first: Hikaru
    full: Hikaru Yokono
    id: hikaru-yokono
    last: Yokono
  - first: Tetsuro
    full: Tetsuro Takahashi
    id: tetsuro-takahashi
    last: Takahashi
  author_string: Shunya Fukunaga, Hitoshi Nishikawa, Takenobu Tokunaga, Hikaru Yokono,
    Tetsuro Takahashi
  bibkey: fukunaga-etal-2018-interpretation
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '477'
  page_last: '486'
  pages: "477\u2013486"
  paper_id: '40'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1040.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1040.jpg
  title: Interpretation of Implicit Conditions in Database Search Dialogues
  title_html: Interpretation of Implicit Conditions in Database Search Dialogues
  url: https://www.aclweb.org/anthology/C18-1040
  year: '2018'
C18-1041:
  abstract: Automatic charge prediction aims to predict the final charges according
    to the fact descriptions in criminal cases and plays a crucial role in legal assistant
    systems. Existing works on charge prediction perform adequately on those high-frequency
    charges but are not yet capable of predicting few-shot charges with limited cases.
    Moreover, these exist many confusing charge pairs, whose fact descriptions are
    fairly similar to each other. To address these issues, we introduce several discriminative
    attributes of charges as the internal mapping between fact descriptions and charges.
    These attributes provide additional information for few-shot charges, as well
    as effective signals for distinguishing confusing charges. More specifically,
    we propose an attribute-attentive charge prediction model to infer the attributes
    and charges simultaneously. Experimental results on real-work datasets demonstrate
    that our proposed model achieves significant and consistent improvements than
    other state-of-the-art baselines. Specifically, our model outperforms other baselines
    by more than 50% in the few-shot scenario. Our codes and datasets can be obtained
    from https://github.com/thunlp/attribute_charge.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Zikun
    full: Zikun Hu
    id: zikun-hu
    last: Hu
  - first: Xiang
    full: Xiang Li
    id: xiang-li
    last: Li
  - first: Cunchao
    full: Cunchao Tu
    id: cunchao-tu
    last: Tu
  - first: Zhiyuan
    full: Zhiyuan Liu
    id: zhiyuan-liu
    last: Liu
  - first: Maosong
    full: Maosong Sun
    id: maosong-sun
    last: Sun
  author_string: Zikun Hu, Xiang Li, Cunchao Tu, Zhiyuan Liu, Maosong Sun
  bibkey: hu-etal-2018-shot
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '487'
  page_last: '498'
  pages: "487\u2013498"
  paper_id: '41'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1041.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1041.jpg
  title: Few-Shot Charge Prediction with Discriminative Legal Attributes
  title_html: Few-Shot Charge Prediction with Discriminative Legal Attributes
  url: https://www.aclweb.org/anthology/C18-1041
  year: '2018'
C18-1042:
  abstract: In this paper, we propose a hybrid technique for semantic question matching.
    It uses a proposed two-layered taxonomy for English questions by augmenting state-of-the-art
    deep learning models with question classes obtained from a deep learning based
    question classifier. Experiments performed on three open-domain datasets demonstrate
    the effectiveness of our proposed approach. We achieve state-of-the-art results
    on partial ordering question ranking (POQR) benchmark dataset. Our empirical analysis
    shows that coupling standard distributional features (provided by the question
    encoder) with knowledge from taxonomy is more effective than either deep learning
    or taxonomy-based knowledge alone.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Deepak
    full: Deepak Gupta
    id: deepak-gupta
    last: Gupta
  - first: Rajkumar
    full: Rajkumar Pujari
    id: rajkumar-pujari
    last: Pujari
  - first: Asif
    full: Asif Ekbal
    id: asif-ekbal
    last: Ekbal
  - first: Pushpak
    full: Pushpak Bhattacharyya
    id: pushpak-bhattacharyya
    last: Bhattacharyya
  - first: Anutosh
    full: Anutosh Maitra
    id: anutosh-maitra
    last: Maitra
  - first: Tom
    full: Tom Jain
    id: tom-jain
    last: Jain
  - first: Shubhashis
    full: Shubhashis Sengupta
    id: shubhashis-sengupta
    last: Sengupta
  author_string: Deepak Gupta, Rajkumar Pujari, Asif Ekbal, Pushpak Bhattacharyya,
    Anutosh Maitra, Tom Jain, Shubhashis Sengupta
  bibkey: gupta-etal-2018-taxonomy
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '499'
  page_last: '513'
  pages: "499\u2013513"
  paper_id: '42'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1042.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1042.jpg
  title: Can Taxonomy Help? Improving Semantic Question Matching using Question Taxonomy
  title_html: Can Taxonomy Help? Improving Semantic Question Matching using Question
    Taxonomy
  url: https://www.aclweb.org/anthology/C18-1042
  year: '2018'
C18-1043:
  abstract: "We propose a sketch-based two-step neural model for generating structured\
    \ queries (SQL) based on a user\u2019s request in natural language. The sketch\
    \ is obtained by using placeholders for specific entities in the SQL query, such\
    \ as column names, table names, aliases and variables, in a process similar to\
    \ semantic parsing. The first step is to apply a sequence-to-sequence (SEQ2SEQ)\
    \ model to determine the most probable SQL sketch based on the request in natural\
    \ language. Then, a second network designed as a dual-encoder SEQ2SEQ model using\
    \ both the text query and the previously obtained sketch is employed to generate\
    \ the final SQL query. Our approach shows improvements over previous approaches\
    \ on two recent large datasets (WikiSQL and SENLIDB) suitable for data-driven\
    \ solutions for natural language interfaces for databases."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Ionel Alexandru
    full: Ionel Alexandru Hosu
    id: ionel-alexandru-hosu
    last: Hosu
  - first: Radu Cristian Alexandru
    full: Radu Cristian Alexandru Iacob
    id: radu-cristian-alexandru-iacob
    last: Iacob
  - first: Florin
    full: Florin Brad
    id: florin-brad
    last: Brad
  - first: Stefan
    full: Stefan Ruseti
    id: stefan-ruseti
    last: Ruseti
  - first: Traian
    full: Traian Rebedea
    id: traian-rebedea
    last: Rebedea
  author_string: Ionel Alexandru Hosu, Radu Cristian Alexandru Iacob, Florin Brad,
    Stefan Ruseti, Traian Rebedea
  bibkey: hosu-etal-2018-natural
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '514'
  page_last: '524'
  pages: "514\u2013524"
  paper_id: '43'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1043.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1043.jpg
  title: Natural Language Interface for Databases Using a Dual-Encoder Model
  title_html: Natural Language Interface for Databases Using a Dual-Encoder Model
  url: https://www.aclweb.org/anthology/C18-1043
  year: '2018'
C18-1044:
  abstract: The task of nuclearity recognition in Chinese discourse remains challenging
    due to the demand for more deep semantic information. In this paper, we propose
    a novel text matching network (TMN) that encodes the discourse units and the paragraphs
    by combining Bi-LSTM and CNN to capture both global dependency information and
    local n-gram information. Moreover, it introduces three components of text matching,
    the Cosine, Bilinear and Single Layer Network, to incorporate various similarities
    and interactions among the discourse units. Experimental results on the Chinese
    Discourse TreeBank show that our proposed TMN model significantly outperforms
    various strong baselines in both micro-F1 and macro-F1.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Sheng
    full: Sheng Xu
    id: sheng-xu
    last: Xu
  - first: Peifeng
    full: Peifeng Li
    id: peifeng-li
    last: Li
  - first: Guodong
    full: Guodong Zhou
    id: guodong-zhou
    last: Zhou
  - first: Qiaoming
    full: Qiaoming Zhu
    id: qiaoming-zhu
    last: Zhu
  author_string: Sheng Xu, Peifeng Li, Guodong Zhou, Qiaoming Zhu
  bibkey: xu-etal-2018-employing
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '525'
  page_last: '535'
  pages: "525\u2013535"
  paper_id: '44'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1044.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1044.jpg
  title: Employing Text Matching Network to Recognise Nuclearity in Chinese Discourse
  title_html: Employing Text Matching Network to Recognise Nuclearity in <span class="acl-fixed-case">C</span>hinese
    Discourse
  url: https://www.aclweb.org/anthology/C18-1044
  year: '2018'
C18-1045:
  abstract: Discourse parsing is a challenging task and plays a critical role in discourse
    analysis. This paper focus on the macro level discourse structure analysis, which
    has been less studied in the previous researches. We explore a macro discourse
    structure presentation schema to present the macro level discourse structure,
    and propose a corresponding corpus, named Macro Chinese Discourse Treebank. On
    these bases, we concentrate on two tasks of macro discourse structure analysis,
    including structure identification and nuclearity recognition. In order to reduce
    the error transmission between the associated tasks, we adopt a joint model of
    the two tasks, and an Integer Linear Programming approach is proposed to achieve
    global optimization with various kinds of constraints.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Xiaomin
    full: Xiaomin Chu
    id: xiaomin-chu
    last: Chu
  - first: Feng
    full: Feng Jiang
    id: feng-jiang
    last: Jiang
  - first: Yi
    full: Yi Zhou
    id: yi-zhou
    last: Zhou
  - first: Guodong
    full: Guodong Zhou
    id: guodong-zhou
    last: Zhou
  - first: Qiaoming
    full: Qiaoming Zhu
    id: qiaoming-zhu
    last: Zhu
  author_string: Xiaomin Chu, Feng Jiang, Yi Zhou, Guodong Zhou, Qiaoming Zhu
  bibkey: chu-etal-2018-joint
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '536'
  page_last: '546'
  pages: "536\u2013546"
  paper_id: '45'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1045.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1045.jpg
  title: Joint Modeling of Structure Identification and Nuclearity Recognition in
    Macro Chinese Discourse Treebank
  title_html: Joint Modeling of Structure Identification and Nuclearity Recognition
    in Macro <span class="acl-fixed-case">C</span>hinese Discourse Treebank
  url: https://www.aclweb.org/anthology/C18-1045
  year: '2018'
C18-1046:
  abstract: Implicit discourse relation recognition aims to understand and annotate
    the latent relations between two discourse arguments, such as temporal, comparison,
    etc. Most previous methods encode two discourse arguments separately, the ones
    considering pair specific clues ignore the bidirectional interactions between
    two arguments and the sparsity of pair patterns. In this paper, we propose a novel
    neural Tensor network framework with Interactive Attention and Sparse Learning
    (TIASL) for implicit discourse relation recognition. (1) We mine the most correlated
    word pairs from two discourse arguments to model pair specific clues, and integrate
    them as interactive attention into argument representations produced by the bidirectional
    long short-term memory network. Meanwhile, (2) the neural tensor network with
    sparse constraint is proposed to explore the deeper and the more important pair
    patterns so as to fully recognize discourse relations. The experimental results
    on PDTB show that our proposed TIASL framework is effective.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Fengyu
    full: Fengyu Guo
    id: fengyu-guo
    last: Guo
  - first: Ruifang
    full: Ruifang He
    id: ruifang-he
    last: He
  - first: Di
    full: Di Jin
    id: di-jin
    last: Jin
  - first: Jianwu
    full: Jianwu Dang
    id: jianwu-dang
    last: Dang
  - first: Longbiao
    full: Longbiao Wang
    id: longbiao-wang
    last: Wang
  - first: Xiangang
    full: Xiangang Li
    id: xiangang-li
    last: Li
  author_string: Fengyu Guo, Ruifang He, Di Jin, Jianwu Dang, Longbiao Wang, Xiangang
    Li
  bibkey: guo-etal-2018-implicit
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '547'
  page_last: '558'
  pages: "547\u2013558"
  paper_id: '46'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1046.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1046.jpg
  title: Implicit Discourse Relation Recognition using Neural Tensor Network with
    Interactive Attention and Sparse Learning
  title_html: Implicit Discourse Relation Recognition using Neural Tensor Network
    with Interactive Attention and Sparse Learning
  url: https://www.aclweb.org/anthology/C18-1046
  year: '2018'
C18-1047:
  abstract: Syntax has been a useful source of information for statistical RST discourse
    parsing. Under the neural setting, a common approach integrates syntax by a recursive
    neural network (RNN), requiring discrete output trees produced by a supervised
    syntax parser. In this paper, we propose an implicit syntax feature extraction
    approach, using hidden-layer vectors extracted from a neural syntax parser. In
    addition, we propose a simple transition-based model as the baseline, further
    enhancing it with dynamic oracle. Experiments on the standard dataset show that
    our baseline model with dynamic oracle is highly competitive. When implicit syntax
    features are integrated, we are able to obtain further improvements, better than
    using explicit Tree-RNN.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Nan
    full: Nan Yu
    id: nan-yu
    last: Yu
  - first: Meishan
    full: Meishan Zhang
    id: meishan-zhang
    last: Zhang
  - first: Guohong
    full: Guohong Fu
    id: guohong-fu
    last: Fu
  author_string: Nan Yu, Meishan Zhang, Guohong Fu
  bibkey: yu-etal-2018-transition
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '559'
  page_last: '570'
  pages: "559\u2013570"
  paper_id: '47'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1047.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1047.jpg
  title: Transition-based Neural RST Parsing with Implicit Syntax Features
  title_html: Transition-based Neural <span class="acl-fixed-case">RST</span> Parsing
    with Implicit Syntax Features
  url: https://www.aclweb.org/anthology/C18-1047
  year: '2018'
C18-1048:
  abstract: Implicit discourse relation recognition is a challenging task as the relation
    prediction without explicit connectives in discourse parsing needs understanding
    of text spans and cannot be easily derived from surface features from the input
    sentence pairs. Thus, properly representing the text is very crucial to this task.
    In this paper, we propose a model augmented with different grained text representations,
    including character, subword, word, sentence, and sentence pair levels. The proposed
    deeper model is evaluated on the benchmark treebank and achieves state-of-the-art
    accuracy with greater than 48% in 11-way and F1 score greater than 50% in 4-way
    classifications for the first time according to our best knowledge.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Hongxiao
    full: Hongxiao Bai
    id: hongxiao-bai
    last: Bai
  - first: Hai
    full: Hai Zhao
    id: hai-zhao
    last: Zhao
  author_string: Hongxiao Bai, Hai Zhao
  bibkey: bai-zhao-2018-deep
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '571'
  page_last: '583'
  pages: "571\u2013583"
  paper_id: '48'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1048.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1048.jpg
  title: Deep Enhanced Representation for Implicit Discourse Relation Recognition
  title_html: Deep Enhanced Representation for Implicit Discourse Relation Recognition
  url: https://www.aclweb.org/anthology/C18-1048
  year: '2018'
C18-1049:
  abstract: Identifying discourse relations that are not overtly marked with discourse
    connectives remains a challenging problem. The absence of explicit clues indicates
    a need for the combination of world knowledge and weak contextual clues, which
    can hardly be learned from a small amount of manually annotated data. In this
    paper, we address this problem by augmenting the input text with external knowledge
    and context and by adopting a neural network model that can effectively handle
    the augmented text. Experiments show that external knowledge did improve the classification
    accuracy. Contextual information provided no significant gain for implicit discourse
    relations, but it did for explicit ones.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yudai
    full: Yudai Kishimoto
    id: yudai-kishimoto
    last: Kishimoto
  - first: Yugo
    full: Yugo Murawaki
    id: yugo-murawaki
    last: Murawaki
  - first: Sadao
    full: Sadao Kurohashi
    id: sadao-kurohashi
    last: Kurohashi
  author_string: Yudai Kishimoto, Yugo Murawaki, Sadao Kurohashi
  bibkey: kishimoto-etal-2018-knowledge
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '584'
  page_last: '595'
  pages: "584\u2013595"
  paper_id: '49'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1049.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1049.jpg
  title: A Knowledge-Augmented Neural Network Model for Implicit Discourse Relation
    Classification
  title_html: A Knowledge-Augmented Neural Network Model for Implicit Discourse Relation
    Classification
  url: https://www.aclweb.org/anthology/C18-1049
  year: '2018'
C18-1050:
  abstract: 'Sentences in a well-formed text are connected to each other via various
    links to form the cohesive structure of the text. Current neural machine translation
    (NMT) systems translate a text in a conventional sentence-by-sentence fashion,
    ignoring such cross-sentence links and dependencies. This may lead to generate
    an incoherent target text for a coherent source text. In order to handle this
    issue, we propose a cache-based approach to modeling coherence for neural machine
    translation by capturing contextual information either from recently translated
    sentences or the entire document. Particularly, we explore two types of caches:
    a dynamic cache, which stores words from the best translation hypotheses of preceding
    sentences, and a topic cache, which maintains a set of target-side topical words
    that are semantically related to the document to be translated. On this basis,
    we build a new layer to score target words in these two caches with a cache-based
    neural model. Here the estimated probabilities from the cache-based neural model
    are combined with NMT probabilities into the final word prediction probabilities
    via a gating mechanism. Finally, the proposed cache-based neural model is trained
    jointly with NMT system in an end-to-end manner. Experiments and analysis presented
    in this paper demonstrate that the proposed cache-based model achieves substantial
    improvements over several state-of-the-art SMT and NMT baselines.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Shaohui
    full: Shaohui Kuang
    id: shaohui-kuang
    last: Kuang
  - first: Deyi
    full: Deyi Xiong
    id: deyi-xiong
    last: Xiong
  - first: Weihua
    full: Weihua Luo
    id: weihua-luo
    last: Luo
  - first: Guodong
    full: Guodong Zhou
    id: guodong-zhou
    last: Zhou
  author_string: Shaohui Kuang, Deyi Xiong, Weihua Luo, Guodong Zhou
  bibkey: kuang-etal-2018-modeling
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '596'
  page_last: '606'
  pages: "596\u2013606"
  paper_id: '50'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1050.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1050.jpg
  title: Modeling Coherence for Neural Machine Translation with Dynamic and Topic
    Caches
  title_html: Modeling Coherence for Neural Machine Translation with Dynamic and Topic
    Caches
  url: https://www.aclweb.org/anthology/C18-1050
  year: '2018'
C18-1051:
  abstract: Neural machine translation (NMT) systems are usually trained on a large
    amount of bilingual sentence pairs and translate one sentence at a time, ignoring
    inter-sentence information. This may make the translation of a sentence ambiguous
    or even inconsistent with the translations of neighboring sentences. In order
    to handle this issue, we propose an inter-sentence gate model that uses the same
    encoder to encode two adjacent sentences and controls the amount of information
    flowing from the preceding sentence to the translation of the current sentence
    with an inter-sentence gate. In this way, our proposed model can capture the connection
    between sentences and fuse recency from neighboring sentences into neural machine
    translation. On several NIST Chinese-English translation tasks, our experiments
    demonstrate that the proposed inter-sentence gate model achieves substantial improvements
    over the baseline.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Shaohui
    full: Shaohui Kuang
    id: shaohui-kuang
    last: Kuang
  - first: Deyi
    full: Deyi Xiong
    id: deyi-xiong
    last: Xiong
  author_string: Shaohui Kuang, Deyi Xiong
  bibkey: kuang-xiong-2018-fusing
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '607'
  page_last: '617'
  pages: "607\u2013617"
  paper_id: '51'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1051.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1051.jpg
  title: Fusing Recency into Neural Machine Translation with an Inter-Sentence Gate
    Model
  title_html: Fusing Recency into Neural Machine Translation with an Inter-Sentence
    Gate Model
  url: https://www.aclweb.org/anthology/C18-1051
  year: '2018'
C18-1052:
  abstract: 'This paper focuses on subword-based Neural Machine Translation (NMT).
    We hypothesize that in the NMT model, the appropriate subword units for the following
    three modules (layers) can differ: (1) the encoder embedding layer, (2) the decoder
    embedding layer, and (3) the decoder output layer. We find the subword based on
    Sennrich et al. (2016) has a feature that a large vocabulary is a superset of
    a small vocabulary and modify the NMT model enables the incorporation of several
    different subword units in a single embedding layer. We refer these small subword
    features as hierarchical subword features. To empirically investigate our assumption,
    we compare the performance of several different subword units and hierarchical
    subword features for both the encoder and decoder embedding layers. We confirmed
    that incorporating hierarchical subword features in the encoder consistently improves
    BLEU scores on the IWSLT evaluation datasets.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Makoto
    full: Makoto Morishita
    id: makoto-morishita
    last: Morishita
  - first: Jun
    full: Jun Suzuki
    id: jun-suzuki
    last: Suzuki
  - first: Masaaki
    full: Masaaki Nagata
    id: masaaki-nagata
    last: Nagata
  author_string: Makoto Morishita, Jun Suzuki, Masaaki Nagata
  bibkey: morishita-etal-2018-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '618'
  page_last: '629'
  pages: "618\u2013629"
  paper_id: '52'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1052.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1052.jpg
  title: Improving Neural Machine Translation by Incorporating Hierarchical Subword
    Features
  title_html: Improving Neural Machine Translation by Incorporating Hierarchical Subword
    Features
  url: https://www.aclweb.org/anthology/C18-1052
  year: '2018'
C18-1053:
  abstract: 'We analyze some of the fundamental design challenges that impact the
    development of a multilingual state-of-the-art named entity transliteration system,
    including curating bilingual named entity datasets and evaluation of multiple
    transliteration methods. We empirically evaluate the transliteration task using
    the traditional weighted finite state transducer (WFST) approach against two neural
    approaches: the encoder-decoder recurrent neural network method and the recent,
    non-sequential Transformer method. In order to improve availability of bilingual
    named entity transliteration datasets, we release personal name bilingual dictionaries
    mined from Wikidata for English to Russian, Hebrew, Arabic, and Japanese Katakana.
    Our code and dictionaries are publicly available.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yuval
    full: Yuval Merhav
    id: yuval-merhav
    last: Merhav
  - first: Stephen
    full: Stephen Ash
    id: stephen-ash
    last: Ash
  author_string: Yuval Merhav, Stephen Ash
  bibkey: merhav-ash-2018-design
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '630'
  page_last: '640'
  pages: "630\u2013640"
  paper_id: '53'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1053.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1053.jpg
  title: Design Challenges in Named Entity Transliteration
  title_html: Design Challenges in Named Entity Transliteration
  url: https://www.aclweb.org/anthology/C18-1053
  year: '2018'
C18-1054:
  abstract: Recently, neural machine translation (NMT) has been extended to multilinguality,
    that is to handle more than one translation direction with a single system. Multilingual
    NMT showed competitive performance against pure bilingual systems. Notably, in
    low-resource settings, it proved to work effectively and efficiently, thanks to
    shared representation space that is forced across languages and induces a sort
    of transfer-learning. Furthermore, multilingual NMT enables so-called zero-shot
    inference across language pairs never seen at training time. Despite the increasing
    interest in this framework, an in-depth analysis of what a multilingual NMT model
    is capable of and what it is not is still missing. Motivated by this, our work
    (i) provides a quantitative and comparative analysis of the translations produced
    by bilingual, multilingual and zero-shot systems; (ii) investigates the translation
    quality of two of the currently dominant neural architectures in MT, which are
    the Recurrent and the Transformer ones; and (iii) quantitatively explores how
    the closeness between languages influences the zero-shot translation. Our analysis
    leverages multiple professional post-edits of automatic translations by several
    different systems and focuses both on automatic standard metrics (BLEU and TER)
    and on widely used error categories, which are lexical, morphology, and word order
    errors.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Surafel Melaku
    full: Surafel Melaku Lakew
    id: surafel-melaku-lakew
    last: Lakew
  - first: Mauro
    full: Mauro Cettolo
    id: mauro-cettolo
    last: Cettolo
  - first: Marcello
    full: Marcello Federico
    id: marcello-federico
    last: Federico
  author_string: Surafel Melaku Lakew, Mauro Cettolo, Marcello Federico
  bibkey: lakew-etal-2018-comparison
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '641'
  page_last: '652'
  pages: "641\u2013652"
  paper_id: '54'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1054.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1054.jpg
  title: A Comparison of Transformer and Recurrent Neural Networks on Multilingual
    Neural Machine Translation
  title_html: A Comparison of Transformer and Recurrent Neural Networks on Multilingual
    Neural Machine Translation
  url: https://www.aclweb.org/anthology/C18-1054
  year: '2018'
C18-1055:
  abstract: "Evaluating on adversarial examples has become a standard procedure to\
    \ measure robustness of deep learning models. Due to the difficulty of creating\
    \ white-box adversarial examples for discrete text input, most analyses of the\
    \ robustness of NLP models have been done through black-box adversarial examples.\
    \ We investigate adversarial examples for character-level neural machine translation\
    \ (NMT), and contrast black-box adversaries with a novel white-box adversary,\
    \ which employs differentiable string-edit operations to rank adversarial changes.\
    \ We propose two novel types of attacks which aim to remove or change a word in\
    \ a translation, rather than simply break the NMT. We demonstrate that white-box\
    \ adversarial examples are significantly stronger than their black-box counterparts\
    \ in different attack scenarios, which show more serious vulnerabilities than\
    \ previously known. In addition, after performing adversarial training, which\
    \ takes only 3 times longer than regular training, we can improve the model\u2019\
    s robustness significantly."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Javid
    full: Javid Ebrahimi
    id: javid-ebrahimi
    last: Ebrahimi
  - first: Daniel
    full: Daniel Lowd
    id: daniel-lowd
    last: Lowd
  - first: Dejing
    full: Dejing Dou
    id: dejing-dou
    last: Dou
  author_string: Javid Ebrahimi, Daniel Lowd, Dejing Dou
  bibkey: ebrahimi-etal-2018-adversarial
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '653'
  page_last: '663'
  pages: "653\u2013663"
  paper_id: '55'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1055.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1055.jpg
  title: On Adversarial Examples for Character-Level Neural Machine Translation
  title_html: On Adversarial Examples for Character-Level Neural Machine Translation
  url: https://www.aclweb.org/anthology/C18-1055
  year: '2018'
C18-1056:
  abstract: State-of-the-art entity linkers achieve high accuracy scores with probabilistic
    methods. However, these scores should be considered in relation to the properties
    of the datasets they are evaluated on. Until now, there has not been a systematic
    investigation of the properties of entity linking datasets and their impact on
    system performance. In this paper we report on a series of hypotheses regarding
    the long tail phenomena in entity linking datasets, their interaction, and their
    impact on system performance. Our systematic study of these hypotheses shows that
    evaluation datasets mainly capture head entities and only incidentally cover data
    from the tail, thus encouraging systems to overfit to popular/frequent and non-ambiguous
    cases. We find the most difficult cases of entity linking among the infrequent
    candidates of ambiguous forms. With our findings, we hope to inspire future designs
    of both entity linking systems and evaluation datasets. To support this goal,
    we provide a list of recommended actions for better inclusion of tail cases.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Filip
    full: Filip Ilievski
    id: filip-ilievski
    last: Ilievski
  - first: Piek
    full: Piek Vossen
    id: piek-vossen
    last: Vossen
  - first: Stefan
    full: Stefan Schlobach
    id: stefan-schlobach
    last: Schlobach
  author_string: Filip Ilievski, Piek Vossen, Stefan Schlobach
  bibkey: ilievski-etal-2018-systematic
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '664'
  page_last: '674'
  pages: "664\u2013674"
  paper_id: '56'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1056.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1056.jpg
  title: Systematic Study of Long Tail Phenomena in Entity Linking
  title_html: Systematic Study of Long Tail Phenomena in Entity Linking
  url: https://www.aclweb.org/anthology/C18-1056
  year: '2018'
C18-1057:
  abstract: Entity Linking aims to link entity mentions in texts to knowledge bases,
    and neural models have achieved recent success in this task. However, most existing
    methods rely on local contexts to resolve entities independently, which may usually
    fail due to the data sparsity of local information. To address this issue, we
    propose a novel neural model for collective entity linking, named as NCEL. NCEL
    apply Graph Convolutional Network to integrate both local contextual features
    and global coherence information for entity linking. To improve the computation
    efficiency, we approximately perform graph convolution on a subgraph of adjacent
    entity mentions instead of those in the entire text. We further introduce an attention
    scheme to improve the robustness of NCEL to data noise and train the model on
    Wikipedia hyperlinks to avoid overfitting and domain bias. In experiments, we
    evaluate NCEL on five publicly available datasets to verify the linking performance
    as well as generalization ability. We also conduct an extensive analysis of time
    complexity, the impact of key modules, and qualitative results, which demonstrate
    the effectiveness and efficiency of our proposed method.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yixin
    full: Yixin Cao
    id: yixin-cao
    last: Cao
  - first: Lei
    full: Lei Hou
    id: lei-hou
    last: Hou
  - first: Juanzi
    full: Juanzi Li
    id: juanzi-li
    last: Li
  - first: Zhiyuan
    full: Zhiyuan Liu
    id: zhiyuan-liu
    last: Liu
  author_string: Yixin Cao, Lei Hou, Juanzi Li, Zhiyuan Liu
  bibkey: cao-etal-2018-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '675'
  page_last: '686'
  pages: "675\u2013686"
  paper_id: '57'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1057.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1057.jpg
  title: Neural Collective Entity Linking
  title_html: Neural Collective Entity Linking
  url: https://www.aclweb.org/anthology/C18-1057
  year: '2018'
C18-1058:
  abstract: Fundamental to several knowledge-centric applications is the need to identify
    named entities from their textual mentions. However, entities lack a unique representation
    and their mentions can differ greatly. These variations arise in complex ways
    that cannot be captured using textual similarity metrics. However, entities have
    underlying structures, typically shared by entities of the same entity type, that
    can help reason over their name variations. Discovering, learning and manipulating
    these structures typically requires high manual effort in the form of large amounts
    of labeled training data and handwritten transformation programs. In this work,
    we propose an active-learning based framework that drastically reduces the labeled
    data required to learn the structures of entities. We show that programs for mapping
    entity mentions to their structures can be automatically generated using human-comprehensible
    labels. Our experiments show that our framework consistently outperforms both
    handwritten programs and supervised learning models. We also demonstrate the utility
    of our framework in relation extraction and entity resolution tasks.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Nikita
    full: Nikita Bhutani
    id: nikita-bhutani
    last: Bhutani
  - first: Kun
    full: Kun Qian
    id: kun-qian
    last: Qian
  - first: Yunyao
    full: Yunyao Li
    id: yunyao-li
    last: Li
  - first: H. V.
    full: H. V. Jagadish
    id: h-v-jagadish
    last: Jagadish
  - first: Mauricio
    full: Mauricio Hernandez
    id: mauricio-hernandez
    last: Hernandez
  - first: Mitesh
    full: Mitesh Vasa
    id: mitesh-vasa
    last: Vasa
  author_string: Nikita Bhutani, Kun Qian, Yunyao Li, H. V. Jagadish, Mauricio Hernandez,
    Mitesh Vasa
  bibkey: bhutani-etal-2018-exploiting
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '687'
  page_last: '699'
  pages: "687\u2013699"
  paper_id: '58'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1058.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1058.jpg
  title: Exploiting Structure in Representation of Named Entities using Active Learning
  title_html: Exploiting Structure in Representation of Named Entities using Active
    Learning
  url: https://www.aclweb.org/anthology/C18-1058
  year: '2018'
C18-1059:
  abstract: This work addresses challenges arising from extracting entities from textual
    data, including the high cost of data annotation, model accuracy, selecting appropriate
    evaluation criteria, and the overall quality of annotation. We present a framework
    that integrates Entity Set Expansion (ESE) and Active Learning (AL) to reduce
    the annotation cost of sparse data and provide an online evaluation method as
    feedback. This incremental and interactive learning framework allows for rapid
    annotation and subsequent extraction of sparse data while maintaining high accuracy.
    We evaluate our framework on three publicly available datasets and show that it
    drastically reduces the cost of sparse entity annotation by an average of 85%
    and 45% to reach 0.9 and 1.0 F-Scores respectively. Moreover, the method exhibited
    robust performance across all datasets.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Hussein
    full: Hussein Al-Olimat
    id: hussein-al-olimat
    last: Al-Olimat
  - first: Steven
    full: Steven Gustafson
    id: steven-gustafson
    last: Gustafson
  - first: Jason
    full: Jason Mackay
    id: jason-mackay
    last: Mackay
  - first: Krishnaprasad
    full: Krishnaprasad Thirunarayan
    id: krishnaprasad-thirunarayan
    last: Thirunarayan
  - first: Amit
    full: Amit Sheth
    id: amit-sheth
    last: Sheth
  author_string: Hussein Al-Olimat, Steven Gustafson, Jason Mackay, Krishnaprasad
    Thirunarayan, Amit Sheth
  bibkey: al-olimat-etal-2018-practical
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '700'
  page_last: '710'
  pages: "700\u2013710"
  paper_id: '59'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1059.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1059.jpg
  title: A Practical Incremental Learning Framework For Sparse Entity Extraction
  title_html: A Practical Incremental Learning Framework For Sparse Entity Extraction
  url: https://www.aclweb.org/anthology/C18-1059
  year: '2018'
C18-1060:
  abstract: Named entity recognition (NER) has attracted a substantial amount of research.
    Recently, several neural network-based models have been proposed and achieved
    high performance. However, there is little research on fine-grained NER (FG-NER),
    in which hundreds of named entity categories must be recognized, especially for
    non-English languages. It is still an open question whether there is a model that
    is robust across various settings or the proper model varies depending on the
    language, the number of named entity categories, and the size of training datasets.
    This paper first presents an empirical comparison of FG-NER models for English
    and Japanese and demonstrates that LSTM+CNN+CRF (Ma and Hovy, 2016), one of the
    state-of-the-art methods for English NER, also works well for English FG-NER but
    does not work well for Japanese, a language that has a large number of character
    types. To tackle this problem, we propose a method to improve the neural network-based
    Japanese FG-NER performance by removing the CNN layer and utilizing dictionary
    and category embeddings. Experiment results show that the proposed method improves
    Japanese FG-NER F-score from 66.76% to 75.18%.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Khai
    full: Khai Mai
    id: khai-mai
    last: Mai
  - first: Thai-Hoang
    full: Thai-Hoang Pham
    id: thai-hoang-pham
    last: Pham
  - first: Minh Trung
    full: Minh Trung Nguyen
    id: minh-trung-nguyen
    last: Nguyen
  - first: Tuan Duc
    full: Tuan Duc Nguyen
    id: tuan-duc-nguyen
    last: Nguyen
  - first: Danushka
    full: Danushka Bollegala
    id: danushka-bollegala
    last: Bollegala
  - first: Ryohei
    full: Ryohei Sasano
    id: ryohei-sasano
    last: Sasano
  - first: Satoshi
    full: Satoshi Sekine
    id: satoshi-sekine
    last: Sekine
  author_string: Khai Mai, Thai-Hoang Pham, Minh Trung Nguyen, Tuan Duc Nguyen, Danushka
    Bollegala, Ryohei Sasano, Satoshi Sekine
  bibkey: mai-etal-2018-empirical
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '711'
  page_last: '722'
  pages: "711\u2013722"
  paper_id: '60'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1060.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1060.jpg
  title: An Empirical Study on Fine-Grained Named Entity Recognition
  title_html: An Empirical Study on Fine-Grained Named Entity Recognition
  url: https://www.aclweb.org/anthology/C18-1060
  year: '2018'
C18-1061:
  abstract: "Existing neural models usually predict the tag of the current token independent\
    \ of the neighboring tags. The popular LSTM-CRF model considers the tag dependencies\
    \ between every two consecutive tags. However, it is hard for existing neural\
    \ models to take longer distance dependencies between tags into consideration.\
    \ The scalability is mainly limited by the complex model structures and the cost\
    \ of dynamic programming during training. In our work, we first design a new model\
    \ called \u201Chigh order LSTM\u201D to predict multiple tags for the current\
    \ token which contains not only the current tag but also the previous several\
    \ tags. We call the number of tags in one prediction as \u201Corder\u201D. Then\
    \ we propose a new method called Multi-Order BiLSTM (MO-BiLSTM) which combines\
    \ low order and high order LSTMs together. MO-BiLSTM keeps the scalability to\
    \ high order models with a pruning technique. We evaluate MO-BiLSTM on all-phrase\
    \ chunking and NER datasets. Experiment results show that MO-BiLSTM achieves the\
    \ state-of-the-art result in chunking and highly competitive results in two NER\
    \ datasets."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yi
    full: Yi Zhang
    id: yi-zhang
    last: Zhang
  - first: Xu
    full: Xu Sun
    id: xu-sun
    last: Sun
  - first: Shuming
    full: Shuming Ma
    id: shuming-ma
    last: Ma
  - first: Yang
    full: Yang Yang
    id: yang-yang
    last: Yang
  - first: Xuancheng
    full: Xuancheng Ren
    id: xuancheng-ren
    last: Ren
  author_string: Yi Zhang, Xu Sun, Shuming Ma, Yang Yang, Xuancheng Ren
  bibkey: zhang-etal-2018-higher
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '723'
  page_last: '733'
  pages: "723\u2013733"
  paper_id: '61'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1061.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1061.jpg
  title: Does Higher Order LSTM Have Better Accuracy for Segmenting and Labeling Sequence
    Data?
  title_html: Does Higher Order <span class="acl-fixed-case">LSTM</span> Have Better
    Accuracy for Segmenting and Labeling Sequence Data?
  url: https://www.aclweb.org/anthology/C18-1061
  year: '2018'
C18-1062:
  abstract: This paper proposes an extractive multi-document summarization approach
    based on an ant colony system to optimize the information coverage of summary
    sentences. The implemented system was evaluated on both English and Arabic versions
    of the corpus of the Text Analysis Conference 2011 MultiLing Pilot by using ROUGE
    metrics. The evaluation results are promising in comparison to those of the participating
    systems. Indeed, our system achieved the best scores based on several ROUGE metrics.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Asma
    full: Asma Al-Saleh
    id: asma-al-saleh
    last: Al-Saleh
  - first: Mohamed El Bachir
    full: Mohamed El Bachir Menai
    id: mohamed-el-bachir-menai
    last: Menai
  author_string: Asma Al-Saleh, Mohamed El Bachir Menai
  bibkey: al-saleh-menai-2018-ant
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '734'
  page_last: '744'
  pages: "734\u2013744"
  paper_id: '62'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1062.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1062.jpg
  title: Ant Colony System for Multi-Document Summarization
  title_html: Ant Colony System for Multi-Document Summarization
  url: https://www.aclweb.org/anthology/C18-1062
  year: '2018'
C18-1063:
  abstract: 'Because of license restrictions, it often becomes impossible to strictly
    reproduce most research results on Twitter data already a few months after the
    creation of the corpus. This situation worsened gradually as time passes and tweets
    become inaccessible. This is a critical issue for reproducible and accountable
    research on social media. We partly solve this challenge by annotating a new Twitter-like
    corpus from an alternative large social medium with licenses that are compatible
    with reproducible experiments: Mastodon. We manually annotate both dialogues and
    sentiments on this corpus, and train a multi-task hierarchical recurrent network
    on joint sentiment and dialog act recognition. We experimentally demonstrate that
    transfer learning may be efficiently achieved between both tasks, and further
    analyze some specific correlations between sentiments and dialogues on social
    media. Both the annotated corpus and deep network are released with an open-source
    license.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Christophe
    full: Christophe Cerisara
    id: christophe-cerisara
    last: Cerisara
  - first: Somayeh
    full: Somayeh Jafaritazehjani
    id: somayeh-jafaritazehjani
    last: Jafaritazehjani
  - first: Adedayo
    full: Adedayo Oluokun
    id: adedayo-oluokun
    last: Oluokun
  - first: Hoa T.
    full: Hoa T. Le
    id: hoa-t-le
    last: Le
  author_string: Christophe Cerisara, Somayeh Jafaritazehjani, Adedayo Oluokun, Hoa
    T. Le
  bibkey: cerisara-etal-2018-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '745'
  page_last: '754'
  pages: "745\u2013754"
  paper_id: '63'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1063.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1063.jpg
  title: Multi-task dialog act and sentiment recognition on Mastodon
  title_html: Multi-task dialog act and sentiment recognition on Mastodon
  url: https://www.aclweb.org/anthology/C18-1063
  year: '2018'
C18-1064:
  abstract: "This paper presents RuSentiment, a new dataset for sentiment analysis\
    \ of social media posts in Russian, and a new set of comprehensive annotation\
    \ guidelines that are extensible to other languages. RuSentiment is currently\
    \ the largest in its class for Russian, with 31,185 posts annotated with Fleiss\u2019\
    \ kappa of 0.58 (3 annotations per post). To diversify the dataset, 6,950 posts\
    \ were pre-selected with an active learning-style strategy. We report baseline\
    \ classification results, and we also release the best-performing embeddings trained\
    \ on 3.2B tokens of Russian VKontakte posts."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Anna
    full: Anna Rogers
    id: anna-rogers
    last: Rogers
  - first: Alexey
    full: Alexey Romanov
    id: alexey-romanov
    last: Romanov
  - first: Anna
    full: Anna Rumshisky
    id: anna-rumshisky
    last: Rumshisky
  - first: Svitlana
    full: Svitlana Volkova
    id: svitlana-volkova
    last: Volkova
  - first: Mikhail
    full: Mikhail Gronas
    id: mikhail-gronas
    last: Gronas
  - first: Alex
    full: Alex Gribov
    id: alex-gribov
    last: Gribov
  author_string: Anna Rogers, Alexey Romanov, Anna Rumshisky, Svitlana Volkova, Mikhail
    Gronas, Alex Gribov
  bibkey: rogers-etal-2018-rusentiment
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '755'
  page_last: '763'
  pages: "755\u2013763"
  paper_id: '64'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1064.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1064.jpg
  title: 'RuSentiment: An Enriched Sentiment Analysis Dataset for Social Media in
    Russian'
  title_html: '<span class="acl-fixed-case">R</span>u<span class="acl-fixed-case">S</span>entiment:
    An Enriched Sentiment Analysis Dataset for Social Media in <span class="acl-fixed-case">R</span>ussian'
  url: https://www.aclweb.org/anthology/C18-1064
  year: '2018'
C18-1065:
  abstract: Self-normalizing discriminative models approximate the normalized probability
    of a class without having to compute the partition function. In the context of
    language modeling, this property is particularly appealing as it may significantly
    reduce run-times due to large word vocabularies. In this study, we provide a comprehensive
    investigation of language modeling self-normalization. First, we theoretically
    analyze the inherent self-normalization properties of Noise Contrastive Estimation
    (NCE) language models. Then, we compare them empirically to softmax-based approaches,
    which are self-normalized using explicit regularization, and suggest a hybrid
    model with compelling properties. Finally, we uncover a surprising negative correlation
    between self-normalization and perplexity across the board, as well as some regularity
    in the observed errors, which may potentially be used for improving self-normalization
    algorithms in the future.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Jacob
    full: Jacob Goldberger
    id: jacob-goldberger
    last: Goldberger
  - first: Oren
    full: Oren Melamud
    id: oren-melamud
    last: Melamud
  author_string: Jacob Goldberger, Oren Melamud
  bibkey: goldberger-melamud-2018-self
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '764'
  page_last: '773'
  pages: "764\u2013773"
  paper_id: '65'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1065.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1065.jpg
  title: Self-Normalization Properties of Language Modeling
  title_html: Self-Normalization Properties of Language Modeling
  url: https://www.aclweb.org/anthology/C18-1065
  year: '2018'
C18-1066:
  abstract: Aspect-level sentiment analysis aims to distinguish the sentiment polarity
    of each specific aspect term in a given sentence. Both industry and academia have
    realized the importance of the relationship between aspect term and sentence,
    and made attempts to model the relationship by designing a series of attention
    models. However, most existing methods usually neglect the fact that the position
    information is also crucial for identifying the sentiment polarity of the aspect
    term. When an aspect term occurs in a sentence, its neighboring words should be
    given more attention than other words with long distance. Therefore, we propose
    a position-aware bidirectional attention network (PBAN) based on bidirectional
    GRU. PBAN not only concentrates on the position information of aspect terms, but
    also mutually models the relation between aspect term and sentence by employing
    bidirectional attention mechanism. The experimental results on SemEval 2014 Datasets
    demonstrate the effectiveness of our proposed PBAN model.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Shuqin
    full: Shuqin Gu
    id: shuqin-gu
    last: Gu
  - first: Lipeng
    full: Lipeng Zhang
    id: lipeng-zhang
    last: Zhang
  - first: Yuexian
    full: Yuexian Hou
    id: yuexian-hou
    last: Hou
  - first: Yin
    full: Yin Song
    id: yin-song
    last: Song
  author_string: Shuqin Gu, Lipeng Zhang, Yuexian Hou, Yin Song
  bibkey: gu-etal-2018-position
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '774'
  page_last: '784'
  pages: "774\u2013784"
  paper_id: '66'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1066.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1066.jpg
  title: A Position-aware Bidirectional Attention Network for Aspect-level Sentiment
    Analysis
  title_html: A Position-aware Bidirectional Attention Network for Aspect-level Sentiment
    Analysis
  url: https://www.aclweb.org/anthology/C18-1066
  year: '2018'
C18-1067:
  abstract: "One main challenge for incremental transition-based parsers, when future\
    \ inputs are invisible, is to extract good features from a limited local context.\
    \ In this work, we present a simple technique to maximally utilize the local features\
    \ with an attention mechanism, which works as context- dependent dynamic feature\
    \ selection. Our model learns, for example, which tokens should a parser focus\
    \ on, to decide the next action. Our multilingual experiment shows its effectiveness\
    \ across many languages. We also present an experiment with augmented test dataset\
    \ and demon- strate it helps to understand the model\u2019s behavior on locally\
    \ ambiguous points."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Ryosuke
    full: Ryosuke Kohita
    id: ryosuke-kohita
    last: Kohita
  - first: Hiroshi
    full: Hiroshi Noji
    id: hiroshi-noji
    last: Noji
  - first: Yuji
    full: Yuji Matsumoto
    id: yuji-matsumoto
    last: Matsumoto
  author_string: Ryosuke Kohita, Hiroshi Noji, Yuji Matsumoto
  bibkey: kohita-etal-2018-dynamic
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '785'
  page_last: '794'
  pages: "785\u2013794"
  paper_id: '67'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1067.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1067.jpg
  title: Dynamic Feature Selection with Attention in Incremental Parsing
  title_html: Dynamic Feature Selection with Attention in Incremental Parsing
  url: https://www.aclweb.org/anthology/C18-1067
  year: '2018'
C18-1068:
  abstract: Neural sequence-to-sequence models have been successfully extended for
    summary generation.However, existing frameworks generate a single summary for
    a given input and do not tune the summaries towards any additional constraints/preferences.
    Such a tunable framework is desirable to account for linguistic preferences of
    the specific audience who will consume the summary. In this paper, we propose
    a neural framework to generate summaries constrained to a vocabulary-defined linguistic
    preferences of a target audience. The proposed method accounts for the generation
    context by tuning the summary words at the time of generation. Our evaluations
    indicate that the proposed approach tunes summaries to the target vocabulary while
    still maintaining a superior summary quality against a state-of-the-art word embedding
    based lexical substitution algorithm, suggesting the feasibility of the proposed
    approach. We demonstrate two applications of the proposed approach - to generate
    understandable summaries with simpler words, and readable summaries with shorter
    words.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Kundan
    full: Kundan Krishna
    id: kundan-krishna
    last: Krishna
  - first: Aniket
    full: Aniket Murhekar
    id: aniket-murhekar
    last: Murhekar
  - first: Saumitra
    full: Saumitra Sharma
    id: saumitra-sharma
    last: Sharma
  - first: Balaji Vasan
    full: Balaji Vasan Srinivasan
    id: balaji-vasan-srinivasan
    last: Srinivasan
  author_string: Kundan Krishna, Aniket Murhekar, Saumitra Sharma, Balaji Vasan Srinivasan
  bibkey: krishna-etal-2018-vocabulary
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '795'
  page_last: '805'
  pages: "795\u2013805"
  paper_id: '68'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1068.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1068.jpg
  title: Vocabulary Tailored Summary Generation
  title_html: Vocabulary Tailored Summary Generation
  url: https://www.aclweb.org/anthology/C18-1068
  year: '2018'
C18-1069:
  abstract: Complex questions in reading comprehension tasks require integrating information
    from multiple sentences. In this work, to answer such questions involving temporal
    and causal relations, we generate event graphs from text based on dependencies,
    and rank answers by aligning event graphs. In particular, the alignments are constrained
    by graph-based reasoning to ensure temporal and causal agreement. Our focused
    approach self-adaptively complements existing solutions; it is automatically triggered
    only when applicable. Experiments on RACE and MCTest show that state-of-the-art
    methods are notably improved by using our approach as an add-on.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yawei
    full: Yawei Sun
    id: yawei-sun
    last: Sun
  - first: Gong
    full: Gong Cheng
    id: gong-cheng
    last: Cheng
  - first: Yuzhong
    full: Yuzhong Qu
    id: yuzhong-qu
    last: Qu
  author_string: Yawei Sun, Gong Cheng, Yuzhong Qu
  bibkey: sun-etal-2018-reading
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '806'
  page_last: '817'
  pages: "806\u2013817"
  paper_id: '69'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1069.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1069.jpg
  title: Reading Comprehension with Graph-based Temporal-Casual Reasoning
  title_html: Reading Comprehension with Graph-based Temporal-Casual Reasoning
  url: https://www.aclweb.org/anthology/C18-1069
  year: '2018'
C18-1070:
  abstract: Domain adaptation for sentiment analysis is challenging due to the fact
    that supervised classifiers are very sensitive to changes in domain. The two most
    prominent approaches to this problem are structural correspondence learning and
    autoencoders. However, they either require long training times or suffer greatly
    on highly divergent domains. Inspired by recent advances in cross-lingual sentiment
    analysis, we provide a novel perspective and cast the domain adaptation problem
    as an embedding projection task. Our model takes as input two mono-domain embedding
    spaces and learns to project them to a bi-domain space, which is jointly optimized
    to (1) project across domains and to (2) predict sentiment. We perform domain
    adaptation experiments on 20 source-target domain pairs for sentiment classification
    and report novel state-of-the-art results on 11 domain pairs, including the Amazon
    domain adaptation datasets and SemEval 2013 and 2016 datasets. Our analysis shows
    that our model performs comparably to state-of-the-art approaches on domains that
    are similar, while performing significantly better on highly divergent domains.
    Our code is available at https://github.com/jbarnesspain/domain_blse
  address: Santa Fe, New Mexico, USA
  author:
  - first: Jeremy
    full: Jeremy Barnes
    id: jeremy-barnes
    last: Barnes
  - first: Roman
    full: Roman Klinger
    id: roman-klinger
    last: Klinger
  - first: Sabine
    full: Sabine Schulte im Walde
    id: sabine-schulte-im-walde
    last: Schulte im Walde
  author_string: Jeremy Barnes, Roman Klinger, Sabine Schulte im Walde
  bibkey: barnes-etal-2018-projecting
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '818'
  page_last: '830'
  pages: "818\u2013830"
  paper_id: '70'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1070.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1070.jpg
  title: 'Projecting Embeddings for Domain Adaption: Joint Modeling of Sentiment Analysis
    in Diverse Domains'
  title_html: 'Projecting Embeddings for Domain Adaption: Joint Modeling of Sentiment
    Analysis in Diverse Domains'
  url: https://www.aclweb.org/anthology/C18-1070
  year: '2018'
C18-1071:
  abstract: Argumentation mining (AM) requires the identification of complex discourse
    structures and has lately been applied with success monolingually. In this work,
    we show that the existing resources are, however, not adequate for assessing cross-lingual
    AM, due to their heterogeneity or lack of complexity. We therefore create suitable
    parallel corpora by (human and machine) translating a popular AM dataset consisting
    of persuasive student essays into German, French, Spanish, and Chinese. We then
    compare (i) annotation projection and (ii) bilingual word embeddings based direct
    transfer strategies for cross-lingual AM, finding that the former performs considerably
    better and almost eliminates the loss from cross-lingual transfer. Moreover, we
    find that annotation projection works equally well when using either costly human
    or cheap machine translations. Our code and data are available at http://github.com/UKPLab/coling2018-xling_argument_mining.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Steffen
    full: Steffen Eger
    id: steffen-eger
    last: Eger
  - first: Johannes
    full: Johannes Daxenberger
    id: johannes-daxenberger
    last: Daxenberger
  - first: Christian
    full: Christian Stab
    id: christian-stab
    last: Stab
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Steffen Eger, Johannes Daxenberger, Christian Stab, Iryna Gurevych
  bibkey: eger-etal-2018-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '831'
  page_last: '844'
  pages: "831\u2013844"
  paper_id: '71'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1071.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1071.jpg
  title: 'Cross-lingual Argumentation Mining: Machine Translation (and a bit of Projection)
    is All You Need!'
  title_html: 'Cross-lingual Argumentation Mining: Machine Translation (and a bit
    of Projection) is All You Need!'
  url: https://www.aclweb.org/anthology/C18-1071
  year: '2018'
C18-1072:
  abstract: Recent years have witnessed a surge of interest on response generation
    for neural conversation systems. Most existing models are implemented by following
    the Encoder-Decoder framework and operate sentences of conversations at word-level.
    The word-level model is suffering from the Unknown Words Issue and the Preference
    Issue, which seriously impact the quality of generated responses, for example,
    generated responses may become irrelevant or too general (i.e. safe responses).
    To address these issues, this paper proposes a hybrid-level Encoder-Decoder model
    (HL-EncDec), which not only utilizes the word-level features but also character-level
    features. We conduct several experiments to evaluate HL-EncDec on a Chinese corpus,
    experimental results show our model significantly outperforms other non-word-level
    models in automatic metrics and human annotations and is able to generate more
    informative responses. We also conduct experiments with a small-scale English
    dataset to show the generalization ability.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Sixing
    full: Sixing Wu
    id: sixing-wu
    last: Wu
  - first: Dawei
    full: Dawei Zhang
    id: dawei-zhang
    last: Zhang
  - first: Ying
    full: Ying Li
    id: ying-li
    last: Li
  - first: Xing
    full: Xing Xie
    id: xing-xie
    last: Xie
  - first: Zhonghai
    full: Zhonghai Wu
    id: zhonghai-wu
    last: Wu
  author_string: Sixing Wu, Dawei Zhang, Ying Li, Xing Xie, Zhonghai Wu
  bibkey: wu-etal-2018-hl
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '845'
  page_last: '856'
  pages: "845\u2013856"
  paper_id: '72'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1072.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1072.jpg
  title: 'HL-EncDec: A Hybrid-Level Encoder-Decoder for Neural Response Generation'
  title_html: '<span class="acl-fixed-case">HL</span>-<span class="acl-fixed-case">E</span>nc<span
    class="acl-fixed-case">D</span>ec: A Hybrid-Level Encoder-Decoder for Neural Response
    Generation'
  url: https://www.aclweb.org/anthology/C18-1072
  year: '2018'
C18-1073:
  abstract: Cloze-style reading comprehension has been a popular task for measuring
    the progress of natural language understanding in recent years. In this paper,
    we design a novel multi-perspective framework, which can be seen as the joint
    training of heterogeneous experts and aggregate context information from different
    perspectives. Each perspective is modeled by a simple aggregation module. The
    outputs of multiple aggregation modules are fed into a one-timestep pointer network
    to get the final answer. At the same time, to tackle the problem of insufficient
    labeled data, we propose an efficient sampling mechanism to automatically generate
    more training examples by matching the distribution of candidates between labeled
    and unlabeled data. We conduct our experiments on a recently released cloze-test
    dataset CLOTH (Xie et al., 2017), which consists of nearly 100k questions designed
    by professional teachers. Results show that our method achieves new state-of-the-art
    performance over previous strong baselines.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Liang
    full: Liang Wang
    id: liang-wang
    last: Wang
  - first: Sujian
    full: Sujian Li
    id: sujian-li
    last: Li
  - first: Wei
    full: Wei Zhao
    id: wei-zhao
    last: Zhao
  - first: Kewei
    full: Kewei Shen
    id: kewei-shen
    last: Shen
  - first: Meng
    full: Meng Sun
    id: meng-sun
    last: Sun
  - first: Ruoyu
    full: Ruoyu Jia
    id: ruoyu-jia
    last: Jia
  - first: Jingming
    full: Jingming Liu
    id: jingming-liu
    last: Liu
  author_string: Liang Wang, Sujian Li, Wei Zhao, Kewei Shen, Meng Sun, Ruoyu Jia,
    Jingming Liu
  bibkey: wang-etal-2018-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '857'
  page_last: '867'
  pages: "857\u2013867"
  paper_id: '73'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1073.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1073.jpg
  title: Multi-Perspective Context Aggregation for Semi-supervised Cloze-style Reading
    Comprehension
  title_html: Multi-Perspective Context Aggregation for Semi-supervised Cloze-style
    Reading Comprehension
  url: https://www.aclweb.org/anthology/C18-1073
  year: '2018'
C18-1074:
  abstract: Attention mechanisms have been leveraged for sentiment classification
    tasks because not all words have the same importance. However, most existing attention
    models did not take full advantage of sentiment lexicons, which provide rich sentiment
    information and play a critical role in sentiment analysis. To achieve the above
    target, in this work, we propose a novel lexicon-based supervised attention model
    (LBSA), which allows a recurrent neural network to focus on the sentiment content,
    thus generating sentiment-informative representations. Compared with general attention
    models, our model has better interpretability and less noise. Experimental results
    on three large-scale sentiment classification datasets showed that the proposed
    method outperforms previous methods.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yicheng
    full: Yicheng Zou
    id: yicheng-zou
    last: Zou
  - first: Tao
    full: Tao Gui
    id: tao-gui
    last: Gui
  - first: Qi
    full: Qi Zhang
    id: qi-zhang
    last: Zhang
  - first: Xuanjing
    full: Xuanjing Huang
    id: xuan-jing-huang
    last: Huang
  author_string: Yicheng Zou, Tao Gui, Qi Zhang, Xuanjing Huang
  bibkey: zou-etal-2018-lexicon
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '868'
  page_last: '877'
  pages: "868\u2013877"
  paper_id: '74'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1074.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1074.jpg
  title: A Lexicon-Based Supervised Attention Model for Neural Sentiment Analysis
  title_html: A Lexicon-Based Supervised Attention Model for Neural Sentiment Analysis
  url: https://www.aclweb.org/anthology/C18-1074
  year: '2018'
C18-1075:
  abstract: This paper introduces open-domain event detection, a new event detection
    paradigm to address issues of prior work on restricted domains and event annotation.
    The goal is to detect all kinds of events regardless of domains. Given the absence
    of training data, we propose a distant supervision method that is able to generate
    high-quality training data. Using a manually annotated event corpus as gold standard,
    our experiments show that despite no direct supervision, the model outperforms
    supervised models. This result indicates that the distant supervision enables
    robust event detection in various domains, while obviating the need for human
    annotation of events.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Jun
    full: Jun Araki
    id: jun-araki
    last: Araki
  - first: Teruko
    full: Teruko Mitamura
    id: teruko-mitamura
    last: Mitamura
  author_string: Jun Araki, Teruko Mitamura
  bibkey: araki-mitamura-2018-open
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '878'
  page_last: '891'
  pages: "878\u2013891"
  paper_id: '75'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1075.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1075.jpg
  title: Open-Domain Event Detection using Distant Supervision
  title_html: Open-Domain Event Detection using Distant Supervision
  url: https://www.aclweb.org/anthology/C18-1075
  year: '2018'
C18-1076:
  abstract: 'Semantic parsers critically rely on accurate and high-coverage lexicons.
    However, traditional semantic parsers usually utilize annotated logical forms
    to learn the lexicon, which often suffer from the lexicon coverage problem. In
    this paper, we propose a graph-based semi-supervised learning framework that makes
    use of large text corpora and lexical resources. This framework first constructs
    a graph with a phrase similarity model learned by utilizing many text corpora
    and lexical resources. Next, graph propagation algorithm identifies the label
    distribution of unlabeled phrases from labeled ones. We evaluate our approach
    on two benchmarks: Webquestions and Free917. The results show that, in both datasets,
    our method achieves substantial improvement when comparing to the base system
    that does not utilize the learned lexicon, and gains competitive results when
    comparing to state-of-the-art systems.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Bo
    full: Bo Chen
    id: bo-chen
    last: Chen
  - first: Bo
    full: Bo An
    id: bo-an
    last: An
  - first: Le
    full: Le Sun
    id: le-sun
    last: Sun
  - first: Xianpei
    full: Xianpei Han
    id: xianpei-han
    last: Han
  author_string: Bo Chen, Bo An, Le Sun, Xianpei Han
  bibkey: chen-etal-2018-semi
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '892'
  page_last: '904'
  pages: "892\u2013904"
  paper_id: '76'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1076.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1076.jpg
  title: Semi-Supervised Lexicon Learning for Wide-Coverage Semantic Parsing
  title_html: Semi-Supervised Lexicon Learning for Wide-Coverage Semantic Parsing
  url: https://www.aclweb.org/anthology/C18-1076
  year: '2018'
C18-1077:
  abstract: We present a new summary evaluation approach that does not require human
    model summaries. Our approach exploits the compositional capabilities of corpus-based
    and lexical resource-based word embeddings to develop the features reflecting
    coverage, diversity, informativeness, and coherence of summaries. The features
    are then used to train a learning model for predicting the summary content quality
    in the absence of gold models. We evaluate the proposed metric in replicating
    the human assigned scores for summarization systems and summaries on data from
    query-focused and update summarization tasks in TAC 2008 and 2009. The results
    show that our feature combination provides reliable estimates of summary content
    quality when model summaries are not available.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Elaheh
    full: Elaheh ShafieiBavani
    id: elaheh-shafieibavani
    last: ShafieiBavani
  - first: Mohammad
    full: Mohammad Ebrahimi
    id: mohammad-ebrahimi
    last: Ebrahimi
  - first: Raymond
    full: Raymond Wong
    id: raymond-wong
    last: Wong
  - first: Fang
    full: Fang Chen
    id: fang-chen
    last: Chen
  author_string: Elaheh ShafieiBavani, Mohammad Ebrahimi, Raymond Wong, Fang Chen
  bibkey: shafieibavani-etal-2018-summarization
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '905'
  page_last: '914'
  pages: "905\u2013914"
  paper_id: '77'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1077.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1077.jpg
  title: Summarization Evaluation in the Absence of Human Model Summaries Using the
    Compositionality of Word Embeddings
  title_html: Summarization Evaluation in the Absence of Human Model Summaries Using
    the Compositionality of Word Embeddings
  url: https://www.aclweb.org/anthology/C18-1077
  year: '2018'
C18-1078:
  abstract: The availability of corpora annotated with negation information is essential
    to develop negation processing systems in any language. However, there is a lack
    of these corpora even for languages like English, and when there are corpora available
    they are small and the annotations are not always compatible across corpora. In
    this paper we review the existing corpora annotated with negation in Spanish with
    the purpose of first, gathering the information to make it available for other
    researchers and, second, analyzing how compatible are the corpora and how has
    the linguistic phenomenon been addressed. Our final aim is to develop a supervised
    negation processing system for Spanish, for which we need training and test data.
    Our analysis shows that it will not be possible to merge the small corpora existing
    for Spanish due to lack of compatibility in the annotations.
  address: Santa Fe, New Mexico, USA
  author:
  - first: "Salud Mar\xEDa"
    full: "Salud Mar\xEDa Jim\xE9nez-Zafra"
    id: salud-maria-jimenez-zafra
    last: "Jim\xE9nez-Zafra"
  - first: Roser
    full: Roser Morante
    id: roser-morante
    last: Morante
  - first: Maite
    full: Maite Martin
    id: m-teresa-martin-valdivia
    last: Martin
  - first: L. Alfonso
    full: "L. Alfonso Ure\xF1a-L\xF3pez"
    id: l-alfonso-urena-lopez
    last: "Ure\xF1a-L\xF3pez"
  author_string: "Salud Mar\xEDa Jim\xE9nez-Zafra, Roser Morante, Maite Martin, L.\
    \ Alfonso Ure\xF1a-L\xF3pez"
  bibkey: jimenez-zafra-etal-2018-review
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '915'
  page_last: '924'
  pages: "915\u2013924"
  paper_id: '78'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1078.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1078.jpg
  title: A review of Spanish corpora annotated with negation
  title_html: A review of <span class="acl-fixed-case">S</span>panish corpora annotated
    with negation
  url: https://www.aclweb.org/anthology/C18-1078
  year: '2018'
C18-1079:
  abstract: "Document-level multi-aspect sentiment classification aims to predict\
    \ user\u2019s sentiment polarities for different aspects of a product in a review.\
    \ Existing approaches mainly focus on text information. However, the authors (i.e.\
    \ users) and overall ratings of reviews are ignored, both of which are proved\
    \ to be significant on interpreting the sentiments of different aspects in this\
    \ paper. Therefore, we propose a model called Hierarchical User Aspect Rating\
    \ Network (HUARN) to consider user preference and overall ratings jointly. Specifically,\
    \ HUARN adopts a hierarchical architecture to encode word, sentence, and document\
    \ level information. Then, user attention and aspect attention are introduced\
    \ into building sentence and document level representation. The document representation\
    \ is combined with user and overall rating information to predict aspect ratings\
    \ of a review. Diverse aspects are treated differently and a multi-task framework\
    \ is adopted. Empirical results on two real-world datasets show that HUARN achieves\
    \ state-of-the-art performances."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Junjie
    full: Junjie Li
    id: junjie-li
    last: Li
  - first: Haitong
    full: Haitong Yang
    id: haitong-yang
    last: Yang
  - first: Chengqing
    full: Chengqing Zong
    id: chengqing-zong
    last: Zong
  author_string: Junjie Li, Haitong Yang, Chengqing Zong
  bibkey: li-etal-2018-document
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '925'
  page_last: '936'
  pages: "925\u2013936"
  paper_id: '79'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1079.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1079.jpg
  title: Document-level Multi-aspect Sentiment Classification by Jointly Modeling
    Users, Aspects, and Overall Ratings
  title_html: Document-level Multi-aspect Sentiment Classification by Jointly Modeling
    Users, Aspects, and Overall Ratings
  url: https://www.aclweb.org/anthology/C18-1079
  year: '2018'
C18-1080:
  abstract: 'Recent evaluations on bilingual lexicon extraction from specialized comparable
    corpora have shown contrasted performance while using word embedding models. This
    can be partially explained by the lack of large specialized comparable corpora
    to build efficient representations. Within this context, we try to answer the
    following questions: First, (i) among the state-of-the-art embedding models, whether
    trained on specialized corpora or pre-trained on large general data sets, which
    one is the most appropriate model for bilingual terminology extraction? Second
    (ii) is it worth it to combine multiple embeddings trained on different data sets?
    For that purpose, we propose the first systematic evaluation of different word
    embedding models for bilingual terminology extraction from specialized comparable
    corpora. We emphasize how the character-based embedding model outperforms other
    models on the quality of the extracted bilingual lexicons. Further more, we propose
    a new efficient way to combine different embedding models learned from specialized
    and general-domain data sets. Our approach leads to higher performance than the
    best individual embedding model.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Amir
    full: Amir Hazem
    id: amir-hazem
    last: Hazem
  - first: Emmanuel
    full: Emmanuel Morin
    id: emmanuel-morin
    last: Morin
  author_string: Amir Hazem, Emmanuel Morin
  bibkey: hazem-morin-2018-leveraging
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '937'
  page_last: '949'
  pages: "937\u2013949"
  paper_id: '80'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1080.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1080.jpg
  title: Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized
    Comparable Corpora
  title_html: Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized
    Comparable Corpora
  url: https://www.aclweb.org/anthology/C18-1080
  year: '2018'
C18-1081:
  abstract: "Most word representation learning methods are based on the distributional\
    \ hypothesis in linguistics, according to which words that are used and occur\
    \ in the same contexts tend to possess similar meanings. As a consequence, emotionally\
    \ dissimilar words, such as \u201Chappy\u201D and \u201Csad\u201D occurring in\
    \ similar contexts would purport more similar meaning than emotionally similar\
    \ words, such as \u201Chappy\u201D and \u201Cjoy\u201D. This complication leads\
    \ to rather undesirable outcome in predictive tasks that relate to affect (emotional\
    \ state), such as emotion classification and emotion similarity. In order to address\
    \ this limitation, we propose a novel method of obtaining emotion-enriched word\
    \ representations, which projects emotionally similar words into neighboring spaces\
    \ and emotionally dissimilar ones far apart. The proposed approach leverages distant\
    \ supervision to automatically obtain a large training dataset of text documents\
    \ and two recurrent neural network architectures for learning the emotion-enriched\
    \ representations. Through extensive evaluation on two tasks, including emotion\
    \ classification and emotion similarity, we demonstrate that the proposed representations\
    \ outperform several competitive general-purpose and affective word representations."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Ameeta
    full: Ameeta Agrawal
    id: ameeta-agrawal
    last: Agrawal
  - first: Aijun
    full: Aijun An
    id: aijun-an
    last: An
  - first: Manos
    full: Manos Papagelis
    id: manos-papagelis
    last: Papagelis
  author_string: Ameeta Agrawal, Aijun An, Manos Papagelis
  bibkey: agrawal-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '950'
  page_last: '961'
  pages: "950\u2013961"
  paper_id: '81'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1081.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1081.jpg
  title: Learning Emotion-enriched Word Representations
  title_html: Learning Emotion-enriched Word Representations
  url: https://www.aclweb.org/anthology/C18-1081
  year: '2018'
C18-1082:
  abstract: "We present an evaluation of PASS, a data-to-text system that generates\
    \ Dutch soccer reports from match statistics which are automatically tailored\
    \ towards fans of one club or the other. The evaluation in this paper consists\
    \ of two studies. An intrinsic human-based evaluation of the system\u2019s output\
    \ is described in the first study. In this study it was found that compared to\
    \ human-written texts, computer-generated texts were rated slightly lower on style-related\
    \ text components (fluency and clarity) and slightly higher in terms of the correctness\
    \ of given information. Furthermore, results from the first study showed that\
    \ tailoring was accurately recognized in most cases, and that participants struggled\
    \ with correctly identifying whether a text was written by a human or computer.\
    \ The second study investigated if tailoring affects perceived text quality, for\
    \ which no results were garnered. This lack of results might be due to negative\
    \ preconceptions about computer-generated texts which were found in the first\
    \ study."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Chris
    full: Chris van der Lee
    id: chris-van-der-lee
    last: van der Lee
  - first: Bart
    full: Bart Verduijn
    id: bart-verduijn
    last: Verduijn
  - first: Emiel
    full: Emiel Krahmer
    id: emiel-krahmer
    last: Krahmer
  - first: Sander
    full: Sander Wubben
    id: sander-wubben
    last: Wubben
  author_string: Chris van der Lee, Bart Verduijn, Emiel Krahmer, Sander Wubben
  bibkey: van-der-lee-etal-2018-evaluating
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '962'
  page_last: '972'
  pages: "962\u2013972"
  paper_id: '82'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1082.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1082.jpg
  title: 'Evaluating the text quality, human likeness and tailoring component of PASS:
    A Dutch data-to-text system for soccer'
  title_html: 'Evaluating the text quality, human likeness and tailoring component
    of <span class="acl-fixed-case">PASS</span>: A <span class="acl-fixed-case">D</span>utch
    data-to-text system for soccer'
  url: https://www.aclweb.org/anthology/C18-1082
  year: '2018'
C18-1083:
  abstract: "Machine-reading comprehension (MRC) has recently attracted attention\
    \ in the fields of natural language processing and machine learning. One of the\
    \ problematic presumptions with current MRC technologies is that each question\
    \ is assumed to be answerable by looking at a given text passage. However, to\
    \ realize human-like language comprehension ability, a machine should also be\
    \ able to distinguish not-answerable questions (NAQs) from answerable questions.\
    \ To develop this functionality, a dataset incorporating hard-to-detect NAQs is\
    \ vital; however, its manual construction would be expensive. This paper proposes\
    \ a dataset creation method that alters an existing MRC dataset, the Stanford\
    \ Question Answering Dataset, and describes the resulting dataset. The value of\
    \ this dataset is likely to increase if each NAQ in the dataset is properly classified\
    \ with the difficulty of identifying it as an NAQ. This difficulty level would\
    \ allow researchers to evaluate a machine\u2019s NAQ detection performance more\
    \ precisely. Therefore, we propose a method for automatically assigning difficulty\
    \ level labels, which measures the similarity between a question and the target\
    \ text passage. Our NAQ detection experiments demonstrate that the resulting dataset,\
    \ having difficulty level annotations, is valid and potentially useful in the\
    \ development of advanced MRC models."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Mao
    full: Mao Nakanishi
    id: mao-nakanishi
    last: Nakanishi
  - first: Tetsunori
    full: Tetsunori Kobayashi
    id: tetsunori-kobayashi
    last: Kobayashi
  - first: Yoshihiko
    full: Yoshihiko Hayashi
    id: yoshihiko-hayashi
    last: Hayashi
  author_string: Mao Nakanishi, Tetsunori Kobayashi, Yoshihiko Hayashi
  bibkey: nakanishi-etal-2018-answerable
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '973'
  page_last: '983'
  pages: "973\u2013983"
  paper_id: '83'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1083.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1083.jpg
  title: 'Answerable or Not: Devising a Dataset for Extending Machine Reading Comprehension'
  title_html: 'Answerable or Not: Devising a Dataset for Extending Machine Reading
    Comprehension'
  url: https://www.aclweb.org/anthology/C18-1083
  year: '2018'
C18-1084:
  abstract: The task of obfuscating writing style using sequence models has previously
    been investigated under the framework of obfuscation-by-transfer, where the input
    text is explicitly rewritten in another style. A side effect of this framework
    are the frequent major alterations to the semantic content of the input. In this
    work, we propose obfuscation-by-invariance, and investigate to what extent models
    trained to be explicitly style-invariant preserve semantics. We evaluate our architectures
    in parallel and non-parallel settings, and compare automatic and human evaluations
    on the obfuscated sentences. Our experiments show that the performance of a style
    classifier can be reduced to chance level, while the output is evaluated to be
    of equal quality to models applying style-transfer. Additionally, human evaluation
    indicates a trade-off between the level of obfuscation and the observed quality
    of the output in terms of meaning preservation and grammaticality.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Chris
    full: Chris Emmery
    id: chris-emmery
    last: Emmery
  - first: Enrique
    full: Enrique Manjavacas Arevalo
    id: enrique-manjavacas
    last: Manjavacas Arevalo
  - first: Grzegorz
    full: "Grzegorz Chrupa\u0142a"
    id: grzegorz-chrupala
    last: "Chrupa\u0142a"
  author_string: "Chris Emmery, Enrique Manjavacas Arevalo, Grzegorz Chrupa\u0142a"
  bibkey: emmery-etal-2018-style
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '984'
  page_last: '996'
  pages: "984\u2013996"
  paper_id: '84'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1084.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1084.jpg
  title: Style Obfuscation by Invariance
  title_html: Style Obfuscation by Invariance
  url: https://www.aclweb.org/anthology/C18-1084
  year: '2018'
C18-1085:
  abstract: General-purpose pre-trained word embeddings have become a mainstay of
    natural language processing, and more recently, methods have been proposed to
    encode external knowledge into word embeddings to benefit specific downstream
    tasks. The goal of this paper is to encode sentiment knowledge into pre-trained
    word vectors to improve the performance of sentiment analysis. Our proposed method
    is based on a convolutional neural network (CNN) and an external sentiment lexicon.
    Experiments on four popular sentiment analysis datasets show that this method
    improves the accuracy of sentiment analysis compared to a number of benchmark
    methods.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Zhe
    full: Zhe Ye
    id: zhe-ye
    last: Ye
  - first: Fang
    full: Fang Li
    id: fang-li
    last: Li
  - first: Timothy
    full: Timothy Baldwin
    id: timothy-baldwin
    last: Baldwin
  author_string: Zhe Ye, Fang Li, Timothy Baldwin
  bibkey: ye-etal-2018-encoding
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '997'
  page_last: '1007'
  pages: "997\u20131007"
  paper_id: '85'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1085.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1085.jpg
  title: Encoding Sentiment Information into Word Vectors for Sentiment Analysis
  title_html: Encoding Sentiment Information into Word Vectors for Sentiment Analysis
  url: https://www.aclweb.org/anthology/C18-1085
  year: '2018'
C18-1086:
  abstract: 'Generating natural language requires conveying content in an appropriate
    style. We explore two related tasks on generating text of varying formality: monolingual
    formality transfer and formality-sensitive machine translation. We propose to
    solve these tasks jointly using multi-task learning, and show that our models
    achieve state-of-the-art performance for formality transfer and are able to perform
    formality-sensitive translation without being explicitly trained on style-annotated
    translation examples.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Xing
    full: Xing Niu
    id: xing-niu
    last: Niu
  - first: Sudha
    full: Sudha Rao
    id: sudha-rao
    last: Rao
  - first: Marine
    full: Marine Carpuat
    id: marine-carpuat
    last: Carpuat
  author_string: Xing Niu, Sudha Rao, Marine Carpuat
  bibkey: niu-etal-2018-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1008'
  page_last: '1021'
  pages: "1008\u20131021"
  paper_id: '86'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1086.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1086.jpg
  title: Multi-Task Neural Models for Translating Between Styles Within and Across
    Languages
  title_html: Multi-Task Neural Models for Translating Between Styles Within and Across
    Languages
  url: https://www.aclweb.org/anthology/C18-1086
  year: '2018'
C18-1087:
  abstract: This paper describes a transduction language suitable for natural language
    treebank transformations and motivates its application to tasks that have been
    used and described in the literature. The language, which is the basis for a tree
    transduction tool allows for clean, precise and concise description of what has
    been very confusingly, ambiguously, and incompletely textually described in the
    literature also allowing easy non-hard-coded implementation. We also aim at getting
    feedback from the NLP community to eventually converge to a de facto standard
    for such transduction language.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Carlos A.
    full: Carlos A. Prolo
    id: carlos-a-prolo
    last: Prolo
  author_string: Carlos A. Prolo
  bibkey: prolo-2018-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1022'
  page_last: '1032'
  pages: "1022\u20131032"
  paper_id: '87'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1087.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1087.jpg
  title: Towards a Language for Natural Language Treebank Transductions
  title_html: Towards a Language for Natural Language Treebank Transductions
  url: https://www.aclweb.org/anthology/C18-1087
  year: '2018'
C18-1088:
  abstract: Story generation is a challenging problem in artificial intelligence (AI)
    and has received a lot of interests in the natural language processing (NLP) community.
    Most previous work tried to solve this problem using Sequence to Sequence (Seq2Seq)
    model trained with Maximum Likelihood Estimation (MLE). However, the pure MLE
    training objective much limits the power of Seq2Seq model in generating high-quality
    storys. In this paper, we propose using adversarial training augmented Seq2Seq
    model to generate reasonable and diversified story endings given a story context.
    Our model includes a generator that defines the policy of generating a story ending,
    and a discriminator that labels story endings as human-generated or machine-generated.
    Carefully designed human and automatic evaluation metrics demonstrate that our
    adversarial training augmented Seq2Seq model can generate more reasonable and
    diversified story endings compared to purely MLE-trained Seq2Seq model. Moreover,
    our model achieves better performance on the task of Story Cloze Test with an
    accuracy of 62.6% compared with state-of-the-art baseline methods.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Zhongyang
    full: Zhongyang Li
    id: zhongyang-li
    last: Li
  - first: Xiao
    full: Xiao Ding
    id: xiao-ding
    last: Ding
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  author_string: Zhongyang Li, Xiao Ding, Ting Liu
  bibkey: li-etal-2018-generating
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1033'
  page_last: '1043'
  pages: "1033\u20131043"
  paper_id: '88'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1088.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1088.jpg
  title: Generating Reasonable and Diversified Story Ending Using Sequence to Sequence
    Model with Adversarial Training
  title_html: Generating Reasonable and Diversified Story Ending Using Sequence to
    Sequence Model with Adversarial Training
  url: https://www.aclweb.org/anthology/C18-1088
  year: '2018'
C18-1089:
  abstract: The task of data-to-text generation aims to generate descriptive texts
    conditioned on a number of database records, and recent neural models have shown
    significant progress on this task. The attention based encoder-decoder models
    with copy mechanism have achieved state-of-the-art results on a few data-to-text
    datasets. However, such models still face the problem of putting incorrect data
    records in the generated texts, especially on some more challenging datasets like
    RotoWire. In this paper, we propose a two-stage approach with a delayed copy mechanism
    to improve the precision of data records in the generated texts. Our approach
    first adopts an encoder-decoder model to generate a template text with data slots
    to be filled and then leverages a proposed delayed copy mechanism to fill in the
    slots with proper data records. Our delayed copy mechanism can take into account
    all the information of the input data records and the full generated template
    text by using double attention, position-aware attention and a pairwise ranking
    loss. The two models in the two stages are trained separately. Evaluation results
    on the RotoWire dataset verify the efficacy of our proposed approach to generate
    better templates and copy data records more precisely.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Liunian
    full: Liunian Li
    id: liunian-li
    last: Li
  - first: Xiaojun
    full: Xiaojun Wan
    id: xiaojun-wan
    last: Wan
  author_string: Liunian Li, Xiaojun Wan
  bibkey: li-wan-2018-point
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1044'
  page_last: '1055'
  pages: "1044\u20131055"
  paper_id: '89'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1089.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1089.jpg
  title: 'Point Precisely: Towards Ensuring the Precision of Data in Generated Texts
    Using Delayed Copy Mechanism'
  title_html: 'Point Precisely: Towards Ensuring the Precision of Data in Generated
    Texts Using Delayed Copy Mechanism'
  url: https://www.aclweb.org/anthology/C18-1089
  year: '2018'
C18-1090:
  abstract: Lexicon based methods for sentiment analysis rely on high quality polarity
    lexicons. In recent years, automatic methods for inducing lexicons have increased
    the viability of lexicon based methods for polarity classification. SentProp is
    a framework for inducing domain-specific polarities from word embeddings. We elaborate
    on SentProp by evaluating its use for enhancing DuOMan, a general-purpose lexicon,
    for use in the political domain. By adding only top sentiment bearing words from
    the vocabulary and applying small polarity shifts in the general-purpose lexicon,
    we increase accuracy in an in-domain classification task. The enhanced lexicon
    performs worse than the original lexicon in an out-domain task, showing that the
    words we added and the polarity shifts we applied are domain-specific and do not
    translate well to an out-domain setting.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Tim
    full: Tim Kreutz
    id: tim-kreutz
    last: Kreutz
  - first: Walter
    full: Walter Daelemans
    id: walter-daelemans
    last: Daelemans
  author_string: Tim Kreutz, Walter Daelemans
  bibkey: kreutz-daelemans-2018-enhancing
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1056'
  page_last: '1064'
  pages: "1056\u20131064"
  paper_id: '90'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1090.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1090.jpg
  title: Enhancing General Sentiment Lexicons for Domain-Specific Use
  title_html: Enhancing General Sentiment Lexicons for Domain-Specific Use
  url: https://www.aclweb.org/anthology/C18-1090
  year: '2018'
C18-1091:
  abstract: Sentence compression condenses a sentence while preserving its most important
    contents. Delete-based models have the strong ability to delete undesired words,
    while generate-based models are able to reorder or rephrase the words, which are
    more coherent to human sentence compression. In this paper, we propose Operation
    Network, a neural network approach for abstractive sentence compression, which
    combines the advantages of both delete-based and generate-based sentence compression
    models. The central idea of Operation Network is to model the sentence compression
    process as an editing procedure. First, unnecessary words are deleted from the
    source sentence, then new words are either generated from a large vocabulary or
    copied directly from the source sentence. A compressed sentence can be obtained
    by a series of such edit operations (delete, copy and generate). Experiments show
    that Operation Network outperforms state-of-the-art baselines.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Naitong
    full: Naitong Yu
    id: naitong-yu
    last: Yu
  - first: Jie
    full: Jie Zhang
    id: jie-zhang
    last: Zhang
  - first: Minlie
    full: Minlie Huang
    id: minlie-huang
    last: Huang
  - first: Xiaoyan
    full: Xiaoyan Zhu
    id: xiaoyan-zhu
    last: Zhu
  author_string: Naitong Yu, Jie Zhang, Minlie Huang, Xiaoyan Zhu
  bibkey: yu-etal-2018-operation
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1065'
  page_last: '1076'
  pages: "1065\u20131076"
  paper_id: '91'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1091.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1091.jpg
  title: An Operation Network for Abstractive Sentence Compression
  title_html: An Operation Network for Abstractive Sentence Compression
  url: https://www.aclweb.org/anthology/C18-1091
  year: '2018'
C18-1092:
  abstract: 'In aspect level sentiment classification, there are two common tasks:
    to identify the sentiment of an aspect (category) or a term. As specific instances
    of aspects, terms explicitly occur in sentences. It is beneficial for models to
    focus on nearby context words. In contrast, as high level semantic concepts of
    terms, aspects usually have more generalizable representations. However, conventional
    methods cannot utilize the information of aspects and terms at the same time,
    because few datasets are annotated with both aspects and terms. In this paper,
    we propose a novel deep memory network with auxiliary memory to address this problem.
    In our model, a main memory is used to capture the important context words for
    sentiment classification. In addition, we build an auxiliary memory to implicitly
    convert aspects and terms to each other, and feed both of them to the main memory.
    With the interaction between two memories, the features of aspects and terms can
    be learnt simultaneously. We compare our model with the state-of-the-art methods
    on four datasets from different domains. The experimental results demonstrate
    the effectiveness of our model.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Peisong
    full: Peisong Zhu
    id: peisong-zhu
    last: Zhu
  - first: Tieyun
    full: Tieyun Qian
    id: tieyun-qian
    last: Qian
  author_string: Peisong Zhu, Tieyun Qian
  bibkey: zhu-qian-2018-enhanced
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1077'
  page_last: '1087'
  pages: "1077\u20131087"
  paper_id: '92'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1092.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1092.jpg
  title: Enhanced Aspect Level Sentiment Classification with Auxiliary Memory
  title_html: Enhanced Aspect Level Sentiment Classification with Auxiliary Memory
  url: https://www.aclweb.org/anthology/C18-1092
  year: '2018'
C18-1093:
  abstract: The rapid growth of social media in recent years has fed into some highly
    undesirable phenomena such as proliferation of hateful and offensive language
    on the Internet. Previous research suggests that such abusive content tends to
    come from users who share a set of common stereotypes and form communities around
    them. The current state-of-the-art approaches to abuse detection are oblivious
    to user and community information and rely entirely on textual (i.e., lexical
    and semantic) cues. In this paper, we propose a novel approach to this problem
    that incorporates community-based profiling features of Twitter users. Experimenting
    with a dataset of 16k tweets, we show that our methods significantly outperform
    the current state of the art in abuse detection. Further, we conduct a qualitative
    analysis of model characteristics. We release our code, pre-trained models and
    all the resources used in the public domain.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Pushkar
    full: Pushkar Mishra
    id: pushkar-mishra
    last: Mishra
  - first: Marco
    full: Marco Del Tredici
    id: marco-del-tredici
    last: Del Tredici
  - first: Helen
    full: Helen Yannakoudakis
    id: helen-yannakoudakis
    last: Yannakoudakis
  - first: Ekaterina
    full: Ekaterina Shutova
    id: ekaterina-shutova
    last: Shutova
  author_string: Pushkar Mishra, Marco Del Tredici, Helen Yannakoudakis, Ekaterina
    Shutova
  bibkey: mishra-etal-2018-author
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1088'
  page_last: '1098'
  pages: "1088\u20131098"
  paper_id: '93'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1093.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1093.jpg
  title: Author Profiling for Abuse Detection
  title_html: Author Profiling for Abuse Detection
  url: https://www.aclweb.org/anthology/C18-1093
  year: '2018'
C18-1094:
  abstract: In this position paper, we argue that building operational automated scoring
    systems is a task that has disciplinary complexity above and beyond standard competitive
    shared tasks which usually involve applying the latest machine learning techniques
    to publicly available data in order to obtain the best accuracy. Automated scoring
    systems warrant significant cross-discipline collaboration of which natural language
    processing and machine learning are just two of many important components. Such
    systems have multiple stakeholders with different but valid perspectives that
    can often times be at odds with each other. Our position is that it is essential
    for us as NLP researchers to understand and incorporate these perspectives in
    our research and work towards a mutually satisfactory solution in order to build
    automated scoring systems that are accurate, fair, unbiased, and useful.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Nitin
    full: Nitin Madnani
    id: nitin-madnani
    last: Madnani
  - first: Aoife
    full: Aoife Cahill
    id: aoife-cahill
    last: Cahill
  author_string: Nitin Madnani, Aoife Cahill
  bibkey: madnani-cahill-2018-automated
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1099'
  page_last: '1109'
  pages: "1099\u20131109"
  paper_id: '94'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1094.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1094.jpg
  title: 'Automated Scoring: Beyond Natural Language Processing'
  title_html: 'Automated Scoring: Beyond Natural Language Processing'
  url: https://www.aclweb.org/anthology/C18-1094
  year: '2018'
C18-1095:
  abstract: Review text has been widely studied in traditional tasks such as sentiment
    analysis and aspect extraction. However, to date, no work is towards the abstractive
    review summarization that is essential for business organizations and individual
    consumers to make informed decisions. This work takes the lead to study the aspect/sentiment-aware
    abstractive review summarization by exploring multi-factor attentions. Specifically,
    we propose an interactive attention mechanism to interactively learns the representations
    of context words, sentiment words and aspect words within the reviews, acted as
    an encoder. The learned sentiment and aspect representations are incorporated
    into the decoder to generate aspect/sentiment-aware review summaries via an attention
    fusion network. In addition, the abstractive summarizer is jointly trained with
    the text categorization task, which helps learn a category-specific text encoder,
    locating salient aspect information and exploring the variations of style and
    wording of content with respect to different text categories. The experimental
    results on a real-life dataset demonstrate that our model achieves impressive
    results compared to other strong competitors.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Min
    full: Min Yang
    id: min-yang
    last: Yang
  - first: Qiang
    full: Qiang Qu
    id: qiang-qu
    last: Qu
  - first: Ying
    full: Ying Shen
    id: ying-shen
    last: Shen
  - first: Qiao
    full: Qiao Liu
    id: qiao-liu
    last: Liu
  - first: Wei
    full: Wei Zhao
    id: wei-zhao
    last: Zhao
  - first: Jia
    full: Jia Zhu
    id: jia-zhu
    last: Zhu
  author_string: Min Yang, Qiang Qu, Ying Shen, Qiao Liu, Wei Zhao, Jia Zhu
  bibkey: yang-etal-2018-aspect
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1110'
  page_last: '1120'
  pages: "1110\u20131120"
  paper_id: '95'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1095.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1095.jpg
  title: Aspect and Sentiment Aware Abstractive Review Summarization
  title_html: Aspect and Sentiment Aware Abstractive Review Summarization
  url: https://www.aclweb.org/anthology/C18-1095
  year: '2018'
C18-1096:
  abstract: Aspect-level sentiment classification aims to determine the sentiment
    polarity of a review sentence towards an opinion target. A sentence could contain
    multiple sentiment-target pairs; thus the main challenge of this task is to separate
    different opinion contexts for different targets. To this end, attention mechanism
    has played an important role in previous state-of-the-art neural models. The mechanism
    is able to capture the importance of each context word towards a target by modeling
    their semantic associations. We build upon this line of research and propose two
    novel approaches for improving the effectiveness of attention. First, we propose
    a method for target representation that better captures the semantic meaning of
    the opinion target. Second, we introduce an attention model that incorporates
    syntactic information into the attention mechanism. We experiment on attention-based
    LSTM (Long Short-Term Memory) models using the datasets from SemEval 2014, 2015,
    and 2016. The experimental results show that the conventional attention-based
    LSTM can be substantially improved by incorporating the two approaches.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Ruidan
    full: Ruidan He
    id: ruidan-he
    last: He
  - first: Wee Sun
    full: Wee Sun Lee
    id: wee-sun-lee
    last: Lee
  - first: Hwee Tou
    full: Hwee Tou Ng
    id: hwee-tou-ng
    last: Ng
  - first: Daniel
    full: Daniel Dahlmeier
    id: daniel-dahlmeier
    last: Dahlmeier
  author_string: Ruidan He, Wee Sun Lee, Hwee Tou Ng, Daniel Dahlmeier
  bibkey: he-etal-2018-effective
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1121'
  page_last: '1131'
  pages: "1121\u20131131"
  paper_id: '96'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1096.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1096.jpg
  title: Effective Attention Modeling for Aspect-Level Sentiment Classification
  title_html: Effective Attention Modeling for Aspect-Level Sentiment Classification
  url: https://www.aclweb.org/anthology/C18-1096
  year: '2018'
C18-1097:
  abstract: Lack of repeatability and generalisability are two significant threats
    to continuing scientific development in Natural Language Processing. Language
    models and learning methods are so complex that scientific conference papers no
    longer contain enough space for the technical depth required for replication or
    reproduction. Taking Target Dependent Sentiment Analysis as a case study, we show
    how recent work in the field has not consistently released code, or described
    settings for learning methods in enough detail, and lacks comparability and generalisability
    in train, test or validation data. To investigate generalisability and to enable
    state of the art comparative evaluations, we carry out the first reproduction
    studies of three groups of complementary methods and perform the first large-scale
    mass evaluation on six different English datasets. Reflecting on our experiences,
    we recommend that future replication or reproduction experiments should always
    consider a variety of datasets alongside documenting and releasing their methods
    and published code in order to minimise the barriers to both repeatability and
    generalisability. We have released our code with a model zoo on GitHub with Jupyter
    Notebooks to aid understanding and full documentation, and we recommend that others
    do the same with their papers at submission time through an anonymised GitHub
    account.
  address: Santa Fe, New Mexico, USA
  attachment:
  - filename: C18-1097.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/C18-1097.Presentation.pdf
  author:
  - first: Andrew
    full: Andrew Moore
    id: andrew-moore
    last: Moore
  - first: Paul
    full: Paul Rayson
    id: paul-rayson
    last: Rayson
  author_string: Andrew Moore, Paul Rayson
  bibkey: moore-rayson-2018-bringing
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1132'
  page_last: '1144'
  pages: "1132\u20131144"
  paper_id: '97'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1097.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1097.jpg
  title: 'Bringing replication and reproduction together with generalisability in
    NLP: Three reproduction studies for Target Dependent Sentiment Analysis'
  title_html: 'Bringing replication and reproduction together with generalisability
    in <span class="acl-fixed-case">NLP</span>: Three reproduction studies for Target
    Dependent Sentiment Analysis'
  url: https://www.aclweb.org/anthology/C18-1097
  year: '2018'
C18-1098:
  abstract: 'Rationale-based models provide a unique way to provide justifiable results
    for relation classification models by identifying rationales (key words and phrases
    that a person can use to justify the relation in the sentence) during the process.
    However, existing generative networks used to extract rationales come with a trade-off
    between extracting diversified rationales and achieving good classification results.
    In this paper, we propose a multilevel heuristic approach to regulate rationale
    extraction to avoid extracting monotonous rationales without compromising classification
    performance. In our model, rationale selection is regularized by a semi-supervised
    process and features from different levels: word, syntax, sentence, and corpus.
    We evaluate our approach on the SemEval 2010 dataset that includes 19 relation
    classes and the quality of extracted rationales with our manually-labeled rationales.
    Experiments show a significant improvement in classification performance and a
    20% gain in rationale interpretability compared to state-of-the-art approaches.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Shiou Tian
    full: Shiou Tian Hsu
    id: shiou-tian-hsu
    last: Hsu
  - first: Mandar
    full: Mandar Chaudhary
    id: mandar-chaudhary
    last: Chaudhary
  - first: Nagiza
    full: Nagiza Samatova
    id: nagiza-samatova
    last: Samatova
  author_string: Shiou Tian Hsu, Mandar Chaudhary, Nagiza Samatova
  bibkey: hsu-etal-2018-multilevel
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1145'
  page_last: '1155'
  pages: "1145\u20131155"
  paper_id: '98'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1098.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1098.jpg
  title: Multilevel Heuristics for Rationale-Based Entity Relation Classification
    in Sentences
  title_html: Multilevel Heuristics for Rationale-Based Entity Relation Classification
    in Sentences
  url: https://www.aclweb.org/anthology/C18-1098
  year: '2018'
C18-1099:
  abstract: Multi-lingual relation extraction aims to find unknown relational facts
    from text in various languages. Existing models cannot well capture the consistency
    and diversity of relation patterns in different languages. To address these issues,
    we propose an adversarial multi-lingual neural relation extraction (AMNRE) model,
    which builds both consistent and individual representations for each sentence
    to consider the consistency and diversity among languages. Further, we adopt an
    adversarial training strategy to ensure those consistent sentence representations
    could effectively extract the language-consistent relation patterns. The experimental
    results on real-world datasets demonstrate that our AMNRE model significantly
    outperforms the state-of-the-art models. The source code of this paper can be
    obtained from https://github.com/thunlp/AMNRE.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Xiaozhi
    full: Xiaozhi Wang
    id: xiaozhi-wang
    last: Wang
  - first: Xu
    full: Xu Han
    id: xu-han
    last: Han
  - first: Yankai
    full: Yankai Lin
    id: yankai-lin
    last: Lin
  - first: Zhiyuan
    full: Zhiyuan Liu
    id: zhiyuan-liu
    last: Liu
  - first: Maosong
    full: Maosong Sun
    id: maosong-sun
    last: Sun
  author_string: Xiaozhi Wang, Xu Han, Yankai Lin, Zhiyuan Liu, Maosong Sun
  bibkey: wang-etal-2018-adversarial
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1156'
  page_last: '1166'
  pages: "1156\u20131166"
  paper_id: '99'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1099.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1099.jpg
  title: Adversarial Multi-lingual Neural Relation Extraction
  title_html: Adversarial Multi-lingual Neural Relation Extraction
  url: https://www.aclweb.org/anthology/C18-1099
  year: '2018'
C18-1100:
  abstract: "Relation classification is an important task in natural language processing\
    \ fields. State-of-the-art methods usually concentrate on building deep neural\
    \ networks based classification models on the training data in which the relations\
    \ of the labeled entity pairs are given. However, these methods usually suffer\
    \ from the data sparsity issue greatly. On the other hand, we notice that it is\
    \ very easily to obtain some concise text descriptions for almost all of the entities\
    \ in a relation classification task. The text descriptions can provide helpful\
    \ supplementary information for relation classification. But they are ignored\
    \ by most of existing methods. In this paper, we propose DesRC, a new neural relation\
    \ classification method which integrates entities\u2019 text descriptions into\
    \ deep neural networks models. We design a two-level attention mechanism to select\
    \ the most useful information from the \u201Cintra-sentence\u201D aspect and the\
    \ \u201Ccross-sentence\u201D aspect. Besides, the adversarial training method\
    \ is also used to further improve the classification per-formance. Finally, we\
    \ evaluate the proposed method on the SemEval 2010 dataset. Extensive experiments\
    \ show that our method achieves much better experimental results than other state-of-the-art\
    \ relation classification methods."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Feiliang
    full: Feiliang Ren
    id: feiliang-ren
    last: Ren
  - first: Di
    full: Di Zhou
    id: di-zhou
    last: Zhou
  - first: Zhihui
    full: Zhihui Liu
    id: zhihui-liu
    last: Liu
  - first: Yongcheng
    full: Yongcheng Li
    id: yongcheng-li
    last: Li
  - first: Rongsheng
    full: Rongsheng Zhao
    id: rongsheng-zhao
    last: Zhao
  - first: Yongkang
    full: Yongkang Liu
    id: yongkang-liu
    last: Liu
  - first: Xiaobo
    full: Xiaobo Liang
    id: xiaobo-liang
    last: Liang
  author_string: Feiliang Ren, Di Zhou, Zhihui Liu, Yongcheng Li, Rongsheng Zhao,
    Yongkang Liu, Xiaobo Liang
  bibkey: ren-etal-2018-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1167'
  page_last: '1177'
  pages: "1167\u20131177"
  paper_id: '100'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1100.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1100.jpg
  title: Neural Relation Classification with Text Descriptions
  title_html: Neural Relation Classification with Text Descriptions
  url: https://www.aclweb.org/anthology/C18-1100
  year: '2018'
C18-1101:
  abstract: Generating an abstract from a collection of documents is a desirable capability
    for many real-world applications. However, abstractive approaches to multi-document
    summarization have not been thoroughly investigated. This paper studies the feasibility
    of using Abstract Meaning Representation (AMR), a semantic representation of natural
    language grounded in linguistic theory, as a form of content representation. Our
    approach condenses source documents to a set of summary graphs following the AMR
    formalism. The summary graphs are then transformed to a set of summary sentences
    in a surface realization step. The framework is fully data-driven and flexible.
    Each component can be optimized independently using small-scale, in-domain training
    data. We perform experiments on benchmark summarization datasets and report promising
    results. We also describe opportunities and challenges for advancing this line
    of research.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Kexin
    full: Kexin Liao
    id: kexin-liao
    last: Liao
  - first: Logan
    full: Logan Lebanoff
    id: logan-lebanoff
    last: Lebanoff
  - first: Fei
    full: Fei Liu
    id: fei-liu-utdallas
    last: Liu
  author_string: Kexin Liao, Logan Lebanoff, Fei Liu
  bibkey: liao-etal-2018-abstract
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1178'
  page_last: '1190'
  pages: "1178\u20131190"
  paper_id: '101'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1101.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1101.jpg
  title: Abstract Meaning Representation for Multi-Document Summarization
  title_html: Abstract Meaning Representation for Multi-Document Summarization
  url: https://www.aclweb.org/anthology/C18-1101
  year: '2018'
C18-1102:
  abstract: In this work, we aim at developing an unsupervised abstractive summarization
    system in the multi-document setting. We design a paraphrastic sentence fusion
    model which jointly performs sentence fusion and paraphrasing using skip-gram
    word embedding model at the sentence level. Our model improves the information
    coverage and at the same time abstractiveness of the generated sentences. We conduct
    our experiments on the human-generated multi-sentence compression datasets and
    evaluate our system on several newly proposed Machine Translation (MT) evaluation
    metrics. Furthermore, we apply our sentence level model to implement an abstractive
    multi-document summarization system where documents usually contain a related
    set of sentences. We also propose an optimal solution for the classical summary
    length limit problem which was not addressed in the past research. For the document
    level summary, we conduct experiments on the datasets of two different domains
    (e.g., news article and user reviews) which are well suited for multi-document
    abstractive summarization. Our experiments demonstrate that the methods bring
    significant improvements over the state-of-the-art methods.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Mir Tafseer
    full: Mir Tafseer Nayeem
    id: mir-tafseer-nayeem
    last: Nayeem
  - first: Tanvir Ahmed
    full: Tanvir Ahmed Fuad
    id: tanvir-ahmed-fuad
    last: Fuad
  - first: Yllias
    full: Yllias Chali
    id: yllias-chali
    last: Chali
  author_string: Mir Tafseer Nayeem, Tanvir Ahmed Fuad, Yllias Chali
  bibkey: nayeem-etal-2018-abstractive
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1191'
  page_last: '1204'
  pages: "1191\u20131204"
  paper_id: '102'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1102.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1102.jpg
  title: Abstractive Unsupervised Multi-Document Summarization using Paraphrastic
    Sentence Fusion
  title_html: Abstractive Unsupervised Multi-Document Summarization using Paraphrastic
    Sentence Fusion
  url: https://www.aclweb.org/anthology/C18-1102
  year: '2018'
C18-1103:
  abstract: Domain Adaptation arises when we aim at learning from source domain a
    model that can perform acceptably well on a different target domain. It is especially
    crucial for Natural Language Generation (NLG) in Spoken Dialogue Systems when
    there are sufficient annotated data in the source domain, but there is a limited
    labeled data in the target domain. How to effectively utilize as much of existing
    abilities from source domains is a crucial issue in domain adaptation. In this
    paper, we propose an adversarial training procedure to train a Variational encoder-decoder
    based language generator via multiple adaptation steps. In this procedure, a model
    is first trained on a source domain data and then fine-tuned on a small set of
    target domain utterances under the guidance of two proposed critics. Experimental
    results show that the proposed method can effectively leverage the existing knowledge
    in the source domain to adapt to another related domain by using only a small
    amount of in-domain data.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Van-Khanh
    full: Van-Khanh Tran
    id: van-khanh-tran
    last: Tran
  - first: Le-Minh
    full: Le-Minh Nguyen
    id: minh-le-nguyen
    last: Nguyen
  author_string: Van-Khanh Tran, Le-Minh Nguyen
  bibkey: tran-nguyen-2018-adversarial
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1205'
  page_last: '1217'
  pages: "1205\u20131217"
  paper_id: '103'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1103.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1103.jpg
  title: Adversarial Domain Adaptation for Variational Neural Language Generation
    in Dialogue Systems
  title_html: Adversarial Domain Adaptation for Variational Neural Language Generation
    in Dialogue Systems
  url: https://www.aclweb.org/anthology/C18-1103
  year: '2018'
C18-1104:
  abstract: Our goal is to explore how the abilities brought in by a dialogue manager
    can be included in end-to-end visually grounded conversational agents. We make
    initial steps towards this general goal by augmenting a task-oriented visual dialogue
    model with a decision-making component that decides whether to ask a follow-up
    question to identify a target referent in an image, or to stop the conversation
    to make a guess. Our analyses show that adding a decision making component produces
    dialogues that are less repetitive and that include fewer unnecessary questions,
    thus potentially leading to more efficient and less unnatural interactions.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Ravi
    full: Ravi Shekhar
    id: ravi-shekhar
    last: Shekhar
  - first: Tim
    full: "Tim Baumg\xE4rtner"
    id: tim-baumgartner
    last: "Baumg\xE4rtner"
  - first: Aashish
    full: Aashish Venkatesh
    id: aashish-venkatesh
    last: Venkatesh
  - first: Elia
    full: Elia Bruni
    id: elia-bruni
    last: Bruni
  - first: Raffaella
    full: Raffaella Bernardi
    id: raffaella-bernardi
    last: Bernardi
  - first: Raquel
    full: Raquel Fernandez
    id: raquel-fernandez
    last: Fernandez
  author_string: "Ravi Shekhar, Tim Baumg\xE4rtner, Aashish Venkatesh, Elia Bruni,\
    \ Raffaella Bernardi, Raquel Fernandez"
  bibkey: shekhar-etal-2018-ask
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1218'
  page_last: '1233'
  pages: "1218\u20131233"
  paper_id: '104'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1104.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1104.jpg
  title: 'Ask No More: Deciding when to guess in referential visual dialogue'
  title_html: 'Ask No More: Deciding when to guess in referential visual dialogue'
  url: https://www.aclweb.org/anthology/C18-1104
  year: '2018'
C18-1105:
  abstract: "In this paper, we study the problem of data augmentation for language\
    \ understanding in task-oriented dialogue system. In contrast to previous work\
    \ which augments an utterance without considering its relation with other utterances,\
    \ we propose a sequence-to-sequence generation based data augmentation framework\
    \ that leverages one utterance\u2019s same semantic alternatives in the training\
    \ data. A novel diversity rank is incorporated into the utterance representation\
    \ to make the model produce diverse utterances and these diversely augmented utterances\
    \ help to improve the language understanding module. Experimental results on the\
    \ Airline Travel Information System dataset and a newly created semantic frame\
    \ annotation on Stanford Multi-turn, Multi-domain Dialogue Dataset show that our\
    \ framework achieves significant improvements of 6.38 and 10.04 F-scores respectively\
    \ when only a training set of hundreds utterances is represented. Case studies\
    \ also confirm that our method generates diverse utterances."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yutai
    full: Yutai Hou
    id: yutai-hou
    last: Hou
  - first: Yijia
    full: Yijia Liu
    id: yijia-liu
    last: Liu
  - first: Wanxiang
    full: Wanxiang Che
    id: wanxiang-che
    last: Che
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  author_string: Yutai Hou, Yijia Liu, Wanxiang Che, Ting Liu
  bibkey: hou-etal-2018-sequence
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1234'
  page_last: '1245'
  pages: "1234\u20131245"
  paper_id: '105'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1105.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1105.jpg
  title: Sequence-to-Sequence Data Augmentation for Dialogue Language Understanding
  title_html: Sequence-to-Sequence Data Augmentation for Dialogue Language Understanding
  url: https://www.aclweb.org/anthology/C18-1105
  year: '2018'
C18-1106:
  abstract: "The utility of additional semantic information for the task of next utterance\
    \ selection in an automated dialogue system is the focus of study in this paper.\
    \ In particular, we show that additional information available in the form of\
    \ dialogue acts \u2013when used along with context given in the form of dialogue\
    \ history\u2013 improves the performance irrespective of the underlying model\
    \ being generative or discriminative. In order to show the model agnostic behavior\
    \ of dialogue acts, we experiment with several well-known models such as sequence-to-sequence\
    \ encoder-decoder model, hierarchical encoder-decoder model, and Siamese-based\
    \ models with and without hierarchy; and show that in all models, incorporating\
    \ dialogue acts improves the performance by a significant margin. We, furthermore,\
    \ propose a novel way of encoding dialogue act information, and use it along with\
    \ hierarchical encoder to build a model that can use the sequential dialogue act\
    \ information in a natural way. Our proposed model achieves an MRR of about 84.8%\
    \ for the task of next utterance selection on a newly introduced Daily Dialogue\
    \ dataset, and outperform the baseline models. We also provide a detailed analysis\
    \ of results including key insights that explain the improvement in MRR because\
    \ of dialog act information."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Harshit
    full: Harshit Kumar
    id: harshit-kumar
    last: Kumar
  - first: Arvind
    full: Arvind Agarwal
    id: arvind-agarwal
    last: Agarwal
  - first: Sachindra
    full: Sachindra Joshi
    id: sachindra-joshi
    last: Joshi
  author_string: Harshit Kumar, Arvind Agarwal, Sachindra Joshi
  bibkey: kumar-etal-2018-dialogue
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1246'
  page_last: '1256'
  pages: "1246\u20131256"
  paper_id: '106'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1106.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1106.jpg
  title: 'Dialogue-act-driven Conversation Model : An Experimental Study'
  title_html: 'Dialogue-act-driven Conversation Model : An Experimental Study'
  url: https://www.aclweb.org/anthology/C18-1106
  year: '2018'
C18-1107:
  abstract: Recently, deep reinforcement learning (DRL) has been used for dialogue
    policy optimization. However, many DRL-based policies are not sample-efficient.
    Most recent advances focus on improving DRL optimization algorithms to address
    this issue. Here, we take an alternative route of designing neural network structure
    that is better suited for DRL-based dialogue management. The proposed structured
    deep reinforcement learning is based on graph neural networks (GNN), which consists
    of some sub-networks, each one for a node on a directed graph. The graph is defined
    according to the domain ontology and each node can be considered as a sub-agent.
    During decision making, these sub-agents have internal message exchange between
    neighbors on the graph. We also propose an approach to jointly optimize the graph
    structure as well as the parameters of GNN. Experiments show that structured DRL
    significantly outperforms previous state-of-the-art approaches in almost all of
    the 18 tasks of the PyDial benchmark.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Lu
    full: Lu Chen
    id: lu-chen
    last: Chen
  - first: Bowen
    full: Bowen Tan
    id: bowen-tan
    last: Tan
  - first: Sishan
    full: Sishan Long
    id: sishan-long
    last: Long
  - first: Kai
    full: Kai Yu
    id: kai-yu
    last: Yu
  author_string: Lu Chen, Bowen Tan, Sishan Long, Kai Yu
  bibkey: chen-etal-2018-structured
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1257'
  page_last: '1268'
  pages: "1257\u20131268"
  paper_id: '107'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1107.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1107.jpg
  title: Structured Dialogue Policy with Graph Neural Networks
  title_html: Structured Dialogue Policy with Graph Neural Networks
  url: https://www.aclweb.org/anthology/C18-1107
  year: '2018'
C18-1108:
  abstract: Learning social media content is the basis of many real-world applications,
    including information retrieval and recommendation systems, among others. In contrast
    with previous works that focus mainly on single modal or bi-modal learning, we
    propose to learn social media content by fusing jointly textual, acoustic, and
    visual information (JTAV). Effective strategies are proposed to extract fine-grained
    features of each modality, that is, attBiGRU and DCRNN. We also introduce cross-modal
    fusion and attentive pooling techniques to integrate multi-modal information comprehensively.
    Extensive experimental evaluation conducted on real-world datasets demonstrate
    our proposed model outperforms the state-of-the-art approaches by a large margin.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Hongru
    full: Hongru Liang
    id: hongru-liang
    last: Liang
  - first: Haozheng
    full: Haozheng Wang
    id: haozheng-wang
    last: Wang
  - first: Jun
    full: Jun Wang
    id: jun-wang
    last: Wang
  - first: Shaodi
    full: Shaodi You
    id: shaodi-you
    last: You
  - first: Zhe
    full: Zhe Sun
    id: zhe-sun
    last: Sun
  - first: Jin-Mao
    full: Jin-Mao Wei
    id: jin-mao-wei
    last: Wei
  - first: Zhenglu
    full: Zhenglu Yang
    id: zhenglu-yang
    last: Yang
  author_string: Hongru Liang, Haozheng Wang, Jun Wang, Shaodi You, Zhe Sun, Jin-Mao
    Wei, Zhenglu Yang
  bibkey: liang-etal-2018-jtav
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1269'
  page_last: '1280'
  pages: "1269\u20131280"
  paper_id: '108'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1108.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1108.jpg
  title: 'JTAV: Jointly Learning Social Media Content Representation by Fusing Textual,
    Acoustic, and Visual Features'
  title_html: '<span class="acl-fixed-case">JTAV</span>: Jointly Learning Social Media
    Content Representation by Fusing Textual, Acoustic, and Visual Features'
  url: https://www.aclweb.org/anthology/C18-1108
  year: '2018'
C18-1109:
  abstract: "Neural encoder-decoder models have been widely applied to conversational\
    \ response generation, which is a research hot spot in recent years. However,\
    \ conventional neural encoder-decoder models tend to generate commonplace responses\
    \ like \u201CI don\u2019t know\u201D regardless of what the input is. In this\
    \ paper, we analyze this problem from a new perspective: latent vectors. Based\
    \ on it, we propose an easy-to-extend learning framework named MEMD (Multi-Encoder\
    \ to Multi-Decoder), in which an auxiliary encoder and an auxiliary decoder are\
    \ introduced to provide necessary training guidance without resorting to extra\
    \ data or complicating network\u2019s inner structure. Experimental results demonstrate\
    \ that our method effectively improve the quality of generated responses according\
    \ to automatic metrics and human evaluations, yielding more diverse and smooth\
    \ replies."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Meng
    full: Meng Zou
    id: meng-zou
    last: Zou
  - first: Xihan
    full: Xihan Li
    id: xihan-li
    last: Li
  - first: Haokun
    full: Haokun Liu
    id: haokun-liu
    last: Liu
  - first: Zhihong
    full: Zhihong Deng
    id: zhi-hong-deng
    last: Deng
  author_string: Meng Zou, Xihan Li, Haokun Liu, Zhihong Deng
  bibkey: zou-etal-2018-memd
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1281'
  page_last: '1291'
  pages: "1281\u20131291"
  paper_id: '109'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1109.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1109.jpg
  title: 'MEMD: A Diversity-Promoting Learning Framework for Short-Text Conversation'
  title_html: '<span class="acl-fixed-case">MEMD</span>: A Diversity-Promoting Learning
    Framework for Short-Text Conversation'
  url: https://www.aclweb.org/anthology/C18-1109
  year: '2018'
C18-1110:
  abstract: Although neural machine translation with the encoder-decoder framework
    has achieved great success recently, it still suffers drawbacks of forgetting
    distant information, which is an inherent disadvantage of recurrent neural network
    structure, and disregarding relationship between source words during encoding
    step. Whereas in practice, the former information and relationship are often useful
    in current step. We target on solving these problems and thus introduce relation
    networks to learn better representations of the source. The relation networks
    are able to facilitate memorization capability of recurrent neural network via
    associating source words with each other, this would also help retain their relationships.
    Then the source representations and all the relations are fed into the attention
    component together while decoding, with the main encoder-decoder framework unchanged.
    Experiments on several datasets show that our method can improve the translation
    performance significantly over the conventional encoder-decoder model and even
    outperform the approach involving supervised syntactic knowledge.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Wen
    full: Wen Zhang
    id: wen-zhang
    last: Zhang
  - first: Jiawei
    full: Jiawei Hu
    id: jiawei-hu
    last: Hu
  - first: Yang
    full: Yang Feng
    id: yang-feng
    last: Feng
  - first: Qun
    full: Qun Liu
    id: qun-liu
    last: Liu
  author_string: Wen Zhang, Jiawei Hu, Yang Feng, Qun Liu
  bibkey: zhang-etal-2018-refining
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1292'
  page_last: '1303'
  pages: "1292\u20131303"
  paper_id: '110'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1110.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1110.jpg
  title: Refining Source Representations with Relation Networks for Neural Machine
    Translation
  title_html: Refining Source Representations with Relation Networks for Neural Machine
    Translation
  url: https://www.aclweb.org/anthology/C18-1110
  year: '2018'
C18-1111:
  abstract: Neural machine translation (NMT) is a deep learning based approach for
    machine translation, which yields the state-of-the-art translation performance
    in scenarios where large-scale parallel corpora are available. Although the high-quality
    and domain-specific translation is crucial in the real world, domain-specific
    corpora are usually scarce or nonexistent, and thus vanilla NMT performs poorly
    in such scenarios. Domain adaptation that leverages both out-of-domain parallel
    corpora as well as monolingual corpora for in-domain translation, is very important
    for domain-specific translation. In this paper, we give a comprehensive survey
    of the state-of-the-art domain adaptation techniques for NMT.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Chenhui
    full: Chenhui Chu
    id: chenhui-chu
    last: Chu
  - first: Rui
    full: Rui Wang
    id: rui-wang
    last: Wang
  author_string: Chenhui Chu, Rui Wang
  bibkey: chu-wang-2018-survey
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1304'
  page_last: '1319'
  pages: "1304\u20131319"
  paper_id: '111'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1111.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1111.jpg
  title: A Survey of Domain Adaptation for Neural Machine Translation
  title_html: A Survey of Domain Adaptation for Neural Machine Translation
  url: https://www.aclweb.org/anthology/C18-1111
  year: '2018'
C18-1112:
  abstract: 'In this paper, we apply different NMT models to the problem of historical
    spelling normalization for five languages: English, German, Hungarian, Icelandic,
    and Swedish. The NMT models are at different levels, have different attention
    mechanisms, and different neural network architectures. Our results show that
    NMT models are much better than SMT models in terms of character error rate. The
    vanilla RNNs are competitive to GRUs/LSTMs in historical spelling normalization.
    Transformer models perform better only when provided with more training data.
    We also find that subword-level models with a small subword vocabulary are better
    than character-level models. In addition, we propose a hybrid method which further
    improves the performance of historical spelling normalization.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Gongbo
    full: Gongbo Tang
    id: gongbo-tang
    last: Tang
  - first: Fabienne
    full: Fabienne Cap
    id: fabienne-cap
    last: Cap
  - first: Eva
    full: Eva Pettersson
    id: eva-pettersson
    last: Pettersson
  - first: Joakim
    full: Joakim Nivre
    id: joakim-nivre
    last: Nivre
  author_string: Gongbo Tang, Fabienne Cap, Eva Pettersson, Joakim Nivre
  bibkey: tang-etal-2018-evaluation
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1320'
  page_last: '1331'
  pages: "1320\u20131331"
  paper_id: '112'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1112.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1112.jpg
  title: An Evaluation of Neural Machine Translation Models on Historical Spelling
    Normalization
  title_html: An Evaluation of Neural Machine Translation Models on Historical Spelling
    Normalization
  url: https://www.aclweb.org/anthology/C18-1112
  year: '2018'
C18-1113:
  abstract: "Previous work on the problem of Arabic Dialect Identification typically\
    \ targeted coarse-grained five dialect classes plus Standard Arabic (6-way classification).\
    \ This paper presents the first results on a fine-grained dialect classification\
    \ task covering 25 specific cities from across the Arab World, in addition to\
    \ Standard Arabic \u2013 a very challenging task. We build several classification\
    \ systems and explore a large space of features. Our results show that we can\
    \ identify the exact city of a speaker at an accuracy of 67.9% for sentences with\
    \ an average length of 7 words (a 9% relative error reduction over the state-of-the-art\
    \ technique for Arabic dialect identification) and reach more than 90% when we\
    \ consider 16 words. We also report on additional insights from a data analysis\
    \ of similarity and difference across Arabic dialects."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Mohammad
    full: Mohammad Salameh
    id: mohammad-salameh
    last: Salameh
  - first: Houda
    full: Houda Bouamor
    id: houda-bouamor
    last: Bouamor
  - first: Nizar
    full: Nizar Habash
    id: nizar-habash
    last: Habash
  author_string: Mohammad Salameh, Houda Bouamor, Nizar Habash
  bibkey: salameh-etal-2018-fine
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1332'
  page_last: '1344'
  pages: "1332\u20131344"
  paper_id: '113'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1113.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1113.jpg
  title: Fine-Grained Arabic Dialect Identification
  title_html: Fine-Grained <span class="acl-fixed-case">A</span>rabic Dialect Identification
  url: https://www.aclweb.org/anthology/C18-1113
  year: '2018'
C18-1114:
  abstract: Most approaches to emotion analysis in fictional texts focus on detecting
    the emotion expressed in text. We argue that this is a simplification which leads
    to an overgeneralized interpretation of the results, as it does not take into
    account who experiences an emotion and why. Emotions play a crucial role in the
    interaction between characters and the events they are involved in. Until today,
    no specific corpora that capture such an interaction were available for literature.
    We aim at filling this gap and present a publicly available corpus based on Project
    Gutenberg, REMAN (Relational EMotion ANnotation), manually annotated for spans
    which correspond to emotion trigger phrases and entities/events in the roles of
    experiencers, targets, and causes of the emotion. We provide baseline results
    for the automatic prediction of these relational structures and show that emotion
    lexicons are not able to encompass the high variability of emotion expressions
    and demonstrate that statistical models benefit from joint modeling of emotions
    with its roles in all subtasks. The corpus that we provide enables future research
    on the recognition of emotions and associated entities in text. It supports qualitative
    literary studies and digital humanities. The corpus is available at http://www.ims.uni-stuttgart.de/data/reman
    .
  address: Santa Fe, New Mexico, USA
  author:
  - first: Evgeny
    full: Evgeny Kim
    id: evgeny-kim
    last: Kim
  - first: Roman
    full: Roman Klinger
    id: roman-klinger
    last: Klinger
  author_string: Evgeny Kim, Roman Klinger
  bibkey: kim-klinger-2018-feels
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1345'
  page_last: '1359'
  pages: "1345\u20131359"
  paper_id: '114'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1114.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1114.jpg
  title: Who Feels What and Why? Annotation of a Literature Corpus with Semantic Roles
    of Emotions
  title_html: Who Feels What and Why? Annotation of a Literature Corpus with Semantic
    Roles of Emotions
  url: https://www.aclweb.org/anthology/C18-1114
  year: '2018'
C18-1115:
  abstract: We show that the general problem of string transduction can be reduced
    to the problem of sequence labeling. While character deletion and insertions are
    allowed in string transduction, they do not exist in sequence labeling. We show
    how to overcome this difference. Our approach can be used with any sequence labeling
    algorithm and it works best for problems in which string transduction imposes
    a strong notion of locality (no long range dependencies). We experiment with spelling
    correction for social media, OCR correction, and morphological inflection, and
    we see that it behaves better than seq2seq models and yields state-of-the-art
    results in several cases.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Joana
    full: Joana Ribeiro
    id: joana-ribeiro
    last: Ribeiro
  - first: Shashi
    full: Shashi Narayan
    id: shashi-narayan
    last: Narayan
  - first: Shay B.
    full: Shay B. Cohen
    id: shay-b-cohen
    last: Cohen
  - first: Xavier
    full: Xavier Carreras
    id: xavier-carreras
    last: Carreras
  author_string: Joana Ribeiro, Shashi Narayan, Shay B. Cohen, Xavier Carreras
  bibkey: ribeiro-etal-2018-local
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1360'
  page_last: '1371'
  pages: "1360\u20131371"
  paper_id: '115'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1115.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1115.jpg
  title: Local String Transduction as Sequence Labeling
  title_html: Local String Transduction as Sequence Labeling
  url: https://www.aclweb.org/anthology/C18-1115
  year: '2018'
C18-1116:
  abstract: Wikipedia provides an invaluable source of parallel multilingual data,
    which are in high demand for various sorts of linguistic inquiry, including both
    theoretical and practical studies. We introduce a novel end-to-end neural model
    for large-scale parallel data harvesting from Wikipedia. Our model is language-independent,
    robust, and highly scalable. We use our system for collecting parallel German-English,
    French-English and Persian-English sentences. Human evaluations at the end show
    the strong performance of this model in collecting high-quality parallel data.
    We also propose a statistical framework which extends the results of our human
    evaluation to other language pairs. Our model also obtained a state-of-the-art
    result on the German-English dataset of BUCC 2017 shared task on parallel sentence
    extraction from comparable corpora.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Ahmad
    full: Ahmad Aghaebrahimian
    id: ahmad-aghaebrahimian
    last: Aghaebrahimian
  author_string: Ahmad Aghaebrahimian
  bibkey: aghaebrahimian-2018-deep
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1372'
  page_last: '1383'
  pages: "1372\u20131383"
  paper_id: '116'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1116.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1116.jpg
  title: Deep Neural Networks at the Service of Multilingual Parallel Sentence Extraction
  title_html: Deep Neural Networks at the Service of Multilingual Parallel Sentence
    Extraction
  url: https://www.aclweb.org/anthology/C18-1116
  year: '2018'
C18-1117:
  abstract: Recent years have witnessed a surge of publications aimed at tracing temporal
    changes in lexical semantics using distributional methods, particularly prediction-based
    word embedding models. However, this vein of research lacks the cohesion, common
    terminology and shared practices of more established areas of natural language
    processing. In this paper, we survey the current state of academic research related
    to diachronic word embeddings and semantic shifts detection. We start with discussing
    the notion of semantic shifts, and then continue with an overview of the existing
    methods for tracing such time-related shifts with word embedding models. We propose
    several axes along which these methods can be compared, and outline the main challenges
    before this emerging subfield of NLP, as well as prospects and possible applications.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Andrey
    full: Andrey Kutuzov
    id: andrey-kutuzov
    last: Kutuzov
  - first: Lilja
    full: "Lilja \xD8vrelid"
    id: lilja-ovrelid
    last: "\xD8vrelid"
  - first: Terrence
    full: Terrence Szymanski
    id: terrence-szymanski
    last: Szymanski
  - first: Erik
    full: Erik Velldal
    id: erik-velldal
    last: Velldal
  author_string: "Andrey Kutuzov, Lilja \xD8vrelid, Terrence Szymanski, Erik Velldal"
  bibkey: kutuzov-etal-2018-diachronic
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1384'
  page_last: '1397'
  pages: "1384\u20131397"
  paper_id: '117'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1117.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1117.jpg
  title: 'Diachronic word embeddings and semantic shifts: a survey'
  title_html: 'Diachronic word embeddings and semantic shifts: a survey'
  url: https://www.aclweb.org/anthology/C18-1117
  year: '2018'
C18-1118:
  abstract: Traditional topic models are insufficient for topic extraction in social
    media. The existing methods only consider text information or simultaneously model
    the posts and the static characteristics of social media. They ignore that one
    discusses diverse topics when dynamically interacting with different people. Moreover,
    people who talk about the same topic have different effects on the topic. In this
    paper, we propose an Interaction-Aware Topic Model (IATM) for microblog conversations
    by integrating network embedding and user attention. A conversation network linking
    users based on reposting and replying relationship is constructed to mine the
    dynamic user behaviours. We model dynamic interactions and user attention so as
    to learn interaction-aware edge embeddings with social context. Then they are
    incorporated into neural variational inference for generating the more consistent
    topics. The experiments on three real-world datasets show that our proposed model
    is effective.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Ruifang
    full: Ruifang He
    id: ruifang-he
    last: He
  - first: Xuefei
    full: Xuefei Zhang
    id: xuefei-zhang
    last: Zhang
  - first: Di
    full: Di Jin
    id: di-jin
    last: Jin
  - first: Longbiao
    full: Longbiao Wang
    id: longbiao-wang
    last: Wang
  - first: Jianwu
    full: Jianwu Dang
    id: jianwu-dang
    last: Dang
  - first: Xiangang
    full: Xiangang Li
    id: xiangang-li
    last: Li
  author_string: Ruifang He, Xuefei Zhang, Di Jin, Longbiao Wang, Jianwu Dang, Xiangang
    Li
  bibkey: he-etal-2018-interaction
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1398'
  page_last: '1409'
  pages: "1398\u20131409"
  paper_id: '118'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1118.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1118.jpg
  title: Interaction-Aware Topic Model for Microblog Conversations through Network
    Embedding and User Attention
  title_html: Interaction-Aware Topic Model for Microblog Conversations through Network
    Embedding and User Attention
  url: https://www.aclweb.org/anthology/C18-1118
  year: '2018'
C18-1119:
  abstract: In realistic scenarios, a user profiling model (e.g., gender classification
    or age regression) learned from one social media might perform rather poorly when
    tested on another social media due to the different data distributions in the
    two media. In this paper, we address cross-media user profiling by bridging the
    knowledge between the source and target media with a uniform user embedding learning
    approach. In our approach, we first construct a cross-media user-word network
    to capture the relationship among users through the textual information and a
    modified cross-media user-user network to capture the relationship among users
    through the social information. Then, we learn user embedding by jointly learning
    the heterogeneous network composed of above two networks. Finally, we train a
    classification (or regression) model with the obtained user embeddings as input
    to perform user profiling. Empirical studies demonstrate the effectiveness of
    the proposed approach to two cross-media user profiling tasks, i.e., cross-media
    gender classification and cross-media age regression.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Jingjing
    full: Jingjing Wang
    id: jingjing-wang
    last: Wang
  - first: Shoushan
    full: Shoushan Li
    id: shoushan-li
    last: Li
  - first: Mingqi
    full: Mingqi Jiang
    id: mingqi-jiang
    last: Jiang
  - first: Hanqian
    full: Hanqian Wu
    id: hanqian-wu
    last: Wu
  - first: Guodong
    full: Guodong Zhou
    id: guodong-zhou
    last: Zhou
  author_string: Jingjing Wang, Shoushan Li, Mingqi Jiang, Hanqian Wu, Guodong Zhou
  bibkey: wang-etal-2018-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1410'
  page_last: '1420'
  pages: "1410\u20131420"
  paper_id: '119'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1119.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1119.jpg
  title: Cross-media User Profiling with Joint Textual and Social User Embedding
  title_html: Cross-media User Profiling with Joint Textual and Social User Embedding
  url: https://www.aclweb.org/anthology/C18-1119
  year: '2018'
C18-1120:
  abstract: Incorporating syntactic information in Neural Machine Translation (NMT)
    can lead to better reorderings, particularly useful when the language pairs are
    syntactically highly divergent or when the training bitext is not large. Previous
    work on using syntactic information, provided by top-1 parse trees generated by
    (inevitably error-prone) parsers, has been promising. In this paper, we propose
    a forest-to-sequence NMT model to make use of exponentially many parse trees of
    the source sentence to compensate for the parser errors. Our method represents
    the collection of parse trees as a packed forest, and learns a neural transducer
    to translate from the input forest to the target sentence. Experiments on English
    to German, Chinese and Farsi translation tasks show the superiority of our approach
    over the sequence-to-sequence and tree-to-sequence neural translation models.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Poorya
    full: Poorya Zaremoodi
    id: poorya-zaremoodi
    last: Zaremoodi
  - first: Gholamreza
    full: Gholamreza Haffari
    id: gholamreza-haffari
    last: Haffari
  author_string: Poorya Zaremoodi, Gholamreza Haffari
  bibkey: zaremoodi-haffari-2018-incorporating
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1421'
  page_last: '1429'
  pages: "1421\u20131429"
  paper_id: '120'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1120.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1120.jpg
  title: Incorporating Syntactic Uncertainty in Neural Machine Translation with a
    Forest-to-Sequence Model
  title_html: Incorporating Syntactic Uncertainty in Neural Machine Translation with
    a Forest-to-Sequence Model
  url: https://www.aclweb.org/anthology/C18-1120
  year: '2018'
C18-1121:
  abstract: In this paper, we investigate the sentence summarization task that produces
    a summary from a source sentence. Neural sequence-to-sequence models have gained
    considerable success for this task, while most existing approaches only focus
    on improving the informativeness of the summary, which ignore the correctness,
    i.e., the summary should not contain unrelated information with respect to the
    source sentence. We argue that correctness is an essential requirement for summarization
    systems. Considering a correct summary is semantically entailed by the source
    sentence, we incorporate entailment knowledge into abstractive summarization models.
    We propose an entailment-aware encoder under multi-task framework (i.e., summarization
    generation and entailment recognition) and an entailment-aware decoder by entailment
    Reward Augmented Maximum Likelihood (RAML) training. Experiment results demonstrate
    that our models significantly outperform baselines from the aspects of informativeness
    and correctness.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Haoran
    full: Haoran Li
    id: haoran-li
    last: Li
  - first: Junnan
    full: Junnan Zhu
    id: junnan-zhu
    last: Zhu
  - first: Jiajun
    full: Jiajun Zhang
    id: jiajun-zhang
    last: Zhang
  - first: Chengqing
    full: Chengqing Zong
    id: chengqing-zong
    last: Zong
  author_string: Haoran Li, Junnan Zhu, Jiajun Zhang, Chengqing Zong
  bibkey: li-etal-2018-ensure
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1430'
  page_last: '1441'
  pages: "1430\u20131441"
  paper_id: '121'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1121.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1121.jpg
  title: 'Ensure the Correctness of the Summary: Incorporate Entailment Knowledge
    into Abstractive Sentence Summarization'
  title_html: 'Ensure the Correctness of the Summary: Incorporate Entailment Knowledge
    into Abstractive Sentence Summarization'
  url: https://www.aclweb.org/anthology/C18-1121
  year: '2018'
C18-1122:
  abstract: Parallel sentence extraction is a task addressing the data sparsity problem
    found in multilingual natural language processing applications. We propose a bidirectional
    recurrent neural network based approach to extract parallel sentences from collections
    of multilingual texts. Our experiments with noisy parallel corpora show that we
    can achieve promising results against a competitive baseline by removing the need
    of specific feature engineering or additional external resources. To justify the
    utility of our approach, we extract sentence pairs from Wikipedia articles to
    train machine translation systems and show significant improvements in translation
    performance.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Francis
    full: "Francis Gr\xE9goire"
    id: francis-gregoire
    last: "Gr\xE9goire"
  - first: Philippe
    full: Philippe Langlais
    id: philippe-langlais
    last: Langlais
  author_string: "Francis Gr\xE9goire, Philippe Langlais"
  bibkey: gregoire-langlais-2018-extracting
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1442'
  page_last: '1453'
  pages: "1442\u20131453"
  paper_id: '122'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1122.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1122.jpg
  title: Extracting Parallel Sentences with Bidirectional Recurrent Neural Networks
    to Improve Machine Translation
  title_html: Extracting Parallel Sentences with Bidirectional Recurrent Neural Networks
    to Improve Machine Translation
  url: https://www.aclweb.org/anthology/C18-1122
  year: '2018'
C18-1123:
  abstract: Attention-based sequence-to-sequence neural network models learn to jointly
    align and translate. The quadratic-time attention mechanism is powerful as it
    is capable of handling arbitrary long-distance reordering, but computationally
    expensive. In this paper, towards making neural translation both accurate and
    efficient, we follow the traditional pre-reordering approach to decouple reordering
    from translation. We add a reordering RNN that shares the input encoder with the
    decoder. The RNNs are trained jointly with a multi-task loss function and applied
    sequentially at inference time. The task of the reordering model is to predict
    the permutation of the input words following the target language word order. After
    reordering, the attention in the decoder becomes more peaked and monotonic. For
    reordering, we adopt the Inversion Transduction Grammars (ITG) and propose a transition
    system to parse input to trees for reordering. We harness the ITG transition system
    with RNN. With the modeling power of RNN, we achieve superior reordering accuracy
    without any feature engineering. In experiments, we apply the model to the task
    of text normalization. Compared to a strong baseline of attention-based RNN, our
    ITG RNN re-ordering model can reach the same reordering accuracy with only 1/10
    of the training data and is 2.5x faster in decoding.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Hao
    full: Hao Zhang
    id: hao-zhang
    last: Zhang
  - first: Axel
    full: Axel Ng
    id: axel-ng
    last: Ng
  - first: Richard
    full: Richard Sproat
    id: richard-sproat
    last: Sproat
  author_string: Hao Zhang, Axel Ng, Richard Sproat
  bibkey: zhang-etal-2018-fast
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1454'
  page_last: '1463'
  pages: "1454\u20131463"
  paper_id: '123'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1123.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1123.jpg
  title: Fast and Accurate Reordering with ITG Transition RNN
  title_html: Fast and Accurate Reordering with <span class="acl-fixed-case">ITG</span>
    Transition <span class="acl-fixed-case">RNN</span>
  url: https://www.aclweb.org/anthology/C18-1123
  year: '2018'
C18-1124:
  abstract: Neural machine translation with source-side attention have achieved remarkable
    performance. however, there has been little work exploring to attend to the target-side
    which can potentially enhance the memory capbility of NMT. We reformulate a Decoding
    History Enhanced Attention mechanism (DHEA) to render NMT model better at selecting
    both source-side and target-side information. DHA enables dynamic control of the
    ratios at which source and target contexts contribute to the generation of target
    words, offering a way to weakly induce structure relations among both source and
    target tokens. It also allows training errors to be directly back-propagated through
    short-cut connections and effectively alleviates the gradient vanishing problem.
    The empirical study on Chinese-English translation shows that our model with proper
    configuration can improve by 0:9 BLEU upon Transformer and the best reported results
    in the dataset. On WMT14 English-German task and a larger WMT14 English-French
    task, our model achieves comparable results with the state-of-the-art.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Mingxuan
    full: Mingxuan Wang
    id: mingxuan-wang
    last: Wang
  - first: Jun
    full: Jun Xie
    id: jun-xie
    last: Xie
  - first: Zhixing
    full: Zhixing Tan
    id: zhixing-tan
    last: Tan
  - first: Jinsong
    full: Jinsong Su
    id: jinsong-su
    last: Su
  - first: Deyi
    full: Deyi Xiong
    id: deyi-xiong
    last: Xiong
  - first: Chao
    full: Chao Bian
    id: chao-bian
    last: Bian
  author_string: Mingxuan Wang, Jun Xie, Zhixing Tan, Jinsong Su, Deyi Xiong, Chao
    Bian
  bibkey: wang-etal-2018-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1464'
  page_last: '1473'
  pages: "1464\u20131473"
  paper_id: '124'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1124.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1124.jpg
  title: Neural Machine Translation with Decoding History Enhanced Attention
  title_html: Neural Machine Translation with Decoding History Enhanced Attention
  url: https://www.aclweb.org/anthology/C18-1124
  year: '2018'
C18-1125:
  abstract: Lack of data can be an issue when beginning a new study on historical
    handwritten documents. In order to deal with this, we present the character-based
    decoder part of a multilingual approach based on transductive transfer learning
    for a historical handwriting recognition task on Italian Comedy Registers. The
    decoder must build a sequence of characters that corresponds to a word from a
    vector of letter-ngrams. As learning data, we created a new dataset from untapped
    resources that covers the same domain and period of our Italian Comedy data, as
    well as resources from common domains, periods, or languages. We obtain a 97.42%
    Character Recognition Rate and a 86.57% Word Recognition Rate on our Italian Comedy
    data, despite a lexical coverage of 67% between the Italian Comedy data and the
    training data. These results show that an efficient system can be obtained by
    a carefully selecting the datasets used for the transfer learning.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Adeline
    full: Adeline Granet
    id: adeline-granet
    last: Granet
  - first: Emmanuel
    full: Emmanuel Morin
    id: emmanuel-morin
    last: Morin
  - first: Harold
    full: "Harold Mouch\xE8re"
    id: harold-mouchere
    last: "Mouch\xE8re"
  - first: Solen
    full: Solen Quiniou
    id: solen-quiniou
    last: Quiniou
  - first: Christian
    full: Christian Viard-Gaudin
    id: christian-viard-gaudin
    last: Viard-Gaudin
  author_string: "Adeline Granet, Emmanuel Morin, Harold Mouch\xE8re, Solen Quiniou,\
    \ Christian Viard-Gaudin"
  bibkey: granet-etal-2018-transfer
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1474'
  page_last: '1484'
  pages: "1474\u20131484"
  paper_id: '125'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1125.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1125.jpg
  title: Transfer Learning for a Letter-Ngrams to Word Decoder in the Context of Historical
    Handwriting Recognition with Scarce Resources
  title_html: Transfer Learning for a Letter-Ngrams to Word Decoder in the Context
    of Historical Handwriting Recognition with Scarce Resources
  url: https://www.aclweb.org/anthology/C18-1125
  year: '2018'
C18-1126:
  abstract: "Mental health is a significant and growing public health concern. As\
    \ language usage can be leveraged to obtain crucial insights into mental health\
    \ conditions, there is a need for large-scale, labeled, mental health-related\
    \ datasets of users who have been diagnosed with one or more of such conditions.\
    \ In this paper, we investigate the creation of high-precision patterns to identify\
    \ self-reported diagnoses of nine different mental health conditions, and obtain\
    \ high-quality labeled data without the need for manual labelling. We introduce\
    \ the SMHD (Self-reported Mental Health Diagnoses) dataset and make it available.\
    \ SMHD is a novel large dataset of social media posts from users with one or multiple\
    \ mental health conditions along with matched control users. We examine distinctions\
    \ in users\u2019 language, as measured by linguistic and psychological variables.\
    \ We further explore text classification methods to identify individuals with\
    \ mental conditions through their language."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Arman
    full: Arman Cohan
    id: arman-cohan
    last: Cohan
  - first: Bart
    full: Bart Desmet
    id: bart-desmet
    last: Desmet
  - first: Andrew
    full: Andrew Yates
    id: andrew-yates
    last: Yates
  - first: Luca
    full: Luca Soldaini
    id: luca-soldaini
    last: Soldaini
  - first: Sean
    full: Sean MacAvaney
    id: sean-macavaney
    last: MacAvaney
  - first: Nazli
    full: Nazli Goharian
    id: nazli-goharian
    last: Goharian
  author_string: Arman Cohan, Bart Desmet, Andrew Yates, Luca Soldaini, Sean MacAvaney,
    Nazli Goharian
  bibkey: cohan-etal-2018-smhd
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1485'
  page_last: '1497'
  pages: "1485\u20131497"
  paper_id: '126'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1126.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1126.jpg
  title: 'SMHD: a Large-Scale Resource for Exploring Online Language Usage for Multiple
    Mental Health Conditions'
  title_html: '<span class="acl-fixed-case">SMHD</span>: a Large-Scale Resource for
    Exploring Online Language Usage for Multiple Mental Health Conditions'
  url: https://www.aclweb.org/anthology/C18-1126
  year: '2018'
C18-1127:
  abstract: "Clickbait has become a nuisance on social media. To address the urging\
    \ task of clickbait detection, we constructed a new corpus of 38,517 annotated\
    \ Twitter tweets, the Webis Clickbait Corpus 2017. To avoid biases in terms of\
    \ publisher and topic, tweets were sampled from the top 27 most retweeted news\
    \ publishers, covering a period of 150 days. Each tweet has been annotated on\
    \ 4-point scale by five annotators recruited at Amazon\u2019s Mechanical Turk.\
    \ The corpus has been employed to evaluate 12 clickbait detectors submitted to\
    \ the Clickbait Challenge 2017. Download: https://webis.de/data/webis-clickbait-17.html\
    \ Challenge: https://clickbait-challenge.org"
  address: Santa Fe, New Mexico, USA
  author:
  - first: Martin
    full: Martin Potthast
    id: martin-potthast
    last: Potthast
  - first: Tim
    full: Tim Gollub
    id: tim-gollub
    last: Gollub
  - first: Kristof
    full: Kristof Komlossy
    id: kristof-komlossy
    last: Komlossy
  - first: Sebastian
    full: Sebastian Schuster
    id: sebastian-schuster
    last: Schuster
  - first: Matti
    full: Matti Wiegmann
    id: matti-wiegmann
    last: Wiegmann
  - first: Erika Patricia
    full: Erika Patricia Garces Fernandez
    id: erika-patricia-garces-fernandez
    last: Garces Fernandez
  - first: Matthias
    full: Matthias Hagen
    id: matthias-hagen
    last: Hagen
  - first: Benno
    full: Benno Stein
    id: benno-stein
    last: Stein
  author_string: Martin Potthast, Tim Gollub, Kristof Komlossy, Sebastian Schuster,
    Matti Wiegmann, Erika Patricia Garces Fernandez, Matthias Hagen, Benno Stein
  bibkey: potthast-etal-2018-crowdsourcing
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1498'
  page_last: '1507'
  pages: "1498\u20131507"
  paper_id: '127'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1127.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1127.jpg
  title: Crowdsourcing a Large Corpus of Clickbait on Twitter
  title_html: Crowdsourcing a Large Corpus of Clickbait on Twitter
  url: https://www.aclweb.org/anthology/C18-1127
  year: '2018'
C18-1128:
  abstract: "Considerable effort has been devoted to building commonsense knowledge\
    \ bases. However, they are not available in many languages because the construction\
    \ of KBs is expensive. To bridge the gap between languages, this paper addresses\
    \ the problem of projecting the knowledge in English, a resource-rich language,\
    \ into other languages, where the main challenge lies in projection ambiguity.\
    \ This ambiguity is partially solved by machine translation and target-side knowledge\
    \ base completion, but neither of them is adequately reliable by itself. We show\
    \ their combination can project English commonsense knowledge into Japanese and\
    \ Chinese with high precision. Our method also achieves a top-10 accuracy of 90%\
    \ on the crowdsourced English\u2013Japanese benchmark. Furthermore, we use our\
    \ method to obtain 18,747 facts of accurate Japanese commonsense within a very\
    \ short period."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Naoki
    full: Naoki Otani
    id: naoki-otani
    last: Otani
  - first: Hirokazu
    full: Hirokazu Kiyomaru
    id: hirokazu-kiyomaru
    last: Kiyomaru
  - first: Daisuke
    full: Daisuke Kawahara
    id: daisuke-kawahara
    last: Kawahara
  - first: Sadao
    full: Sadao Kurohashi
    id: sadao-kurohashi
    last: Kurohashi
  author_string: Naoki Otani, Hirokazu Kiyomaru, Daisuke Kawahara, Sadao Kurohashi
  bibkey: otani-etal-2018-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1508'
  page_last: '1520'
  pages: "1508\u20131520"
  paper_id: '128'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1128.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1128.jpg
  title: Cross-lingual Knowledge Projection Using Machine Translation and Target-side
    Knowledge Base Completion
  title_html: Cross-lingual Knowledge Projection Using Machine Translation and Target-side
    Knowledge Base Completion
  url: https://www.aclweb.org/anthology/C18-1128
  year: '2018'
C18-1129:
  abstract: This paper provides an evaluation of a wide range of advanced sentence-level
    Quality Estimation models, including Support Vector Regression, Ride Regression,
    Neural Networks, Gaussian Processes, Bayesian Neural Networks, Deep Kernel Learning
    and Deep Gaussian Processes. Beside the accurateness, our main concerns are also
    the robustness of Quality Estimation models. Our work raises the difficulty in
    building strong models. Specifically, we show that Quality Estimation models often
    behave differently in Quality Estimation feature space, depending on whether the
    scale of feature space is small, medium or large. We also show that Quality Estimation
    models often behave differently in evaluation settings, depending on whether test
    data come from the same domain as the training data or not. Our work suggests
    several strong candidates to use in different circumstances.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Hoang
    full: Hoang Cuong
    id: hoang-cuong
    last: Cuong
  - first: Jia
    full: Jia Xu
    id: jia-xu
    last: Xu
  author_string: Hoang Cuong, Jia Xu
  bibkey: cuong-xu-2018-assessing
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1521'
  page_last: '1533'
  pages: "1521\u20131533"
  paper_id: '129'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1129.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1129.jpg
  title: Assessing Quality Estimation Models for Sentence-Level Prediction
  title_html: Assessing Quality Estimation Models for Sentence-Level Prediction
  url: https://www.aclweb.org/anthology/C18-1129
  year: '2018'
C18-1130:
  abstract: User demographic inference from social media text has the potential to
    improve a range of downstream applications, including real-time passive polling
    or quantifying demographic bias. This study focuses on developing models for user-level
    race and ethnicity prediction. We introduce a data set of users who self-report
    their race/ethnicity through a survey, in contrast to previous approaches that
    use distantly supervised data or perceived labels. We develop predictive models
    from text which accurately predict the membership of a user to the four largest
    racial and ethnic groups with up to .884 AUC and make these available to the research
    community.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Daniel
    full: "Daniel Preo\u0163iuc-Pietro"
    id: daniel-preotiuc-pietro
    last: "Preo\u0163iuc-Pietro"
  - first: Lyle
    full: Lyle Ungar
    id: lyle-ungar
    last: Ungar
  author_string: "Daniel Preo\u0163iuc-Pietro, Lyle Ungar"
  bibkey: preotiuc-pietro-ungar-2018-user
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1534'
  page_last: '1545'
  pages: "1534\u20131545"
  paper_id: '130'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1130.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1130.jpg
  title: User-Level Race and Ethnicity Predictors from Twitter Text
  title_html: User-Level Race and Ethnicity Predictors from Twitter Text
  url: https://www.aclweb.org/anthology/C18-1130
  year: '2018'
C18-1131:
  abstract: Fake news spreading through media outlets poses a real threat to the trustworthiness
    of information and detecting fake news has attracted increasing attention in recent
    years. Fake news is typically written intentionally to mislead readers, which
    determines that fake news detection merely based on news content is tremendously
    challenging. Meanwhile, fake news could contain true evidence to mock true news
    and presents different degrees of fakeness, which further exacerbates the detection
    difficulty. On the other hand, the spread of fake news produces various types
    of data from different perspectives. These multiple sources provide rich contextual
    information about fake news and offer unprecedented opportunities for advanced
    fake news detection. In this paper, we study fake news detection with different
    degrees of fakeness by integrating multiple sources. In particular, we introduce
    approaches to combine information from multiple sources and to discriminate between
    different degrees of fakeness, and propose a Multi-source Multi-class Fake news
    Detection framework MMFD, which combines automated feature extraction, multi-source
    fusion and automated degrees of fakeness detection into a coherent and interpretable
    model. Experimental results on the real-world data demonstrate the effectiveness
    of the proposed framework and extensive experiments are further conducted to understand
    the working of the proposed framework.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Hamid
    full: Hamid Karimi
    id: hamid-karimi
    last: Karimi
  - first: Proteek
    full: Proteek Roy
    id: proteek-roy
    last: Roy
  - first: Sari
    full: Sari Saba-Sadiya
    id: sari-saba-sadiya
    last: Saba-Sadiya
  - first: Jiliang
    full: Jiliang Tang
    id: jiliang-tang
    last: Tang
  author_string: Hamid Karimi, Proteek Roy, Sari Saba-Sadiya, Jiliang Tang
  bibkey: karimi-etal-2018-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1546'
  page_last: '1557'
  pages: "1546\u20131557"
  paper_id: '131'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1131.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1131.jpg
  title: Multi-Source Multi-Class Fake News Detection
  title_html: Multi-Source Multi-Class Fake News Detection
  url: https://www.aclweb.org/anthology/C18-1131
  year: '2018'
C18-1132:
  abstract: Non-literal language phenomena such as idioms or metaphors are commonly
    studied in isolation from each other in NLP. However, often similar definitions
    and features are being used for different phenomena, challenging the distinction.
    Instead, we propose to view the detection problem as a generalized non-literal
    language classification problem. In this paper we investigate multi-task learning
    for related non-literal language phenomena. We show that in contrast to simply
    joining the data of multiple tasks, multi-task learning consistently improves
    upon four metaphor and idiom detection tasks in two languages, English and German.
    Comparing two state-of-the-art multi-task learning architectures, we also investigate
    when soft parameter sharing and learned information flow can be beneficial for
    our related tasks. We make our adapted code publicly available.
  address: Santa Fe, New Mexico, USA
  author:
  - first: "Erik-L\xE2n"
    full: "Erik-L\xE2n Do Dinh"
    id: erik-lan-do-dinh
    last: Do Dinh
  - first: Steffen
    full: Steffen Eger
    id: steffen-eger
    last: Eger
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: "Erik-L\xE2n Do Dinh, Steffen Eger, Iryna Gurevych"
  bibkey: do-dinh-etal-2018-killing
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1558'
  page_last: '1569'
  pages: "1558\u20131569"
  paper_id: '132'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1132.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1132.jpg
  title: 'Killing Four Birds with Two Stones: Multi-Task Learning for Non-Literal
    Language Detection'
  title_html: 'Killing Four Birds with Two Stones: Multi-Task Learning for Non-Literal
    Language Detection'
  url: https://www.aclweb.org/anthology/C18-1132
  year: '2018'
C18-1133:
  abstract: In this paper, we leverage social media platforms such as twitter for
    developing corpus across multiple languages. The corpus creation methodology is
    applicable for resource-scarce languages provided the speakers of that particular
    language are active users on social media platforms. We present an approach to
    extract social media microblogs such as tweets (Twitter). In this paper, we create
    corpus for multilingual sentiment analysis and emoji prediction in Hindi, Bengali
    and Telugu. Further, we perform and analyze multiple NLP tasks utilizing the corpus
    to get interesting observations.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Nurendra
    full: Nurendra Choudhary
    id: nurendra-choudhary
    last: Choudhary
  - first: Rajat
    full: Rajat Singh
    id: rajat-singh
    last: Singh
  - first: Vijjini
    full: Vijjini Anvesh Rao
    id: vijjini-anvesh-rao
    last: Anvesh Rao
  - first: Manish
    full: Manish Shrivastava
    id: manish-shrivastava
    last: Shrivastava
  author_string: Nurendra Choudhary, Rajat Singh, Vijjini Anvesh Rao, Manish Shrivastava
  bibkey: choudhary-etal-2018-twitter
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1570'
  page_last: '1577'
  pages: "1570\u20131577"
  paper_id: '133'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1133.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1133.jpg
  title: Twitter corpus of Resource-Scarce Languages for Sentiment Analysis and Multilingual
    Emoji Prediction
  title_html: Twitter corpus of Resource-Scarce Languages for Sentiment Analysis and
    Multilingual Emoji Prediction
  url: https://www.aclweb.org/anthology/C18-1133
  year: '2018'
C18-1134:
  abstract: "Bayesian linguistic phylogenies are standardly based on cognate matrices\
    \ for words referring to a fix set of meanings\u2014typically around 100-200.\
    \ To this day there has not been any empirical investigation into which datasize\
    \ is optimal. Here we determine, across a set of language families, the optimal\
    \ number of meanings required for the best performance in Bayesian phylogenetic\
    \ inference. We rank meanings by stability, infer phylogenetic trees using first\
    \ the most stable meaning, then the two most stable meanings, and so on, computing\
    \ the quartet distance of the resulting tree to the tree proposed by language\
    \ family experts at each step of datasize increase. When a gold standard tree\
    \ is not available we propose to instead compute the quartet distance between\
    \ the tree based on the n-most stable meaning and the one based on the n + 1-most\
    \ stable meanings, increasing n from 1 to N \u2212 1, where N is the total number\
    \ of meanings. The assumption here is that the value of n for which the quartet\
    \ distance begins to stabilize is also the value at which the quality of the tree\
    \ ceases to improve. We show that this assumption is borne out. The results of\
    \ the two methods vary across families, and the optimal number of meanings appears\
    \ to correlate with the number of languages under consideration."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Taraka
    full: Taraka Rama
    id: taraka-rama
    last: Rama
  - first: "S\xF8ren"
    full: "S\xF8ren Wichmann"
    id: soren-wichmann
    last: Wichmann
  author_string: "Taraka Rama, S\xF8ren Wichmann"
  bibkey: rama-wichmann-2018-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1578'
  page_last: '1590'
  pages: "1578\u20131590"
  paper_id: '134'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1134.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1134.jpg
  title: Towards identifying the optimal datasize for lexically-based Bayesian inference
    of linguistic phylogenies
  title_html: Towards identifying the optimal datasize for lexically-based <span class="acl-fixed-case">B</span>ayesian
    inference of linguistic phylogenies
  url: https://www.aclweb.org/anthology/C18-1134
  year: '2018'
C18-1135:
  abstract: We investigate the birth and diffusion of lexical innovations in a large
    dataset of online social communities. We build on sociolinguistic theories and
    focus on the relation between the spread of a novel term and the social role of
    the individuals who use it, uncovering characteristics of innovators and adopters.
    Finally, we perform a prediction task that allows us to anticipate whether an
    innovation will successfully spread within a community.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Marco
    full: Marco Del Tredici
    id: marco-del-tredici
    last: Del Tredici
  - first: Raquel
    full: "Raquel Fern\xE1ndez"
    id: raquel-fernandez
    last: "Fern\xE1ndez"
  author_string: "Marco Del Tredici, Raquel Fern\xE1ndez"
  bibkey: del-tredici-fernandez-2018-road
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1591'
  page_last: '1603'
  pages: "1591\u20131603"
  paper_id: '135'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1135.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1135.jpg
  title: 'The Road to Success: Assessing the Fate of Linguistic Innovations in Online
    Communities'
  title_html: 'The Road to Success: Assessing the Fate of Linguistic Innovations in
    Online Communities'
  url: https://www.aclweb.org/anthology/C18-1135
  year: '2018'
C18-1136:
  abstract: Proto-word reconstruction is central to the study of language evolution.
    It consists of recreating the words in an ancient language from its modern daughter
    languages. In this paper we investigate automatic word form reconstruction for
    Latin proto-words. Having modern word forms in multiple Romance languages (French,
    Italian, Spanish, Portuguese and Romanian), we infer the form of their common
    Latin ancestors. Our approach relies on the regularities that occurred when the
    Latin words entered the modern languages. We leverage information from all modern
    languages, building an ensemble system for proto-word reconstruction. We use conditional
    random fields for sequence labeling, but we conduct preliminary experiments with
    recurrent neural networks as well. We apply our method on multiple datasets, showing
    that our method improves on previous results, having also the advantage of requiring
    less input data, which is essential in historical linguistics, where resources
    are generally scarce.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Alina Maria
    full: Alina Maria Ciobanu
    id: alina-maria-ciobanu
    last: Ciobanu
  - first: Liviu P.
    full: Liviu P. Dinu
    id: liviu-p-dinu
    last: Dinu
  author_string: Alina Maria Ciobanu, Liviu P. Dinu
  bibkey: ciobanu-dinu-2018-ab
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1604'
  page_last: '1614'
  pages: "1604\u20131614"
  paper_id: '136'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1136.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1136.jpg
  title: 'Ab Initio: Automatic Latin Proto-word Reconstruction'
  title_html: 'Ab Initio: Automatic <span class="acl-fixed-case">L</span>atin Proto-word
    Reconstruction'
  url: https://www.aclweb.org/anthology/C18-1136
  year: '2018'
C18-1137:
  abstract: "In supervised learning of morphological patterns, the strategy of generalizing\
    \ inflectional tables into more abstract paradigms through alignment of the longest\
    \ common subsequence found in an inflection table has been proposed as an efficient\
    \ method to deduce the inflectional behavior of unseen word forms. In this paper,\
    \ we extend this notion of morphological \u2018paradigm\u2019 from earlier work\
    \ and provide a formalization that more accurately matches linguist intuitions\
    \ about what an inflectional paradigm is. Additionally, we propose and evaluate\
    \ a mechanism for learning full human-readable paradigm specifications from incomplete\
    \ data\u2014a scenario when we only have access to a few inflected forms for each\
    \ lexeme, and want to reconstruct the missing inflections as well as generalize\
    \ and group the witnessed patterns into a model of more abstract paradigmatic\
    \ behavior of lexemes."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Miikka
    full: Miikka Silfverberg
    id: miikka-silfverberg
    last: Silfverberg
  - first: Ling
    full: Ling Liu
    id: ling-liu
    last: Liu
  - first: Mans
    full: Mans Hulden
    id: mans-hulden
    last: Hulden
  author_string: Miikka Silfverberg, Ling Liu, Mans Hulden
  bibkey: silfverberg-etal-2018-computational
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1615'
  page_last: '1626'
  pages: "1615\u20131626"
  paper_id: '137'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1137.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1137.jpg
  title: A Computational Model for the Linguistic Notion of Morphological Paradigm
  title_html: A Computational Model for the Linguistic Notion of Morphological Paradigm
  url: https://www.aclweb.org/anthology/C18-1137
  year: '2018'
C18-1138:
  abstract: Given a set of instances of some relation, the relation induction task
    is to predict which other word pairs are likely to be related in the same way.
    While it is natural to use word embeddings for this task, standard approaches
    based on vector translations turn out to perform poorly. To address this issue,
    we propose two probabilistic relation induction models. The first model is based
    on translations, but uses Gaussians to explicitly model the variability of these
    translations and to encode soft constraints on the source and target words that
    may be chosen. In the second model, we use Bayesian linear regression to encode
    the assumption that there is a linear relationship between the vector representations
    of related words, which is considerably weaker than the assumption underlying
    translation based models.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Zied
    full: Zied Bouraoui
    id: zied-bouraoui
    last: Bouraoui
  - first: Shoaib
    full: Shoaib Jameel
    id: shoaib-jameel
    last: Jameel
  - first: Steven
    full: Steven Schockaert
    id: steven-schockaert
    last: Schockaert
  author_string: Zied Bouraoui, Shoaib Jameel, Steven Schockaert
  bibkey: bouraoui-etal-2018-relation
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1627'
  page_last: '1637'
  pages: "1627\u20131637"
  paper_id: '138'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1138.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1138.jpg
  title: Relation Induction in Word Embeddings Revisited
  title_html: Relation Induction in Word Embeddings Revisited
  url: https://www.aclweb.org/anthology/C18-1138
  year: '2018'
C18-1139:
  abstract: 'Recent advances in language modeling using recurrent neural networks
    have made it viable to model language as distributions over characters. By learning
    to predict the next character on the basis of previous characters, such models
    have been shown to automatically internalize linguistic concepts such as words,
    sentences, subclauses and even sentiment. In this paper, we propose to leverage
    the internal states of a trained character language model to produce a novel type
    of word embedding which we refer to as contextual string embeddings. Our proposed
    embeddings have the distinct properties that they (a) are trained without any
    explicit notion of words and thus fundamentally model words as sequences of characters,
    and (b) are contextualized by their surrounding text, meaning that the same word
    will have different embeddings depending on its contextual use. We conduct a comparative
    evaluation against previous embeddings and find that our embeddings are highly
    useful for downstream tasks: across four classic sequence labeling tasks we consistently
    outperform the previous state-of-the-art. In particular, we significantly outperform
    previous work on English and German named entity recognition (NER), allowing us
    to report new state-of-the-art F1-scores on the CoNLL03 shared task. We release
    all code and pre-trained language models in a simple-to-use framework to the research
    community, to enable reproduction of these experiments and application of our
    proposed embeddings to other tasks: https://github.com/zalandoresearch/flair'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Alan
    full: Alan Akbik
    id: alan-akbik
    last: Akbik
  - first: Duncan
    full: Duncan Blythe
    id: duncan-blythe
    last: Blythe
  - first: Roland
    full: Roland Vollgraf
    id: roland-vollgraf
    last: Vollgraf
  author_string: Alan Akbik, Duncan Blythe, Roland Vollgraf
  bibkey: akbik-etal-2018-contextual
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1638'
  page_last: '1649'
  pages: "1638\u20131649"
  paper_id: '139'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1139.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1139.jpg
  title: Contextual String Embeddings for Sequence Labeling
  title_html: Contextual String Embeddings for Sequence Labeling
  url: https://www.aclweb.org/anthology/C18-1139
  year: '2018'
C18-1140:
  abstract: Distributed word embeddings have shown superior performances in numerous
    Natural Language Processing (NLP) tasks. However, their performances vary significantly
    across different tasks, implying that the word embeddings learnt by those methods
    capture complementary aspects of lexical semantics. Therefore, we believe that
    it is important to combine the existing word embeddings to produce more accurate
    and complete meta-embeddings of words. We model the meta-embedding learning problem
    as an autoencoding problem, where we would like to learn a meta-embedding space
    that can accurately reconstruct all source embeddings simultaneously. Thereby,
    the meta-embedding space is enforced to capture complementary information in different
    source embeddings via a coherent common embedding space. We propose three flavours
    of autoencoded meta-embeddings motivated by different requirements that must be
    satisfied by a meta-embedding. Our experimental results on a series of benchmark
    evaluations show that the proposed autoencoded meta-embeddings outperform the
    existing state-of-the-art meta-embeddings in multiple tasks.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Danushka
    full: Danushka Bollegala
    id: danushka-bollegala
    last: Bollegala
  - first: Cong
    full: Cong Bao
    id: cong-bao
    last: Bao
  author_string: Danushka Bollegala, Cong Bao
  bibkey: bollegala-bao-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1650'
  page_last: '1661'
  pages: "1650\u20131661"
  paper_id: '140'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1140.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1140.jpg
  title: Learning Word Meta-Embeddings by Autoencoding
  title_html: Learning Word Meta-Embeddings by Autoencoding
  url: https://www.aclweb.org/anthology/C18-1140
  year: '2018'
C18-1141:
  abstract: 'With the aid of recently proposed word embedding algorithms, the study
    of semantic similarity has progressed and advanced rapidly. However, many natural
    language processing tasks need sense level representation. To address this issue,
    some researches propose sense embedding learning algorithms. In this paper, we
    present a generalized model from existing sense retrofitting model. The generalization
    takes three major components: semantic relations between the senses, the relation
    strength and the semantic strength. In the experiment, we show that the generalized
    model can outperform previous approaches in three types of experiment: semantic
    relatedness, contextual word similarity and semantic difference.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yang-Yin
    full: Yang-Yin Lee
    id: yang-yin-lee
    last: Lee
  - first: Ting-Yu
    full: Ting-Yu Yen
    id: ting-yu-yen
    last: Yen
  - first: Hen-Hsen
    full: Hen-Hsen Huang
    id: hen-hsen-huang
    last: Huang
  - first: Yow-Ting
    full: Yow-Ting Shiue
    id: yow-ting-shiue
    last: Shiue
  - first: Hsin-Hsi
    full: Hsin-Hsi Chen
    id: hsin-hsi-chen
    last: Chen
  author_string: Yang-Yin Lee, Ting-Yu Yen, Hen-Hsen Huang, Yow-Ting Shiue, Hsin-Hsi
    Chen
  bibkey: lee-etal-2018-gensense
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1662'
  page_last: '1671'
  pages: "1662\u20131671"
  paper_id: '141'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1141.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1141.jpg
  title: 'GenSense: A Generalized Sense Retrofitting Model'
  title_html: '<span class="acl-fixed-case">G</span>en<span class="acl-fixed-case">S</span>ense:
    A Generalized Sense Retrofitting Model'
  url: https://www.aclweb.org/anthology/C18-1141
  year: '2018'
C18-1142:
  abstract: The variational encoder-decoder (VED) encodes source information as a
    set of random variables using a neural network, which in turn is decoded into
    target data using another neural network. In natural language processing, sequence-to-sequence
    (Seq2Seq) models typically serve as encoder-decoder networks. When combined with
    a traditional (deterministic) attention mechanism, the variational latent space
    may be bypassed by the attention model, and thus becomes ineffective. In this
    paper, we propose a variational attention mechanism for VED, where the attention
    vector is also modeled as Gaussian distributed random variables. Results on two
    experiments show that, without loss of quality, our proposed method alleviates
    the bypassing phenomenon as it increases the diversity of generated sentences.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Hareesh
    full: Hareesh Bahuleyan
    id: hareesh-bahuleyan
    last: Bahuleyan
  - first: Lili
    full: Lili Mou
    id: lili-mou
    last: Mou
  - first: Olga
    full: Olga Vechtomova
    id: olga-vechtomova
    last: Vechtomova
  - first: Pascal
    full: Pascal Poupart
    id: pascal-poupart
    last: Poupart
  author_string: Hareesh Bahuleyan, Lili Mou, Olga Vechtomova, Pascal Poupart
  bibkey: bahuleyan-etal-2018-variational
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1672'
  page_last: '1682'
  pages: "1672\u20131682"
  paper_id: '142'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1142.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1142.jpg
  title: Variational Attention for Sequence-to-Sequence Models
  title_html: Variational Attention for Sequence-to-Sequence Models
  url: https://www.aclweb.org/anthology/C18-1142
  year: '2018'
C18-1143:
  abstract: "In this paper, a new deep reinforcement learning based augmented general\
    \ tagging system is proposed. The new system contains two parts: a deep neural\
    \ network (DNN) based sequence labeling model and a deep reinforcement learning\
    \ (DRL) based augmented tagger. The augmented tagger helps improve system performance\
    \ by modeling the data with minority tags. The new system is evaluated on SLU\
    \ and NLU sequence labeling tasks using ATIS and CoNLL-2003 benchmark datasets,\
    \ to demonstrate the new system\u2019s outstanding performance on general tagging\
    \ tasks. Evaluated by F1 scores, it shows that the new system outperforms the\
    \ current state-of-the-art model on ATIS dataset by 1.9% and that on CoNLL-2003\
    \ dataset by 1.4%."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yu
    full: Yu Wang
    id: yu-wang
    last: Wang
  - first: Abhishek
    full: Abhishek Patel
    id: abhishek-patel
    last: Patel
  - first: Hongxia
    full: Hongxia Jin
    id: hongxia-jin
    last: Jin
  author_string: Yu Wang, Abhishek Patel, Hongxia Jin
  bibkey: wang-etal-2018-new
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1683'
  page_last: '1693'
  pages: "1683\u20131693"
  paper_id: '143'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1143.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1143.jpg
  title: A New Concept of Deep Reinforcement Learning based Augmented General Tagging
    System
  title_html: A New Concept of Deep Reinforcement Learning based Augmented General
    Tagging System
  url: https://www.aclweb.org/anthology/C18-1143
  year: '2018'
C18-1144:
  abstract: 'Annotated corpora enable supervised machine learning and data analysis.
    To reduce the cost of manual annotation, tasks are often assigned to internet
    workers whose judgments are reconciled by crowdsourcing models. We approach the
    problem of crowdsourcing using a framework for learning from rich prior knowledge,
    and we identify a family of crowdsourcing models with the novel ability to combine
    annotations with differing structures: e.g., document labels and word labels.
    Annotator judgments are given in the form of the predicted expected value of measurement
    functions computed over annotations and the data, unifying annotation models.
    Our model, a specific instance of this framework, compares favorably with previous
    work. Furthermore, it enables active sample selection, jointly selecting annotator,
    data item, and annotation structure to reduce annotation effort.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Paul
    full: Paul Felt
    id: paul-felt
    last: Felt
  - first: Eric
    full: Eric Ringger
    id: eric-ringger
    last: Ringger
  - first: Jordan
    full: Jordan Boyd-Graber
    id: jordan-boyd-graber
    last: Boyd-Graber
  - first: Kevin
    full: Kevin Seppi
    id: kevin-seppi
    last: Seppi
  author_string: Paul Felt, Eric Ringger, Jordan Boyd-Graber, Kevin Seppi
  bibkey: felt-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1694'
  page_last: '1704'
  pages: "1694\u20131704"
  paper_id: '144'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1144.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1144.jpg
  title: 'Learning from Measurements in Crowdsourcing Models: Inferring Ground Truth
    from Diverse Annotation Types'
  title_html: 'Learning from Measurements in Crowdsourcing Models: Inferring Ground
    Truth from Diverse Annotation Types'
  url: https://www.aclweb.org/anthology/C18-1144
  year: '2018'
C18-1145:
  abstract: We reproduce the Structurally Constrained Recurrent Network (SCRN) model,
    and then regularize it using the existing widespread techniques, such as naive
    dropout, variational dropout, and weight tying. We show that when regularized
    and optimized appropriately the SCRN model can achieve performance comparable
    with the ubiquitous LSTM model in language modeling task on English data, while
    outperforming it on non-English data.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Olzhas
    full: Olzhas Kabdolov
    id: olzhas-kabdolov
    last: Kabdolov
  - first: Zhenisbek
    full: Zhenisbek Assylbekov
    id: zhenisbek-assylbekov
    last: Assylbekov
  - first: Rustem
    full: Rustem Takhanov
    id: rustem-takhanov
    last: Takhanov
  author_string: Olzhas Kabdolov, Zhenisbek Assylbekov, Rustem Takhanov
  bibkey: kabdolov-etal-2018-reproducing
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1705'
  page_last: '1716'
  pages: "1705\u20131716"
  paper_id: '145'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1145.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1145.jpg
  title: Reproducing and Regularizing the SCRN Model
  title_html: Reproducing and Regularizing the <span class="acl-fixed-case">SCRN</span>
    Model
  url: https://www.aclweb.org/anthology/C18-1145
  year: '2018'
C18-1146:
  abstract: Seq2seq learning has produced promising results on summarization. However,
    in many cases, system summaries still struggle to keep the meaning of the original
    intact. They may miss out important words or relations that play critical roles
    in the syntactic structure of source sentences. In this paper, we present structure-infused
    copy mechanisms to facilitate copying important words and relations from the source
    sentence to summary sentence. The approach naturally combines source dependency
    structure with the copy mechanism of an abstractive sentence summarizer. Experimental
    results demonstrate the effectiveness of incorporating source-side syntactic information
    in the system, and our proposed approach compares favorably to state-of-the-art
    methods.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Kaiqiang
    full: Kaiqiang Song
    id: kaiqiang-song
    last: Song
  - first: Lin
    full: Lin Zhao
    id: lin-zhao
    last: Zhao
  - first: Fei
    full: Fei Liu
    id: fei-liu-utdallas
    last: Liu
  author_string: Kaiqiang Song, Lin Zhao, Fei Liu
  bibkey: song-etal-2018-structure
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1717'
  page_last: '1729'
  pages: "1717\u20131729"
  paper_id: '146'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1146.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1146.jpg
  title: Structure-Infused Copy Mechanisms for Abstractive Summarization
  title_html: Structure-Infused Copy Mechanisms for Abstractive Summarization
  url: https://www.aclweb.org/anthology/C18-1146
  year: '2018'
C18-1147:
  abstract: Automatic image description systems typically produce generic sentences
    that only make use of a small subset of the vocabulary available to them. In this
    paper, we consider the production of generic descriptions as a lack of diversity
    in the output, which we quantify using established metrics and two new metrics
    that frame image description as a word recall task. This framing allows us to
    evaluate system performance on the head of the vocabulary, as well as on the long
    tail, where system performance degrades. We use these metrics to examine the diversity
    of the sentences generated by nine state-of-the-art systems on the MS COCO data
    set. We find that the systems trained with maximum likelihood objectives produce
    less diverse output than those trained with additional adversarial objectives.
    However, the adversarially-trained models only produce more types from the head
    of the vocabulary and not the tail. Besides vocabulary-based methods, we also
    look at the compositional capacity of the systems, specifically their ability
    to create compound nouns and prepositional phrases of different lengths. We conclude
    that there is still much room for improvement, and offer a toolkit to measure
    progress towards the goal of generating more diverse image descriptions.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Emiel
    full: Emiel van Miltenburg
    id: emiel-van-miltenburg
    last: van Miltenburg
  - first: Desmond
    full: Desmond Elliott
    id: desmond-elliott
    last: Elliott
  - first: Piek
    full: Piek Vossen
    id: piek-vossen
    last: Vossen
  author_string: Emiel van Miltenburg, Desmond Elliott, Piek Vossen
  bibkey: van-miltenburg-etal-2018-measuring
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1730'
  page_last: '1741'
  pages: "1730\u20131741"
  paper_id: '147'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1147.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1147.jpg
  title: Measuring the Diversity of Automatic Image Descriptions
  title_html: Measuring the Diversity of Automatic Image Descriptions
  url: https://www.aclweb.org/anthology/C18-1147
  year: '2018'
C18-1148:
  abstract: User-generated content such as the questions on community question answering
    (CQA) forums does not always come with appropriate headlines, in contrast to the
    news articles used in various headline generation tasks. In such cases, we cannot
    use paired supervised data, e.g., pairs of articles and headlines, to learn a
    headline generation model. To overcome this problem, we propose an extractive
    headline generation method based on learning to rank for CQA that extracts the
    most informative substring from each question as its headline. Experimental results
    show that our method outperforms several baselines, including a prefix-based method,
    which is widely used in real services.
  address: Santa Fe, New Mexico, USA
  attachment:
  - filename: C18-1148.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/C18-1148.Presentation.pdf
  author:
  - first: Tatsuru
    full: Tatsuru Higurashi
    id: tatsuru-higurashi
    last: Higurashi
  - first: Hayato
    full: Hayato Kobayashi
    id: hayato-kobayashi
    last: Kobayashi
  - first: Takeshi
    full: Takeshi Masuyama
    id: takeshi-masuyama
    last: Masuyama
  - first: Kazuma
    full: Kazuma Murao
    id: kazuma-murao
    last: Murao
  author_string: Tatsuru Higurashi, Hayato Kobayashi, Takeshi Masuyama, Kazuma Murao
  bibkey: higurashi-etal-2018-extractive
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1742'
  page_last: '1753'
  pages: "1742\u20131753"
  paper_id: '148'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1148.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1148.jpg
  title: Extractive Headline Generation Based on Learning to Rank for Community Question
    Answering
  title_html: Extractive Headline Generation Based on Learning to Rank for Community
    Question Answering
  url: https://www.aclweb.org/anthology/C18-1148
  year: '2018'
C18-1149:
  abstract: Enabling a mechanism to understand a temporal story and predict its ending
    is an interesting issue that has attracted considerable attention, as in case
    of the ROC Story Cloze Task (SCT). In this paper, we develop a multi-attention-based
    neural network (MANN) with well-designed optimizations, like Highway Network,
    and concatenated features with embedding representations into the hierarchical
    neural network model. Considering the particulars of the specific task, we thoughtfully
    extend MANN with external knowledge resources, exceeding state-of-the-art results
    obviously. Furthermore, we develop a thorough understanding of our model through
    a careful hand analysis on a subset of the stories. We identify what traits of
    MANN contribute to its outperformance and how external knowledge is obtained in
    such an ending prediction task.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Qian
    full: Qian Li
    id: qian-li
    last: Li
  - first: Ziwei
    full: Ziwei Li
    id: ziwei-li
    last: Li
  - first: Jin-Mao
    full: Jin-Mao Wei
    id: jin-mao-wei
    last: Wei
  - first: Yanhui
    full: Yanhui Gu
    id: yanhui-gu
    last: Gu
  - first: Adam
    full: Adam Jatowt
    id: adam-jatowt
    last: Jatowt
  - first: Zhenglu
    full: Zhenglu Yang
    id: zhenglu-yang
    last: Yang
  author_string: Qian Li, Ziwei Li, Jin-Mao Wei, Yanhui Gu, Adam Jatowt, Zhenglu Yang
  bibkey: li-etal-2018-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1754'
  page_last: '1762'
  pages: "1754\u20131762"
  paper_id: '149'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1149.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1149.jpg
  title: A Multi-Attention based Neural Network with External Knowledge for Story
    Ending Predicting Task
  title_html: A Multi-Attention based Neural Network with External Knowledge for Story
    Ending Predicting Task
  url: https://www.aclweb.org/anthology/C18-1149
  year: '2018'
C18-1150:
  abstract: Visual Question Generation (VQG) aims to ask natural questions about an
    image automatically. Existing research focus on training model to fit the annotated
    data set that makes it indifferent from other language generation tasks. We argue
    that natural questions need to have two specific attributes from the perspectives
    of content and linguistic respectively, namely, natural and human-written. Inspired
    by the setting of discriminator in adversarial learning, we propose two discriminators,
    one for each attribute, to enhance the training. We then use the reinforcement
    learning framework to incorporate scores from the two discriminators as the reward
    to guide the training of the question generator. Experimental results on a benchmark
    VQG dataset show the effectiveness and robustness of our model compared to some
    state-of-the-art models in terms of both automatic and human evaluation metrics.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Zhihao
    full: Zhihao Fan
    id: zhihao-fan
    last: Fan
  - first: Zhongyu
    full: Zhongyu Wei
    id: zhongyu-wei
    last: Wei
  - first: Siyuan
    full: Siyuan Wang
    id: siyuan-wang
    last: Wang
  - first: Yang
    full: Yang Liu
    id: yang-liu-icsi
    last: Liu
  - first: Xuanjing
    full: Xuanjing Huang
    id: xuan-jing-huang
    last: Huang
  author_string: Zhihao Fan, Zhongyu Wei, Siyuan Wang, Yang Liu, Xuanjing Huang
  bibkey: fan-etal-2018-reinforcement
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1763'
  page_last: '1774'
  pages: "1763\u20131774"
  paper_id: '150'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1150.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1150.jpg
  title: A Reinforcement Learning Framework for Natural Question Generation using
    Bi-discriminators
  title_html: A Reinforcement Learning Framework for Natural Question Generation using
    Bi-discriminators
  url: https://www.aclweb.org/anthology/C18-1150
  year: '2018'
C18-1151:
  abstract: "We introduce a method for embedding words as probability densities in\
    \ a low-dimensional space. Rather than assuming that a word embedding is fixed\
    \ across the entire text collection, as in standard word embedding methods, in\
    \ our Bayesian model we generate it from a word-specific prior density for each\
    \ occurrence of a given word. Intuitively, for each word, the prior density encodes\
    \ the distribution of its potential \u2018meanings\u2019. These prior densities\
    \ are conceptually similar to Gaussian embeddings of \u0117wcitevilnis2014word.\
    \ Interestingly, unlike the Gaussian embeddings, we can also obtain context-specific\
    \ densities: they encode uncertainty about the sense of a word given its context\
    \ and correspond to the approximate posterior distributions within our model.\
    \ The context-dependent densities have many potential applications: for example,\
    \ we show that they can be directly used in the lexical substitution task. We\
    \ describe an effective estimation method based on the variational autoencoding\
    \ framework. We demonstrate the effectiveness of our embedding technique on a\
    \ range of standard benchmarks."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Arthur
    full: "Arthur Bra\u017Einskas"
    id: arthur-brazinskas
    last: "Bra\u017Einskas"
  - first: Serhii
    full: Serhii Havrylov
    id: serhii-havrylov
    last: Havrylov
  - first: Ivan
    full: Ivan Titov
    id: ivan-titov
    last: Titov
  author_string: "Arthur Bra\u017Einskas, Serhii Havrylov, Ivan Titov"
  bibkey: brazinskas-etal-2018-embedding
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1775'
  page_last: '1789'
  pages: "1775\u20131789"
  paper_id: '151'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1151.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1151.jpg
  title: Embedding Words as Distributions with a Bayesian Skip-gram Model
  title_html: Embedding Words as Distributions with a <span class="acl-fixed-case">B</span>ayesian
    Skip-gram Model
  url: https://www.aclweb.org/anthology/C18-1151
  year: '2018'
C18-1152:
  abstract: "An important component of achieving language understanding is mastering\
    \ the composition of sentence meaning, but an immediate challenge to solving this\
    \ problem is the opacity of sentence vector representations produced by current\
    \ neural sentence composition models. We present a method to address this challenge,\
    \ developing tasks that directly target compositional meaning information in sentence\
    \ vector representations with a high degree of precision and control. To enable\
    \ the creation of these controlled tasks, we introduce a specialized sentence\
    \ generation system that produces large, annotated sentence sets meeting specified\
    \ syntactic, semantic and lexical constraints. We describe the details of the\
    \ method and generation system, and then present results of experiments applying\
    \ our method to probe for compositional information in embeddings from a number\
    \ of existing sentence composition models. We find that the method is able to\
    \ extract useful information about the differing capacities of these models, and\
    \ we discuss the implications of our results with respect to these systems\u2019\
    \ capturing of sentence information. We make available for public use the datasets\
    \ used for these experiments, as well as the generation system."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Allyson
    full: Allyson Ettinger
    id: allyson-ettinger
    last: Ettinger
  - first: Ahmed
    full: Ahmed Elgohary
    id: ahmed-elgohary
    last: Elgohary
  - first: Colin
    full: Colin Phillips
    id: colin-phillips
    last: Phillips
  - first: Philip
    full: Philip Resnik
    id: philip-resnik
    last: Resnik
  author_string: Allyson Ettinger, Ahmed Elgohary, Colin Phillips, Philip Resnik
  bibkey: ettinger-etal-2018-assessing
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1790'
  page_last: '1801'
  pages: "1790\u20131801"
  paper_id: '152'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1152.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1152.jpg
  title: Assessing Composition in Sentence Vector Representations
  title_html: Assessing Composition in Sentence Vector Representations
  url: https://www.aclweb.org/anthology/C18-1152
  year: '2018'
C18-1153:
  abstract: Representation learning is the foundation of machine reading comprehension.
    In state-of-the-art models, deep learning methods broadly use word and character
    level representations. However, character is not naturally the minimal linguistic
    unit. In addition, with a simple concatenation of character and word embedding,
    previous models actually give suboptimal solution. In this paper, we propose to
    use subword rather than character for word embedding enhancement. We also empirically
    explore different augmentation strategies on subword-augmented embedding to enhance
    the cloze-style reading comprehension model (reader). In detail, we present a
    reader that uses subword-level representation to augment word embedding with a
    short list to handle rare words effectively. A thorough examination is conducted
    to evaluate the comprehensive performance and generalization ability of the proposed
    reader. Experimental results show that the proposed approach helps the reader
    significantly outperform the state-of-the-art baselines on various public datasets.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Zhuosheng
    full: Zhuosheng Zhang
    id: zhuosheng-zhang
    last: Zhang
  - first: Yafang
    full: Yafang Huang
    id: yafang-huang
    last: Huang
  - first: Hai
    full: Hai Zhao
    id: hai-zhao
    last: Zhao
  author_string: Zhuosheng Zhang, Yafang Huang, Hai Zhao
  bibkey: zhang-etal-2018-subword
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1802'
  page_last: '1814'
  pages: "1802\u20131814"
  paper_id: '153'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1153.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1153.jpg
  title: Subword-augmented Embedding for Cloze Reading Comprehension
  title_html: Subword-augmented Embedding for Cloze Reading Comprehension
  url: https://www.aclweb.org/anthology/C18-1153
  year: '2018'
C18-1154:
  abstract: 'Pooling is an essential component of a wide variety of sentence representation
    and embedding models. This paper explores generalized pooling methods to enhance
    sentence embedding. We propose vector-based multi-head attention that includes
    the widely used max pooling, mean pooling, and scalar self-attention as special
    cases. The model benefits from properly designed penalization terms to reduce
    redundancy in multi-head attention. We evaluate the proposed model on three different
    tasks: natural language inference (NLI), author profiling, and sentiment classification.
    The experiments show that the proposed model achieves significant improvement
    over strong sentence-encoding-based methods, resulting in state-of-the-art performances
    on four datasets. The proposed approach can be easily implemented for more problems
    than we discuss in this paper.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Qian
    full: Qian Chen
    id: qian-chen
    last: Chen
  - first: Zhen-Hua
    full: Zhen-Hua Ling
    id: zhen-hua-ling
    last: Ling
  - first: Xiaodan
    full: Xiaodan Zhu
    id: xiaodan-zhu
    last: Zhu
  author_string: Qian Chen, Zhen-Hua Ling, Xiaodan Zhu
  bibkey: chen-etal-2018-enhancing
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1815'
  page_last: '1826'
  pages: "1815\u20131826"
  paper_id: '154'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1154.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1154.jpg
  title: Enhancing Sentence Embedding with Generalized Pooling
  title_html: Enhancing Sentence Embedding with Generalized Pooling
  url: https://www.aclweb.org/anthology/C18-1154
  year: '2018'
C18-1155:
  abstract: Interpreting noun compounds is a challenging task. It involves uncovering
    the underlying predicate which is dropped in the formation of the compound. In
    most cases, this predicate is of the form VERB+PREP. It has been observed that
    uncovering the preposition is a significant step towards uncovering the predicate.
    In this paper, we attempt to paraphrase noun compounds using prepositions. We
    consider noun compounds and their corresponding prepositional paraphrases as parallelly
    aligned sequences of words. This enables us to adapt different architectures from
    cross-lingual embedding literature. We choose the architecture where we create
    representations of both noun compound (source sequence) and its corresponding
    prepositional paraphrase (target sequence), such that their sim- ilarity is high.
    We use LSTMs to learn these representations. We use these representations to decide
    the correct preposition. Our experiments show that this approach performs considerably
    well on different datasets of noun compounds that are manually annotated with
    prepositions.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Girishkumar
    full: Girishkumar Ponkiya
    id: girishkumar-ponkiya
    last: Ponkiya
  - first: Kevin
    full: Kevin Patel
    id: kevin-patel
    last: Patel
  - first: Pushpak
    full: Pushpak Bhattacharyya
    id: pushpak-bhattacharyya
    last: Bhattacharyya
  - first: Girish
    full: Girish Palshikar
    id: girish-palshikar
    last: Palshikar
  author_string: Girishkumar Ponkiya, Kevin Patel, Pushpak Bhattacharyya, Girish Palshikar
  bibkey: ponkiya-etal-2018-treat
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1827'
  page_last: '1836'
  pages: "1827\u20131836"
  paper_id: '155'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1155.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1155.jpg
  title: 'Treat us like the sequences we are: Prepositional Paraphrasing of Noun Compounds
    using LSTM'
  title_html: 'Treat us like the sequences we are: Prepositional Paraphrasing of Noun
    Compounds using <span class="acl-fixed-case">LSTM</span>'
  url: https://www.aclweb.org/anthology/C18-1155
  year: '2018'
C18-1156:
  abstract: The literature in automated sarcasm detection has mainly focused on lexical-,
    syntactic- and semantic-level analysis of text. However, a sarcastic sentence
    can be expressed with contextual presumptions, background and commonsense knowledge.
    In this paper, we propose a ContextuAl SarCasm DEtector (CASCADE), which adopts
    a hybrid approach of both content- and context-driven modeling for sarcasm detection
    in online social media discussions. For the latter, CASCADE aims at extracting
    contextual information from the discourse of a discussion thread. Also, since
    the sarcastic nature and form of expression can vary from person to person, CASCADE
    utilizes user embeddings that encode stylometric and personality features of users.
    When used along with content-based feature extractors such as convolutional neural
    networks, we see a significant boost in the classification performance on a large
    Reddit corpus.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Devamanyu
    full: Devamanyu Hazarika
    id: devamanyu-hazarika
    last: Hazarika
  - first: Soujanya
    full: Soujanya Poria
    id: soujanya-poria
    last: Poria
  - first: Sruthi
    full: Sruthi Gorantla
    id: sruthi-gorantla
    last: Gorantla
  - first: Erik
    full: Erik Cambria
    id: erik-cambria
    last: Cambria
  - first: Roger
    full: Roger Zimmermann
    id: roger-zimmermann
    last: Zimmermann
  - first: Rada
    full: Rada Mihalcea
    id: rada-mihalcea
    last: Mihalcea
  author_string: Devamanyu Hazarika, Soujanya Poria, Sruthi Gorantla, Erik Cambria,
    Roger Zimmermann, Rada Mihalcea
  bibkey: hazarika-etal-2018-cascade
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1837'
  page_last: '1848'
  pages: "1837\u20131848"
  paper_id: '156'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1156.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1156.jpg
  title: 'CASCADE: Contextual Sarcasm Detection in Online Discussion Forums'
  title_html: '<span class="acl-fixed-case">CASCADE</span>: Contextual Sarcasm Detection
    in Online Discussion Forums'
  url: https://www.aclweb.org/anthology/C18-1156
  year: '2018'
C18-1157:
  abstract: This paper attempts to marry the interpretability of statistical machine
    learning approaches with the more robust models of joke structure and joke semantics
    capable of being learned by neural models. Specifically, we explore the use of
    semantic relatedness features based on word associations, rather than the more
    common Word2Vec similarity, on a binary humour identification task and identify
    several factors that make word associations a better fit for humour. We also explore
    the effects of using joke structure, in the form of humour anchors (Yang et al.,
    2015), for improving the performance of semantic features and show that, while
    an intriguing idea, humour anchors contain several pitfalls that can hurt performance.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Andrew
    full: Andrew Cattle
    id: andrew-cattle
    last: Cattle
  - first: Xiaojuan
    full: Xiaojuan Ma
    id: xiaojuan-ma
    last: Ma
  author_string: Andrew Cattle, Xiaojuan Ma
  bibkey: cattle-ma-2018-recognizing
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1849'
  page_last: '1858'
  pages: "1849\u20131858"
  paper_id: '157'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1157.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1157.jpg
  title: Recognizing Humour using Word Associations and Humour Anchor Extraction
  title_html: Recognizing Humour using Word Associations and Humour Anchor Extraction
  url: https://www.aclweb.org/anthology/C18-1157
  year: '2018'
C18-1158:
  abstract: "The 2017 Fake News Challenge Stage 1 (FNC-1) shared task addressed a\
    \ stance classification task as a crucial first step towards detecting fake news.\
    \ To date, there is no in-depth analysis paper to critically discuss FNC-1\u2019\
    s experimental setup, reproduce the results, and draw conclusions for next-generation\
    \ stance classification methods. In this paper, we provide such an in-depth analysis\
    \ for the three top-performing systems. We first find that FNC-1\u2019s proposed\
    \ evaluation metric favors the majority class, which can be easily classified,\
    \ and thus overestimates the true discriminative power of the methods. Therefore,\
    \ we propose a new F1-based metric yielding a changed system ranking. Next, we\
    \ compare the features and architectures used, which leads to a novel feature-rich\
    \ stacked LSTM model that performs on par with the best systems, but is superior\
    \ in predicting minority classes. To understand the methods\u2019 ability to generalize,\
    \ we derive a new dataset and perform both in-domain and cross-domain experiments.\
    \ Our qualitative and quantitative study helps interpreting the original FNC-1\
    \ scores and understand which features help improving performance and why. Our\
    \ new dataset and all source code used during the reproduction study are publicly\
    \ available for future research."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Andreas
    full: Andreas Hanselowski
    id: andreas-hanselowski
    last: Hanselowski
  - first: Avinesh
    full: Avinesh PVS
    id: avinesh-pvs
    last: PVS
  - first: Benjamin
    full: Benjamin Schiller
    id: benjamin-schiller
    last: Schiller
  - first: Felix
    full: Felix Caspelherr
    id: felix-caspelherr
    last: Caspelherr
  - first: Debanjan
    full: Debanjan Chaudhuri
    id: debanjan-chaudhuri
    last: Chaudhuri
  - first: Christian M.
    full: Christian M. Meyer
    id: christian-m-meyer
    last: Meyer
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Andreas Hanselowski, Avinesh PVS, Benjamin Schiller, Felix Caspelherr,
    Debanjan Chaudhuri, Christian M. Meyer, Iryna Gurevych
  bibkey: hanselowski-etal-2018-retrospective
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1859'
  page_last: '1874'
  pages: "1859\u20131874"
  paper_id: '158'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1158.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1158.jpg
  title: A Retrospective Analysis of the Fake News Challenge Stance-Detection Task
  title_html: A Retrospective Analysis of the Fake News Challenge Stance-Detection
    Task
  url: https://www.aclweb.org/anthology/C18-1158
  year: '2018'
C18-1159:
  abstract: Humor recognition is an interesting and challenging task in natural language
    processing. This paper proposes to exploit syntactic structure features to enhance
    humor recognition. Our method achieves significant improvements compared with
    humor theory driven baselines. We found that some syntactic structure features
    consistently correlate with humor, which indicate interesting linguistic phenomena.
    Both the experimental results and the analysis demonstrate that humor can be viewed
    as a kind of style and content independent syntactic structures can help identify
    humor and have good interpretability.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Lizhen
    full: Lizhen Liu
    id: lizhen-liu
    last: Liu
  - first: Donghai
    full: Donghai Zhang
    id: donghai-zhang
    last: Zhang
  - first: Wei
    full: Wei Song
    id: wei-song
    last: Song
  author_string: Lizhen Liu, Donghai Zhang, Wei Song
  bibkey: liu-etal-2018-exploiting
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1875'
  page_last: '1883'
  pages: "1875\u20131883"
  paper_id: '159'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1159.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1159.jpg
  title: Exploiting Syntactic Structures for Humor Recognition
  title_html: Exploiting Syntactic Structures for Humor Recognition
  url: https://www.aclweb.org/anthology/C18-1159
  year: '2018'
C18-1160:
  abstract: Spam detection has long been a research topic in both academic and industry
    due to its wide applications. Previous studies are mainly focused on extracting
    linguistic or behavior features to distinguish the spam and legitimate reviews.
    Such features are either ineffective or take long time to collect and thus are
    hard to be applied to cold-start spam review detection tasks. Recent advance leveraged
    the neural network to encode the textual and behavior features for the cold-start
    problem. However, the abundant attribute information are largely neglected by
    the existing framework. In this paper, we propose a novel deep learning architecture
    for incorporating entities and their inherent attributes from various domains
    into a unified framework. Specifically, our model not only encodes the entities
    of reviewer, item, and review, but also their attributes such as location, date,
    price ranges. Furthermore, we present a domain classifier to adapt the knowledge
    from one domain to the other. With the abundant attributes in existing entities
    and knowledge in other domains, we successfully solve the problem of data scarcity
    in the cold-start settings. Experimental results on two Yelp datasets prove that
    our proposed framework significantly outperforms the state-of-the-art methods.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Zhenni
    full: Zhenni You
    id: zhenni-you
    last: You
  - first: Tieyun
    full: Tieyun Qian
    id: tieyun-qian
    last: Qian
  - first: Bing
    full: Bing Liu
    id: bing-liu
    last: Liu
  author_string: Zhenni You, Tieyun Qian, Bing Liu
  bibkey: you-etal-2018-attribute
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1884'
  page_last: '1895'
  pages: "1884\u20131895"
  paper_id: '160'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1160.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1160.jpg
  title: An Attribute Enhanced Domain Adaptive Model for Cold-Start Spam Review Detection
  title_html: An Attribute Enhanced Domain Adaptive Model for Cold-Start Spam Review
    Detection
  url: https://www.aclweb.org/anthology/C18-1160
  year: '2018'
C18-1161:
  abstract: "Neural network approaches to Named-Entity Recognition reduce the need\
    \ for carefully hand-crafted features. While some features do remain in state-of-the-art\
    \ systems, lexical features have been mostly discarded, with the exception of\
    \ gazetteers. In this work, we show that this is unfair: lexical features are\
    \ actually quite useful. We propose to embed words and entity types into a low-dimensional\
    \ vector space we train from annotated data produced by distant supervision thanks\
    \ to Wikipedia. From this, we compute \u2014 offline \u2014 a feature vector representing\
    \ each word. When used with a vanilla recurrent neural network model, this representation\
    \ yields substantial improvements. We establish a new state-of-the-art F1 score\
    \ of 87.95 on ONTONOTES 5.0, while matching state-of-the-art performance with\
    \ a F1 score of 91.73 on the over-studied CONLL-2003 dataset."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Abbas
    full: Abbas Ghaddar
    id: abbas-ghaddar
    last: Ghaddar
  - first: Phillippe
    full: Phillippe Langlais
    id: philippe-langlais
    last: Langlais
  author_string: Abbas Ghaddar, Phillippe Langlais
  bibkey: ghaddar-langlais-2018-robust
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1896'
  page_last: '1907'
  pages: "1896\u20131907"
  paper_id: '161'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1161.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1161.jpg
  title: Robust Lexical Features for Improved Neural Network Named-Entity Recognition
  title_html: Robust Lexical Features for Improved Neural Network Named-Entity Recognition
  url: https://www.aclweb.org/anthology/C18-1161
  year: '2018'
C18-1162:
  abstract: Traditional supervised text classifiers require a large number of manually
    labeled documents, which are often expensive to obtain. Recently, dataless text
    classification has attracted more attention, since it only requires very few seed
    words of categories that are much cheaper. In this paper, we develop a pseudo-label
    based dataless Naive Bayes (PL-DNB) classifier with seed words. We initialize
    pseudo-labels for each document using seed word occurrences, and employ the expectation
    maximization algorithm to train PL-DNB in a semi-supervised manner. The pseudo-labels
    are iteratively updated using a mixture of seed word occurrences and estimations
    of label posteriors. To avoid noisy pseudo-labels, we also consider the information
    of nearest neighboring documents in the pseudo-label update step, i.e., preserving
    local neighborhood structure of documents. We empirically show that PL-DNB outperforms
    traditional dataless text classification algorithms with seed words. Especially,
    PL-DNB performs well on the imbalanced dataset.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Ximing
    full: Ximing Li
    id: ximing-li
    last: Li
  - first: Bo
    full: Bo Yang
    id: bo-yang
    last: Yang
  author_string: Ximing Li, Bo Yang
  bibkey: li-yang-2018-pseudo
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1908'
  page_last: '1917'
  pages: "1908\u20131917"
  paper_id: '162'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1162.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1162.jpg
  title: A Pseudo Label based Dataless Naive Bayes Algorithm for Text Classification
    with Seed Words
  title_html: A Pseudo Label based Dataless Naive <span class="acl-fixed-case">B</span>ayes
    Algorithm for Text Classification with Seed Words
  url: https://www.aclweb.org/anthology/C18-1162
  year: '2018'
C18-1163:
  abstract: Visual question answering (VQA) is a challenging task that requires a
    computer system to understand both a question and an image. While there is much
    research on VQA in English, there is a lack of datasets for other languages, and
    English annotation is not directly applicable in those languages. To deal with
    this, we have created a Japanese VQA dataset by using crowdsourced annotation
    with images from the Visual Genome dataset. This is the first such dataset in
    Japanese. As another contribution, we propose a cross-lingual method for making
    use of English annotation to improve a Japanese VQA system. The proposed method
    is based on a popular VQA method that uses an attention mechanism. We use attention
    maps generated from English questions to help improve the Japanese VQA task. The
    proposed method experimentally performed better than simply using a monolingual
    corpus, which demonstrates the effectiveness of using attention maps to transfer
    cross-lingual information.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Nobuyuki
    full: Nobuyuki Shimizu
    id: nobuyuki-shimizu
    last: Shimizu
  - first: Na
    full: Na Rong
    id: na-rong
    last: Rong
  - first: Takashi
    full: Takashi Miyazaki
    id: takashi-miyazaki
    last: Miyazaki
  author_string: Nobuyuki Shimizu, Na Rong, Takashi Miyazaki
  bibkey: shimizu-etal-2018-visual
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1918'
  page_last: '1928'
  pages: "1918\u20131928"
  paper_id: '163'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1163.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1163.jpg
  title: 'Visual Question Answering Dataset for Bilingual Image Understanding: A Study
    of Cross-Lingual Transfer Using Attention Maps'
  title_html: 'Visual Question Answering Dataset for Bilingual Image Understanding:
    A Study of Cross-Lingual Transfer Using Attention Maps'
  url: https://www.aclweb.org/anthology/C18-1163
  year: '2018'
C18-1164:
  abstract: "Modern and post-modern free verse poems feature a large and complex variety\
    \ in their poetic prosodies that falls along a continuum from a more fluent to\
    \ a more disfluent and choppy style. As the poets of modernism overcame rhyme\
    \ and meter, they oriented themselves in these two opposing directions, creating\
    \ a free verse spectrum that calls for new analyses of prosodic forms. We present\
    \ a method, grounded in philological analysis and current research on cognitive\
    \ (dis)fluency, for automatically analyzing this spectrum. We define and relate\
    \ six classes of poetic styles (ranging from parlando to lettristic decomposition)\
    \ by their gradual differentiation. Based on this discussion, we present a model\
    \ for automatic prosodic classification of spoken free verse poetry that uses\
    \ deep hierarchical attention networks to integrate the source text and audio\
    \ and predict the assigned class. We evaluate our model on a large corpus of German\
    \ author-read post-modern poetry and find that classes can reliably be differentiated,\
    \ reaching a weighted f-measure of 0.73, when combining textual and phonetic evidence.\
    \ In our further analyses, we validate the model\u2019s decision-making process,\
    \ the philologically hypothesized continuum of fluency and investigate the relative\
    \ importance of various features."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Timo
    full: Timo Baumann
    id: timo-baumann
    last: Baumann
  - first: Hussein
    full: Hussein Hussein
    id: hussein-hussein
    last: Hussein
  - first: Burkhard
    full: Burkhard Meyer-Sickendiek
    id: burkhard-meyer-sickendiek
    last: Meyer-Sickendiek
  author_string: Timo Baumann, Hussein Hussein, Burkhard Meyer-Sickendiek
  bibkey: baumann-etal-2018-style
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1929'
  page_last: '1940'
  pages: "1929\u20131940"
  paper_id: '164'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1164.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1164.jpg
  title: Style Detection for Free Verse Poetry from Text and Speech
  title_html: Style Detection for Free Verse Poetry from Text and Speech
  url: https://www.aclweb.org/anthology/C18-1164
  year: '2018'
C18-1165:
  abstract: Most question answering (QA) systems are based on raw text and structured
    knowledge graph. However, raw text corpora are hard for QA system to understand,
    and structured knowledge graph needs intensive manual work, while it is relatively
    easy to obtain semi-structured tables from many sources directly, or build them
    automatically. In this paper, we build an end-to-end system to answer multiple
    choice questions with semi-structured tables as its knowledge. Our system answers
    queries by two steps. First, it finds the most similar tables. Then the system
    measures the relevance between each question and candidate table cells, and choose
    the most related cell as the source of answer. The system is evaluated with TabMCQ
    dataset, and gets a huge improvement compared to the state of the art.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Hao
    full: Hao Wang
    id: hao-wang
    last: Wang
  - first: Xiaodong
    full: Xiaodong Zhang
    id: xiaodong-zhang
    last: Zhang
  - first: Shuming
    full: Shuming Ma
    id: shuming-ma
    last: Ma
  - first: Xu
    full: Xu Sun
    id: xu-sun
    last: Sun
  - first: Houfeng
    full: Houfeng Wang
    id: houfeng-wang
    last: Wang
  - first: Mengxiang
    full: Mengxiang Wang
    id: mengxiang-wang
    last: Wang
  author_string: Hao Wang, Xiaodong Zhang, Shuming Ma, Xu Sun, Houfeng Wang, Mengxiang
    Wang
  bibkey: wang-etal-2018-neural-question
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1941'
  page_last: '1951'
  pages: "1941\u20131951"
  paper_id: '165'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1165.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1165.jpg
  title: A Neural Question Answering Model Based on Semi-Structured Tables
  title_html: A Neural Question Answering Model Based on Semi-Structured Tables
  url: https://www.aclweb.org/anthology/C18-1165
  year: '2018'
C18-1166:
  abstract: The lack of large-scale question matching corpora greatly limits the development
    of matching methods in question answering (QA) system, especially for non-English
    languages. To ameliorate this situation, in this paper, we introduce a large-scale
    Chinese question matching corpus (named LCQMC), which is released to the public1.
    LCQMC is more general than paraphrase corpus as it focuses on intent matching
    rather than paraphrase. How to collect a large number of question pairs in variant
    linguistic forms, which may present the same intent, is the key point for such
    corpus construction. In this paper, we first use a search engine to collect large-scale
    question pairs related to high-frequency words from various domains, then filter
    irrelevant pairs by the Wasserstein distance, and finally recruit three annotators
    to manually check the left pairs. After this process, a question matching corpus
    that contains 260,068 question pairs is constructed. In order to verify the LCQMC
    corpus, we split it into three parts, i.e., a training set containing 238,766
    question pairs, a development set with 8,802 question pairs, and a test set with
    12,500 question pairs, and test several well-known sentence matching methods on
    it. The experimental results not only demonstrate the good quality of LCQMC but
    also provide solid baseline performance for further researches on this corpus.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Xin
    full: Xin Liu
    id: xin-liu
    last: Liu
  - first: Qingcai
    full: Qingcai Chen
    id: qingcai-chen
    last: Chen
  - first: Chong
    full: Chong Deng
    id: chong-deng
    last: Deng
  - first: Huajun
    full: Huajun Zeng
    id: huajun-zeng
    last: Zeng
  - first: Jing
    full: Jing Chen
    id: jing-chen
    last: Chen
  - first: Dongfang
    full: Dongfang Li
    id: dongfang-li
    last: Li
  - first: Buzhou
    full: Buzhou Tang
    id: buzhou-tang
    last: Tang
  author_string: Xin Liu, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Dongfang
    Li, Buzhou Tang
  bibkey: liu-etal-2018-lcqmc
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1952'
  page_last: '1962'
  pages: "1952\u20131962"
  paper_id: '166'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1166.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1166.jpg
  title: LCQMC:A Large-scale Chinese Question Matching Corpus
  title_html: <span class="acl-fixed-case">LCQMC</span>:A Large-scale <span class="acl-fixed-case">C</span>hinese
    Question Matching Corpus
  url: https://www.aclweb.org/anthology/C18-1166
  year: '2018'
C18-1167:
  abstract: Recent advances in Natural Language Processing are finding ways to place
    an emphasis on the hierarchical nature of text instead of representing language
    as a flat sequence or unordered collection of words or letters. A human reader
    must capture multiple levels of abstraction and meaning in order to formulate
    an understanding of a document. In this paper, we address the problem of developing
    approaches which are capable of working with extremely large and complex literary
    documents to perform Genre Identification. The task is to assign the literary
    classification to a full-length book belonging to a corpus of literature, where
    the works on average are well over 200,000 words long and genre is an abstract
    thematic concept. We introduce the Gutenberg Dataset for Genre Identification.
    Additionally, we present a study on how current deep learning models compare to
    traditional methods for this task. The results are presented as a baseline along
    with findings on how using an ensemble of chapters can significantly improve results
    in deep learning methods. The motivation behind the ensemble of chapters method
    is discussed as the compositionality of subtexts which make up a larger work and
    contribute to the overall genre.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Joseph
    full: Joseph Worsham
    id: joseph-worsham
    last: Worsham
  - first: Jugal
    full: Jugal Kalita
    id: jugal-kalita
    last: Kalita
  author_string: Joseph Worsham, Jugal Kalita
  bibkey: worsham-kalita-2018-genre
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1963'
  page_last: '1973'
  pages: "1963\u20131973"
  paper_id: '167'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1167.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1167.jpg
  title: Genre Identification and the Compositional Effect of Genre in Literature
  title_html: Genre Identification and the Compositional Effect of Genre in Literature
  url: https://www.aclweb.org/anthology/C18-1167
  year: '2018'
C18-1168:
  abstract: In this reproduction paper, we replicate and extend several past studies
    on transfer learning for entity recognition. In particular, we are interested
    in entity recognition problems where the class labels in the source and target
    domains are different. Our work is the first direct comparison of these previously
    published approaches in this problem setting. In addition, we perform experiments
    on seven new source/target corpus pairs, nearly doubling the total number of corpus
    pairs that have been studied in all past work combined. Our results empirically
    demonstrate when each of the published approaches tends to do well. In particular,
    simpler approaches often work best when there is very little labeled target data,
    while neural transfer approaches tend to do better when there is more labeled
    target data.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Juan Diego
    full: Juan Diego Rodriguez
    id: juan-diego-rodriguez
    last: Rodriguez
  - first: Adam
    full: Adam Caldwell
    id: adam-caldwell
    last: Caldwell
  - first: Alexander
    full: Alexander Liu
    id: alex-liu
    last: Liu
  author_string: Juan Diego Rodriguez, Adam Caldwell, Alexander Liu
  bibkey: rodriguez-etal-2018-transfer
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1974'
  page_last: '1985'
  pages: "1974\u20131985"
  paper_id: '168'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1168.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1168.jpg
  title: Transfer Learning for Entity Recognition of Novel Classes
  title_html: Transfer Learning for Entity Recognition of Novel Classes
  url: https://www.aclweb.org/anthology/C18-1168
  year: '2018'
C18-1169:
  abstract: Extracting location names from informal and unstructured social media
    data requires the identification of referent boundaries and partitioning compound
    names. Variability, particularly systematic variability in location names (Carroll,
    1983), challenges the identification task. Some of this variability can be anticipated
    as operations within a statistical language model, in this case drawn from gazetteers
    such as OpenStreetMap (OSM), Geonames, and DBpedia. This permits evaluation of
    an observed n-gram in Twitter targeted text as a legitimate location name variant
    from the same location-context. Using n-gram statistics and location-related dictionaries,
    our Location Name Extraction tool (LNEx) handles abbreviations and automatically
    filters and augments the location names in gazetteers (handling name contractions
    and auxiliary contents) to help detect the boundaries of multi-word location names
    and thereby delimit them in texts. We evaluated our approach on 4,500 event-specific
    tweets from three targeted streams to compare the performance of LNEx against
    that of ten state-of-the-art taggers that rely on standard semantic, syntactic
    and/or orthographic features. LNEx improved the average F-Score by 33-179%, outperforming
    all taggers. Further, LNEx is capable of stream processing.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Hussein
    full: Hussein Al-Olimat
    id: hussein-al-olimat
    last: Al-Olimat
  - first: Krishnaprasad
    full: Krishnaprasad Thirunarayan
    id: krishnaprasad-thirunarayan
    last: Thirunarayan
  - first: Valerie
    full: Valerie Shalin
    id: valerie-shalin
    last: Shalin
  - first: Amit
    full: Amit Sheth
    id: amit-sheth
    last: Sheth
  author_string: Hussein Al-Olimat, Krishnaprasad Thirunarayan, Valerie Shalin, Amit
    Sheth
  bibkey: al-olimat-etal-2018-location
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1986'
  page_last: '1997'
  pages: "1986\u20131997"
  paper_id: '169'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1169.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1169.jpg
  title: Location Name Extraction from Targeted Text Streams using Gazetteer-based
    Statistical Language Models
  title_html: Location Name Extraction from Targeted Text Streams using Gazetteer-based
    Statistical Language Models
  url: https://www.aclweb.org/anthology/C18-1169
  year: '2018'
C18-1170:
  abstract: In this paper, we study the problem of question answering over knowledge
    base. We identify that the primary bottleneck in this problem is the difficulty
    in accurately predicting the relations connecting the subject entity to the object
    entities. We advocate a new model architecture, APVA, which includes a verification
    mechanism responsible for checking the correctness of predicted relations. The
    APVA framework naturally supports a well-principled iterative training procedure,
    which we call turbo training. We demonstrate via experiments that the APVA-TUBRO
    approach drastically improves the question answering performance.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yue
    full: Yue Wang
    id: yue-wang
    last: Wang
  - first: Richong
    full: Richong Zhang
    id: richong-zhang
    last: Zhang
  - first: Cheng
    full: Cheng Xu
    id: cheng-xu
    last: Xu
  - first: Yongyi
    full: Yongyi Mao
    id: yongyi-mao
    last: Mao
  author_string: Yue Wang, Richong Zhang, Cheng Xu, Yongyi Mao
  bibkey: wang-etal-2018-apva
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '1998'
  page_last: '2009'
  pages: "1998\u20132009"
  paper_id: '170'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1170.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1170.jpg
  title: The APVA-TURBO Approach To Question Answering in Knowledge Base
  title_html: The <span class="acl-fixed-case">APVA</span>-<span class="acl-fixed-case">TURBO</span>
    Approach To Question Answering in Knowledge Base
  url: https://www.aclweb.org/anthology/C18-1170
  year: '2018'
C18-1171:
  abstract: Multi-relation Question Answering is a challenging task, due to the requirement
    of elaborated analysis on questions and reasoning over multiple fact triples in
    knowledge base. In this paper, we present a novel model called Interpretable Reasoning
    Network that employs an interpretable, hop-by-hop reasoning process for question
    answering. The model dynamically decides which part of an input question should
    be analyzed at each hop; predicts a relation that corresponds to the current parsed
    results; utilizes the predicted relation to update the question representation
    and the state of the reasoning process; and then drives the next-hop reasoning.
    Experiments show that our model yields state-of-the-art results on two datasets.
    More interestingly, the model can offer traceable and observable intermediate
    predictions for reasoning analysis and failure diagnosis, thereby allowing manual
    manipulation in predicting the final answer.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Mantong
    full: Mantong Zhou
    id: mantong-zhou
    last: Zhou
  - first: Minlie
    full: Minlie Huang
    id: minlie-huang
    last: Huang
  - first: Xiaoyan
    full: Xiaoyan Zhu
    id: xiaoyan-zhu
    last: Zhu
  author_string: Mantong Zhou, Minlie Huang, Xiaoyan Zhu
  bibkey: zhou-etal-2018-interpretable
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2010'
  page_last: '2022'
  pages: "2010\u20132022"
  paper_id: '171'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1171.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1171.jpg
  title: An Interpretable Reasoning Network for Multi-Relation Question Answering
  title_html: An Interpretable Reasoning Network for Multi-Relation Question Answering
  url: https://www.aclweb.org/anthology/C18-1171
  year: '2018'
C18-1172:
  abstract: Distributed word representation plays a pivotal role in various natural
    language processing tasks. In spite of its success, most existing methods only
    consider contextual information, which is suboptimal when used in various tasks
    due to a lack of task-specific features. The rational word embeddings should have
    the ability to capture both the semantic features and task-specific features of
    words. In this paper, we propose a task-oriented word embedding method and apply
    it to the text classification task. With the function-aware component, our method
    regularizes the distribution of words to enable the embedding space to have a
    clear classification boundary. We evaluate our method using five text classification
    datasets. The experiment results show that our method significantly outperforms
    the state-of-the-art methods.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Qian
    full: Qian Liu
    id: qian-liu
    last: Liu
  - first: Heyan
    full: Heyan Huang
    id: he-yan-huang
    last: Huang
  - first: Yang
    full: Yang Gao
    id: yang-gao
    last: Gao
  - first: Xiaochi
    full: Xiaochi Wei
    id: xiaochi-wei
    last: Wei
  - first: Yuxin
    full: Yuxin Tian
    id: yuxin-tian
    last: Tian
  - first: Luyang
    full: Luyang Liu
    id: luyang-liu
    last: Liu
  author_string: Qian Liu, Heyan Huang, Yang Gao, Xiaochi Wei, Yuxin Tian, Luyang
    Liu
  bibkey: liu-etal-2018-task
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2023'
  page_last: '2032'
  pages: "2023\u20132032"
  paper_id: '172'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1172.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1172.jpg
  title: Task-oriented Word Embedding for Text Classification
  title_html: Task-oriented Word Embedding for Text Classification
  url: https://www.aclweb.org/anthology/C18-1172
  year: '2018'
C18-1173:
  abstract: 'Representation learning is a key issue for most Natural Language Processing
    (NLP) tasks. Most existing representation models either learn little structure
    information or just rely on pre-defined structures, leading to degradation of
    performance and generalization capability. This paper focuses on learning both
    local semantic and global structure representations for text classification. In
    detail, we propose a novel Sandwich Neural Network (SNN) to learn semantic and
    structure representations automatically without relying on parsers. More importantly,
    semantic and structure information contribute unequally to the text representation
    at corpus and instance level. To solve the fusion problem, we propose two strategies:
    Adaptive Learning Sandwich Neural Network (AL-SNN) and Self-Attention Sandwich
    Neural Network (SA-SNN). The former learns the weights at corpus level, and the
    latter further combines attention mechanism to assign the weights at instance
    level. Experimental results demonstrate that our approach achieves competitive
    performance on several text classification tasks, including sentiment analysis,
    question type classification and subjectivity classification. Specifically, the
    accuracies are MR (82.1%), SST-5 (50.4%), TREC (96%) and SUBJ (93.9%).'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Jianyu
    full: Jianyu Zhao
    id: jianyu-zhao
    last: Zhao
  - first: Zhiqiang
    full: Zhiqiang Zhan
    id: zhiqiang-zhan
    last: Zhan
  - first: Qichuan
    full: Qichuan Yang
    id: qichuan-yang
    last: Yang
  - first: Yang
    full: Yang Zhang
    id: yang-zhang
    last: Zhang
  - first: Changjian
    full: Changjian Hu
    id: changjian-hu
    last: Hu
  - first: Zhensheng
    full: Zhensheng Li
    id: zhensheng-li
    last: Li
  - first: Liuxin
    full: Liuxin Zhang
    id: liuxin-zhang
    last: Zhang
  - first: Zhiqiang
    full: Zhiqiang He
    id: zhiqiang-he
    last: He
  author_string: Jianyu Zhao, Zhiqiang Zhan, Qichuan Yang, Yang Zhang, Changjian Hu,
    Zhensheng Li, Liuxin Zhang, Zhiqiang He
  bibkey: zhao-etal-2018-adaptive
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2033'
  page_last: '2043'
  pages: "2033\u20132043"
  paper_id: '173'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1173.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1173.jpg
  title: Adaptive Learning of Local Semantic and Global Structure Representations
    for Text Classification
  title_html: Adaptive Learning of Local Semantic and Global Structure Representations
    for Text Classification
  url: https://www.aclweb.org/anthology/C18-1173
  year: '2018'
C18-1174:
  abstract: "Lyrics contain repeated patterns that are correlated with the repetitions\
    \ found in the music they accompany. Repetitions in song texts have been shown\
    \ to enable lyrics segmentation \u2013 a fundamental prerequisite of automatically\
    \ detecting the building blocks (e.g. chorus, verse) of a song text. In this article\
    \ we improve on the state-of-the-art in lyrics segmentation by applying a convolutional\
    \ neural network to the task, and experiment with novel features as a step towards\
    \ deeper macrostructure detection of lyrics."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Michael
    full: Michael Fell
    id: michael-fell
    last: Fell
  - first: Yaroslav
    full: Yaroslav Nechaev
    id: yaroslav-nechaev
    last: Nechaev
  - first: Elena
    full: Elena Cabrio
    id: elena-cabrio
    last: Cabrio
  - first: Fabien
    full: Fabien Gandon
    id: fabien-gandon
    last: Gandon
  author_string: Michael Fell, Yaroslav Nechaev, Elena Cabrio, Fabien Gandon
  bibkey: fell-etal-2018-lyrics
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2044'
  page_last: '2054'
  pages: "2044\u20132054"
  paper_id: '174'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1174.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1174.jpg
  title: 'Lyrics Segmentation: Textual Macrostructure Detection using Convolutions'
  title_html: 'Lyrics Segmentation: Textual Macrostructure Detection using Convolutions'
  url: https://www.aclweb.org/anthology/C18-1174
  year: '2018'
C18-1175:
  abstract: Neural network based multi-task learning has achieved great success on
    many NLP problems, which focuses on sharing knowledge among tasks by linking some
    layers to enhance the performance. However, most existing approaches suffer from
    the interference between tasks because they lack of selection mechanism for feature
    sharing. In this way, the feature spaces of tasks may be easily contaminated by
    helpless features borrowed from others, which will confuse the models for making
    correct prediction. In this paper, we propose a multi-task convolutional neural
    network with the Leaky Unit, which has memory and forgetting mechanism to filter
    the feature flows between tasks. Experiments on five different datasets for text
    classification validate the benefits of our approach.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Liqiang
    full: Liqiang Xiao
    id: liqiang-xiao
    last: Xiao
  - first: Honglun
    full: Honglun Zhang
    id: honglun-zhang
    last: Zhang
  - first: Wenqing
    full: Wenqing Chen
    id: wenqing-chen
    last: Chen
  - first: Yongkun
    full: Yongkun Wang
    id: yongkun-wang
    last: Wang
  - first: Yaohui
    full: Yaohui Jin
    id: yaohui-jin
    last: Jin
  author_string: Liqiang Xiao, Honglun Zhang, Wenqing Chen, Yongkun Wang, Yaohui Jin
  bibkey: xiao-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2055'
  page_last: '2065'
  pages: "2055\u20132065"
  paper_id: '175'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1175.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1175.jpg
  title: 'Learning What to Share: Leaky Multi-Task Network for Text Classification'
  title_html: 'Learning What to Share: Leaky Multi-Task Network for Text Classification'
  url: https://www.aclweb.org/anthology/C18-1175
  year: '2018'
C18-1176:
  abstract: "Searching for sentences containing claims in a large text corpus is a\
    \ key component in developing an argumentative content search engine. Previous\
    \ works focused on detecting claims in a small set of documents or within documents\
    \ enriched with argumentative content. However, pinpointing relevant claims in\
    \ massive unstructured corpora, received little attention. A step in this direction\
    \ was taken in (Levy et al. 2017), where the authors suggested using a weak signal\
    \ to develop a relatively strict query for claim\u2013sentence detection. Here,\
    \ we leverage this work to define weak signals for training DNNs to obtain significantly\
    \ greater performance. This approach allows to relax the query and increase the\
    \ potential coverage. Our results clearly indicate that the system is able to\
    \ successfully generalize from the weak signal, outperforming previously reported\
    \ results in terms of both precision and coverage. Finally, we adapt our system\
    \ to solve a recent argument mining task of identifying argumentative sentences\
    \ in Web texts retrieved from heterogeneous sources, and obtain F1 scores comparable\
    \ to the supervised baseline."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Ran
    full: Ran Levy
    id: ran-levy
    last: Levy
  - first: Ben
    full: Ben Bogin
    id: ben-bogin
    last: Bogin
  - first: Shai
    full: Shai Gretz
    id: shai-gretz
    last: Gretz
  - first: Ranit
    full: Ranit Aharonov
    id: ranit-aharonov
    last: Aharonov
  - first: Noam
    full: Noam Slonim
    id: noam-slonim
    last: Slonim
  author_string: Ran Levy, Ben Bogin, Shai Gretz, Ranit Aharonov, Noam Slonim
  bibkey: levy-etal-2018-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2066'
  page_last: '2081'
  pages: "2066\u20132081"
  paper_id: '176'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1176.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1176.jpg
  title: Towards an argumentative content search engine using weak supervision
  title_html: Towards an argumentative content search engine using weak supervision
  url: https://www.aclweb.org/anthology/C18-1176
  year: '2018'
C18-1177:
  abstract: "Previous studies have shown that linguistic features of a word such as\
    \ possession, genitive or other grammatical cases can be employed in word representations\
    \ of a named entity recognition (NER) tagger to improve the performance for morphologically\
    \ rich languages. However, these taggers require external morphological disambiguation\
    \ (MD) tools to function which are hard to obtain or non-existent for many languages.\
    \ In this work, we propose a model which alleviates the need for such disambiguators\
    \ by jointly learning NER and MD taggers in languages for which one can provide\
    \ a list of candidate morphological analyses. We show that this can be done independent\
    \ of the morphological annotation schemes, which differ among languages. Our experiments\
    \ employing three different model architectures that join these two tasks show\
    \ that joint learning improves NER performance. Furthermore, the morphological\
    \ disambiguator\u2019s performance is shown to be competitive."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Onur
    full: "Onur G\xFCng\xF6r"
    id: onur-gungor
    last: "G\xFCng\xF6r"
  - first: Suzan
    full: Suzan Uskudarli
    id: suzan-uskudarli
    last: Uskudarli
  - first: Tunga
    full: "Tunga G\xFCng\xF6r"
    id: tunga-gungor
    last: "G\xFCng\xF6r"
  author_string: "Onur G\xFCng\xF6r, Suzan Uskudarli, Tunga G\xFCng\xF6r"
  bibkey: gungor-etal-2018-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2082'
  page_last: '2092'
  pages: "2082\u20132092"
  paper_id: '177'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1177.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1177.jpg
  title: Improving Named Entity Recognition by Jointly Learning to Disambiguate Morphological
    Tags
  title_html: Improving Named Entity Recognition by Jointly Learning to Disambiguate
    Morphological Tags
  url: https://www.aclweb.org/anthology/C18-1177
  year: '2018'
C18-1178:
  abstract: "Question answering over knowledge graphs is an important problem of interest\
    \ both commercially and academically. There is substantial interest in the class\
    \ of natural language questions that can be answered via the lookup of a single\
    \ fact, driven by the availability of the popular SimpleQuestions dataset. The\
    \ problem with this dataset, however, is that answer triples are provided from\
    \ Freebase, which has been defunct for several years. As a result, it is difficult\
    \ to build \u201Creal-world\u201D question answering systems that are operationally\
    \ deployable. Furthermore, a defunct knowledge graph means that much of the infrastructure\
    \ for querying, browsing, and manipulating triples no longer exists. To address\
    \ this problem, we present SimpleDBpediaQA, a new benchmark dataset for simple\
    \ question answering over knowledge graphs that was created by mapping SimpleQuestions\
    \ entities and predicates from Freebase to DBpedia. Although this mapping is conceptually\
    \ straightforward, there are a number of nuances that make the task non-trivial,\
    \ owing to the different conceptual organizations of the two knowledge graphs.\
    \ To lay the foundation for future research using this dataset, we leverage recent\
    \ work to provide simple yet strong baselines with and without neural networks."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Michael
    full: Michael Azmy
    id: michael-azmy
    last: Azmy
  - first: Peng
    full: Peng Shi
    id: peng-shi
    last: Shi
  - first: Jimmy
    full: Jimmy Lin
    id: jimmy-lin
    last: Lin
  - first: Ihab
    full: Ihab Ilyas
    id: ihab-ilyas
    last: Ilyas
  author_string: Michael Azmy, Peng Shi, Jimmy Lin, Ihab Ilyas
  bibkey: azmy-etal-2018-farewell
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2093'
  page_last: '2103'
  pages: "2093\u20132103"
  paper_id: '178'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1178.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1178.jpg
  title: 'Farewell Freebase: Migrating the SimpleQuestions Dataset to DBpedia'
  title_html: 'Farewell <span class="acl-fixed-case">F</span>reebase: Migrating the
    <span class="acl-fixed-case">S</span>imple<span class="acl-fixed-case">Q</span>uestions
    Dataset to <span class="acl-fixed-case">DB</span>pedia'
  url: https://www.aclweb.org/anthology/C18-1178
  year: '2018'
C18-1179:
  abstract: 'Several datasets have been annotated and published for classification
    of emotions. They differ in several ways: (1) the use of different annotation
    schemata (e. g., discrete label sets, including joy, anger, fear, or sadness or
    continuous values including valence, or arousal), (2) the domain, and, (3) the
    file formats. This leads to several research gaps: supervised models often only
    use a limited set of available resources. Additionally, no previous work has compared
    emotion corpora in a systematic manner. We aim at contributing to this situation
    with a survey of the datasets, and aggregate them in a common file format with
    a common annotation schema. Based on this aggregation, we perform the first cross-corpus
    classification experiments in the spirit of future research enabled by this paper,
    in order to gain insight and a better understanding of differences of models inferred
    from the data. This work also simplifies the choice of the most appropriate resources
    for developing a model for a novel domain. One result from our analysis is that
    a subset of corpora is better classified with models trained on a different corpus.
    For none of the corpora, training on all data altogether is better than using
    a subselection of the resources. Our unified corpus is available at http://www.ims.uni-stuttgart.de/data/unifyemotion.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Laura-Ana-Maria
    full: Laura-Ana-Maria Bostan
    id: laura-ana-maria-bostan
    last: Bostan
  - first: Roman
    full: Roman Klinger
    id: roman-klinger
    last: Klinger
  author_string: Laura-Ana-Maria Bostan, Roman Klinger
  bibkey: bostan-klinger-2018-analysis
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2104'
  page_last: '2119'
  pages: "2104\u20132119"
  paper_id: '179'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1179.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1179.jpg
  title: An Analysis of Annotated Corpora for Emotion Classification in Text
  title_html: An Analysis of Annotated Corpora for Emotion Classification in Text
  url: https://www.aclweb.org/anthology/C18-1179
  year: '2018'
C18-1180:
  abstract: Text classification is one of the most widely studied tasks in natural
    language processing. Motivated by the principle of compositionality, large multilayer
    neural network models have been employed for this task in an attempt to effectively
    utilize the constituent expressions. Almost all of the reported work train large
    networks using discriminative approaches, which come with a caveat of no proper
    capacity control, as they tend to latch on to any signal that may not generalize.
    Using various recent state-of-the-art approaches for text classification, we explore
    whether these models actually learn to compose the meaning of the sentences or
    still just focus on some keywords or lexicons for classifying the document. To
    test our hypothesis, we carefully construct datasets where the training and test
    splits have no direct overlap of such lexicons, but overall language structure
    would be similar. We study various text classifiers and observe that there is
    a big performance drop on these datasets. Finally, we show that even simple models
    with our proposed regularization techniques, which disincentivize focusing on
    key lexicons, can substantially improve classification accuracy.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Devendra
    full: Devendra Sachan
    id: devendra-sachan
    last: Sachan
  - first: Manzil
    full: Manzil Zaheer
    id: manzil-zaheer
    last: Zaheer
  - first: Ruslan
    full: Ruslan Salakhutdinov
    id: ruslan-salakhutdinov
    last: Salakhutdinov
  author_string: Devendra Sachan, Manzil Zaheer, Ruslan Salakhutdinov
  bibkey: sachan-etal-2018-investigating
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2120'
  page_last: '2131'
  pages: "2120\u20132131"
  paper_id: '180'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1180.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1180.jpg
  title: Investigating the Working of Text Classifiers
  title_html: Investigating the Working of Text Classifiers
  url: https://www.aclweb.org/anthology/C18-1180
  year: '2018'
C18-1181:
  abstract: Given a question and a set of candidate answers, answer selection is the
    task of identifying which of the candidates answers the question correctly. It
    is an important problem in natural language processing, with applications in many
    areas. Recently, many deep learning based methods have been proposed for the task.
    They produce impressive performance without relying on any feature engineering
    or expensive external resources. In this paper, we aim to provide a comprehensive
    review on deep learning methods applied to answer selection.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Tuan Manh
    full: Tuan Manh Lai
    id: tuan-lai
    last: Lai
  - first: Trung
    full: Trung Bui
    id: trung-bui
    last: Bui
  - first: Sheng
    full: Sheng Li
    id: sheng-li
    last: Li
  author_string: Tuan Manh Lai, Trung Bui, Sheng Li
  bibkey: lai-etal-2018-review
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2132'
  page_last: '2144'
  pages: "2132\u20132144"
  paper_id: '181'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1181.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1181.jpg
  title: A Review on Deep Learning Techniques Applied to Answer Selection
  title_html: A Review on Deep Learning Techniques Applied to Answer Selection
  url: https://www.aclweb.org/anthology/C18-1181
  year: '2018'
C18-1182:
  abstract: Named Entity Recognition (NER) is a key component in NLP systems for question
    answering, information retrieval, relation extraction, etc. NER systems have been
    studied and developed widely for decades, but accurate systems using deep neural
    networks (NN) have only been introduced in the last few years. We present a comprehensive
    survey of deep neural network architectures for NER, and contrast them with previous
    approaches to NER based on feature engineering and other supervised or semi-supervised
    learning algorithms. Our results highlight the improvements achieved by neural
    networks, and show how incorporating some of the lessons learned from past work
    on feature-based NER systems can yield further improvements.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Vikas
    full: Vikas Yadav
    id: vikas-yadav
    last: Yadav
  - first: Steven
    full: Steven Bethard
    id: steven-bethard
    last: Bethard
  author_string: Vikas Yadav, Steven Bethard
  bibkey: yadav-bethard-2018-survey
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2145'
  page_last: '2158'
  pages: "2145\u20132158"
  paper_id: '182'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1182.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1182.jpg
  title: A Survey on Recent Advances in Named Entity Recognition from Deep Learning
    models
  title_html: A Survey on Recent Advances in Named Entity Recognition from Deep Learning
    models
  url: https://www.aclweb.org/anthology/C18-1182
  year: '2018'
C18-1183:
  abstract: 'A bottleneck problem with Chinese named entity recognition (NER) in new
    domains is the lack of annotated data. One solution is to utilize the method of
    distant supervision, which has been widely used in relation extraction, to automatically
    populate annotated training data without humancost. The distant supervision assumption
    here is that if a string in text is included in a predefined dictionary of entities,
    the string might be an entity. However, this kind of auto-generated data suffers
    from two main problems: incomplete and noisy annotations, which affect the performance
    of NER models. In this paper, we propose a novel approach which can partially
    solve the above problems of distant supervision for NER. In our approach, to handle
    the incomplete problem, we apply partial annotation learning to reduce the effect
    of unknown labels of characters. As for noisy annotation, we design an instance
    selector based on reinforcement learning to distinguish positive sentences from
    auto-generated annotations. In experiments, we create two datasets for Chinese
    named entity recognition in two domains with the help of distant supervision.
    The experimental results show that the proposed approach obtains better performance
    than the comparison systems on both two datasets.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yaosheng
    full: Yaosheng Yang
    id: yaosheng-yang
    last: Yang
  - first: Wenliang
    full: Wenliang Chen
    id: wenliang-chen
    last: Chen
  - first: Zhenghua
    full: Zhenghua Li
    id: zhenghua-li
    last: Li
  - first: Zhengqiu
    full: Zhengqiu He
    id: zhengqiu-he
    last: He
  - first: Min
    full: Min Zhang
    id: min-zhang
    last: Zhang
  author_string: Yaosheng Yang, Wenliang Chen, Zhenghua Li, Zhengqiu He, Min Zhang
  bibkey: yang-etal-2018-distantly
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2159'
  page_last: '2169'
  pages: "2159\u20132169"
  paper_id: '183'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1183.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1183.jpg
  title: Distantly Supervised NER with Partial Annotation Learning and Reinforcement
    Learning
  title_html: Distantly Supervised <span class="acl-fixed-case">NER</span> with Partial
    Annotation Learning and Reinforcement Learning
  url: https://www.aclweb.org/anthology/C18-1183
  year: '2018'
C18-1184:
  abstract: In this paper, we present a novel model for entity disambiguation that
    combines both local contextual information and global evidences through Limited
    Discrepancy Search (LDS). Given an input document, we start from a complete solution
    constructed by a local model and conduct a search in the space of possible corrections
    to improve the local solution from a global view point. Our search utilizes a
    heuristic function to focus more on the least confident local decisions and a
    pruning function to score the global solutions based on their local fitness and
    the global coherences among the predicted entities. Experimental results on CoNLL
    2003 and TAC 2010 benchmarks verify the effectiveness of our model.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Hamed
    full: Hamed Shahbazi
    id: hamed-shahbazi
    last: Shahbazi
  - first: Xiaoli
    full: Xiaoli Fern
    id: xiaoli-fern
    last: Fern
  - first: Reza
    full: Reza Ghaeini
    id: reza-ghaeini
    last: Ghaeini
  - first: Chao
    full: Chao Ma
    id: chao-ma
    last: Ma
  - first: Rasha Mohammad
    full: Rasha Mohammad Obeidat
    id: rasha-mohammad-obeidat
    last: Obeidat
  - first: Prasad
    full: Prasad Tadepalli
    id: prasad-tadepalli
    last: Tadepalli
  author_string: Hamed Shahbazi, Xiaoli Fern, Reza Ghaeini, Chao Ma, Rasha Mohammad
    Obeidat, Prasad Tadepalli
  bibkey: shahbazi-etal-2018-joint
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2170'
  page_last: '2180'
  pages: "2170\u20132180"
  paper_id: '184'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1184.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1184.jpg
  title: Joint Neural Entity Disambiguation with Output Space Search
  title_html: Joint Neural Entity Disambiguation with Output Space Search
  url: https://www.aclweb.org/anthology/C18-1184
  year: '2018'
C18-1185:
  abstract: "In this paper, we propose to use a sequence to sequence model for Named\
    \ Entity Recognition (NER) and we explore the effectiveness of such model in a\
    \ progressive NER setting \u2013 a Transfer Learning (TL) setting. We train an\
    \ initial model on source data and transfer it to a model that can recognize new\
    \ NE categories in the target data during a subsequent step, when the source data\
    \ is no longer available. Our solution consists in: (i) to reshape and re-parametrize\
    \ the output layer of the first learned model to enable the recognition of new\
    \ NEs; (ii) to leave the rest of the architecture unchanged, such that it is initialized\
    \ with parameters transferred from the initial model; and (iii) to fine tune the\
    \ network on the target data. Most importantly, we design a new NER approach based\
    \ on sequence to sequence (Seq2Seq) models, which can intuitively work better\
    \ in our progressive setting. We compare our approach with a Bidirectional LSTM,\
    \ which is a strong neural NER model. Our experiments show that the Seq2Seq model\
    \ performs very well on the standard NER setting and it is more robust in the\
    \ progressive setting. Our approach can recognize previously unseen NE categories\
    \ while preserving the knowledge of the seen data."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Lingzhen
    full: Lingzhen Chen
    id: lingzhen-chen
    last: Chen
  - first: Alessandro
    full: Alessandro Moschitti
    id: alessandro-moschitti
    last: Moschitti
  author_string: Lingzhen Chen, Alessandro Moschitti
  bibkey: chen-moschitti-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2181'
  page_last: '2191'
  pages: "2181\u20132191"
  paper_id: '185'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1185.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1185.jpg
  title: Learning to Progressively Recognize New Named Entities with Sequence to Sequence
    Models
  title_html: Learning to Progressively Recognize New Named Entities with Sequence
    to Sequence Models
  url: https://www.aclweb.org/anthology/C18-1185
  year: '2018'
C18-1186:
  abstract: Providing instant responses for product questions in E-commerce sites
    can significantly improve satisfaction of potential consumers. We propose a new
    framework for automatically responding product questions newly posed by users
    via exploiting existing QA collections and review collections in a coordinated
    manner. Our framework can return a ranked list of snippets serving as the automated
    response for a given question, where each snippet can be a sentence from reviews
    or an existing question-answer pair. One major subtask in our framework is question-based
    response review ranking. Learning for response review ranking is challenging since
    there is no labeled response review available. The collection of existing QA pairs
    are exploited as distant supervision for learning to rank responses. With proposed
    distant supervision paradigm, the learned response ranking model makes use of
    the knowledge in the QA pairs and the corresponding retrieved review lists. Extensive
    experiments on datasets collected from a real-world commercial E-commerce site
    demonstrate the effectiveness of our proposed framework.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Qian
    full: Qian Yu
    id: qian-yu
    last: Yu
  - first: Wai
    full: Wai Lam
    id: wai-lam
    last: Lam
  - first: Zihao
    full: Zihao Wang
    id: zihao-wang
    last: Wang
  author_string: Qian Yu, Wai Lam, Zihao Wang
  bibkey: yu-etal-2018-responding
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2192'
  page_last: '2203'
  pages: "2192\u20132203"
  paper_id: '186'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1186.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1186.jpg
  title: Responding E-commerce Product Questions via Exploiting QA Collections and
    Reviews
  title_html: Responding E-commerce Product Questions via Exploiting <span class="acl-fixed-case">QA</span>
    Collections and Reviews
  url: https://www.aclweb.org/anthology/C18-1186
  year: '2018'
C18-1187:
  abstract: Human communication includes information, opinions and reactions. Reactions
    are often captured by the affective-messages in written as well as verbal communications.
    While there has been work in affect modeling and to some extent affective content
    generation, the area of affective word distributions is not well studied. Synsets
    and lexica capture semantic relationships across words. These models, however,
    lack in encoding affective or emotional word interpretations. Our proposed model,
    Aff2Vec, provides a method for enriched word embeddings that are representative
    of affective interpretations of words. Aff2Vec outperforms the state-of-the-art
    in intrinsic word-similarity tasks. Further, the use of Aff2Vec representations
    outperforms baseline embeddings in downstream natural language understanding tasks
    including sentiment analysis, personality detection, and frustration prediction.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Sopan
    full: Sopan Khosla
    id: sopan-khosla
    last: Khosla
  - first: Niyati
    full: Niyati Chhaya
    id: niyati-chhaya
    last: Chhaya
  - first: Kushal
    full: Kushal Chawla
    id: kushal-chawla
    last: Chawla
  author_string: Sopan Khosla, Niyati Chhaya, Kushal Chawla
  bibkey: khosla-etal-2018-aff2vec
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2204'
  page_last: '2218'
  pages: "2204\u20132218"
  paper_id: '187'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1187.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1187.jpg
  title: "Aff2Vec: Affect\u2013Enriched Distributional Word Representations"
  title_html: "<span class=\"acl-fixed-case\">A</span>ff2<span class=\"acl-fixed-case\"\
    >V</span>ec: Affect\u2013Enriched Distributional Word Representations"
  url: https://www.aclweb.org/anthology/C18-1187
  year: '2018'
C18-1188:
  abstract: "We developed three systems for generating pros and cons summaries of\
    \ product reviews. Automating this task eases the writing of product reviews,\
    \ and offers readers quick access to the most important information. We compared\
    \ SynPat, a system based on syntactic phrases selected on the basis of valence\
    \ scores, against a neural-network-based system trained to map bag-of-words representations\
    \ of reviews directly to pros and cons, and the same neural system trained on\
    \ clusters of word-embedding encodings of similar pros and cons. We evaluated\
    \ the systems in two ways: first on held-out reviews with gold-standard pros and\
    \ cons, and second by asking human annotators to rate the systems\u2019 output\
    \ on relevance and completeness. In the second evaluation, the gold-standard pros\
    \ and cons were assessed along with the system output. We find that the human-generated\
    \ summaries are not deemed as significantly more relevant or complete than the\
    \ SynPat systems; the latter are scored higher than the human-generated summaries\
    \ on a precision metric. The neural approaches yield a lower performance in the\
    \ human assessment, and are outperformed by the baseline."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Florian
    full: Florian Kunneman
    id: florian-kunneman
    last: Kunneman
  - first: Sander
    full: Sander Wubben
    id: sander-wubben
    last: Wubben
  - first: Antal
    full: Antal van den Bosch
    id: antal-van-den-bosch
    last: van den Bosch
  - first: Emiel
    full: Emiel Krahmer
    id: emiel-krahmer
    last: Krahmer
  author_string: Florian Kunneman, Sander Wubben, Antal van den Bosch, Emiel Krahmer
  bibkey: kunneman-etal-2018-aspect
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2219'
  page_last: '2229'
  pages: "2219\u20132229"
  paper_id: '188'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1188.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1188.jpg
  title: Aspect-based summarization of pros and cons in unstructured product reviews
  title_html: Aspect-based summarization of pros and cons in unstructured product
    reviews
  url: https://www.aclweb.org/anthology/C18-1188
  year: '2018'
C18-1189:
  abstract: Sentiment composition is a fundamental sentiment analysis problem. Previous
    work relied on manual rules and manually-created lexical resources such as negator
    lists, or learned a composition function from sentiment-annotated phrases or sentences.
    We propose a new approach for learning sentiment composition from a large, unlabeled
    corpus, which only requires a word-level sentiment lexicon for supervision. We
    automatically generate large sentiment lexicons of bigrams and unigrams, from
    which we induce a set of lexicons for a variety of sentiment composition processes.
    The effectiveness of our approach is confirmed through manual annotation, as well
    as sentiment classification experiments with both phrase-level and sentence-level
    benchmarks.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Orith
    full: Orith Toledo-Ronen
    id: orith-toledo-ronen
    last: Toledo-Ronen
  - first: Roy
    full: Roy Bar-Haim
    id: roy-bar-haim
    last: Bar-Haim
  - first: Alon
    full: Alon Halfon
    id: alon-halfon
    last: Halfon
  - first: Charles
    full: Charles Jochim
    id: charles-jochim
    last: Jochim
  - first: Amir
    full: Amir Menczel
    id: amir-menczel
    last: Menczel
  - first: Ranit
    full: Ranit Aharonov
    id: ranit-aharonov
    last: Aharonov
  - first: Noam
    full: Noam Slonim
    id: noam-slonim
    last: Slonim
  author_string: Orith Toledo-Ronen, Roy Bar-Haim, Alon Halfon, Charles Jochim, Amir
    Menczel, Ranit Aharonov, Noam Slonim
  bibkey: toledo-ronen-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2230'
  page_last: '2241'
  pages: "2230\u20132241"
  paper_id: '189'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1189.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1189.jpg
  title: Learning Sentiment Composition from Sentiment Lexicons
  title_html: Learning Sentiment Composition from Sentiment Lexicons
  url: https://www.aclweb.org/anthology/C18-1189
  year: '2018'
C18-1190:
  abstract: "This paper empirically studies the effects of representation choices\
    \ on neural sentiment analysis for Modern Hebrew, a morphologically rich language\
    \ (MRL) for which no sentiment analyzer currently exists. We study two dimensions\
    \ of representational choices: (i) the granularity of the input signal (token-based\
    \ vs. morpheme-based), and (ii) the level of encoding of vocabulary items (string-based\
    \ vs. character-based). We hypothesise that for MRLs, languages where multiple\
    \ meaning-bearing elements may be carried by a single space-delimited token, these\
    \ choices will have measurable effects on task perfromance, and that these effects\
    \ may vary for different architectural designs \u2014 fully-connected, convolutional\
    \ or recurrent. Specifically, we hypothesize that morpheme-based representations\
    \ will have advantages in terms of their generalization capacity and task accuracy,\
    \ due to their better OOV coverage. To empirically study these effects, we develop\
    \ a new sentiment analysis benchmark for Hebrew, based on 12K social media comments,\
    \ and provide two instances of these data: in token-based and morpheme-based settings.\
    \ Our experiments show that representation choices empirical effects vary with\
    \ architecture type. While fully-connected and convolutional networks slightly\
    \ prefer token-based settings, RNNs benefit from a morpheme-based representation,\
    \ in accord with the hypothesis that explicit morphological information may help\
    \ generalize. Our endeavour also delivers the first state-of-the-art broad-coverage\
    \ sentiment analyzer for Hebrew, with over 89% accuracy, alongside an established\
    \ benchmark to further study the effects of linguistic representation choices\
    \ on neural networks\u2019 task performance."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Adam
    full: Adam Amram
    id: adam-amram
    last: Amram
  - first: Anat
    full: Anat Ben David
    id: anat-ben-david
    last: Ben David
  - first: Reut
    full: Reut Tsarfaty
    id: reut-tsarfaty
    last: Tsarfaty
  author_string: Adam Amram, Anat Ben David, Reut Tsarfaty
  bibkey: amram-etal-2018-representations
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2242'
  page_last: '2252'
  pages: "2242\u20132252"
  paper_id: '190'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1190.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1190.jpg
  title: 'Representations and Architectures in Neural Sentiment Analysis for Morphologically
    Rich Languages: A Case Study from Modern Hebrew'
  title_html: 'Representations and Architectures in Neural Sentiment Analysis for
    Morphologically Rich Languages: A Case Study from Modern <span class="acl-fixed-case">H</span>ebrew'
  url: https://www.aclweb.org/anthology/C18-1190
  year: '2018'
C18-1191:
  abstract: This paper reports on a reimplementation of a system on detecting implicit
    positive meaning from negated statements. In the original regression experiment,
    different positive interpretations per negation are scored according to their
    likelihood. We convert the scores to classes and report our results on both the
    regression and classification tasks. We show that a baseline taking the mean score
    or most frequent class is hard to beat because of class imbalance in the dataset.
    Our error analysis indicates that an approach that takes the information structure
    into account (i.e. which information is new or contrastive) may be promising,
    which requires looking beyond the syntactic and semantic characteristics of negated
    statements.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Chantal
    full: Chantal van Son
    id: chantal-van-son
    last: van Son
  - first: Roser
    full: Roser Morante
    id: roser-morante
    last: Morante
  - first: Lora
    full: Lora Aroyo
    id: lora-aroyo
    last: Aroyo
  - first: Piek
    full: Piek Vossen
    id: piek-vossen
    last: Vossen
  author_string: Chantal van Son, Roser Morante, Lora Aroyo, Piek Vossen
  bibkey: van-son-etal-2018-scoring
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2253'
  page_last: '2264'
  pages: "2253\u20132264"
  paper_id: '191'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1191.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1191.jpg
  title: 'Scoring and Classifying Implicit Positive Interpretations: A Challenge of
    Class Imbalance'
  title_html: 'Scoring and Classifying Implicit Positive Interpretations: A Challenge
    of Class Imbalance'
  url: https://www.aclweb.org/anthology/C18-1191
  year: '2018'
C18-1192:
  abstract: The state-of-the-art methods for relation classification are primarily
    based on deep neural net- works. This kind of supervised learning method suffers
    from not only limited training data, but also the large number of low-frequency
    relations in specific domains. In this paper, we propose the task of exploratory
    relation classification for domain knowledge harvesting. The goal is to learn
    a classifier on pre-defined relations and discover new relations expressed in
    texts. A dynamically structured neural network is introduced to classify entity
    pairs to a continuously expanded relation set. We further propose the similarity
    sensitive Chinese restaurant process to discover new relations. Experiments conducted
    on a large corpus show the effectiveness of our neural network, while new relations
    are discovered with high precision and recall.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yan
    full: Yan Fan
    id: yan-fan
    last: Fan
  - first: Chengyu
    full: Chengyu Wang
    id: chengyu-wang
    last: Wang
  - first: Xiaofeng
    full: Xiaofeng He
    id: xiaofeng-he
    last: He
  author_string: Yan Fan, Chengyu Wang, Xiaofeng He
  bibkey: fan-etal-2018-exploratory
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2265'
  page_last: '2276'
  pages: "2265\u20132276"
  paper_id: '192'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1192.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1192.jpg
  title: Exploratory Neural Relation Classification for Domain Knowledge Acquisition
  title_html: Exploratory Neural Relation Classification for Domain Knowledge Acquisition
  url: https://www.aclweb.org/anthology/C18-1192
  year: '2018'
C18-1193:
  abstract: Finding names of people killed by police has become increasingly important
    as police shootings get more and more public attention (police killing detection).
    Unfortunately, there has been not much work in the literature addressing this
    problem. The early work in this field (Keith etal., 2017) proposed a distant supervision
    framework based on Expectation Maximization (EM) to deal with the multiple appearances
    of the names in documents. However, such EM-based framework cannot take full advantages
    of deep learning models, necessitating the use of handdesigned features to improve
    the detection performance. In this work, we present a novel deep learning method
    to solve the problem of police killing recognition. The proposed method relies
    on hierarchical LSTMs to model the multiple sentences that contain the person
    names of interests, and introduce supervised attention mechanisms based on semantical
    word lists and dependency trees to upweight the important contextual words. Our
    experiments demonstrate the benefits of the proposed model and yield the state-of-the-art
    performance for police killing detection.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Minh
    full: Minh Nguyen
    id: minh-nguyen
    last: Nguyen
  - first: Thien Huu
    full: Thien Huu Nguyen
    id: thien-huu-nguyen
    last: Nguyen
  author_string: Minh Nguyen, Thien Huu Nguyen
  bibkey: nguyen-nguyen-2018-killed
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2277'
  page_last: '2287'
  pages: "2277\u20132287"
  paper_id: '193'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1193.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1193.jpg
  title: 'Who is Killed by Police: Introducing Supervised Attention for Hierarchical
    LSTMs'
  title_html: 'Who is Killed by Police: Introducing Supervised Attention for Hierarchical
    <span class="acl-fixed-case">LSTM</span>s'
  url: https://www.aclweb.org/anthology/C18-1193
  year: '2018'
C18-1194:
  abstract: We develop CALM, a coordination analyzer that improves upon the conjuncts
    identified from dependency parses. It uses a language model based scoring and
    several linguistic constraints to search over hierarchical conjunct boundaries
    (for nested coordination). By splitting a conjunctive sentence around these conjuncts,
    CALM outputs several simple sentences. We demonstrate the value of our coordination
    analyzer in the end task of Open Information Extraction (Open IE). State-of-the-art
    Open IE systems lose substantial yield due to ineffective processing of conjunctive
    sentences. Our Open IE system, CALMIE, performs extraction over the simple sentences
    identified by CALM to obtain up to 1.8x yield with a moderate increase in precision
    compared to extractions from original sentences.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Swarnadeep
    full: Swarnadeep Saha
    id: swarnadeep-saha
    last: Saha
  - first: ''
    full: Mausam
    id: mausam
    last: Mausam
  author_string: Swarnadeep Saha, Mausam
  bibkey: saha-mausam-2018-open
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2288'
  page_last: '2299'
  pages: "2288\u20132299"
  paper_id: '194'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1194.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1194.jpg
  title: Open Information Extraction from Conjunctive Sentences
  title_html: Open Information Extraction from Conjunctive Sentences
  url: https://www.aclweb.org/anthology/C18-1194
  year: '2018'
C18-1195:
  abstract: We present an Open Information Extraction (IE) approach that uses a two-layered
    transformation stage consisting of a clausal disembedding layer and a phrasal
    disembedding layer, together with rhetorical relation identification. In that
    way, we convert sentences that present a complex linguistic structure into simplified,
    syntactically sound sentences, from which we can extract propositions that are
    represented in a two-layered hierarchy in the form of core relational tuples and
    accompanying contextual information which are semantically linked via rhetorical
    relations. In a comparative evaluation, we demonstrate that our reference implementation
    Graphene outperforms state-of-the-art Open IE systems in the construction of correct
    n-ary predicate-argument structures. Moreover, we show that existing Open IE approaches
    can benefit from the transformation process of our framework.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Matthias
    full: Matthias Cetto
    id: matthias-cetto
    last: Cetto
  - first: Christina
    full: Christina Niklaus
    id: christina-niklaus
    last: Niklaus
  - first: "Andr\xE9"
    full: "Andr\xE9 Freitas"
    id: andre-freitas
    last: Freitas
  - first: Siegfried
    full: Siegfried Handschuh
    id: siegfried-handschuh
    last: Handschuh
  author_string: "Matthias Cetto, Christina Niklaus, Andr\xE9 Freitas, Siegfried Handschuh"
  bibkey: cetto-etal-2018-graphene
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2300'
  page_last: '2311'
  pages: "2300\u20132311"
  paper_id: '195'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1195.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1195.jpg
  title: 'Graphene: Semantically-Linked Propositions in Open Information Extraction'
  title_html: '<span class="acl-fixed-case">G</span>raphene: Semantically-Linked Propositions
    in Open Information Extraction'
  url: https://www.aclweb.org/anthology/C18-1195
  year: '2018'
C18-1196:
  abstract: 'Several semi-supervised representation learning methods have been proposed
    recently that mitigate the drawbacks of traditional bootstrapping: they reduce
    the amount of semantic drift introduced by iterative approaches through one-shot
    learning; others address the sparsity of data through the learning of custom,
    dense representation for the information modeled. In this work, we are the first
    to adapt three of these methods, most of which have been originally proposed for
    image processing, to an information extraction task, specifically, named entity
    classification. Further, we perform a rigorous comparative analysis on two distinct
    datasets. Our analysis yields several important observations. First, all representation
    learning methods outperform state-of-the-art semi-supervised methods that do not
    rely on representation learning. To the best of our knowledge, we report the latest
    state-of-the-art results on the semi-supervised named entity classification task.
    Second, one-shot learning methods clearly outperform iterative representation
    learning approaches. Lastly, one of the best performers relies on the mean teacher
    framework (Tarvainen and Valpola, 2017), a simple teacher/student approach that
    is independent of the underlying task-specific model.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Ajay
    full: Ajay Nagesh
    id: ajay-nagesh
    last: Nagesh
  - first: Mihai
    full: Mihai Surdeanu
    id: mihai-surdeanu
    last: Surdeanu
  author_string: Ajay Nagesh, Mihai Surdeanu
  bibkey: nagesh-surdeanu-2018-exploration
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2312'
  page_last: '2324'
  pages: "2312\u20132324"
  paper_id: '196'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1196.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1196.jpg
  title: An Exploration of Three Lightly-supervised Representation Learning Approaches
    for Named Entity Classification
  title_html: An Exploration of Three Lightly-supervised Representation Learning Approaches
    for Named Entity Classification
  url: https://www.aclweb.org/anthology/C18-1196
  year: '2018'
C18-1197:
  abstract: This survey discusses how recent developments in multimodal processing
    facilitate conceptual grounding of language. We categorize the information flow
    in multimodal processing with respect to cognitive models of human information
    processing and analyze different methods for combining multimodal representations.
    Based on this methodological inventory, we discuss the benefit of multimodal grounding
    for a variety of language processing tasks and the challenges that arise. We particularly
    focus on multimodal grounding of verbs which play a crucial role for the compositional
    power of language.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Lisa
    full: Lisa Beinborn
    id: lisa-beinborn
    last: Beinborn
  - first: Teresa
    full: Teresa Botschen
    id: teresa-botschen
    last: Botschen
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Lisa Beinborn, Teresa Botschen, Iryna Gurevych
  bibkey: beinborn-etal-2018-multimodal
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2325'
  page_last: '2339'
  pages: "2325\u20132339"
  paper_id: '197'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1197.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1197.jpg
  title: Multimodal Grounding for Language Processing
  title_html: Multimodal Grounding for Language Processing
  url: https://www.aclweb.org/anthology/C18-1197
  year: '2018'
C18-1198:
  abstract: "Natural language inference (NLI) is the task of determining if a natural\
    \ language hypothesis can be inferred from a given premise in a justifiable manner.\
    \ NLI was proposed as a benchmark task for natural language understanding. Existing\
    \ models perform well at standard datasets for NLI, achieving impressive results\
    \ across different genres of text. However, the extent to which these models understand\
    \ the semantic content of sentences is unclear. In this work, we propose an evaluation\
    \ methodology consisting of automatically constructed \u201Cstress tests\u201D\
    \ that allow us to examine whether systems have the ability to make real inferential\
    \ decisions. Our evaluation of six sentence-encoder models on these stress tests\
    \ reveals strengths and weaknesses of these models with respect to challenging\
    \ linguistic phenomena, and suggests important directions for future work in this\
    \ area."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Aakanksha
    full: Aakanksha Naik
    id: aakanksha-naik
    last: Naik
  - first: Abhilasha
    full: Abhilasha Ravichander
    id: abhilasha-ravichander
    last: Ravichander
  - first: Norman
    full: Norman Sadeh
    id: norman-sadeh
    last: Sadeh
  - first: Carolyn
    full: Carolyn Rose
    id: carolyn-rose
    last: Rose
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  author_string: Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose,
    Graham Neubig
  bibkey: naik-etal-2018-stress
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2340'
  page_last: '2353'
  pages: "2340\u20132353"
  paper_id: '198'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1198.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1198.jpg
  title: Stress Test Evaluation for Natural Language Inference
  title_html: Stress Test Evaluation for Natural Language Inference
  url: https://www.aclweb.org/anthology/C18-1198
  year: '2018'
C18-1199:
  abstract: "Capturing semantic relations between sentences, such as entailment, is\
    \ a long-standing challenge for computational semantics. Logic-based models analyse\
    \ entailment in terms of possible worlds (interpretations, or situations) where\
    \ a premise P entails a hypothesis H iff in all worlds where P is true, H is also\
    \ true. Statistical models view this relationship probabilistically, addressing\
    \ it in terms of whether a human would likely infer H from P. In this paper, we\
    \ wish to bridge these two perspectives, by arguing for a visually-grounded version\
    \ of the Textual Entailment task. Specifically, we ask whether models can perform\
    \ better if, in addition to P and H, there is also an image (corresponding to\
    \ the relevant \u201Cworld\u201D or \u201Csituation\u201D). We use a multimodal\
    \ version of the SNLI dataset (Bowman et al., 2015) and we compare \u201Cblind\u201D\
    \ and visually-augmented models of textual entailment. We show that visual information\
    \ is beneficial, but we also conduct an in-depth error analysis that reveals that\
    \ current multimodal models are not performing \u201Cgrounding\u201D in an optimal\
    \ fashion."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Hoa Trong
    full: Hoa Trong Vu
    id: hoa-trong-vu
    last: Vu
  - first: Claudio
    full: Claudio Greco
    id: claudio-greco
    last: Greco
  - first: Aliia
    full: Aliia Erofeeva
    id: aliia-erofeeva
    last: Erofeeva
  - first: Somayeh
    full: Somayeh Jafaritazehjan
    id: somayeh-jafaritazehjani
    last: Jafaritazehjan
  - first: Guido
    full: Guido Linders
    id: guido-linders
    last: Linders
  - first: Marc
    full: Marc Tanti
    id: marc-tanti
    last: Tanti
  - first: Alberto
    full: Alberto Testoni
    id: alberto-testoni
    last: Testoni
  - first: Raffaella
    full: Raffaella Bernardi
    id: raffaella-bernardi
    last: Bernardi
  - first: Albert
    full: Albert Gatt
    id: albert-gatt
    last: Gatt
  author_string: Hoa Trong Vu, Claudio Greco, Aliia Erofeeva, Somayeh Jafaritazehjan,
    Guido Linders, Marc Tanti, Alberto Testoni, Raffaella Bernardi, Albert Gatt
  bibkey: vu-etal-2018-grounded
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2354'
  page_last: '2368'
  pages: "2354\u20132368"
  paper_id: '199'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1199.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1199.jpg
  title: Grounded Textual Entailment
  title_html: Grounded Textual Entailment
  url: https://www.aclweb.org/anthology/C18-1199
  year: '2018'
C18-1200:
  abstract: 'Large scale knowledge graphs (KGs) such as Freebase are generally incomplete.
    Reasoning over multi-hop (mh) KG paths is thus an important capability that is
    needed for question answering or other NLP tasks that require knowledge about
    the world. mh-KG reasoning includes diverse scenarios, e.g., given a head entity
    and a relation path, predict the tail entity; or given two entities connected
    by some relation paths, predict the unknown relation between them. We present
    ROPs, recurrent one-hop predictors, that predict entities at each step of mh-KB
    paths by using recurrent neural networks and vector representations of entities
    and relations, with two benefits: (i) modeling mh-paths of arbitrary lengths while
    updating the entity and relation representations by the training signal at each
    step; (ii) handling different types of mh-KG reasoning in a unified framework.
    Our models show state-of-the-art for two important multi-hop KG reasoning tasks:
    Knowledge Base Completion and Path Query Answering.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Wenpeng
    full: Wenpeng Yin
    id: wenpeng-yin
    last: Yin
  - first: Yadollah
    full: Yadollah Yaghoobzadeh
    id: yadollah-yaghoobzadeh
    last: Yaghoobzadeh
  - first: Hinrich
    full: "Hinrich Sch\xFCtze"
    id: hinrich-schutze
    last: "Sch\xFCtze"
  author_string: "Wenpeng Yin, Yadollah Yaghoobzadeh, Hinrich Sch\xFCtze"
  bibkey: yin-etal-2018-recurrent
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2369'
  page_last: '2378'
  pages: "2369\u20132378"
  paper_id: '200'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1200.pdf
  publisher: Association for Computational Linguistics
  revision:
  - id: '1'
    url: https://www.aclweb.org/anthology/C18-1200v1.pdf
    value: C18-1200v1
  - explanation: A footnote is added in the page 8 to show that the created 'EnhancedKGP'
      dataset has a potential leak of test information to training set, due to an
      inadvertent data preprocessing.
    id: '2'
    url: https://www.aclweb.org/anthology/C18-1200v2.pdf
    value: C18-1200v2
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1200.jpg
  title: Recurrent One-Hop Predictions for Reasoning over Knowledge Graphs
  title_html: Recurrent One-Hop Predictions for Reasoning over Knowledge Graphs
  url: https://www.aclweb.org/anthology/C18-1200
  year: '2018'
C18-1201:
  abstract: We examine the utility of linguistic content and vocal characteristics
    for multimodal deep learning in human spoken language understanding. We present
    a deep multimodal network with both feature attention and modality attention to
    classify utterance-level speech data. The proposed hybrid attention architecture
    helps the system focus on learning informative representations for both modality-specific
    feature extraction and model fusion. The experimental results show that our system
    achieves state-of-the-art or competitive results on three published multimodal
    datasets. We also demonstrated the effectiveness and generalization of our system
    on a medical speech dataset from an actual trauma scenario. Furthermore, we provided
    a detailed comparison and analysis of traditional approaches and deep learning
    methods on both feature extraction and fusion.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yue
    full: Yue Gu
    id: yue-gu
    last: Gu
  - first: Kangning
    full: Kangning Yang
    id: kangning-yang
    last: Yang
  - first: Shiyu
    full: Shiyu Fu
    id: shiyu-fu
    last: Fu
  - first: Shuhong
    full: Shuhong Chen
    id: shuhong-chen
    last: Chen
  - first: Xinyu
    full: Xinyu Li
    id: xinyu-li
    last: Li
  - first: Ivan
    full: Ivan Marsic
    id: ivan-marsic
    last: Marsic
  author_string: Yue Gu, Kangning Yang, Shiyu Fu, Shuhong Chen, Xinyu Li, Ivan Marsic
  bibkey: gu-etal-2018-hybrid
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2379'
  page_last: '2390'
  pages: "2379\u20132390"
  paper_id: '201'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1201.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1201.jpg
  title: Hybrid Attention based Multimodal Network for Spoken Language Classification
  title_html: Hybrid Attention based Multimodal Network for Spoken Language Classification
  url: https://www.aclweb.org/anthology/C18-1201
  year: '2018'
C18-1202:
  abstract: "This paper explores the influence of spelling errors on lexical variation\
    \ measures. Lexical richness measures such as Type-Token Ration (TTR) and Yule\u2019\
    s K are often used for learner English analysis and assessment. When applied to\
    \ learner English, however, they can be unreliable because of the spelling errors\
    \ appearing in it. Namely, they are, directly or indirectly, based on the counts\
    \ of distinct word types, and spelling errors undesirably increase the number\
    \ of distinct words. This paper introduces and examines the hypothesis that lexical\
    \ richness measures become unstable in learner English because of spelling errors.\
    \ Specifically, it tests the hypothesis on English learner corpora of three groups\
    \ (middle school, high school, and college students). To be precise, it estimates\
    \ the difference in TTR and Yule\u2019s K caused by spelling errors, by calculating\
    \ their values before and after spelling errors are manually corrected. Furthermore,\
    \ it examines the results theoretically and empirically to deepen the understanding\
    \ of the influence of spelling errors on them."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Ryo
    full: Ryo Nagata
    id: ryo-nagata
    last: Nagata
  - first: Taisei
    full: Taisei Sato
    id: taisei-sato
    last: Sato
  - first: Hiroya
    full: Hiroya Takamura
    id: hiroya-takamura
    last: Takamura
  author_string: Ryo Nagata, Taisei Sato, Hiroya Takamura
  bibkey: nagata-etal-2018-exploring
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2391'
  page_last: '2398'
  pages: "2391\u20132398"
  paper_id: '202'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1202.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1202.jpg
  title: Exploring the Influence of Spelling Errors on Lexical Variation Measures
  title_html: Exploring the Influence of Spelling Errors on Lexical Variation Measures
  url: https://www.aclweb.org/anthology/C18-1202
  year: '2018'
C18-1203:
  abstract: Stance detection aims to assign a stance label (for or against) to a post
    toward a specific target. Recently, there is a growing interest in using neural
    models to detect stance of documents. Most of these works model the sequence of
    words to learn document representation. However, much linguistic information,
    such as polarity and arguments of the document, is correlated with the stance
    of the document, and can inspire us to explore the stance. Hence, we present a
    neural model to fully employ various linguistic information to construct the document
    representation. In addition, since the influences of different linguistic information
    are different, we propose a hierarchical attention network to weigh the importance
    of various linguistic information, and learn the mutual attention between the
    document and the linguistic information. The experimental results on two datasets
    demonstrate the effectiveness of the proposed hierarchical attention neural model.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Qingying
    full: Qingying Sun
    id: qingying-sun
    last: Sun
  - first: Zhongqing
    full: Zhongqing Wang
    id: zhongqing-wang
    last: Wang
  - first: Qiaoming
    full: Qiaoming Zhu
    id: qiaoming-zhu
    last: Zhu
  - first: Guodong
    full: Guodong Zhou
    id: guodong-zhou
    last: Zhou
  author_string: Qingying Sun, Zhongqing Wang, Qiaoming Zhu, Guodong Zhou
  bibkey: sun-etal-2018-stance
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2399'
  page_last: '2409'
  pages: "2399\u20132409"
  paper_id: '203'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1203.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1203.jpg
  title: Stance Detection with Hierarchical Attention Network
  title_html: Stance Detection with Hierarchical Attention Network
  url: https://www.aclweb.org/anthology/C18-1203
  year: '2018'
C18-1204:
  abstract: With more and more people around the world learning Chinese as a second
    language, the need of Chinese error correction tools is increasing. In the HSK
    dynamic composition corpus, word usage error (WUE) is the most common error type.
    In this paper, we build a neural network model that considers both target erroneous
    token and context to generate a correction vector and compare it against a candidate
    vocabulary to propose suitable corrections. To deal with potential alternative
    corrections, the top five proposed candidates are judged by native Chinese speakers.
    For more than 91% of the cases, our system can propose at least one acceptable
    correction within a list of five candidates. To the best of our knowledge, this
    is the first research addressing general-type Chinese WUE correction. Our system
    can help non-native Chinese learners revise their sentences by themselves.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yow-Ting
    full: Yow-Ting Shiue
    id: yow-ting-shiue
    last: Shiue
  - first: Hen-Hsen
    full: Hen-Hsen Huang
    id: hen-hsen-huang
    last: Huang
  - first: Hsin-Hsi
    full: Hsin-Hsi Chen
    id: hsin-hsi-chen
    last: Chen
  author_string: Yow-Ting Shiue, Hen-Hsen Huang, Hsin-Hsi Chen
  bibkey: shiue-etal-2018-correcting
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2410'
  page_last: '2422'
  pages: "2410\u20132422"
  paper_id: '204'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1204.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1204.jpg
  title: Correcting Chinese Word Usage Errors for Learning Chinese as a Second Language
  title_html: Correcting <span class="acl-fixed-case">C</span>hinese Word Usage Errors
    for Learning <span class="acl-fixed-case">C</span>hinese as a Second Language
  url: https://www.aclweb.org/anthology/C18-1204
  year: '2018'
C18-1205:
  abstract: "Knowledge graphs are a versatile framework to encode richly structured\
    \ data relationships, but it can be challenging to combine these graphs with unstructured\
    \ data. Methods for retrofitting pre-trained entity representations to the structure\
    \ of a knowledge graph typically assume that entities are embedded in a connected\
    \ space and that relations imply similarity. However, useful knowledge graphs\
    \ often contain diverse entities and relations (with potentially disjoint underlying\
    \ corpora) which do not accord with these assumptions. To overcome these limitations,\
    \ we present Functional Retrofitting, a framework that generalizes current retrofitting\
    \ methods by explicitly modeling pairwise relations. Our framework can directly\
    \ incorporate a variety of pairwise penalty functions previously developed for\
    \ knowledge graph completion. Further, it allows users to encode, learn, and extract\
    \ information about relation semantics. We present both linear and neural instantiations\
    \ of the framework. Functional Retrofitting significantly outperforms existing\
    \ retrofitting methods on complex knowledge graphs and loses no accuracy on simpler\
    \ graphs (in which relations do imply similarity). Finally, we demonstrate the\
    \ utility of the framework by predicting new drug\u2013disease treatment pairs\
    \ in a large, complex health knowledge graph."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Ben
    full: Ben Lengerich
    id: ben-lengerich
    last: Lengerich
  - first: Andrew
    full: Andrew Maas
    id: andrew-maas
    last: Maas
  - first: Christopher
    full: Christopher Potts
    id: christopher-potts
    last: Potts
  author_string: Ben Lengerich, Andrew Maas, Christopher Potts
  bibkey: lengerich-etal-2018-retrofitting
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2423'
  page_last: '2436'
  pages: "2423\u20132436"
  paper_id: '205'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1205.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1205.jpg
  title: Retrofitting Distributional Embeddings to Knowledge Graphs with Functional
    Relations
  title_html: Retrofitting Distributional Embeddings to Knowledge Graphs with Functional
    Relations
  url: https://www.aclweb.org/anthology/C18-1205
  year: '2018'
C18-1206:
  abstract: Despite the success of existing works on single-turn conversation generation,
    taking the coherence in consideration, human conversing is actually a context-sensitive
    process. Inspired by the existing studies, this paper proposed the static and
    dynamic attention based approaches for context-sensitive generation of open-domain
    conversational responses. Experimental results on two public datasets show that
    the proposed static attention based approach outperforms all the baselines on
    automatic and human evaluation.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Weinan
    full: Weinan Zhang
    id: weinan-zhang
    last: Zhang
  - first: Yiming
    full: Yiming Cui
    id: yiming-cui
    last: Cui
  - first: Yifa
    full: Yifa Wang
    id: yifa-wang
    last: Wang
  - first: Qingfu
    full: Qingfu Zhu
    id: qingfu-zhu
    last: Zhu
  - first: Lingzhi
    full: Lingzhi Li
    id: lingzhi-li
    last: Li
  - first: Lianqiang
    full: Lianqiang Zhou
    id: lianqiang-zhou
    last: Zhou
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  author_string: Weinan Zhang, Yiming Cui, Yifa Wang, Qingfu Zhu, Lingzhi Li, Lianqiang
    Zhou, Ting Liu
  bibkey: zhang-etal-2018-context
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2437'
  page_last: '2447'
  pages: "2437\u20132447"
  paper_id: '206'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1206.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1206.jpg
  title: Context-Sensitive Generation of Open-Domain Conversational Responses
  title_html: Context-Sensitive Generation of Open-Domain Conversational Responses
  url: https://www.aclweb.org/anthology/C18-1206
  year: '2018'
C18-1207:
  abstract: In this paper, we first utilize the word embedding that focuses on sub-word
    units to the Mongolian Phrase Break (PB) prediction task by using Long-Short-Term-Memory
    (LSTM) model. Mongolian is an agglutinative language. Each root can be followed
    by several suffixes to form probably millions of words, but the existing Mongolian
    corpus is not enough to build a robust entire word embedding, thus it suffers
    a serious data sparse problem and brings a great difficulty for Mongolian PB prediction.
    To solve this problem, we look at sub-word units in Mongolian word, and encode
    their information to a meaningful representation, then fed it to LSTM to decode
    the best corresponding PB label. Experimental results show that the proposed model
    significantly outperforms traditional CRF model using manually features and obtains
    7.49% F-Measure gain.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Rui
    full: Rui Liu
    id: rui-liu
    last: Liu
  - first: Feilong
    full: Feilong Bao
    id: feilong-bao
    last: Bao
  - first: Guanglai
    full: Guanglai Gao
    id: guanglai-gao
    last: Gao
  - first: Hui
    full: Hui Zhang
    id: hui-zhang
    last: Zhang
  - first: Yonghe
    full: Yonghe Wang
    id: yonghe-wang
    last: Wang
  author_string: Rui Liu, Feilong Bao, Guanglai Gao, Hui Zhang, Yonghe Wang
  bibkey: liu-etal-2018-lstm
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2448'
  page_last: '2455'
  pages: "2448\u20132455"
  paper_id: '207'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1207.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1207.jpg
  title: A LSTM Approach with Sub-Word Embeddings for Mongolian Phrase Break Prediction
  title_html: A <span class="acl-fixed-case">LSTM</span> Approach with Sub-Word Embeddings
    for <span class="acl-fixed-case">M</span>ongolian Phrase Break Prediction
  url: https://www.aclweb.org/anthology/C18-1207
  year: '2018'
C18-1208:
  abstract: "This paper describes CzEngClass, a bilingual lexical resource being built\
    \ to investigate verbal synonymy in bilingual context and to relate semantic roles\
    \ common to one synonym class to verb arguments (verb valency). In addition, the\
    \ resource is linked to existing resources with the same of a similar aim: English\
    \ and Czech WordNet, FrameNet, PropBank, VerbNet (SemLink), and valency lexicons\
    \ for Czech and English (PDT-Vallex, Vallex, and EngVallex). There are several\
    \ goals of this work and resource: (a) to provide gold standard data for automatic\
    \ experiments in the future (such as automatic discovery of synonym classes, word\
    \ sense disambiguation, assignment of classes to occurrences of verbs in text,\
    \ coreferential linking of verb and event arguments in text, etc.), (b) to build\
    \ a core (bilingual) lexicon linked to existing resources, for comparative studies\
    \ and possibly for training automatic tools, and (c) to enrich the annotation\
    \ of a parallel treebank, the Prague Czech English Dependency Treebank, which\
    \ so far contained valency annotation but has not linked synonymous senses of\
    \ verbs together. The method used for extracting the synonym classes is a semi-automatic\
    \ process with a substantial amount of manual work during filtering, role assignment\
    \ to classes and individual Class members\u2019 arguments, and linking to the\
    \ external lexical resources. We present the first version with 200 classes (about\
    \ 1800 verbs) and evaluate interannotator agreement using several metrics."
  address: Santa Fe, New Mexico, USA
  author:
  - first: "Zde\u0148ka"
    full: "Zde\u0148ka Ure\u0161ov\xE1"
    id: zdenka-uresova
    last: "Ure\u0161ov\xE1"
  - first: Eva
    full: "Eva Fu\u010D\xEDkov\xE1"
    id: eva-fucikova
    last: "Fu\u010D\xEDkov\xE1"
  - first: Eva
    full: "Eva Haji\u010Dov\xE1"
    id: eva-hajicova
    last: "Haji\u010Dov\xE1"
  - first: Jan
    full: "Jan Haji\u010D"
    id: jan-hajic
    last: "Haji\u010D"
  author_string: "Zde\u0148ka Ure\u0161ov\xE1, Eva Fu\u010D\xEDkov\xE1, Eva Haji\u010D\
    ov\xE1, Jan Haji\u010D"
  bibkey: uresova-etal-2018-synonymy
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2456'
  page_last: '2469'
  pages: "2456\u20132469"
  paper_id: '208'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1208.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1208.jpg
  title: 'Synonymy in Bilingual Context: The CzEngClass Lexicon'
  title_html: 'Synonymy in Bilingual Context: The <span class="acl-fixed-case">C</span>z<span
    class="acl-fixed-case">E</span>ng<span class="acl-fixed-case">C</span>lass Lexicon'
  url: https://www.aclweb.org/anthology/C18-1208
  year: '2018'
C18-1209:
  abstract: This paper proposes a simple CNN model for creating general-purpose sentence
    embeddings that can transfer easily across domains and can also act as effective
    initialization for downstream tasks. Recently, averaging the embeddings of words
    in a sentence has proven to be a surprisingly successful and efficient way of
    obtaining sentence embeddings. However, these models represent a sentence, only
    in terms of features of words or uni-grams in it. In contrast, our model (CSE)
    utilizes both features of words and n-grams to encode sentences, which is actually
    a generalization of these bag-of-words models. The extensive experiments demonstrate
    that CSE performs better than average models in transfer learning setting and
    exceeds the state of the art in supervised learning setting by initializing the
    parameters with the pre-trained sentence embeddings.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Xiaoqi
    full: Xiaoqi Jiao
    id: xiaoqi-jiao
    last: Jiao
  - first: Fang
    full: Fang Wang
    id: fang-wang
    last: Wang
  - first: Dan
    full: Dan Feng
    id: dan-feng
    last: Feng
  author_string: Xiaoqi Jiao, Fang Wang, Dan Feng
  bibkey: jiao-etal-2018-convolutional
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2470'
  page_last: '2481'
  pages: "2470\u20132481"
  paper_id: '209'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1209.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1209.jpg
  title: Convolutional Neural Network for Universal Sentence Embeddings
  title_html: Convolutional Neural Network for Universal Sentence Embeddings
  url: https://www.aclweb.org/anthology/C18-1209
  year: '2018'
C18-1210:
  abstract: Due to the fact that Korean is a highly agglutinative, character-rich
    language, previous work on Korean morphological analysis typically employs the
    use of sub-character features known as graphemes or otherwise utilizes comprehensive
    prior linguistic knowledge (i.e., a dictionary of known morphological transformation
    forms, or actions). These models have been created with the assumption that character-level,
    dictionary-less morphological analysis was intractable due to the number of actions
    required. We present, in this study, a multi-stage action-based model that can
    perform morphological transformation and part-of-speech tagging using arbitrary
    units of input and apply it to the case of character-level Korean morphological
    analysis. Among models that do not employ prior linguistic knowledge, we achieve
    state-of-the-art word and sentence-level tagging accuracy with the Sejong Korean
    corpus using our proposed data-driven Bi-LSTM model.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Andrew
    full: Andrew Matteson
    id: andrew-matteson
    last: Matteson
  - first: Chanhee
    full: Chanhee Lee
    id: chanhee-lee
    last: Lee
  - first: Youngbum
    full: Youngbum Kim
    id: youngbum-kim
    last: Kim
  - first: Heuiseok
    full: Heuiseok Lim
    id: heui-seok-lim
    last: Lim
  author_string: Andrew Matteson, Chanhee Lee, Youngbum Kim, Heuiseok Lim
  bibkey: matteson-etal-2018-rich
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2482'
  page_last: '2492'
  pages: "2482\u20132492"
  paper_id: '210'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1210.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1210.jpg
  title: Rich Character-Level Information for Korean Morphological Analysis and Part-of-Speech
    Tagging
  title_html: Rich Character-Level Information for <span class="acl-fixed-case">K</span>orean
    Morphological Analysis and Part-of-Speech Tagging
  url: https://www.aclweb.org/anthology/C18-1210
  year: '2018'
C18-1211:
  abstract: Representing the semantic relations that exist between two given words
    (or entities) is an important first step in a wide-range of NLP applications such
    as analogical reasoning, knowledge base completion and relational information
    retrieval. A simple, yet surprisingly accurate method for representing a relation
    between two words is to compute the vector offset (PairDiff) between their corresponding
    word embeddings. Despite the empirical success, it remains unclear as to whether
    PairDiff is the best operator for obtaining a relational representation from word
    embeddings. We conduct a theoretical analysis of generalised bilinear operators
    that can be used to measure the l2 relational distance between two word-pairs.
    We show that, if the word embed- dings are standardised and uncorrelated, such
    an operator will be independent of bilinear terms, and can be simplified to a
    linear form, where PairDiff is a special case. For numerous word embedding types,
    we empirically verify the uncorrelation assumption, demonstrating the general
    applicability of our theoretical result. Moreover, we experimentally discover
    PairDiff from the bilinear relational compositional operator on several benchmark
    analogy datasets.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Huda
    full: Huda Hakami
    id: huda-hakami
    last: Hakami
  - first: Kohei
    full: Kohei Hayashi
    id: kohei-hayashi
    last: Hayashi
  - first: Danushka
    full: Danushka Bollegala
    id: danushka-bollegala
    last: Bollegala
  author_string: Huda Hakami, Kohei Hayashi, Danushka Bollegala
  bibkey: hakami-etal-2018-pairdiff
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2493'
  page_last: '2504'
  pages: "2493\u20132504"
  paper_id: '211'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1211.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1211.jpg
  title: Why does PairDiff work? - A Mathematical Analysis of Bilinear Relational
    Compositional Operators for Analogy Detection
  title_html: Why does <span class="acl-fixed-case">P</span>air<span class="acl-fixed-case">D</span>iff
    work? - A Mathematical Analysis of Bilinear Relational Compositional Operators
    for Analogy Detection
  url: https://www.aclweb.org/anthology/C18-1211
  year: '2018'
C18-1212:
  abstract: Detecting changes within an unfolding event in real time from news articles
    or social media enables to react promptly to serious issues in public safety,
    public health or natural disasters. In this study, we use on-line Latent Dirichlet
    Allocation (LDA) to model shifts in topics, and apply on-line change point detection
    (CPD) algorithms to detect when significant changes happen. We describe an on-line
    Bayesian change point detection algorithm that we use to detect topic changes
    from on-line LDA output. Extensive experiments on social media data and news articles
    show the benefits of on-line LDA versus standard LDA, and of on-line change point
    detection compared to off-line algorithms. This yields F-scores up to 52% on the
    detection of significant real-life changes from these document streams.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yunli
    full: Yunli Wang
    id: yunli-wang
    last: Wang
  - first: Cyril
    full: Cyril Goutte
    id: cyril-goutte
    last: Goutte
  author_string: Yunli Wang, Cyril Goutte
  bibkey: wang-goutte-2018-real
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2505'
  page_last: '2515'
  pages: "2505\u20132515"
  paper_id: '212'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1212.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1212.jpg
  title: Real-time Change Point Detection using On-line Topic Models
  title_html: Real-time Change Point Detection using On-line Topic Models
  url: https://www.aclweb.org/anthology/C18-1212
  year: '2018'
C18-1213:
  abstract: "In this paper we use methods for creating a large lexicon of verbal polarity\
    \ shifters and apply them to German. Polarity shifters are content words that\
    \ can move the polarity of a phrase towards its opposite, such as the verb \u201C\
    abandon\u201D in \u201Cabandon all hope\u201D. This is similar to how negation\
    \ words like \u201Cnot\u201D can influence polarity. Both shifters and negation\
    \ are required for high precision sentiment analysis. Lists of negation words\
    \ are available for many languages, but the only language for which a sizable\
    \ lexicon of verbal polarity shifters exists is English. This lexicon was created\
    \ by bootstrapping a sample of annotated verbs with a supervised classifier that\
    \ uses a set of data- and resource-driven features. We reproduce and adapt this\
    \ approach to create a German lexicon of verbal polarity shifters. Thereby, we\
    \ confirm that the approach works for multiple languages. We further improve classification\
    \ by leveraging cross-lingual information from the English shifter lexicon. Using\
    \ this improved approach, we bootstrap a large number of German verbal polarity\
    \ shifters, reducing the annotation effort drastically. The resulting German lexicon\
    \ of verbal polarity shifters is made publicly available."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Marc
    full: Marc Schulder
    id: marc-schulder
    last: Schulder
  - first: Michael
    full: Michael Wiegand
    id: michael-wiegand
    last: Wiegand
  - first: Josef
    full: Josef Ruppenhofer
    id: josef-ruppenhofer
    last: Ruppenhofer
  author_string: Marc Schulder, Michael Wiegand, Josef Ruppenhofer
  bibkey: schulder-etal-2018-automatically
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2516'
  page_last: '2528'
  pages: "2516\u20132528"
  paper_id: '213'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1213.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1213.jpg
  title: 'Automatically Creating a Lexicon of Verbal Polarity Shifters: Mono- and
    Cross-lingual Methods for German'
  title_html: 'Automatically Creating a Lexicon of Verbal Polarity Shifters: Mono-
    and Cross-lingual Methods for <span class="acl-fixed-case">G</span>erman'
  url: https://www.aclweb.org/anthology/C18-1213
  year: '2018'
C18-1214:
  abstract: Most work on part-of-speech (POS) tagging is focused on high resource
    languages, or examines low-resource and active learning settings through simulated
    studies. We evaluate POS tagging techniques on an actual endangered language,
    Griko. We present a resource that contains 114 narratives in Griko, along with
    sentence-level translations in Italian, and provides gold annotations for the
    test set. Based on a previously collected small corpus, we investigate several
    traditional methods, as well as methods that take advantage of monolingual data
    or project cross-lingual POS tags. We show that the combination of a semi-supervised
    method with cross-lingual transfer is more appropriate for this extremely challenging
    setting, with the best tagger achieving an accuracy of 72.9%. With an applied
    active learning scheme, which we use to collect sentence-level annotations over
    the test set, we achieve improvements of more than 21 percentage points.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Antonios
    full: Antonios Anastasopoulos
    id: antonios-anastasopoulos
    last: Anastasopoulos
  - first: Marika
    full: Marika Lekakou
    id: marika-lekakou
    last: Lekakou
  - first: Josep
    full: Josep Quer
    id: josep-quer
    last: Quer
  - first: Eleni
    full: Eleni Zimianiti
    id: eleni-zimianiti
    last: Zimianiti
  - first: Justin
    full: Justin DeBenedetto
    id: justin-debenedetto
    last: DeBenedetto
  - first: David
    full: David Chiang
    id: david-chiang
    last: Chiang
  author_string: Antonios Anastasopoulos, Marika Lekakou, Josep Quer, Eleni Zimianiti,
    Justin DeBenedetto, David Chiang
  bibkey: anastasopoulos-etal-2018-part
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2529'
  page_last: '2539'
  pages: "2529\u20132539"
  paper_id: '214'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1214.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1214.jpg
  title: 'Part-of-Speech Tagging on an Endangered Language: a Parallel Griko-Italian
    Resource'
  title_html: 'Part-of-Speech Tagging on an Endangered Language: a Parallel <span
    class="acl-fixed-case">G</span>riko-<span class="acl-fixed-case">I</span>talian
    Resource'
  url: https://www.aclweb.org/anthology/C18-1214
  year: '2018'
C18-1215:
  abstract: Question-Answer (QA) matching is a fundamental task in the Natural Language
    Processing community. In this paper, we first build a novel QA matching corpus
    with informal text which is collected from a product reviewing website. Then,
    we propose a novel QA matching approach, namely One vs. Many Matching, which aims
    to address the novel scenario where one question sentence often has an answer
    with multiple sentences. Furthermore, we improve our matching approach by employing
    both word-level and sentence-level attentions for solving the noisy problem in
    the informal text. Empirical studies demonstrate the effectiveness of the proposed
    approach to question-answer matching.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Lu
    full: Lu Wang
    id: lu-wang
    last: Wang
  - first: Shoushan
    full: Shoushan Li
    id: shoushan-li
    last: Li
  - first: Changlong
    full: Changlong Sun
    id: changlong-sun
    last: Sun
  - first: Luo
    full: Luo Si
    id: luo-si
    last: Si
  - first: Xiaozhong
    full: Xiaozhong Liu
    id: xiaozhong-liu
    last: Liu
  - first: Min
    full: Min Zhang
    id: min-zhang
    last: Zhang
  - first: Guodong
    full: Guodong Zhou
    id: guodong-zhou
    last: Zhou
  author_string: Lu Wang, Shoushan Li, Changlong Sun, Luo Si, Xiaozhong Liu, Min Zhang,
    Guodong Zhou
  bibkey: wang-etal-2018-one
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2540'
  page_last: '2550'
  pages: "2540\u20132550"
  paper_id: '215'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1215.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1215.jpg
  title: One vs. Many QA Matching with both Word-level and Sentence-level Attention
    Network
  title_html: One vs. Many <span class="acl-fixed-case">QA</span> Matching with both
    Word-level and Sentence-level Attention Network
  url: https://www.aclweb.org/anthology/C18-1215
  year: '2018'
C18-1216:
  abstract: Distributed representations of words play a major role in the field of
    natural language processing by encoding semantic and syntactic information of
    words. However, most existing works on learning word representations typically
    regard words as individual atomic units and thus are blind to subword information
    in words. This further gives rise to a difficulty in representing out-of-vocabulary
    (OOV) words. In this paper, we present a character-based word representation approach
    to deal with this limitation. The proposed model learns to generate word representations
    from characters. In our model, we employ a convolutional neural network and a
    highway network over characters to extract salient features effectively. Unlike
    previous models that learn word representations from a large corpus, we take a
    set of pre-trained word embeddings and generalize it to word entries, including
    OOV words. To demonstrate the efficacy of the proposed model, we perform both
    an intrinsic and an extrinsic task which are word similarity and language modeling,
    respectively. Experimental results show clearly that the proposed model significantly
    outperforms strong baseline models that regard words or their subwords as atomic
    units. For example, we achieve as much as 18.5% improvement on average in perplexity
    for morphologically rich languages compared to strong baselines in the language
    modeling task.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yeachan
    full: Yeachan Kim
    id: yeachan-kim
    last: Kim
  - first: Kang-Min
    full: Kang-Min Kim
    id: kang-min-kim
    last: Kim
  - first: Ji-Min
    full: Ji-Min Lee
    id: ji-min-lee
    last: Lee
  - first: SangKeun
    full: SangKeun Lee
    id: sangkeun-lee
    last: Lee
  author_string: Yeachan Kim, Kang-Min Kim, Ji-Min Lee, SangKeun Lee
  bibkey: kim-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2551'
  page_last: '2561'
  pages: "2551\u20132561"
  paper_id: '216'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1216.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1216.jpg
  title: Learning to Generate Word Representations using Subword Information
  title_html: Learning to Generate Word Representations using Subword Information
  url: https://www.aclweb.org/anthology/C18-1216
  year: '2018'
C18-1217:
  abstract: State-of-the-art Natural Language Processing algorithms rely heavily on
    efficient word segmentation. Urdu is amongst languages for which word segmentation
    is a complex task as it exhibits space omission as well as space insertion issues.
    This is partly due to the Arabic script which although cursive in nature, consists
    of characters that have inherent joining and non-joining attributes regardless
    of word boundary. This paper presents a word segmentation system for Urdu which
    uses a Conditional Random Field sequence modeler with orthographic, linguistic
    and morphological features. Our proposed model automatically learns to predict
    white space as word boundary as well as Zero Width Non-Joiner (ZWNJ) as sub-word
    boundary. Using a manually annotated corpus, our model achieves F1 score of 0.97
    for word boundary identification and 0.85 for sub-word boundary identification
    tasks. We have made our code and corpus publicly available to make our results
    reproducible.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Haris
    full: Haris Bin Zia
    id: haris-bin-zia
    last: Bin Zia
  - first: Agha Ali
    full: Agha Ali Raza
    id: agha-ali-raza
    last: Raza
  - first: Awais
    full: Awais Athar
    id: awais-athar
    last: Athar
  author_string: Haris Bin Zia, Agha Ali Raza, Awais Athar
  bibkey: bin-zia-etal-2018-urdu
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2562'
  page_last: '2569'
  pages: "2562\u20132569"
  paper_id: '217'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1217.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1217.jpg
  title: Urdu Word Segmentation using Conditional Random Fields (CRFs)
  title_html: <span class="acl-fixed-case">U</span>rdu Word Segmentation using Conditional
    Random Fields (<span class="acl-fixed-case">CRF</span>s)
  url: https://www.aclweb.org/anthology/C18-1217
  year: '2018'
C18-1218:
  abstract: In this article, we present ReSyf, a lexical resource of monolingual synonyms
    ranked according to their difficulty to be read and understood by native learners
    of French. The synonyms come from an existing lexical network and they have been
    semantically disambiguated and refined. A ranking algorithm, based on a wide range
    of linguistic features and validated through an evaluation campaign with human
    annotators, automatically sorts the synonyms corresponding to a given word sense
    by reading difficulty. ReSyf is freely available and will be integrated into a
    web platform for reading assistance. It can also be applied to perform lexical
    simplification of French texts.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Mokhtar B.
    full: Mokhtar B. Billami
    id: mokhtar-b-billami
    last: Billami
  - first: Thomas
    full: "Thomas Fran\xE7ois"
    id: thomas-francois
    last: "Fran\xE7ois"
  - first: "N\xFAria"
    full: "N\xFAria Gala"
    id: nuria-gala
    last: Gala
  author_string: "Mokhtar B. Billami, Thomas Fran\xE7ois, N\xFAria Gala"
  bibkey: billami-etal-2018-resyf
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2570'
  page_last: '2581'
  pages: "2570\u20132581"
  paper_id: '218'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1218.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1218.jpg
  title: 'ReSyf: a French lexicon with ranked synonyms'
  title_html: '<span class="acl-fixed-case">R</span>e<span class="acl-fixed-case">S</span>yf:
    a <span class="acl-fixed-case">F</span>rench lexicon with ranked synonyms'
  url: https://www.aclweb.org/anthology/C18-1218
  year: '2018'
C18-1219:
  abstract: Multiword expressions, especially verbal ones (VMWEs), show idiosyncratic
    variability, which is challenging for NLP applications, hence the need for VMWE
    identification. We focus on the task of variant identification, i.e. identifying
    variants of previously seen VMWEs, whatever their surface form. We model the problem
    as a classification task. Syntactic subtrees with previously seen combinations
    of lemmas are first extracted, and then classified on the basis of features relevant
    to morpho-syntactic variation of VMWEs. Feature values are both absolute, i.e.
    hold for a particular VMWE candidate, and relative, i.e. based on comparing a
    candidate with previously seen VMWEs. This approach outperforms a baseline by
    4 percent points of F-measure on a French corpus.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Caroline
    full: Caroline Pasquer
    id: caroline-pasquer
    last: Pasquer
  - first: Agata
    full: Agata Savary
    id: agata-savary
    last: Savary
  - first: Carlos
    full: Carlos Ramisch
    id: carlos-ramisch
    last: Ramisch
  - first: Jean-Yves
    full: Jean-Yves Antoine
    id: jean-yves-antoine
    last: Antoine
  author_string: Caroline Pasquer, Agata Savary, Carlos Ramisch, Jean-Yves Antoine
  bibkey: pasquer-etal-2018-youve
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2582'
  page_last: '2594'
  pages: "2582\u20132594"
  paper_id: '219'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1219.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1219.jpg
  title: "If you\u2019ve seen some, you\u2019ve seen them all: Identifying variants\
    \ of multiword expressions"
  title_html: "If you\u2019ve seen some, you\u2019ve seen them all: Identifying variants\
    \ of multiword expressions"
  url: https://www.aclweb.org/anthology/C18-1219
  year: '2018'
C18-1220:
  abstract: Multilingual topic models enable crosslingual tasks by extracting consistent
    topics from multilingual corpora. Most models require parallel or comparable training
    corpora, which limits their ability to generalize. In this paper, we first demystify
    the knowledge transfer mechanism behind multilingual topic models by defining
    an alternative but equivalent formulation. Based on this analysis, we then relax
    the assumption of training data required by most existing models, creating a model
    that only requires a dictionary for training. Experiments show that our new method
    effectively learns coherent multilingual topics from partially and fully incomparable
    corpora with limited amounts of dictionary resources.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Shudong
    full: Shudong Hao
    id: shudong-hao
    last: Hao
  - first: Michael J.
    full: Michael J. Paul
    id: michael-paul
    last: Paul
  author_string: Shudong Hao, Michael J. Paul
  bibkey: hao-paul-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2595'
  page_last: '2609'
  pages: "2595\u20132609"
  paper_id: '220'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1220.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1220.jpg
  title: Learning Multilingual Topics from Incomparable Corpora
  title_html: Learning Multilingual Topics from Incomparable Corpora
  url: https://www.aclweb.org/anthology/C18-1220
  year: '2018'
C18-1221:
  abstract: Scientific papers from all disciplines contain many abbreviations and
    acronyms. In many cases these acronyms are ambiguous. We present a method to choose
    the contextual correct definition of an acronym that does not require training
    for each acronym and thus can be applied to a large number of different acronyms
    with only few instances. We constructed a set of 19,954 examples of 4,365 ambiguous
    acronyms from image captions in scientific papers along with their contextually
    correct definition from different domains. We learn word embeddings for all words
    in the corpus and compare the averaged context vector of the words in the expansion
    of an acronym with the weighted average vector of the words in the context of
    the acronym. We show that this method clearly outperforms (classical) cosine similarity.
    Furthermore, we show that word embeddings learned from a 1 billion word corpus
    of scientific texts outperform word embeddings learned on much large general corpora.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Jean
    full: Jean Charbonnier
    id: jean-charbonnier
    last: Charbonnier
  - first: Christian
    full: Christian Wartena
    id: christian-wartena
    last: Wartena
  author_string: Jean Charbonnier, Christian Wartena
  bibkey: charbonnier-wartena-2018-using
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2610'
  page_last: '2619'
  pages: "2610\u20132619"
  paper_id: '221'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1221.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1221.jpg
  title: Using Word Embeddings for Unsupervised Acronym Disambiguation
  title_html: Using Word Embeddings for Unsupervised Acronym Disambiguation
  url: https://www.aclweb.org/anthology/C18-1221
  year: '2018'
C18-1222:
  abstract: In this article, we discuss which text, speech, and image technologies
    have been developed, and would be feasible to develop, for the approximately 60
    Indigenous languages spoken in Canada. In particular, we concentrate on technologies
    that may be feasible to develop for most or all of these languages, not just those
    that may be feasible for the few most-resourced of these. We assess past achievements
    and consider future horizons for Indigenous language transliteration, text prediction,
    spell-checking, approximate search, machine translation, speech recognition, speaker
    diarization, speech synthesis, optical character recognition, and computer-aided
    language learning.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Patrick
    full: Patrick Littell
    id: patrick-littell
    last: Littell
  - first: Anna
    full: Anna Kazantseva
    id: anna-kazantseva
    last: Kazantseva
  - first: Roland
    full: Roland Kuhn
    id: roland-kuhn
    last: Kuhn
  - first: Aidan
    full: Aidan Pine
    id: aidan-pine
    last: Pine
  - first: Antti
    full: Antti Arppe
    id: antti-arppe
    last: Arppe
  - first: Christopher
    full: Christopher Cox
    id: christopher-cox
    last: Cox
  - first: Marie-Odile
    full: Marie-Odile Junker
    id: marie-odile-junker
    last: Junker
  author_string: Patrick Littell, Anna Kazantseva, Roland Kuhn, Aidan Pine, Antti
    Arppe, Christopher Cox, Marie-Odile Junker
  bibkey: littell-etal-2018-indigenous
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2620'
  page_last: '2632'
  pages: "2620\u20132632"
  paper_id: '222'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1222.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1222.jpg
  title: 'Indigenous language technologies in Canada: Assessment, challenges, and
    successes'
  title_html: 'Indigenous language technologies in Canada: Assessment, challenges,
    and successes'
  url: https://www.aclweb.org/anthology/C18-1222
  year: '2018'
C18-1223:
  abstract: "Text generation may require the pluralization of nouns, such as in context-sensitive\
    \ user interfaces and in natural language generation more broadly. While this\
    \ has been solved for the widely-used languages, this is still a challenge for\
    \ the languages in the Bantu language family. Pluralization results obtained for\
    \ isiZulu and Runyankore showed there were similarities in approach, including\
    \ the need to combine syntax with semantics, despite belonging to different language\
    \ zones. This suggests that bootstrapping and generalizability might be feasible.\
    \ We investigated this systematically for seven languages across three different\
    \ Guthrie language zones. The first outcome is that Meinhof\u2019s 1948 specification\
    \ of the noun classes are indeed inadequate for computational purposes for all\
    \ examined languages, due to non-determinism in prefixes, and we thus redefined\
    \ the characteristic noun class tables of 29 noun classes into 53. The second\
    \ main result is that the generic pluralizer achieved over 93% accuracy in coverage\
    \ testing and over 94% on a random sample. This is comparable to the language-specific\
    \ isiZulu and Runyankore pluralizers."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Joan
    full: Joan Byamugisha
    id: joan-byamugisha
    last: Byamugisha
  - first: C. Maria
    full: C. Maria Keet
    id: c-maria-keet
    last: Keet
  - first: Brian
    full: Brian DeRenzi
    id: brian-derenzi
    last: DeRenzi
  author_string: Joan Byamugisha, C. Maria Keet, Brian DeRenzi
  bibkey: byamugisha-etal-2018-pluralizing
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2633'
  page_last: '2643'
  pages: "2633\u20132643"
  paper_id: '223'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1223.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1223.jpg
  title: Pluralizing Nouns across Agglutinating Bantu Languages
  title_html: Pluralizing Nouns across Agglutinating <span class="acl-fixed-case">B</span>antu
    Languages
  url: https://www.aclweb.org/anthology/C18-1223
  year: '2018'
C18-1224:
  abstract: "Commonsense, real-world knowledge about the events that entities or \u201C\
    things in the world\u201D are typically involved in, as well as part-whole relationships,\
    \ is valuable for allowing computational systems to draw everyday inferences about\
    \ the world. Here, we focus on automatically extracting information about (1)\
    \ the events that typically bring about certain entities (origins), (2) the events\
    \ that are the typical functions of entities, and (3) part-whole relationships\
    \ in entities. These correspond to the agentive, telic and constitutive qualia\
    \ central to the Generative Lexicon. We describe our motivations and methods for\
    \ extracting these qualia relations from the Suggested Upper Merged Ontology (SUMO)\
    \ and show that human annotators overwhelmingly find the information extracted\
    \ to be reasonable. Because ontologies provide a way of structuring this information\
    \ and making it accessible to agents and computational systems generally, efforts\
    \ are underway to incorporate the extracted information to an ontology hub of\
    \ Natural Language Processing semantic role labeling resources, the Rich Event\
    \ Ontology."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Ghazaleh
    full: Ghazaleh Kazeminejad
    id: ghazaleh-kazeminejad
    last: Kazeminejad
  - first: Claire
    full: Claire Bonial
    id: claire-bonial
    last: Bonial
  - first: Susan Windisch
    full: Susan Windisch Brown
    id: susan-windisch-brown
    last: Brown
  - first: Martha
    full: Martha Palmer
    id: martha-palmer
    last: Palmer
  author_string: Ghazaleh Kazeminejad, Claire Bonial, Susan Windisch Brown, Martha
    Palmer
  bibkey: kazeminejad-etal-2018-automatically
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2644'
  page_last: '2652'
  pages: "2644\u20132652"
  paper_id: '224'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1224.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1224.jpg
  title: Automatically Extracting Qualia Relations for the Rich Event Ontology
  title_html: Automatically Extracting Qualia Relations for the Rich Event Ontology
  url: https://www.aclweb.org/anthology/C18-1224
  year: '2018'
C18-1225:
  abstract: 'We present SeVeN (Semantic Vector Networks), a hybrid resource that encodes
    relationships between words in the form of a graph. Different from traditional
    semantic networks, these relations are represented as vectors in a continuous
    vector space. We propose a simple pipeline for learning such relation vectors,
    which is based on word vector averaging in combination with an ad hoc autoencoder.
    We show that by explicitly encoding relational information in a dedicated vector
    space we can capture aspects of word meaning that are complementary to what is
    captured by word embeddings. For example, by examining clusters of relation vectors,
    we observe that relational similarities can be identified at a more abstract level
    than with traditional word vector differences. Finally, we test the effectiveness
    of semantic vector networks in two tasks: measuring word similarity and neural
    text categorization. SeVeN is available at bitbucket.org/luisespinosa/seven.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Luis
    full: Luis Espinosa-Anke
    id: luis-espinosa-anke
    last: Espinosa-Anke
  - first: Steven
    full: Steven Schockaert
    id: steven-schockaert
    last: Schockaert
  author_string: Luis Espinosa-Anke, Steven Schockaert
  bibkey: espinosa-anke-schockaert-2018-seven
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2653'
  page_last: '2665'
  pages: "2653\u20132665"
  paper_id: '225'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1225.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1225.jpg
  title: 'SeVeN: Augmenting Word Embeddings with Unsupervised Relation Vectors'
  title_html: '<span class="acl-fixed-case">S</span>e<span class="acl-fixed-case">V</span>e<span
    class="acl-fixed-case">N</span>: Augmenting Word Embeddings with Unsupervised
    Relation Vectors'
  url: https://www.aclweb.org/anthology/C18-1225
  year: '2018'
C18-1226:
  abstract: We evaluated various compositional models, from bag-of-words representations
    to compositional RNN-based models, on several extrinsic supervised and unsupervised
    evaluation benchmarks. Our results confirm that weighted vector averaging can
    outperform context-sensitive models in most benchmarks, but structural features
    encoded in RNN models can also be useful in certain classification tasks. We analyzed
    some of the evaluation datasets to identify the aspects of meaning they measure
    and the characteristics of the various models that explain their performance variance.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Hanan
    full: Hanan Aldarmaki
    id: hanan-aldarmaki
    last: Aldarmaki
  - first: Mona
    full: Mona Diab
    id: mona-diab
    last: Diab
  author_string: Hanan Aldarmaki, Mona Diab
  bibkey: aldarmaki-diab-2018-evaluation
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2666'
  page_last: '2677'
  pages: "2666\u20132677"
  paper_id: '226'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1226.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1226.jpg
  title: Evaluation of Unsupervised Compositional Representations
  title_html: Evaluation of Unsupervised Compositional Representations
  url: https://www.aclweb.org/anthology/C18-1226
  year: '2018'
C18-1227:
  abstract: "Formulaic expressions (FEs) used in scholarly papers, such as \u2018\
    there has been little discussion about\u2019, are helpful for non-native English\
    \ speakers. However, it is time-consuming for users to manually search for an\
    \ appropriate expression every time they want to consult FE dictionaries. For\
    \ this reason, we tackle the task of semantic searches of FE dictionaries. At\
    \ the start of our research, we identified two salient difficulties in this task.\
    \ First, the paucity of example sentences in existing FE dictionaries results\
    \ in a shortage of context information, which is necessary for acquiring semantic\
    \ representation of FEs. Second, while a semantic category label is assigned to\
    \ each FE in many FE dictionaries, it is difficult to predict the labels from\
    \ user input, forcing users to manually designate the semantic category when searching.\
    \ To address these difficulties, we propose a new framework for semantic searches\
    \ of FEs and propose a new method to leverage both existing dictionaries and domain\
    \ sentence corpora. Further, we expand an existing FE dictionary to consider building\
    \ a more comprehensive and domain-specific FE dictionary and to verify the effectiveness\
    \ of our method."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Kenichi
    full: Kenichi Iwatsuki
    id: kenichi-iwatsuki
    last: Iwatsuki
  - first: Akiko
    full: Akiko Aizawa
    id: akiko-aizawa
    last: Aizawa
  author_string: Kenichi Iwatsuki, Akiko Aizawa
  bibkey: iwatsuki-aizawa-2018-using
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2678'
  page_last: '2689'
  pages: "2678\u20132689"
  paper_id: '227'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1227.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1227.jpg
  title: Using Formulaic Expressions in Writing Assistance Systems
  title_html: Using Formulaic Expressions in Writing Assistance Systems
  url: https://www.aclweb.org/anthology/C18-1227
  year: '2018'
C18-1228:
  abstract: "Attempts to find a single technique for general-purpose intrinsic evaluation\
    \ of word embeddings have so far not been successful. We present a new approach\
    \ based on scaled-up qualitative analysis of word vector neighborhoods that quantifies\
    \ interpretable characteristics of a given model (e.g. its preference for synonyms\
    \ or shared morphological forms as nearest neighbors). We analyze 21 such factors\
    \ and show how they correlate with performance on 14 extrinsic and intrinsic task\
    \ datasets (and also explain the lack of correlation between some of them). Our\
    \ approach enables multi-faceted evaluation, parameter search, and generally \u2013\
    \ a more principled, hypothesis-driven approach to development of distributional\
    \ semantic representations."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Anna
    full: Anna Rogers
    id: anna-rogers
    last: Rogers
  - first: Shashwath
    full: Shashwath Hosur Ananthakrishna
    id: shashwath-hosur-ananthakrishna
    last: Hosur Ananthakrishna
  - first: Anna
    full: Anna Rumshisky
    id: anna-rumshisky
    last: Rumshisky
  author_string: Anna Rogers, Shashwath Hosur Ananthakrishna, Anna Rumshisky
  bibkey: rogers-etal-2018-whats
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2690'
  page_last: '2703'
  pages: "2690\u20132703"
  paper_id: '228'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1228.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1228.jpg
  title: "What\u2019s in Your Embedding, And How It Predicts Task Performance"
  title_html: "What\u2019s in Your Embedding, And How It Predicts Task Performance"
  url: https://www.aclweb.org/anthology/C18-1228
  year: '2018'
C18-1229:
  abstract: Word sense disambiguation (WSD) is the task to determine the word sense
    according to its context. Many existing WSD studies have been using an external
    knowledge-based unsupervised approach because it has fewer word set constraints
    than supervised approaches requiring training data. In this paper, we propose
    a new WSD method to generate the context of an ambiguous word by using similarities
    between an ambiguous word and words in the input document. In addition, to leverage
    our WSD method, we further propose a new word similarity calculation method based
    on the semantic network structure of BabelNet. We evaluate the proposed methods
    on the SemEval-13 and SemEval-15 for English WSD dataset. Experimental results
    demonstrate that the proposed WSD method significantly improves the baseline WSD
    method. Furthermore, our WSD system outperforms the state-of-the-art WSD systems
    in the Semeval-13 dataset. Finally, it has higher performance than the state-of-the-art
    unsupervised knowledge-based WSD system in the average performance of both datasets.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Dongsuk
    full: Dongsuk O
    id: dongsuk-o
    last: O
  - first: Sunjae
    full: Sunjae Kwon
    id: sunjae-kwon
    last: Kwon
  - first: Kyungsun
    full: Kyungsun Kim
    id: kyungsun-kim
    last: Kim
  - first: Youngjoong
    full: Youngjoong Ko
    id: youngjoong-ko
    last: Ko
  author_string: Dongsuk O, Sunjae Kwon, Kyungsun Kim, Youngjoong Ko
  bibkey: o-etal-2018-word
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2704'
  page_last: '2714'
  pages: "2704\u20132714"
  paper_id: '229'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1229.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1229.jpg
  title: Word Sense Disambiguation Based on Word Similarity Calculation Using Word
    Vector Representation from a Knowledge-based Graph
  title_html: Word Sense Disambiguation Based on Word Similarity Calculation Using
    Word Vector Representation from a Knowledge-based Graph
  url: https://www.aclweb.org/anthology/C18-1229
  year: '2018'
C18-1230:
  abstract: In this paper, we propose a method for obtaining sentence-level embeddings.
    While the problem of securing word-level embeddings is very well studied, we propose
    a novel method for obtaining sentence-level embeddings. This is obtained by a
    simple method in the context of solving the paraphrase generation task. If we
    use a sequential encoder-decoder model for generating paraphrase, we would like
    the generated paraphrase to be semantically close to the original sentence. One
    way to ensure this is by adding constraints for true paraphrase embeddings to
    be close and unrelated paraphrase candidate sentence embeddings to be far. This
    is ensured by using a sequential pair-wise discriminator that shares weights with
    the encoder that is trained with a suitable loss function. Our loss function penalizes
    paraphrase sentence embedding distances from being too large. This loss is used
    in combination with a sequential encoder-decoder network. We also validated our
    method by evaluating the obtained embeddings for a sentiment analysis task. The
    proposed method results in semantic embeddings and outperforms the state-of-the-art
    on the paraphrase generation and sentiment analysis task on standard datasets.
    These results are also shown to be statistically significant.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Badri Narayana
    full: Badri Narayana Patro
    id: badri-narayana-patro
    last: Patro
  - first: Vinod Kumar
    full: Vinod Kumar Kurmi
    id: vinod-kumar-kurmi
    last: Kurmi
  - first: Sandeep
    full: Sandeep Kumar
    id: sandeep-kumar
    last: Kumar
  - first: Vinay
    full: Vinay Namboodiri
    id: vinay-namboodiri
    last: Namboodiri
  author_string: Badri Narayana Patro, Vinod Kumar Kurmi, Sandeep Kumar, Vinay Namboodiri
  bibkey: patro-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2715'
  page_last: '2729'
  pages: "2715\u20132729"
  paper_id: '230'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1230.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1230.jpg
  title: Learning Semantic Sentence Embeddings using Sequential Pair-wise Discriminator
  title_html: Learning Semantic Sentence Embeddings using Sequential Pair-wise Discriminator
  url: https://www.aclweb.org/anthology/C18-1230
  year: '2018'
C18-1231:
  abstract: Several metrics have been proposed for evaluating grammatical error correction
    (GEC) systems based on grammaticality, fluency, and adequacy of the output sentences.
    Previous studies of the correlation of these metrics with human quality judgments
    were inconclusive, due to the lack of appropriate significance tests, discrepancies
    in the methods, and choice of datasets used. In this paper, we re-evaluate reference-based
    GEC metrics by measuring the system-level correlations with humans on a large
    dataset of human judgments of GEC outputs, and by properly conducting statistical
    significance tests. Our results show no significant advantage of GLEU over MaxMatch
    (M2), contradicting previous studies that claim GLEU to be superior. For a finer-grained
    analysis, we additionally evaluate these metrics for their agreement with human
    judgments at the sentence level. Our sentence-level analysis indicates that comparing
    GLEU and M2, one metric may be more useful than the other depending on the scenario.
    We further qualitatively analyze these metrics and our findings show that apart
    from being less interpretable and non-deterministic, GLEU also produces counter-intuitive
    scores in commonly occurring test examples.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Shamil
    full: Shamil Chollampatt
    id: shamil-chollampatt
    last: Chollampatt
  - first: Hwee Tou
    full: Hwee Tou Ng
    id: hwee-tou-ng
    last: Ng
  author_string: Shamil Chollampatt, Hwee Tou Ng
  bibkey: chollampatt-ng-2018-reassessment
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2730'
  page_last: '2741'
  pages: "2730\u20132741"
  paper_id: '231'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1231.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1231.jpg
  title: A Reassessment of Reference-Based Grammatical Error Correction Metrics
  title_html: A Reassessment of Reference-Based Grammatical Error Correction Metrics
  url: https://www.aclweb.org/anthology/C18-1231
  year: '2018'
C18-1232:
  abstract: While much progress has been made in how to encode a text sequence into
    a sequence of vectors, less attention has been paid to how to aggregate these
    preceding vectors (outputs of RNN/CNN) into fixed-size encoding vector. Usually,
    a simple max or average pooling is used, which is a bottom-up and passive way
    of aggregation and lack of guidance by task information. In this paper, we propose
    an aggregation mechanism to obtain a fixed-size encoding with a dynamic routing
    policy. The dynamic routing policy is dynamically deciding that what and how much
    information need be transferred from each word to the final encoding of the text
    sequence. Following the work of Capsule Network, we design two dynamic routing
    policies to aggregate the outputs of RNN/CNN encoding layer into a final encoding
    vector. Compared to the other aggregation methods, dynamic routing can refine
    the messages according to the state of final encoding vector. Experimental results
    on five text classification tasks show that our method outperforms other aggregating
    models by a significant margin. Related source code is released on our github
    page.Related source code is released on our github page.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Jingjing
    full: Jingjing Gong
    id: jingjing-gong
    last: Gong
  - first: Xipeng
    full: Xipeng Qiu
    id: xipeng-qiu
    last: Qiu
  - first: Shaojing
    full: Shaojing Wang
    id: shaojing-wang
    last: Wang
  - first: Xuanjing
    full: Xuanjing Huang
    id: xuan-jing-huang
    last: Huang
  author_string: Jingjing Gong, Xipeng Qiu, Shaojing Wang, Xuanjing Huang
  bibkey: gong-etal-2018-information
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2742'
  page_last: '2752'
  pages: "2742\u20132752"
  paper_id: '232'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1232.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1232.jpg
  title: Information Aggregation via Dynamic Routing for Sequence Encoding
  title_html: Information Aggregation via Dynamic Routing for Sequence Encoding
  url: https://www.aclweb.org/anthology/C18-1232
  year: '2018'
C18-1233:
  abstract: Semantic role labeling (SRL) is to recognize the predicate-argument structure
    of a sentence, including subtasks of predicate disambiguation and argument labeling.
    Previous studies usually formulate the entire SRL problem into two or more subtasks.
    For the first time, this paper introduces an end-to-end neural model which unifiedly
    tackles the predicate disambiguation and the argument labeling in one shot. Using
    a biaffine scorer, our model directly predicts all semantic role labels for all
    given word pairs in the sentence without relying on any syntactic parse information.
    Specifically, we augment the BiLSTM encoder with a non-linear transformation to
    further distinguish the predicate and the argument in a given sentence, and model
    the semantic role labeling process as a word pair classification task by employing
    the biaffine attentional mechanism. Though the proposed model is syntax-agnostic
    with local decoder, it outperforms the state-of-the-art syntax-aware SRL systems
    on the CoNLL-2008, 2009 benchmarks for both English and Chinese. To our best knowledge,
    we report the first syntax-agnostic SRL model that surpasses all known syntax-aware
    models.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Jiaxun
    full: Jiaxun Cai
    id: jiaxun-cai
    last: Cai
  - first: Shexia
    full: Shexia He
    id: shexia-he
    last: He
  - first: Zuchao
    full: Zuchao Li
    id: zuchao-li
    last: Li
  - first: Hai
    full: Hai Zhao
    id: hai-zhao
    last: Zhao
  author_string: Jiaxun Cai, Shexia He, Zuchao Li, Hai Zhao
  bibkey: cai-etal-2018-full
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2753'
  page_last: '2765'
  pages: "2753\u20132765"
  paper_id: '233'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1233.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1233.jpg
  title: A Full End-to-End Semantic Role Labeler, Syntactic-agnostic Over Syntactic-aware?
  title_html: A Full End-to-End Semantic Role Labeler, Syntactic-agnostic Over Syntactic-aware?
  url: https://www.aclweb.org/anthology/C18-1233
  year: '2018'
C18-1234:
  abstract: "Most existing research on authorship attribution uses various lexical,\
    \ syntactic and semantic features. In this paper we demonstrate an effective template-based\
    \ approach for combining various syntactic features of a document for authorship\
    \ analysis. The parse-tree based features that we propose are independent of the\
    \ topic of a document and reflect the innate writing styles of authors. We show\
    \ that the use of templates including sub-trees of parse trees in conjunction\
    \ with other syntactic features result in improved author attribution rates. Another\
    \ contribution is the demonstration that Dempster\u2019s rule based combination\
    \ of evidence from syntactic features performs better than other evidence-combination\
    \ methods. We also demonstrate that our methodology works well for the case where\
    \ actual author is not included in the candidate author set."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Jagadeesh
    full: Jagadeesh Patchala
    id: jagadeesh-patchala
    last: Patchala
  - first: Raj
    full: Raj Bhatnagar
    id: raj-bhatnagar
    last: Bhatnagar
  author_string: Jagadeesh Patchala, Raj Bhatnagar
  bibkey: patchala-bhatnagar-2018-authorship
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2766'
  page_last: '2777'
  pages: "2766\u20132777"
  paper_id: '234'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1234.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1234.jpg
  title: Authorship Attribution By Consensus Among Multiple Features
  title_html: Authorship Attribution By Consensus Among Multiple Features
  url: https://www.aclweb.org/anthology/C18-1234
  year: '2018'
C18-1235:
  abstract: "Dealing with \u2018open-vocabulary\u2019 slots has been among the challenges\
    \ in the natural language area. While recent studies on attention-based recurrent\
    \ neural network (RNN) models have performed well in completing several language\
    \ related tasks such as spoken language understanding and dialogue systems, there\
    \ has been a lack of attempts to address filling slots that take on values from\
    \ a virtually unlimited set. In this paper, we propose a new RNN model that can\
    \ capture the vital concept: Understanding the role of a word may vary according\
    \ to how long a reader focuses on a particular part of a sentence. The proposed\
    \ model utilizes a long-term aware attention structure, positional encoding primarily\
    \ considering the relative distance between words, and multi-task learning of\
    \ a character-based language model and an intent detection model. We show that\
    \ the model outperforms the existing RNN models with respect to discovering \u2018\
    open-vocabulary\u2019 slots without any external information, such as a named\
    \ entity database or knowledge base. In particular, we confirm that it performs\
    \ better with a greater number of slots in a dataset, including unknown words,\
    \ by evaluating the models on a dataset of several domains. In addition, the proposed\
    \ model also demonstrates superior performance with regard to intent detection."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Jun-Seong
    full: Jun-Seong Kim
    id: jun-seong-kim
    last: Kim
  - first: Junghoe
    full: Junghoe Kim
    id: junghoe-kim
    last: Kim
  - first: SeungUn
    full: SeungUn Park
    id: seungun-park
    last: Park
  - first: Kwangyong
    full: Kwangyong Lee
    id: kwangyong-lee
    last: Lee
  - first: Yoonju
    full: Yoonju Lee
    id: yoonju-lee
    last: Lee
  author_string: Jun-Seong Kim, Junghoe Kim, SeungUn Park, Kwangyong Lee, Yoonju Lee
  bibkey: kim-etal-2018-modeling
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2778'
  page_last: '2790'
  pages: "2778\u20132790"
  paper_id: '235'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1235.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1235.jpg
  title: Modeling with Recurrent Neural Networks for Open Vocabulary Slots
  title_html: Modeling with Recurrent Neural Networks for Open Vocabulary Slots
  url: https://www.aclweb.org/anthology/C18-1235
  year: '2018'
C18-1236:
  abstract: The Business Process Management (BPM) field focuses in the coordination
    of labor so that organizational processes are smoothly executed in a way that
    products and services are properly delivered. At the same time, NLP has reached
    a maturity level that enables its widespread application in many contexts, thanks
    to publicly available frameworks. In this position paper, we show how NLP has
    potential in raising the benefits of BPM practices at different levels. Instead
    of being exhaustive, we show selected key challenges were a successful application
    of NLP techniques would facilitate the automation of particular tasks that nowadays
    require a significant effort to accomplish. Finally, we report on applications
    that consider both the process perspective and its enhancement through NLP.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Han
    full: Han van der Aa
    id: han-van-der-aa
    last: van der Aa
  - first: Josep
    full: Josep Carmona
    id: josep-carmona
    last: Carmona
  - first: Henrik
    full: Henrik Leopold
    id: henrik-leopold
    last: Leopold
  - first: Jan
    full: Jan Mendling
    id: jan-mendling
    last: Mendling
  - first: "Llu\xEDs"
    full: "Llu\xEDs Padr\xF3"
    id: lluis-padro
    last: "Padr\xF3"
  author_string: "Han van der Aa, Josep Carmona, Henrik Leopold, Jan Mendling, Llu\xED\
    s Padr\xF3"
  bibkey: van-der-aa-etal-2018-challenges
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2791'
  page_last: '2801'
  pages: "2791\u20132801"
  paper_id: '236'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1236.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1236.jpg
  title: Challenges and Opportunities of Applying Natural Language Processing in Business
    Process Management
  title_html: Challenges and Opportunities of Applying Natural Language Processing
    in Business Process Management
  url: https://www.aclweb.org/anthology/C18-1236
  year: '2018'
C18-1237:
  abstract: "The rapid growth of documents across the web has necessitated finding\
    \ means of discarding redundant documents and retaining novel ones. Capturing\
    \ redundancy is challenging as it may involve investigating at a deep semantic\
    \ level. Techniques for detecting such semantic redundancy at the document level\
    \ are scarce. In this work we propose a deep Convolutional Neural Networks (CNN)\
    \ based model to classify a document as novel or redundant with respect to a set\
    \ of relevant documents already seen by the system. The system is simple and do\
    \ not require any manual feature engineering. Our novel scheme encodes relevant\
    \ and relative information from both source and target texts to generate an intermediate\
    \ representation which we coin as the Relative Document Vector (RDV). The proposed\
    \ method outperforms the existing state-of-the-art on a document-level novelty\
    \ detection dataset by a margin of \u223C5% in terms of accuracy. We further demonstrate\
    \ the effectiveness of our approach on a standard paraphrase detection dataset\
    \ where paraphrased passages closely resemble to semantically redundant documents."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Tirthankar
    full: Tirthankar Ghosal
    id: tirthankar-ghosal
    last: Ghosal
  - first: Vignesh
    full: Vignesh Edithal
    id: vignesh-edithal
    last: Edithal
  - first: Asif
    full: Asif Ekbal
    id: asif-ekbal
    last: Ekbal
  - first: Pushpak
    full: Pushpak Bhattacharyya
    id: pushpak-bhattacharyya
    last: Bhattacharyya
  - first: George
    full: George Tsatsaronis
    id: george-tsatsaronis
    last: Tsatsaronis
  - first: Srinivasa Satya Sameer Kumar
    full: Srinivasa Satya Sameer Kumar Chivukula
    id: srinivasa-satya-sameer-kumar-chivukula
    last: Chivukula
  author_string: Tirthankar Ghosal, Vignesh Edithal, Asif Ekbal, Pushpak Bhattacharyya,
    George Tsatsaronis, Srinivasa Satya Sameer Kumar Chivukula
  bibkey: ghosal-etal-2018-novelty
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2802'
  page_last: '2813'
  pages: "2802\u20132813"
  paper_id: '237'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1237.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1237.jpg
  title: Novelty Goes Deep. A Deep Neural Solution To Document Level Novelty Detection
  title_html: Novelty Goes Deep. A Deep Neural Solution To Document Level Novelty
    Detection
  url: https://www.aclweb.org/anthology/C18-1237
  year: '2018'
C18-1238:
  abstract: Authorship attribution typically uses all information representing both
    content and style whereas attribution based only on stylistic aspects may be robust
    in cross-domain settings. This paper analyzes different linguistic aspects that
    may help represent style. Specifically, we study the role of syntax and lexical
    words (nouns, verbs, adjectives and adverbs) in representing style. We use a purely
    syntactic language model to study the significance of sentence structures in both
    single-domain and cross-domain attribution, i.e. cross-topic and cross-genre attribution.
    We show that syntax may be helpful for cross-genre attribution while cross-topic
    attribution and single-domain may benefit from additional lexical information.
    Further, pure syntactic models may not be effective by themselves and need to
    be used in combination with other robust models. To study the role of word choice,
    we perform attribution by masking all words or specific topic words corresponding
    to nouns, verbs, adjectives and adverbs. Using a single-domain dataset, IMDB1M
    reviews, we demonstrate the heavy influence of common nouns and proper nouns in
    attribution, thereby highlighting topic interference. Using cross-domain Guardian10
    dataset, we show that some common nouns, verbs, adjectives and adverbs may help
    with stylometric attribution as demonstrated by masking topic words corresponding
    to these parts-of-speech. As expected, it was observed that proper nouns are heavily
    influenced by content and cross-domain attribution will benefit from completely
    masking them.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Kalaivani
    full: Kalaivani Sundararajan
    id: kalaivani-sundararajan
    last: Sundararajan
  - first: Damon
    full: Damon Woodard
    id: damon-woodard
    last: Woodard
  author_string: Kalaivani Sundararajan, Damon Woodard
  bibkey: sundararajan-woodard-2018-represents
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2814'
  page_last: '2822'
  pages: "2814\u20132822"
  paper_id: '238'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1238.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1238.jpg
  title: "What represents \u201Cstyle\u201D in authorship attribution?"
  title_html: "What represents \u201Cstyle\u201D in authorship attribution?"
  url: https://www.aclweb.org/anthology/C18-1238
  year: '2018'
C18-1239:
  abstract: Texts from the Internet serve as important data sources for financial
    market modeling. Early statistical approaches rely on manually defined features
    to capture lexical, sentiment and event information, which suffers from feature
    sparsity. Recent work has considered learning dense representations for news titles
    and abstracts. Compared to news titles, full documents can contain more potentially
    helpful information, but also noise compared to events and sentences, which has
    been less investigated in previous work. To fill this gap, we propose a novel
    target-specific abstract-guided news document representation model. The model
    uses a target-sensitive representation of the news abstract to weigh sentences
    in the news content, so as to select and combine the most informative sentences
    for market modeling. Results show that document representations can give better
    performance for estimating cumulative abnormal returns of companies when compared
    to titles and abstracts. Our model is especially effective when it used to combine
    information from multiple document sources compared to the sentence-level baselines.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Junwen
    full: Junwen Duan
    id: junwen-duan
    last: Duan
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  - first: Xiao
    full: Xiao Ding
    id: xiao-ding
    last: Ding
  - first: Ching-Yun
    full: Ching-Yun Chang
    id: ching-yun-chang
    last: Chang
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  author_string: Junwen Duan, Yue Zhang, Xiao Ding, Ching-Yun Chang, Ting Liu
  bibkey: duan-etal-2018-learning-target
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2823'
  page_last: '2833'
  pages: "2823\u20132833"
  paper_id: '239'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1239.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1239.jpg
  title: Learning Target-Specific Representations of Financial News Documents For
    Cumulative Abnormal Return Prediction
  title_html: Learning Target-Specific Representations of Financial News Documents
    For Cumulative Abnormal Return Prediction
  url: https://www.aclweb.org/anthology/C18-1239
  year: '2018'
C18-1240:
  abstract: Word composition is a promising technique for representation learning
    of large linguistic units (e.g., phrases, sentences and documents). However, most
    of the current composition models do not take the ambiguity of words and the context
    outside of a linguistic unit into consideration for learning representations,
    and consequently suffer from the inaccurate representation of semantics. To address
    this issue, we propose a model-free context-aware word composition model, which
    employs the latent semantic information as global context for learning representations.
    The proposed model attempts to resolve the word sense disambiguation and word
    composition in a unified framework. Extensive evaluation shows consistent improvements
    over various strong word representation/composition models at different granularities
    (including word, phrase and sentence), demonstrating the effectiveness of our
    proposed method.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Bo
    full: Bo An
    id: bo-an
    last: An
  - first: Xianpei
    full: Xianpei Han
    id: xianpei-han
    last: Han
  - first: Le
    full: Le Sun
    id: le-sun
    last: Sun
  author_string: Bo An, Xianpei Han, Le Sun
  bibkey: an-etal-2018-model
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2834'
  page_last: '2845'
  pages: "2834\u20132845"
  paper_id: '240'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1240.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1240.jpg
  title: Model-Free Context-Aware Word Composition
  title_html: Model-Free Context-Aware Word Composition
  url: https://www.aclweb.org/anthology/C18-1240
  year: '2018'
C18-1241:
  abstract: Representing a word by its co-occurrences with other words in context
    is an effective way to capture the meaning of the word. However, the theory behind
    remains a challenge. In this work, taking the example of a word classification
    task, we give a theoretical analysis of the approaches that represent a word X
    by a function f(P(C|X)), where C is a context feature, P(C|X) is the conditional
    probability estimated from a text corpus, and the function f maps the co-occurrence
    measure to a prediction score. We investigate the impact of context feature C
    and the function f . We also explain the reasons why using the co-occurrences
    with multiple context features may be better than just using a single one. In
    addition, based on the analysis, we propose a hypothesis about the conditional
    probability on zero probability events.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yanpeng
    full: Yanpeng Li
    id: yanpeng-li
    last: Li
  author_string: Yanpeng Li
  bibkey: li-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2846'
  page_last: '2854'
  pages: "2846\u20132854"
  paper_id: '241'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1241.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1241.jpg
  title: 'Learning Features from Co-occurrences: A Theoretical Analysis'
  title_html: 'Learning Features from Co-occurrences: A Theoretical Analysis'
  url: https://www.aclweb.org/anthology/C18-1241
  year: '2018'
C18-1242:
  abstract: Extracting a bilingual terminology for multi-word terms from comparable
    corpora has not been widely researched. In this work we propose a unified framework
    for aligning bilingual terms independently of the term lengths. We also introduce
    some enhancements to the context-based and the neural network based approaches.
    Our experiments show the effectiveness of our enhancements of previous works and
    the system can be adapted in specialized domains.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Jingshu
    full: Jingshu Liu
    id: jingshu-liu
    last: Liu
  - first: Emmanuel
    full: Emmanuel Morin
    id: emmanuel-morin
    last: Morin
  - first: "Pe\xF1a"
    full: "Pe\xF1a Saldarriaga"
    id: sebastian-pena-saldarriaga
    last: Saldarriaga
  author_string: "Jingshu Liu, Emmanuel Morin, Pe\xF1a Saldarriaga"
  bibkey: liu-etal-2018-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2855'
  page_last: '2866'
  pages: "2855\u20132866"
  paper_id: '242'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1242.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1242.jpg
  title: Towards a unified framework for bilingual terminology extraction of single-word
    and multi-word terms
  title_html: Towards a unified framework for bilingual terminology extraction of
    single-word and multi-word terms
  url: https://www.aclweb.org/anthology/C18-1242
  year: '2018'
C18-1243:
  abstract: Neural activation models have been proposed in the literature that use
    a set of example words for which fMRI measurements are available in order to find
    a mapping between word semantics and localized neural activations. Successful
    mappings let us expand to the full lexicon of concrete nouns using the assumption
    that similarity of meaning implies similar neural activation patterns. In this
    paper, we propose a computational model that estimates semantic similarity in
    the neural activation space and investigates the relative performance of this
    model for various natural language processing tasks. Despite the simplicity of
    the proposed model and the very small number of example words used to bootstrap
    it, the neural activation semantic model performs surprisingly well compared to
    state-of-the-art word embeddings. Specifically, the neural activation semantic
    model performs better than the state-of-the-art for the task of semantic similarity
    estimation between very similar or very dissimilar words, while performing well
    on other tasks such as entailment and word categorization. These are strong indications
    that neural activation semantic models can not only shed some light into human
    cognition but also contribute to computation models for certain tasks.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Nikos
    full: Nikos Athanasiou
    id: nikos-athanasiou
    last: Athanasiou
  - first: Elias
    full: Elias Iosif
    id: elias-iosif
    last: Iosif
  - first: Alexandros
    full: Alexandros Potamianos
    id: alexandros-potamianos
    last: Potamianos
  author_string: Nikos Athanasiou, Elias Iosif, Alexandros Potamianos
  bibkey: athanasiou-etal-2018-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2867'
  page_last: '2878'
  pages: "2867\u20132878"
  paper_id: '243'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1243.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1243.jpg
  title: 'Neural Activation Semantic Models: Computational lexical semantic models
    of localized neural activations'
  title_html: 'Neural Activation Semantic Models: Computational lexical semantic models
    of localized neural activations'
  url: https://www.aclweb.org/anthology/C18-1243
  year: '2018'
C18-1244:
  abstract: "Folksonomy of movies covers a wide range of heterogeneous information\
    \ about movies, like the genre, plot structure, visual experiences, soundtracks,\
    \ metadata, and emotional experiences from watching a movie. Being able to automatically\
    \ generate or predict tags for movies can help recommendation engines improve\
    \ retrieval of similar movies, and help viewers know what to expect from a movie\
    \ in advance. In this work, we explore the problem of creating tags for movies\
    \ from plot synopses. We propose a novel neural network model that merges information\
    \ from synopses and emotion flows throughout the plots to predict a set of tags\
    \ for movies. We compare our system with multiple baselines and found that the\
    \ addition of emotion flows boosts the performance of the network by learning\
    \ \u224818% more tags than a traditional machine learning system."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Sudipta
    full: Sudipta Kar
    id: sudipta-kar
    last: Kar
  - first: Suraj
    full: Suraj Maharjan
    id: suraj-maharjan
    last: Maharjan
  - first: Thamar
    full: Thamar Solorio
    id: thamar-solorio
    last: Solorio
  author_string: Sudipta Kar, Suraj Maharjan, Thamar Solorio
  bibkey: kar-etal-2018-folksonomication
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2879'
  page_last: '2891'
  pages: "2879\u20132891"
  paper_id: '244'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1244.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1244.jpg
  title: 'Folksonomication: Predicting Tags for Movies from Plot Synopses using Emotion
    Flow Encoded Neural Network'
  title_html: '<span class="acl-fixed-case">F</span>olksonomication: Predicting Tags
    for Movies from Plot Synopses using Emotion Flow Encoded Neural Network'
  url: https://www.aclweb.org/anthology/C18-1244
  year: '2018'
C18-1245:
  abstract: "Emotion Representation Mapping (ERM) has the goal to convert existing\
    \ emotion ratings from one representation format into another one, e.g., mapping\
    \ Valence-Arousal-Dominance annotations for words or sentences into Ekman\u2019\
    s Basic Emotions and vice versa. ERM can thus not only be considered as an alternative\
    \ to Word Emotion Induction (WEI) techniques for automatic emotion lexicon construction\
    \ but may also help mitigate problems that come from the proliferation of emotion\
    \ representation formats in recent years. We propose a new neural network approach\
    \ to ERM that not only outperforms the previous state-of-the-art. Equally important,\
    \ we present a refined evaluation methodology and gather strong evidence that\
    \ our model yields results which are (almost) as reliable as human annotations,\
    \ even in cross-lingual settings. Based on these results we generate new emotion\
    \ ratings for 13 typologically diverse languages and claim that they have near-gold\
    \ quality, at least."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Sven
    full: Sven Buechel
    id: sven-buechel
    last: Buechel
  - first: Udo
    full: Udo Hahn
    id: udo-hahn
    last: Hahn
  author_string: Sven Buechel, Udo Hahn
  bibkey: buechel-hahn-2018-emotion
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2892'
  page_last: '2904'
  pages: "2892\u20132904"
  paper_id: '245'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1245.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1245.jpg
  title: Emotion Representation Mapping for Automatic Lexicon Construction (Mostly)
    Performs on Human Level
  title_html: Emotion Representation Mapping for Automatic Lexicon Construction (Mostly)
    Performs on Human Level
  url: https://www.aclweb.org/anthology/C18-1245
  year: '2018'
C18-1246:
  abstract: Detection and classification of emotion categories expressed by a sentence
    is a challenging task due to subjectivity of emotion. To date, most of the models
    are trained and evaluated on single genre and when used to predict emotion in
    different genre their performance drops by a large margin. To address the issue
    of robustness, we model the problem within a joint multi-task learning framework.
    We train this model with a multigenre emotion corpus to predict emotions across
    various genre. Each genre is represented as a separate task, we use soft parameter
    shared layers across the various tasks. our experimental results show that this
    model improves the results across the various genres, compared to a single genre
    training in the same neural net architecture.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Shabnam
    full: Shabnam Tafreshi
    id: shabnam-tafreshi
    last: Tafreshi
  - first: Mona
    full: Mona Diab
    id: mona-diab
    last: Diab
  author_string: Shabnam Tafreshi, Mona Diab
  bibkey: tafreshi-diab-2018-emotion
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2905'
  page_last: '2913'
  pages: "2905\u20132913"
  paper_id: '246'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1246.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1246.jpg
  title: Emotion Detection and Classification in a Multigenre Corpus with Joint Multi-Task
    Deep Learning
  title_html: Emotion Detection and Classification in a Multigenre Corpus with Joint
    Multi-Task Deep Learning
  url: https://www.aclweb.org/anthology/C18-1246
  year: '2018'
C18-1247:
  abstract: "Social media based micro-blogging sites like Twitter have become a common\
    \ source of real-time information (impacting organizations and their strategies,\
    \ and are used for expressing emotions and opinions. Automated analysis of such\
    \ content therefore rises in importance. To this end, we explore the viability\
    \ of using deep neural networks on the specific task of emotion intensity prediction\
    \ in tweets. We propose a neural architecture combining convolutional and fully\
    \ connected layers in a non-sequential manner - done for the first time in context\
    \ of natural language based tasks. Combined with lexicon-based features along\
    \ with transfer learning, our model achieves state-of-the-art performance, outperforming\
    \ the previous system by 0.044 or 4.4% Pearson correlation on the WASSA\u2019\
    17 EmoInt shared task dataset. We investigate the performance of deep multi-task\
    \ learning models trained for all emotions at once in a unified architecture and\
    \ get encouraging results. Experiments performed on evaluating correlation between\
    \ emotion pairs offer interesting insights into the relationship between them."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Devang
    full: Devang Kulshreshtha
    id: devang-kulshreshtha
    last: Kulshreshtha
  - first: Pranav
    full: Pranav Goel
    id: pranav-goel
    last: Goel
  - first: Anil
    full: Anil Kumar Singh
    id: anil-kumar-singh
    last: Kumar Singh
  author_string: Devang Kulshreshtha, Pranav Goel, Anil Kumar Singh
  bibkey: kulshreshtha-etal-2018-emotional
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2914'
  page_last: '2926'
  pages: "2914\u20132926"
  paper_id: '247'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1247.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1247.jpg
  title: How emotional are you? Neural Architectures for Emotion Intensity Prediction
    in Microblogs
  title_html: How emotional are you? Neural Architectures for Emotion Intensity Prediction
    in Microblogs
  url: https://www.aclweb.org/anthology/C18-1247
  year: '2018'
C18-1248:
  abstract: Vulgarity is a common linguistic expression and is used to perform several
    linguistic functions. Understanding their usage can aid both linguistic and psychological
    phenomena as well as benefit downstream natural language processing applications
    such as sentiment analysis. This study performs a large-scale, data-driven empirical
    analysis of vulgar words using social media data. We analyze the socio-cultural
    and pragmatic aspects of vulgarity using tweets from users with known demographics.
    Further, we collect sentiment ratings for vulgar tweets to study the relationship
    between the use of vulgar words and perceived sentiment and show that explicitly
    modeling vulgar words can boost sentiment analysis performance.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Isabel
    full: Isabel Cachola
    id: isabel-cachola
    last: Cachola
  - first: Eric
    full: Eric Holgate
    id: eric-holgate
    last: Holgate
  - first: Daniel
    full: "Daniel Preo\u0163iuc-Pietro"
    id: daniel-preotiuc-pietro
    last: "Preo\u0163iuc-Pietro"
  - first: Junyi Jessy
    full: Junyi Jessy Li
    id: junyi-jessy-li
    last: Li
  author_string: "Isabel Cachola, Eric Holgate, Daniel Preo\u0163iuc-Pietro, Junyi\
    \ Jessy Li"
  bibkey: cachola-etal-2018-expressively
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2927'
  page_last: '2938'
  pages: "2927\u20132938"
  paper_id: '248'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1248.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1248.jpg
  title: 'Expressively vulgar: The socio-dynamics of vulgarity and its effects on
    sentiment analysis in social media'
  title_html: 'Expressively vulgar: The socio-dynamics of vulgarity and its effects
    on sentiment analysis in social media'
  url: https://www.aclweb.org/anthology/C18-1248
  year: '2018'
C18-1249:
  abstract: "We extend the coverage of an existing grammar customization system to\
    \ clausal modifiers, also referred to as adverbial clauses. We present an analysis,\
    \ taking a typologically-driven approach to account for this phenomenon across\
    \ the world\u2019s languages, which we implement in the Grammar Matrix customization\
    \ system (Bender et al., 2002, 2010). Testing our analysis on testsuites from\
    \ five genetically and geographically diverse languages that were not considered\
    \ in development, we achieve 88.4% coverage and 1.5% overgeneration."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Kristen
    full: Kristen Howell
    id: kristen-howell
    last: Howell
  - first: Olga
    full: Olga Zamaraeva
    id: olga-zamaraeva
    last: Zamaraeva
  author_string: Kristen Howell, Olga Zamaraeva
  bibkey: howell-zamaraeva-2018-clausal
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2939'
  page_last: '2952'
  pages: "2939\u20132952"
  paper_id: '249'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1249.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1249.jpg
  title: Clausal Modifiers in the Grammar Matrix
  title_html: Clausal Modifiers in the Grammar Matrix
  url: https://www.aclweb.org/anthology/C18-1249
  year: '2018'
C18-1250:
  abstract: Recurrent neural networks have achieved great success in many NLP tasks.
    However, they have difficulty in parallelization because of the recurrent structure,
    so it takes much time to train RNNs. In this paper, we introduce sliced recurrent
    neural networks (SRNNs), which could be parallelized by slicing the sequences
    into many subsequences. SRNNs have the ability to obtain high-level information
    through multiple layers with few extra parameters. We prove that the standard
    RNN is a special case of the SRNN when we use linear activation functions. Without
    changing the recurrent units, SRNNs are 136 times as fast as standard RNNs and
    could be even faster when we train longer sequences. Experiments on six large-scale
    sentiment analysis datasets show that SRNNs achieve better performance than standard
    RNNs.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Zeping
    full: Zeping Yu
    id: zeping-yu
    last: Yu
  - first: Gongshen
    full: Gongshen Liu
    id: gongshen-liu
    last: Liu
  author_string: Zeping Yu, Gongshen Liu
  bibkey: yu-liu-2018-sliced
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2953'
  page_last: '2964'
  pages: "2953\u20132964"
  paper_id: '250'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1250.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1250.jpg
  title: Sliced Recurrent Neural Networks
  title_html: Sliced Recurrent Neural Networks
  url: https://www.aclweb.org/anthology/C18-1250
  year: '2018'
C18-1251:
  abstract: We study three general multi-task learning (MTL) approaches on 11 sequence
    tagging tasks. Our extensive empirical results show that in about 50% of the cases,
    jointly learning all 11 tasks improves upon either independent or pairwise learning
    of the tasks. We also show that pairwise MTL can inform us what tasks can benefit
    others or what tasks can be benefited if they are learned jointly. In particular,
    we identify tasks that can always benefit others as well as tasks that can always
    be harmed by others. Interestingly, one of our MTL approaches yields embeddings
    of the tasks that reveal the natural clustering of semantic and syntactic tasks.
    Our inquiries have opened the doors to further utilization of MTL in NLP.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Soravit
    full: Soravit Changpinyo
    id: soravit-changpinyo
    last: Changpinyo
  - first: Hexiang
    full: Hexiang Hu
    id: hexiang-hu
    last: Hu
  - first: Fei
    full: Fei Sha
    id: fei-sha
    last: Sha
  author_string: Soravit Changpinyo, Hexiang Hu, Fei Sha
  bibkey: changpinyo-etal-2018-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2965'
  page_last: '2977'
  pages: "2965\u20132977"
  paper_id: '251'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1251.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1251.jpg
  title: 'Multi-Task Learning for Sequence Tagging: An Empirical Study'
  title_html: 'Multi-Task Learning for Sequence Tagging: An Empirical Study'
  url: https://www.aclweb.org/anthology/C18-1251
  year: '2018'
C18-1252:
  abstract: 'K-fold cross validation (CV) is a popular method for estimating the true
    performance of machine learning models, allowing model selection and parameter
    tuning. However, the very process of CV requires random partitioning of the data
    and so our performance estimates are in fact stochastic, with variability that
    can be substantial for natural language processing tasks. We demonstrate that
    these unstable estimates cannot be relied upon for effective parameter tuning.
    The resulting tuned parameters are highly sensitive to how our data is partitioned,
    meaning that we often select sub-optimal parameter choices and have serious reproducibility
    issues. Instead, we propose to use the less variable J-K-fold CV, in which J independent
    K-fold cross validations are used to assess performance. Our main contributions
    are extending J-K-fold CV from performance estimation to parameter tuning and
    investigating how to choose J and K. We argue that variability is more important
    than bias for effective tuning and so advocate lower choices of K than are typically
    seen in the NLP literature and instead use the saved computation to increase J.
    To demonstrate the generality of our recommendations we investigate a wide range
    of case-studies: sentiment classification (both general and target-specific),
    part-of-speech tagging and document classification.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Henry
    full: Henry Moss
    id: henry-moss
    last: Moss
  - first: David
    full: David Leslie
    id: david-leslie
    last: Leslie
  - first: Paul
    full: Paul Rayson
    id: paul-rayson
    last: Rayson
  author_string: Henry Moss, David Leslie, Paul Rayson
  bibkey: moss-etal-2018-using
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2978'
  page_last: '2989'
  pages: "2978\u20132989"
  paper_id: '252'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1252.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1252.jpg
  title: Using J-K-fold Cross Validation To Reduce Variance When Tuning NLP Models
  title_html: Using J-K-fold Cross Validation To Reduce Variance When Tuning <span
    class="acl-fixed-case">NLP</span> Models
  url: https://www.aclweb.org/anthology/C18-1252
  year: '2018'
C18-1253:
  abstract: Incrementality is ubiquitous in human-human interaction and beneficial
    for human-computer interaction. It has been a topic of research in different parts
    of the NLP community, mostly with focus on the specific topic at hand even though
    incremental systems have to deal with similar challenges regardless of domain.
    In this survey, I consolidate and categorize the approaches, identifying similarities
    and differences in the computation and data, and show trade-offs that have to
    be considered. A focus lies on evaluating incremental systems because the standard
    metrics often fail to capture the incremental properties of a system and coming
    up with a suitable evaluation scheme is non-trivial.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Arne
    full: "Arne K\xF6hn"
    id: arne-kohn
    last: "K\xF6hn"
  author_string: "Arne K\xF6hn"
  bibkey: kohn-2018-incremental
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '2990'
  page_last: '3003'
  pages: "2990\u20133003"
  paper_id: '253'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1253.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1253.jpg
  title: 'Incremental Natural Language Processing: Challenges, Strategies, and Evaluation'
  title_html: 'Incremental Natural Language Processing: Challenges, Strategies, and
    Evaluation'
  url: https://www.aclweb.org/anthology/C18-1253
  year: '2018'
C18-1254:
  abstract: This paper describes the augmentation of an existing corpus of child-directed
    speech. The resulting corpus is a gold-standard labeled corpus for supervised
    learning of semantic role labels in adult-child dialogues. Semantic role labeling
    (SRL) models assign semantic roles to sentence constituents, thus indicating who
    has done what to whom (and in what way). The current corpus is derived from the
    Adam files in the Brown corpus (Brown 1973) of the CHILDES corpora, and augments
    the partial annotation described in Connor et al. (2010). It provides labels for
    both semantic arguments of verbs and semantic arguments of prepositions. The semantic
    role labels and senses of verbs follow Propbank guidelines Kingsbury and Palmer,
    2002; Gildea and Palmer 2002; Palmer et al., 2005) and those for prepositions
    follow Srikumar and Roth (2011). The corpus was annotated by two annotators. Inter-annotator
    agreement is given separately for prepositions and verbs, and for adult speech
    and child speech. Overall, across child and adult samples, including verbs and
    prepositions, the kappa score for sense is 72.6, for the number of semantic-role-bearing
    arguments, the kappa score is 77.4, for identical semantic role labels on a given
    argument, the kappa score is 91.1, for the span of semantic role labels, and the
    kappa for agreement is 93.9. The sense and number of arguments was often open
    to multiple interpretations in child speech, due to the rapidly changing discourse
    and omission of constituents in production. Annotators used a discourse context
    window of ten sentences before and ten sentences after the target utterance to
    determine the annotation labels. The derived corpus is available for use in CHAT
    (MacWhinney, 2000) and XML format.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Lori
    full: Lori Moon
    id: lori-moon
    last: Moon
  - first: Christos
    full: Christos Christodoulopoulos
    id: christos-christodoulopoulos
    last: Christodoulopoulos
  - first: Cynthia
    full: Cynthia Fisher
    id: cynthia-fisher
    last: Fisher
  - first: Sandra
    full: Sandra Franco
    id: sandra-franco
    last: Franco
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Lori Moon, Christos Christodoulopoulos, Cynthia Fisher, Sandra Franco,
    Dan Roth
  bibkey: moon-etal-2018-gold
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3004'
  page_last: '3014'
  pages: "3004\u20133014"
  paper_id: '254'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1254.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1254.jpg
  title: Gold Standard Annotations for Preposition and Verb Sense with Semantic Role
    Labels in Adult-Child Interactions
  title_html: Gold Standard Annotations for Preposition and Verb Sense with Semantic
    Role Labels in Adult-Child Interactions
  url: https://www.aclweb.org/anthology/C18-1254
  year: '2018'
C18-1255:
  abstract: Neural machine translation systems require a number of stacked layers
    for deep models. But the prediction depends on the sentence representation of
    the top-most layer with no access to low-level representations. This makes it
    more difficult to train the model and poses a risk of information loss to prediction.
    In this paper, we propose a multi-layer representation fusion (MLRF) approach
    to fusing stacked layers. In particular, we design three fusion functions to learn
    a better representation from the stack. Experimental results show that our approach
    yields improvements of 0.92 and 0.56 BLEU points over the strong Transformer baseline
    on IWSLT German-English and NIST Chinese-English MT tasks respectively. The result
    is new state-of-the-art in German-English translation.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Qiang
    full: Qiang Wang
    id: qiang-wang
    last: Wang
  - first: Fuxue
    full: Fuxue Li
    id: fuxue-li
    last: Li
  - first: Tong
    full: Tong Xiao
    id: tong-xiao
    last: Xiao
  - first: Yanyang
    full: Yanyang Li
    id: yanyang-li
    last: Li
  - first: Yinqiao
    full: Yinqiao Li
    id: yinqiao-li
    last: Li
  - first: Jingbo
    full: Jingbo Zhu
    id: jingbo-zhu
    last: Zhu
  author_string: Qiang Wang, Fuxue Li, Tong Xiao, Yanyang Li, Yinqiao Li, Jingbo Zhu
  bibkey: wang-etal-2018-multi-layer
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3015'
  page_last: '3026'
  pages: "3015\u20133026"
  paper_id: '255'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1255.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1255.jpg
  title: Multi-layer Representation Fusion for Neural Machine Translation
  title_html: Multi-layer Representation Fusion for Neural Machine Translation
  url: https://www.aclweb.org/anthology/C18-1255
  year: '2018'
C18-1256:
  abstract: 'To enrich vocabulary of low resource settings, we proposed a novel method
    which identify loanwords in monolingual corpora. More specifically, we first use
    cross-lingual word embeddings as the core feature to generate semantically related
    candidates based on comparable corpora and a small bilingual lexicon; then, a
    log-linear model which combines several shallow features such as pronunciation
    similarity and hybrid language model features to predict the final results. In
    this paper, we use Uyghur as the receipt language and try to detect loanwords
    in four donor languages: Arabic, Chinese, Persian and Russian. We conduct two
    groups of experiments to evaluate the effectiveness of our proposed approach:
    loanword identification and OOV translation in four language pairs and eight translation
    directions (Uyghur-Arabic, Arabic-Uyghur, Uyghur-Chinese, Chinese-Uyghur, Uyghur-Persian,
    Persian-Uyghur, Uyghur-Russian, and Russian-Uyghur). Experimental results on loanword
    identification show that our method outperforms other baseline models significantly.
    Neural machine translation models integrating results of loanword identification
    experiments achieve the best results on OOV translation(with 0.5-0.9 BLEU improvements)'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Chenggang
    full: Chenggang Mi
    id: chenggang-mi
    last: Mi
  - first: Yating
    full: Yating Yang
    id: yating-yang
    last: Yang
  - first: Lei
    full: Lei Wang
    id: lei-wang
    last: Wang
  - first: Xi
    full: Xi Zhou
    id: xi-zhou
    last: Zhou
  - first: Tonghai
    full: Tonghai Jiang
    id: tonghai-jiang
    last: Jiang
  author_string: Chenggang Mi, Yating Yang, Lei Wang, Xi Zhou, Tonghai Jiang
  bibkey: mi-etal-2018-toward
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3027'
  page_last: '3037'
  pages: "3027\u20133037"
  paper_id: '256'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1256.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1256.jpg
  title: Toward Better Loanword Identification in Uyghur Using Cross-lingual Word
    Embeddings
  title_html: Toward Better Loanword Identification in <span class="acl-fixed-case">U</span>yghur
    Using Cross-lingual Word Embeddings
  url: https://www.aclweb.org/anthology/C18-1256
  year: '2018'
C18-1257:
  abstract: In the popular sequence to sequence (seq2seq) neural machine translation
    (NMT), there exist many weighted sum models (WSMs), each of which takes a set
    of input and generates one output. However, the weights in a WSM are independent
    of each other and fixed for all inputs, suggesting that by ignoring different
    needs of inputs, the WSM lacks effective control on the influence of each input.
    In this paper, we propose adaptive weighting for WSMs to control the contribution
    of each input. Specifically, we apply adaptive weighting for both GRU and the
    output state in NMT. Experimentation on Chinese-to-English translation and English-to-German
    translation demonstrates that the proposed adaptive weighting is able to much
    improve translation accuracy by achieving significant improvement of 1.49 and
    0.92 BLEU points for the two translation tasks. Moreover, we discuss in-depth
    on what type of information is encoded in the encoder and how information influences
    the generation of target words in the decoder.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yachao
    full: Yachao Li
    id: yachao-li
    last: Li
  - first: Junhui
    full: Junhui Li
    id: junhui-li
    last: Li
  - first: Min
    full: Min Zhang
    id: min-zhang
    last: Zhang
  author_string: Yachao Li, Junhui Li, Min Zhang
  bibkey: li-etal-2018-adaptive
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3038'
  page_last: '3048'
  pages: "3038\u20133048"
  paper_id: '257'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1257.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1257.jpg
  title: Adaptive Weighting for Neural Machine Translation
  title_html: Adaptive Weighting for Neural Machine Translation
  url: https://www.aclweb.org/anthology/C18-1257
  year: '2018'
C18-1258:
  abstract: "We formulate a generalization of Petrov et al. (2006)\u2019s split/merge\
    \ algorithm for interpreted regular tree grammars (Koller and Kuhlmann, 2011),\
    \ which capture a large class of grammar formalisms. We evaluate its effectiveness\
    \ empirically on the task of discontinuous constituent parsing with two mildly\
    \ context-sensitive grammar formalisms: linear context-free rewriting systems\
    \ (Vijay-Shanker et al., 1987) as well as hybrid grammars (Nederhof and Vogler,\
    \ 2014)."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Kilian
    full: Kilian Gebhardt
    id: kilian-gebhardt
    last: Gebhardt
  author_string: Kilian Gebhardt
  bibkey: gebhardt-2018-generic
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3049'
  page_last: '3063'
  pages: "3049\u20133063"
  paper_id: '258'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1258.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1258.jpg
  title: Generic refinement of expressive grammar formalisms with an application to
    discontinuous constituent parsing
  title_html: Generic refinement of expressive grammar formalisms with an application
    to discontinuous constituent parsing
  url: https://www.aclweb.org/anthology/C18-1258
  year: '2018'
C18-1259:
  abstract: 'Encoder-decoder based Sequence to Sequence learning (S2S) has made remarkable
    progress in recent years. Different network architectures have been used in the
    encoder/decoder. Among them, Convolutional Neural Networks (CNN) and Self Attention
    Networks (SAN) are the prominent ones. The two architectures achieve similar performances
    but use very different ways to encode and decode context: CNN use convolutional
    layers to focus on the local connectivity of the sequence, while SAN uses self-attention
    layers to focus on global semantics. In this work we propose Double Path Networks
    for Sequence to Sequence learning (DPN-S2S), which leverage the advantages of
    both models by using double path information fusion. During the encoding step,
    we develop a double path architecture to maintain the information coming from
    different paths with convolutional layers and self-attention layers separately.
    To effectively use the encoded context, we develop a gated attention fusion module
    and use it to automatically pick up the information needed during the decoding
    step, which is also a double path network. By deeply integrating the two paths,
    both types of information are combined and well exploited. Experiments show that
    our proposed method can significantly improve the performance of sequence to sequence
    learning over state-of-the-art systems.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Kaitao
    full: Kaitao Song
    id: kaitao-song
    last: Song
  - first: Xu
    full: Xu Tan
    id: xu-tan
    last: Tan
  - first: Di
    full: Di He
    id: di-he
    last: He
  - first: Jianfeng
    full: Jianfeng Lu
    id: jianfeng-lu
    last: Lu
  - first: Tao
    full: Tao Qin
    id: tao-qin
    last: Qin
  - first: Tie-Yan
    full: Tie-Yan Liu
    id: tie-yan-liu
    last: Liu
  author_string: Kaitao Song, Xu Tan, Di He, Jianfeng Lu, Tao Qin, Tie-Yan Liu
  bibkey: song-etal-2018-double
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3064'
  page_last: '3074'
  pages: "3064\u20133074"
  paper_id: '259'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1259.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1259.jpg
  title: Double Path Networks for Sequence to Sequence Learning
  title_html: Double Path Networks for Sequence to Sequence Learning
  url: https://www.aclweb.org/anthology/C18-1259
  year: '2018'
C18-1260:
  abstract: Syntactic parsing plays a crucial role in improving the quality of natural
    language processing tasks. Although there have been several research projects
    on syntactic parsing in Vietnamese, the parsing quality has been far inferior
    than those reported in major languages, such as English and Chinese. In this work,
    we evaluated representative constituency parsing models on a Vietnamese Treebank
    to look for the most suitable parsing method for Vietnamese. We then combined
    the advantages of automatic and manual analysis to investigate errors produced
    by the experimented parsers and find the reasons for them. Our analysis focused
    on three possible sources of parsing errors, namely limited training data, part-of-speech
    (POS) tagging errors, and ambiguous constructions. As a result, we found that
    the last two sources, which frequently appear in Vietnamese text, significantly
    attributed to the poor performance of Vietnamese parsing.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Quy
    full: Quy Nguyen
    id: quy-nguyen
    last: Nguyen
  - first: Yusuke
    full: Yusuke Miyao
    id: yusuke-miyao
    last: Miyao
  - first: Hiroshi
    full: Hiroshi Noji
    id: hiroshi-noji
    last: Noji
  - first: Nhung
    full: Nhung Nguyen
    id: nhung-nguyen
    last: Nguyen
  author_string: Quy Nguyen, Yusuke Miyao, Hiroshi Noji, Nhung Nguyen
  bibkey: nguyen-etal-2018-empirical
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3075'
  page_last: '3089'
  pages: "3075\u20133089"
  paper_id: '260'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1260.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1260.jpg
  title: An Empirical Investigation of Error Types in Vietnamese Parsing
  title_html: An Empirical Investigation of Error Types in <span class="acl-fixed-case">V</span>ietnamese
    Parsing
  url: https://www.aclweb.org/anthology/C18-1260
  year: '2018'
C18-1261:
  abstract: Noise-Contrastive Estimation (NCE) is a learning criterion that is regularly
    used to train neural language models in place of Maximum Likelihood Estimation,
    since it avoids the computational bottleneck caused by the output softmax. In
    this paper, we analyse and explain some of the weaknesses of this objective function,
    linked to the mechanism of self-normalization, by closely monitoring comparative
    experiments. We then explore several remedies and modifications to propose tractable
    and efficient NCE training strategies. In particular, we propose to make the scaling
    factor a trainable parameter of the model, and to use the noise distribution to
    initialize the output bias. These solutions, yet simple, yield stable and competitive
    performances in either small and large scale language modelling tasks.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Matthieu
    full: Matthieu Labeau
    id: matthieu-labeau
    last: Labeau
  - first: Alexandre
    full: Alexandre Allauzen
    id: alexandre-allauzen
    last: Allauzen
  author_string: Matthieu Labeau, Alexandre Allauzen
  bibkey: labeau-allauzen-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3090'
  page_last: '3101'
  pages: "3090\u20133101"
  paper_id: '261'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1261.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1261.jpg
  title: 'Learning with Noise-Contrastive Estimation: Easing training by learning
    to scale'
  title_html: 'Learning with Noise-Contrastive Estimation: Easing training by learning
    to scale'
  url: https://www.aclweb.org/anthology/C18-1261
  year: '2018'
C18-1262:
  abstract: "In this paper, we describe an attempt towards the development of parallel\
    \ corpora for English and Ethiopian Languages, such as Amharic, Tigrigna, Afan-Oromo,\
    \ Wolaytta and Ge\u2019ez. The corpora are used for conducting a bi-directional\
    \ statistical machine translation experiments. The BLEU scores of the bi-directional\
    \ Statistical Machine Translation (SMT) systems show a promising result. The morphological\
    \ richness of the Ethiopian languages has a great impact on the performance of\
    \ SMT specially when the targets are Ethiopian languages. Now we are working towards\
    \ an optimal alignment for a bi-directional English-Ethiopian languages SMT."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Solomon Teferra
    full: Solomon Teferra Abate
    id: solomon-teferra-abate
    last: Abate
  - first: Michael
    full: Michael Melese
    id: michael-melese
    last: Melese
  - first: Martha Yifiru
    full: Martha Yifiru Tachbelie
    id: martha-yifiru-tachbelie
    last: Tachbelie
  - first: Million
    full: Million Meshesha
    id: million-meshesha
    last: Meshesha
  - first: Solomon
    full: Solomon Atinafu
    id: solomon-atinafu
    last: Atinafu
  - first: Wondwossen
    full: Wondwossen Mulugeta
    id: wondwossen-mulugeta
    last: Mulugeta
  - first: Yaregal
    full: Yaregal Assabie
    id: yaregal-assabie
    last: Assabie
  - first: Hafte
    full: Hafte Abera
    id: hafte-abera
    last: Abera
  - first: Binyam
    full: Binyam Ephrem
    id: binyam-ephrem-seyoum
    last: Ephrem
  - first: Tewodros
    full: Tewodros Abebe
    id: tewodros-abebe
    last: Abebe
  - first: Wondimagegnhue
    full: Wondimagegnhue Tsegaye
    id: wondimagegnhue-tsegaye
    last: Tsegaye
  - first: Amanuel
    full: Amanuel Lemma
    id: amanuel-lemma
    last: Lemma
  - first: Tsegaye
    full: Tsegaye Andargie
    id: tsegaye-andargie
    last: Andargie
  - first: Seifedin
    full: Seifedin Shifaw
    id: seifedin-shifaw
    last: Shifaw
  author_string: Solomon Teferra Abate, Michael Melese, Martha Yifiru Tachbelie, Million
    Meshesha, Solomon Atinafu, Wondwossen Mulugeta, Yaregal Assabie, Hafte Abera,
    Binyam Ephrem, Tewodros Abebe, Wondimagegnhue Tsegaye, Amanuel Lemma, Tsegaye
    Andargie, Seifedin Shifaw
  bibkey: abate-etal-2018-parallel
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3102'
  page_last: '3111'
  pages: "3102\u20133111"
  paper_id: '262'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1262.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1262.jpg
  title: Parallel Corpora for bi-lingual English-Ethiopian Languages Statistical Machine
    Translation
  title_html: Parallel Corpora for bi-lingual <span class="acl-fixed-case">E</span>nglish-<span
    class="acl-fixed-case">E</span>thiopian Languages Statistical Machine Translation
  url: https://www.aclweb.org/anthology/C18-1262
  year: '2018'
C18-1263:
  abstract: Multilingual machine translation addresses the task of translating between
    multiple source and target languages. We propose task-specific attention models,
    a simple but effective technique for improving the quality of sequence-to-sequence
    neural multilingual translation. Our approach seeks to retain as much of the parameter
    sharing generalization of NMT models as possible, while still allowing for language-specific
    specialization of the attention model to a particular language-pair or task. Our
    experiments on four languages of the Europarl corpus show that using a target-specific
    model of attention provides consistent gains in translation quality for all possible
    translation directions, compared to a model in which all parameters are shared.
    We observe improved translation quality even in the (extreme) low-resource zero-shot
    translation directions for which the model never saw explicitly paired parallel
    data.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Graeme
    full: Graeme Blackwood
    id: graeme-blackwood
    last: Blackwood
  - first: Miguel
    full: Miguel Ballesteros
    id: miguel-ballesteros
    last: Ballesteros
  - first: Todd
    full: Todd Ward
    id: todd-ward
    last: Ward
  author_string: Graeme Blackwood, Miguel Ballesteros, Todd Ward
  bibkey: blackwood-etal-2018-multilingual
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3112'
  page_last: '3122'
  pages: "3112\u20133122"
  paper_id: '263'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1263.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1263.jpg
  title: Multilingual Neural Machine Translation with Task-Specific Attention
  title_html: Multilingual Neural Machine Translation with Task-Specific Attention
  url: https://www.aclweb.org/anthology/C18-1263
  year: '2018'
C18-1264:
  abstract: Methods for automated cognate detection in historical linguistics invariably
    build on some measure of form similarity which is designed to capture the remaining
    systematic similarities between cognate word forms after thousands of years of
    divergence. A wide range of clustering and classification algorithms has been
    explored for the purpose, whereas possible improvements on the level of pairwise
    form similarity measures have not been the main focus of research. The approach
    presented in this paper improves on this core component of cognate detection systems
    by a novel combination of information weighting, a technique for putting less
    weight on reoccurring morphological material, with sound correspondence modeling
    by means of pointwise mutual information. In evaluations on expert cognacy judgments
    over a subset of the IPA-encoded NorthEuraLex database, the combination of both
    techniques is shown to lead to considerable improvements in average precision
    for binary cognate detection, and modest improvements for distance-based cognate
    clustering.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Johannes
    full: Johannes Dellert
    id: johannes-dellert
    last: Dellert
  author_string: Johannes Dellert
  bibkey: dellert-2018-combining
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3123'
  page_last: '3133'
  pages: "3123\u20133133"
  paper_id: '264'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1264.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1264.jpg
  title: Combining Information-Weighted Sequence Alignment and Sound Correspondence
    Models for Improved Cognate Detection
  title_html: Combining Information-Weighted Sequence Alignment and Sound Correspondence
    Models for Improved Cognate Detection
  url: https://www.aclweb.org/anthology/C18-1264
  year: '2018'
C18-1265:
  abstract: A morphologically complex word (MCW) is a hierarchical constituent with
    meaning-preserving subunits, so word-based models which rely on surface forms
    might not be powerful enough to translate such structures. When translating from
    morphologically rich languages (MRLs), a source word could be mapped to several
    words or even a full sentence on the target side, which means an MCW should not
    be treated as an atomic unit. In order to provide better translations for MRLs,
    we boost the existing neural machine translation (NMT) architecture with a double-
    channel encoder and a double-attentive decoder. The main goal targeted in this
    research is to provide richer information on the encoder side and redesign the
    decoder accordingly to benefit from such information. Our experimental results
    demonstrate that we could achieve our goal as the proposed model outperforms existing
    subword- and character-based architectures and showed significant improvements
    on translating from German, Russian, and Turkish into English.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Peyman
    full: Peyman Passban
    id: peyman-passban
    last: Passban
  - first: Andy
    full: Andy Way
    id: andy-way
    last: Way
  - first: Qun
    full: Qun Liu
    id: qun-liu
    last: Liu
  author_string: Peyman Passban, Andy Way, Qun Liu
  bibkey: passban-etal-2018-tailoring
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3134'
  page_last: '3145'
  pages: "3134\u20133145"
  paper_id: '265'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1265.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1265.jpg
  title: Tailoring Neural Architectures for Translating from Morphologically Rich
    Languages
  title_html: Tailoring Neural Architectures for Translating from Morphologically
    Rich Languages
  url: https://www.aclweb.org/anthology/C18-1265
  year: '2018'
C18-1266:
  abstract: 'Predicting Machine Translation (MT) quality can help in many practical
    tasks such as MT post-editing. The performance of Quality Estimation (QE) methods
    has drastically improved recently with the introduction of neural approaches to
    the problem. However, thus far neural approaches have only been designed for word
    and sentence-level prediction. We present a neural framework that is able to accommodate
    neural QE approaches at these fine-grained levels and generalize them to the level
    of documents. We test the framework with two sentence-level neural QE approaches:
    a state of the art approach that requires extensive pre-training, and a new light-weight
    approach that we propose, which employs basic encoders. Our approach is significantly
    faster and yields performance improvements for a range of document-level quality
    estimation tasks. To our knowledge, this is the first neural architecture for
    document-level QE. In addition, for the first time we apply QE models to the output
    of both statistical and neural MT systems for a series of European languages and
    highlight the new challenges resulting from the use of neural MT.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Julia
    full: Julia Ive
    id: julia-ive
    last: Ive
  - first: "Fr\xE9d\xE9ric"
    full: "Fr\xE9d\xE9ric Blain"
    id: frederic-blain
    last: Blain
  - first: Lucia
    full: Lucia Specia
    id: lucia-specia
    last: Specia
  author_string: "Julia Ive, Fr\xE9d\xE9ric Blain, Lucia Specia"
  bibkey: ive-etal-2018-deepquest
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3146'
  page_last: '3157'
  pages: "3146\u20133157"
  paper_id: '266'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1266.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1266.jpg
  title: 'deepQuest: A Framework for Neural-based Quality Estimation'
  title_html: 'deep<span class="acl-fixed-case">Q</span>uest: A Framework for Neural-based
    Quality Estimation'
  url: https://www.aclweb.org/anthology/C18-1266
  year: '2018'
C18-1267:
  abstract: Knowing the state-of-the-art for a particular task is an essential component
    of any computational linguistics investigation. But can we be truly confident
    that the current state-of-the-art is indeed the best performing model? In this
    paper, we study the case of frame semantic parsing, a well-established task with
    multiple shared datasets. We show that in spite of all the care taken to provide
    a standard evaluation resource, small variations in data processing can have dramatic
    consequences for ranking parser performance. This leads us to propose an open-source
    standardized processing pipeline, which can be shared and reused for robust model
    comparison.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Alexandre
    full: Alexandre Kabbach
    id: alexandre-kabbach
    last: Kabbach
  - first: Corentin
    full: Corentin Ribeyre
    id: corentin-ribeyre
    last: Ribeyre
  - first: "Aur\xE9lie"
    full: "Aur\xE9lie Herbelot"
    id: aurelie-herbelot
    last: Herbelot
  author_string: "Alexandre Kabbach, Corentin Ribeyre, Aur\xE9lie Herbelot"
  bibkey: kabbach-etal-2018-butterfly
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3158'
  page_last: '3169'
  pages: "3158\u20133169"
  paper_id: '267'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1267.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1267.jpg
  title: 'Butterfly Effects in Frame Semantic Parsing: impact of data processing on
    model ranking'
  title_html: 'Butterfly Effects in Frame Semantic Parsing: impact of data processing
    on model ranking'
  url: https://www.aclweb.org/anthology/C18-1267
  year: '2018'
C18-1268:
  abstract: We present a variation of the incremental and memory-limited algorithm
    in (Sadeghi et al., 2017) for Bayesian cross-situational word learning and evaluate
    the model in terms of its functional performance and its sensitivity to input
    order. We show that the functional performance of our sub-optimal model on corpus
    data is close to that of its optimal counterpart (Frank et al., 2009), while only
    the sub-optimal model is capable of predicting the input order effects reported
    in experimental studies.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Sepideh
    full: Sepideh Sadeghi
    id: sepideh-sadeghi
    last: Sadeghi
  - first: Matthias
    full: Matthias Scheutz
    id: matthias-scheutz
    last: Scheutz
  author_string: Sepideh Sadeghi, Matthias Scheutz
  bibkey: sadeghi-scheutz-2018-sensitivity
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3170'
  page_last: '3180'
  pages: "3170\u20133180"
  paper_id: '268'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1268.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1268.jpg
  title: 'Sensitivity to Input Order: Evaluation of an Incremental and Memory-Limited
    Bayesian Cross-Situational Word Learning Model'
  title_html: 'Sensitivity to Input Order: Evaluation of an Incremental and Memory-Limited
    <span class="acl-fixed-case">B</span>ayesian Cross-Situational Word Learning Model'
  url: https://www.aclweb.org/anthology/C18-1268
  year: '2018'
C18-1269:
  abstract: In this paper, we propose a new sentence weighting method for the domain
    adaptation of neural machine translation. We introduce a domain similarity metric
    to evaluate the relevance between a sentence and an available entire domain dataset.
    The similarity of each sentence to the target domain is calculated with various
    methods. The computed similarity is then integrated into the training objective
    to weight sentences. The adaptation results on both IWSLT Chinese-English TED
    task and a task with only synthetic training parallel data show that our sentence
    weighting method is able to achieve an significant improvement over strong baselines.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Shiqi
    full: Shiqi Zhang
    id: shiqi-zhang
    last: Zhang
  - first: Deyi
    full: Deyi Xiong
    id: deyi-xiong
    last: Xiong
  author_string: Shiqi Zhang, Deyi Xiong
  bibkey: zhang-xiong-2018-sentence
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3181'
  page_last: '3190'
  pages: "3181\u20133190"
  paper_id: '269'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1269.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1269.jpg
  title: Sentence Weighting for Neural Machine Translation Domain Adaptation
  title_html: Sentence Weighting for Neural Machine Translation Domain Adaptation
  url: https://www.aclweb.org/anthology/C18-1269
  year: '2018'
C18-1270:
  abstract: 'Not all dependencies are equal when training a dependency parser: some
    are straightforward enough to be learned with only a sample of data, others embed
    more complexity. This work introduces a series of metrics to quantify those differences,
    and thereby to expose the shortcomings of various parsing algorithms and strategies.
    Apart from a more thorough comparison of parsing systems, these new tools also
    prove useful for characterizing the information conveyed by cross-lingual parsers,
    in a quantitative but still interpretable way.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Lauriane
    full: Lauriane Aufrant
    id: lauriane-aufrant
    last: Aufrant
  - first: Guillaume
    full: Guillaume Wisniewski
    id: guillaume-wisniewski
    last: Wisniewski
  - first: "Fran\xE7ois"
    full: "Fran\xE7ois Yvon"
    id: francois-yvon
    last: Yvon
  author_string: "Lauriane Aufrant, Guillaume Wisniewski, Fran\xE7ois Yvon"
  bibkey: aufrant-etal-2018-quantifying
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3191'
  page_last: '3202'
  pages: "3191\u20133202"
  paper_id: '270'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1270.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1270.jpg
  title: Quantifying training challenges of dependency parsers
  title_html: Quantifying training challenges of dependency parsers
  url: https://www.aclweb.org/anthology/C18-1270
  year: '2018'
C18-1271:
  abstract: This paper presents a sequence to sequence (seq2seq) dependency parser
    by directly predicting the relative position of head for each given word, which
    therefore results in a truly end-to-end seq2seq dependency parser for the first
    time. Enjoying the advantage of seq2seq modeling, we enrich a series of embedding
    enhancement, including firstly introduced subword and node2vec augmentation. Meanwhile,
    we propose a beam search decoder with tree constraint and subroot decomposition
    over the sequence to furthermore enhance our seq2seq parser. Our parser is evaluated
    on benchmark treebanks, being on par with the state-of-the-art parsers by achieving
    94.11% UAS on PTB and 88.78% UAS on CTB, respectively.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Zuchao
    full: Zuchao Li
    id: zuchao-li
    last: Li
  - first: Jiaxun
    full: Jiaxun Cai
    id: jiaxun-cai
    last: Cai
  - first: Shexia
    full: Shexia He
    id: shexia-he
    last: He
  - first: Hai
    full: Hai Zhao
    id: hai-zhao
    last: Zhao
  author_string: Zuchao Li, Jiaxun Cai, Shexia He, Hai Zhao
  bibkey: li-etal-2018-seq2seq
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3203'
  page_last: '3214'
  pages: "3203\u20133214"
  paper_id: '271'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1271.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1271.jpg
  title: Seq2seq Dependency Parsing
  title_html: Seq2seq Dependency Parsing
  url: https://www.aclweb.org/anthology/C18-1271
  year: '2018'
C18-1272:
  abstract: Hierarchical Multiscale LSTM (Chung et. al., 2016) is a state-of-the-art
    language model that learns interpretable structure from character-level input.
    Such models can provide fertile ground for (cognitive) computational linguistics
    studies. However, the high complexity of the architecture, training and implementations
    might hinder its applicability. We provide a detailed reproduction and ablation
    study of the architecture, shedding light on some of the potential caveats of
    re-purposing complex deep-learning architectures. We further show that simplifying
    certain aspects of the architecture can in fact improve its performance. We also
    investigate the linguistic units (segments) learned by various levels of the model,
    and argue that their quality does not correlate with the overall performance of
    the model on language modeling.
  address: Santa Fe, New Mexico, USA
  author:
  - first: "\xC1kos"
    full: "\xC1kos K\xE1d\xE1r"
    id: akos-kadar
    last: "K\xE1d\xE1r"
  - first: Marc-Alexandre
    full: "Marc-Alexandre C\xF4t\xE9"
    id: marc-alexandre-cote
    last: "C\xF4t\xE9"
  - first: Grzegorz
    full: "Grzegorz Chrupa\u0142a"
    id: grzegorz-chrupala
    last: "Chrupa\u0142a"
  - first: Afra
    full: Afra Alishahi
    id: afra-alishahi
    last: Alishahi
  author_string: "\xC1kos K\xE1d\xE1r, Marc-Alexandre C\xF4t\xE9, Grzegorz Chrupa\u0142\
    a, Afra Alishahi"
  bibkey: kadar-etal-2018-revisiting
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3215'
  page_last: '3227'
  pages: "3215\u20133227"
  paper_id: '272'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1272.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1272.jpg
  title: Revisiting the Hierarchical Multiscale LSTM
  title_html: Revisiting the Hierarchical Multiscale <span class="acl-fixed-case">LSTM</span>
  url: https://www.aclweb.org/anthology/C18-1272
  year: '2018'
C18-1273:
  abstract: Generating character-level features is an important step for achieving
    good results in various natural language processing tasks. To alleviate the need
    for human labor in generating hand-crafted features, methods that utilize neural
    architectures such as Convolutional Neural Network (CNN) or Recurrent Neural Network
    (RNN) to automatically extract such features have been proposed and have shown
    great results. However, CNN generates position-independent features, and RNN is
    slow since it needs to process the characters sequentially. In this paper, we
    propose a novel method of using a densely connected network to automatically extract
    character-level features. The proposed method does not require any language or
    task specific assumptions, and shows robustness and effectiveness while being
    faster than CNN- or RNN-based methods. Evaluating this method on three sequence
    labeling tasks - slot tagging, Part-of-Speech (POS) tagging, and Named-Entity
    Recognition (NER) - we obtain state-of-the-art performance with a 96.62 F1-score
    and 97.73% accuracy on slot tagging and POS tagging, respectively, and comparable
    performance to the state-of-the-art 91.13 F1-score on NER.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Chanhee
    full: Chanhee Lee
    id: chanhee-lee
    last: Lee
  - first: Young-Bum
    full: Young-Bum Kim
    id: young-bum-kim
    last: Kim
  - first: Dongyub
    full: Dongyub Lee
    id: dongyub-lee
    last: Lee
  - first: Heuiseok
    full: Heuiseok Lim
    id: heui-seok-lim
    last: Lim
  author_string: Chanhee Lee, Young-Bum Kim, Dongyub Lee, Heuiseok Lim
  bibkey: lee-etal-2018-character
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3228'
  page_last: '3239'
  pages: "3228\u20133239"
  paper_id: '273'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1273.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1273.jpg
  title: Character-Level Feature Extraction with Densely Connected Networks
  title_html: Character-Level Feature Extraction with Densely Connected Networks
  url: https://www.aclweb.org/anthology/C18-1273
  year: '2018'
C18-1274:
  abstract: "This study proposes a new neural machine translation (NMT) model based\
    \ on the encoder-decoder model that incorporates named entity (NE) tags of source-language\
    \ sentences. Conventional NMT models have two problems enumerated as follows:\
    \ (i) they tend to have difficulty in translating words with multiple meanings\
    \ because of the high ambiguity, and (ii) these models\u2019abilitytotranslatecompoundwordsseemschallengingbecausetheencoderreceivesaword,\
    \ a part of the compound word, at each time step. To alleviate these problems,\
    \ the encoder of the proposed model encodes the input word on the basis of its\
    \ NE tag at each time step, which could reduce the ambiguity of the input word.\
    \ Furthermore,the encoder introduces a chunk-level LSTM layer over a word-level\
    \ LSTM layer and hierarchically encodes a source-language sentence to capture\
    \ a compound NE as a chunk on the basis of the NE tags. We evaluate the proposed\
    \ model on an English-to-Japanese translation task with the ASPEC, and English-to-Bulgarian\
    \ and English-to-Romanian translation tasks with the Europarl corpus. The evaluation\
    \ results show that the proposed model achieves up to 3.11 point improvement in\
    \ BLEU."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Arata
    full: Arata Ugawa
    id: arata-ugawa
    last: Ugawa
  - first: Akihiro
    full: Akihiro Tamura
    id: akihiro-tamura
    last: Tamura
  - first: Takashi
    full: Takashi Ninomiya
    id: takashi-ninomiya
    last: Ninomiya
  - first: Hiroya
    full: Hiroya Takamura
    id: hiroya-takamura
    last: Takamura
  - first: Manabu
    full: Manabu Okumura
    id: manabu-okumura
    last: Okumura
  author_string: Arata Ugawa, Akihiro Tamura, Takashi Ninomiya, Hiroya Takamura, Manabu
    Okumura
  bibkey: ugawa-etal-2018-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3240'
  page_last: '3250'
  pages: "3240\u20133250"
  paper_id: '274'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1274.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1274.jpg
  title: Neural Machine Translation Incorporating Named Entity
  title_html: Neural Machine Translation Incorporating Named Entity
  url: https://www.aclweb.org/anthology/C18-1274
  year: '2018'
C18-1275:
  abstract: Technical support problems are very complex. In contrast to regular web
    queries (that contain few keywords) or factoid questions (which are a few sentences),
    these problems usually include attributes like a detailed description of what
    is failing (symptom), steps taken in an effort to remediate the failure (activity),
    and sometimes a specific request or ask (intent). Automating support is the task
    of automatically providing answers to these problems given a corpus of solution
    documents. Traditional approaches to this task rely on information retrieval and
    are keyword based; looking for keyword overlap between the question and solution
    documents and ignoring these attributes. We present an approach for semantic parsing
    of technical questions that uses grammatical structure to extract these attributes
    as a baseline, and a CRF based model that can improve performance considerably
    in the presence of annotated data for training. We also demonstrate that combined
    with reasoning, these attributes help outperform retrieval baselines.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Abhirut
    full: Abhirut Gupta
    id: abhirut-gupta
    last: Gupta
  - first: Anupama
    full: Anupama Ray
    id: anupama-ray
    last: Ray
  - first: Gargi
    full: Gargi Dasgupta
    id: gargi-dasgupta
    last: Dasgupta
  - first: Gautam
    full: Gautam Singh
    id: gautam-singh
    last: Singh
  - first: Pooja
    full: Pooja Aggarwal
    id: pooja-aggarwal
    last: Aggarwal
  - first: Prateeti
    full: Prateeti Mohapatra
    id: prateeti-mohapatra
    last: Mohapatra
  author_string: Abhirut Gupta, Anupama Ray, Gargi Dasgupta, Gautam Singh, Pooja Aggarwal,
    Prateeti Mohapatra
  bibkey: gupta-etal-2018-semantic
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3251'
  page_last: '3259'
  pages: "3251\u20133259"
  paper_id: '275'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1275.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1275.jpg
  title: Semantic Parsing for Technical Support Questions
  title_html: Semantic Parsing for Technical Support Questions
  url: https://www.aclweb.org/anthology/C18-1275
  year: '2018'
C18-1276:
  abstract: A great proportion of sequence-to-sequence (Seq2Seq) models for Neural
    Machine Translation (NMT) adopt Recurrent Neural Network (RNN) to generate translation
    word by word following a sequential order. As the studies of linguistics have
    proved that language is not linear word sequence but sequence of complex structure,
    translation at each step should be conditioned on the whole target-side context.
    To tackle the problem, we propose a new NMT model that decodes the sequence with
    the guidance of its structural prediction of the context of the target sequence.
    Our model generates translation based on the structural prediction of the target-side
    context so that the translation can be freed from the bind of sequential order.
    Experimental results demonstrate that our model is more competitive compared with
    the state-of-the-art methods, and the analysis reflects that our model is also
    robust to translating sentences of different lengths and it also reduces repetition
    with the instruction from the target-side context for decoding.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Junyang
    full: Junyang Lin
    id: junyang-lin
    last: Lin
  - first: Xu
    full: Xu Sun
    id: xu-sun
    last: Sun
  - first: Xuancheng
    full: Xuancheng Ren
    id: xuancheng-ren
    last: Ren
  - first: Shuming
    full: Shuming Ma
    id: shuming-ma
    last: Ma
  - first: Jinsong
    full: Jinsong Su
    id: jinsong-su
    last: Su
  - first: Qi
    full: Qi Su
    id: qi-su
    last: Su
  author_string: Junyang Lin, Xu Sun, Xuancheng Ren, Shuming Ma, Jinsong Su, Qi Su
  bibkey: lin-etal-2018-deconvolution
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3260'
  page_last: '3271'
  pages: "3260\u20133271"
  paper_id: '276'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1276.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1276.jpg
  title: Deconvolution-Based Global Decoding for Neural Machine Translation
  title_html: Deconvolution-Based Global Decoding for Neural Machine Translation
  url: https://www.aclweb.org/anthology/C18-1276
  year: '2018'
C18-1277:
  abstract: Question Answering over Knowledge Bases (KB-QA), which automatically answer
    natural language questions based on the facts contained by a knowledge base, is
    one of the most important natural language processing (NLP) tasks. Simple questions
    constitute a large part of questions queried on the web, still being a challenge
    to QA systems. In this work, we propose to conduct pattern extraction and entity
    linking first, and put forward pattern revising procedure to mitigate the error
    propagation problem. In order to learn to rank candidate subject-predicate pairs
    to enable the relevant facts retrieval given a question, we propose to do joint
    fact selection enhanced by relation detection. Multi-level encodings and multi-dimension
    information are leveraged to strengthen the whole procedure. The experimental
    results demonstrate that our approach sets a new record in this task, outperforming
    the current state-of-the-art by an absolute large margin.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yanchao
    full: Yanchao Hao
    id: yanchao-hao
    last: Hao
  - first: Hao
    full: Hao Liu
    id: hao-liu
    last: Liu
  - first: Shizhu
    full: Shizhu He
    id: shizhu-he
    last: He
  - first: Kang
    full: Kang Liu
    id: kang-liu
    last: Liu
  - first: Jun
    full: Jun Zhao
    id: jun-zhao
    last: Zhao
  author_string: Yanchao Hao, Hao Liu, Shizhu He, Kang Liu, Jun Zhao
  bibkey: hao-etal-2018-pattern
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3272'
  page_last: '3282'
  pages: "3272\u20133282"
  paper_id: '277'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1277.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1277.jpg
  title: Pattern-revising Enhanced Simple Question Answering over Knowledge Bases
  title_html: Pattern-revising Enhanced Simple Question Answering over Knowledge Bases
  url: https://www.aclweb.org/anthology/C18-1277
  year: '2018'
C18-1278:
  abstract: We present a system for Answer Selection that integrates fine-grained
    Question Classification with a Deep Learning model designed for Answer Selection.
    We detail the necessary changes to the Question Classification taxonomy and system,
    the creation of a new Entity Identification system and methods of highlighting
    entities to achieve this objective. Our experiments show that Question Classes
    are a strong signal to Deep Learning models for Answer Selection, and enable us
    to outperform the current state of the art in all variations of our experiments
    except one. In the best configuration, our MRR and MAP scores outperform the current
    state of the art by between 3 and 5 points on both versions of the TREC Answer
    Selection test set, a standard dataset for this task.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Harish
    full: Harish Tayyar Madabushi
    id: harish-tayyar-madabushi
    last: Tayyar Madabushi
  - first: Mark
    full: Mark Lee
    id: mark-lee
    last: Lee
  - first: John
    full: John Barnden
    id: john-barnden
    last: Barnden
  author_string: Harish Tayyar Madabushi, Mark Lee, John Barnden
  bibkey: tayyar-madabushi-etal-2018-integrating
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3283'
  page_last: '3294'
  pages: "3283\u20133294"
  paper_id: '278'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1278.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1278.jpg
  title: Integrating Question Classification and Deep Learning for improved Answer
    Selection
  title_html: Integrating Question Classification and Deep Learning for improved Answer
    Selection
  url: https://www.aclweb.org/anthology/C18-1278
  year: '2018'
C18-1279:
  abstract: Answer selection is an important but challenging task. Significant progresses
    have been made in domains where a large amount of labeled training data is available.
    However, obtaining rich annotated data is a time-consuming and expensive process,
    creating a substantial barrier for applying answer selection models to a new domain
    which has limited labeled data. In this paper, we propose Knowledge-aware Attentive
    Network (KAN), a transfer learning framework for cross-domain answer selection,
    which uses the knowledge base as a bridge to enable knowledge transfer from the
    source domain to the target domains. Specifically, we design a knowledge module
    to integrate the knowledge-based representational learning into answer selection
    models. The learned knowledge-based representations are shared by source and target
    domains, which not only leverages large amounts of cross-domain data, but also
    benefits from a regularization effect that leads to more general representations
    to help tasks in new domains. To verify the effectiveness of our model, we use
    SQuAD-T dataset as the source domain and three other datasets (i.e., Yahoo QA,
    TREC QA and InsuranceQA) as the target domains. The experimental results demonstrate
    that KAN has remarkable applicability and generality, and consistently outperforms
    the strong competitors by a noticeable margin for cross-domain answer selection.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yang
    full: Yang Deng
    id: yang-deng
    last: Deng
  - first: Ying
    full: Ying Shen
    id: ying-shen
    last: Shen
  - first: Min
    full: Min Yang
    id: min-yang
    last: Yang
  - first: Yaliang
    full: Yaliang Li
    id: yaliang-li
    last: Li
  - first: Nan
    full: Nan Du
    id: nan-du
    last: Du
  - first: Wei
    full: Wei Fan
    id: wei-fan
    last: Fan
  - first: Kai
    full: Kai Lei
    id: kai-lei
    last: Lei
  author_string: Yang Deng, Ying Shen, Min Yang, Yaliang Li, Nan Du, Wei Fan, Kai
    Lei
  bibkey: deng-etal-2018-knowledge
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3295'
  page_last: '3305'
  pages: "3295\u20133305"
  paper_id: '279'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1279.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1279.jpg
  title: 'Knowledge as A Bridge: Improving Cross-domain Answer Selection with External
    Knowledge'
  title_html: 'Knowledge as A Bridge: Improving Cross-domain Answer Selection with
    External Knowledge'
  url: https://www.aclweb.org/anthology/C18-1279
  year: '2018'
C18-1280:
  abstract: 'The most approaches to Knowledge Base Question Answering are based on
    semantic parsing. In this paper, we address the problem of learning vector representations
    for complex semantic parses that consist of multiple entities and relations. Previous
    work largely focused on selecting the correct semantic relations for a question
    and disregarded the structure of the semantic parse: the connections between entities
    and the directions of the relations. We propose to use Gated Graph Neural Networks
    to encode the graph structure of the semantic parse. We show on two data sets
    that the graph networks outperform all baseline models that do not explicitly
    model the structure. The error analysis confirms that our approach can successfully
    process complex semantic parses.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Daniil
    full: Daniil Sorokin
    id: daniil-sorokin
    last: Sorokin
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Daniil Sorokin, Iryna Gurevych
  bibkey: sorokin-gurevych-2018-modeling
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3306'
  page_last: '3317'
  pages: "3306\u20133317"
  paper_id: '280'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1280.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1280.jpg
  title: Modeling Semantics with Gated Graph Neural Networks for Knowledge Base Question
    Answering
  title_html: Modeling Semantics with Gated Graph Neural Networks for Knowledge Base
    Question Answering
  url: https://www.aclweb.org/anthology/C18-1280
  year: '2018'
C18-1281:
  abstract: Human evaluations are broadly thought to be more valuable the higher the
    inter-annotator agreement. In this paper we examine this idea. We will describe
    our experiments and analysis within the area of Automatic Question Generation.
    Our experiments show how annotators diverge in language annotation tasks due to
    a range of ineliminable factors. For this reason, we believe that annotation schemes
    for natural language generation tasks that are aimed at evaluating language quality
    need to be treated with great care. In particular, an unchecked focus on reduction
    of disagreement among annotators runs the danger of creating generation goals
    that reward output that is more distant from, rather than closer to, natural human-like
    language. We conclude the paper by suggesting a new approach to the use of the
    agreement metrics in natural language generation evaluation tasks.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Jacopo
    full: Jacopo Amidei
    id: jacopo-amidei
    last: Amidei
  - first: Paul
    full: Paul Piwek
    id: paul-piwek
    last: Piwek
  - first: Alistair
    full: Alistair Willis
    id: alistair-willis
    last: Willis
  author_string: Jacopo Amidei, Paul Piwek, Alistair Willis
  bibkey: amidei-etal-2018-rethinking
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3318'
  page_last: '3329'
  pages: "3318\u20133329"
  paper_id: '281'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1281.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1281.jpg
  title: Rethinking the Agreement in Human Evaluation Tasks
  title_html: Rethinking the Agreement in Human Evaluation Tasks
  url: https://www.aclweb.org/anthology/C18-1281
  year: '2018'
C18-1282:
  abstract: "We present a novel deep learning architecture to address the cloze-style\
    \ question answering task. Existing approaches employ reading mechanisms that\
    \ do not fully exploit the interdependency between the document and the query.\
    \ In this paper, we propose a novel dependent gated reading bidirectional GRU\
    \ network (DGR) to efficiently model the relationship between the document and\
    \ the query during encoding and decision making. Our evaluation shows that DGR\
    \ obtains highly competitive performance on well-known machine comprehension benchmarks\
    \ such as the Children\u2019s Book Test (CBT-NE and CBT-CN) and Who DiD What (WDW,\
    \ Strict and Relaxed). Finally, we extensively analyze and validate our model\
    \ by ablation and attention studies."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Reza
    full: Reza Ghaeini
    id: reza-ghaeini
    last: Ghaeini
  - first: Xiaoli
    full: Xiaoli Fern
    id: xiaoli-fern
    last: Fern
  - first: Hamed
    full: Hamed Shahbazi
    id: hamed-shahbazi
    last: Shahbazi
  - first: Prasad
    full: Prasad Tadepalli
    id: prasad-tadepalli
    last: Tadepalli
  author_string: Reza Ghaeini, Xiaoli Fern, Hamed Shahbazi, Prasad Tadepalli
  bibkey: ghaeini-etal-2018-dependent
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3330'
  page_last: '3345'
  pages: "3330\u20133345"
  paper_id: '282'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1282.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1282.jpg
  title: Dependent Gated Reading for Cloze-Style Question Answering
  title_html: Dependent Gated Reading for Cloze-Style Question Answering
  url: https://www.aclweb.org/anthology/C18-1282
  year: '2018'
C18-1283:
  abstract: The recently increased focus on misinformation has stimulated research
    in fact checking, the task of assessing the truthfulness of a claim. Research
    in automating this task has been conducted in a variety of disciplines including
    natural language processing, machine learning, knowledge representation, databases,
    and journalism. While there has been substantial progress, relevant papers and
    articles have been published in research communities that are often unaware of
    each other and use inconsistent terminology, thus impeding understanding and further
    progress. In this paper we survey automated fact checking research stemming from
    natural language processing and related disciplines, unifying the task formulations
    and methodologies across papers and authors. Furthermore, we highlight the use
    of evidence as an important distinguishing factor among them cutting across task
    formulations and methods. We conclude with proposing avenues for future NLP research
    on automated fact checking.
  address: Santa Fe, New Mexico, USA
  author:
  - first: James
    full: James Thorne
    id: james-thorne
    last: Thorne
  - first: Andreas
    full: Andreas Vlachos
    id: andreas-vlachos
    last: Vlachos
  author_string: James Thorne, Andreas Vlachos
  bibkey: thorne-vlachos-2018-automated
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3346'
  page_last: '3359'
  pages: "3346\u20133359"
  paper_id: '283'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1283.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1283.jpg
  title: 'Automated Fact Checking: Task Formulations, Methods and Future Directions'
  title_html: 'Automated Fact Checking: Task Formulations, Methods and Future Directions'
  url: https://www.aclweb.org/anthology/C18-1283
  year: '2018'
C18-1284:
  abstract: "Prior manual studies of rumours suggested that crowd stance can give\
    \ insights into the actual rumour veracity. Even though numerous studies of automatic\
    \ veracity classification of social media rumours have been carried out, none\
    \ explored the effectiveness of leveraging crowd stance to determine veracity.\
    \ We use stance as an additional feature to those commonly used in earlier studies.\
    \ We also model the veracity of a rumour using variants of Hidden Markov Models\
    \ (HMM) and the collective stance information. This paper demonstrates that HMMs\
    \ that use stance and tweets\u2019 times as the only features for modelling true\
    \ and false rumours achieve F1 scores in the range of 80%, outperforming those\
    \ approaches where stance is used jointly with content and user based features."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Sebastian
    full: Sebastian Dungs
    id: sebastian-dungs
    last: Dungs
  - first: Ahmet
    full: Ahmet Aker
    id: ahmet-aker
    last: Aker
  - first: Norbert
    full: Norbert Fuhr
    id: norbert-fuhr
    last: Fuhr
  - first: Kalina
    full: Kalina Bontcheva
    id: kalina-bontcheva
    last: Bontcheva
  author_string: Sebastian Dungs, Ahmet Aker, Norbert Fuhr, Kalina Bontcheva
  bibkey: dungs-etal-2018-rumour
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3360'
  page_last: '3370'
  pages: "3360\u20133370"
  paper_id: '284'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1284.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1284.jpg
  title: Can Rumour Stance Alone Predict Veracity?
  title_html: Can Rumour Stance Alone Predict Veracity?
  url: https://www.aclweb.org/anthology/C18-1284
  year: '2018'
C18-1285:
  abstract: Satirical news detection is important in order to prevent the spread of
    misinformation over the Internet. Existing approaches to capture news satire use
    machine learning models such as SVM and hierarchical neural networks along with
    hand-engineered features, but do not explore sentence and document difference.
    This paper proposes a robust, hierarchical deep neural network approach for satire
    detection, which is capable of capturing satire both at the sentence level and
    at the document level. The architecture incorporates pluggable generic neural
    networks like CNN, GRU, and LSTM. Experimental results on real world news satire
    dataset show substantial performance gains demonstrating the effectiveness of
    our proposed approach. An inspection of the learned models reveals the existence
    of key sentences that control the presence of satire in news.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Sohan
    full: Sohan De Sarkar
    id: sohan-de-sarkar
    last: De Sarkar
  - first: Fan
    full: Fan Yang
    id: fan-yang
    last: Yang
  - first: Arjun
    full: Arjun Mukherjee
    id: arjun-mukherjee
    last: Mukherjee
  author_string: Sohan De Sarkar, Fan Yang, Arjun Mukherjee
  bibkey: de-sarkar-etal-2018-attending
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3371'
  page_last: '3380'
  pages: "3371\u20133380"
  paper_id: '285'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1285.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1285.jpg
  title: Attending Sentences to detect Satirical Fake News
  title_html: Attending Sentences to detect Satirical Fake News
  url: https://www.aclweb.org/anthology/C18-1285
  year: '2018'
C18-1286:
  abstract: "Social media provide platforms to express, discuss, and shape opinions\
    \ about events and issues in the real world. An important step to analyze the\
    \ discussions on social media and to assist in healthy decision-making is stance\
    \ detection. This paper presents an approach to detect the stance of a user toward\
    \ a topic based on their stances toward other topics and the social media posts\
    \ of the user. We apply factorization machines, a widely used method in item recommendation,\
    \ to model user preferences toward topics from the social media data. The experimental\
    \ results demonstrate that users\u2019 posts are useful to model topic preferences\
    \ and therefore predict stances of silent users."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Akira
    full: Akira Sasaki
    id: akira-sasaki
    last: Sasaki
  - first: Kazuaki
    full: Kazuaki Hanawa
    id: kazuaki-hanawa
    last: Hanawa
  - first: Naoaki
    full: Naoaki Okazaki
    id: naoaki-okazaki
    last: Okazaki
  - first: Kentaro
    full: Kentaro Inui
    id: kentaro-inui
    last: Inui
  author_string: Akira Sasaki, Kazuaki Hanawa, Naoaki Okazaki, Kentaro Inui
  bibkey: sasaki-etal-2018-predicting
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3381'
  page_last: '3390'
  pages: "3381\u20133390"
  paper_id: '286'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1286.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1286.jpg
  title: Predicting Stances from Social Media Posts using Factorization Machines
  title_html: Predicting Stances from Social Media Posts using Factorization Machines
  url: https://www.aclweb.org/anthology/C18-1286
  year: '2018'
C18-1287:
  abstract: The proliferation of misleading information in everyday access media outlets
    such as social media feeds, news blogs, and online newspapers have made it challenging
    to identify trustworthy news sources, thus increasing the need for computational
    tools able to provide insights into the reliability of online content. In this
    paper, we focus on the automatic identification of fake content in online news.
    Our contribution is twofold. First, we introduce two novel datasets for the task
    of fake news detection, covering seven different news domains. We describe the
    collection, annotation, and validation process in detail and present several exploratory
    analyses on the identification of linguistic differences in fake and legitimate
    news content. Second, we conduct a set of learning experiments to build accurate
    fake news detectors, and show that we can achieve accuracies of up to 76%. In
    addition, we provide comparative analyses of the automatic and manual identification
    of fake news.
  address: Santa Fe, New Mexico, USA
  author:
  - first: "Ver\xF3nica"
    full: "Ver\xF3nica P\xE9rez-Rosas"
    id: veronica-perez-rosas
    last: "P\xE9rez-Rosas"
  - first: Bennett
    full: Bennett Kleinberg
    id: bennett-kleinberg
    last: Kleinberg
  - first: Alexandra
    full: Alexandra Lefevre
    id: alexandra-lefevre
    last: Lefevre
  - first: Rada
    full: Rada Mihalcea
    id: rada-mihalcea
    last: Mihalcea
  author_string: "Ver\xF3nica P\xE9rez-Rosas, Bennett Kleinberg, Alexandra Lefevre,\
    \ Rada Mihalcea"
  bibkey: perez-rosas-etal-2018-automatic
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3391'
  page_last: '3401'
  pages: "3391\u20133401"
  paper_id: '287'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1287.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1287.jpg
  title: Automatic Detection of Fake News
  title_html: Automatic Detection of Fake News
  url: https://www.aclweb.org/anthology/C18-1287
  year: '2018'
C18-1288:
  abstract: Automatic resolution of rumours is a challenging task that can be broken
    down into smaller components that make up a pipeline, including rumour detection,
    rumour tracking and stance classification, leading to the final outcome of determining
    the veracity of a rumour. In previous work, these steps in the process of rumour
    verification have been developed as separate components where the output of one
    feeds into the next. We propose a multi-task learning approach that allows joint
    training of the main and auxiliary tasks, improving the performance of rumour
    verification. We examine the connection between the dataset properties and the
    outcomes of the multi-task learning models used.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Elena
    full: Elena Kochkina
    id: elena-kochkina
    last: Kochkina
  - first: Maria
    full: Maria Liakata
    id: maria-liakata
    last: Liakata
  - first: Arkaitz
    full: Arkaitz Zubiaga
    id: arkaitz-zubiaga
    last: Zubiaga
  author_string: Elena Kochkina, Maria Liakata, Arkaitz Zubiaga
  bibkey: kochkina-etal-2018-one
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3402'
  page_last: '3413'
  pages: "3402\u20133413"
  paper_id: '288'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1288.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1288.jpg
  title: 'All-in-one: Multi-task Learning for Rumour Verification'
  title_html: 'All-in-one: Multi-task Learning for Rumour Verification'
  url: https://www.aclweb.org/anthology/C18-1288
  year: '2018'
C18-1289:
  abstract: Open Information Extraction (OIE) is the task of the unsupervised creation
    of structured information from text. OIE is often used as a starting point for
    a number of downstream tasks including knowledge base construction, relation extraction,
    and question answering. While OIE methods are targeted at being domain independent,
    they have been evaluated primarily on newspaper, encyclopedic or general web text.
    In this article, we evaluate the performance of OIE on scientific texts originating
    from 10 different disciplines. To do so, we use two state-of-the-art OIE systems
    using a crowd-sourcing approach. We find that OIE systems perform significantly
    worse on scientific text than encyclopedic text. We also provide an error analysis
    and suggest areas of work to reduce errors. Our corpus of sentences and judgments
    are made available.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Paul
    full: Paul Groth
    id: paul-groth
    last: Groth
  - first: Mike
    full: Mike Lauruhn
    id: mike-lauruhn
    last: Lauruhn
  - first: Antony
    full: Antony Scerri
    id: antony-scerri
    last: Scerri
  - first: Ron
    full: Ron Daniel Jr.
    id: ron-daniel-jr
    last: Daniel Jr.
  author_string: Paul Groth, Mike Lauruhn, Antony Scerri, Ron Daniel Jr.
  bibkey: groth-etal-2018-open
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3414'
  page_last: '3423'
  pages: "3414\u20133423"
  paper_id: '289'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1289.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1289.jpg
  title: 'Open Information Extraction on Scientific Text: An Evaluation'
  title_html: 'Open Information Extraction on Scientific Text: An Evaluation'
  url: https://www.aclweb.org/anthology/C18-1289
  year: '2018'
C18-1290:
  abstract: Standard word embedding algorithms learn vector representations from large
    corpora of text documents in an unsupervised fashion. However, the quality of
    word embeddings learned from these algorithms is affected by the size of training
    data sets. Thus, applications of these algorithms in domains with only moderate
    amounts of available data is limited. In this paper we introduce an algorithm
    that learns word embeddings jointly with a classifier. Our algorithm is called
    SWESA (Supervised Word Embeddings for Sentiment Analysis). SWESA leverages document
    label information to learn vector representations of words from a modest corpus
    of text documents by solving an optimization problem that minimizes a cost function
    with respect to both word embeddings and the weight vector used for classification.
    Experiments on several real world data sets show that SWESA has superior performance
    on domains with limited data, when compared to previously suggested approaches
    to word embeddings and sentiment analysis tasks.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Prathusha
    full: Prathusha K Sarma
    id: prathusha-kameswara-sarma
    last: K Sarma
  - first: William
    full: William Sethares
    id: william-sethares
    last: Sethares
  author_string: Prathusha K Sarma, William Sethares
  bibkey: k-sarma-sethares-2018-simple
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3424'
  page_last: '3435'
  pages: "3424\u20133435"
  paper_id: '290'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1290.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1290.jpg
  title: Simple Algorithms For Sentiment Analysis On Sentiment Rich, Data Poor Domains.
  title_html: Simple Algorithms For Sentiment Analysis On Sentiment Rich, Data Poor
    Domains.
  url: https://www.aclweb.org/anthology/C18-1290
  year: '2018'
C18-1291:
  abstract: "Unsupervised pre-trained word embeddings are used effectively for many\
    \ tasks in natural language processing to leverage unlabeled textual data. Often\
    \ these embeddings are either used as initializations or as fixed word representations\
    \ for task-specific classification models. In this work, we extend our classification\
    \ model\u2019s task loss with an unsupervised auxiliary loss on the word-embedding\
    \ level of the model. This is to ensure that the learned word representations\
    \ contain both task-specific features, learned from the supervised loss component,\
    \ and more general features learned from the unsupervised loss component. We evaluate\
    \ our approach on the task of temporal relation extraction, in particular, narrative\
    \ containment relation extraction from clinical records, and show that continued\
    \ training of the embeddings on the unsupervised objective together with the task\
    \ objective gives better task-specific embeddings, and results in an improvement\
    \ over the state of the art on the THYME dataset, using only a general-domain\
    \ part-of-speech tagger as linguistic resource."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Artuur
    full: Artuur Leeuwenberg
    id: artuur-leeuwenberg
    last: Leeuwenberg
  - first: Marie-Francine
    full: Marie-Francine Moens
    id: marie-francine-moens
    last: Moens
  author_string: Artuur Leeuwenberg, Marie-Francine Moens
  bibkey: leeuwenberg-moens-2018-word
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3436'
  page_last: '3447'
  pages: "3436\u20133447"
  paper_id: '291'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1291.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1291.jpg
  title: Word-Level Loss Extensions for Neural Temporal Relation Classification
  title_html: Word-Level Loss Extensions for Neural Temporal Relation Classification
  url: https://www.aclweb.org/anthology/C18-1291
  year: '2018'
C18-1292:
  abstract: This paper describes a personalized text retrieval algorithm that helps
    language learners select the most suitable reading material in terms of vocabulary
    complexity. The user first rates their knowledge of a small set of words, chosen
    by a graph-based active learning model. The system trains a complex word identification
    model on this set, and then applies the model to find texts that contain the desired
    proportion of new, challenging, and familiar vocabulary. In an evaluation on learners
    of Chinese as a foreign language, we show that this algorithm is effective in
    identifying simpler texts for low-proficiency learners, and more challenging ones
    for high-proficiency learners.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Chak Yan
    full: Chak Yan Yeung
    id: chak-yan-yeung
    last: Yeung
  - first: John
    full: John Lee
    id: john-s-y-lee
    last: Lee
  author_string: Chak Yan Yeung, John Lee
  bibkey: yeung-lee-2018-personalized
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3448'
  page_last: '3455'
  pages: "3448\u20133455"
  paper_id: '292'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1292.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1292.jpg
  title: Personalized Text Retrieval for Learners of Chinese as a Foreign Language
  title_html: Personalized Text Retrieval for Learners of <span class="acl-fixed-case">C</span>hinese
    as a Foreign Language
  url: https://www.aclweb.org/anthology/C18-1292
  year: '2018'
C18-1293:
  abstract: 'In this paper, we describe experiments designed to explore and evaluate
    the impact of punctuation marks on the task of native language identification.
    Punctuation is specific to each language, and is part of the indicators that overtly
    represent the manner in which each language organizes and conveys information.
    Our experiments are organized in various set-ups: the usual multi-class classification
    for individual languages, also considering classification by language groups,
    across different proficiency levels, topics and even cross-corpus. The results
    support our hypothesis that punctuation marks are persistent and robust indicators
    of the native language of the author, which do not diminish in influence even
    when a high proficiency level in a non-native language is achieved.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Ilia
    full: Ilia Markov
    id: ilia-markov
    last: Markov
  - first: Vivi
    full: Vivi Nastase
    id: vivi-nastase
    last: Nastase
  - first: Carlo
    full: Carlo Strapparava
    id: carlo-strapparava
    last: Strapparava
  author_string: Ilia Markov, Vivi Nastase, Carlo Strapparava
  bibkey: markov-etal-2018-punctuation
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3456'
  page_last: '3466'
  pages: "3456\u20133466"
  paper_id: '293'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1293.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1293.jpg
  title: Punctuation as Native Language Interference
  title_html: Punctuation as Native Language Interference
  url: https://www.aclweb.org/anthology/C18-1293
  year: '2018'
C18-1294:
  abstract: 'The literature frequently addresses the differences in receptive and
    productive vocabulary, but grammar is often left unacknowledged in second language
    acquisition studies. In this paper, we used two corpora to investigate the divergences
    in the behavior of pedagogically relevant grammatical structures in reception
    and production texts. We further improved the divergence scores observed in this
    investigation by setting a polarity to them that indicates whether there is overuse
    or underuse of a grammatical structure by language learners. This led to the compilation
    of a language profile that was later combined with vocabulary and readability
    features for classifying reception and production texts in three classes: beginner,
    intermediate, and advanced. The results of the automatic classification task in
    both production (0.872 of F-measure) and reception (0.942 of F-measure) were comparable
    to the current state of the art. We also attempted to automatically attribute
    a score to texts produced by learners, and the correlation results were encouraging,
    but there is still a good amount of room for improvement in this task. The developed
    language profile will serve as input for a system that helps language learners
    to activate more of their passive knowledge in writing texts.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Leonardo
    full: Leonardo Zilio
    id: leonardo-zilio
    last: Zilio
  - first: Rodrigo
    full: Rodrigo Wilkens
    id: rodrigo-wilkens
    last: Wilkens
  - first: "C\xE9drick"
    full: "C\xE9drick Fairon"
    id: cedrick-fairon
    last: Fairon
  author_string: "Leonardo Zilio, Rodrigo Wilkens, C\xE9drick Fairon"
  bibkey: zilio-etal-2018-investigating
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3467'
  page_last: '3478'
  pages: "3467\u20133478"
  paper_id: '294'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1294.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1294.jpg
  title: 'Investigating Productive and Receptive Knowledge: A Profile for Second Language
    Learning'
  title_html: 'Investigating Productive and Receptive Knowledge: A Profile for Second
    Language Learning'
  url: https://www.aclweb.org/anthology/C18-1294
  year: '2018'
C18-1295:
  abstract: A paraphrase is a restatement of the meaning of a text in other words.
    Paraphrases have been studied to enhance the performance of many natural language
    processing tasks. In this paper, we propose a novel task iParaphrasing to extract
    visually grounded paraphrases (VGPs), which are different phrasal expressions
    describing the same visual concept in an image. These extracted VGPs have the
    potential to improve language and image multimodal tasks such as visual question
    answering and image captioning. How to model the similarity between VGPs is the
    key of iParaphrasing. We apply various existing methods as well as propose a novel
    neural network-based method with image attention, and report the results of the
    first attempt toward iParaphrasing.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Chenhui
    full: Chenhui Chu
    id: chenhui-chu
    last: Chu
  - first: Mayu
    full: Mayu Otani
    id: mayu-otani
    last: Otani
  - first: Yuta
    full: Yuta Nakashima
    id: yuta-nakashima
    last: Nakashima
  author_string: Chenhui Chu, Mayu Otani, Yuta Nakashima
  bibkey: chu-etal-2018-iparaphrasing
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3479'
  page_last: '3492'
  pages: "3479\u20133492"
  paper_id: '295'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1295.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1295.jpg
  title: 'iParaphrasing: Extracting Visually Grounded Paraphrases via an Image'
  title_html: 'i<span class="acl-fixed-case">P</span>araphrasing: Extracting Visually
    Grounded Paraphrases via an Image'
  url: https://www.aclweb.org/anthology/C18-1295
  year: '2018'
C18-1296:
  abstract: In view of the differences between the annotations of micro and macro
    discourse rela-tionships, this paper describes the relevant experiments on the
    construction of the Macro Chinese Discourse Treebank (MCDTB), a higher-level Chinese
    discourse corpus. Fol-lowing RST (Rhetorical Structure Theory), we annotate the
    macro discourse information, including discourse structure, nuclearity and relationship,
    and the additional discourse information, including topic sentences, lead and
    abstract, to make the macro discourse annotation more objective and accurate.
    Finally, we annotated 720 articles with a Kappa value greater than 0.6. Preliminary
    experiments on this corpus verify the computability of MCDTB.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Feng
    full: Feng Jiang
    id: feng-jiang
    last: Jiang
  - first: Sheng
    full: Sheng Xu
    id: sheng-xu
    last: Xu
  - first: Xiaomin
    full: Xiaomin Chu
    id: xiaomin-chu
    last: Chu
  - first: Peifeng
    full: Peifeng Li
    id: peifeng-li
    last: Li
  - first: Qiaoming
    full: Qiaoming Zhu
    id: qiaoming-zhu
    last: Zhu
  - first: Guodong
    full: Guodong Zhou
    id: guodong-zhou
    last: Zhou
  author_string: Feng Jiang, Sheng Xu, Xiaomin Chu, Peifeng Li, Qiaoming Zhu, Guodong
    Zhou
  bibkey: jiang-etal-2018-mcdtb
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3493'
  page_last: '3504'
  pages: "3493\u20133504"
  paper_id: '296'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1296.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1296.jpg
  title: 'MCDTB: A Macro-level Chinese Discourse TreeBank'
  title_html: '<span class="acl-fixed-case">MCDTB</span>: A Macro-level <span class="acl-fixed-case">C</span>hinese
    Discourse <span class="acl-fixed-case">T</span>ree<span class="acl-fixed-case">B</span>ank'
  url: https://www.aclweb.org/anthology/C18-1296
  year: '2018'
C18-1297:
  abstract: "Enterprise content writers are engaged in writing textual content for\
    \ various purposes. Often, the text being written may already be present in the\
    \ enterprise corpus in the form of past articles and can be re-purposed for the\
    \ current needs. In the absence of suitable tools, authors manually curate/create\
    \ such content (sometimes from scratch) which reduces their productivity. To address\
    \ this, we propose an automatic approach to generate an initial version of the\
    \ author\u2019s intended text based on an input content snippet. Starting with\
    \ a set of extracted textual fragments related to the snippet based on the query\
    \ words in it, the proposed approach builds the desired text from these fragment\
    \ by simultaneously optimizing the information coverage, relevance, diversity\
    \ and coherence in the generated content. Evaluations on standard datasets shows\
    \ improved performance against existing baselines on several metrics."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Balaji Vasan
    full: Balaji Vasan Srinivasan
    id: balaji-vasan-srinivasan
    last: Srinivasan
  - first: Pranav
    full: Pranav Maneriker
    id: pranav-maneriker
    last: Maneriker
  - first: Kundan
    full: Kundan Krishna
    id: kundan-krishna
    last: Krishna
  - first: Natwar
    full: Natwar Modani
    id: natwar-modani
    last: Modani
  author_string: Balaji Vasan Srinivasan, Pranav Maneriker, Kundan Krishna, Natwar
    Modani
  bibkey: srinivasan-etal-2018-corpus
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3505'
  page_last: '3515'
  pages: "3505\u20133515"
  paper_id: '297'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1297.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1297.jpg
  title: Corpus-based Content Construction
  title_html: Corpus-based Content Construction
  url: https://www.aclweb.org/anthology/C18-1297
  year: '2018'
C18-1298:
  abstract: "Recent work on bridging resolution has so far been based on the corpus\
    \ ISNotes (Markert et al. 2012), as this was the only corpus available with unrestricted\
    \ bridging annotation. Hou et al. 2014\u2019s rule-based system currently achieves\
    \ state-of-the-art performance on this corpus, as learning-based approaches suffer\
    \ from the lack of available training data. Recently, a number of new corpora\
    \ with bridging annotations have become available. To test the generalisability\
    \ of the approach by Hou et al. 2014, we apply a slightly extended rule-based\
    \ system to these corpora. Besides the expected out-of-domain effects, we also\
    \ observe low performance on some of the in-domain corpora. Our analysis shows\
    \ that this is the result of two very different phenomena being defined as bridging,\
    \ namely referential and lexical bridging. We also report that filtering out gold\
    \ or predicted coreferent anaphors before applying the bridging resolution system\
    \ helps improve bridging resolution."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Ina
    full: Ina Roesiger
    id: ina-roesiger
    last: Roesiger
  - first: Arndt
    full: Arndt Riester
    id: arndt-riester
    last: Riester
  - first: Jonas
    full: Jonas Kuhn
    id: jonas-kuhn
    last: Kuhn
  author_string: Ina Roesiger, Arndt Riester, Jonas Kuhn
  bibkey: roesiger-etal-2018-bridging
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3516'
  page_last: '3528'
  pages: "3516\u20133528"
  paper_id: '298'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1298.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1298.jpg
  title: 'Bridging resolution: Task definition, corpus resources and rule-based experiments'
  title_html: 'Bridging resolution: Task definition, corpus resources and rule-based
    experiments'
  url: https://www.aclweb.org/anthology/C18-1298
  year: '2018'
C18-1299:
  abstract: While the disfluency detection has achieved notable success in the past
    years, it still severely suffers from the data scarcity. To tackle this problem,
    we propose a novel semi-supervised approach which can utilize large amounts of
    unlabelled data. In this work, a light-weight neural net is proposed to extract
    the hidden features based solely on self-attention without any Recurrent Neural
    Network (RNN) or Convolutional Neural Network (CNN). In addition, we use the unlabelled
    corpus to enhance the performance. Besides, the Generative Adversarial Network
    (GAN) training is applied to enforce the similar distribution between the labelled
    and unlabelled data. The experimental results show that our approach achieves
    significant improvements over strong baselines.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Feng
    full: Feng Wang
    id: feng-wang
    last: Wang
  - first: Wei
    full: Wei Chen
    id: wei-chen
    last: Chen
  - first: Zhen
    full: Zhen Yang
    id: zhen-yang
    last: Yang
  - first: Qianqian
    full: Qianqian Dong
    id: qianqian-dong
    last: Dong
  - first: Shuang
    full: Shuang Xu
    id: shuang-xu
    last: Xu
  - first: Bo
    full: Bo Xu
    id: bo-xu
    last: Xu
  author_string: Feng Wang, Wei Chen, Zhen Yang, Qianqian Dong, Shuang Xu, Bo Xu
  bibkey: wang-etal-2018-semi
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3529'
  page_last: '3538'
  pages: "3529\u20133538"
  paper_id: '299'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1299.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1299.jpg
  title: Semi-Supervised Disfluency Detection
  title_html: Semi-Supervised Disfluency Detection
  url: https://www.aclweb.org/anthology/C18-1299
  year: '2018'
C18-1300:
  abstract: "Dialogue Act (DA) tagging is crucial for spoken language understanding\
    \ systems, as it provides a general representation of speakers\u2019 intents,\
    \ not bound to a particular dialogue system. Unfortunately, publicly available\
    \ data sets with DA annotation are all based on different annotation schemes and\
    \ thus incompatible with each other. Moreover, their schemes often do not cover\
    \ all aspects necessary for open-domain human-machine interaction. In this paper,\
    \ we propose a methodology to map several publicly available corpora to a subset\
    \ of the ISO standard, in order to create a large task-independent training corpus\
    \ for DA classification. We show the feasibility of using this corpus to train\
    \ a domain-independent DA tagger testing it on out-of-domain conversational data,\
    \ and argue the importance of training on multiple corpora to achieve robustness\
    \ across different DA categories."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Stefano
    full: Stefano Mezza
    id: stefano-mezza
    last: Mezza
  - first: Alessandra
    full: Alessandra Cervone
    id: alessandra-cervone
    last: Cervone
  - first: Evgeny
    full: Evgeny Stepanov
    id: evgeny-stepanov
    last: Stepanov
  - first: Giuliano
    full: Giuliano Tortoreto
    id: giuliano-tortoreto
    last: Tortoreto
  - first: Giuseppe
    full: Giuseppe Riccardi
    id: giuseppe-riccardi
    last: Riccardi
  author_string: Stefano Mezza, Alessandra Cervone, Evgeny Stepanov, Giuliano Tortoreto,
    Giuseppe Riccardi
  bibkey: mezza-etal-2018-iso
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3539'
  page_last: '3551'
  pages: "3539\u20133551"
  paper_id: '300'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1300.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1300.jpg
  title: ISO-Standard Domain-Independent Dialogue Act Tagging for Conversational Agents
  title_html: <span class="acl-fixed-case">ISO</span>-Standard Domain-Independent
    Dialogue Act Tagging for Conversational Agents
  url: https://www.aclweb.org/anthology/C18-1300
  year: '2018'
C18-1301:
  abstract: 'Arrows are a key ingredient of schematic pictorial communication. This
    paper investigates the interpretation of arrows through linguistic, crowdsourcing
    and machine-learning methodology. Our work establishes a novel analogy between
    arrows and verbs: we advocate representing arrows in terms of qualitatively different
    structural and semantic frames, and resolving frames to specific interpretations
    using shallow world knowledge.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Malihe
    full: Malihe Alikhani
    id: malihe-alikhani
    last: Alikhani
  - first: Matthew
    full: Matthew Stone
    id: matthew-stone
    last: Stone
  author_string: Malihe Alikhani, Matthew Stone
  bibkey: alikhani-stone-2018-arrows
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3552'
  page_last: '3563'
  pages: "3552\u20133563"
  paper_id: '301'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1301.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1301.jpg
  title: Arrows are the Verbs of Diagrams
  title_html: Arrows are the Verbs of Diagrams
  url: https://www.aclweb.org/anthology/C18-1301
  year: '2018'
C18-1302:
  abstract: We use a broad coverage, linguistically precise English Resource Grammar
    (ERG) to detect negation scope in sentences taken from pathology reports. We show
    that incorporating this information in feature extraction has a positive effect
    on classification of the reports with respect to cancer laterality compared with
    NegEx, a commonly used tool for negation detection. We analyze the differences
    between NegEx and ERG results on our dataset and how these differences indicate
    some directions for future work.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Olga
    full: Olga Zamaraeva
    id: olga-zamaraeva
    last: Zamaraeva
  - first: Kristen
    full: Kristen Howell
    id: kristen-howell
    last: Howell
  - first: Adam
    full: Adam Rhine
    id: adam-rhine
    last: Rhine
  author_string: Olga Zamaraeva, Kristen Howell, Adam Rhine
  bibkey: zamaraeva-etal-2018-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3564'
  page_last: '3575'
  pages: "3564\u20133575"
  paper_id: '302'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1302.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1302.jpg
  title: Improving Feature Extraction for Pathology Reports with Precise Negation
    Scope Detection
  title_html: Improving Feature Extraction for Pathology Reports with Precise Negation
    Scope Detection
  url: https://www.aclweb.org/anthology/C18-1302
  year: '2018'
C18-1303:
  abstract: 'We present a video captioning approach that encodes features by progressively
    completing syntactic structure (LSTM-CSS). To construct basic syntactic structure
    (i.e., subject, predicate, and object), we use a Conditional Random Field to label
    semantic representations (i.e., motions, objects). We argue that in order to improve
    the comprehensiveness of the description, the local features within object regions
    can be used to generate complementary syntactic elements (e.g., attribute, adverbial).
    Inspired by redundancy of human receptors, we utilize a Region Proposal Network
    to focus on the object regions. To model the final temporal dynamics, Recurrent
    Neural Network with Path Embeddings is adopted. We demonstrate the effectiveness
    of LSTM-CSS on generating natural sentences: 42.3% and 28.5% in terms of BLEU@4
    and METEOR. Superior performance when compared to state-of-the-art methods are
    reported on a large video description dataset (i.e., MSR-VTT-2016).'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Guolong
    full: Guolong Wang
    id: guolong-wang
    last: Wang
  - first: Zheng
    full: Zheng Qin
    id: zheng-qin
    last: Qin
  - first: Kaiping
    full: Kaiping Xu
    id: kaiping-xu
    last: Xu
  - first: Kai
    full: Kai Huang
    id: kai-huang
    last: Huang
  - first: Shuxiong
    full: Shuxiong Ye
    id: shuxiong-ye
    last: Ye
  author_string: Guolong Wang, Zheng Qin, Kaiping Xu, Kai Huang, Shuxiong Ye
  bibkey: wang-etal-2018-bridge
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3576'
  page_last: '3585'
  pages: "3576\u20133585"
  paper_id: '303'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1303.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1303.jpg
  title: Bridge Video and Text with Cascade Syntactic Structure
  title_html: Bridge Video and Text with Cascade Syntactic Structure
  url: https://www.aclweb.org/anthology/C18-1303
  year: '2018'
C18-1304:
  abstract: This paper is an initial study on multi-task and multi-lingual joint learning
    for lexical utterance classification. A major problem in constructing lexical
    utterance classification modules for spoken dialogue systems is that individual
    data resources are often limited or unbalanced among tasks and/or languages. Various
    studies have examined joint learning using neural-network based shared modeling;
    however, previous joint learning studies focused on either cross-task or cross-lingual
    knowledge transfer. In order to simultaneously support both multi-task and multi-lingual
    joint learning, our idea is to explicitly divide state-of-the-art neural lexical
    utterance classification into language-specific components that can be shared
    between different tasks and task-specific components that can be shared between
    different languages. In addition, in order to effectively transfer knowledge between
    different task data sets and different language data sets, this paper proposes
    a partially-shared modeling method that possesses both shared components and components
    specific to individual data sets. We demonstrate the effectiveness of proposed
    method using Japanese and English data sets with three different lexical utterance
    classification tasks.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Ryo
    full: Ryo Masumura
    id: ryo-masumura
    last: Masumura
  - first: Tomohiro
    full: Tomohiro Tanaka
    id: tomohiro-tanaka
    last: Tanaka
  - first: Ryuichiro
    full: Ryuichiro Higashinaka
    id: ryuichiro-higashinaka
    last: Higashinaka
  - first: Hirokazu
    full: Hirokazu Masataki
    id: hirokazu-masataki
    last: Masataki
  - first: Yushi
    full: Yushi Aono
    id: yushi-aono
    last: Aono
  author_string: Ryo Masumura, Tomohiro Tanaka, Ryuichiro Higashinaka, Hirokazu Masataki,
    Yushi Aono
  bibkey: masumura-etal-2018-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3586'
  page_last: '3596'
  pages: "3586\u20133596"
  paper_id: '304'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1304.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1304.jpg
  title: Multi-task and Multi-lingual Joint Learning of Neural Lexical Utterance Classification
    based on Partially-shared Modeling
  title_html: Multi-task and Multi-lingual Joint Learning of Neural Lexical Utterance
    Classification based on Partially-shared Modeling
  url: https://www.aclweb.org/anthology/C18-1304
  year: '2018'
C18-1305:
  abstract: To deploy a spoken language understanding (SLU) model to a new language,
    language transferring is desired to avoid the trouble of acquiring and labeling
    a new big SLU corpus. An SLU corpus is a monolingual corpus with domain/intent/slot
    labels. Translating the original SLU corpus into the target language is an attractive
    strategy. However, SLU corpora consist of plenty of semantic labels (slots), which
    general-purpose translators cannot handle well, not to mention additional culture
    differences. This paper focuses on the language transferring task given a small
    in-domain parallel SLU corpus. The in-domain parallel corpus can be used as the
    first adaptation on the general translator. But more importantly, we show how
    to use reinforcement learning (RL) to further adapt the adapted translator, where
    translated sentences with more proper slot tags receive higher rewards. Our reward
    is derived from the source input sentence exclusively, unlike reward via actor-critical
    methods or computing reward with a ground truth target sentence. Hence we can
    adapt the translator the second time, using the big monolingual SLU corpus from
    the source language. We evaluate our approach on Chinese to English language transferring
    for SLU systems. The experimental results show that the generated English SLU
    corpus via adaptation and reinforcement learning gives us over 97% in the slot
    F1 score and over 84% accuracy in domain classification. It demonstrates the effectiveness
    of the proposed language transferring method. Compared with naive translation,
    our proposed method improves domain classification accuracy by relatively 22%,
    and the slot filling F1 score by relatively more than 71%.
  address: Santa Fe, New Mexico, USA
  author:
  - first: He
    full: He Bai
    id: he-bai
    last: Bai
  - first: Yu
    full: Yu Zhou
    id: yu-zhou
    last: Zhou
  - first: Jiajun
    full: Jiajun Zhang
    id: jiajun-zhang
    last: Zhang
  - first: Liang
    full: Liang Zhao
    id: liang-zhao
    last: Zhao
  - first: Mei-Yuh
    full: Mei-Yuh Hwang
    id: mei-yuh-hwang
    last: Hwang
  - first: Chengqing
    full: Chengqing Zong
    id: chengqing-zong
    last: Zong
  author_string: He Bai, Yu Zhou, Jiajun Zhang, Liang Zhao, Mei-Yuh Hwang, Chengqing
    Zong
  bibkey: bai-etal-2018-source
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3597'
  page_last: '3607'
  pages: "3597\u20133607"
  paper_id: '305'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1305.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1305.jpg
  title: Source Critical Reinforcement Learning for Transferring Spoken Language Understanding
    to a New Language
  title_html: Source Critical Reinforcement Learning for Transferring Spoken Language
    Understanding to a New Language
  url: https://www.aclweb.org/anthology/C18-1305
  year: '2018'
C18-1306:
  abstract: Generative dialog models usually adopt beam search as the inference method
    to generate responses. However, small-width beam search only focuses on the limited
    current optima. This deficiency named as myopic bias ultimately suppresses the
    diversity and probability of generated responses. Although increasing the beam
    width mitigates the myopic bias, it also proportionally slows down the inference
    efficiency. To alleviate the myopic bias in small-width beam search, this paper
    proposes a Prospective-Performance Network (PPN) to predict the future reward
    of the given partially-generated response, and the future reward is defined by
    the expectation of the partial response appearing in the top-ranked responses
    given by a larger-width beam search. Enhanced by PPN, the decoder can promote
    the results with great potential during the beam search phase. The experimental
    results on both Chinese and English corpora show that our method is promising
    to increase the quality and diversity of generated responses, with inference efficiency
    well maintained.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Zongsheng
    full: Zongsheng Wang
    id: zongsheng-wang
    last: Wang
  - first: Yunzhi
    full: Yunzhi Bai
    id: yunzhi-bai
    last: Bai
  - first: Bowen
    full: Bowen Wu
    id: bowen-wu
    last: Wu
  - first: Zhen
    full: Zhen Xu
    id: zhen-xu
    last: Xu
  - first: Zhuoran
    full: Zhuoran Wang
    id: zhuoran-wang
    last: Wang
  - first: Baoxun
    full: Baoxun Wang
    id: baoxun-wang
    last: Wang
  author_string: Zongsheng Wang, Yunzhi Bai, Bowen Wu, Zhen Xu, Zhuoran Wang, Baoxun
    Wang
  bibkey: wang-etal-2018-prospective
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3608'
  page_last: '3618'
  pages: "3608\u20133618"
  paper_id: '306'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1306.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1306.jpg
  title: A Prospective-Performance Network to Alleviate Myopia in Beam Search for
    Response Generation
  title_html: A Prospective-Performance Network to Alleviate Myopia in Beam Search
    for Response Generation
  url: https://www.aclweb.org/anthology/C18-1306
  year: '2018'
C18-1307:
  abstract: Chinese word segmentation (CWS) trained from open source corpus faces
    dramatic performance drop when dealing with domain text, especially for a domain
    with lots of special terms and diverse writing styles, such as the biomedical
    domain. However, building domain-specific CWS requires extremely high annotation
    cost. In this paper, we propose an approach by exploiting domain-invariant knowledge
    from high resource to low resource domains. Extensive experiments show that our
    model achieves consistently higher accuracy than the single-task CWS and other
    transfer learning baselines, especially when there is a large disparity between
    source and target domains.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Junjie
    full: Junjie Xing
    id: junjie-xing
    last: Xing
  - first: Kenny
    full: Kenny Zhu
    id: kenny-zhu
    last: Zhu
  - first: Shaodian
    full: Shaodian Zhang
    id: shaodian-zhang
    last: Zhang
  author_string: Junjie Xing, Kenny Zhu, Shaodian Zhang
  bibkey: xing-etal-2018-adaptive
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3619'
  page_last: '3630'
  pages: "3619\u20133630"
  paper_id: '307'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1307.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1307.jpg
  title: Adaptive Multi-Task Transfer Learning for Chinese Word Segmentation in Medical
    Text
  title_html: Adaptive Multi-Task Transfer Learning for <span class="acl-fixed-case">C</span>hinese
    Word Segmentation in Medical Text
  url: https://www.aclweb.org/anthology/C18-1307
  year: '2018'
C18-1308:
  abstract: Developing conversational systems that can converse in many languages
    is an interesting challenge for natural language processing. In this paper, we
    introduce multilingual addressee and response selection. In this task, a conversational
    system predicts an appropriate addressee and response for an input message in
    multiple languages. A key to developing such multilingual responding systems is
    how to utilize high-resource language data to compensate for low-resource language
    data. We present several knowledge transfer methods for conversational systems.
    To evaluate our methods, we create a new multilingual conversation dataset. Experiments
    on the dataset demonstrate the effectiveness of our methods.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Motoki
    full: Motoki Sato
    id: motoki-sano
    last: Sato
  - first: Hiroki
    full: Hiroki Ouchi
    id: hiroki-ouchi
    last: Ouchi
  - first: Yuta
    full: Yuta Tsuboi
    id: yuta-tsuboi
    last: Tsuboi
  author_string: Motoki Sato, Hiroki Ouchi, Yuta Tsuboi
  bibkey: sato-etal-2018-addressee
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3631'
  page_last: '3644'
  pages: "3631\u20133644"
  paper_id: '308'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1308.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1308.jpg
  title: Addressee and Response Selection for Multilingual Conversation
  title_html: Addressee and Response Selection for Multilingual Conversation
  url: https://www.aclweb.org/anthology/C18-1308
  year: '2018'
C18-1309:
  abstract: 'Events in text documents are interrelated in complex ways. In this paper,
    we study two types of relation: Event Coreference and Event Sequencing. We show
    that the popular tree-like decoding structure for automated Event Coreference
    is not suitable for Event Sequencing. To this end, we propose a graph-based decoding
    algorithm that is applicable to both tasks. The new decoding algorithm supports
    flexible feature sets for both tasks. Empirically, our event coreference system
    has achieved state-of-the-art performance on the TAC-KBP 2015 event coreference
    task and our event sequencing system beats a strong temporal-based, oracle-informed
    baseline. We discuss the challenges of studying these event relations.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Zhengzhong
    full: Zhengzhong Liu
    id: zhengzhong-liu
    last: Liu
  - first: Teruko
    full: Teruko Mitamura
    id: teruko-mitamura
    last: Mitamura
  - first: Eduard
    full: Eduard Hovy
    id: eduard-hovy
    last: Hovy
  author_string: Zhengzhong Liu, Teruko Mitamura, Eduard Hovy
  bibkey: liu-etal-2018-graph
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3645'
  page_last: '3657'
  pages: "3645\u20133657"
  paper_id: '309'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1309.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1309.jpg
  title: Graph Based Decoding for Event Sequencing and Coreference Resolution
  title_html: Graph Based Decoding for Event Sequencing and Coreference Resolution
  url: https://www.aclweb.org/anthology/C18-1309
  year: '2018'
C18-1310:
  abstract: "We present a corpus of spoken Dutch image descriptions, paired with two\
    \ sets of eye-tracking data: Free viewing, where participants look at images without\
    \ any particular purpose, and Description viewing, where we track eye movements\
    \ while participants produce spoken descriptions of the images they are viewing.\
    \ This paper describes the data collection procedure and the corpus itself, and\
    \ provides an initial analysis of self-corrections in image descriptions. We also\
    \ present two studies showing the potential of this data. Though these studies\
    \ mainly serve as an example, we do find two interesting results: (1) the eye-tracking\
    \ data for the description viewing task is more coherent than for the free-viewing\
    \ task; (2) variation in image descriptions (also called \u2018image specificity\u2019\
    ; Jas and Parikh, 2015) is only moderately correlated across different languages.\
    \ Our corpus can be used to gain a deeper understanding of the image description\
    \ task, particularly how visual attention is correlated with the image description\
    \ process."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Emiel
    full: Emiel van Miltenburg
    id: emiel-van-miltenburg
    last: van Miltenburg
  - first: "\xC1kos"
    full: "\xC1kos K\xE1d\xE1r"
    id: akos-kadar
    last: "K\xE1d\xE1r"
  - first: Ruud
    full: Ruud Koolen
    id: ruud-koolen
    last: Koolen
  - first: Emiel
    full: Emiel Krahmer
    id: emiel-krahmer
    last: Krahmer
  author_string: "Emiel van Miltenburg, \xC1kos K\xE1d\xE1r, Ruud Koolen, Emiel Krahmer"
  bibkey: van-miltenburg-etal-2018-didec
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3658'
  page_last: '3669'
  pages: "3658\u20133669"
  paper_id: '310'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1310.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1310.jpg
  title: 'DIDEC: The Dutch Image Description and Eye-tracking Corpus'
  title_html: '<span class="acl-fixed-case">DIDEC</span>: The <span class="acl-fixed-case">D</span>utch
    Image Description and Eye-tracking Corpus'
  url: https://www.aclweb.org/anthology/C18-1310
  year: '2018'
C18-1311:
  abstract: "We investigate the stability of narrative schemas (Chambers and Jurafsky,\
    \ 2009) automatically induced from a news corpus, representing recurring narratives\
    \ in a corpus. If such techniques produce meaningful results, we should expect\
    \ that small changes to the corpus will result in only small changes to the induced\
    \ schemas. We describe experiments involving successive ablation of a corpus and\
    \ cross-validation at each stage of ablation, on schemas generated by three different\
    \ techniques over a general news corpus and topically-specific subcorpora. We\
    \ also develop a method for evaluating the similarity between sets of narrative\
    \ schemas, and thus the stability of the schema induction algorithms. This stability\
    \ analysis affirms the heterogeneous/homogeneous document category hypothesis\
    \ first presented in Simonson and Davis (2016), whose technique is problematically\
    \ limited. Additionally, increased ablation leads to increasing stability, so\
    \ the smaller the remaining corpus, the more stable schema generation appears\
    \ to be. We surmise that as a corpus grows larger, novel and more varied narratives\
    \ continue to appear and stability declines, though at some point this decline\
    \ levels off as new additions to the corpus consist essentially of \u201Cmore\
    \ of the same.\u201D"
  address: Santa Fe, New Mexico, USA
  author:
  - first: Dan
    full: Dan Simonson
    id: dan-simonson
    last: Simonson
  - first: Anthony
    full: Anthony Davis
    id: anthony-davis
    last: Davis
  author_string: Dan Simonson, Anthony Davis
  bibkey: simonson-davis-2018-narrative
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3670'
  page_last: '3680'
  pages: "3670\u20133680"
  paper_id: '311'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1311.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1311.jpg
  title: Narrative Schema Stability in News Text
  title_html: Narrative Schema Stability in News Text
  url: https://www.aclweb.org/anthology/C18-1311
  year: '2018'
C18-1312:
  abstract: 'We present bot#1337: a dialog system developed for the 1st NIPS Conversational
    Intelligence Challenge 2017 (ConvAI). The aim of the competition was to implement
    a bot capable of conversing with humans based on a given passage of text. To enable
    conversation, we implemented a set of skills for our bot, including chit-chat,
    topic detection, text summarization, question answering and question generation.
    The system has been trained in a supervised setting using a dialogue manager to
    select an appropriate skill for generating a response. The latter allows a developer
    to focus on the skill implementation rather than the finite state machine based
    dialog manager. The proposed system bot#1337 won the competition with an average
    dialogue quality score of 2.78 out of 5 given by human evaluators. Source code
    and trained models for the bot#1337 are available on GitHub.'
  address: Santa Fe, New Mexico, USA
  author:
  - first: Idris
    full: Idris Yusupov
    id: idris-yusupov
    last: Yusupov
  - first: Yurii
    full: Yurii Kuratov
    id: yurii-kuratov
    last: Kuratov
  author_string: Idris Yusupov, Yurii Kuratov
  bibkey: yusupov-kuratov-2018-nips
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3681'
  page_last: '3692'
  pages: "3681\u20133692"
  paper_id: '312'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1312.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1312.jpg
  title: 'NIPS Conversational Intelligence Challenge 2017 Winner System: Skill-based
    Conversational Agent with Supervised Dialog Manager'
  title_html: '<span class="acl-fixed-case">NIPS</span> Conversational Intelligence
    Challenge 2017 Winner System: Skill-based Conversational Agent with Supervised
    Dialog Manager'
  url: https://www.aclweb.org/anthology/C18-1312
  year: '2018'
C18-1313:
  abstract: There are few corpora that endeavor to represent the semantic content
    of entire documents. We present a corpus that accomplishes one way of capturing
    document level semantics, by annotating coreference and similar phenomena (bridging
    and implicit roles) on top of gold Abstract Meaning Representations of sentence-level
    semantics. We present a new corpus of this annotation, with analysis of its quality,
    alongside a plausible baseline for comparison. It is hoped that this Multi-Sentence
    AMR corpus (MS-AMR) may become a feasible method for developing rich representations
    of document meaning, useful for tasks such as information extraction and question
    answering.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Tim
    full: "Tim O\u2019Gorman"
    id: tim-ogorman
    last: "O\u2019Gorman"
  - first: Michael
    full: Michael Regan
    id: michael-regan
    last: Regan
  - first: Kira
    full: Kira Griffitt
    id: kira-griffitt
    last: Griffitt
  - first: Ulf
    full: Ulf Hermjakob
    id: ulf-hermjakob
    last: Hermjakob
  - first: Kevin
    full: Kevin Knight
    id: kevin-knight
    last: Knight
  - first: Martha
    full: Martha Palmer
    id: martha-palmer
    last: Palmer
  author_string: "Tim O\u2019Gorman, Michael Regan, Kira Griffitt, Ulf Hermjakob,\
    \ Kevin Knight, Martha Palmer"
  bibkey: ogorman-etal-2018-amr
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3693'
  page_last: '3702'
  pages: "3693\u20133702"
  paper_id: '313'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1313.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1313.jpg
  title: 'AMR Beyond the Sentence: the Multi-sentence AMR corpus'
  title_html: '<span class="acl-fixed-case">AMR</span> Beyond the Sentence: the Multi-sentence
    <span class="acl-fixed-case">AMR</span> corpus'
  url: https://www.aclweb.org/anthology/C18-1313
  year: '2018'
C18-1314:
  abstract: In this paper, we investigate the issue of persuasiveness evaluation for
    argumentative comments. Most of the existing research explores different text
    features of reply comments on word level and ignores interactions between participants.
    In general, viewpoints are usually expressed by multiple arguments and exchanged
    on argument level. To better model the process of dialogical argumentation, we
    propose a novel co-attention mechanism based neural network to capture the interactions
    between participants on argument level. Experimental results on a publicly available
    dataset show that the proposed model significantly outperforms some state-of-the-art
    methods for persuasiveness evaluation. Further analysis reveals that attention
    weights computed in our model are able to extract interactive argument pairs from
    the original post and the reply.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Lu
    full: Lu Ji
    id: lu-ji
    last: Ji
  - first: Zhongyu
    full: Zhongyu Wei
    id: zhongyu-wei
    last: Wei
  - first: Xiangkun
    full: Xiangkun Hu
    id: xiangkun-hu
    last: Hu
  - first: Yang
    full: Yang Liu
    id: yang-liu-icsi
    last: Liu
  - first: Qi
    full: Qi Zhang
    id: qi-zhang
    last: Zhang
  - first: Xuanjing
    full: Xuanjing Huang
    id: xuan-jing-huang
    last: Huang
  author_string: Lu Ji, Zhongyu Wei, Xiangkun Hu, Yang Liu, Qi Zhang, Xuanjing Huang
  bibkey: ji-etal-2018-incorporating
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3703'
  page_last: '3714'
  pages: "3703\u20133714"
  paper_id: '314'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1314.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1314.jpg
  title: Incorporating Argument-Level Interactions for Persuasion Comments Evaluation
    using Co-attention Model
  title_html: Incorporating Argument-Level Interactions for Persuasion Comments Evaluation
    using Co-attention Model
  url: https://www.aclweb.org/anthology/C18-1314
  year: '2018'
C18-1315:
  abstract: We study the problem of grounding distributional representations of texts
    on the visual domain, namely visual-semantic embeddings (VSE for short). Begin
    with an insightful adversarial attack on VSE embeddings, we show the limitation
    of current frameworks and image-text datasets (e.g., MS-COCO) both quantitatively
    and qualitatively. The large gap between the number of possible constitutions
    of real-world semantics and the size of parallel data, to a large extent, restricts
    the model to establish a strong link between textual semantics and visual concepts.
    We alleviate this problem by augmenting the MS-COCO image captioning datasets
    with textual contrastive adversarial samples. These samples are synthesized using
    language priors of human and the WordNet knowledge base, and enforce the model
    to ground learned embeddings to concrete concepts within the image. This simple
    but powerful technique brings a noticeable improvement over the baselines on a
    diverse set of downstream tasks, in addition to defending known-type adversarial
    attacks. Codes are available at https://github.com/ExplorerFreda/VSE-C.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Haoyue
    full: Haoyue Shi
    id: haoyue-shi
    last: Shi
  - first: Jiayuan
    full: Jiayuan Mao
    id: jiayuan-mao
    last: Mao
  - first: Tete
    full: Tete Xiao
    id: tete-xiao
    last: Xiao
  - first: Yuning
    full: Yuning Jiang
    id: yuning-jiang
    last: Jiang
  - first: Jian
    full: Jian Sun
    id: jian-sun
    last: Sun
  author_string: Haoyue Shi, Jiayuan Mao, Tete Xiao, Yuning Jiang, Jian Sun
  bibkey: shi-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3715'
  page_last: '3727'
  pages: "3715\u20133727"
  paper_id: '315'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1315.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1315.jpg
  title: Learning Visually-Grounded Semantics from Contrastive Adversarial Samples
  title_html: Learning Visually-Grounded Semantics from Contrastive Adversarial Samples
  url: https://www.aclweb.org/anthology/C18-1315
  year: '2018'
C18-1316:
  abstract: "Online debates can help provide valuable information about various perspectives\
    \ on a wide range of issues. However, understanding the stances expressed in these\
    \ debates is a highly challenging task, which requires modeling both textual content\
    \ and users\u2019 conversational interactions. Current approaches take a collective\
    \ classification approach, which ignores the relationships between different debate\
    \ topics. In this work, we suggest to view this task as a representation learning\
    \ problem, and embed the text and authors jointly based on their interactions.\
    \ We evaluate our model over the Internet Argumentation Corpus, and compare different\
    \ approaches for structural information embedding. Experimental results show that\
    \ our model can achieve significantly better results compared to previous competitive\
    \ models."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Chang
    full: Chang Li
    id: chang-li
    last: Li
  - first: Aldo
    full: Aldo Porco
    id: aldo-porco
    last: Porco
  - first: Dan
    full: Dan Goldwasser
    id: dan-goldwasser
    last: Goldwasser
  author_string: Chang Li, Aldo Porco, Dan Goldwasser
  bibkey: li-etal-2018-structured
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3728'
  page_last: '3739'
  pages: "3728\u20133739"
  paper_id: '316'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1316.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1316.jpg
  title: Structured Representation Learning for Online Debate Stance Prediction
  title_html: Structured Representation Learning for Online Debate Stance Prediction
  url: https://www.aclweb.org/anthology/C18-1316
  year: '2018'
C18-1317:
  abstract: Multi-turn conversation understanding is a major challenge for building
    intelligent dialogue systems. This work focuses on retrieval-based response matching
    for multi-turn conversation whose related work simply concatenates the conversation
    utterances, ignoring the interactions among previous utterances for context modeling.
    In this paper, we formulate previous utterances into context using a proposed
    deep utterance aggregation model to form a fine-grained context representation.
    In detail, a self-matching attention is first introduced to route the vital information
    in each utterance. Then the model matches a response with each refined utterance
    and the final matching score is obtained after attentive turns aggregation. Experimental
    results show our model outperforms the state-of-the-art methods on three multi-turn
    conversation benchmarks, including a newly introduced e-commerce dialogue corpus.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Zhuosheng
    full: Zhuosheng Zhang
    id: zhuosheng-zhang
    last: Zhang
  - first: Jiangtong
    full: Jiangtong Li
    id: jiangtong-li
    last: Li
  - first: Pengfei
    full: Pengfei Zhu
    id: pengfei-zhu
    last: Zhu
  - first: Hai
    full: Hai Zhao
    id: hai-zhao
    last: Zhao
  - first: Gongshen
    full: Gongshen Liu
    id: gongshen-liu
    last: Liu
  author_string: Zhuosheng Zhang, Jiangtong Li, Pengfei Zhu, Hai Zhao, Gongshen Liu
  bibkey: zhang-etal-2018-modeling
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3740'
  page_last: '3752'
  pages: "3740\u20133752"
  paper_id: '317'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1317.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1317.jpg
  title: Modeling Multi-turn Conversation with Deep Utterance Aggregation
  title_html: Modeling Multi-turn Conversation with Deep Utterance Aggregation
  url: https://www.aclweb.org/anthology/C18-1317
  year: '2018'
C18-1318:
  abstract: Persuasion is rarely achieved through a loose set of arguments alone.
    Rather, an effective delivery of arguments follows a rhetorical strategy, combining
    logical reasoning with appeals to ethics and emotion. We argue that such a strategy
    means to select, arrange, and phrase a set of argumentative discourse units. In
    this paper, we model rhetorical strategies for the computational synthesis of
    effective argumentation. In a study, we let 26 experts synthesize argumentative
    texts with different strategies for 10 topics. We find that the experts agree
    in the selection significantly more when following the same strategy. While the
    texts notably vary for different strategies, especially their arrangement remains
    stable. The results suggest that our model enables a strategical synthesis.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Henning
    full: Henning Wachsmuth
    id: henning-wachsmuth
    last: Wachsmuth
  - first: Manfred
    full: Manfred Stede
    id: manfred-stede
    last: Stede
  - first: Roxanne
    full: Roxanne El Baff
    id: roxanne-el-baff
    last: El Baff
  - first: Khalid
    full: Khalid Al-Khatib
    id: khalid-al-khatib
    last: Al-Khatib
  - first: Maria
    full: Maria Skeppstedt
    id: maria-skeppstedt
    last: Skeppstedt
  - first: Benno
    full: Benno Stein
    id: benno-stein
    last: Stein
  author_string: Henning Wachsmuth, Manfred Stede, Roxanne El Baff, Khalid Al-Khatib,
    Maria Skeppstedt, Benno Stein
  bibkey: wachsmuth-etal-2018-argumentation
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3753'
  page_last: '3765'
  pages: "3753\u20133765"
  paper_id: '318'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1318.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1318.jpg
  title: Argumentation Synthesis following Rhetorical Strategies
  title_html: Argumentation Synthesis following Rhetorical Strategies
  url: https://www.aclweb.org/anthology/C18-1318
  year: '2018'
C18-1319:
  abstract: "There is an increasing demand for goal-oriented conversation systems\
    \ which can assist users in various day-to-day activities such as booking tickets,\
    \ restaurant reservations, shopping, etc. Most of the existing datasets for building\
    \ such conversation systems focus on monolingual conversations and there is hardly\
    \ any work on multilingual and/or code-mixed conversations. Such datasets and\
    \ systems thus do not cater to the multilingual regions of the world, such as\
    \ India, where it is very common for people to speak more than one language and\
    \ seamlessly switch between them resulting in code-mixed conversations. For example,\
    \ a Hindi speaking user looking to book a restaurant would typically ask, \u201C\
    Kya tum is restaurant mein ek table book karne mein meri help karoge?\u201D (\u201C\
    Can you help me in booking a table at this restaurant?\u201D). To facilitate the\
    \ development of such code-mixed conversation models, we build a goal-oriented\
    \ dialog dataset containing code-mixed conversations. Specifically, we take the\
    \ text from the DSTC2 restaurant reservation dataset and create code-mixed versions\
    \ of it in Hindi-English, Bengali-English, Gujarati-English and Tamil-English.\
    \ We also establish initial baselines on this dataset using existing state of\
    \ the art models. This dataset along with our baseline implementations will be\
    \ made publicly available for research purposes."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Suman
    full: Suman Banerjee
    id: suman-banerjee
    last: Banerjee
  - first: Nikita
    full: Nikita Moghe
    id: nikita-moghe
    last: Moghe
  - first: Siddhartha
    full: Siddhartha Arora
    id: siddhartha-arora
    last: Arora
  - first: Mitesh M.
    full: Mitesh M. Khapra
    id: mitesh-m-khapra
    last: Khapra
  author_string: Suman Banerjee, Nikita Moghe, Siddhartha Arora, Mitesh M. Khapra
  bibkey: banerjee-etal-2018-dataset
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3766'
  page_last: '3780'
  pages: "3766\u20133780"
  paper_id: '319'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1319.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1319.jpg
  title: A Dataset for Building Code-Mixed Goal Oriented Conversation Systems
  title_html: A Dataset for Building Code-Mixed Goal Oriented Conversation Systems
  url: https://www.aclweb.org/anthology/C18-1319
  year: '2018'
C18-1320:
  abstract: Classic pipeline models for task-oriented dialogue system require explicit
    modeling the dialogue states and hand-crafted action spaces to query a domain-specific
    knowledge base. Conversely, sequence-to-sequence models learn to map dialogue
    history to the response in current turn without explicit knowledge base querying.
    In this work, we propose a novel framework that leverages the advantages of classic
    pipeline and sequence-to-sequence models. Our framework models a dialogue state
    as a fixed-size distributed representation and use this representation to query
    a knowledge base via an attention mechanism. Experiment on Stanford Multi-turn
    Multi-domain Task-oriented Dialogue Dataset shows that our framework significantly
    outperforms other sequence-to-sequence based baseline models on both automatic
    and human evaluation.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Haoyang
    full: Haoyang Wen
    id: haoyang-wen
    last: Wen
  - first: Yijia
    full: Yijia Liu
    id: yijia-liu
    last: Liu
  - first: Wanxiang
    full: Wanxiang Che
    id: wanxiang-che
    last: Che
  - first: Libo
    full: Libo Qin
    id: libo-qin
    last: Qin
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  author_string: Haoyang Wen, Yijia Liu, Wanxiang Che, Libo Qin, Ting Liu
  bibkey: wen-etal-2018-sequence
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3781'
  page_last: '3792'
  pages: "3781\u20133792"
  paper_id: '320'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1320.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1320.jpg
  title: Sequence-to-Sequence Learning for Task-oriented Dialogue with Dialogue State
    Representation
  title_html: Sequence-to-Sequence Learning for Task-oriented Dialogue with Dialogue
    State Representation
  url: https://www.aclweb.org/anthology/C18-1320
  year: '2018'
C18-1321:
  abstract: Current paper explores the use of multi-view learning for search result
    clustering. A web-snippet can be represented using multiple views. Apart from
    textual view cued by both the semantic and syntactic information, a complimentary
    view extracted from images contained in the web-snippets is also utilized in the
    current framework. A single consensus partitioning is finally obtained after consulting
    these two individual views by the deployment of a multiobjective based clustering
    technique. Several objective functions including the values of a cluster quality
    measure measuring the goodness of partitionings obtained using different views
    and an agreement-disagreement index, quantifying the amount of oneness among multiple
    views in generating partitionings are optimized simultaneously using AMOSA. In
    order to detect the number of clusters automatically, concepts of variable length
    solutions and a vast range of permutation operators are introduced in the clustering
    process. Finally, a set of alternative partitioning are obtained on the final
    Pareto front by the proposed multi-view based multiobjective technique. Experimental
    results by the proposed approach on several benchmark test datasets of SRC with
    respect to different performance metrics evidently establish the power of visual
    and text-based views in achieving better search result clustering.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Sayantan
    full: Sayantan Mitra
    id: sayantan-mitra
    last: Mitra
  - first: Mohammed
    full: Mohammed Hasanuzzaman
    id: mohammed-hasanuzzaman
    last: Hasanuzzaman
  - first: Sriparna
    full: Sriparna Saha
    id: sriparna-saha
    last: Saha
  - first: Andy
    full: Andy Way
    id: andy-way
    last: Way
  author_string: Sayantan Mitra, Mohammed Hasanuzzaman, Sriparna Saha, Andy Way
  bibkey: mitra-etal-2018-incorporating
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3793'
  page_last: '3805'
  pages: "3793\u20133805"
  paper_id: '321'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1321.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1321.jpg
  title: Incorporating Deep Visual Features into Multiobjective based Multi-view Search
    Results Clustering
  title_html: Incorporating Deep Visual Features into Multiobjective based Multi-view
    Search Results Clustering
  url: https://www.aclweb.org/anthology/C18-1321
  year: '2018'
C18-1322:
  abstract: We proposed a model that integrates discussion structures with neural
    networks to classify discourse acts. Several attempts have been made in earlier
    works to analyze texts that are used in various discussions. The importance of
    discussion structures has been explored in those works but their methods required
    a sophisticated design to combine structural features with a classifier. Our model
    introduces tree learning approaches and a graph learning approach to directly
    capture discussion structures without structural features. In an evaluation to
    classify discussion discourse acts in Reddit, the model achieved improvements
    of 1.5% in accuracy and 2.2 in FB1 score compared to the previous best model.
    We further analyzed the model using an attention mechanism to inspect interactions
    among different learning approaches.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yasuhide
    full: Yasuhide Miura
    id: yasuhide-miura
    last: Miura
  - first: Ryuji
    full: Ryuji Kano
    id: ryuji-kano
    last: Kano
  - first: Motoki
    full: Motoki Taniguchi
    id: motoki-taniguchi
    last: Taniguchi
  - first: Tomoki
    full: Tomoki Taniguchi
    id: tomoki-taniguchi
    last: Taniguchi
  - first: Shotaro
    full: Shotaro Misawa
    id: shotaro-misawa
    last: Misawa
  - first: Tomoko
    full: Tomoko Ohkuma
    id: tomoko-ohkuma
    last: Ohkuma
  author_string: Yasuhide Miura, Ryuji Kano, Motoki Taniguchi, Tomoki Taniguchi, Shotaro
    Misawa, Tomoko Ohkuma
  bibkey: miura-etal-2018-integrating
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3806'
  page_last: '3818'
  pages: "3806\u20133818"
  paper_id: '322'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1322.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1322.jpg
  title: Integrating Tree Structures and Graph Structures with Neural Networks to
    Classify Discussion Discourse Acts
  title_html: Integrating Tree Structures and Graph Structures with Neural Networks
    to Classify Discussion Discourse Acts
  url: https://www.aclweb.org/anthology/C18-1322
  year: '2018'
C18-1323:
  abstract: In this paper, we present AnlamVer, which is a semantic model evaluation
    dataset for Turkish designed to evaluate word similarity and word relatedness
    tasks while discriminating those two relations from each other. Our dataset consists
    of 500 word-pairs annotated by 12 human subjects, and each pair has two distinct
    scores for similarity and relatedness. Word-pairs are selected to enable the evaluation
    of distributional semantic models by multiple attributes of words and word-pair
    relations such as frequency, morphology, concreteness and relation types (e.g.,
    synonymy, antonymy). Our aim is to provide insights to semantic model researchers
    by evaluating models in multiple attributes. We balance dataset word-pairs by
    their frequencies to evaluate the robustness of semantic models concerning out-of-vocabulary
    and rare words problems, which are caused by the rich derivational and inflectional
    morphology of the Turkish language.
  address: Santa Fe, New Mexico, USA
  author:
  - first: "G\xF6khan"
    full: "G\xF6khan Ercan"
    id: gokhan-ercan
    last: Ercan
  - first: Olcay Taner
    full: "Olcay Taner Y\u0131ld\u0131z"
    id: olcay-taner-yildiz
    last: "Y\u0131ld\u0131z"
  author_string: "G\xF6khan Ercan, Olcay Taner Y\u0131ld\u0131z"
  bibkey: ercan-yildiz-2018-anlamver
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3819'
  page_last: '3836'
  pages: "3819\u20133836"
  paper_id: '323'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1323.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1323.jpg
  title: 'AnlamVer: Semantic Model Evaluation Dataset for Turkish - Word Similarity
    and Relatedness'
  title_html: '<span class="acl-fixed-case">A</span>nlam<span class="acl-fixed-case">V</span>er:
    Semantic Model Evaluation Dataset for <span class="acl-fixed-case">T</span>urkish
    - Word Similarity and Relatedness'
  url: https://www.aclweb.org/anthology/C18-1323
  year: '2018'
C18-1324:
  abstract: "The aim of this paper is to argue for a coherent Universal Dependencies\
    \ approach to the core vs. non-core distinction. We demonstrate inconsistencies\
    \ in the current version 2 of UD in this respect \u2013 mostly resulting from\
    \ the preservation of the argument\u2013adjunct dichotomy despite the declared\
    \ avoidance of this distinction \u2013 and propose a relatively conservative modification\
    \ of UD that is free from these problems."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Adam
    full: "Adam Przepi\xF3rkowski"
    id: adam-przepiorkowski
    last: "Przepi\xF3rkowski"
  - first: Agnieszka
    full: Agnieszka Patejuk
    id: agnieszka-patejuk
    last: Patejuk
  author_string: "Adam Przepi\xF3rkowski, Agnieszka Patejuk"
  bibkey: przepiorkowski-patejuk-2018-arguments
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3837'
  page_last: '3852'
  pages: "3837\u20133852"
  paper_id: '324'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1324.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1324.jpg
  title: Arguments and Adjuncts in Universal Dependencies
  title_html: Arguments and Adjuncts in Universal Dependencies
  url: https://www.aclweb.org/anthology/C18-1324
  year: '2018'
C18-1325:
  abstract: "We study German affixoids, a type of morpheme in between affixes and\
    \ free stems. Several properties have been associated with them \u2013 increased\
    \ productivity; a bleached semantics, which is often evaluative and/or intensifying\
    \ and thus of relevance to sentiment analysis; and the existence of a free morpheme\
    \ counterpart \u2013 but not been validated empirically. In experiments on a new\
    \ data set that we make available, we put these key assumptions from the morphological\
    \ literature to the test and show that despite the fact that affixoids generate\
    \ many low-frequency formations, we can classify these as affixoid or non-affixoid\
    \ instances with a best F1-score of 74%."
  address: Santa Fe, New Mexico, USA
  author:
  - first: Josef
    full: Josef Ruppenhofer
    id: josef-ruppenhofer
    last: Ruppenhofer
  - first: Michael
    full: Michael Wiegand
    id: michael-wiegand
    last: Wiegand
  - first: Rebecca
    full: Rebecca Wilm
    id: rebecca-wilm
    last: Wilm
  - first: Katja
    full: Katja Markert
    id: katja-markert
    last: Markert
  author_string: Josef Ruppenhofer, Michael Wiegand, Rebecca Wilm, Katja Markert
  bibkey: ruppenhofer-etal-2018-distinguishing
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3853'
  page_last: '3865'
  pages: "3853\u20133865"
  paper_id: '325'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1325.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1325.jpg
  title: Distinguishing affixoid formations from compounds
  title_html: Distinguishing affixoid formations from compounds
  url: https://www.aclweb.org/anthology/C18-1325
  year: '2018'
C18-1326:
  abstract: We provide a detailed overview of the various approaches that were proposed
    to date to solve the task of Open Information Extraction. We present the major
    challenges that such systems face, show the evolution of the suggested approaches
    over time and depict the specific issues they address. In addition, we provide
    a critique of the commonly applied evaluation procedures for assessing the performance
    of Open IE systems and highlight some directions for future work.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Christina
    full: Christina Niklaus
    id: christina-niklaus
    last: Niklaus
  - first: Matthias
    full: Matthias Cetto
    id: matthias-cetto
    last: Cetto
  - first: "Andr\xE9"
    full: "Andr\xE9 Freitas"
    id: andre-freitas
    last: Freitas
  - first: Siegfried
    full: Siegfried Handschuh
    id: siegfried-handschuh
    last: Handschuh
  author_string: "Christina Niklaus, Matthias Cetto, Andr\xE9 Freitas, Siegfried Handschuh"
  bibkey: niklaus-etal-2018-survey
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3866'
  page_last: '3878'
  pages: "3866\u20133878"
  paper_id: '326'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1326.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1326.jpg
  title: A Survey on Open Information Extraction
  title_html: A Survey on Open Information Extraction
  url: https://www.aclweb.org/anthology/C18-1326
  year: '2018'
C18-1327:
  abstract: We investigate the design challenges of constructing effective and efficient
    neural sequence labeling systems, by reproducing twelve neural sequence labeling
    models, which include most of the state-of-the-art structures, and conduct a systematic
    model comparison on three benchmarks (i.e. NER, Chunking, and POS tagging). Misconceptions
    and inconsistent conclusions in existing literature are examined and clarified
    under statistical experiments. In the comparison and analysis process, we reach
    several practical conclusions which can be useful to practitioners.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Jie
    full: Jie Yang
    id: jie-yang
    last: Yang
  - first: Shuailong
    full: Shuailong Liang
    id: shuailong-liang
    last: Liang
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  author_string: Jie Yang, Shuailong Liang, Yue Zhang
  bibkey: yang-etal-2018-design
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3879'
  page_last: '3889'
  pages: "3879\u20133889"
  paper_id: '327'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1327.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1327.jpg
  title: Design Challenges and Misconceptions in Neural Sequence Labeling
  title_html: Design Challenges and Misconceptions in Neural Sequence Labeling
  url: https://www.aclweb.org/anthology/C18-1327
  year: '2018'
C18-1328:
  abstract: In this paper, we analyze several neural network designs (and their variations)
    for sentence pair modeling and compare their performance extensively across eight
    datasets, including paraphrase identification, semantic textual similarity, natural
    language inference, and question answering tasks. Although most of these models
    have claimed state-of-the-art performance, the original papers often reported
    on only one or two selected datasets. We provide a systematic study and show that
    (i) encoding contextual information by LSTM and inter-sentence interactions are
    critical, (ii) Tree-LSTM does not help as much as previously claimed but surprisingly
    improves performance on Twitter datasets, (iii) the Enhanced Sequential Inference
    Model is the best so far for larger datasets, while the Pairwise Word Interaction
    Model achieves the best performance when less data is available. We release our
    implementations as an open-source toolkit.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Wuwei
    full: Wuwei Lan
    id: wuwei-lan
    last: Lan
  - first: Wei
    full: Wei Xu
    id: wei-xu
    last: Xu
  author_string: Wuwei Lan, Wei Xu
  bibkey: lan-xu-2018-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3890'
  page_last: '3902'
  pages: "3890\u20133902"
  paper_id: '328'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1328.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1328.jpg
  title: Neural Network Models for Paraphrase Identification, Semantic Textual Similarity,
    Natural Language Inference, and Question Answering
  title_html: Neural Network Models for Paraphrase Identification, Semantic Textual
    Similarity, Natural Language Inference, and Question Answering
  url: https://www.aclweb.org/anthology/C18-1328
  year: '2018'
C18-1329:
  abstract: Most previous work in unsupervised semantic modeling in the presence of
    metadata has assumed that our goal is to make latent dimensions more correlated
    with metadata, but in practice the exact opposite is often true. Some users want
    topic models that highlight differences between, for example, authors, but others
    seek more subtle connections across authors. We introduce three metrics for identifying
    topics that are highly correlated with metadata, and demonstrate that this problem
    affects between 30 and 50% of the topics in models trained on two real-world collections,
    regardless of the size of the model. We find that we can predict which words cause
    this phenomenon and that by selectively subsampling these words we dramatically
    reduce topic-metadata correlation, improve topic stability, and maintain or even
    improve model quality.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Laure
    full: Laure Thompson
    id: laure-thompson
    last: Thompson
  - first: David
    full: David Mimno
    id: david-mimno
    last: Mimno
  author_string: Laure Thompson, David Mimno
  bibkey: thompson-mimno-2018-authorless
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3903'
  page_last: '3914'
  pages: "3903\u20133914"
  paper_id: '329'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1329.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1329.jpg
  title: 'Authorless Topic Models: Biasing Models Away from Known Structure'
  title_html: 'Authorless Topic Models: Biasing Models Away from Known Structure'
  url: https://www.aclweb.org/anthology/C18-1329
  year: '2018'
C18-1330:
  abstract: Multi-label classification is an important yet challenging task in natural
    language processing. It is more complex than single-label classification in that
    the labels tend to be correlated. Existing methods tend to ignore the correlations
    between labels. Besides, different parts of the text can contribute differently
    for predicting different labels, which is not considered by existing models. In
    this paper, we propose to view the multi-label classification task as a sequence
    generation problem, and apply a sequence generation model with a novel decoder
    structure to solve it. Extensive experimental results show that our proposed methods
    outperform previous work by a substantial margin. Further analysis of experimental
    results demonstrates that the proposed methods not only capture the correlations
    between labels, but also select the most informative words automatically when
    predicting different labels.
  address: Santa Fe, New Mexico, USA
  author:
  - first: Pengcheng
    full: Pengcheng Yang
    id: pengcheng-yang
    last: Yang
  - first: Xu
    full: Xu Sun
    id: xu-sun
    last: Sun
  - first: Wei
    full: Wei Li
    id: wei-li
    last: Li
  - first: Shuming
    full: Shuming Ma
    id: shuming-ma
    last: Ma
  - first: Wei
    full: Wei Wu
    id: wei-wu
    last: Wu
  - first: Houfeng
    full: Houfeng Wang
    id: houfeng-wang
    last: Wang
  author_string: Pengcheng Yang, Xu Sun, Wei Li, Shuming Ma, Wei Wu, Houfeng Wang
  bibkey: yang-etal-2018-sgm
  bibtype: inproceedings
  booktitle: Proceedings of the 27th International Conference on Computational Linguistics
  booktitle_html: Proceedings of the 27th International Conference on Computational
    Linguistics
  month: August
  page_first: '3915'
  page_last: '3926'
  pages: "3915\u20133926"
  paper_id: '330'
  parent_volume_id: C18-1
  pdf: https://www.aclweb.org/anthology/C18-1330.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-1330.jpg
  title: 'SGM: Sequence Generation Model for Multi-label Classification'
  title_html: '<span class="acl-fixed-case">SGM</span>: Sequence Generation Model
    for Multi-label Classification'
  url: https://www.aclweb.org/anthology/C18-1330
  year: '2018'
C18-2000:
  address: Santa Fe, New Mexico
  author:
  - first: Dongyan
    full: Dongyan Zhao
    id: dongyan-zhao
    last: Zhao
  author_string: Dongyan Zhao
  bibkey: coling-2018-international-linguistics
  bibtype: proceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  month: August
  paper_id: '0'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2000.jpg
  title: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  title_html: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  url: https://www.aclweb.org/anthology/C18-2000
  year: '2018'
C18-2001:
  abstract: Abbreviations and acronyms are a part of textual communication in most
    domains. However, abbreviations are not necessarily defined in documents that
    employ them. Understanding all abbreviations used in a given document often requires
    extensive knowledge of the target domain and the ability to disambiguate based
    on context. This creates considerable entry barriers to newcomers and difficulties
    in automated document processing. Existing abbreviation expansion systems or tools
    require substantial technical knowledge for set up or make strong assumptions
    which limit their use in practice. Here, we present Abbreviation Expander, a system
    that builds on state of the art methods for identification of abbreviations, acronyms
    and their definitions and a novel disambiguator for abbreviation expansion in
    an easily accessible web-based solution.
  address: Santa Fe, New Mexico
  author:
  - first: Manuel R.
    full: Manuel R. Ciosici
    id: manuel-r-ciosici
    last: Ciosici
  - first: Ira
    full: Ira Assent
    id: ira-assent
    last: Assent
  author_string: Manuel R. Ciosici, Ira Assent
  bibkey: ciosici-assent-2018-abbreviation
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '1'
  page_last: '4'
  pages: "1\u20134"
  paper_id: '1'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2001.jpg
  title: Abbreviation Expander - a Web-based System for Easy Reading of Technical
    Documents
  title_html: Abbreviation Expander - a Web-based System for Easy Reading of Technical
    Documents
  url: https://www.aclweb.org/anthology/C18-2001
  year: '2018'
C18-2002:
  abstract: We introduce INCEpTION, a new annotation platform for tasks including
    interactive and semantic annotation (e.g., concept linking, fact linking, knowledge
    base population, semantic frame annotation). These tasks are very time consuming
    and demanding for annotators, especially when knowledge bases are used. We address
    these issues by developing an annotation platform that incorporates machine learning
    capabilities which actively assist and guide annotators. The platform is both
    generic and modular. It targets a range of research domains in need of semantic
    annotation, such as digital humanities, bioinformatics, or linguistics. INCEpTION
    is publicly available as open-source software.
  address: Santa Fe, New Mexico
  author:
  - first: Jan-Christoph
    full: Jan-Christoph Klie
    id: jan-christoph-klie
    last: Klie
  - first: Michael
    full: Michael Bugert
    id: michael-bugert
    last: Bugert
  - first: Beto
    full: Beto Boullosa
    id: beto-boullosa
    last: Boullosa
  - first: Richard
    full: Richard Eckart de Castilho
    id: richard-eckart-de-castilho
    last: Eckart de Castilho
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Jan-Christoph Klie, Michael Bugert, Beto Boullosa, Richard Eckart
    de Castilho, Iryna Gurevych
  bibkey: klie-etal-2018-inception
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '5'
  page_last: '9'
  pages: "5\u20139"
  paper_id: '2'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2002.jpg
  title: 'The INCEpTION Platform: Machine-Assisted and Knowledge-Oriented Interactive
    Annotation'
  title_html: 'The <span class="acl-fixed-case">INCE</span>p<span class="acl-fixed-case">TION</span>
    Platform: Machine-Assisted and Knowledge-Oriented Interactive Annotation'
  url: https://www.aclweb.org/anthology/C18-2002
  year: '2018'
C18-2003:
  abstract: We here introduce a substantially extended version of JeSemE, an interactive
    website for visually exploring computationally derived time-variant information
    on word meanings and lexical emotions assembled from five large diachronic text
    corpora. JeSemE is designed for scholars in the (digital) humanities as an alternative
    to consulting manually compiled, printed dictionaries for such information (if
    available at all). This tool uniquely combines state-of-the-art distributional
    semantics with a nuanced model of human emotions, two information streams we deem
    beneficial for a data-driven interpretation of texts in the humanities.
  address: Santa Fe, New Mexico
  author:
  - first: Johannes
    full: Johannes Hellrich
    id: johannes-hellrich
    last: Hellrich
  - first: Sven
    full: Sven Buechel
    id: sven-buechel
    last: Buechel
  - first: Udo
    full: Udo Hahn
    id: udo-hahn
    last: Hahn
  author_string: Johannes Hellrich, Sven Buechel, Udo Hahn
  bibkey: hellrich-etal-2018-jeseme
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '10'
  page_last: '14'
  pages: "10\u201314"
  paper_id: '3'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2003.jpg
  title: 'JeSemE: Interleaving Semantics and Emotions in a Web Service for the Exploration
    of Language Change Phenomena'
  title_html: '<span class="acl-fixed-case">J</span>e<span class="acl-fixed-case">S</span>em<span
    class="acl-fixed-case">E</span>: Interleaving Semantics and Emotions in a Web
    Service for the Exploration of Language Change Phenomena'
  url: https://www.aclweb.org/anthology/C18-2003
  year: '2018'
C18-2004:
  abstract: T-Know is a knowledge service system based on the constructed knowledge
    graph of Traditional Chinese Medicine (TCM). Using authorized and anonymized clinical
    records, medicine clinical guidelines, teaching materials, classic medical books,
    academic publications, etc., as data resources, the system extracts triples from
    free texts to build a TCM knowledge graph by our developed natural language processing
    methods. On the basis of the knowledge graph, a deep learning algorithm is implemented
    for single-round question understanding and multiple-round dialogue. In addition,
    the TCM knowledge graph also is used to support human-computer interactive knowledge
    retrieval by normalizing search keywords to medical terminology.
  address: Santa Fe, New Mexico
  author:
  - first: Ziqing
    full: Ziqing Liu
    id: ziqing-liu
    last: Liu
  - first: Enwei
    full: Enwei Peng
    id: enwei-peng
    last: Peng
  - first: Shixing
    full: Shixing Yan
    id: shixing-yan
    last: Yan
  - first: Guozheng
    full: Guozheng Li
    id: guozheng-li
    last: Li
  - first: Tianyong
    full: Tianyong Hao
    id: tianyong-hao
    last: Hao
  author_string: Ziqing Liu, Enwei Peng, Shixing Yan, Guozheng Li, Tianyong Hao
  bibkey: liu-etal-2018-know
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '15'
  page_last: '19'
  pages: "15\u201319"
  paper_id: '4'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2004.jpg
  title: 'T-Know: a Knowledge Graph-based Question Answering and Infor-mation Retrieval
    System for Traditional Chinese Medicine'
  title_html: 'T-Know: a Knowledge Graph-based Question Answering and Infor-mation
    Retrieval System for Traditional <span class="acl-fixed-case">C</span>hinese Medicine'
  url: https://www.aclweb.org/anthology/C18-2004
  year: '2018'
C18-2005:
  abstract: The increased demand for structured knowledge has created considerable
    interest in knowledge extraction from natural language sentences. This study presents
    a new Korean knowledge extraction system and web interface for enriching a KBox
    knowledge base that expands based on the Korean DBpedia. The aim is to create
    an endpoint where knowledge can be extracted and added to KBox anytime and anywhere.
  address: Santa Fe, New Mexico
  author:
  - first: Sangha
    full: Sangha Nam
    id: sangha-nam
    last: Nam
  - first: Eun-kyung
    full: Eun-kyung Kim
    id: eun-kyung-kim
    last: Kim
  - first: Jiho
    full: Jiho Kim
    id: jiho-kim
    last: Kim
  - first: Yoosung
    full: Yoosung Jung
    id: yoosung-jung
    last: Jung
  - first: Kijong
    full: Kijong Han
    id: kijong-han
    last: Han
  - first: Key-Sun
    full: Key-Sun Choi
    id: key-sun-choi
    last: Choi
  author_string: Sangha Nam, Eun-kyung Kim, Jiho Kim, Yoosung Jung, Kijong Han, Key-Sun
    Choi
  bibkey: nam-etal-2018-korean
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '20'
  page_last: '24'
  pages: "20\u201324"
  paper_id: '5'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2005.jpg
  title: A Korean Knowledge Extraction System for Enriching a KBox
  title_html: A <span class="acl-fixed-case">K</span>orean Knowledge Extraction System
    for Enriching a <span class="acl-fixed-case">KB</span>ox
  url: https://www.aclweb.org/anthology/C18-2005
  year: '2018'
C18-2006:
  abstract: Twitter has become one of the most import channels to spread latest scholarly
    information because of its fast information spread speed. How to predict whether
    a scholarly tweet will be retweeted is a key task in understanding the message
    propagation within large user communities. Hence, we present the real-time scholarly
    retweeting prediction system that retrieves scholarly tweets which will be retweeted.
    First, we filter scholarly tweets from tracking a tweet stream. Then, we extract
    Tweet Scholar Blocks indicating metadata of papers. At last, we combine scholarly
    features with the Tweet Scholar Blocks to predict whether a scholarly tweet will
    be retweeted. Our system outperforms chosen baseline systems. Additionally, our
    system has the potential to predict scientific impact in real-time.
  address: Santa Fe, New Mexico
  author:
  - first: Zhunchen
    full: Zhunchen Luo
    id: zhunchen-luo
    last: Luo
  - first: Xiao
    full: Xiao Liu
    id: xiao-liu
    last: Liu
  author_string: Zhunchen Luo, Xiao Liu
  bibkey: luo-liu-2018-real
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '25'
  page_last: '29'
  pages: "25\u201329"
  paper_id: '6'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2006.jpg
  title: Real-time Scholarly Retweeting Prediction System
  title_html: Real-time Scholarly Retweeting Prediction System
  url: https://www.aclweb.org/anthology/C18-2006
  year: '2018'
C18-2007:
  abstract: "We tackle the problem of generating a diagrammatic summary of a set of\
    \ documents each of which pertains to loosely related topics. In particular, we\
    \ aim at visualizing the medical histories of patients. In medicine, choosing\
    \ relevant reports from a patient\u2019s past exams for comparison provide valuable\
    \ information for precise treatment planning. Manually finding the relevant reports\
    \ for comparison studies from a large database is time-consuming, which could\
    \ result overlooking of some critical information. This task can be automated\
    \ by defining similarity among documents which is a nontrivial task since these\
    \ documents are often stored in an unstructured text format. To facilitate this,\
    \ we have used a representation learning algorithm that creates a semantic representation\
    \ space for documents where the clinically related documents lie close to each\
    \ other. We have utilized referral information to weakly supervise a LSTM network\
    \ to learn this semantic space. The abstract representations within this semantic\
    \ space are not only useful to visualize disease progressions corresponding to\
    \ the relevant report groups of a patient, but are also beneficial to analyze\
    \ diseases at the population level. The proposed key tool here is clustering of\
    \ documents based on the document similarity whose metric is learned from corpora."
  address: Santa Fe, New Mexico
  author:
  - first: Halid Ziya
    full: Halid Ziya Yerebakan
    id: halid-ziya-yerebakan
    last: Yerebakan
  - first: Yoshihisa
    full: Yoshihisa Shinagawa
    id: yoshihisa-shinagawa
    last: Shinagawa
  - first: Parmeet
    full: Parmeet Bhatia
    id: parmeet-bhatia
    last: Bhatia
  - first: Yiqiang
    full: Yiqiang Zhan
    id: yiqiang-zhan
    last: Zhan
  author_string: Halid Ziya Yerebakan, Yoshihisa Shinagawa, Parmeet Bhatia, Yiqiang
    Zhan
  bibkey: yerebakan-etal-2018-document
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '30'
  page_last: '33'
  pages: "30\u201333"
  paper_id: '7'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2007.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2007.jpg
  title: Document Representation Learning for Patient History Visualization
  title_html: Document Representation Learning for Patient History Visualization
  url: https://www.aclweb.org/anthology/C18-2007
  year: '2018'
C18-2008:
  abstract: 'As the quantity of publications increases daily, researchers are forced
    to narrow their attention to their own specialism and are therefore less likely
    to make new connections with other areas. Literature based discovery (LBD) supports
    the identification of such connections. A number of LBD tools are available, however,
    they often suffer from limitations such as constraining possible searches or not
    producing results in real-time. We introduce HiDE (Hidden Discovery Explorer),
    an online knowledge browsing tool which allows fast access to hidden knowledge
    generated from all abstracts in Medline. HiDE is fast enough to allow users to
    explore the full range of hidden connections generated by an LBD system. The tool
    employs a novel combination of two approaches to LBD: a graph-based approach which
    allows hidden knowledge to be generated on a large scale and an inference algorithm
    to identify the most promising (most likely to be non trivial) information. Available
    at https://skye.shef.ac.uk/kdisc'
  address: Santa Fe, New Mexico
  author:
  - first: Judita
    full: Judita Preiss
    id: judita-preiss
    last: Preiss
  - first: Mark
    full: Mark Stevenson
    id: mark-stevenson
    last: Stevenson
  author_string: Judita Preiss, Mark Stevenson
  bibkey: preiss-stevenson-2018-hide
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '34'
  page_last: '37'
  pages: "34\u201337"
  paper_id: '8'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2008.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2008.jpg
  title: 'HiDE: a Tool for Unrestricted Literature Based Discovery'
  title_html: '<span class="acl-fixed-case">H</span>i<span class="acl-fixed-case">DE</span>:
    a Tool for Unrestricted Literature Based Discovery'
  url: https://www.aclweb.org/anthology/C18-2008
  year: '2018'
C18-2009:
  abstract: We present a language-independent treebank annotation tool supporting
    rich annotations with discontinuous constituents and function tags. Candidate
    analyses are generated by an exemplar-based parsing model that immediately learns
    from each new annotated sentence during annotation. This makes it suitable for
    situations in which only a limited seed treebank is available, or a radically
    different domain is being annotated. The tool offers the possibility to experiment
    with and evaluate active learning methods to speed up annotation in a naturalistic
    setting, i.e., measuring actual annotation costs and tracking specific user interactions.
    The code is made available under the GNU GPL license at https://github.com/andreasvc/activedop.
  address: Santa Fe, New Mexico
  author:
  - first: Andreas
    full: Andreas van Cranenburgh
    id: andreas-van-cranenburgh
    last: van Cranenburgh
  author_string: Andreas van Cranenburgh
  bibkey: van-cranenburgh-2018-active
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '38'
  page_last: '42'
  pages: "38\u201342"
  paper_id: '9'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2009.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2009.jpg
  title: 'Active DOP: an Active Learning Constituency Treebank Annotation Tool'
  title_html: 'Active <span class="acl-fixed-case">DOP</span>: an Active Learning
    Constituency Treebank Annotation Tool'
  url: https://www.aclweb.org/anthology/C18-2009
  year: '2018'
C18-2010:
  abstract: For controversial topics, collecting argumentation-containing tweets which
    tend to be more convincing will help researchers analyze public opinions. Meanwhile,
    claim is the heart of argumentation. Hence, we present the first real-time claim
    retrieval system CRST that retrieves tweets containing claims for a given topic
    from Twitter. We propose a claim-oriented ranking module which can be divided
    into the offline topic-independent learning to rank model and the online topic-dependent
    lexicon model. Our system outperforms previous claim retrieval system and argument
    mining system. Moreover, the claim-oriented ranking module can be easily adapted
    to new topics without any manual process or external information, guaranteeing
    the practicability of our system.
  address: Santa Fe, New Mexico
  author:
  - first: Wenjia
    full: Wenjia Ma
    id: wenjia-ma
    last: Ma
  - first: WenHan
    full: WenHan Chao
    id: wenhan-chao
    last: Chao
  - first: Zhunchen
    full: Zhunchen Luo
    id: zhunchen-luo
    last: Luo
  - first: Xin
    full: Xin Jiang
    id: xin-jiang
    last: Jiang
  author_string: Wenjia Ma, WenHan Chao, Zhunchen Luo, Xin Jiang
  bibkey: ma-etal-2018-crst
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '43'
  page_last: '47'
  pages: "43\u201347"
  paper_id: '10'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2010.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2010.jpg
  title: 'CRST: a Claim Retrieval System in Twitter'
  title_html: '<span class="acl-fixed-case">CRST</span>: a Claim Retrieval System
    in Twitter'
  url: https://www.aclweb.org/anthology/C18-2010
  year: '2018'
C18-2011:
  abstract: This demo deals with the problem of capturing omitted arguments in relation
    extraction given a proper knowledge base for entities of interest. This paper
    introduces the concept of a salient entity and use this information to deduce
    omitted entities in the paragraph which allows improving the relation extraction
    quality. The main idea to compute salient entities is to construct a graph on
    the given information (by identifying the entities but without parsing it), rank
    it with standard graph measures and embed it in the context of the sentences.
  address: Santa Fe, New Mexico
  author:
  - first: Eun-kyung
    full: Eun-kyung Kim
    id: eun-kyung-kim
    last: Kim
  - first: Kijong
    full: Kijong Han
    id: kijong-han
    last: Han
  - first: Jiho
    full: Jiho Kim
    id: jiho-kim
    last: Kim
  - first: Key-Sun
    full: Key-Sun Choi
    id: key-sun-choi
    last: Choi
  author_string: Eun-kyung Kim, Kijong Han, Jiho Kim, Key-Sun Choi
  bibkey: kim-etal-2018-utilizing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '48'
  page_last: '52'
  pages: "48\u201352"
  paper_id: '11'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2011.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2011.jpg
  title: Utilizing Graph Measure to Deduce Omitted Entities in Paragraphs
  title_html: Utilizing Graph Measure to Deduce Omitted Entities in Paragraphs
  url: https://www.aclweb.org/anthology/C18-2011
  year: '2018'
C18-2012:
  abstract: 'We present WOMBAT, a Python tool which supports NLP practitioners in
    accessing word embeddings from code. WOMBAT addresses common research problems,
    including unified access, scaling, and robust and reproducible preprocessing.
    Code that uses WOMBAT for accessing word embeddings is not only cleaner, more
    readable, and easier to reuse, but also much more efficient than code using standard
    in-memory methods: a Python script using WOMBAT for evaluating seven large word
    embedding collections (8.7M embedding vectors in total) on a simple SemEval sentence
    similarity task involving 250 raw sentence pairs completes in under ten seconds
    end-to-end on a standard notebook computer.'
  address: Santa Fe, New Mexico
  author:
  - first: Mark-Christoph
    full: "Mark-Christoph M\xFCller"
    id: mark-christoph-muller
    last: "M\xFCller"
  - first: Michael
    full: Michael Strube
    id: michael-strube
    last: Strube
  author_string: "Mark-Christoph M\xFCller, Michael Strube"
  bibkey: muller-strube-2018-transparent
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '53'
  page_last: '57'
  pages: "53\u201357"
  paper_id: '12'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2012.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2012.jpg
  title: Transparent, Efficient, and Robust Word Embedding Access with WOMBAT
  title_html: Transparent, Efficient, and Robust Word Embedding Access with <span
    class="acl-fixed-case">WOMBAT</span>
  url: https://www.aclweb.org/anthology/C18-2012
  year: '2018'
C18-2013:
  abstract: We present SetExpander, a corpus-based system for expanding a seed set
    of terms into a more complete set of terms that belong to the same semantic class.
    SetExpander implements an iterative end-to end workflow for term set expansion.
    It enables users to easily select a seed set of terms, expand it, view the expanded
    set, validate it, re-expand the validated set and store it, thus simplifying the
    extraction of domain-specific fine-grained semantic classes. SetExpander has been
    used for solving real-life use cases including integration in an automated recruitment
    system and an issues and defects resolution system. A video demo of SetExpander
    is available at https://drive.google.com/open?id=1e545bB87Autsch36DjnJHmq3HWfSd1Rv
    .
  address: Santa Fe, New Mexico
  author:
  - first: Jonathan
    full: Jonathan Mamou
    id: jonathan-mamou
    last: Mamou
  - first: Oren
    full: Oren Pereg
    id: oren-pereg
    last: Pereg
  - first: Moshe
    full: Moshe Wasserblat
    id: moshe-wasserblat
    last: Wasserblat
  - first: Ido
    full: Ido Dagan
    id: ido-dagan
    last: Dagan
  - first: Yoav
    full: Yoav Goldberg
    id: yoav-goldberg
    last: Goldberg
  - first: Alon
    full: Alon Eirew
    id: alon-eirew
    last: Eirew
  - first: Yael
    full: Yael Green
    id: yael-green
    last: Green
  - first: Shira
    full: Shira Guskin
    id: shira-guskin
    last: Guskin
  - first: Peter
    full: Peter Izsak
    id: peter-izsak
    last: Izsak
  - first: Daniel
    full: Daniel Korat
    id: daniel-korat
    last: Korat
  author_string: Jonathan Mamou, Oren Pereg, Moshe Wasserblat, Ido Dagan, Yoav Goldberg,
    Alon Eirew, Yael Green, Shira Guskin, Peter Izsak, Daniel Korat
  bibkey: mamou-etal-2018-setexpander
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '58'
  page_last: '62'
  pages: "58\u201362"
  paper_id: '13'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2013.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2013.jpg
  title: 'SetExpander: End-to-end Term Set Expansion Based on Multi-Context Term Embeddings'
  title_html: '<span class="acl-fixed-case">S</span>et<span class="acl-fixed-case">E</span>xpander:
    End-to-end Term Set Expansion Based on Multi-Context Term Embeddings'
  url: https://www.aclweb.org/anthology/C18-2013
  year: '2018'
C18-2014:
  abstract: We present our system that assists to detect heavy rain disaster, which
    is being used in real world in Japan. Our system selects tweets about heavy rain
    disaster with a document classifier. Then, the locations mentioned in the selected
    tweets are estimated by a location estimator. Finally, combined the selected tweets
    with amount of rainfall given by physical sensors and a statistical analysis,
    our system provides users with visualized results for detecting heavy rain disaster.
  address: Santa Fe, New Mexico
  author:
  - first: Tomoya
    full: Tomoya Iwakura
    id: tomoya-iwakura
    last: Iwakura
  - first: Seiji
    full: Seiji Okajima
    id: seiji-okajima
    last: Okajima
  - first: Nobuyuki
    full: Nobuyuki Igata
    id: nobuyuki-igata
    last: Igata
  - first: Kunihiro
    full: Kunihiro Takeda
    id: kunihiro-takeda
    last: Takeda
  - first: Yuzuru
    full: Yuzuru Yamakage
    id: yuzuru-yamakage
    last: Yamakage
  - first: Naoshi
    full: Naoshi Morita
    id: naoshi-morita
    last: Morita
  author_string: Tomoya Iwakura, Seiji Okajima, Nobuyuki Igata, Kunihiro Takeda, Yuzuru
    Yamakage, Naoshi Morita
  bibkey: iwakura-etal-2018-detecting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '63'
  page_last: '67'
  pages: "63\u201367"
  paper_id: '14'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2014.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2014.jpg
  title: Detecting Heavy Rain Disaster from Social and Physical Sensor
  title_html: Detecting Heavy Rain Disaster from Social and Physical Sensor
  url: https://www.aclweb.org/anthology/C18-2014
  year: '2018'
C18-2015:
  abstract: 'Language change across space and time is one of the main concerns in
    historical linguistics. In this paper, we develop a language evolution simulator:
    a web-based tool for word form production to assist in historical linguistics,
    in studying the evolution of the languages. Given a word in a source language,
    the system automatically predicts how the word evolves in a target language. The
    method that we propose is language-agnostic and does not use any external knowledge,
    except for the training word pairs.'
  address: Santa Fe, New Mexico
  author:
  - first: Alina Maria
    full: Alina Maria Ciobanu
    id: alina-maria-ciobanu
    last: Ciobanu
  - first: Liviu P.
    full: Liviu P. Dinu
    id: liviu-p-dinu
    last: Dinu
  author_string: Alina Maria Ciobanu, Liviu P. Dinu
  bibkey: ciobanu-dinu-2018-simulating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '68'
  page_last: '72'
  pages: "68\u201372"
  paper_id: '15'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2015.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2015.jpg
  title: 'Simulating Language Evolution: a Tool for Historical Linguistics'
  title_html: 'Simulating Language Evolution: a Tool for Historical Linguistics'
  url: https://www.aclweb.org/anthology/C18-2015
  year: '2018'
C18-2016:
  abstract: This paper demonstrates an end-to-end Chinese discourse parser. We propose
    a unified framework based on recursive neural network (RvNN) to jointly model
    the subtasks including elementary discourse unit (EDU) segmentation, tree structure
    construction, center labeling, and sense labeling. Experimental results show our
    parser achieves the state-of-the-art performance in the Chinese Discourse Treebank
    (CDTB) dataset. We release the source code with a pre-trained model for the NLP
    community. To the best of our knowledge, this is the first open source toolkit
    for Chinese discourse parsing. The standalone toolkit can be integrated into subsequent
    applications without the need of external resources such as syntactic parser.
  address: Santa Fe, New Mexico
  author:
  - first: Lin
    full: Lin Chuan-An
    id: lin-chuan-an
    last: Chuan-An
  - first: Hen-Hsen
    full: Hen-Hsen Huang
    id: hen-hsen-huang
    last: Huang
  - first: Zi-Yuan
    full: Zi-Yuan Chen
    id: zi-yuan-chen
    last: Chen
  - first: Hsin-Hsi
    full: Hsin-Hsi Chen
    id: hsin-hsi-chen
    last: Chen
  author_string: Lin Chuan-An, Hen-Hsen Huang, Zi-Yuan Chen, Hsin-Hsi Chen
  bibkey: chuan-an-etal-2018-unified
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '73'
  page_last: '77'
  pages: "73\u201377"
  paper_id: '16'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2016.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2016.jpg
  title: A Unified RvNN Framework for End-to-End Chinese Discourse Parsing
  title_html: A Unified <span class="acl-fixed-case">R</span>v<span class="acl-fixed-case">NN</span>
    Framework for End-to-End <span class="acl-fixed-case">C</span>hinese Discourse
    Parsing
  url: https://www.aclweb.org/anthology/C18-2016
  year: '2018'
C18-2017:
  abstract: 'Automatically highlighting a text aims at identifying key portions that
    are the most important to a reader. In this paper, we present a web-based framework
    designed to efficiently and scalably crowdsource two independent but related tasks:
    collecting highlight annotations, and comparing the performance of automated highlighting
    systems. The first task is necessary to understand human preferences and train
    supervised automated highlighting systems. The second task yields a more accurate
    and fine-grained evaluation than existing automated performance metrics.'
  address: Santa Fe, New Mexico
  author:
  - first: Sasha
    full: Sasha Spala
    id: sasha-spala
    last: Spala
  - first: Franck
    full: Franck Dernoncourt
    id: franck-dernoncourt
    last: Dernoncourt
  - first: Walter
    full: Walter Chang
    id: walter-chang
    last: Chang
  - first: Carl
    full: Carl Dockhorn
    id: carl-dockhorn
    last: Dockhorn
  author_string: Sasha Spala, Franck Dernoncourt, Walter Chang, Carl Dockhorn
  bibkey: spala-etal-2018-web
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '78'
  page_last: '81'
  pages: "78\u201381"
  paper_id: '17'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2017.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2017.jpg
  title: A Web-based Framework for Collecting and Assessing Highlighted Sentences
    in a Document
  title_html: A Web-based Framework for Collecting and Assessing Highlighted Sentences
    in a Document
  url: https://www.aclweb.org/anthology/C18-2017
  year: '2018'
C18-2018:
  abstract: This paper presents a grammatical error correction (GEC) system that provides
    corrective feedback for essays. We apply the sequence-to-sequence model, which
    is frequently used in machine translation and text summarization, to this GEC
    task. The model is trained by EF-Cambridge Open Language Database (EFCAMDAT),
    a large learner corpus annotated with grammatical errors and corrections. Evaluation
    shows that our system achieves competitive performance on a number of publicly
    available testsets.
  address: Santa Fe, New Mexico
  author:
  - first: Yu-Chun
    full: Yu-Chun Lo
    id: yu-chun-lo
    last: Lo
  - first: Jhih-Jie
    full: Jhih-Jie Chen
    id: jhih-jie-chen
    last: Chen
  - first: Chingyu
    full: Chingyu Yang
    id: chingyu-yang
    last: Yang
  - first: Jason
    full: Jason Chang
    id: jason-s-chang
    last: Chang
  author_string: Yu-Chun Lo, Jhih-Jie Chen, Chingyu Yang, Jason Chang
  bibkey: lo-etal-2018-cool
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '82'
  page_last: '85'
  pages: "82\u201385"
  paper_id: '18'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2018.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2018.jpg
  title: 'Cool English: a Grammatical Error Correction System Based on Large Learner
    Corpora'
  title_html: 'Cool <span class="acl-fixed-case">E</span>nglish: a Grammatical Error
    Correction System Based on Large Learner Corpora'
  url: https://www.aclweb.org/anthology/C18-2018
  year: '2018'
C18-2019:
  abstract: We present Appraise, an open-source framework for crowd-based annotation
    tasks, notably for evaluation of machine translation output. This is the software
    used to run the yearly evaluation campaigns for shared tasks at the WMT Conference
    on Machine Translation. It has also been used at IWSLT 2017 and, recently, to
    measure human parity for machine translation for Chinese to English news text.
    The demo will present the full end-to-end lifecycle of an Appraise evaluation
    campaign, from task creation to annotation and interpretation of results.
  address: Santa Fe, New Mexico
  author:
  - first: Christian
    full: Christian Federmann
    id: christian-federmann
    last: Federmann
  author_string: Christian Federmann
  bibkey: federmann-2018-appraise
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '86'
  page_last: '88'
  pages: "86\u201388"
  paper_id: '19'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2019.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2019.jpg
  title: Appraise Evaluation Framework for Machine Translation
  title_html: Appraise Evaluation Framework for Machine Translation
  url: https://www.aclweb.org/anthology/C18-2019
  year: '2018'
C18-2020:
  abstract: "In today\u2019s globalized world we have the ability to communicate with\
    \ people across the world. However, in many situations the language barrier still\
    \ presents a major issue. For example, many foreign students coming to KIT to\
    \ study are initially unable to follow a lecture in German. Therefore, we offer\
    \ an automatic simultaneous interpretation service for students. To fulfill this\
    \ task, we have developed a low-latency translation system that is adapted to\
    \ lectures and covers several language pairs. While the switch from traditional\
    \ Statistical Machine Translation to Neural Machine Translation (NMT) significantly\
    \ improved performance, to integrate NMT into the speech translation framework\
    \ required several adjustments. We have addressed the run-time constraints and\
    \ different types of input. Furthermore, we utilized one-shot learning to easily\
    \ add new topic-specific terms to the system. Besides better performance, NMT\
    \ also enabled us increase our covered languages through multilingual NMT. % Combining\
    \ these techniques, we are able to provide an adapted speech translation system\
    \ for several European languages."
  address: Santa Fe, New Mexico
  author:
  - first: Florian
    full: Florian Dessloch
    id: florian-dessloch
    last: Dessloch
  - first: Thanh-Le
    full: Thanh-Le Ha
    id: thanh-le-ha
    last: Ha
  - first: Markus
    full: "Markus M\xFCller"
    id: markus-muller
    last: "M\xFCller"
  - first: Jan
    full: Jan Niehues
    id: jan-niehues
    last: Niehues
  - first: Thai-Son
    full: Thai-Son Nguyen
    id: thai-son-nguyen1
    last: Nguyen
  - first: Ngoc-Quan
    full: Ngoc-Quan Pham
    id: ngoc-quan-pham
    last: Pham
  - first: Elizabeth
    full: Elizabeth Salesky
    id: elizabeth-salesky
    last: Salesky
  - first: Matthias
    full: Matthias Sperber
    id: matthias-sperber
    last: Sperber
  - first: Sebastian
    full: "Sebastian St\xFCker"
    id: sebastian-stuker
    last: "St\xFCker"
  - first: Thomas
    full: Thomas Zenkel
    id: thomas-zenkel
    last: Zenkel
  - first: Alexander
    full: Alexander Waibel
    id: alex-waibel
    last: Waibel
  author_string: "Florian Dessloch, Thanh-Le Ha, Markus M\xFCller, Jan Niehues, Thai-Son\
    \ Nguyen, Ngoc-Quan Pham, Elizabeth Salesky, Matthias Sperber, Sebastian St\xFC\
    ker, Thomas Zenkel, Alexander Waibel"
  bibkey: dessloch-etal-2018-kit
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '89'
  page_last: '93'
  pages: "89\u201393"
  paper_id: '20'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2020.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2020.jpg
  title: 'KIT Lecture Translator: Multilingual Speech Translation with One-Shot Learning'
  title_html: '<span class="acl-fixed-case">KIT</span> Lecture Translator: Multilingual
    Speech Translation with One-Shot Learning'
  url: https://www.aclweb.org/anthology/C18-2020
  year: '2018'
C18-2021:
  abstract: We introduce Graphene, an Open IE system whose goal is to generate accurate,
    meaningful and complete propositions that may facilitate a variety of downstream
    semantic applications. For this purpose, we transform syntactically complex input
    sentences into clean, compact structures in the form of core facts and accompanying
    contexts, while identifying the rhetorical relations that hold between them in
    order to maintain their semantic relationship. In that way, we preserve the context
    of the relational tuples extracted from a source sentence, generating a novel
    lightweight semantic representation for Open IE that enhances the expressiveness
    of the extracted propositions.
  address: Santa Fe, New Mexico
  author:
  - first: Matthias
    full: Matthias Cetto
    id: matthias-cetto
    last: Cetto
  - first: Christina
    full: Christina Niklaus
    id: christina-niklaus
    last: Niklaus
  - first: "Andr\xE9"
    full: "Andr\xE9 Freitas"
    id: andre-freitas
    last: Freitas
  - first: Siegfried
    full: Siegfried Handschuh
    id: siegfried-handschuh
    last: Handschuh
  author_string: "Matthias Cetto, Christina Niklaus, Andr\xE9 Freitas, Siegfried Handschuh"
  bibkey: cetto-etal-2018-graphene-context
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '94'
  page_last: '98'
  pages: "94\u201398"
  paper_id: '21'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2021.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2021.jpg
  title: 'Graphene: a Context-Preserving Open Information Extraction System'
  title_html: '<span class="acl-fixed-case">G</span>raphene: a Context-Preserving
    Open Information Extraction System'
  url: https://www.aclweb.org/anthology/C18-2021
  year: '2018'
C18-2022:
  abstract: In this paper, we present a system, LanguageNet, which can help second
    language learners to search for different meanings and usages of a word. We disambiguate
    word senses based on the pairs of an English word and its corresponding Chinese
    translations in a parallel corpus, UM-Corpus. The process involved performing
    word alignment, learning vector space representations of words and training a
    classifier to distinguish words into groups of senses. LanguageNet directly shows
    the definition of a sense, bilingual synonyms and sense relevant examples.
  address: Santa Fe, New Mexico
  author:
  - first: Shang-Chien
    full: Shang-Chien Cheng
    id: shang-chien-cheng
    last: Cheng
  - first: Jhih-Jie
    full: Jhih-Jie Chen
    id: jhih-jie-chen
    last: Chen
  - first: Chingyu
    full: Chingyu Yang
    id: chingyu-yang
    last: Yang
  - first: Jason
    full: Jason Chang
    id: jason-s-chang
    last: Chang
  author_string: Shang-Chien Cheng, Jhih-Jie Chen, Chingyu Yang, Jason Chang
  bibkey: cheng-etal-2018-languagenet
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '99'
  page_last: '102'
  pages: "99\u2013102"
  paper_id: '22'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2022.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2022.jpg
  title: 'LanguageNet: Learning to Find Sense Relevant Example Sentences'
  title_html: '<span class="acl-fixed-case">L</span>anguage<span class="acl-fixed-case">N</span>et:
    Learning to Find Sense Relevant Example Sentences'
  url: https://www.aclweb.org/anthology/C18-2022
  year: '2018'
C18-2023:
  abstract: In this paper, we demonstrate a system for the automatic extraction and
    curation of crime-related information from multi-source digitally published News
    articles collected over a period of five years. We have leveraged the use of deep
    convolution recurrent neural network model to analyze crime articles to extract
    different crime related entities and events. The proposed methods are not restricted
    to detecting known crimes only but contribute actively towards maintaining an
    updated crime ontology. We have done experiments with a collection of 5000 crime-reporting
    News articles span over time, and multiple sources. The end-product of our experiments
    is a crime-register that contains details of crime committed across geographies
    and time. This register can be further utilized for analytical and reporting purposes.
  address: Santa Fe, New Mexico
  author:
  - first: Tirthankar
    full: Tirthankar Dasgupta
    id: tirthankar-dasgupta
    last: Dasgupta
  - first: Lipika
    full: Lipika Dey
    id: lipika-dey
    last: Dey
  - first: Rupsa
    full: Rupsa Saha
    id: rupsa-saha
    last: Saha
  - first: Abir
    full: Abir Naskar
    id: abir-naskar
    last: Naskar
  author_string: Tirthankar Dasgupta, Lipika Dey, Rupsa Saha, Abir Naskar
  bibkey: dasgupta-etal-2018-automatic
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '103'
  page_last: '107'
  pages: "103\u2013107"
  paper_id: '23'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2023.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2023.jpg
  title: Automatic Curation and Visualization of Crime Related Information from Incrementally
    Crawled Multi-source News Reports
  title_html: Automatic Curation and Visualization of Crime Related Information from
    Incrementally Crawled Multi-source News Reports
  url: https://www.aclweb.org/anthology/C18-2023
  year: '2018'
C18-2024:
  abstract: Traditional chatbots usually need a mass of human dialogue data, especially
    when using supervised machine learning method. Though they can easily deal with
    single-turn question answering, for multi-turn the performance is usually unsatisfactory.
    In this paper, we present Lingke, an information retrieval augmented chatbot which
    is able to answer questions based on given product introduction document and deal
    with multi-turn conversations. We will introduce a fine-grained pipeline processing
    to distill responses based on unstructured documents, and attentive sequential
    context-response matching for multi-turn conversations.
  address: Santa Fe, New Mexico
  author:
  - first: Pengfei
    full: Pengfei Zhu
    id: pengfei-zhu
    last: Zhu
  - first: Zhuosheng
    full: Zhuosheng Zhang
    id: zhuosheng-zhang
    last: Zhang
  - first: Jiangtong
    full: Jiangtong Li
    id: jiangtong-li
    last: Li
  - first: Yafang
    full: Yafang Huang
    id: yafang-huang
    last: Huang
  - first: Hai
    full: Hai Zhao
    id: hai-zhao
    last: Zhao
  author_string: Pengfei Zhu, Zhuosheng Zhang, Jiangtong Li, Yafang Huang, Hai Zhao
  bibkey: zhu-etal-2018-lingke
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '108'
  page_last: '112'
  pages: "108\u2013112"
  paper_id: '24'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2024.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2024.jpg
  title: 'Lingke: a Fine-grained Multi-turn Chatbot for Customer Service'
  title_html: '<span class="acl-fixed-case">L</span>ingke: a Fine-grained Multi-turn
    Chatbot for Customer Service'
  url: https://www.aclweb.org/anthology/C18-2024
  year: '2018'
C18-2025:
  abstract: Writing Mentor is a free Google Docs add-on designed to provide feedback
    to struggling writers and help them improve their writing in a self-paced and
    self-regulated fashion. Writing Mentor uses natural language processing (NLP)
    methods and resources to generate feedback in terms of features that research
    into post-secondary struggling writers has classified as developmental (Burstein
    et al., 2016b). These features span many writing sub-constructs (use of sources,
    claims, and evidence; topic development; coherence; and knowledge of English conventions).
    Prelimi- nary analysis indicates that users have a largely positive impression
    of Writing Mentor in terms of usability and potential impact on their writing.
  address: Santa Fe, New Mexico
  author:
  - first: Nitin
    full: Nitin Madnani
    id: nitin-madnani
    last: Madnani
  - first: Jill
    full: Jill Burstein
    id: jill-burstein
    last: Burstein
  - first: Norbert
    full: Norbert Elliot
    id: norbert-elliot
    last: Elliot
  - first: Beata
    full: Beata Beigman Klebanov
    id: beata-beigman-klebanov
    last: Beigman Klebanov
  - first: Diane
    full: Diane Napolitano
    id: diane-napolitano
    last: Napolitano
  - first: Slava
    full: Slava Andreyev
    id: slava-andreyev
    last: Andreyev
  - first: Maxwell
    full: Maxwell Schwartz
    id: maxwell-schwartz
    last: Schwartz
  author_string: Nitin Madnani, Jill Burstein, Norbert Elliot, Beata Beigman Klebanov,
    Diane Napolitano, Slava Andreyev, Maxwell Schwartz
  bibkey: madnani-etal-2018-writing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '113'
  page_last: '117'
  pages: "113\u2013117"
  paper_id: '25'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2025.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2025.jpg
  title: 'Writing Mentor: Self-Regulated Writing Feedback for Struggling Writers'
  title_html: 'Writing Mentor: Self-Regulated Writing Feedback for Struggling Writers'
  url: https://www.aclweb.org/anthology/C18-2025
  year: '2018'
C18-2026:
  abstract: Today, we see an ever growing number of tools supporting text annotation.
    Each of these tools is optimized for specific use-cases such as named entity recognition.
    However, we see large growing knowledge bases such as Wikipedia or the Google
    Knowledge Graph. In this paper, we introduce NLATool, a web application developed
    using a human-centered design process. The application combines supporting text
    annotation and enriching the text with additional information from a number of
    sources directly within the application. The tool assists users to efficiently
    recognize named entities, annotate text, and automatically provide users additional
    information while solving deep text understanding tasks.
  address: Santa Fe, New Mexico
  author:
  - first: Markus
    full: "Markus G\xE4rtner"
    id: markus-gartner
    last: "G\xE4rtner"
  - first: Sven
    full: Sven Mayer
    id: sven-mayer
    last: Mayer
  - first: Valentin
    full: Valentin Schwind
    id: valentin-schwind
    last: Schwind
  - first: Eric
    full: "Eric H\xE4mmerle"
    id: eric-hammerle
    last: "H\xE4mmerle"
  - first: Emine
    full: Emine Turcan
    id: emine-turcan
    last: Turcan
  - first: Florin
    full: Florin Rheinwald
    id: florin-rheinwald
    last: Rheinwald
  - first: Gustav
    full: Gustav Murawski
    id: gustav-murawski
    last: Murawski
  - first: Lars
    full: Lars Lischke
    id: lars-lischke
    last: Lischke
  - first: Jonas
    full: Jonas Kuhn
    id: jonas-kuhn
    last: Kuhn
  author_string: "Markus G\xE4rtner, Sven Mayer, Valentin Schwind, Eric H\xE4mmerle,\
    \ Emine Turcan, Florin Rheinwald, Gustav Murawski, Lars Lischke, Jonas Kuhn"
  bibkey: gartner-etal-2018-nlatool
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '118'
  page_last: '122'
  pages: "118\u2013122"
  paper_id: '26'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2026.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2026.jpg
  title: 'NLATool: an Application for Enhanced Deep Text Understanding'
  title_html: '<span class="acl-fixed-case">NLAT</span>ool: an Application for Enhanced
    Deep Text Understanding'
  url: https://www.aclweb.org/anthology/C18-2026
  year: '2018'
C18-2027:
  abstract: "Here we describe Sensala , an open source framework for the semantic\
    \ interpretation of natural language that provides the logical meaning of a given\
    \ text. The framework\u2019s theory is based on a lambda calculus with exception\
    \ handling and uses contexts, continuations, events and dependent types to handle\
    \ a wide range of complex linguistic phenomena, such as donkey anaphora, verb\
    \ phrase anaphora, propositional anaphora, presuppositions and implicatures."
  address: Santa Fe, New Mexico
  author:
  - first: Daniyar
    full: Daniyar Itegulov
    id: daniyar-itegulov
    last: Itegulov
  - first: Ekaterina
    full: Ekaterina Lebedeva
    id: ekaterina-lebedeva
    last: Lebedeva
  - first: Bruno
    full: Bruno Woltzenlogel Paleo
    id: bruno-woltzenlogel-paleo
    last: Woltzenlogel Paleo
  author_string: Daniyar Itegulov, Ekaterina Lebedeva, Bruno Woltzenlogel Paleo
  bibkey: itegulov-etal-2018-sensala
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '123'
  page_last: '127'
  pages: "123\u2013127"
  paper_id: '27'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2027.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2027.jpg
  title: 'Sensala: a Dynamic Semantics System for Natural Language Processing'
  title_html: '<span class="acl-fixed-case">S</span>ensala: a Dynamic Semantics System
    for Natural Language Processing'
  url: https://www.aclweb.org/anthology/C18-2027
  year: '2018'
C18-2028:
  abstract: Recent developments in deep learning with application to language modeling
    have led to success in tasks of text processing, summarizing and machine translation.
    However, deploying huge language models for the mobile device such as on-device
    keyboards poses computation as a bottle-neck due to their puny computation capacities.
    In this work, we propose an on-device neural language model based word prediction
    method that optimizes run-time memory and also provides a real-time prediction
    environment. Our model size is 7.40MB and has average prediction time of 6.47
    ms. Our proposed model outperforms the existing methods for word prediction in
    terms of keystroke savings and word prediction rate and has been successfully
    commercialized.
  address: Santa Fe, New Mexico
  author:
  - first: Seunghak
    full: Seunghak Yu
    id: seunghak-yu
    last: Yu
  - first: Nilesh
    full: Nilesh Kulkarni
    id: nilesh-kulkarni
    last: Kulkarni
  - first: Haejun
    full: Haejun Lee
    id: haejun-lee
    last: Lee
  - first: Jihie
    full: Jihie Kim
    id: jihie-kim
    last: Kim
  author_string: Seunghak Yu, Nilesh Kulkarni, Haejun Lee, Jihie Kim
  bibkey: yu-etal-2018-device
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '128'
  page_last: '131'
  pages: "128\u2013131"
  paper_id: '28'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2028.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2028.jpg
  title: On-Device Neural Language Model Based Word Prediction
  title_html: On-Device Neural Language Model Based Word Prediction
  url: https://www.aclweb.org/anthology/C18-2028
  year: '2018'
C18-2029:
  abstract: We present WARP-Text, an open-source web-based tool for annotating relationships
    between pairs of texts. WARP-Text supports multi-layer annotation and custom definitions
    of inter-textual and intra-textual relationships. Annotation can be performed
    at different granularity levels (such as sentences, phrases, or tokens). WARP-Text
    has an intuitive user-friendly interface both for project managers and annotators.
    WARP-Text fills a gap in the currently available NLP toolbox, as open-source alternatives
    for annotation of pairs of text are not readily available. WARP-Text has already
    been used in several annotation tasks and can be of interest to the researchers
    working in the areas of Paraphrasing, Entailment, Simplification, and Summarization,
    among others.
  address: Santa Fe, New Mexico
  author:
  - first: Venelin
    full: Venelin Kovatchev
    id: venelin-kovatchev
    last: Kovatchev
  - first: "M. Ant\xF2nia"
    full: "M. Ant\xF2nia Mart\xED"
    id: m-antonia-marti
    last: "Mart\xED"
  - first: Maria
    full: "Maria Salam\xF3"
    id: maria-salamo
    last: "Salam\xF3"
  author_string: "Venelin Kovatchev, M. Ant\xF2nia Mart\xED, Maria Salam\xF3"
  bibkey: kovatchev-etal-2018-warp
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '132'
  page_last: '136'
  pages: "132\u2013136"
  paper_id: '29'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2029.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2029.jpg
  title: 'WARP-Text: a Web-Based Tool for Annotating Relationships between Pairs of
    Texts'
  title_html: '<span class="acl-fixed-case">WARP</span>-Text: a Web-Based Tool for
    Annotating Relationships between Pairs of Texts'
  url: https://www.aclweb.org/anthology/C18-2029
  year: '2018'
C18-2030:
  abstract: We present a Chinese writing correction system for learning Chinese as
    a foreign language. The system takes a wrong input sentence and generates several
    correction suggestions. It also retrieves example Chinese sentences with English
    translations, helping users understand the correct usages of certain grammar patterns.
    This is the first available Chinese writing error correction system based on the
    neural machine translation framework. We discuss several design choices and show
    empirical results to support our decisions.
  address: Santa Fe, New Mexico
  author:
  - first: Yow-Ting
    full: Yow-Ting Shiue
    id: yow-ting-shiue
    last: Shiue
  - first: Hen-Hsen
    full: Hen-Hsen Huang
    id: hen-hsen-huang
    last: Huang
  - first: Hsin-Hsi
    full: Hsin-Hsi Chen
    id: hsin-hsi-chen
    last: Chen
  author_string: Yow-Ting Shiue, Hen-Hsen Huang, Hsin-Hsi Chen
  bibkey: shiue-etal-2018-chinese
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '137'
  page_last: '141'
  pages: "137\u2013141"
  paper_id: '30'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2030.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2030.jpg
  title: A Chinese Writing Correction System for Learning Chinese as a Foreign Language
  title_html: A <span class="acl-fixed-case">C</span>hinese Writing Correction System
    for Learning <span class="acl-fixed-case">C</span>hinese as a Foreign Language
  url: https://www.aclweb.org/anthology/C18-2030
  year: '2018'
C18-2031:
  abstract: In this paper we present LTV, a website and API that generates labeled
    topic classifications based on the Dewey Decimal Classification (DDC), an international
    standard for topic classification in libraries. We introduce nnDDC, a largely
    language-independent natural network-based classifier for DDC, which we optimized
    using a wide range of linguistic features to achieve an F-score of 87.4%. To show
    that our approach is language-independent, we evaluate nnDDC using up to 40 different
    languages. We derive a topic model based on nnDDC, which generates probability
    distributions over semantic units for any input on sense-, word- and text-level.
    Unlike related approaches, however, these probabilities are estimated by means
    of nnDDC so that each dimension of the resulting vector representation is uniquely
    labeled by a DDC class. In this way, we introduce a neural network-based Classifier-Induced
    Semantic Space (nnCISS).
  address: Santa Fe, New Mexico
  author:
  - first: Daniel
    full: Daniel Baumartz
    id: daniel-baumartz
    last: Baumartz
  - first: Tolga
    full: Tolga Uslu
    id: tolga-uslu
    last: Uslu
  - first: Alexander
    full: Alexander Mehler
    id: alexander-mehler
    last: Mehler
  author_string: Daniel Baumartz, Tolga Uslu, Alexander Mehler
  bibkey: baumartz-etal-2018-ltv
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '142'
  page_last: '145'
  pages: "142\u2013145"
  paper_id: '31'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2031.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2031.jpg
  title: 'LTV: Labeled Topic Vector'
  title_html: '<span class="acl-fixed-case">LTV</span>: Labeled Topic Vector'
  url: https://www.aclweb.org/anthology/C18-2031
  year: '2018'
C18-2032:
  abstract: This paper proposes a neural based system to solve the essential interpretability
    problem existing in text classification, especially in charge prediction task.
    First, we use a deep reinforcement learning method to extract rationales which
    mean short, readable and decisive snippets from input text. Then a rationale augmented
    classification model is proposed to elevate the prediction accuracy. Naturally,
    the extracted rationales serve as the introspection explanation for the prediction
    result of the model, enhancing the transparency of the model. Experimental results
    demonstrate that our system is able to extract readable rationales in a high consistency
    with manual annotation and is comparable with the attention model in prediction
    accuracy.
  address: Santa Fe, New Mexico
  author:
  - first: Xin
    full: Xin Jiang
    id: xin-jiang
    last: Jiang
  - first: Hai
    full: Hai Ye
    id: hai-ye
    last: Ye
  - first: Zhunchen
    full: Zhunchen Luo
    id: zhunchen-luo
    last: Luo
  - first: WenHan
    full: WenHan Chao
    id: wenhan-chao
    last: Chao
  - first: Wenjia
    full: Wenjia Ma
    id: wenjia-ma
    last: Ma
  author_string: Xin Jiang, Hai Ye, Zhunchen Luo, WenHan Chao, Wenjia Ma
  bibkey: jiang-etal-2018-interpretable
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '146'
  page_last: '151'
  pages: "146\u2013151"
  paper_id: '32'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2032.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2032.jpg
  title: Interpretable Rationale Augmented Charge Prediction System
  title_html: Interpretable Rationale Augmented Charge Prediction System
  url: https://www.aclweb.org/anthology/C18-2032
  year: '2018'
C18-2033:
  abstract: We present Qutr (Query Translator), a smart cross-lingual communication
    application for the travel domain. Qutr is a real-time messaging app that automatically
    translates conversations while supporting keyword-to-sentence matching. Qutr relies
    on querying a database that holds commonly used pre-translated travel-domain phrases
    and phrase templates in different languages with the use of keywords. The query
    matching supports paraphrases, incomplete keywords and some input spelling errors.
    The application addresses common cross-lingual communication issues such as translation
    accuracy, speed, privacy, and personalization.
  address: Santa Fe, New Mexico
  author:
  - first: Shehroze
    full: Shehroze Khan
    id: shehroze-khan
    last: Khan
  - first: Jihyun
    full: Jihyun Kim
    id: jihyun-kim
    last: Kim
  - first: Tarik
    full: Tarik Zulfikarpasic
    id: tarik-zulfikarpasic
    last: Zulfikarpasic
  - first: Peter
    full: Peter Chen
    id: yuanzhu-peter-chen
    last: Chen
  - first: Nizar
    full: Nizar Habash
    id: nizar-habash
    last: Habash
  author_string: Shehroze Khan, Jihyun Kim, Tarik Zulfikarpasic, Peter Chen, Nizar
    Habash
  bibkey: khan-etal-2018-cross
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '152'
  page_last: '156'
  pages: "152\u2013156"
  paper_id: '33'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2033.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2033.jpg
  title: A Cross-lingual Messenger with Keyword Searchable Phrases for the Travel
    Domain
  title_html: A Cross-lingual Messenger with Keyword Searchable Phrases for the Travel
    Domain
  url: https://www.aclweb.org/anthology/C18-2033
  year: '2018'
C18-2034:
  abstract: 'Large organizations spend considerable resources in reviewing regulations
    and ensuring that their business processes are compliant with the law. To make
    compliance workflows more efficient and responsive, we present a system for machine-driven
    annotations of legal documents. A set of natural language processing pipelines
    are designed and aimed at addressing some key questions in this domain: (a) is
    this (new) regulation relevant for me? (b) what set of requirements does this
    law impose?, and (c) what is the regulatory intent of a law? The system is currently
    undergoing user trials within our organization.'
  address: Santa Fe, New Mexico
  author:
  - first: Rahul
    full: Rahul Nair
    id: rahul-nair
    last: Nair
  - first: Killian
    full: Killian Levacher
    id: killian-levacher
    last: Levacher
  - first: Martin
    full: Martin Stephenson
    id: martin-stephenson
    last: Stephenson
  author_string: Rahul Nair, Killian Levacher, Martin Stephenson
  bibkey: nair-etal-2018-towards
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '157'
  page_last: '160'
  pages: "157\u2013160"
  paper_id: '34'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2034.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2034.jpg
  title: Towards Automated Extraction of Business Constraints from Unstructured Regulatory
    Text
  title_html: Towards Automated Extraction of Business Constraints from Unstructured
    Regulatory Text
  url: https://www.aclweb.org/anthology/C18-2034
  year: '2018'
C18-2035:
  abstract: This paper presents a flexible and open source framework for deep semantic
    role labeling. We aim at facilitating easy exploration of model structures for
    multiple languages with different characteristics. It provides flexibility in
    its model construction in terms of word representation, sequence representation,
    output modeling, and inference styles and comes with clear output visualization.
    The framework is available under the Apache 2.0 license.
  address: Santa Fe, New Mexico
  author:
  - first: Quynh Ngoc Thi
    full: Quynh Ngoc Thi Do
    id: quynh-ngoc-thi-do
    last: Do
  - first: Artuur
    full: Artuur Leeuwenberg
    id: artuur-leeuwenberg
    last: Leeuwenberg
  - first: Geert
    full: Geert Heyman
    id: geert-heyman
    last: Heyman
  - first: Marie-Francine
    full: Marie-Francine Moens
    id: marie-francine-moens
    last: Moens
  author_string: Quynh Ngoc Thi Do, Artuur Leeuwenberg, Geert Heyman, Marie-Francine
    Moens
  bibkey: do-etal-2018-flexible
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    System Demonstrations'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: System Demonstrations'
  month: August
  page_first: '161'
  page_last: '165'
  pages: "161\u2013165"
  paper_id: '35'
  parent_volume_id: C18-2
  pdf: https://www.aclweb.org/anthology/C18-2035.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-2035.jpg
  title: A Flexible and Easy-to-use Semantic Role Labeling Framework for Different
    Languages
  title_html: A Flexible and Easy-to-use Semantic Role Labeling Framework for Different
    Languages
  url: https://www.aclweb.org/anthology/C18-2035
  year: '2018'
C18-3000:
  address: Santa Fe, New Mexico, USA
  author:
  - first: Donia
    full: Donia Scott
    id: donia-scott
    last: Scott
  - first: Marilyn
    full: Marilyn Walker
    id: marilyn-walker
    last: Walker
  - first: Pascale
    full: Pascale Fung
    id: pascale-fung
    last: Fung
  author_string: Donia Scott, Marilyn Walker, Pascale Fung
  bibkey: coling-2018-international-linguistics-tutorial
  bibtype: proceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    Tutorial Abstracts'
  month: August
  paper_id: '0'
  parent_volume_id: C18-3
  pdf: https://www.aclweb.org/anthology/C18-3000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-3000.jpg
  title: 'Proceedings of the 27th International Conference on Computational Linguistics:
    Tutorial Abstracts'
  title_html: 'Proceedings of the 27th International Conference on Computational Linguistics:
    Tutorial Abstracts'
  url: https://www.aclweb.org/anthology/C18-3000
  year: '2018'
C18-3001:
  address: Santa Fe, New Mexico, USA
  author:
  - first: Gabriel
    full: Gabriel Murray
    id: gabriel-murray
    last: Murray
  - first: Giuseppe
    full: Giuseppe Carenini
    id: giuseppe-carenini
    last: Carenini
  - first: Shafiq
    full: Shafiq Joty
    id: shafiq-joty
    last: Joty
  author_string: Gabriel Murray, Giuseppe Carenini, Shafiq Joty
  bibkey: murray-etal-2018-nlp
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: Tutorial Abstracts'
  month: August
  page_first: '1'
  page_last: '4'
  pages: "1\u20134"
  paper_id: '1'
  parent_volume_id: C18-3
  pdf: https://www.aclweb.org/anthology/C18-3001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-3001.jpg
  title: 'NLP for Conversations: Sentiment, Summarization, and Group Dynamics'
  title_html: '<span class="acl-fixed-case">NLP</span> for Conversations: Sentiment,
    Summarization, and Group Dynamics'
  url: https://www.aclweb.org/anthology/C18-3001
  year: '2018'
C18-3002:
  address: Santa Fe, New Mexico, USA
  author:
  - first: Daniel
    full: Daniel Dakota
    id: daniel-dakota
    last: Dakota
  - first: Sandra
    full: "Sandra K\xFCbler"
    id: sandra-kubler
    last: "K\xFCbler"
  author_string: "Daniel Dakota, Sandra K\xFCbler"
  bibkey: dakota-kubler-2018-practical
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: Tutorial Abstracts'
  month: August
  page_first: '5'
  page_last: '7'
  pages: "5\u20137"
  paper_id: '2'
  parent_volume_id: C18-3
  pdf: https://www.aclweb.org/anthology/C18-3002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-3002.jpg
  title: Practical Parsing for Downstream Applications
  title_html: Practical Parsing for Downstream Applications
  url: https://www.aclweb.org/anthology/C18-3002
  year: '2018'
C18-3003:
  address: Santa Fe, New Mexico, USA
  author:
  - first: Collin
    full: Collin Baker
    id: collin-f-baker
    last: Baker
  - first: Michael
    full: Michael Ellsworth
    id: michael-ellsworth
    last: Ellsworth
  - first: Miriam R. L.
    full: Miriam R. L. Petruck
    id: miriam-r-l-petruck
    last: Petruck
  - first: Swabha
    full: Swabha Swayamdipta
    id: swabha-swayamdipta
    last: Swayamdipta
  author_string: Collin Baker, Michael Ellsworth, Miriam R. L. Petruck, Swabha Swayamdipta
  bibkey: baker-etal-2018-frame
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: Tutorial Abstracts'
  month: August
  page_first: '9'
  page_last: '12'
  pages: "9\u201312"
  paper_id: '3'
  parent_volume_id: C18-3
  pdf: https://www.aclweb.org/anthology/C18-3003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-3003.jpg
  title: 'Frame Semantics across Languages: Towards a Multilingual FrameNet'
  title_html: 'Frame Semantics across Languages: Towards a Multilingual <span class="acl-fixed-case">F</span>rame<span
    class="acl-fixed-case">N</span>et'
  url: https://www.aclweb.org/anthology/C18-3003
  year: '2018'
C18-3004:
  address: Santa Fe, New Mexico, USA
  attachment:
  - filename: C18-3004.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/C18-3004.Presentation.pdf
  author:
  - first: Jen-Tzung
    full: Jen-Tzung Chien
    id: jen-tzung-chien
    last: Chien
  author_string: Jen-Tzung Chien
  bibkey: chien-2018-deep
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: Tutorial Abstracts'
  month: August
  page_first: '13'
  page_last: '18'
  pages: "13\u201318"
  paper_id: '4'
  parent_volume_id: C18-3
  pdf: https://www.aclweb.org/anthology/C18-3004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-3004.jpg
  title: Deep Bayesian Learning and Understanding
  title_html: Deep <span class="acl-fixed-case">B</span>ayesian Learning and Understanding
  url: https://www.aclweb.org/anthology/C18-3004
  year: '2018'
C18-3005:
  address: Santa Fe, New Mexico, USA
  attachment:
  - filename: C18-3005.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/C18-3005.Presentation.pdf
  author:
  - first: Sanja
    full: "Sanja \u0160tajner"
    id: sanja-stajner
    last: "\u0160tajner"
  - first: Horacio
    full: Horacio Saggion
    id: horacio-saggion
    last: Saggion
  author_string: "Sanja \u0160tajner, Horacio Saggion"
  bibkey: stajner-saggion-2018-data
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: Tutorial Abstracts'
  month: August
  page_first: '19'
  page_last: '23'
  pages: "19\u201323"
  paper_id: '5'
  parent_volume_id: C18-3
  pdf: https://www.aclweb.org/anthology/C18-3005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-3005.jpg
  title: Data-Driven Text Simplification
  title_html: Data-Driven Text Simplification
  url: https://www.aclweb.org/anthology/C18-3005
  year: '2018'
C18-3006:
  address: Santa Fe, New Mexico, USA
  author:
  - first: Yun-Nung
    full: Yun-Nung Chen
    id: yun-nung-chen
    last: Chen
  - first: Asli
    full: Asli Celikyilmaz
    id: asli-celikyilmaz
    last: Celikyilmaz
  - first: Dilek
    full: "Dilek Hakkani-T\xFCr"
    id: dilek-hakkani-tur
    last: "Hakkani-T\xFCr"
  author_string: "Yun-Nung Chen, Asli Celikyilmaz, Dilek Hakkani-T\xFCr"
  bibkey: chen-etal-2018-deep
  bibtype: inproceedings
  booktitle: 'Proceedings of the 27th International Conference on Computational Linguistics:
    Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 27th International Conference on Computational
    Linguistics: Tutorial Abstracts'
  month: August
  page_first: '25'
  page_last: '31'
  pages: "25\u201331"
  paper_id: '6'
  parent_volume_id: C18-3
  pdf: https://www.aclweb.org/anthology/C18-3006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/C18-3006.jpg
  title: Deep Learning for Dialogue Systems
  title_html: Deep Learning for Dialogue Systems
  url: https://www.aclweb.org/anthology/C18-3006
  year: '2018'
