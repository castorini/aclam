D17-1000:
  address: Copenhagen, Denmark
  author:
  - first: Martha
    full: Martha Palmer
    id: martha-palmer
    last: Palmer
  - first: Rebecca
    full: Rebecca Hwa
    id: rebecca-hwa
    last: Hwa
  - first: Sebastian
    full: Sebastian Riedel
    id: sebastian-riedel
    last: Riedel
  author_string: Martha Palmer, Rebecca Hwa, Sebastian Riedel
  bibkey: emnlp-2017-2017
  bibtype: proceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  doi: 10.18653/v1/D17-1
  month: September
  paper_id: '0'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1000.jpg
  title: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  title_html: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  url: https://www.aclweb.org/anthology/D17-1000
  year: '2017'
D17-1001:
  abstract: We propose an efficient method to conduct phrase alignment on parse forests
    for paraphrase detection. Unlike previous studies, our method identifies syntactic
    paraphrases under linguistically motivated grammar. In addition, it allows phrases
    to non-compositionally align to handle paraphrases with non-homographic phrase
    correspondences. A dataset that provides gold parse trees and their phrase alignments
    is created. The experimental results confirm that the proposed method conducts
    highly accurate phrase alignment compared to human performance.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1001.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1001.Attachment.zip
  - filename: https://vimeo.com/238234373
    type: video
    url: https://vimeo.com/238234373
  author:
  - first: Yuki
    full: Yuki Arase
    id: yuki-arase
    last: Arase
  - first: Junichi
    full: Junichi Tsujii
    id: junichi-tsujii
    last: Tsujii
  author_string: Yuki Arase, Junichi Tsujii
  bibkey: arase-tsujii-2017-monolingual
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1001
  month: September
  page_first: '1'
  page_last: '11'
  pages: "1\u201311"
  paper_id: '1'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1001.jpg
  title: Monolingual Phrase Alignment on Parse Forests
  title_html: Monolingual Phrase Alignment on Parse Forests
  url: https://www.aclweb.org/anthology/D17-1001
  year: '2017'
D17-1002:
  abstract: "We first present a minimal feature set for transition-based dependency\
    \ parsing, continuing a recent trend started by Kiperwasser and Goldberg (2016a)\
    \ and Cross and Huang (2016a) of using bi-directional LSTM features. We plug our\
    \ minimal feature set into the dynamic-programming framework of Huang and Sagae\
    \ (2010) and Kuhlmann et al. (2011) to produce the first implementation of worst-case\
    \ O(n3) exact decoders for arc-hybrid and arc-eager transition systems. With our\
    \ minimal features, we also present exact decoders for arc-hybrid and arc-eager\
    \ transition systems. With our minimal features, we also present O(n3) global\
    \ training methods. Finally, using ensembles including our new parsers, we achieve\
    \ the best unlabeled attachment score reported (to our knowledge) on the Chinese\
    \ Treebank and the \u201Csecond-best-in-class\u201D result on the English Penn\
    \ Treebank. global training methods. Finally, using ensembles including our new\
    \ parsers, we achieve the best unlabeled attachment score reported (to our knowledge)\
    \ on the Chinese Treebank and the \u201Csecond-best-in-class\u201D result on the\
    \ English Penn Treebank."
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238233093
    type: video
    url: https://vimeo.com/238233093
  author:
  - first: Tianze
    full: Tianze Shi
    id: tianze-shi
    last: Shi
  - first: Liang
    full: Liang Huang
    id: liang-huang
    last: Huang
  - first: Lillian
    full: Lillian Lee
    id: lillian-lee
    last: Lee
  author_string: Tianze Shi, Liang Huang, Lillian Lee
  bibkey: shi-etal-2017-fast
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1002
  month: September
  page_first: '12'
  page_last: '23'
  pages: "12\u201323"
  paper_id: '2'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1002.jpg
  title: Fast(er) Exact Decoding and Global Training for Transition-Based Dependency
    Parsing via a Minimal Feature Set
  title_html: Fast(er) Exact Decoding and Global Training for Transition-Based Dependency
    Parsing via a Minimal Feature Set
  url: https://www.aclweb.org/anthology/D17-1002
  year: '2017'
D17-1003:
  abstract: 'We propose a new Maximum Subgraph algorithm for first-order parsing to
    1-endpoint-crossing, pagenumber-2 graphs. Our algorithm has two characteristics:
    (1) it separates the construction for noncrossing edges and crossing edges; (2)
    in a single construction step, whether to create a new arc is deterministic. These
    two characteristics make our algorithm relatively easy to be extended to incorporiate
    crossing-sensitive second-order features. We then introduce a new algorithm for
    quasi-second-order parsing. Experiments demonstrate that second-order features
    are helpful for Maximum Subgraph parsing.'
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1003.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1003.Attachment.pdf
  - filename: https://vimeo.com/238235370
    type: video
    url: https://vimeo.com/238235370
  author:
  - first: Junjie
    full: Junjie Cao
    id: junjie-cao
    last: Cao
  - first: Sheng
    full: Sheng Huang
    id: sheng-huang
    last: Huang
  - first: Weiwei
    full: Weiwei Sun
    id: weiwei-sun
    last: Sun
  - first: Xiaojun
    full: Xiaojun Wan
    id: xiaojun-wan
    last: Wan
  author_string: Junjie Cao, Sheng Huang, Weiwei Sun, Xiaojun Wan
  bibkey: cao-etal-2017-quasi
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1003
  month: September
  page_first: '24'
  page_last: '34'
  pages: "24\u201334"
  paper_id: '3'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1003.jpg
  title: Quasi-Second-Order Parsing for 1-Endpoint-Crossing, Pagenumber-2 Graphs
  title_html: Quasi-Second-Order Parsing for 1-Endpoint-Crossing, Pagenumber-2 Graphs
  url: https://www.aclweb.org/anthology/D17-1003
  year: '2017'
D17-1004:
  abstract: "Organized relational knowledge in the form of \u201Cknowledge graphs\u201D\
    \ is important for many applications. However, the ability to populate knowledge\
    \ bases with facts automatically extracted from documents has improved frustratingly\
    \ slowly. This paper simultaneously addresses two issues that have held back prior\
    \ work. We first propose an effective new model, which combines an LSTM sequence\
    \ model with a form of entity position-aware attention that is better suited to\
    \ relation extraction. Then we build TACRED, a large (119,474 examples) supervised\
    \ relation extraction dataset obtained via crowdsourcing and targeted towards\
    \ TAC KBP relations. The combination of better supervised data and a more appropriate\
    \ high-capacity model enables much better relation extraction performance. When\
    \ the model trained on this new dataset replaces the previous relation extraction\
    \ component of the best TAC KBP 2015 slot filling system, its F1 score increases\
    \ markedly from 22.2% to 26.7%."
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1004.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1004.Attachment.pdf
  - filename: https://vimeo.com/238230211
    type: video
    url: https://vimeo.com/238230211
  author:
  - first: Yuhao
    full: Yuhao Zhang
    id: yuhao-zhang
    last: Zhang
  - first: Victor
    full: Victor Zhong
    id: victor-zhong
    last: Zhong
  - first: Danqi
    full: Danqi Chen
    id: danqi-chen
    last: Chen
  - first: Gabor
    full: Gabor Angeli
    id: gabor-angeli
    last: Angeli
  - first: Christopher D.
    full: Christopher D. Manning
    id: christopher-d-manning
    last: Manning
  author_string: Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, Christopher
    D. Manning
  bibkey: zhang-etal-2017-position
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1004
  month: September
  page_first: '35'
  page_last: '45'
  pages: "35\u201345"
  paper_id: '4'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1004.jpg
  title: Position-aware Attention and Supervised Data Improve Slot Filling
  title_html: Position-aware Attention and Supervised Data Improve Slot Filling
  url: https://www.aclweb.org/anthology/D17-1004
  year: '2017'
D17-1005:
  abstract: 'Relation extraction is a fundamental task in information extraction.
    Most existing methods have heavy reliance on annotations labeled by human experts,
    which are costly and time-consuming. To overcome this drawback, we propose a novel
    framework, REHession, to conduct relation extractor learning using annotations
    from heterogeneous information source, e.g., knowledge base and domain heuristics.
    These annotations, referred as heterogeneous supervision, often conflict with
    each other, which brings a new challenge to the original relation extraction task:
    how to infer the true label from noisy labels for a given instance. Identifying
    context information as the backbone of both relation extraction and true label
    discovery, we adopt embedding techniques to learn the distributed representations
    of context, which bridges all components with mutual enhancement in an iterative
    fashion. Extensive experimental results demonstrate the superiority of REHession
    over the state-of-the-art.'
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238228823
    type: video
    url: https://vimeo.com/238228823
  author:
  - first: Liyuan
    full: Liyuan Liu
    id: liyuan-liu
    last: Liu
  - first: Xiang
    full: Xiang Ren
    id: xiang-ren
    last: Ren
  - first: Qi
    full: Qi Zhu
    id: qi-zhu
    last: Zhu
  - first: Shi
    full: Shi Zhi
    id: shi-zhi
    last: Zhi
  - first: Huan
    full: Huan Gui
    id: huan-gui
    last: Gui
  - first: Heng
    full: Heng Ji
    id: heng-ji
    last: Ji
  - first: Jiawei
    full: Jiawei Han
    id: jiawei-han
    last: Han
  author_string: Liyuan Liu, Xiang Ren, Qi Zhu, Shi Zhi, Huan Gui, Heng Ji, Jiawei
    Han
  bibkey: liu-etal-2017-heterogeneous
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1005
  month: September
  page_first: '46'
  page_last: '56'
  pages: "46\u201356"
  paper_id: '5'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1005.jpg
  title: 'Heterogeneous Supervision for Relation Extraction: A Representation Learning
    Approach'
  title_html: 'Heterogeneous Supervision for Relation Extraction: A Representation
    Learning Approach'
  url: https://www.aclweb.org/anthology/D17-1005
  year: '2017'
D17-1006:
  abstract: There has been a recent line of work automatically learning scripts from
    unstructured texts, by modeling narrative event chains. While the dominant approach
    group events using event pair relations, LSTMs have been used to encode full chains
    of narrative events. The latter has the advantage of learning long-range temporal
    orders, yet the former is more adaptive to partial orders. We propose a neural
    model that leverages the advantages of both methods, by using LSTM hidden states
    as features for event pair modelling. A dynamic memory network is utilized to
    automatically induce weights on existing events for inferring a subsequent event.
    Standard evaluation shows that our method significantly outperforms both methods
    above, giving the best results reported so far.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238228864
    type: video
    url: https://vimeo.com/238228864
  author:
  - first: Zhongqing
    full: Zhongqing Wang
    id: zhongqing-wang
    last: Wang
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  - first: Ching-Yun
    full: Ching-Yun Chang
    id: ching-yun-chang
    last: Chang
  author_string: Zhongqing Wang, Yue Zhang, Ching-Yun Chang
  bibkey: wang-etal-2017-integrating
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1006
  month: September
  page_first: '57'
  page_last: '67'
  pages: "57\u201367"
  paper_id: '6'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1006.jpg
  title: Integrating Order Information and Event Relation for Script Event Prediction
  title_html: Integrating Order Information and Event Relation for Script Event Prediction
  url: https://www.aclweb.org/anthology/D17-1006
  year: '2017'
D17-1007:
  abstract: We present a simple yet effective approach for linking entities in queries.
    The key idea is to search sentences similar to a query from Wikipedia articles
    and directly use the human-annotated entities in the similar sentences as candidate
    entities for the query. Then, we employ a rich set of features, such as link-probability,
    context-matching, word embeddings, and relatedness among candidate entities as
    well as their related entities, to rank the candidates under a regression based
    framework. The advantages of our approach lie in two aspects, which contribute
    to the ranking process and final linking result. First, it can greatly reduce
    the number of candidate entities by filtering out irrelevant entities with the
    words in the query. Second, we can obtain the query sensitive prior probability
    in addition to the static link-probability derived from all Wikipedia articles.
    We conduct experiments on two benchmark datasets on entity linking for queries,
    namely the ERD14 dataset and the GERDAQ dataset. Experimental results show that
    our method outperforms state-of-the-art systems and yields 75.0% in F1 on the
    ERD14 dataset and 56.9% on the GERDAQ dataset.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238228743
    type: video
    url: https://vimeo.com/238228743
  author:
  - first: Chuanqi
    full: Chuanqi Tan
    id: chuanqi-tan
    last: Tan
  - first: Furu
    full: Furu Wei
    id: furu-wei
    last: Wei
  - first: Pengjie
    full: Pengjie Ren
    id: pengjie-ren
    last: Ren
  - first: Weifeng
    full: Weifeng Lv
    id: weifeng-lv
    last: Lv
  - first: Ming
    full: Ming Zhou
    id: ming-zhou
    last: Zhou
  author_string: Chuanqi Tan, Furu Wei, Pengjie Ren, Weifeng Lv, Ming Zhou
  bibkey: tan-etal-2017-entity
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1007
  month: September
  page_first: '68'
  page_last: '77'
  pages: "68\u201377"
  paper_id: '7'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1007.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1007.jpg
  title: Entity Linking for Queries by Searching Wikipedia Sentences
  title_html: Entity Linking for Queries by Searching <span class="acl-fixed-case">W</span>ikipedia
    Sentences
  url: https://www.aclweb.org/anthology/D17-1007
  year: '2017'
D17-1008:
  abstract: "Annotating large numbers of sentences with senses is the heaviest requirement\
    \ of current Word Sense Disambiguation. We present Train-O-Matic, a language-independent\
    \ method for generating millions of sense-annotated training instances for virtually\
    \ all meanings of words in a language\u2019s vocabulary. The approach is fully\
    \ automatic: no human intervention is required and the only type of human knowledge\
    \ used is a WordNet-like resource. Train-O-Matic achieves consistently state-of-the-art\
    \ performance across gold standard datasets and languages, while at the same time\
    \ removing the burden of manual annotation. All the training data is available\
    \ for research purposes at http://trainomatic.org."
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1008.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1008.Attachment.zip
  - filename: https://vimeo.com/238236475
    type: video
    url: https://vimeo.com/238236475
  author:
  - first: Tommaso
    full: Tommaso Pasini
    id: tommaso-pasini
    last: Pasini
  - first: Roberto
    full: Roberto Navigli
    id: roberto-navigli
    last: Navigli
  author_string: Tommaso Pasini, Roberto Navigli
  bibkey: pasini-navigli-2017-train
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1008
  month: September
  page_first: '78'
  page_last: '88'
  pages: "78\u201388"
  paper_id: '8'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1008.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1008.jpg
  title: 'Train-O-Matic: Large-Scale Supervised Word Sense Disambiguation in Multiple
    Languages without Manual Training Data'
  title_html: 'Train-O-<span class="acl-fixed-case">M</span>atic: Large-Scale Supervised
    Word Sense Disambiguation in Multiple Languages without Manual Training Data'
  url: https://www.aclweb.org/anthology/D17-1008
  year: '2017'
D17-1009:
  abstract: Universal Dependencies (UD) offer a uniform cross-lingual syntactic representation,
    with the aim of advancing multilingual applications. Recent work shows that semantic
    parsing can be accomplished by transforming syntactic dependencies to logical
    forms. However, this work is limited to English, and cannot process dependency
    graphs, which allow handling complex phenomena such as control. In this work,
    we introduce UDepLambda, a semantic interface for UD, which maps natural language
    to logical forms in an almost language-independent fashion and can process dependency
    graphs. We perform experiments on question answering against Freebase and provide
    German and Spanish translations of the WebQuestions and GraphQuestions datasets
    to facilitate multilingual evaluation. Results show that UDepLambda outperforms
    strong baselines across languages and datasets. For English, it achieves a 4.9
    F1 point improvement over the state-of-the-art on GraphQuestions.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1009.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1009.Attachment.pdf
  - filename: https://vimeo.com/238236598
    type: video
    url: https://vimeo.com/238236598
  author:
  - first: Siva
    full: Siva Reddy
    id: siva-reddy
    last: Reddy
  - first: Oscar
    full: "Oscar T\xE4ckstr\xF6m"
    id: oscar-tackstrom
    last: "T\xE4ckstr\xF6m"
  - first: Slav
    full: Slav Petrov
    id: slav-petrov
    last: Petrov
  - first: Mark
    full: Mark Steedman
    id: mark-steedman
    last: Steedman
  - first: Mirella
    full: Mirella Lapata
    id: mirella-lapata
    last: Lapata
  author_string: "Siva Reddy, Oscar T\xE4ckstr\xF6m, Slav Petrov, Mark Steedman, Mirella\
    \ Lapata"
  bibkey: reddy-etal-2017-universal
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1009
  month: September
  page_first: '89'
  page_last: '101'
  pages: "89\u2013101"
  paper_id: '9'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1009.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1009.jpg
  title: Universal Semantic Parsing
  title_html: Universal Semantic Parsing
  url: https://www.aclweb.org/anthology/D17-1009
  year: '2017'
D17-1010:
  abstract: Word embeddings improve generalization over lexical features by placing
    each word in a lower-dimensional space, using distributional information obtained
    from unlabeled data. However, the effectiveness of word embeddings for downstream
    NLP tasks is limited by out-of-vocabulary (OOV) words, for which embeddings do
    not exist. In this paper, we present MIMICK, an approach to generating OOV word
    embeddings compositionally, by learning a function from spellings to distributional
    embeddings. Unlike prior work, MIMICK does not require re-training on the original
    word embedding corpus; instead, learning is performed at the type level. Intrinsic
    and extrinsic evaluations demonstrate the power of this simple approach. On 23
    languages, MIMICK improves performance over a word-based baseline for tagging
    part-of-speech and morphosyntactic attributes. It is competitive with (and complementary
    to) a supervised character-based model in low resource settings.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238234299
    type: video
    url: https://vimeo.com/238234299
  - filename: D17-1010.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/D17-1010.Presentation.pdf
  author:
  - first: Yuval
    full: Yuval Pinter
    id: yuval-pinter
    last: Pinter
  - first: Robert
    full: Robert Guthrie
    id: robert-guthrie
    last: Guthrie
  - first: Jacob
    full: Jacob Eisenstein
    id: jacob-eisenstein
    last: Eisenstein
  author_string: Yuval Pinter, Robert Guthrie, Jacob Eisenstein
  bibkey: pinter-etal-2017-mimicking
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1010
  month: September
  page_first: '102'
  page_last: '112'
  pages: "102\u2013112"
  paper_id: '10'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1010.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1010.jpg
  title: Mimicking Word Embeddings using Subword RNNs
  title_html: Mimicking Word Embeddings using Subword <span class="acl-fixed-case">RNN</span>s
  url: https://www.aclweb.org/anthology/D17-1010
  year: '2017'
D17-1011:
  abstract: "We present SuperPivot, an analysis method for low-resource languages\
    \ that occur in a superparallel corpus, i.e., in a corpus that contains an order\
    \ of magnitude more languages than parallel corpora currently in use. We show\
    \ that SuperPivot performs well for the crosslingual analysis of the linguistic\
    \ phenomenon of tense. We produce analysis results for more than 1000 languages,\
    \ conducting \u2013 to the best of our knowledge \u2013 the largest crosslingual\
    \ computational study performed to date. We extend existing methodology for leveraging\
    \ parallel corpora for typological analysis by overcoming a limiting assumption\
    \ of earlier work: We only require that a linguistic feature is overtly marked\
    \ in a few of thousands of languages as opposed to requiring that it be marked\
    \ in all languages under investigation."
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1011.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1011.Attachment.zip
  - filename: https://vimeo.com/238235308
    type: video
    url: https://vimeo.com/238235308
  author:
  - first: Ehsaneddin
    full: Ehsaneddin Asgari
    id: ehsaneddin-asgari
    last: Asgari
  - first: Hinrich
    full: "Hinrich Sch\xFCtze"
    id: hinrich-schutze
    last: "Sch\xFCtze"
  author_string: "Ehsaneddin Asgari, Hinrich Sch\xFCtze"
  bibkey: asgari-schutze-2017-past
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1011
  month: September
  page_first: '113'
  page_last: '124'
  pages: "113\u2013124"
  paper_id: '11'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1011.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1011.jpg
  title: 'Past, Present, Future: A Computational Investigation of the Typology of
    Tense in 1000 Languages'
  title_html: 'Past, Present, Future: A Computational Investigation of the Typology
    of Tense in 1000 Languages'
  url: https://www.aclweb.org/anthology/D17-1011
  year: '2017'
D17-1012:
  abstract: This paper presents a novel neural machine translation model which jointly
    learns translation and source-side latent graph representations of sentences.
    Unlike existing pipelined approaches using syntactic parsers, our end-to-end model
    learns a latent graph parser as part of the encoder of an attention-based neural
    machine translation model, and thus the parser is optimized according to the translation
    objective. In experiments, we first show that our model compares favorably with
    state-of-the-art sequential and pipelined syntax-based NMT models. We also show
    that the performance of our model can be further improved by pre-training it with
    a small amount of treebank annotations. Our final ensemble model significantly
    outperforms the previous best models on the standard English-to-Japanese translation
    dataset.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238234769
    type: video
    url: https://vimeo.com/238234769
  author:
  - first: Kazuma
    full: Kazuma Hashimoto
    id: kazuma-hashimoto
    last: Hashimoto
  - first: Yoshimasa
    full: Yoshimasa Tsuruoka
    id: yoshimasa-tsuruoka
    last: Tsuruoka
  author_string: Kazuma Hashimoto, Yoshimasa Tsuruoka
  bibkey: hashimoto-tsuruoka-2017-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1012
  month: September
  page_first: '125'
  page_last: '135'
  pages: "125\u2013135"
  paper_id: '12'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1012.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1012.jpg
  title: Neural Machine Translation with Source-Side Latent Graph Parsing
  title_html: Neural Machine Translation with Source-Side Latent Graph Parsing
  url: https://www.aclweb.org/anthology/D17-1012
  year: '2017'
D17-1013:
  abstract: In the encoder-decoder architecture for neural machine translation (NMT),
    the hidden states of the recurrent structures in the encoder and decoder carry
    the crucial information about the sentence. These vectors are generated by parameters
    which are updated by back-propagation of translation errors through time.We argue
    that propagating errors through the end-to-end recurrent structures are not a
    direct way of control the hidden vectors. In this paper, we propose to use word
    predictions as a mechanism for direct supervision. More specifically, we require
    these vectors to be able to predict the vocabulary in target sentence. Our simple
    mechanism ensures better representations in the encoder and decoder without using
    any extra data or annotation. It is also helpful in reducing the target side vocabulary
    and improving the decoding efficiency. Experiments on Chinese-English machine
    translation task show an average BLEU improvement by 4.53, respectively.
  address: Copenhagen, Denmark
  author:
  - first: Rongxiang
    full: Rongxiang Weng
    id: rongxiang-weng
    last: Weng
  - first: Shujian
    full: Shujian Huang
    id: shujian-huang
    last: Huang
  - first: Zaixiang
    full: Zaixiang Zheng
    id: zaixiang-zheng
    last: Zheng
  - first: Xinyu
    full: Xinyu Dai
    id: xinyu-dai
    last: Dai
  - first: Jiajun
    full: Jiajun Chen
    id: jiajun-chen
    last: Chen
  author_string: Rongxiang Weng, Shujian Huang, Zaixiang Zheng, Xinyu Dai, Jiajun
    Chen
  bibkey: weng-etal-2017-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1013
  month: September
  page_first: '136'
  page_last: '145'
  pages: "136\u2013145"
  paper_id: '13'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1013.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1013.jpg
  title: Neural Machine Translation with Word Predictions
  title_html: Neural Machine Translation with Word Predictions
  url: https://www.aclweb.org/anthology/D17-1013
  year: '2017'
D17-1014:
  abstract: We propose a novel decoding approach for neural machine translation (NMT)
    based on continuous optimisation. We reformulate decoding, a discrete optimization
    problem, into a continuous problem, such that optimization can make use of efficient
    gradient-based techniques. Our powerful decoding framework allows for more accurate
    decoding for standard neural machine translation models, as well as enabling decoding
    in intractable models such as intersection of several different NMT models. Our
    empirical results show that our decoding framework is effective, and can leads
    to substantial improvements in translations, especially in situations where greedy
    search and beam search are not feasible. Finally, we show how the technique is
    highly competitive with, and complementary to, reranking.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1014.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1014.Attachment.pdf
  - filename: https://vimeo.com/238236392
    type: video
    url: https://vimeo.com/238236392
  author:
  - first: Cong Duy Vu
    full: Cong Duy Vu Hoang
    id: cong-duy-vu-hoang
    last: Hoang
  - first: Gholamreza
    full: Gholamreza Haffari
    id: gholamreza-haffari
    last: Haffari
  - first: Trevor
    full: Trevor Cohn
    id: trevor-cohn
    last: Cohn
  author_string: Cong Duy Vu Hoang, Gholamreza Haffari, Trevor Cohn
  bibkey: hoang-etal-2017-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1014
  month: September
  page_first: '146'
  page_last: '156'
  pages: "146\u2013156"
  paper_id: '14'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1014.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1014.jpg
  title: Towards Decoding as Continuous Optimisation in Neural Machine Translation
  title_html: Towards Decoding as Continuous Optimisation in Neural Machine Translation
  url: https://www.aclweb.org/anthology/D17-1014
  year: '2017'
D17-1015:
  abstract: "We present a model for locating regions in space based on natural language\
    \ descriptions. Starting with a 3D scene and a sentence, our model is able to\
    \ associate words in the sentence with regions in the scene, interpret relations\
    \ such as \u2018on top of\u2019 or \u2018next to,\u2019 and finally locate the\
    \ region described in the sentence. All components form a single neural network\
    \ that is trained end-to-end without prior knowledge of object segmentation. To\
    \ evaluate our model, we construct and release a new dataset consisting of Minecraft\
    \ scenes with crowdsourced natural language descriptions. We achieve a 32% relative\
    \ error reduction compared to a strong neural baseline."
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238230308
    type: video
    url: https://vimeo.com/238230308
  author:
  - first: Nikita
    full: Nikita Kitaev
    id: nikita-kitaev
    last: Kitaev
  - first: Dan
    full: Dan Klein
    id: dan-klein
    last: Klein
  author_string: Nikita Kitaev, Dan Klein
  bibkey: kitaev-klein-2017-misty
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1015
  month: September
  page_first: '157'
  page_last: '166'
  pages: "157\u2013166"
  paper_id: '15'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1015.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1015.jpg
  title: Where is Misty? Interpreting Spatial Descriptors by Modeling Regions in Space
  title_html: Where is Misty? Interpreting Spatial Descriptors by Modeling Regions
    in Space
  url: https://www.aclweb.org/anthology/D17-1015
  year: '2017'
D17-1016:
  abstract: We propose a method for embedding two-dimensional locations in a continuous
    vector space using a neural network-based model incorporating mixtures of Gaussian
    distributions, presenting two model variants for text-based geolocation and lexical
    dialectology. Evaluated over Twitter data, the proposed model outperforms conventional
    regression-based geolocation and provides a better estimate of uncertainty. We
    also show the effectiveness of the representation for predicting words from location
    in lexical dialectology, and evaluate it using the DARE dataset.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238228698
    type: video
    url: https://vimeo.com/238228698
  author:
  - first: Afshin
    full: Afshin Rahimi
    id: afshin-rahimi
    last: Rahimi
  - first: Timothy
    full: Timothy Baldwin
    id: timothy-baldwin
    last: Baldwin
  - first: Trevor
    full: Trevor Cohn
    id: trevor-cohn
    last: Cohn
  author_string: Afshin Rahimi, Timothy Baldwin, Trevor Cohn
  bibkey: rahimi-etal-2017-continuous
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1016
  month: September
  page_first: '167'
  page_last: '176'
  pages: "167\u2013176"
  paper_id: '16'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1016.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1016.jpg
  title: Continuous Representation of Location for Geolocation and Lexical Dialectology
    using Mixture Density Networks
  title_html: Continuous Representation of Location for Geolocation and Lexical Dialectology
    using Mixture Density Networks
  url: https://www.aclweb.org/anthology/D17-1016
  year: '2017'
D17-1017:
  abstract: Generating captions for images is a task that has recently received considerable
    attention. Another type of visual inputs are abstract scenes or object layouts
    where the only information provided is a set of objects and their locations. This
    type of imagery is commonly found in many applications in computer graphics, virtual
    reality, and storyboarding. We explore in this paper OBJ2TEXT, a sequence-to-sequence
    model that encodes a set of objects and their locations as an input sequence using
    an LSTM network, and decodes this representation using an LSTM language model.
    We show in our paper that this model despite using a sequence encoder can effectively
    represent complex spatial object-object relationships and produce descriptions
    that are globally coherent and semantically relevant. We test our approach for
    the task of describing object layouts in the MS-COCO dataset by producing sentences
    given only object annotations. We additionally show that our model combined with
    a state-of-the-art object detector can improve the accuracy of an image captioning
    model.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238230148
    type: video
    url: https://vimeo.com/238230148
  author:
  - first: Xuwang
    full: Xuwang Yin
    id: xuwang-yin
    last: Yin
  - first: Vicente
    full: Vicente Ordonez
    id: vicente-ordonez
    last: Ordonez
  author_string: Xuwang Yin, Vicente Ordonez
  bibkey: yin-ordonez-2017-obj2text
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1017
  month: September
  page_first: '177'
  page_last: '187'
  pages: "177\u2013187"
  paper_id: '17'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1017.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1017.jpg
  title: 'Obj2Text: Generating Visually Descriptive Language from Object Layouts'
  title_html: '<span class="acl-fixed-case">O</span>bj2<span class="acl-fixed-case">T</span>ext:
    Generating Visually Descriptive Language from Object Layouts'
  url: https://www.aclweb.org/anthology/D17-1017
  year: '2017'
D17-1018:
  abstract: We introduce the first end-to-end coreference resolution model and show
    that it significantly outperforms all previous work without using a syntactic
    parser or hand-engineered mention detector. The key idea is to directly consider
    all spans in a document as potential mentions and learn distributions over possible
    antecedents for each. The model computes span embeddings that combine context-dependent
    boundary representations with a head-finding attention mechanism. It is trained
    to maximize the marginal likelihood of gold antecedent spans from coreference
    clusters and is factored to enable aggressive pruning of potential mentions. Experiments
    demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes
    benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is
    the first approach to be successfully trained with no external resources.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238232979
    type: video
    url: https://vimeo.com/238232979
  author:
  - first: Kenton
    full: Kenton Lee
    id: kenton-lee
    last: Lee
  - first: Luheng
    full: Luheng He
    id: luheng-he
    last: He
  - first: Mike
    full: Mike Lewis
    id: mike-lewis
    last: Lewis
  - first: Luke
    full: Luke Zettlemoyer
    id: luke-zettlemoyer
    last: Zettlemoyer
  author_string: Kenton Lee, Luheng He, Mike Lewis, Luke Zettlemoyer
  bibkey: lee-etal-2017-end
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1018
  month: September
  page_first: '188'
  page_last: '197'
  pages: "188\u2013197"
  paper_id: '18'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1018.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1018.jpg
  title: End-to-end Neural Coreference Resolution
  title_html: End-to-end Neural Coreference Resolution
  url: https://www.aclweb.org/anthology/D17-1018
  year: '2017'
D17-1019:
  abstract: Discourse coherence is strongly associated with text quality, making it
    important to natural language generation and understanding. Yet existing models
    of coherence focus on measuring individual aspects of coherence (lexical overlap,
    rhetorical structure, entity centering) in narrow domains. In this paper, we describe
    domain-independent neural models of discourse coherence that are capable of measuring
    multiple aspects of coherence in existing sentences and can maintain coherence
    while generating new sentences. We study both discriminative models that learn
    to distinguish coherent from incoherent discourse, and generative models that
    produce coherent text, including a novel neural latent-variable Markovian generative
    model that captures the latent discourse dependencies between sentences in a text.
    Our work achieves state-of-the-art performance on multiple coherence evaluations,
    and marks an initial step in generating coherent texts given discourse contexts.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238234840
    type: video
    url: https://vimeo.com/238234840
  author:
  - first: Jiwei
    full: Jiwei Li
    id: jiwei-li
    last: Li
  - first: Dan
    full: Dan Jurafsky
    id: dan-jurafsky
    last: Jurafsky
  author_string: Jiwei Li, Dan Jurafsky
  bibkey: li-jurafsky-2017-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1019
  month: September
  page_first: '198'
  page_last: '209'
  pages: "198\u2013209"
  paper_id: '19'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1019.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1019.jpg
  title: Neural Net Models of Open-domain Discourse Coherence
  title_html: Neural Net Models of Open-domain Discourse Coherence
  url: https://www.aclweb.org/anthology/D17-1019
  year: '2017'
D17-1020:
  abstract: Multi-document summarization provides users with a short text that summarizes
    the information in a set of related documents. This paper introduces affinity-preserving
    random walk to the summarization task, which preserves the affinity relations
    of sentences by an absorbing random walk model. Meanwhile, we put forward adjustable
    affinity-preserving random walk to enforce the diversity constraint of summarization
    in the random walk process. The ROUGE evaluations on DUC 2003 topic-focused summarization
    task and DUC 2004 generic summarization task show the good performance of our
    method, which has the best ROUGE-2 recall among the graph-based ranking methods.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238231488
    type: video
    url: https://vimeo.com/238231488
  author:
  - first: Kexiang
    full: Kexiang Wang
    id: kexiang-wang
    last: Wang
  - first: Tianyu
    full: Tianyu Liu
    id: tianyu-liu
    last: Liu
  - first: Zhifang
    full: Zhifang Sui
    id: zhifang-sui
    last: Sui
  - first: Baobao
    full: Baobao Chang
    id: baobao-chang
    last: Chang
  author_string: Kexiang Wang, Tianyu Liu, Zhifang Sui, Baobao Chang
  bibkey: wang-etal-2017-affinity
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1020
  month: September
  page_first: '210'
  page_last: '220'
  pages: "210\u2013220"
  paper_id: '20'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1020.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1020.jpg
  title: Affinity-Preserving Random Walk for Multi-Document Summarization
  title_html: Affinity-Preserving Random Walk for Multi-Document Summarization
  url: https://www.aclweb.org/anthology/D17-1020
  year: '2017'
D17-1021:
  abstract: "Resolving abstract anaphora is an important, but difficult task for text\
    \ understanding. Yet, with recent advances in representation learning this task\
    \ becomes a more tangible aim. A central property of abstract anaphora is that\
    \ it establishes a relation between the anaphor embedded in the anaphoric sentence\
    \ and its (typically non-nominal) antecedent. We propose a mention-ranking model\
    \ that learns how abstract anaphors relate to their antecedents with an LSTM-Siamese\
    \ Net. We overcome the lack of training data by generating artificial anaphoric\
    \ sentence\u2013antecedent pairs. Our model outperforms state-of-the-art results\
    \ on shell noun resolution. We also report first benchmark results on an abstract\
    \ anaphora subset of the ARRAU corpus. This corpus presents a greater challenge\
    \ due to a mixture of nominal and pronominal anaphors and a greater range of confounders.\
    \ We found model variants that outperform the baselines for nominal anaphors,\
    \ without training on individual anaphor data, but still lag behind for pronominal\
    \ anaphors. Our model selects syntactically plausible candidates and \u2013 if\
    \ disregarding syntax \u2013 discriminates candidates using deeper features."
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1021.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1021.Attachment.pdf
  - filename: https://vimeo.com/238231072
    type: video
    url: https://vimeo.com/238231072
  author:
  - first: Ana
    full: "Ana Marasovi\u0107"
    id: ana-marasovic
    last: "Marasovi\u0107"
  - first: Leo
    full: Leo Born
    id: leo-born
    last: Born
  - first: Juri
    full: Juri Opitz
    id: juri-opitz
    last: Opitz
  - first: Anette
    full: Anette Frank
    id: anette-frank
    last: Frank
  author_string: "Ana Marasovi\u0107, Leo Born, Juri Opitz, Anette Frank"
  bibkey: marasovic-etal-2017-mention
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1021
  month: September
  page_first: '221'
  page_last: '232'
  pages: "221\u2013232"
  paper_id: '21'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1021.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1021.jpg
  title: A Mention-Ranking Model for Abstract Anaphora Resolution
  title_html: A Mention-Ranking Model for Abstract Anaphora Resolution
  url: https://www.aclweb.org/anthology/D17-1021
  year: '2017'
D17-1022:
  abstract: "We present a novel neural model HyperVec to learn hierarchical embeddings\
    \ for hypernymy detection and directionality. While previous embeddings have shown\
    \ limitations on prototypical hypernyms, HyperVec represents an unsupervised measure\
    \ where embeddings are learned in a specific order and capture the hypernym\u2013\
    hyponym distributional hierarchy. Moreover, our model is able to generalize over\
    \ unseen hypernymy pairs, when using only small sets of training data, and by\
    \ mapping to other languages. Results on benchmark datasets show that HyperVec\
    \ outperforms both state-of-the-art unsupervised measures and embedding models\
    \ on hypernymy detection and directionality, and on predicting graded lexical\
    \ entailment."
  address: Copenhagen, Denmark
  author:
  - first: Kim Anh
    full: Kim Anh Nguyen
    id: kim-anh-nguyen
    last: Nguyen
  - first: Maximilian
    full: "Maximilian K\xF6per"
    id: maximilian-koper
    last: "K\xF6per"
  - first: Sabine
    full: Sabine Schulte im Walde
    id: sabine-schulte-im-walde
    last: Schulte im Walde
  - first: Ngoc Thang
    full: Ngoc Thang Vu
    id: ngoc-thang-vu
    last: Vu
  author_string: "Kim Anh Nguyen, Maximilian K\xF6per, Sabine Schulte im Walde, Ngoc\
    \ Thang Vu"
  bibkey: nguyen-etal-2017-hierarchical
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1022
  month: September
  page_first: '233'
  page_last: '243'
  pages: "233\u2013243"
  paper_id: '22'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1022.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1022.jpg
  title: Hierarchical Embeddings for Hypernymy Detection and Directionality
  title_html: Hierarchical Embeddings for Hypernymy Detection and Directionality
  url: https://www.aclweb.org/anthology/D17-1022
  year: '2017'
D17-1023:
  abstract: 'The existing word representation methods mostly limit their information
    source to word co-occurrence statistics. In this paper, we introduce ngrams into
    four representation methods: SGNS, GloVe, PPMI matrix, and its SVD factorization.
    Comprehensive experiments are conducted on word analogy and similarity tasks.
    The results show that improved word representations are learned from ngram co-occurrence
    statistics. We also demonstrate that the trained ngram representations are useful
    in many aspects such as finding antonyms and collocations. Besides, a novel approach
    of building co-occurrence matrix is proposed to alleviate the hardware burdens
    brought by ngrams.'
  address: Copenhagen, Denmark
  author:
  - first: Zhe
    full: Zhe Zhao
    id: zhe-zhao
    last: Zhao
  - first: Tao
    full: Tao Liu
    id: tao-liu
    last: Liu
  - first: Shen
    full: Shen Li
    id: shen-li
    last: Li
  - first: Bofang
    full: Bofang Li
    id: bofang-li
    last: Li
  - first: Xiaoyong
    full: Xiaoyong Du
    id: xiaoyong-du
    last: Du
  author_string: Zhe Zhao, Tao Liu, Shen Li, Bofang Li, Xiaoyong Du
  bibkey: zhao-etal-2017-ngram2vec
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1023
  erratum:
  - id: '1'
    url: https://www.aclweb.org/anthology/D17-1023e1.pdf
    value: D17-1023e1
  month: September
  page_first: '244'
  page_last: '253'
  pages: "244\u2013253"
  paper_id: '23'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1023.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1023.jpg
  title: 'Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence
    Statistics'
  title_html: '<span class="acl-fixed-case">N</span>gram2vec: Learning Improved Word
    Representations from Ngram Co-occurrence Statistics'
  url: https://www.aclweb.org/anthology/D17-1023
  year: '2017'
D17-1024:
  abstract: "Learning word embeddings on large unlabeled corpus has been shown to\
    \ be successful in improving many natural language tasks. The most efficient and\
    \ popular approaches learn or retrofit such representations using additional external\
    \ data. Resulting embeddings are generally better than their corpus-only counterparts,\
    \ although such resources cover a fraction of words in the vocabulary. In this\
    \ paper, we propose a new approach, Dict2vec, based on one of the largest yet\
    \ refined datasource for describing words \u2013 natural language dictionaries.\
    \ Dict2vec builds new word pairs from dictionary entries so that semantically-related\
    \ words are moved closer, and negative sampling filters out pairs whose words\
    \ are unrelated in dictionaries. We evaluate the word representations obtained\
    \ using Dict2vec on eleven datasets for the word similarity task and on four datasets\
    \ for a text classification task."
  address: Copenhagen, Denmark
  author:
  - first: Julien
    full: Julien Tissier
    id: julien-tissier
    last: Tissier
  - first: Christophe
    full: Christophe Gravier
    id: christophe-gravier
    last: Gravier
  - first: Amaury
    full: Amaury Habrard
    id: amaury-habrard
    last: Habrard
  author_string: Julien Tissier, Christophe Gravier, Amaury Habrard
  bibkey: tissier-etal-2017-dict2vec
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1024
  month: September
  page_first: '254'
  page_last: '263'
  pages: "254\u2013263"
  paper_id: '24'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1024.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1024.jpg
  title: 'Dict2vec : Learning Word Embeddings using Lexical Dictionaries'
  title_html: '<span class="acl-fixed-case">D</span>ict2vec : Learning Word Embeddings
    using Lexical Dictionaries'
  url: https://www.aclweb.org/anthology/D17-1024
  year: '2017'
D17-1025:
  abstract: In this paper, we propose new methods to learn Chinese word representations.
    Chinese characters are composed of graphical components, which carry rich semantics.
    It is common for a Chinese learner to comprehend the meaning of a word from these
    graphical components. As a result, we propose models that enhance word representations
    by character glyphs. The character glyph features are directly learned from the
    bitmaps of characters by convolutional auto-encoder(convAE), and the glyph features
    improve Chinese word representations which are already enhanced by character embeddings.
    Another contribution in this paper is that we created several evaluation datasets
    in traditional Chinese and made them public.
  address: Copenhagen, Denmark
  author:
  - first: Tzu-Ray
    full: Tzu-Ray Su
    id: tzu-ray-su
    last: Su
  - first: Hung-Yi
    full: Hung-Yi Lee
    id: hung-yi-lee
    last: Lee
  author_string: Tzu-Ray Su, Hung-Yi Lee
  bibkey: su-lee-2017-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1025
  month: September
  page_first: '264'
  page_last: '273'
  pages: "264\u2013273"
  paper_id: '25'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1025.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1025.jpg
  title: Learning Chinese Word Representations From Glyphs Of Characters
  title_html: Learning <span class="acl-fixed-case">C</span>hinese Word Representations
    From Glyphs Of Characters
  url: https://www.aclweb.org/anthology/D17-1025
  year: '2017'
D17-1026:
  abstract: We consider the problem of learning general-purpose, paraphrastic sentence
    embeddings in the setting of Wieting et al. (2016b). We use neural machine translation
    to generate sentential paraphrases via back-translation of bilingual sentence
    pairs. We evaluate the paraphrase pairs by their ability to serve as training
    data for learning paraphrastic sentence embeddings. We find that the data quality
    is stronger than prior work based on bitext and on par with manually-written English
    paraphrase pairs, with the advantage that our approach can scale up to generate
    large training sets for many languages and domains. We experiment with several
    language pairs and data sources, and develop a variety of data filtering techniques.
    In the process, we explore how neural machine translation output differs from
    human-written sentences, finding clear differences in length, the amount of repetition,
    and the use of rare words.
  address: Copenhagen, Denmark
  author:
  - first: John
    full: John Wieting
    id: john-wieting
    last: Wieting
  - first: Jonathan
    full: Jonathan Mallinson
    id: jonathan-mallinson
    last: Mallinson
  - first: Kevin
    full: Kevin Gimpel
    id: kevin-gimpel
    last: Gimpel
  author_string: John Wieting, Jonathan Mallinson, Kevin Gimpel
  bibkey: wieting-etal-2017-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1026
  month: September
  page_first: '274'
  page_last: '285'
  pages: "274\u2013285"
  paper_id: '26'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1026.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1026.jpg
  title: Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext
  title_html: Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext
  url: https://www.aclweb.org/anthology/D17-1026
  year: '2017'
D17-1027:
  abstract: Word embeddings have attracted much attention recently. Different from
    alphabetic writing systems, Chinese characters are often composed of subcharacter
    components which are also semantically informative. In this work, we propose an
    approach to jointly embed Chinese words as well as their characters and fine-grained
    subcharacter components. We use three likelihoods to evaluate whether the context
    words, characters, and components can predict the current target word, and collected
    13,253 subcharacter components to demonstrate the existing approaches of decomposing
    Chinese characters are not enough. Evaluation on both word similarity and word
    analogy tasks demonstrates the superior performance of our model.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1027.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1027.Attachment.zip
  author:
  - first: Jinxing
    full: Jinxing Yu
    id: jinxing-yu
    last: Yu
  - first: Xun
    full: Xun Jian
    id: xun-jian
    last: Jian
  - first: Hao
    full: Hao Xin
    id: hao-xin
    last: Xin
  - first: Yangqiu
    full: Yangqiu Song
    id: yangqiu-song
    last: Song
  author_string: Jinxing Yu, Xun Jian, Hao Xin, Yangqiu Song
  bibkey: yu-etal-2017-joint
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1027
  month: September
  page_first: '286'
  page_last: '291'
  pages: "286\u2013291"
  paper_id: '27'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1027.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1027.jpg
  title: Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter
    Components
  title_html: Joint Embeddings of <span class="acl-fixed-case">C</span>hinese Words,
    Characters, and Fine-grained Subcharacter Components
  url: https://www.aclweb.org/anthology/D17-1027
  year: '2017'
D17-1028:
  abstract: We present an unsupervised, language agnostic approach for exploiting
    morphological regularities present in high dimensional vector spaces. We propose
    a novel method for generating embeddings of words from their morphological variants
    using morphological transformation operators. We evaluate this approach on MSR
    word analogy test set with an accuracy of 85% which is 12% higher than the previous
    best known system.
  address: Copenhagen, Denmark
  author:
  - first: Arihant
    full: Arihant Gupta
    id: arihant-gupta
    last: Gupta
  - first: Syed Sarfaraz
    full: Syed Sarfaraz Akhtar
    id: syed-sarfaraz-akhtar
    last: Akhtar
  - first: Avijit
    full: Avijit Vajpayee
    id: avijit-vajpayee
    last: Vajpayee
  - first: Arjit
    full: Arjit Srivastava
    id: arjit-srivastava
    last: Srivastava
  - first: Madan Gopal
    full: Madan Gopal Jhanwar
    id: madan-gopal-jhanwar
    last: Jhanwar
  - first: Manish
    full: Manish Shrivastava
    id: manish-shrivastava
    last: Shrivastava
  author_string: Arihant Gupta, Syed Sarfaraz Akhtar, Avijit Vajpayee, Arjit Srivastava,
    Madan Gopal Jhanwar, Manish Shrivastava
  bibkey: gupta-etal-2017-exploiting
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1028
  month: September
  page_first: '292'
  page_last: '297'
  pages: "292\u2013297"
  paper_id: '28'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1028.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1028.jpg
  title: Exploiting Morphological Regularities in Distributional Word Representations
  title_html: Exploiting Morphological Regularities in Distributional Word Representations
  url: https://www.aclweb.org/anthology/D17-1028
  year: '2017'
D17-1029:
  abstract: We introduce a novel mixed characterword architecture to improve Chinese
    sentence representations, by utilizing rich semantic information of word internal
    structures. Our architecture uses two key strategies. The first is a mask gate
    on characters, learning the relation among characters in a word. The second is
    a maxpooling operation on words, adaptively finding the optimal mixture of the
    atomic and compositional word representations. Finally, the proposed architecture
    is applied to various sentence composition models, which achieves substantial
    performance gains over baseline models on sentence similarity task.
  address: Copenhagen, Denmark
  author:
  - first: Shaonan
    full: Shaonan Wang
    id: shaonan-wang
    last: Wang
  - first: Jiajun
    full: Jiajun Zhang
    id: jiajun-zhang
    last: Zhang
  - first: Chengqing
    full: Chengqing Zong
    id: chengqing-zong
    last: Zong
  author_string: Shaonan Wang, Jiajun Zhang, Chengqing Zong
  bibkey: wang-etal-2017-exploiting
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1029
  month: September
  page_first: '298'
  page_last: '303'
  pages: "298\u2013303"
  paper_id: '29'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1029.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1029.jpg
  title: Exploiting Word Internal Structures for Generic Chinese Sentence Representation
  title_html: Exploiting Word Internal Structures for Generic <span class="acl-fixed-case">C</span>hinese
    Sentence Representation
  url: https://www.aclweb.org/anthology/D17-1029
  year: '2017'
D17-1030:
  abstract: "Distributional semantics models are known to struggle with small data.\
    \ It is generally accepted that in order to learn \u2018a good vector\u2019 for\
    \ a word, a model must have sufficient examples of its usage. This contradicts\
    \ the fact that humans can guess the meaning of a word from a few occurrences\
    \ only. In this paper, we show that a neural language model such as Word2Vec only\
    \ necessitates minor modifications to its standard architecture to learn new terms\
    \ from tiny data, using background knowledge from a previously learnt semantic\
    \ space. We test our model on word definitions and on a nonce task involving 2-6\
    \ sentences\u2019 worth of context, showing a large increase in performance over\
    \ state-of-the-art models on the definitional task."
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1030.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1030.Attachment.zip
  author:
  - first: "Aur\xE9lie"
    full: "Aur\xE9lie Herbelot"
    id: aurelie-herbelot
    last: Herbelot
  - first: Marco
    full: Marco Baroni
    id: marco-baroni
    last: Baroni
  author_string: "Aur\xE9lie Herbelot, Marco Baroni"
  bibkey: herbelot-baroni-2017-high
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1030
  month: September
  page_first: '304'
  page_last: '309'
  pages: "304\u2013309"
  paper_id: '30'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1030.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1030.jpg
  title: 'High-risk learning: acquiring new word vectors from tiny data'
  title_html: 'High-risk learning: acquiring new word vectors from tiny data'
  url: https://www.aclweb.org/anthology/D17-1030
  year: '2017'
D17-1031:
  abstract: In this paper, we propose to learn word embeddings based on the recent
    fixed-size ordinally forgetting encoding (FOFE) method, which can almost uniquely
    encode any variable-length sequence into a fixed-size representation. We use FOFE
    to fully encode the left and right context of each word in a corpus to construct
    a novel word-context matrix, which is further weighted and factorized using truncated
    SVD to generate low-dimension word embedding vectors. We evaluate this alternate
    method in encoding word-context statistics and show the new FOFE method has a
    notable effect on the resulting word embeddings. Experimental results on several
    popular word similarity tasks have demonstrated that the proposed method outperforms
    other SVD models that use canonical count based techniques to generate word context
    matrices.
  address: Copenhagen, Denmark
  author:
  - first: Joseph
    full: Joseph Sanu
    id: joseph-sanu
    last: Sanu
  - first: Mingbin
    full: Mingbin Xu
    id: mingbin-xu
    last: Xu
  - first: Hui
    full: Hui Jiang
    id: hui-jiang
    last: Jiang
  - first: Quan
    full: Quan Liu
    id: quan-liu
    last: Liu
  author_string: Joseph Sanu, Mingbin Xu, Hui Jiang, Quan Liu
  bibkey: sanu-etal-2017-word
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1031
  month: September
  page_first: '310'
  page_last: '315'
  pages: "310\u2013315"
  paper_id: '31'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1031.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1031.jpg
  title: Word Embeddings based on Fixed-Size Ordinally Forgetting Encoding
  title_html: Word Embeddings based on Fixed-Size Ordinally Forgetting Encoding
  url: https://www.aclweb.org/anthology/D17-1031
  year: '2017'
D17-1032:
  abstract: "Many Natural Language Processing (NLP) models rely on distributed vector\
    \ representations of words. Because the process of training word vectors can require\
    \ large amounts of data and computation, NLP researchers and practitioners often\
    \ utilize pre-trained embeddings downloaded from the Web. However, finding the\
    \ best embeddings for a given task is difficult, and can be computationally prohibitive.\
    \ We present a framework, called VecShare, that makes it easy to share and retrieve\
    \ word embeddings on the Web. The framework leverages a public data-sharing infrastructure\
    \ to host embedding sets, and provides automated mechanisms for retrieving the\
    \ embeddings most similar to a given corpus. We perform an experimental evaluation\
    \ of VecShare\u2019s similarity strategies, and show that they are effective at\
    \ efficiently retrieving embeddings that boost accuracy in a document classification\
    \ task. Finally, we provide an open-source Python library for using the VecShare\
    \ framework."
  address: Copenhagen, Denmark
  author:
  - first: Jared
    full: Jared Fernandez
    id: jared-fernandez
    last: Fernandez
  - first: Zhaocheng
    full: Zhaocheng Yu
    id: zhaocheng-yu
    last: Yu
  - first: Doug
    full: Doug Downey
    id: doug-downey
    last: Downey
  author_string: Jared Fernandez, Zhaocheng Yu, Doug Downey
  bibkey: fernandez-etal-2017-vecshare
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1032
  month: September
  page_first: '316'
  page_last: '320'
  pages: "316\u2013320"
  paper_id: '32'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1032.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1032.jpg
  title: 'VecShare: A Framework for Sharing Word Representation Vectors'
  title_html: '<span class="acl-fixed-case">V</span>ec<span class="acl-fixed-case">S</span>hare:
    A Framework for Sharing Word Representation Vectors'
  url: https://www.aclweb.org/anthology/D17-1032
  year: '2017'
D17-1033:
  abstract: Word embeddings seek to recover a Euclidean metric space by mapping words
    into vectors, starting from words co-occurrences in a corpus. Word embeddings
    may underestimate the similarity between nearby words, and overestimate it between
    distant words in the Euclidean metric space. In this paper, we re-embed pre-trained
    word embeddings with a stage of manifold learning which retains dimensionality.
    We show that this approach is theoretically founded in the metric recovery paradigm,
    and empirically show that it can improve on state-of-the-art embeddings in word
    similarity tasks 0.5 - 5.0% points depending on the original space.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1033.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1033.Attachment.zip
  author:
  - first: Souleiman
    full: Souleiman Hasan
    id: souleiman-hasan
    last: Hasan
  - first: Edward
    full: Edward Curry
    id: edward-curry
    last: Curry
  author_string: Souleiman Hasan, Edward Curry
  bibkey: hasan-curry-2017-word
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1033
  month: September
  page_first: '321'
  page_last: '326'
  pages: "321\u2013326"
  paper_id: '33'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1033.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1033.jpg
  title: Word Re-Embedding via Manifold Dimensionality Retention
  title_html: Word Re-Embedding via Manifold Dimensionality Retention
  url: https://www.aclweb.org/anthology/D17-1033
  year: '2017'
D17-1034:
  abstract: This paper proposes to address the word sense ambiguity issue in an unsupervised
    manner, where word sense representations are learned along a word sense selection
    mechanism given contexts. Prior work focused on designing a single model to deliver
    both mechanisms, and thus suffered from either coarse-grained representation learning
    or inefficient sense selection. The proposed modular approach, MUSE, implements
    flexible modules to optimize distinct mechanisms, achieving the first purely sense-level
    representation learning system with linear-time sense selection. We leverage reinforcement
    learning to enable joint training on the proposed modules, and introduce various
    exploration techniques on sense selection for better robustness. The experiments
    on benchmark data show that the proposed approach achieves the state-of-the-art
    performance on synonym selection as well as on contextual word similarities in
    terms of MaxSimC.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1034.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1034.Attachment.zip
  author:
  - first: Guang-He
    full: Guang-He Lee
    id: guang-he-lee
    last: Lee
  - first: Yun-Nung
    full: Yun-Nung Chen
    id: yun-nung-chen
    last: Chen
  author_string: Guang-He Lee, Yun-Nung Chen
  bibkey: lee-chen-2017-muse
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1034
  month: September
  page_first: '327'
  page_last: '337'
  pages: "327\u2013337"
  paper_id: '34'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1034.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1034.jpg
  title: 'MUSE: Modularizing Unsupervised Sense Embeddings'
  title_html: '<span class="acl-fixed-case">MUSE</span>: Modularizing Unsupervised
    Sense Embeddings'
  url: https://www.aclweb.org/anthology/D17-1034
  year: '2017'
D17-1035:
  abstract: "In this paper we show that reporting a single performance score is insufficient\
    \ to compare non-deterministic approaches. We demonstrate for common sequence\
    \ tagging tasks that the seed value for the random number generator can result\
    \ in statistically significant (p < 10-4) differences for state-of-the-art systems.\
    \ For two recent systems for NER, we observe an absolute difference of one percentage\
    \ point F\u2081-score depending on the selected seed value, making these systems\
    \ perceived either as state-of-the-art or mediocre. Instead of publishing and\
    \ reporting single performance scores, we propose to compare score distributions\
    \ based on multiple executions. Based on the evaluation of 50.000 LSTM-networks\
    \ for five sequence tagging tasks, we present network architectures that produce\
    \ both superior performance as well as are more stable with respect to the remaining\
    \ hyperparameters. ) differences for state-of-the-art systems. For two recent\
    \ systems for NER, we observe an absolute difference of one percentage point F\u2081\
    -score depending on the selected seed value, making these systems perceived either\
    \ as state-of-the-art or mediocre. Instead of publishing and reporting single\
    \ performance scores, we propose to compare score distributions based on multiple\
    \ executions. Based on the evaluation of 50.000 LSTM-networks for five sequence\
    \ tagging tasks, we present network architectures that produce both superior performance\
    \ as well as are more stable with respect to the remaining hyperparameters."
  address: Copenhagen, Denmark
  author:
  - first: Nils
    full: Nils Reimers
    id: nils-reimers
    last: Reimers
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Nils Reimers, Iryna Gurevych
  bibkey: reimers-gurevych-2017-reporting
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1035
  month: September
  page_first: '338'
  page_last: '348'
  pages: "338\u2013348"
  paper_id: '35'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1035.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1035.jpg
  title: 'Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks
    for Sequence Tagging'
  title_html: 'Reporting Score Distributions Makes a Difference: Performance Study
    of <span class="acl-fixed-case">LSTM</span>-networks for Sequence Tagging'
  url: https://www.aclweb.org/anthology/D17-1035
  year: '2017'
D17-1036:
  abstract: "We introduce a novel neural easy-first decoder that learns to solve sequence\
    \ tagging tasks in a flexible order. In contrast to previous easy-first decoders,\
    \ our models are end-to-end differentiable. The decoder iteratively updates a\
    \ \u201Csketch\u201D of the predictions over the sequence. At its core is an attention\
    \ mechanism that controls which parts of the input are strategically the best\
    \ to process next. We present a new constrained softmax transformation that ensures\
    \ the same cumulative attention to every word, and show how to efficiently evaluate\
    \ and backpropagate over it. Our models compare favourably to BILSTM taggers on\
    \ three sequence tagging tasks."
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1036.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1036.Attachment.pdf
  author:
  - first: "Andr\xE9 F. T."
    full: "Andr\xE9 F. T. Martins"
    id: andre-f-t-martins
    last: Martins
  - first: Julia
    full: Julia Kreutzer
    id: julia-kreutzer
    last: Kreutzer
  author_string: "Andr\xE9 F. T. Martins, Julia Kreutzer"
  bibkey: martins-kreutzer-2017-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1036
  month: September
  page_first: '349'
  page_last: '362'
  pages: "349\u2013362"
  paper_id: '36'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1036.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1036.jpg
  title: "Learning What\u2019s Easy: Fully Differentiable Neural Easy-First Taggers"
  title_html: "Learning What\u2019s Easy: Fully Differentiable Neural Easy-First Taggers"
  url: https://www.aclweb.org/anthology/D17-1036
  year: '2017'
D17-1037:
  abstract: This paper explores an incremental training strategy for the skip-gram
    model with negative sampling (SGNS) from both empirical and theoretical perspectives.
    Existing methods of neural word embeddings, including SGNS, are multi-pass algorithms
    and thus cannot perform incremental model update. To address this problem, we
    present a simple incremental extension of SGNS and provide a thorough theoretical
    analysis to demonstrate its validity. Empirical experiments demonstrated the correctness
    of the theoretical analysis as well as the practical usefulness of the incremental
    algorithm.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1037.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1037.Attachment.pdf
  - filename: D17-1037.Poster.pdf
    type: poster
    url: https://www.aclweb.org/anthology/attachments/D17-1037.Poster.pdf
  author:
  - first: Nobuhiro
    full: Nobuhiro Kaji
    id: nobuhiro-kaji
    last: Kaji
  - first: Hayato
    full: Hayato Kobayashi
    id: hayato-kobayashi
    last: Kobayashi
  author_string: Nobuhiro Kaji, Hayato Kobayashi
  bibkey: kaji-kobayashi-2017-incremental
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1037
  month: September
  page_first: '363'
  page_last: '371'
  pages: "363\u2013371"
  paper_id: '37'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1037.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1037.jpg
  title: Incremental Skip-gram Model with Negative Sampling
  title_html: Incremental Skip-gram Model with Negative Sampling
  url: https://www.aclweb.org/anthology/D17-1037
  year: '2017'
D17-1038:
  abstract: "Domain similarity measures can be used to gauge adaptability and select\
    \ suitable data for transfer learning, but existing approaches define ad hoc measures\
    \ that are deemed suitable for respective tasks. Inspired by work on curriculum\
    \ learning, we propose to learn data selection measures using Bayesian Optimization\
    \ and evaluate them across models, domains and tasks. Our learned measures outperform\
    \ existing domain similarity measures significantly on three tasks: sentiment\
    \ analysis, part-of-speech tagging, and parsing. We show the importance of complementing\
    \ similarity with diversity, and that learned measures are\u2013to some degree\u2013\
    transferable across models, domains, and even tasks."
  address: Copenhagen, Denmark
  author:
  - first: Sebastian
    full: Sebastian Ruder
    id: sebastian-ruder
    last: Ruder
  - first: Barbara
    full: Barbara Plank
    id: barbara-plank
    last: Plank
  author_string: Sebastian Ruder, Barbara Plank
  bibkey: ruder-plank-2017-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1038
  month: September
  page_first: '372'
  page_last: '382'
  pages: "372\u2013382"
  paper_id: '38'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1038.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1038.jpg
  title: Learning to select data for transfer learning with Bayesian Optimization
  title_html: Learning to select data for transfer learning with <span class="acl-fixed-case">B</span>ayesian
    Optimization
  url: https://www.aclweb.org/anthology/D17-1038
  year: '2017'
D17-1039:
  abstract: "This work presents a general unsupervised learning method to improve\
    \ the accuracy of sequence to sequence (seq2seq) models. In our method, the weights\
    \ of the encoder and decoder of a seq2seq model are initialized with the pretrained\
    \ weights of two language models and then fine-tuned with labeled data. We apply\
    \ this method to challenging benchmarks in machine translation and abstractive\
    \ summarization and find that it significantly improves the subsequent supervised\
    \ models. Our main result is that pretraining improves the generalization of seq2seq\
    \ models. We achieve state-of-the-art results on the WMT English\u2192German task,\
    \ surpassing a range of methods using both phrase-based machine translation and\
    \ neural machine translation. Our method achieves a significant improvement of\
    \ 1.3 BLEU from th previous best models on both WMT\u201914 and WMT\u201915 English\u2192\
    German. We also conduct human evaluations on abstractive summarization and find\
    \ that our method outperforms a purely supervised learning baseline in a statistically\
    \ significant manner."
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1039.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1039.Attachment.zip
  author:
  - first: Prajit
    full: Prajit Ramachandran
    id: prajit-ramachandran
    last: Ramachandran
  - first: Peter
    full: Peter Liu
    id: peter-j-liu
    last: Liu
  - first: Quoc
    full: Quoc Le
    id: quoc-le
    last: Le
  author_string: Prajit Ramachandran, Peter Liu, Quoc Le
  bibkey: ramachandran-etal-2017-unsupervised
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1039
  month: September
  page_first: '383'
  page_last: '391'
  pages: "383\u2013391"
  paper_id: '39'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1039.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1039.jpg
  title: Unsupervised Pretraining for Sequence to Sequence Learning
  title_html: Unsupervised Pretraining for Sequence to Sequence Learning
  url: https://www.aclweb.org/anthology/D17-1039
  year: '2017'
D17-1040:
  abstract: The standard content-based attention mechanism typically used in sequence-to-sequence
    models is computationally expensive as it requires the comparison of large encoder
    and decoder states at each time step. In this work, we propose an alternative
    attention mechanism based on a fixed size memory representation that is more efficient.
    Our technique predicts a compact set of K attention contexts during encoding and
    lets the decoder compute an efficient lookup that does not need to consult the
    memory. We show that our approach performs on-par with the standard attention
    mechanism while yielding inference speedups of 20% for real-world translation
    tasks and more for tasks with longer sequences. By visualizing attention scores
    we demonstrate that our models learn distinct, meaningful alignments.
  address: Copenhagen, Denmark
  author:
  - first: Denny
    full: Denny Britz
    id: denny-britz
    last: Britz
  - first: Melody
    full: Melody Guan
    id: melody-guan
    last: Guan
  - first: Minh-Thang
    full: Minh-Thang Luong
    id: minh-thang-luong
    last: Luong
  author_string: Denny Britz, Melody Guan, Minh-Thang Luong
  bibkey: britz-etal-2017-efficient
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1040
  month: September
  page_first: '392'
  page_last: '400'
  pages: "392\u2013400"
  paper_id: '40'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1040.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1040.jpg
  title: Efficient Attention using a Fixed-Size Memory Representation
  title_html: Efficient Attention using a Fixed-Size Memory Representation
  url: https://www.aclweb.org/anthology/D17-1040
  year: '2017'
D17-1041:
  abstract: Vector representation of words improves performance in various NLP tasks,
    but the high dimensional word vectors are very difficult to interpret. We apply
    several rotation algorithms to the vector representation of words to improve the
    interpretability. Unlike previous approaches that induce sparsity, the rotated
    vectors are interpretable while preserving the expressive performance of the original
    vectors. Furthermore, any prebuilt word vector representation can be rotated for
    improved interpretability. We apply rotation to skipgrams and glove and compare
    the expressive power and interpretability with the original vectors and the sparse
    overcomplete vectors. The results show that the rotated vectors outperform the
    original and the sparse overcomplete vectors for interpretability and expressiveness
    tasks.
  address: Copenhagen, Denmark
  author:
  - first: Sungjoon
    full: Sungjoon Park
    id: sungjoon-park
    last: Park
  - first: JinYeong
    full: JinYeong Bak
    id: jinyeong-bak
    last: Bak
  - first: Alice
    full: Alice Oh
    id: alice-oh
    last: Oh
  author_string: Sungjoon Park, JinYeong Bak, Alice Oh
  bibkey: park-etal-2017-rotated
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1041
  month: September
  page_first: '401'
  page_last: '411'
  pages: "401\u2013411"
  paper_id: '41'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1041.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1041.jpg
  title: Rotated Word Vector Representations and their Interpretability
  title_html: Rotated Word Vector Representations and their Interpretability
  url: https://www.aclweb.org/anthology/D17-1041
  year: '2017'
D17-1042:
  abstract: "We interpret the predictions of any black-box structured input-structured\
    \ output model around a specific input-output pair. Our method returns an \u201C\
    explanation\u201D consisting of groups of input-output tokens that are causally\
    \ related. These dependencies are inferred by querying the model with perturbed\
    \ inputs, generating a graph over tokens from the responses, and solving a partitioning\
    \ problem to select the most relevant components. We focus the general approach\
    \ on sequence-to-sequence problems, adopting a variational autoencoder to yield\
    \ meaningful input perturbations. We test our method across several NLP sequence\
    \ generation tasks."
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1042.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1042.Attachment.zip
  author:
  - first: David
    full: David Alvarez-Melis
    id: david-alvarez-melis
    last: Alvarez-Melis
  - first: Tommi
    full: Tommi Jaakkola
    id: tommi-jaakkola
    last: Jaakkola
  author_string: David Alvarez-Melis, Tommi Jaakkola
  bibkey: alvarez-melis-jaakkola-2017-causal
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1042
  month: September
  page_first: '412'
  page_last: '421'
  pages: "412\u2013421"
  paper_id: '42'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1042.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1042.jpg
  title: A causal framework for explaining the predictions of black-box sequence-to-sequence
    models
  title_html: A causal framework for explaining the predictions of black-box sequence-to-sequence
    models
  url: https://www.aclweb.org/anthology/D17-1042
  year: '2017'
D17-1043:
  abstract: Advances in neural variational inference have facilitated the learning
    of powerful directed graphical models with continuous latent variables, such as
    variational autoencoders. The hope is that such models will learn to represent
    rich, multi-modal latent factors in real-world data, such as natural language
    text. However, current models often assume simplistic priors on the latent variables
    - such as the uni-modal Gaussian distribution - which are incapable of representing
    complex latent factors efficiently. To overcome this restriction, we propose the
    simple, but highly flexible, piecewise constant distribution. This distribution
    has the capacity to represent an exponential number of modes of a latent target
    distribution, while remaining mathematically tractable. Our results demonstrate
    that incorporating this new latent distribution into different models yields substantial
    improvements in natural language processing tasks such as document modeling and
    natural language generation for dialogue.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1043.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1043.Attachment.zip
  author:
  - first: Iulian Vlad
    full: Iulian Vlad Serban
    id: iulian-vlad-serban
    last: Serban
  - first: Alexander G.
    full: Alexander G. Ororbia
    id: alexander-g-ororbia
    last: Ororbia
  - first: Joelle
    full: Joelle Pineau
    id: joelle-pineau
    last: Pineau
  - first: Aaron
    full: Aaron Courville
    id: aaron-courville
    last: Courville
  author_string: Iulian Vlad Serban, Alexander G. Ororbia, Joelle Pineau, Aaron Courville
  bibkey: serban-etal-2017-piecewise-latent
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1043
  month: September
  page_first: '422'
  page_last: '432'
  pages: "422\u2013432"
  paper_id: '43'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1043.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1043.jpg
  title: Piecewise Latent Variables for Neural Variational Text Processing
  title_html: Piecewise Latent Variables for Neural Variational Text Processing
  url: https://www.aclweb.org/anthology/D17-1043
  year: '2017'
D17-1044:
  abstract: The computational complexity of linear-chain Conditional Random Fields
    (CRFs) makes it difficult to deal with very large label sets and long range dependencies.
    Such situations are not rare and arise when dealing with morphologically rich
    languages or joint labelling tasks. We extend here recent proposals to consider
    variable order CRFs. Using an effective finite-state representation of variable-length
    dependencies, we propose new ways to perform feature selection at large scale
    and report experimental results where we outperform strong baselines on a tagging
    task.
  address: Copenhagen, Denmark
  author:
  - first: Thomas
    full: Thomas Lavergne
    id: thomas-lavergne
    last: Lavergne
  - first: "Fran\xE7ois"
    full: "Fran\xE7ois Yvon"
    id: francois-yvon
    last: Yvon
  author_string: "Thomas Lavergne, Fran\xE7ois Yvon"
  bibkey: lavergne-yvon-2017-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1044
  month: September
  page_first: '433'
  page_last: '439'
  pages: "433\u2013439"
  paper_id: '44'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1044.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1044.jpg
  title: 'Learning the Structure of Variable-Order CRFs: a finite-state perspective'
  title_html: 'Learning the Structure of Variable-Order <span class="acl-fixed-case">CRF</span>s:
    a finite-state perspective'
  url: https://www.aclweb.org/anthology/D17-1044
  year: '2017'
D17-1045:
  abstract: We make distributed stochastic gradient descent faster by exchanging sparse
    updates instead of dense updates. Gradient updates are positively skewed as most
    updates are near zero, so we map the 99% smallest updates (by absolute value)
    to zero then exchange sparse matrices. This method can be combined with quantization
    to further improve the compression. We explore different configurations and apply
    them to neural machine translation and MNIST image classification tasks. Most
    configurations work on MNIST, whereas different configurations reduce convergence
    rate on the more complex translation task. Our experiments show that we can achieve
    up to 49% speed up on MNIST and 22% on NMT without damaging the final accuracy
    or BLEU.
  address: Copenhagen, Denmark
  author:
  - first: Alham Fikri
    full: Alham Fikri Aji
    id: alham-fikri-aji
    last: Aji
  - first: Kenneth
    full: Kenneth Heafield
    id: kenneth-heafield
    last: Heafield
  author_string: Alham Fikri Aji, Kenneth Heafield
  bibkey: aji-heafield-2017-sparse
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1045
  month: September
  page_first: '440'
  page_last: '445'
  pages: "440\u2013445"
  paper_id: '45'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1045.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1045.jpg
  title: Sparse Communication for Distributed Gradient Descent
  title_html: Sparse Communication for Distributed Gradient Descent
  url: https://www.aclweb.org/anthology/D17-1045
  year: '2017'
D17-1046:
  abstract: "Online topic modeling, i.e., topic modeling with stochastic variational\
    \ inference, is a powerful and efficient technique for analyzing large datasets,\
    \ and ADAGRAD is a widely-used technique for tuning learning rates during online\
    \ gradient optimization. However, these two techniques do not work well together.\
    \ We show that this is because ADAGRAD uses accumulation of previous gradients\
    \ as the learning rates\u2019 denominators. For online topic modeling, the magnitude\
    \ of gradients is very large. It causes learning rates to shrink very quickly,\
    \ so the parameters cannot fully converge until the training ends"
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1046.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1046.Attachment.zip
  author:
  - first: You
    full: You Lu
    id: you-lu
    last: Lu
  - first: Jeffrey
    full: Jeffrey Lund
    id: jeffrey-lund
    last: Lund
  - first: Jordan
    full: Jordan Boyd-Graber
    id: jordan-boyd-graber
    last: Boyd-Graber
  author_string: You Lu, Jeffrey Lund, Jordan Boyd-Graber
  bibkey: lu-etal-2017-adagrad
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1046
  month: September
  page_first: '446'
  page_last: '451'
  pages: "446\u2013451"
  paper_id: '46'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1046.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1046.jpg
  title: Why ADAGRAD Fails for Online Topic Modeling
  title_html: Why <span class="acl-fixed-case">ADAGRAD</span> Fails for Online Topic
    Modeling
  url: https://www.aclweb.org/anthology/D17-1046
  year: '2017'
D17-1047:
  abstract: 'We propose a novel framework based on neural networks to identify the
    sentiment of opinion targets in a comment/review. Our framework adopts multiple-attention
    mechanism to capture sentiment features separated by a long distance, so that
    it is more robust against irrelevant information. The results of multiple attentions
    are non-linearly combined with a recurrent neural network, which strengthens the
    expressive power of our model for handling more complications. The weighted-memory
    mechanism not only helps us avoid the labor-intensive feature engineering work,
    but also provides a tailor-made memory for different opinion targets of a sentence.
    We examine the merit of our model on four datasets: two are from SemEval2014,
    i.e. reviews of restaurants and laptops; a twitter dataset, for testing its performance
    on social media data; and a Chinese news comment dataset, for testing its language
    sensitivity. The experimental results show that our model consistently outperforms
    the state-of-the-art methods on different types of data.'
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1047.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1047.Attachment.zip
  author:
  - first: Peng
    full: Peng Chen
    id: peng-chen
    last: Chen
  - first: Zhongqian
    full: Zhongqian Sun
    id: zhongqian-sun
    last: Sun
  - first: Lidong
    full: Lidong Bing
    id: lidong-bing
    last: Bing
  - first: Wei
    full: Wei Yang
    id: wei-yang
    last: Yang
  author_string: Peng Chen, Zhongqian Sun, Lidong Bing, Wei Yang
  bibkey: chen-etal-2017-recurrent-attention
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1047
  month: September
  page_first: '452'
  page_last: '461'
  pages: "452\u2013461"
  paper_id: '47'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1047.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1047.jpg
  title: Recurrent Attention Network on Memory for Aspect Sentiment Analysis
  title_html: Recurrent Attention Network on Memory for Aspect Sentiment Analysis
  url: https://www.aclweb.org/anthology/D17-1047
  year: '2017'
D17-1048:
  abstract: Attention models are proposed in sentiment analysis because some words
    are more important than others. However,most existing methods either use local
    context based text information or user preference information. In this work, we
    propose a novel attention model trained by cognition grounded eye-tracking data.
    A reading prediction model is first built using eye-tracking data as dependent
    data and other features in the context as independent data. The predicted reading
    time is then used to build a cognition based attention (CBA) layer for neural
    sentiment analysis. As a comprehensive model, We can capture attentions of words
    in sentences as well as sentences in documents. Different attention mechanisms
    can also be incorporated to capture other aspects of attentions. Evaluations show
    the CBA based method outperforms the state-of-the-art local context based attention
    methods significantly. This brings insight to how cognition grounded data can
    be brought into NLP tasks.
  address: Copenhagen, Denmark
  author:
  - first: Yunfei
    full: Yunfei Long
    id: yunfei-long
    last: Long
  - first: Qin
    full: Qin Lu
    id: qin-lu
    last: Lu
  - first: Rong
    full: Rong Xiang
    id: rong-xiang
    last: Xiang
  - first: Minglei
    full: Minglei Li
    id: minglei-li
    last: Li
  - first: Chu-Ren
    full: Chu-Ren Huang
    id: chu-ren-huang
    last: Huang
  author_string: Yunfei Long, Qin Lu, Rong Xiang, Minglei Li, Chu-Ren Huang
  bibkey: long-etal-2017-cognition
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1048
  month: September
  page_first: '462'
  page_last: '471'
  pages: "462\u2013471"
  paper_id: '48'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1048.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1048.jpg
  title: A Cognition Based Attention Model for Sentiment Analysis
  title_html: A Cognition Based Attention Model for Sentiment Analysis
  url: https://www.aclweb.org/anthology/D17-1048
  year: '2017'
D17-1049:
  abstract: User generated content about products and services in the form of reviews
    are often diverse and even contradictory. This makes it difficult for users to
    know if an opinion in a review is prevalent or biased. We study the problem of
    searching for supporting opinions in the context of reviews. We propose a framework
    called SURF, that first identifies opinions expressed in a review, and then finds
    similar opinions from other reviews. We design a novel probabilistic graphical
    model that captures opinions as a combination of aspect, topic and sentiment dimensions,
    takes into account the preferences of individual authors, as well as the quality
    of the entity under review, and encodes the flow of thoughts in a review by constraining
    the aspect distribution dynamically among successive review segments. We derive
    a similarity measure that considers both lexical and semantic similarity to find
    supporting opinions. Experiments on TripAdvisor hotel reviews and Yelp restaurant
    reviews show that our model outperforms existing methods for modeling opinions,
    and the proposed framework is effective in finding supporting opinions.
  address: Copenhagen, Denmark
  author:
  - first: Lahari
    full: Lahari Poddar
    id: lahari-poddar
    last: Poddar
  - first: Wynne
    full: Wynne Hsu
    id: wynne-hsu
    last: Hsu
  - first: Mong Li
    full: Mong Li Lee
    id: mong-li-lee
    last: Lee
  author_string: Lahari Poddar, Wynne Hsu, Mong Li Lee
  bibkey: poddar-etal-2017-author
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1049
  month: September
  page_first: '472'
  page_last: '481'
  pages: "472\u2013481"
  paper_id: '49'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1049.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1049.jpg
  title: Author-aware Aspect Topic Sentiment Model to Retrieve Supporting Opinions
    from Reviews
  title_html: Author-aware Aspect Topic Sentiment Model to Retrieve Supporting Opinions
    from Reviews
  url: https://www.aclweb.org/anthology/D17-1049
  year: '2017'
D17-1050:
  abstract: "Sarcasm is a pervasive phenomenon in social media, permitting the concise\
    \ communication of meaning, affect and attitude. Concision requires wit to produce\
    \ and wit to understand, which demands from each party knowledge of norms, context\
    \ and a speaker\u2019s mindset. Insight into a speaker\u2019s psychological profile\
    \ at the time of production is a valuable source of context for sarcasm detection.\
    \ Using a neural architecture, we show significant gains in detection accuracy\
    \ when knowledge of the speaker\u2019s mood at the time of production can be inferred.\
    \ Our focus is on sarcasm detection on Twitter, and show that the mood exhibited\
    \ by a speaker over tweets leading up to a new post is as useful a cue for sarcasm\
    \ as the topical context of the post itself. The work opens the door to an empirical\
    \ exploration not just of sarcasm in text but of the sarcastic state of mind."
  address: Copenhagen, Denmark
  author:
  - first: Aniruddha
    full: Aniruddha Ghosh
    id: aniruddha-ghosh
    last: Ghosh
  - first: Tony
    full: Tony Veale
    id: tony-veale
    last: Veale
  author_string: Aniruddha Ghosh, Tony Veale
  bibkey: ghosh-veale-2017-magnets
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1050
  month: September
  page_first: '482'
  page_last: '491'
  pages: "482\u2013491"
  paper_id: '50'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1050.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1050.jpg
  title: 'Magnets for Sarcasm: Making Sarcasm Detection Timely, Contextual and Very
    Personal'
  title_html: 'Magnets for Sarcasm: Making Sarcasm Detection Timely, Contextual and
    Very Personal'
  url: https://www.aclweb.org/anthology/D17-1050
  year: '2017'
D17-1051:
  abstract: We study the problem of automatically identifying humorous text from a
    new kind of text data, i.e., online reviews. We propose a generative language
    model, based on the theory of incongruity, to model humorous text, which allows
    us to leverage background text sources, such as Wikipedia entry descriptions,
    and enables construction of multiple features for identifying humorous reviews.
    Evaluation of these features using supervised learning for classifying reviews
    into humorous and non-humorous reviews shows that the features constructed based
    on the proposed generative model are much more effective than the major features
    proposed in the existing literature, allowing us to achieve almost 86% accuracy.
    These humorous review predictions can also supply good indicators for identifying
    helpful reviews.
  address: Copenhagen, Denmark
  author:
  - first: Alex
    full: Alex Morales
    id: alex-morales
    last: Morales
  - first: Chengxiang
    full: Chengxiang Zhai
    id: chengxiang-zhai
    last: Zhai
  author_string: Alex Morales, Chengxiang Zhai
  bibkey: morales-zhai-2017-identifying
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1051
  month: September
  page_first: '492'
  page_last: '501'
  pages: "492\u2013501"
  paper_id: '51'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1051.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1051.jpg
  title: Identifying Humor in Reviews using Background Text Sources
  title_html: Identifying Humor in Reviews using Background Text Sources
  url: https://www.aclweb.org/anthology/D17-1051
  year: '2017'
D17-1052:
  abstract: Sentiment lexicon is an important tool for identifying the sentiment polarity
    of words and texts. How to automatically construct sentiment lexicons has become
    a research topic in the field of sentiment analysis and opinion mining. Recently
    there were some attempts to employ representation learning algorithms to construct
    a sentiment lexicon with sentiment-aware word embedding. However, these methods
    were normally trained under document-level sentiment supervision. In this paper,
    we develop a neural architecture to train a sentiment-aware word embedding by
    integrating the sentiment supervision at both document and word levels, to enhance
    the quality of word embedding as well as the sentiment lexicon. Experiments on
    the SemEval 2013-2016 datasets indicate that the sentiment lexicon generated by
    our approach achieves the state-of-the-art performance in both supervised and
    unsupervised sentiment classification, in comparison with several strong sentiment
    lexicon construction methods.
  address: Copenhagen, Denmark
  author:
  - first: Leyi
    full: Leyi Wang
    id: leyi-wang
    last: Wang
  - first: Rui
    full: Rui Xia
    id: rui-xia
    last: Xia
  author_string: Leyi Wang, Rui Xia
  bibkey: wang-xia-2017-sentiment
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1052
  month: September
  page_first: '502'
  page_last: '510'
  pages: "502\u2013510"
  paper_id: '52'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1052.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1052.jpg
  title: Sentiment Lexicon Construction with Representation Learning Based on Hierarchical
    Sentiment Supervision
  title_html: Sentiment Lexicon Construction with Representation Learning Based on
    Hierarchical Sentiment Supervision
  url: https://www.aclweb.org/anthology/D17-1052
  year: '2017'
D17-1053:
  abstract: Existing sentiment classifiers usually work for only one specific language,
    and different classification models are used in different languages. In this paper
    we aim to build a universal sentiment classifier with a single classification
    model in multiple different languages. In order to achieve this goal, we propose
    to learn multilingual sentiment-aware word embeddings simultaneously based only
    on the labeled reviews in English and unlabeled parallel data available in a few
    language pairs. It is not required that the parallel data exist between English
    and any other language, because the sentiment information can be transferred into
    any language via pivot languages. We present the evaluation results of our universal
    sentiment classifier in five languages, and the results are very promising even
    when the parallel data between English and the target languages are not used.
    Furthermore, the universal single classifier is compared with a few cross-language
    sentiment classifiers relying on direct parallel data between the source and target
    languages, and the results show that the performance of our universal sentiment
    classifier is very promising compared to that of different cross-language classifiers
    in multiple target languages.
  address: Copenhagen, Denmark
  author:
  - first: Kui
    full: Kui Xu
    id: kui-xu
    last: Xu
  - first: Xiaojun
    full: Xiaojun Wan
    id: xiaojun-wan
    last: Wan
  author_string: Kui Xu, Xiaojun Wan
  bibkey: xu-wan-2017-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1053
  month: September
  page_first: '511'
  page_last: '520'
  pages: "511\u2013520"
  paper_id: '53'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1053.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1053.jpg
  title: Towards a Universal Sentiment Classifier in Multiple languages
  title_html: Towards a Universal Sentiment Classifier in Multiple languages
  url: https://www.aclweb.org/anthology/D17-1053
  year: '2017'
D17-1054:
  abstract: "Document-level sentiment classification is a fundamental problem which\
    \ aims to predict a user\u2019s overall sentiment about a product in a document.\
    \ Several methods have been proposed to tackle the problem whereas most of them\
    \ fail to consider the influence of users who express the sentiment and products\
    \ which are evaluated. To address the issue, we propose a deep memory network\
    \ for document-level sentiment classification which could capture the user and\
    \ product information at the same time. To prove the effectiveness of our algorithm,\
    \ we conduct experiments on IMDB and Yelp datasets and the results indicate that\
    \ our model can achieve better performance than several existing methods."
  address: Copenhagen, Denmark
  author:
  - first: Zi-Yi
    full: Zi-Yi Dou
    id: zi-yi-dou
    last: Dou
  author_string: Zi-Yi Dou
  bibkey: dou-2017-capturing
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1054
  month: September
  page_first: '521'
  page_last: '526'
  pages: "521\u2013526"
  paper_id: '54'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1054.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1054.jpg
  title: Capturing User and Product Information for Document Level Sentiment Analysis
    with Deep Memory Network
  title_html: Capturing User and Product Information for Document Level Sentiment
    Analysis with Deep Memory Network
  url: https://www.aclweb.org/anthology/D17-1054
  year: '2017'
D17-1055:
  abstract: We study the problem of identifying the topics and sentiments and tracking
    their shifts from social media texts in different geographical regions during
    emergencies and disasters. We propose a location-based dynamic sentiment-topic
    model (LDST) which can jointly model topic, sentiment, time and Geolocation information.
    The experimental results demonstrate that LDST performs very well at discovering
    topics and sentiments from social media and tracking their shifts in different
    geographical regions during emergencies and disasters. We will release the data
    and source code after this work is published.
  address: Copenhagen, Denmark
  author:
  - first: Min
    full: Min Yang
    id: min-yang
    last: Yang
  - first: Jincheng
    full: Jincheng Mei
    id: jincheng-mei
    last: Mei
  - first: Heng
    full: Heng Ji
    id: heng-ji
    last: Ji
  - first: Wei
    full: Wei Zhao
    id: wei-zhao
    last: Zhao
  - first: Zhou
    full: Zhou Zhao
    id: zhou-zhao
    last: Zhao
  - first: Xiaojun
    full: Xiaojun Chen
    id: xiaojun-chen
    last: Chen
  author_string: Min Yang, Jincheng Mei, Heng Ji, Wei Zhao, Zhou Zhao, Xiaojun Chen
  bibkey: yang-etal-2017-identifying
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1055
  month: September
  page_first: '527'
  page_last: '533'
  pages: "527\u2013533"
  paper_id: '55'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1055.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1055.jpg
  title: Identifying and Tracking Sentiments and Topics from Social Media Texts during
    Natural Disasters
  title_html: Identifying and Tracking Sentiments and Topics from Social Media Texts
    during Natural Disasters
  url: https://www.aclweb.org/anthology/D17-1055
  year: '2017'
D17-1056:
  abstract: Word embeddings that can capture semantic and syntactic information from
    contexts have been extensively used for various natural language processing tasks.
    However, existing methods for learning context-based word embeddings typically
    fail to capture sufficient sentiment information. This may result in words with
    similar vector representations having an opposite sentiment polarity (e.g., good
    and bad), thus degrading sentiment analysis performance. Therefore, this study
    proposes a word vector refinement model that can be applied to any pre-trained
    word vectors (e.g., Word2vec and GloVe). The refinement model is based on adjusting
    the vector representations of words such that they can be closer to both semantically
    and sentimentally similar words and further away from sentimentally dissimilar
    words. Experimental results show that the proposed method can improve conventional
    word embeddings and outperform previously proposed sentiment embeddings for both
    binary and fine-grained classification on Stanford Sentiment Treebank (SST).
  address: Copenhagen, Denmark
  author:
  - first: Liang-Chih
    full: Liang-Chih Yu
    id: liang-chih-yu
    last: Yu
  - first: Jin
    full: Jin Wang
    id: jin-wang
    last: Wang
  - first: K. Robert
    full: K. Robert Lai
    id: k-robert-lai
    last: Lai
  - first: Xuejie
    full: Xuejie Zhang
    id: xuejie-zhang
    last: Zhang
  author_string: Liang-Chih Yu, Jin Wang, K. Robert Lai, Xuejie Zhang
  bibkey: yu-etal-2017-refining
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1056
  month: September
  page_first: '534'
  page_last: '539'
  pages: "534\u2013539"
  paper_id: '56'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1056.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1056.jpg
  title: Refining Word Embeddings for Sentiment Analysis
  title_html: Refining Word Embeddings for Sentiment Analysis
  url: https://www.aclweb.org/anthology/D17-1056
  year: '2017'
D17-1057:
  abstract: In this paper, we propose a novel method for combining deep learning and
    classical feature based models using a Multi-Layer Perceptron (MLP) network for
    financial sentiment analysis. We develop various deep learning models based on
    Convolutional Neural Network (CNN), Long Short Term Memory (LSTM) and Gated Recurrent
    Unit (GRU). These are trained on top of pre-trained, autoencoder-based, financial
    word embeddings and lexicon features. An ensemble is constructed by combining
    these deep learning models and a classical supervised model based on Support Vector
    Regression (SVR). We evaluate our proposed technique on a benchmark dataset of
    SemEval-2017 shared task on financial sentiment analysis. The propose model shows
    impressive results on two datasets, i.e. microblogs and news headlines datasets.
    Comparisons show that our proposed model performs better than the existing state-of-the-art
    systems for the above two datasets by 2.0 and 4.1 cosine points, respectively.
  address: Copenhagen, Denmark
  author:
  - first: Md Shad
    full: Md Shad Akhtar
    id: md-shad-akhtar
    last: Akhtar
  - first: Abhishek
    full: Abhishek Kumar
    id: abhishek-kumar
    last: Kumar
  - first: Deepanway
    full: Deepanway Ghosal
    id: deepanway-ghosal
    last: Ghosal
  - first: Asif
    full: Asif Ekbal
    id: asif-ekbal
    last: Ekbal
  - first: Pushpak
    full: Pushpak Bhattacharyya
    id: pushpak-bhattacharyya
    last: Bhattacharyya
  author_string: Md Shad Akhtar, Abhishek Kumar, Deepanway Ghosal, Asif Ekbal, Pushpak
    Bhattacharyya
  bibkey: akhtar-etal-2017-multilayer
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1057
  month: September
  page_first: '540'
  page_last: '546'
  pages: "540\u2013546"
  paper_id: '57'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1057.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1057.jpg
  title: A Multilayer Perceptron based Ensemble Technique for Fine-grained Financial
    Sentiment Analysis
  title_html: A Multilayer Perceptron based Ensemble Technique for Fine-grained Financial
    Sentiment Analysis
  url: https://www.aclweb.org/anthology/D17-1057
  year: '2017'
D17-1058:
  abstract: "Identification of intensity ordering among polar (positive or negative)\
    \ words which have the same semantics can lead to a fine-grained sentiment analysis.\
    \ For example, \u2018master\u2019, \u2018seasoned\u2019 and \u2018familiar\u2019\
    \ point to different intensity levels, though they all convey the same meaning\
    \ (semantics), i.e., expertise: having a good knowledge of. In this paper, we\
    \ propose a semi-supervised technique that uses sentiment bearing word embeddings\
    \ to produce a continuous ranking among adjectives that share common semantics.\
    \ Our system demonstrates a strong Spearman\u2019s rank correlation of 0.83 with\
    \ the gold standard ranking. We show that sentiment bearing word embeddings facilitate\
    \ a more accurate intensity ranking system than other standard word embeddings\
    \ (word2vec and GloVe). Word2vec is the state-of-the-art for intensity ordering\
    \ task."
  address: Copenhagen, Denmark
  author:
  - first: Raksha
    full: Raksha Sharma
    id: raksha-sharma
    last: Sharma
  - first: Arpan
    full: Arpan Somani
    id: arpan-somani
    last: Somani
  - first: Lakshya
    full: Lakshya Kumar
    id: lakshya-kumar
    last: Kumar
  - first: Pushpak
    full: Pushpak Bhattacharyya
    id: pushpak-bhattacharyya
    last: Bhattacharyya
  author_string: Raksha Sharma, Arpan Somani, Lakshya Kumar, Pushpak Bhattacharyya
  bibkey: sharma-etal-2017-sentiment
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1058
  month: September
  page_first: '547'
  page_last: '552'
  pages: "547\u2013552"
  paper_id: '58'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1058.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1058.jpg
  title: Sentiment Intensity Ranking among Adjectives Using Sentiment Bearing Word
    Embeddings
  title_html: Sentiment Intensity Ranking among Adjectives Using Sentiment Bearing
    Word Embeddings
  url: https://www.aclweb.org/anthology/D17-1058
  year: '2017'
D17-1059:
  abstract: Although many sentiment lexicons in different languages exist, most are
    not comprehensive. In a recent sentiment analysis application, we used a large
    Chinese sentiment lexicon and found that it missed a large number of sentiment
    words in social media. This prompted us to make a new attempt to study sentiment
    lexicon expansion. This paper first poses the problem as a PU learning problem,
    which is a new formulation. It then proposes a new PU learning method suitable
    for our problem using a neural network. The results are enhanced further with
    a new dictionary-based technique and a novel polarity classification technique.
    Experimental results show that the proposed approach outperforms baseline methods
    greatly.
  address: Copenhagen, Denmark
  author:
  - first: Yasheng
    full: Yasheng Wang
    id: yasheng-wang
    last: Wang
  - first: Yang
    full: Yang Zhang
    id: yang-zhang
    last: Zhang
  - first: Bing
    full: Bing Liu
    id: bing-liu
    last: Liu
  author_string: Yasheng Wang, Yang Zhang, Bing Liu
  bibkey: wang-etal-2017-sentiment
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1059
  month: September
  page_first: '553'
  page_last: '563'
  pages: "553\u2013563"
  paper_id: '59'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1059.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1059.jpg
  title: Sentiment Lexicon Expansion Based on Neural PU Learning, Double Dictionary
    Lookup, and Polarity Association
  title_html: Sentiment Lexicon Expansion Based on Neural <span class="acl-fixed-case">PU</span>
    Learning, Double Dictionary Lookup, and Polarity Association
  url: https://www.aclweb.org/anthology/D17-1059
  year: '2017'
D17-1060:
  abstract: 'We study the problem of learning to reason in large scale knowledge graphs
    (KGs). More specifically, we describe a novel reinforcement learning framework
    for learning multi-hop relational paths: we use a policy-based agent with continuous
    states based on knowledge graph embeddings, which reasons in a KG vector-space
    by sampling the most promising relation to extend its path. In contrast to prior
    work, our approach includes a reward function that takes the accuracy, diversity,
    and efficiency into consideration. Experimentally, we show that our proposed method
    outperforms a path-ranking based algorithm and knowledge graph embedding methods
    on Freebase and Never-Ending Language Learning datasets.'
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238232302
    type: video
    url: https://vimeo.com/238232302
  author:
  - first: Wenhan
    full: Wenhan Xiong
    id: wenhan-xiong
    last: Xiong
  - first: Thien
    full: Thien Hoang
    id: thien-hoang
    last: Hoang
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  author_string: Wenhan Xiong, Thien Hoang, William Yang Wang
  bibkey: xiong-etal-2017-deeppath
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1060
  month: September
  page_first: '564'
  page_last: '573'
  pages: "564\u2013573"
  paper_id: '60'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1060.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1060.jpg
  title: 'DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning'
  title_html: '<span class="acl-fixed-case">D</span>eep<span class="acl-fixed-case">P</span>ath:
    A Reinforcement Learning Method for Knowledge Graph Reasoning'
  url: https://www.aclweb.org/anthology/D17-1060
  year: '2017'
D17-1061:
  abstract: Search engines play an important role in our everyday lives by assisting
    us in finding the information we need. When we input a complex query, however,
    results are often far from satisfactory. In this work, we introduce a query reformulation
    system based on a neural network that rewrites a query to maximize the number
    of relevant documents returned. We train this neural network with reinforcement
    learning. The actions correspond to selecting terms to build a reformulated query,
    and the reward is the document recall. We evaluate our approach on three datasets
    against strong baselines and show a relative improvement of 5-20% in terms of
    recall. Furthermore, we present a simple method to estimate a conservative upper-bound
    performance of a model in a particular environment and verify that there is still
    large room for improvements.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238236064
    type: video
    url: https://vimeo.com/238236064
  author:
  - first: Rodrigo
    full: Rodrigo Nogueira
    id: rodrigo-nogueira
    last: Nogueira
  - first: Kyunghyun
    full: Kyunghyun Cho
    id: kyunghyun-cho
    last: Cho
  author_string: Rodrigo Nogueira, Kyunghyun Cho
  bibkey: nogueira-cho-2017-task
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1061
  month: September
  page_first: '574'
  page_last: '583'
  pages: "574\u2013583"
  paper_id: '61'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1061.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1061.jpg
  title: Task-Oriented Query Reformulation with Reinforcement Learning
  title_html: Task-Oriented Query Reformulation with Reinforcement Learning
  url: https://www.aclweb.org/anthology/D17-1061
  year: '2017'
D17-1062:
  abstract: Sentence simplification aims to make sentences easier to read and understand.
    Most recent approaches draw on insights from machine translation to learn simplification
    rewrites from monolingual corpora of complex and simple sentences. We address
    the simplification problem with an encoder-decoder model coupled with a deep reinforcement
    learning framework. Our model, which we call DRESS (as shorthand for Deep REinforcement
    Sentence Simplification), explores the space of possible simplifications while
    learning to optimize a reward function that encourages outputs which are simple,
    fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate
    that our model outperforms competitive simplification systems.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238235599
    type: video
    url: https://vimeo.com/238235599
  author:
  - first: Xingxing
    full: Xingxing Zhang
    id: xingxing-zhang
    last: Zhang
  - first: Mirella
    full: Mirella Lapata
    id: mirella-lapata
    last: Lapata
  author_string: Xingxing Zhang, Mirella Lapata
  bibkey: zhang-lapata-2017-sentence
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1062
  month: September
  page_first: '584'
  page_last: '594'
  pages: "584\u2013594"
  paper_id: '62'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1062.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1062.jpg
  title: Sentence Simplification with Deep Reinforcement Learning
  title_html: Sentence Simplification with Deep Reinforcement Learning
  url: https://www.aclweb.org/anthology/D17-1062
  year: '2017'
D17-1063:
  abstract: Active learning aims to select a small subset of data for annotation such
    that a classifier learned on the data is highly accurate. This is usually done
    using heuristic selection methods, however the effectiveness of such methods is
    limited and moreover, the performance of heuristics varies between datasets. To
    address these shortcomings, we introduce a novel formulation by reframing the
    active learning as a reinforcement learning problem and explicitly learning a
    data selection policy, where the policy takes the role of the active learning
    heuristic. Importantly, our method allows the selection policy learned using simulation
    to one language to be transferred to other languages. We demonstrate our method
    using cross-lingual named entity recognition, observing uniform improvements over
    traditional active learning algorithms.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238234005
    type: video
    url: https://vimeo.com/238234005
  author:
  - first: Meng
    full: Meng Fang
    id: meng-fang
    last: Fang
  - first: Yuan
    full: Yuan Li
    id: yuan-li
    last: Li
  - first: Trevor
    full: Trevor Cohn
    id: trevor-cohn
    last: Cohn
  author_string: Meng Fang, Yuan Li, Trevor Cohn
  bibkey: fang-etal-2017-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1063
  month: September
  page_first: '595'
  page_last: '605'
  pages: "595\u2013605"
  paper_id: '63'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1063.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1063.jpg
  title: 'Learning how to Active Learn: A Deep Reinforcement Learning Approach'
  title_html: 'Learning how to Active Learn: A Deep Reinforcement Learning Approach'
  url: https://www.aclweb.org/anthology/D17-1063
  year: '2017'
D17-1064:
  abstract: We propose a new sentence simplification task (Split-and-Rephrase) where
    the aim is to split a complex sentence into a meaning preserving sequence of shorter
    sentences. Like sentence simplification, splitting-and-rephrasing has the potential
    of benefiting both natural language processing and societal applications. Because
    shorter sentences are generally better processed by NLP systems, it could be used
    as a preprocessing step which facilitates and improves the performance of parsers,
    semantic role labellers and machine translation systems. It should also be of
    use for people with reading disabilities because it allows the conversion of longer
    sentences into shorter ones. This paper makes two contributions towards this new
    task. First, we create and make available a benchmark consisting of 1,066,115
    tuples mapping a single complex sentence to a sequence of sentences expressing
    the same meaning. Second, we propose five models (vanilla sequence-to-sequence
    to semantically-motivated models) to understand the difficulty of the proposed
    task.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238230265
    type: video
    url: https://vimeo.com/238230265
  author:
  - first: Shashi
    full: Shashi Narayan
    id: shashi-narayan
    last: Narayan
  - first: Claire
    full: Claire Gardent
    id: claire-gardent
    last: Gardent
  - first: Shay B.
    full: Shay B. Cohen
    id: shay-b-cohen
    last: Cohen
  - first: Anastasia
    full: Anastasia Shimorina
    id: anastasia-shimorina
    last: Shimorina
  author_string: Shashi Narayan, Claire Gardent, Shay B. Cohen, Anastasia Shimorina
  bibkey: narayan-etal-2017-split
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1064
  month: September
  page_first: '606'
  page_last: '616'
  pages: "606\u2013616"
  paper_id: '64'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1064.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1064.jpg
  title: Split and Rephrase
  title_html: Split and Rephrase
  url: https://www.aclweb.org/anthology/D17-1064
  year: '2017'
D17-1065:
  abstract: "This paper presents a Generative Adversarial Network (GAN) to model single-turn\
    \ short-text conversations, which trains a sequence-to-sequence (Seq2Seq) network\
    \ for response generation simultaneously with a discriminative classifier that\
    \ measures the differences between human-produced responses and machine-generated\
    \ ones. In addition, the proposed method introduces an approximate embedding layer\
    \ to solve the non-differentiable problem caused by the sampling-based output\
    \ decoding procedure in the Seq2Seq generative model. The GAN setup provides an\
    \ effective way to avoid noninformative responses (a.k.a \u201Csafe responses\u201D\
    ), which are frequently observed in traditional neural response generators. The\
    \ experimental results show that the proposed approach significantly outperforms\
    \ existing neural response generation models in diversity metrics, with slight\
    \ increases in relevance scores as well, when evaluated on both a Mandarin corpus\
    \ and an English corpus."
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238230011
    type: video
    url: https://vimeo.com/238230011
  author:
  - first: Zhen
    full: Zhen Xu
    id: zhen-xu
    last: Xu
  - first: Bingquan
    full: Bingquan Liu
    id: bingquan-liu
    last: Liu
  - first: Baoxun
    full: Baoxun Wang
    id: baoxun-wang
    last: Wang
  - first: Chengjie
    full: Chengjie Sun
    id: cheng-jie-sun
    last: Sun
  - first: Xiaolong
    full: Xiaolong Wang
    id: xiaolong-wang
    last: Wang
  - first: Zhuoran
    full: Zhuoran Wang
    id: zhuoran-wang
    last: Wang
  - first: Chao
    full: Chao Qi
    id: chao-qi
    last: Qi
  author_string: Zhen Xu, Bingquan Liu, Baoxun Wang, Chengjie Sun, Xiaolong Wang,
    Zhuoran Wang, Chao Qi
  bibkey: xu-etal-2017-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1065
  month: September
  page_first: '617'
  page_last: '626'
  pages: "617\u2013626"
  paper_id: '65'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1065.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1065.jpg
  title: Neural Response Generation via GAN with an Approximate Embedding Layer
  title_html: Neural Response Generation via <span class="acl-fixed-case">GAN</span>
    with an Approximate Embedding Layer
  url: https://www.aclweb.org/anthology/D17-1065
  year: '2017'
D17-1066:
  abstract: In this paper we explore the effect of architectural choices on learning
    a variational autoencoder (VAE) for text generation. In contrast to the previously
    introduced VAE model for text where both the encoder and decoder are RNNs, we
    propose a novel hybrid architecture that blends fully feed-forward convolutional
    and deconvolutional components with a recurrent language model. Our architecture
    exhibits several attractive properties such as faster run time and convergence,
    ability to better handle long sequences and, more importantly, it helps to avoid
    the issue of the VAE collapsing to a deterministic model.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1066.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1066.Attachment.zip
  - filename: https://vimeo.com/238230365
    type: video
    url: https://vimeo.com/238230365
  author:
  - first: Stanislau
    full: Stanislau Semeniuta
    id: stanislau-semeniuta
    last: Semeniuta
  - first: Aliaksei
    full: Aliaksei Severyn
    id: aliaksei-severyn
    last: Severyn
  - first: Erhardt
    full: Erhardt Barth
    id: erhardt-barth
    last: Barth
  author_string: Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth
  bibkey: semeniuta-etal-2017-hybrid
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1066
  month: September
  page_first: '627'
  page_last: '637'
  pages: "627\u2013637"
  paper_id: '66'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1066.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1066.jpg
  title: A Hybrid Convolutional Variational Autoencoder for Text Generation
  title_html: A Hybrid Convolutional Variational Autoencoder for Text Generation
  url: https://www.aclweb.org/anthology/D17-1066
  year: '2017'
D17-1067:
  abstract: Computerized generation of humor is a notoriously difficult AI problem.
    We develop an algorithm called Libitum that helps humans generate humor in a Mad
    Lib, which is a popular fill-in-the-blank game. The algorithm is based on a machine
    learned classifier that determines whether a potential fill-in word is funny in
    the context of the Mad Lib story. We use Amazon Mechanical Turk to create ground
    truth data and to judge humor for our classifier to mimic, and we make this data
    freely available. Our testing shows that Libitum successfully aids humans in filling
    in Mad Libs that are usually judged funnier than those filled in by humans with
    no computerized help. We go on to analyze why some words are better than others
    at making a Mad Lib funny.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238228773
    type: video
    url: https://vimeo.com/238228773
  author:
  - first: Nabil
    full: Nabil Hossain
    id: nabil-hossain
    last: Hossain
  - first: John
    full: John Krumm
    id: john-krumm
    last: Krumm
  - first: Lucy
    full: Lucy Vanderwende
    id: lucy-vanderwende
    last: Vanderwende
  - first: Eric
    full: Eric Horvitz
    id: eric-horvitz
    last: Horvitz
  - first: Henry
    full: Henry Kautz
    id: henry-kautz
    last: Kautz
  author_string: Nabil Hossain, John Krumm, Lucy Vanderwende, Eric Horvitz, Henry
    Kautz
  bibkey: hossain-etal-2017-filling
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1067
  month: September
  page_first: '638'
  page_last: '647'
  pages: "638\u2013647"
  paper_id: '67'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1067.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1067.jpg
  title: 'Filling the Blanks (hint: plural noun) for Mad Libs Humor'
  title_html: 'Filling the Blanks (hint: plural noun) for Mad <span class="acl-fixed-case">L</span>ibs
    Humor'
  url: https://www.aclweb.org/anthology/D17-1067
  year: '2017'
D17-1068:
  abstract: 'In this paper, we introduce a new distributional method for modeling
    predicate-argument thematic fit judgments. We use a syntax-based DSM to build
    a prototypical representation of verb-specific roles: for every verb, we extract
    the most salient second order contexts for each of its roles (i.e. the most salient
    dimensions of typical role fillers), and then we compute thematic fit as a weighted
    overlap between the top features of candidate fillers and role prototypes. Our
    experiments show that our method consistently outperforms a baseline re-implementing
    a state-of-the-art system, and achieves better or comparable results to those
    reported in the literature for the other unsupervised systems. Moreover, it provides
    an explicit representation of the features characterizing verb-specific semantic
    roles.'
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238234243
    type: video
    url: https://vimeo.com/238234243
  author:
  - first: Enrico
    full: Enrico Santus
    id: enrico-santus
    last: Santus
  - first: Emmanuele
    full: Emmanuele Chersoni
    id: emmanuele-chersoni
    last: Chersoni
  - first: Alessandro
    full: Alessandro Lenci
    id: alessandro-lenci
    last: Lenci
  - first: Philippe
    full: Philippe Blache
    id: philippe-blache
    last: Blache
  author_string: Enrico Santus, Emmanuele Chersoni, Alessandro Lenci, Philippe Blache
  bibkey: santus-etal-2017-measuring
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1068
  month: September
  page_first: '648'
  page_last: '658'
  pages: "648\u2013658"
  paper_id: '68'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1068.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1068.jpg
  title: Measuring Thematic Fit with Distributional Feature Overlap
  title_html: Measuring Thematic Fit with Distributional Feature Overlap
  url: https://www.aclweb.org/anthology/D17-1068
  year: '2017'
D17-1069:
  abstract: We present a feature vector formation technique for documents - Sparse
    Composite Document Vector (SCDV) - which overcomes several shortcomings of the
    current distributional paragraph vector representations that are widely used for
    text representation. In SCDV, word embeddings are clustered to capture multiple
    semantic contexts in which words occur. They are then chained together to form
    document topic-vectors that can express complex, multi-topic documents. Through
    extensive experiments on multi-class and multi-label classification tasks, we
    outperform the previous state-of-the-art method, NTSG. We also show that SCDV
    embeddings perform well on heterogeneous tasks like Topic Coherence, context-sensitive
    Learning and Information Retrieval. Moreover, we achieve a significant reduction
    in training and prediction times compared to other representation methods. SCDV
    achieves best of both worlds - better performance with lower time and space complexity.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238235553
    type: video
    url: https://vimeo.com/238235553
  author:
  - first: Dheeraj
    full: Dheeraj Mekala
    id: dheeraj-mekala
    last: Mekala
  - first: Vivek
    full: Vivek Gupta
    id: vivek-gupta
    last: Gupta
  - first: Bhargavi
    full: Bhargavi Paranjape
    id: bhargavi-paranjape
    last: Paranjape
  - first: Harish
    full: Harish Karnick
    id: harish-karnick
    last: Karnick
  author_string: Dheeraj Mekala, Vivek Gupta, Bhargavi Paranjape, Harish Karnick
  bibkey: mekala-etal-2017-scdv
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1069
  month: September
  page_first: '659'
  page_last: '669'
  pages: "659\u2013669"
  paper_id: '69'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1069.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1069.jpg
  title: 'SCDV : Sparse Composite Document Vectors using soft clustering over distributional
    representations'
  title_html: '<span class="acl-fixed-case">SCDV</span> : Sparse Composite Document
    Vectors using soft clustering over distributional representations'
  url: https://www.aclweb.org/anthology/D17-1069
  year: '2017'
D17-1070:
  abstract: Many modern NLP systems rely on word embeddings, previously trained in
    an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings
    for larger chunks of text, such as sentences, have however not been so successful.
    Several attempts at learning unsupervised representations of sentences have not
    reached satisfactory enough performance to be widely adopted. In this paper, we
    show how universal sentence representations trained using the supervised data
    of the Stanford Natural Language Inference datasets can consistently outperform
    unsupervised methods like SkipThought vectors on a wide range of transfer tasks.
    Much like how computer vision uses ImageNet to obtain features, which can then
    be transferred to other tasks, our work tends to indicate the suitability of natural
    language inference for transfer learning to other NLP tasks. Our encoder is publicly
    available.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238236002
    type: video
    url: https://vimeo.com/238236002
  author:
  - first: Alexis
    full: Alexis Conneau
    id: alexis-conneau
    last: Conneau
  - first: Douwe
    full: Douwe Kiela
    id: douwe-kiela
    last: Kiela
  - first: Holger
    full: Holger Schwenk
    id: holger-schwenk
    last: Schwenk
  - first: "Lo\xEFc"
    full: "Lo\xEFc Barrault"
    id: loic-barrault
    last: Barrault
  - first: Antoine
    full: Antoine Bordes
    id: antoine-bordes
    last: Bordes
  author_string: "Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo\xEFc Barrault, Antoine\
    \ Bordes"
  bibkey: conneau-etal-2017-supervised
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1070
  month: September
  page_first: '670'
  page_last: '680'
  pages: "670\u2013680"
  paper_id: '70'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1070.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1070.jpg
  title: Supervised Learning of Universal Sentence Representations from Natural Language
    Inference Data
  title_html: Supervised Learning of Universal Sentence Representations from Natural
    Language Inference Data
  url: https://www.aclweb.org/anthology/D17-1070
  year: '2017'
D17-1071:
  abstract: Determining semantic textual similarity is a core research subject in
    natural language processing. Since vector-based models for sentence representation
    often use shallow information, capturing accurate semantics is difficult. By contrast,
    logical semantic representations capture deeper levels of sentence semantics,
    but their symbolic nature does not offer graded notions of textual similarity.
    We propose a method for determining semantic textual similarity by combining shallow
    features with features extracted from natural deduction proofs of bidirectional
    entailment relations between sentence pairs. For the natural deduction proofs,
    we use ccg2lambda, a higher-order automatic inference system, which converts Combinatory
    Categorial Grammar (CCG) derivation trees into semantic representations and conducts
    natural deduction proofs. Experiments show that our system was able to outperform
    other logic-based systems and that features derived from the proofs are effective
    for learning textual similarity.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238232412
    type: video
    url: https://vimeo.com/238232412
  author:
  - first: Hitomi
    full: Hitomi Yanaka
    id: hitomi-yanaka
    last: Yanaka
  - first: Koji
    full: Koji Mineshima
    id: koji-mineshima
    last: Mineshima
  - first: Pascual
    full: "Pascual Mart\xEDnez-G\xF3mez"
    id: pascual-martinez-gomez
    last: "Mart\xEDnez-G\xF3mez"
  - first: Daisuke
    full: Daisuke Bekki
    id: daisuke-bekki
    last: Bekki
  author_string: "Hitomi Yanaka, Koji Mineshima, Pascual Mart\xEDnez-G\xF3mez, Daisuke\
    \ Bekki"
  bibkey: yanaka-etal-2017-determining
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1071
  month: September
  page_first: '681'
  page_last: '691'
  pages: "681\u2013691"
  paper_id: '71'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1071.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1071.jpg
  title: Determining Semantic Textual Similarity using Natural Deduction Proofs
  title_html: Determining Semantic Textual Similarity using Natural Deduction Proofs
  url: https://www.aclweb.org/anthology/D17-1071
  year: '2017'
D17-1072:
  abstract: Traditionally, word segmentation (WS) adopts the single-grained formalism,
    where a sentence corresponds to a single word sequence. However, Sproat et al.
    (1997) show that the inter-native-speaker consistency ratio over Chinese word
    boundaries is only 76%, indicating single-grained WS (SWS) imposes unnecessary
    challenges on both manual annotation and statistical modeling. Moreover, WS results
    of different granularities can be complementary and beneficial for high-level
    applications. This work proposes and addresses multi-grained WS (MWS). We build
    a large-scale pseudo MWS dataset for model training and tuning by leveraging the
    annotation heterogeneity of three SWS datasets. Then we manually annotate 1,500
    test sentences with true MWS annotations. Finally, we propose three benchmark
    approaches by casting MWS as constituent parsing and sequence labeling. Experiments
    and analysis lead to many interesting findings.
  address: Copenhagen, Denmark
  author:
  - first: Chen
    full: Chen Gong
    id: chen-gong
    last: Gong
  - first: Zhenghua
    full: Zhenghua Li
    id: zhenghua-li
    last: Li
  - first: Min
    full: Min Zhang
    id: min-zhang
    last: Zhang
  - first: Xinzhou
    full: Xinzhou Jiang
    id: xinzhou-jiang
    last: Jiang
  author_string: Chen Gong, Zhenghua Li, Min Zhang, Xinzhou Jiang
  bibkey: gong-etal-2017-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1072
  month: September
  page_first: '692'
  page_last: '703'
  pages: "692\u2013703"
  paper_id: '72'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1072.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1072.jpg
  title: Multi-Grained Chinese Word Segmentation
  title_html: Multi-Grained <span class="acl-fixed-case">C</span>hinese Word Segmentation
  url: https://www.aclweb.org/anthology/D17-1072
  year: '2017'
D17-1073:
  abstract: This paper presents a model for Arabic morphological disambiguation based
    on Recurrent Neural Networks (RNN). We train Long Short-Term Memory (LSTM) cells
    in several configurations and embedding levels to model the various morphological
    features. Our experiments show that these models outperform state-of-the-art systems
    without explicit use of feature engineering. However, adding learning features
    from a morphological analyzer to model the space of possible analyses provides
    additional improvement. We make use of the resulting morphological models for
    scoring and ranking the analyses of the morphological analyzer for morphological
    disambiguation. The results show significant gains in accuracy across several
    evaluation metrics. Our system results in 4.4% absolute increase over the state-of-the-art
    in full morphological analysis accuracy (30.6% relative error reduction), and
    10.6% (31.5% relative error reduction) for out-of-vocabulary words.
  address: Copenhagen, Denmark
  author:
  - first: Nasser
    full: Nasser Zalmout
    id: nasser-zalmout
    last: Zalmout
  - first: Nizar
    full: Nizar Habash
    id: nizar-habash
    last: Habash
  author_string: Nasser Zalmout, Nizar Habash
  bibkey: zalmout-habash-2017-dont
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1073
  month: September
  page_first: '704'
  page_last: '713'
  pages: "704\u2013713"
  paper_id: '73'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1073.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1073.jpg
  title: "Don\u2019t Throw Those Morphological Analyzers Away Just Yet: Neural Morphological\
    \ Disambiguation for Arabic"
  title_html: "Don\u2019t Throw Those Morphological Analyzers Away Just Yet: Neural\
    \ Morphological Disambiguation for <span class=\"acl-fixed-case\">A</span>rabic"
  url: https://www.aclweb.org/anthology/D17-1073
  year: '2017'
D17-1074:
  abstract: The generation of complex derived word forms has been an overlooked problem
    in NLP; we fill this gap by applying neural sequence-to-sequence models to the
    task. We overview the theoretical motivation for a paradigmatic treatment of derivational
    morphology, and introduce the task of derivational paradigm completion as a parallel
    to inflectional paradigm completion. State-of-the-art neural models adapted from
    the inflection task are able to learn the range of derivation patterns, and outperform
    a non-neural baseline by 16.4%. However, due to semantic, historical, and lexical
    considerations involved in derivational morphology, future work will be needed
    to achieve performance parity with inflection-generating systems.
  address: Copenhagen, Denmark
  author:
  - first: Ryan
    full: Ryan Cotterell
    id: ryan-cotterell
    last: Cotterell
  - first: Ekaterina
    full: Ekaterina Vylomova
    id: ekaterina-vylomova
    last: Vylomova
  - first: Huda
    full: Huda Khayrallah
    id: huda-khayrallah
    last: Khayrallah
  - first: Christo
    full: Christo Kirov
    id: christo-kirov
    last: Kirov
  - first: David
    full: David Yarowsky
    id: david-yarowsky
    last: Yarowsky
  author_string: Ryan Cotterell, Ekaterina Vylomova, Huda Khayrallah, Christo Kirov,
    David Yarowsky
  bibkey: cotterell-etal-2017-paradigm
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1074
  month: September
  page_first: '714'
  page_last: '720'
  pages: "714\u2013720"
  paper_id: '74'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1074.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1074.jpg
  title: Paradigm Completion for Derivational Morphology
  title_html: Paradigm Completion for Derivational Morphology
  url: https://www.aclweb.org/anthology/D17-1074
  year: '2017'
D17-1075:
  abstract: We introduce a novel sub-character architecture that exploits a unique
    compositional structure of the Korean language. Our method decomposes each character
    into a small set of primitive phonetic units called jamo letters from which character-
    and word-level representations are induced. The jamo letters divulge syntactic
    and semantic information that is difficult to access with conventional character-level
    units. They greatly alleviate the data sparsity problem, reducing the observation
    space to 1.6% of the original while increasing accuracy in our experiments. We
    apply our architecture to dependency parsing and achieve dramatic improvement
    over strong lexical baselines.
  address: Copenhagen, Denmark
  author:
  - first: Karl
    full: Karl Stratos
    id: karl-stratos
    last: Stratos
  author_string: Karl Stratos
  bibkey: stratos-2017-sub
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1075
  month: September
  page_first: '721'
  page_last: '726'
  pages: "721\u2013726"
  paper_id: '75'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1075.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1075.jpg
  title: A Sub-Character Architecture for Korean Language Processing
  title_html: A Sub-Character Architecture for <span class="acl-fixed-case">K</span>orean
    Language Processing
  url: https://www.aclweb.org/anthology/D17-1075
  year: '2017'
D17-1076:
  abstract: A recent study by Plank et al. (2016) found that LSTM-based PoS taggers
    considerably improve over the current state-of-the-art when evaluated on the corpora
    of the Universal Dependencies project that use a coarse-grained tagset. We replicate
    this study using a fresh collection of 27 corpora of 21 languages that are annotated
    with fine-grained tagsets of varying size. Our replication confirms the result
    in general, and we additionally find that the advantage of LSTMs is even bigger
    for larger tagsets. However, we also find that for the very large tagsets of morphologically
    rich languages, hand-crafted morphological lexicons are still necessary to reach
    state-of-the-art performance.
  address: Copenhagen, Denmark
  author:
  - first: Tobias
    full: Tobias Horsmann
    id: tobias-horsmann
    last: Horsmann
  - first: Torsten
    full: Torsten Zesch
    id: torsten-zesch
    last: Zesch
  author_string: Tobias Horsmann, Torsten Zesch
  bibkey: horsmann-zesch-2017-lstms
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1076
  month: September
  page_first: '727'
  page_last: '736'
  pages: "727\u2013736"
  paper_id: '76'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1076.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1076.jpg
  title: "Do LSTMs really work so well for PoS tagging? \u2013 A replication study"
  title_html: "Do <span class=\"acl-fixed-case\">LSTM</span>s really work so well\
    \ for <span class=\"acl-fixed-case\">P</span>o<span class=\"acl-fixed-case\">S</span>\
    \ tagging? \u2013 A replication study"
  url: https://www.aclweb.org/anthology/D17-1076
  year: '2017'
D17-1077:
  abstract: 'We introduce the task of book structure labeling: segmenting and assigning
    a fixed category (such as Table of Contents, Preface, Index) to the document structure
    of printed books. We manually annotate the page-level structural categories for
    a large dataset totaling 294,816 pages in 1,055 books evenly sampled from 1750-1922,
    and present empirical results comparing the performance of several classes of
    models. The best-performing model, a bidirectional LSTM with rich features, achieves
    an overall accuracy of 95.8 and a class-balanced macro F-score of 71.4.'
  address: Copenhagen, Denmark
  author:
  - first: Lara
    full: Lara McConnaughey
    id: lara-mcconnaughey
    last: McConnaughey
  - first: Jennifer
    full: Jennifer Dai
    id: jennifer-dai
    last: Dai
  - first: David
    full: David Bamman
    id: david-bamman
    last: Bamman
  author_string: Lara McConnaughey, Jennifer Dai, David Bamman
  bibkey: mcconnaughey-etal-2017-labeled
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1077
  month: September
  page_first: '737'
  page_last: '747'
  pages: "737\u2013747"
  paper_id: '77'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1077.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1077.jpg
  title: The Labeled Segmentation of Printed Books
  title_html: The Labeled Segmentation of Printed Books
  url: https://www.aclweb.org/anthology/D17-1077
  year: '2017'
D17-1078:
  abstract: "Even for common NLP tasks, sufficient supervision is not available in\
    \ many languages \u2013 morphological tagging is no exception. In the work presented\
    \ here, we explore a transfer learning scheme, whereby we train character-level\
    \ recurrent neural taggers to predict morphological taggings for high-resource\
    \ languages and low-resource languages together. Learning joint character representations\
    \ among multiple related languages successfully enables knowledge transfer from\
    \ the high-resource languages to the low-resource ones."
  address: Copenhagen, Denmark
  author:
  - first: Ryan
    full: Ryan Cotterell
    id: ryan-cotterell
    last: Cotterell
  - first: Georg
    full: Georg Heigold
    id: georg-heigold
    last: Heigold
  author_string: Ryan Cotterell, Georg Heigold
  bibkey: cotterell-heigold-2017-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1078
  month: September
  page_first: '748'
  page_last: '759'
  pages: "748\u2013759"
  paper_id: '78'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1078.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1078.jpg
  title: Cross-lingual Character-Level Neural Morphological Tagging
  title_html: Cross-lingual Character-Level Neural Morphological Tagging
  url: https://www.aclweb.org/anthology/D17-1078
  year: '2017'
D17-1079:
  abstract: Neural parsers have benefited from automatically labeled data via dependency-context
    word embeddings. We investigate training character embeddings on a word-based
    context in a similar way, showing that the simple method improves state-of-the-art
    neural word segmentation models significantly, beating tri-training baselines
    for leveraging auto-segmented data.
  address: Copenhagen, Denmark
  author:
  - first: Hao
    full: Hao Zhou
    id: hao-zhou
    last: Zhou
  - first: Zhenting
    full: Zhenting Yu
    id: zhenting-yu
    last: Yu
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  - first: Shujian
    full: Shujian Huang
    id: shujian-huang
    last: Huang
  - first: Xinyu
    full: Xinyu Dai
    id: xinyu-dai
    last: Dai
  - first: Jiajun
    full: Jiajun Chen
    id: jiajun-chen
    last: Chen
  author_string: Hao Zhou, Zhenting Yu, Yue Zhang, Shujian Huang, Xinyu Dai, Jiajun
    Chen
  bibkey: zhou-etal-2017-word
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1079
  month: September
  page_first: '760'
  page_last: '766'
  pages: "760\u2013766"
  paper_id: '79'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1079.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1079.jpg
  title: Word-Context Character Embeddings for Chinese Word Segmentation
  title_html: Word-Context Character Embeddings for <span class="acl-fixed-case">C</span>hinese
    Word Segmentation
  url: https://www.aclweb.org/anthology/D17-1079
  year: '2017'
D17-1080:
  abstract: In this paper, we propose a new pipeline of word embedding for unsegmented
    languages, called segmentation-free word embedding, which does not require word
    segmentation as a preprocessing step. Unlike space-delimited languages, unsegmented
    languages, such as Chinese and Japanese, require word segmentation as a preprocessing
    step. However, word segmentation, that often requires manually annotated resources,
    is difficult and expensive, and unavoidable errors in word segmentation affect
    downstream tasks. To avoid these problems in learning word vectors of unsegmented
    languages, we consider word co-occurrence statistics over all possible candidates
    of segmentations based on frequent character n-grams instead of segmented sentences
    provided by conventional word segmenters. Our experiments of noun category prediction
    tasks on raw Twitter, Weibo, and Wikipedia corpora show that the proposed method
    outperforms the conventional approaches that require word segmenters.
  address: Copenhagen, Denmark
  author:
  - first: Takamasa
    full: Takamasa Oshikiri
    id: takamasa-oshikiri
    last: Oshikiri
  author_string: Takamasa Oshikiri
  bibkey: oshikiri-2017-segmentation
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1080
  month: September
  page_first: '767'
  page_last: '772'
  pages: "767\u2013772"
  paper_id: '80'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1080.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1080.jpg
  title: Segmentation-Free Word Embedding for Unsegmented Languages
  title_html: Segmentation-Free Word Embedding for Unsegmented Languages
  url: https://www.aclweb.org/anthology/D17-1080
  year: '2017'
D17-1081:
  abstract: Textbooks are rich sources of information. Harvesting structured knowledge
    from textbooks is a key challenge in many educational applications. As a case
    study, we present an approach for harvesting structured axiomatic knowledge from
    math textbooks. Our approach uses rich contextual and typographical features extracted
    from raw textbooks. It leverages the redundancy and shared ordering across multiple
    textbooks to further refine the harvested axioms. These axioms are then parsed
    into rules that are used to improve the state-of-the-art in solving geometry problems.
  address: Copenhagen, Denmark
  author:
  - first: Mrinmaya
    full: Mrinmaya Sachan
    id: mrinmaya-sachan
    last: Sachan
  - first: Kumar
    full: Kumar Dubey
    id: kumar-dubey
    last: Dubey
  - first: Eric
    full: Eric Xing
    id: eric-xing
    last: Xing
  author_string: Mrinmaya Sachan, Kumar Dubey, Eric Xing
  bibkey: sachan-etal-2017-textbooks
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1081
  month: September
  page_first: '773'
  page_last: '784'
  pages: "773\u2013784"
  paper_id: '81'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1081.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1081.jpg
  title: 'From Textbooks to Knowledge: A Case Study in Harvesting Axiomatic Knowledge
    from Textbooks to Solve Geometry Problems'
  title_html: 'From Textbooks to Knowledge: A Case Study in Harvesting Axiomatic Knowledge
    from Textbooks to Solve Geometry Problems'
  url: https://www.aclweb.org/anthology/D17-1081
  year: '2017'
D17-1082:
  abstract: "We present RACE, a new dataset for benchmark evaluation of methods in\
    \ the reading comprehension task. Collected from the English exams for middle\
    \ and high school Chinese students in the age range between 12 to 18, RACE consists\
    \ of near 28,000 passages and near 100,000 questions generated by human experts\
    \ (English instructors), and covers a variety of topics which are carefully designed\
    \ for evaluating the students\u2019 ability in understanding and reasoning. In\
    \ particular, the proportion of questions that requires reasoning is much larger\
    \ in RACE than that in other benchmark datasets for reading comprehension, and\
    \ there is a significant gap between the performance of the state-of-the-art models\
    \ (43%) and the ceiling human performance (95%). We hope this new dataset can\
    \ serve as a valuable resource for research and evaluation in machine comprehension.\
    \ The dataset is freely available at http://www.cs.cmu.edu/~glai1/data/race/and\
    \ the code is available at https://github.com/qizhex/RACE_AR_baselines."
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1082.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1082.Attachment.pdf
  author:
  - first: Guokun
    full: Guokun Lai
    id: guokun-lai
    last: Lai
  - first: Qizhe
    full: Qizhe Xie
    id: qizhe-xie
    last: Xie
  - first: Hanxiao
    full: Hanxiao Liu
    id: hanxiao-liu
    last: Liu
  - first: Yiming
    full: Yiming Yang
    id: yiming-yang
    last: Yang
  - first: Eduard
    full: Eduard Hovy
    id: eduard-hovy
    last: Hovy
  author_string: Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, Eduard Hovy
  bibkey: lai-etal-2017-race
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1082
  month: September
  page_first: '785'
  page_last: '794'
  pages: "785\u2013794"
  paper_id: '82'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1082.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1082.jpg
  title: 'RACE: Large-scale ReAding Comprehension Dataset From Examinations'
  title_html: '<span class="acl-fixed-case">RACE</span>: Large-scale <span class="acl-fixed-case">R</span>e<span
    class="acl-fixed-case">A</span>ding Comprehension Dataset From Examinations'
  url: https://www.aclweb.org/anthology/D17-1082
  year: '2017'
D17-1083:
  abstract: "We present an approach for answering questions that span multiple sentences\
    \ and exhibit sophisticated cross-sentence anaphoric phenomena, evaluating on\
    \ a rich source of such questions \u2013 the math portion of the Scholastic Aptitude\
    \ Test (SAT). By using a tree transducer cascade as its basic architecture, our\
    \ system propagates uncertainty from multiple sources (e.g. coreference resolution\
    \ or verb interpretation) until it can be confidently resolved. Experiments show\
    \ the first-ever results 43% recall and 91% precision) on SAT algebra word problems.\
    \ We also apply our system to the public Dolphin algebra question set, and improve\
    \ the state-of-the-art F1-score from 73.9% to 77.0%."
  address: Copenhagen, Denmark
  author:
  - first: Mark
    full: Mark Hopkins
    id: mark-hopkins
    last: Hopkins
  - first: Cristian
    full: Cristian Petrescu-Prahova
    id: cristian-petrescu-prahova
    last: Petrescu-Prahova
  - first: Roie
    full: Roie Levin
    id: roie-levin
    last: Levin
  - first: Ronan
    full: Ronan Le Bras
    id: ronan-le-bras
    last: Le Bras
  - first: Alvaro
    full: Alvaro Herrasti
    id: alvaro-herrasti
    last: Herrasti
  - first: Vidur
    full: Vidur Joshi
    id: vidur-joshi
    last: Joshi
  author_string: Mark Hopkins, Cristian Petrescu-Prahova, Roie Levin, Ronan Le Bras,
    Alvaro Herrasti, Vidur Joshi
  bibkey: hopkins-etal-2017-beyond
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1083
  month: September
  page_first: '795'
  page_last: '804'
  pages: "795\u2013804"
  paper_id: '83'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1083.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1083.jpg
  title: 'Beyond Sentential Semantic Parsing: Tackling the Math SAT with a Cascade
    of Tree Transducers'
  title_html: 'Beyond Sentential Semantic Parsing: Tackling the Math <span class="acl-fixed-case">SAT</span>
    with a Cascade of Tree Transducers'
  url: https://www.aclweb.org/anthology/D17-1083
  year: '2017'
D17-1084:
  abstract: This paper presents a novel template-based method to solve math word problems.
    This method learns the mappings between math concept phrases in math word problems
    and their math expressions from training data. For each equation template, we
    automatically construct a rich template sketch by aggregating information from
    various problems with the same template. Our approach is implemented in a two-stage
    system. It first retrieves a few relevant equation system templates and aligns
    numbers in math word problems to those templates for candidate equation generation.
    It then does a fine-grained inference to obtain the final answer. Experiment results
    show that our method achieves an accuracy of 28.4% on the linear Dolphin18K benchmark,
    which is 10% (54% relative) higher than previous state-of-the-art systems while
    achieving an accuracy increase of 12% (59% relative) on the TS6 benchmark subset.
  address: Copenhagen, Denmark
  author:
  - first: Danqing
    full: Danqing Huang
    id: danqing-huang
    last: Huang
  - first: Shuming
    full: Shuming Shi
    id: shuming-shi
    last: Shi
  - first: Chin-Yew
    full: Chin-Yew Lin
    id: chin-yew-lin
    last: Lin
  - first: Jian
    full: Jian Yin
    id: jian-yin
    last: Yin
  author_string: Danqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin
  bibkey: huang-etal-2017-learning-fine
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1084
  month: September
  page_first: '805'
  page_last: '814'
  pages: "805\u2013814"
  paper_id: '84'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1084.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1084.jpg
  title: Learning Fine-Grained Expressions to Solve Math Word Problems
  title_html: Learning Fine-Grained Expressions to Solve Math Word Problems
  url: https://www.aclweb.org/anthology/D17-1084
  year: '2017'
D17-1085:
  abstract: Deep neural networks for machine comprehension typically utilizes only
    word or character embeddings without explicitly taking advantage of structured
    linguistic information such as constituency trees and dependency trees. In this
    paper, we propose structural embedding of syntactic trees (SEST), an algorithm
    framework to utilize structured information and encode them into vector representations
    that can boost the performance of algorithms for the machine comprehension. We
    evaluate our approach using a state-of-the-art neural attention model on the SQuAD
    dataset. Experimental results demonstrate that our model can accurately identify
    the syntactic boundaries of the sentences and extract answers that are syntactically
    coherent over the baseline methods.
  address: Copenhagen, Denmark
  author:
  - first: Rui
    full: Rui Liu
    id: rui-liu
    last: Liu
  - first: Junjie
    full: Junjie Hu
    id: junjie-hu
    last: Hu
  - first: Wei
    full: Wei Wei
    id: wei-wei
    last: Wei
  - first: Zi
    full: Zi Yang
    id: zi-yang
    last: Yang
  - first: Eric
    full: Eric Nyberg
    id: eric-nyberg
    last: Nyberg
  author_string: Rui Liu, Junjie Hu, Wei Wei, Zi Yang, Eric Nyberg
  bibkey: liu-etal-2017-structural
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1085
  month: September
  page_first: '815'
  page_last: '824'
  pages: "815\u2013824"
  paper_id: '85'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1085.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1085.jpg
  title: Structural Embedding of Syntactic Trees for Machine Comprehension
  title_html: Structural Embedding of Syntactic Trees for Machine Comprehension
  url: https://www.aclweb.org/anthology/D17-1085
  year: '2017'
D17-1086:
  abstract: 'Humans interpret texts with respect to some background information, or
    world knowledge, and we would like to develop automatic reading comprehension
    systems that can do the same. In this paper, we introduce a task and several models
    to drive progress towards this goal. In particular, we propose the task of rare
    entity prediction: given a web document with several entities removed, models
    are tasked with predicting the correct missing entities conditioned on the document
    context and the lexical resources. This task is challenging due to the diversity
    of language styles and the extremely large number of rare entities. We propose
    two recurrent neural network architectures which make use of external knowledge
    in the form of entity descriptions. Our experiments show that our hierarchical
    LSTM model performs significantly better at the rare entity prediction task than
    those that do not make use of external resources.'
  address: Copenhagen, Denmark
  author:
  - first: Teng
    full: Teng Long
    id: teng-long
    last: Long
  - first: Emmanuel
    full: Emmanuel Bengio
    id: emmanuel-bengio
    last: Bengio
  - first: Ryan
    full: Ryan Lowe
    id: ryan-lowe
    last: Lowe
  - first: Jackie Chi Kit
    full: Jackie Chi Kit Cheung
    id: jackie-chi-kit-cheung
    last: Cheung
  - first: Doina
    full: Doina Precup
    id: doina-precup
    last: Precup
  author_string: Teng Long, Emmanuel Bengio, Ryan Lowe, Jackie Chi Kit Cheung, Doina
    Precup
  bibkey: long-etal-2017-world
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1086
  month: September
  page_first: '825'
  page_last: '834'
  pages: "825\u2013834"
  paper_id: '86'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1086.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1086.jpg
  title: 'World Knowledge for Reading Comprehension: Rare Entity Prediction with Hierarchical
    LSTMs Using External Descriptions'
  title_html: 'World Knowledge for Reading Comprehension: Rare Entity Prediction with
    Hierarchical <span class="acl-fixed-case">LSTM</span>s Using External Descriptions'
  url: https://www.aclweb.org/anthology/D17-1086
  year: '2017'
D17-1087:
  abstract: We develop a technique for transfer learning in machine comprehension
    (MC) using a novel two-stage synthesis network. Given a high performing MC model
    in one domain, our technique aims to answer questions about documents in another
    domain, where we use no labeled data of question-answer pairs. Using the proposed
    synthesis network with a pretrained model on the SQuAD dataset, we achieve an
    F1 measure of 46.6% on the challenging NewsQA dataset, approaching performance
    of in-domain models (F1 measure of 50.0%) and outperforming the out-of-domain
    baseline by 7.6%, without use of provided annotations.
  address: Copenhagen, Denmark
  author:
  - first: David
    full: David Golub
    id: david-golub
    last: Golub
  - first: Po-Sen
    full: Po-Sen Huang
    id: po-sen-huang
    last: Huang
  - first: Xiaodong
    full: Xiaodong He
    id: xiaodong-he
    last: He
  - first: Li
    full: Li Deng
    id: li-deng
    last: Deng
  author_string: David Golub, Po-Sen Huang, Xiaodong He, Li Deng
  bibkey: golub-etal-2017-two
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1087
  month: September
  page_first: '835'
  page_last: '844'
  pages: "835\u2013844"
  paper_id: '87'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1087.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1087.jpg
  title: Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension
  title_html: Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension
  url: https://www.aclweb.org/anthology/D17-1087
  year: '2017'
D17-1088:
  abstract: This paper presents a deep neural solver to automatically solve math word
    problems. In contrast to previous statistical learning approaches, we directly
    translate math word problems to equation templates using a recurrent neural network
    (RNN) model, without sophisticated feature engineering. We further design a hybrid
    model that combines the RNN model and a similarity-based retrieval model to achieve
    additional performance improvement. Experiments conducted on a large dataset show
    that the RNN model and the hybrid model significantly outperform state-of-the-art
    statistical learning methods for math word problem solving.
  address: Copenhagen, Denmark
  author:
  - first: Yan
    full: Yan Wang
    id: yan-wang
    last: Wang
  - first: Xiaojiang
    full: Xiaojiang Liu
    id: xiaojiang-liu
    last: Liu
  - first: Shuming
    full: Shuming Shi
    id: shuming-shi
    last: Shi
  author_string: Yan Wang, Xiaojiang Liu, Shuming Shi
  bibkey: wang-etal-2017-deep
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1088
  month: September
  page_first: '845'
  page_last: '854'
  pages: "845\u2013854"
  paper_id: '88'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1088.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1088.jpg
  title: Deep Neural Solver for Math Word Problems
  title_html: Deep Neural Solver for Math Word Problems
  url: https://www.aclweb.org/anthology/D17-1088
  year: '2017'
D17-1089:
  abstract: Community-driven Question Answering (CQA) systems such as Yahoo! Answers
    have become valuable sources of reusable information. CQA retrieval enables usage
    of historical CQA archives to solve new questions posed by users. This task has
    received much recent attention, with methods building upon literature from translation
    models, topic models, and deep learning. In this paper, we devise a CQA retrieval
    technique, LASER-QA, that embeds question-answer pairs within a unified latent
    space preserving the local neighborhood structure of question and answer spaces.
    The idea is that such a space mirrors semantic similarity among questions as well
    as answers, thereby enabling high quality retrieval. Through an empirical analysis
    on various real-world QA datasets, we illustrate the improved effectiveness of
    LASER-QA over state-of-the-art methods.
  address: Copenhagen, Denmark
  author:
  - first: Deepak
    full: Deepak P
    id: deepak-p
    last: P
  - first: Dinesh
    full: Dinesh Garg
    id: dinesh-garg
    last: Garg
  - first: Shirish
    full: Shirish Shevade
    id: shirish-shevade
    last: Shevade
  author_string: Deepak P, Dinesh Garg, Shirish Shevade
  bibkey: p-etal-2017-latent
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1089
  month: September
  page_first: '855'
  page_last: '865'
  pages: "855\u2013865"
  paper_id: '89'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1089.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1089.jpg
  title: Latent Space Embedding for Retrieval in Question-Answer Archives
  title_html: Latent Space Embedding for Retrieval in Question-Answer Archives
  url: https://www.aclweb.org/anthology/D17-1089
  year: '2017'
D17-1090:
  abstract: 'This paper presents how to generate questions from given passages using
    neural networks, where large scale QA pairs are automatically crawled and processed
    from Community-QA website, and used as training data. The contribution of the
    paper is 2-fold: First, two types of question generation approaches are proposed,
    one is a retrieval-based method using convolution neural network (CNN), the other
    is a generation-based method using recurrent neural network (RNN); Second, we
    show how to leverage the generated questions to improve existing question answering
    systems. We evaluate our question generation method for the answer sentence selection
    task on three benchmark datasets, including SQuAD, MS MARCO, and WikiQA. Experimental
    results show that, by using generated questions as an extra signal, significant
    QA improvement can be achieved.'
  address: Copenhagen, Denmark
  author:
  - first: Nan
    full: Nan Duan
    id: nan-duan
    last: Duan
  - first: Duyu
    full: Duyu Tang
    id: duyu-tang
    last: Tang
  - first: Peng
    full: Peng Chen
    id: peng-chen
    last: Chen
  - first: Ming
    full: Ming Zhou
    id: ming-zhou
    last: Zhou
  author_string: Nan Duan, Duyu Tang, Peng Chen, Ming Zhou
  bibkey: duan-etal-2017-question
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1090
  month: September
  page_first: '866'
  page_last: '874'
  pages: "866\u2013874"
  paper_id: '90'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1090.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1090.jpg
  title: Question Generation for Question Answering
  title_html: Question Generation for Question Answering
  url: https://www.aclweb.org/anthology/D17-1090
  year: '2017'
D17-1091:
  abstract: Question answering (QA) systems are sensitive to the many different ways
    natural language expresses the same information need. In this paper we turn to
    paraphrases as a means of capturing this knowledge and present a general framework
    which learns felicitous paraphrases for various QA tasks. Our method is trained
    end-to-end using question-answer pairs as a supervision signal. A question and
    its paraphrases serve as input to a neural scoring model which assigns higher
    weights to linguistic expressions most likely to yield correct answers. We evaluate
    our approach on QA over Freebase and answer sentence selection. Experimental results
    on three datasets show that our framework consistently improves performance, achieving
    competitive results despite the use of simple QA models.
  address: Copenhagen, Denmark
  author:
  - first: Li
    full: Li Dong
    id: li-dong
    last: Dong
  - first: Jonathan
    full: Jonathan Mallinson
    id: jonathan-mallinson
    last: Mallinson
  - first: Siva
    full: Siva Reddy
    id: siva-reddy
    last: Reddy
  - first: Mirella
    full: Mirella Lapata
    id: mirella-lapata
    last: Lapata
  author_string: Li Dong, Jonathan Mallinson, Siva Reddy, Mirella Lapata
  bibkey: dong-etal-2017-learning-paraphrase
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1091
  month: September
  page_first: '875'
  page_last: '886'
  pages: "875\u2013886"
  paper_id: '91'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1091.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1091.jpg
  title: Learning to Paraphrase for Question Answering
  title_html: Learning to Paraphrase for Question Answering
  url: https://www.aclweb.org/anthology/D17-1091
  year: '2017'
D17-1092:
  abstract: "In this paper, we propose to use a set of simple, uniform in architecture\
    \ LSTM-based models to recover different kinds of temporal relations from text.\
    \ Using the shortest dependency path between entities as input, the same architecture\
    \ is used to extract intra-sentence, cross-sentence, and document creation time\
    \ relations. A \u201Cdouble-checking\u201D technique reverses entity pairs in\
    \ classification, boosting the recall of positive cases and reducing misclassifications\
    \ between opposite classes. An efficient pruning algorithm resolves conflicts\
    \ globally. Evaluated on QA-TempEval (SemEval2015 Task 5), our proposed technique\
    \ outperforms state-of-the-art methods by a large margin. We also conduct intrinsic\
    \ evaluation and post state-of-the-art results on Timebank-Dense."
  address: Copenhagen, Denmark
  author:
  - first: Yuanliang
    full: Yuanliang Meng
    id: yuanliang-meng
    last: Meng
  - first: Anna
    full: Anna Rumshisky
    id: anna-rumshisky
    last: Rumshisky
  - first: Alexey
    full: Alexey Romanov
    id: alexey-romanov
    last: Romanov
  author_string: Yuanliang Meng, Anna Rumshisky, Alexey Romanov
  bibkey: meng-etal-2017-temporal
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1092
  month: September
  page_first: '887'
  page_last: '896'
  pages: "887\u2013896"
  paper_id: '92'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1092.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1092.jpg
  title: Temporal Information Extraction for Question Answering Using Syntactic Dependencies
    in an LSTM-based Architecture
  title_html: Temporal Information Extraction for Question Answering Using Syntactic
    Dependencies in an <span class="acl-fixed-case">LSTM</span>-based Architecture
  url: https://www.aclweb.org/anthology/D17-1092
  year: '2017'
D17-1093:
  abstract: 'Recent work has shown that Tree Kernels (TKs) and Convolutional Neural
    Networks (CNNs) obtain the state of the art in answer sentence reranking. Additionally,
    their combination used in Support Vector Machines (SVMs) is promising as it can
    exploit both the syntactic patterns captured by TKs and the embeddings learned
    by CNNs. However, the embeddings are constructed according to a classification
    function, which is not directly exploitable in the preference ranking algorithm
    of SVMs. In this work, we propose a new hybrid approach combining preference ranking
    applied to TKs and pointwise ranking applied to CNNs. We show that our approach
    produces better results on two well-known and rather different datasets: WikiQA
    for answer sentence selection and SemEval cQA for comment selection in Community
    Question Answering.'
  address: Copenhagen, Denmark
  author:
  - first: Kateryna
    full: Kateryna Tymoshenko
    id: kateryna-tymoshenko
    last: Tymoshenko
  - first: Daniele
    full: Daniele Bonadiman
    id: daniele-bonadiman
    last: Bonadiman
  - first: Alessandro
    full: Alessandro Moschitti
    id: alessandro-moschitti
    last: Moschitti
  author_string: Kateryna Tymoshenko, Daniele Bonadiman, Alessandro Moschitti
  bibkey: tymoshenko-etal-2017-ranking
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1093
  month: September
  page_first: '897'
  page_last: '902'
  pages: "897\u2013902"
  paper_id: '93'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1093.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1093.jpg
  title: 'Ranking Kernels for Structures and Embeddings: A Hybrid Preference and Classification
    Model'
  title_html: 'Ranking Kernels for Structures and Embeddings: A Hybrid Preference
    and Classification Model'
  url: https://www.aclweb.org/anthology/D17-1093
  year: '2017'
D17-1094:
  abstract: The existing factoid QA systems often lack a post-inspection component
    that can help models recover from their own mistakes. In this work, we propose
    to crosscheck the corresponding KB relations behind the predicted answers and
    identify potential inconsistencies. Instead of developing a new model that accepts
    evidences collected from these relations, we choose to plug them back to the original
    questions directly and check if the revised question makes sense or not. A bidirectional
    LSTM is applied to encode revised questions. We develop a scoring mechanism over
    the revised question encodings to refine the predictions of a base QA system.
    This approach can improve the F1 score of STAGG (Yih et al., 2015), one of the
    leading QA systems, from 52.5% to 53.9% on WEBQUESTIONS data.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1094.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1094.Attachment.zip
  author:
  - first: Semih
    full: Semih Yavuz
    id: semih-yavuz
    last: Yavuz
  - first: Izzeddin
    full: Izzeddin Gur
    id: izzeddin-gur1
    last: Gur
  - first: Yu
    full: Yu Su
    id: yu-su
    last: Su
  - first: Xifeng
    full: Xifeng Yan
    id: xifeng-yan
    last: Yan
  author_string: Semih Yavuz, Izzeddin Gur, Yu Su, Xifeng Yan
  bibkey: yavuz-etal-2017-recovering
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1094
  month: September
  page_first: '903'
  page_last: '909'
  pages: "903\u2013909"
  paper_id: '94'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1094.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1094.jpg
  title: Recovering Question Answering Errors via Query Revision
  title_html: Recovering Question Answering Errors via Query Revision
  url: https://www.aclweb.org/anthology/D17-1094
  year: '2017'
D17-1095:
  abstract: "In state-of-the-art Neural Machine Translation (NMT), an attention mechanism\
    \ is used during decoding to enhance the translation. At every step, the decoder\
    \ uses this mechanism to focus on different parts of the source sentence to gather\
    \ the most useful information before outputting its target word. Recently, the\
    \ effectiveness of the attention mechanism has also been explored for multi-modal\
    \ tasks, where it becomes possible to focus both on sentence parts and image regions\
    \ that they describe. In this paper, we compare several attention mechanism on\
    \ the multi-modal translation task (English, image \u2192 German) and evaluate\
    \ the ability of the model to make use of images to improve translation. We surpass\
    \ state-of-the-art scores on the Multi30k data set, we nevertheless identify and\
    \ report different misbehavior of the machine while translating."
  address: Copenhagen, Denmark
  author:
  - first: Jean-Benoit
    full: Jean-Benoit Delbrouck
    id: jean-benoit-delbrouck
    last: Delbrouck
  - first: "St\xE9phane"
    full: "St\xE9phane Dupont"
    id: stephane-dupont
    last: Dupont
  author_string: "Jean-Benoit Delbrouck, St\xE9phane Dupont"
  bibkey: delbrouck-dupont-2017-empirical
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1095
  month: September
  page_first: '910'
  page_last: '919'
  pages: "910\u2013919"
  paper_id: '95'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1095.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1095.jpg
  title: An empirical study on the effectiveness of images in Multimodal Neural Machine
    Translation
  title_html: An empirical study on the effectiveness of images in Multimodal Neural
    Machine Translation
  url: https://www.aclweb.org/anthology/D17-1095
  year: '2017'
D17-1096:
  abstract: "To be able to interact better with humans, it is crucial for machines\
    \ to understand sound \u2013 a primary modality of human perception. Previous\
    \ works have used sound to learn embeddings for improved generic semantic similarity\
    \ assessment. In this work, we treat sound as a first-class citizen, studying\
    \ downstream 6textual tasks which require aural grounding. To this end, we propose\
    \ sound-word2vec \u2013 a new embedding scheme that learns specialized word embeddings\
    \ grounded in sounds. For example, we learn that two seemingly (semantically)\
    \ unrelated concepts, like leaves and paper are similar due to the similar rustling\
    \ sounds they make. Our embeddings prove useful in textual tasks requiring aural\
    \ reasoning like text-based sound retrieval and discovering Foley sound effects\
    \ (used in movies). Moreover, our embedding space captures interesting dependencies\
    \ between words and onomatopoeia and outperforms prior work on aurally-relevant\
    \ word relatedness datasets such as AMEN and ASLex."
  address: Copenhagen, Denmark
  author:
  - first: Ashwin
    full: Ashwin Vijayakumar
    id: ashwin-vijayakumar
    last: Vijayakumar
  - first: Ramakrishna
    full: Ramakrishna Vedantam
    id: ramakrishna-vedantam
    last: Vedantam
  - first: Devi
    full: Devi Parikh
    id: devi-parikh
    last: Parikh
  author_string: Ashwin Vijayakumar, Ramakrishna Vedantam, Devi Parikh
  bibkey: vijayakumar-etal-2017-sound
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1096
  month: September
  page_first: '920'
  page_last: '925'
  pages: "920\u2013925"
  paper_id: '96'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1096.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1096.jpg
  title: 'Sound-Word2Vec: Learning Word Representations Grounded in Sounds'
  title_html: 'Sound-<span class="acl-fixed-case">W</span>ord2<span class="acl-fixed-case">V</span>ec:
    Learning Word Representations Grounded in Sounds'
  url: https://www.aclweb.org/anthology/D17-1096
  year: '2017'
D17-1097:
  abstract: "In this paper, we make a simple observation that questions about images\
    \ often contain premises \u2013 objects and relationships implied by the question\
    \ \u2013 and that reasoning about premises can help Visual Question Answering\
    \ (VQA) models respond more intelligently to irrelevant or previously unseen questions.\
    \ When presented with a question that is irrelevant to an image, state-of-the-art\
    \ VQA models will still answer purely based on learned language biases, resulting\
    \ in non-sensical or even misleading answers. We note that a visual question is\
    \ irrelevant to an image if at least one of its premises is false (i.e. not depicted\
    \ in the image). We leverage this observation to construct a dataset for Question\
    \ Relevance Prediction and Explanation (QRPE) by searching for false premises.\
    \ We train novel question relevance detection models and show that models that\
    \ reason about premises consistently outperform models that do not. We also find\
    \ that forcing standard VQA models to reason about premises during training can\
    \ lead to improvements on tasks requiring compositional reasoning."
  address: Copenhagen, Denmark
  author:
  - first: Aroma
    full: Aroma Mahendru
    id: aroma-mahendru
    last: Mahendru
  - first: Viraj
    full: Viraj Prabhu
    id: viraj-prabhu
    last: Prabhu
  - first: Akrit
    full: Akrit Mohapatra
    id: akrit-mohapatra
    last: Mohapatra
  - first: Dhruv
    full: Dhruv Batra
    id: dhruv-batra
    last: Batra
  - first: Stefan
    full: Stefan Lee
    id: stefan-lee
    last: Lee
  author_string: Aroma Mahendru, Viraj Prabhu, Akrit Mohapatra, Dhruv Batra, Stefan
    Lee
  bibkey: mahendru-etal-2017-promise
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1097
  month: September
  page_first: '926'
  page_last: '935'
  pages: "926\u2013935"
  paper_id: '97'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1097.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1097.jpg
  title: 'The Promise of Premise: Harnessing Question Premises in Visual Question
    Answering'
  title_html: 'The Promise of Premise: Harnessing Question Premises in Visual Question
    Answering'
  url: https://www.aclweb.org/anthology/D17-1097
  year: '2017'
D17-1098:
  abstract: Existing image captioning models do not generalize well to out-of-domain
    images containing novel scenes or objects. This limitation severely hinders the
    use of these models in real world applications dealing with images in the wild.
    We address this problem using a flexible approach that enables existing deep captioning
    architectures to take advantage of image taggers at test time, without re-training.
    Our method uses constrained beam search to force the inclusion of selected tag
    words in the output, and fixed, pretrained word embeddings to facilitate vocabulary
    expansion to previously unseen tag words. Using this approach we achieve state
    of the art results for out-of-domain captioning on MSCOCO (and improved results
    for in-domain captioning). Perhaps surprisingly, our results significantly outperform
    approaches that incorporate the same tag predictions into the learning algorithm.
    We also show that we can significantly improve the quality of generated ImageNet
    captions by leveraging ground-truth labels.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1098.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1098.Attachment.zip
  author:
  - first: Peter
    full: Peter Anderson
    id: peter-anderson
    last: Anderson
  - first: Basura
    full: Basura Fernando
    id: basura-fernando
    last: Fernando
  - first: Mark
    full: Mark Johnson
    id: mark-johnson
    last: Johnson
  - first: Stephen
    full: Stephen Gould
    id: stephen-gould
    last: Gould
  author_string: Peter Anderson, Basura Fernando, Mark Johnson, Stephen Gould
  bibkey: anderson-etal-2017-guided
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1098
  month: September
  page_first: '936'
  page_last: '945'
  pages: "936\u2013945"
  paper_id: '98'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1098.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1098.jpg
  title: Guided Open Vocabulary Image Captioning with Constrained Beam Search
  title_html: Guided Open Vocabulary Image Captioning with Constrained Beam Search
  url: https://www.aclweb.org/anthology/D17-1098
  year: '2017'
D17-1099:
  abstract: "In this paper, we investigate large-scale zero-shot activity recognition\
    \ by modeling the visual and linguistic attributes of action verbs. For example,\
    \ the verb \u201Csalute\u201D has several properties, such as being a light movement,\
    \ a social act, and short in duration. We use these attributes as the internal\
    \ mapping between visual and textual representations to reason about a previously\
    \ unseen action. In contrast to much prior work that assumes access to gold standard\
    \ attributes for zero-shot classes and focuses primarily on object attributes,\
    \ our model uniquely learns to infer action attributes from dictionary definitions\
    \ and distributed word representations. Experimental results confirm that action\
    \ attributes inferred from language can provide a predictive signal for zero-shot\
    \ prediction of previously unseen activities."
  address: Copenhagen, Denmark
  author:
  - first: Rowan
    full: Rowan Zellers
    id: rowan-zellers
    last: Zellers
  - first: Yejin
    full: Yejin Choi
    id: yejin-choi
    last: Choi
  author_string: Rowan Zellers, Yejin Choi
  bibkey: zellers-choi-2017-zero
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1099
  month: September
  page_first: '946'
  page_last: '958'
  pages: "946\u2013958"
  paper_id: '99'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1099.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1099.jpg
  title: Zero-Shot Activity Recognition with Verb Attribute Induction
  title_html: Zero-Shot Activity Recognition with Verb Attribute Induction
  url: https://www.aclweb.org/anthology/D17-1099
  year: '2017'
D17-1100:
  abstract: "Corpora of referring expressions paired with their visual referents are\
    \ a good source for learning word meanings directly grounded in visual representations.\
    \ Here, we explore additional ways of extracting from them word representations\
    \ linked to multi-modal context: through expressions that refer to the same object,\
    \ and through expressions that refer to different objects in the same scene. We\
    \ show that continuous meaning representations derived from these contexts capture\
    \ complementary aspects of similarity, , even if not outperforming textual embeddings\
    \ trained on very large amounts of raw text when tested on standard similarity\
    \ benchmarks. We propose a new task for evaluating grounded meaning representations\u2014\
    detection of potentially co-referential phrases\u2014and show that it requires\
    \ precise denotational representations of attribute meanings, which our method\
    \ provides."
  address: Copenhagen, Denmark
  author:
  - first: Sina
    full: "Sina Zarrie\xDF"
    id: sina-zarriess
    last: "Zarrie\xDF"
  - first: David
    full: David Schlangen
    id: david-schlangen
    last: Schlangen
  author_string: "Sina Zarrie\xDF, David Schlangen"
  bibkey: zarriess-schlangen-2017-deriving
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1100
  month: September
  page_first: '959'
  page_last: '965'
  pages: "959\u2013965"
  paper_id: '100'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1100.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1100.jpg
  title: Deriving continous grounded meaning representations from referentially structured
    multimodal contexts
  title_html: Deriving continous grounded meaning representations from referentially
    structured multimodal contexts
  url: https://www.aclweb.org/anthology/D17-1100
  year: '2017'
D17-1101:
  abstract: 'We address the problem of end-to-end visual storytelling. Given a photo
    album, our model first selects the most representative (summary) photos, and then
    composes a natural language story for the album. For this task, we make use of
    the Visual Storytelling dataset and a model composed of three hierarchically-attentive
    Recurrent Neural Nets (RNNs) to: encode the album photos, select representative
    (summary) photos, and compose the story. Automatic and human evaluations show
    our model achieves better performance on selection, generation, and retrieval
    than baselines.'
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1101.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1101.Attachment.pdf
  author:
  - first: Licheng
    full: Licheng Yu
    id: licheng-yu
    last: Yu
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  - first: Tamara
    full: Tamara Berg
    id: tamara-berg
    last: Berg
  author_string: Licheng Yu, Mohit Bansal, Tamara Berg
  bibkey: yu-etal-2017-hierarchically
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1101
  month: September
  page_first: '966'
  page_last: '971'
  pages: "966\u2013971"
  paper_id: '101'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1101.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1101.jpg
  title: Hierarchically-Attentive RNN for Album Summarization and Storytelling
  title_html: Hierarchically-Attentive <span class="acl-fixed-case">RNN</span> for
    Album Summarization and Storytelling
  url: https://www.aclweb.org/anthology/D17-1101
  year: '2017'
D17-1102:
  abstract: Sports channel video portals offer an exciting domain for research on
    multimodal, multilingual analysis. We present methods addressing the problem of
    automatic video highlight prediction based on joint visual features and textual
    analysis of the real-world audience discourse with complex slang, in both English
    and traditional Chinese. We present a novel dataset based on League of Legends
    championships recorded from North American and Taiwanese Twitch.tv channels (will
    be released for further research), and demonstrate strong results on these using
    multimodal, character-level CNN-RNN model architectures.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1102.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1102.Attachment.zip
  author:
  - first: Cheng-Yang
    full: Cheng-Yang Fu
    id: cheng-yang-fu
    last: Fu
  - first: Joon
    full: Joon Lee
    id: joon-lee
    last: Lee
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  - first: Alexander
    full: Alexander Berg
    id: alexander-berg
    last: Berg
  author_string: Cheng-Yang Fu, Joon Lee, Mohit Bansal, Alexander Berg
  bibkey: fu-etal-2017-video
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1102
  month: September
  page_first: '972'
  page_last: '978'
  pages: "972\u2013978"
  paper_id: '102'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1102.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1102.jpg
  title: Video Highlight Prediction Using Audience Chat Reactions
  title_html: Video Highlight Prediction Using Audience Chat Reactions
  url: https://www.aclweb.org/anthology/D17-1102
  year: '2017'
D17-1103:
  abstract: Sequence-to-sequence models have shown promising improvements on the temporal
    task of video captioning, but they optimize word-level cross-entropy loss during
    training. First, using policy gradient and mixed-loss methods for reinforcement
    learning, we directly optimize sentence-level task-based metrics (as rewards),
    achieving significant improvements over the baseline, based on both automatic
    metrics and human evaluation on multiple datasets. Next, we propose a novel entailment-enhanced
    reward (CIDEnt) that corrects phrase-matching based metrics (such as CIDEr) to
    only allow for logically-implied partial matches and avoid contradictions, achieving
    further significant improvements over the CIDEr-reward model. Overall, our CIDEnt-reward
    model achieves the new state-of-the-art on the MSR-VTT dataset.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1103.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1103.Attachment.pdf
  author:
  - first: Ramakanth
    full: Ramakanth Pasunuru
    id: ramakanth-pasunuru
    last: Pasunuru
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  author_string: Ramakanth Pasunuru, Mohit Bansal
  bibkey: pasunuru-bansal-2017-reinforced
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1103
  month: September
  page_first: '979'
  page_last: '985'
  pages: "979\u2013985"
  paper_id: '103'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1103.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1103.jpg
  title: Reinforced Video Captioning with Entailment Rewards
  title_html: Reinforced Video Captioning with Entailment Rewards
  url: https://www.aclweb.org/anthology/D17-1103
  year: '2017'
D17-1104:
  abstract: Verbs can only be used with a few specific arrangements of their arguments
    (syntactic frames). Most theorists note that verbs can be organized into a hierarchy
    of verb classes based on the frames they admit. Here we show that such a hierarchy
    is objectively well-supported by the patterns of verbs and frames in English,
    since a systematic hierarchical clustering algorithm converges on the same structure
    as the handcrafted taxonomy of VerbNet, a broad-coverage verb lexicon. We also
    show that the hierarchies capture meaningful psychological dimensions of generalization
    by predicting novel verb coercions by human participants. We discuss limitations
    of a simple hierarchical representation and suggest similar approaches for identifying
    the representations underpinning verb argument structure.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1104.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1104.Attachment.zip
  author:
  - first: Jesse
    full: Jesse Mu
    id: jesse-mu
    last: Mu
  - first: Joshua K.
    full: Joshua K. Hartshorne
    id: joshua-k-hartshorne
    last: Hartshorne
  - first: Timothy
    full: "Timothy O\u2019Donnell"
    id: timothy-odonnell
    last: "O\u2019Donnell"
  author_string: "Jesse Mu, Joshua K. Hartshorne, Timothy O\u2019Donnell"
  bibkey: mu-etal-2017-evaluating
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1104
  month: September
  page_first: '986'
  page_last: '991'
  pages: "986\u2013991"
  paper_id: '104'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1104.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1104.jpg
  title: Evaluating Hierarchies of Verb Argument Structure with Hierarchical Clustering
  title_html: Evaluating Hierarchies of Verb Argument Structure with Hierarchical
    Clustering
  url: https://www.aclweb.org/anthology/D17-1104
  year: '2017'
D17-1105:
  abstract: We introduce multi-modal, attention-based neural machine translation (NMT)
    models which incorporate visual features into different parts of both the encoder
    and the decoder. Global image features are extracted using a pre-trained convolutional
    neural network and are incorporated (i) as words in the source sentence, (ii)
    to initialise the encoder hidden state, and (iii) as additional data to initialise
    the decoder hidden state. In our experiments, we evaluate translations into English
    and German, how different strategies to incorporate global image features compare
    and which ones perform best. We also study the impact that adding synthetic multi-modal,
    multilingual data brings and find that the additional data have a positive impact
    on multi-modal NMT models. We report new state-of-the-art results and our best
    models also significantly improve on a comparable phrase-based Statistical MT
    (PBSMT) model trained on the Multi30k data set according to all metrics evaluated.
    To the best of our knowledge, it is the first time a purely neural model significantly
    improves over a PBSMT model on all metrics evaluated on this data set.
  address: Copenhagen, Denmark
  author:
  - first: Iacer
    full: Iacer Calixto
    id: iacer-calixto
    last: Calixto
  - first: Qun
    full: Qun Liu
    id: qun-liu
    last: Liu
  author_string: Iacer Calixto, Qun Liu
  bibkey: calixto-liu-2017-incorporating
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1105
  month: September
  page_first: '992'
  page_last: '1003'
  pages: "992\u20131003"
  paper_id: '105'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1105.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1105.jpg
  title: Incorporating Global Visual Features into Attention-based Neural Machine
    Translation.
  title_html: Incorporating Global Visual Features into Attention-based Neural Machine
    Translation.
  url: https://www.aclweb.org/anthology/D17-1105
  year: '2017'
D17-1106:
  abstract: "We propose to directly map raw visual observations and text input to\
    \ actions for instruction execution. While existing approaches assume access to\
    \ structured environment representations or use a pipeline of separately trained\
    \ models, we learn a single model to jointly reason about linguistic and visual\
    \ input. We use reinforcement learning in a contextual bandit setting to train\
    \ a neural network agent. To guide the agent\u2019s exploration, we use reward\
    \ shaping with different forms of supervision. Our approach does not require intermediate\
    \ representations, planning procedures, or training different models. We evaluate\
    \ in a simulated environment, and show significant improvements over supervised\
    \ learning and common reinforcement learning variants."
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1106.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1106.Attachment.pdf
  author:
  - first: Dipendra
    full: Dipendra Misra
    id: dipendra-misra
    last: Misra
  - first: John
    full: John Langford
    id: john-langford
    last: Langford
  - first: Yoav
    full: Yoav Artzi
    id: yoav-artzi
    last: Artzi
  author_string: Dipendra Misra, John Langford, Yoav Artzi
  bibkey: misra-etal-2017-mapping
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1106
  month: September
  page_first: '1004'
  page_last: '1015'
  pages: "1004\u20131015"
  paper_id: '106'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1106.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1106.jpg
  title: Mapping Instructions and Visual Observations to Actions with Reinforcement
    Learning
  title_html: Mapping Instructions and Visual Observations to Actions with Reinforcement
    Learning
  url: https://www.aclweb.org/anthology/D17-1106
  year: '2017'
D17-1107:
  abstract: We present a machine learning analysis of eye-tracking data for the detection
    of mild cognitive impairment, a decline in cognitive abilities that is associated
    with an increased risk of developing dementia. We compare two experimental configurations
    (reading aloud versus reading silently), as well as two methods of combining information
    from the two trials (concatenation and merging). Additionally, we annotate the
    words being read with information about their frequency and syntactic category,
    and use these annotations to generate new features. Ultimately, we are able to
    distinguish between participants with and without cognitive impairment with up
    to 86% accuracy.
  address: Copenhagen, Denmark
  author:
  - first: Kathleen C.
    full: Kathleen C. Fraser
    id: kathleen-c-fraser
    last: Fraser
  - first: Kristina
    full: Kristina Lundholm Fors
    id: kristina-lundholm-fors
    last: Lundholm Fors
  - first: Dimitrios
    full: Dimitrios Kokkinakis
    id: dimitrios-kokkinakis
    last: Kokkinakis
  - first: Arto
    full: Arto Nordlund
    id: arto-nordlund
    last: Nordlund
  author_string: Kathleen C. Fraser, Kristina Lundholm Fors, Dimitrios Kokkinakis,
    Arto Nordlund
  bibkey: fraser-etal-2017-analysis
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1107
  month: September
  page_first: '1016'
  page_last: '1026'
  pages: "1016\u20131026"
  paper_id: '107'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1107.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1107.jpg
  title: An analysis of eye-movements during reading for the detection of mild cognitive
    impairment
  title_html: An analysis of eye-movements during reading for the detection of mild
    cognitive impairment
  url: https://www.aclweb.org/anthology/D17-1107
  year: '2017'
D17-1108:
  abstract: Identifying temporal relations between events is an essential step towards
    natural language understanding. However, the temporal relation between two events
    in a story depends on, and is often dictated by, relations among other events.
    Consequently, effectively identifying temporal relations between events is a challenging
    problem even for human annotators. This paper suggests that it is important to
    take these dependencies into account while learning to identify these relations
    and proposes a structured learning approach to address this challenge. As a byproduct,
    this provides a new perspective on handling missing relations, a known issue that
    hurts existing methods. As we show, the proposed approach results in significant
    improvements on the two commonly used data sets for this problem.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238231266
    type: video
    url: https://vimeo.com/238231266
  author:
  - first: Qiang
    full: Qiang Ning
    id: qiang-ning
    last: Ning
  - first: Zhili
    full: Zhili Feng
    id: zhili-feng
    last: Feng
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Qiang Ning, Zhili Feng, Dan Roth
  bibkey: ning-etal-2017-structured
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1108
  month: September
  page_first: '1027'
  page_last: '1037'
  pages: "1027\u20131037"
  paper_id: '108'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1108.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1108.jpg
  title: A Structured Learning Approach to Temporal Relation Extraction
  title_html: A Structured Learning Approach to Temporal Relation Extraction
  url: https://www.aclweb.org/anthology/D17-1108
  year: '2017'
D17-1109:
  abstract: "Knowledge base population (KBP) systems take in a large document corpus\
    \ and extract entities and their relations. Thus far, KBP evaluation has relied\
    \ on judgements on the pooled predictions of existing systems. We show that this\
    \ evaluation is problematic: when a new system predicts a previously unseen relation,\
    \ it is penalized even if it is correct. This leads to significant bias against\
    \ new systems, which counterproductively discourages innovation in the field.\
    \ Our first contribution is a new importance-sampling based evaluation which corrects\
    \ for this bias by annotating a new system\u2019s predictions on-demand via crowdsourcing.\
    \ We show this eliminates bias and reduces variance using data from the 2015 TAC\
    \ KBP task. Our second contribution is an implementation of our method made publicly\
    \ available as an online KBP evaluation service. We pilot the service by testing\
    \ diverse state-of-the-art systems on the TAC KBP 2016 corpus and obtain accurate\
    \ scores in a cost effective manner."
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238233745
    type: video
    url: https://vimeo.com/238233745
  author:
  - first: Arun
    full: Arun Chaganty
    id: arun-chaganty
    last: Chaganty
  - first: Ashwin
    full: Ashwin Paranjape
    id: ashwin-paranjape
    last: Paranjape
  - first: Percy
    full: Percy Liang
    id: percy-liang
    last: Liang
  - first: Christopher D.
    full: Christopher D. Manning
    id: christopher-d-manning
    last: Manning
  author_string: Arun Chaganty, Ashwin Paranjape, Percy Liang, Christopher D. Manning
  bibkey: chaganty-etal-2017-importance
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1109
  month: September
  page_first: '1038'
  page_last: '1048'
  pages: "1038\u20131048"
  paper_id: '109'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1109.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1109.jpg
  title: Importance sampling for unbiased on-demand evaluation of knowledge base population
  title_html: Importance sampling for unbiased on-demand evaluation of knowledge base
    population
  url: https://www.aclweb.org/anthology/D17-1109
  year: '2017'
D17-1110:
  abstract: "In order to adopt deep learning for information retrieval, models are\
    \ needed that can capture all relevant information required to assess the relevance\
    \ of a document to a given user query. While previous works have successfully\
    \ captured unigram term matches, how to fully employ position-dependent information\
    \ such as proximity and term dependencies has been insufficiently explored. In\
    \ this work, we propose a novel neural IR model named PACRR aiming at better modeling\
    \ position-dependent interactions between a query and a document. Extensive experiments\
    \ on six years\u2019 TREC Web Track data confirm that the proposed model yields\
    \ better results under multiple benchmarks."
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238235171
    type: video
    url: https://vimeo.com/238235171
  author:
  - first: Kai
    full: Kai Hui
    id: kai-hui
    last: Hui
  - first: Andrew
    full: Andrew Yates
    id: andrew-yates
    last: Yates
  - first: Klaus
    full: Klaus Berberich
    id: klaus-berberich
    last: Berberich
  - first: Gerard
    full: Gerard de Melo
    id: gerard-de-melo
    last: de Melo
  author_string: Kai Hui, Andrew Yates, Klaus Berberich, Gerard de Melo
  bibkey: hui-etal-2017-pacrr
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1110
  month: September
  page_first: '1049'
  page_last: '1058'
  pages: "1049\u20131058"
  paper_id: '110'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1110.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1110.jpg
  title: 'PACRR: A Position-Aware Neural IR Model for Relevance Matching'
  title_html: '<span class="acl-fixed-case">PACRR</span>: A Position-Aware Neural
    <span class="acl-fixed-case">IR</span> Model for Relevance Matching'
  url: https://www.aclweb.org/anthology/D17-1110
  year: '2017'
D17-1111:
  abstract: "Rapid progress has been made towards question answering (QA) systems\
    \ that can extract answers from text. Existing neural approaches make use of expensive\
    \ bi-directional attention mechanisms or score all possible answer spans, limiting\
    \ scalability. We propose instead to cast extractive QA as an iterative search\
    \ problem: select the answer\u2019s sentence, start word, and end word. This representation\
    \ reduces the space of each search step and allows computation to be conditionally\
    \ allocated to promising search paths. We show that globally normalizing the decision\
    \ process and back-propagating through beam search makes this representation viable\
    \ and learning efficient. We empirically demonstrate the benefits of this approach\
    \ using our model, Globally Normalized Reader (GNR), which achieves the second\
    \ highest single model performance on the Stanford Question Answering Dataset\
    \ (68.4 EM, 76.21 F1 dev) and is 24.7x faster than bi-attention-flow. We also\
    \ introduce a data-augmentation method to produce semantically valid examples\
    \ by aligning named entities to a knowledge base and swapping them with new entities\
    \ of the same type. This method improves the performance of all models considered\
    \ in this work and is of independent interest for a variety of NLP tasks."
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238233236
    type: video
    url: https://vimeo.com/238233236
  author:
  - first: Jonathan
    full: Jonathan Raiman
    id: jonathan-raiman
    last: Raiman
  - first: John
    full: John Miller
    id: john-miller
    last: Miller
  author_string: Jonathan Raiman, John Miller
  bibkey: raiman-miller-2017-globally
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1111
  month: September
  page_first: '1059'
  page_last: '1069'
  pages: "1059\u20131069"
  paper_id: '111'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1111.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1111.jpg
  title: Globally Normalized Reader
  title_html: Globally Normalized Reader
  url: https://www.aclweb.org/anthology/D17-1111
  year: '2017'
D17-1112:
  abstract: We present the first unsupervised LSTM speech segmenter as a cognitive
    model of the acquisition of words from unsegmented input. Cognitive biases toward
    phonological and syntactic predictability in speech are rooted in the limitations
    of human memory (Baddeley et al., 1998); compressed representations are easier
    to acquire and retain in memory. To model the biases introduced by these memory
    limitations, our system uses an LSTM-based encoder-decoder with a small number
    of hidden units, then searches for a segmentation that minimizes autoencoding
    loss. Linguistically meaningful segments (e.g. words) should share regular patterns
    of features that facilitate decoder performance in comparison to random segmentations,
    and we show that our learner discovers these patterns when trained on either phoneme
    sequences or raw acoustics. To our knowledge, ours is the first fully unsupervised
    system to be able to segment both symbolic and acoustic representations of speech.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1112.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1112.Attachment.zip
  - filename: https://vimeo.com/238235902
    type: video
    url: https://vimeo.com/238235902
  author:
  - first: Micha
    full: Micha Elsner
    id: micha-elsner
    last: Elsner
  - first: Cory
    full: Cory Shain
    id: cory-shain
    last: Shain
  author_string: Micha Elsner, Cory Shain
  bibkey: elsner-shain-2017-speech
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1112
  month: September
  page_first: '1070'
  page_last: '1080'
  pages: "1070\u20131080"
  paper_id: '112'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1112.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1112.jpg
  title: Speech segmentation with a neural encoder model of working memory
  title_html: Speech segmentation with a neural encoder model of working memory
  url: https://www.aclweb.org/anthology/D17-1112
  year: '2017'
D17-1113:
  abstract: Research in computational semantics is increasingly guided by our understanding
    of human semantic processing. However, semantic models are typically studied in
    the context of natural language processing system performance. In this paper,
    we present a systematic evaluation and comparison of a range of widely-used, state-of-the-art
    semantic models in their ability to predict patterns of conceptual representation
    in the human brain. Our results provide new insights both for the design of computational
    semantic models and for further research in cognitive neuroscience.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238235824
    type: video
    url: https://vimeo.com/238235824
  author:
  - first: Luana
    full: Luana Bulat
    id: luana-bulat
    last: Bulat
  - first: Stephen
    full: Stephen Clark
    id: stephen-clark
    last: Clark
  - first: Ekaterina
    full: Ekaterina Shutova
    id: ekaterina-shutova
    last: Shutova
  author_string: Luana Bulat, Stephen Clark, Ekaterina Shutova
  bibkey: bulat-etal-2017-speaking
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1113
  month: September
  page_first: '1081'
  page_last: '1091'
  pages: "1081\u20131091"
  paper_id: '113'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1113.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1113.jpg
  title: 'Speaking, Seeing, Understanding: Correlating semantic models with conceptual
    representation in the brain'
  title_html: 'Speaking, Seeing, Understanding: Correlating semantic models with conceptual
    representation in the brain'
  url: https://www.aclweb.org/anthology/D17-1113
  year: '2017'
D17-1114:
  abstract: The rapid increase of the multimedia data over the Internet necessitates
    multi-modal summarization from collections of text, image, audio and video. In
    this work, we propose an extractive Multi-modal Summarization (MMS) method which
    can automatically generate a textual summary given a set of documents, images,
    audios and videos related to a specific topic. The key idea is to bridge the semantic
    gaps between multi-modal contents. For audio information, we design an approach
    to selectively use its transcription. For vision information, we learn joint representations
    of texts and images using a neural network. Finally, all the multi-modal aspects
    are considered to generate the textural summary by maximizing the salience, non-redundancy,
    readability and coverage through budgeted optimization of submodular functions.
    We further introduce an MMS corpus in English and Chinese. The experimental results
    on this dataset demonstrate that our method outperforms other competitive baseline
    methods.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238234442
    type: video
    url: https://vimeo.com/238234442
  author:
  - first: Haoran
    full: Haoran Li
    id: haoran-li
    last: Li
  - first: Junnan
    full: Junnan Zhu
    id: junnan-zhu
    last: Zhu
  - first: Cong
    full: Cong Ma
    id: cong-ma
    last: Ma
  - first: Jiajun
    full: Jiajun Zhang
    id: jiajun-zhang
    last: Zhang
  - first: Chengqing
    full: Chengqing Zong
    id: chengqing-zong
    last: Zong
  author_string: Haoran Li, Junnan Zhu, Cong Ma, Jiajun Zhang, Chengqing Zong
  bibkey: li-etal-2017-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1114
  month: September
  page_first: '1092'
  page_last: '1102'
  pages: "1092\u20131102"
  paper_id: '114'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1114.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1114.jpg
  title: Multi-modal Summarization for Asynchronous Collection of Text, Image, Audio
    and Video
  title_html: Multi-modal Summarization for Asynchronous Collection of Text, Image,
    Audio and Video
  url: https://www.aclweb.org/anthology/D17-1114
  year: '2017'
D17-1115:
  abstract: Multimodal sentiment analysis is an increasingly popular research area,
    which extends the conventional language-based definition of sentiment analysis
    to a multimodal setup where other relevant modalities accompany language. In this
    paper, we pose the problem of multimodal sentiment analysis as modeling intra-modality
    and inter-modality dynamics. We introduce a novel model, termed Tensor Fusion
    Networks, which learns both such dynamics end-to-end. The proposed approach is
    tailored for the volatile nature of spoken language in online videos as well as
    accompanying gestures and voice. In the experiments, our model outperforms state-of-the-art
    approaches for both multimodal and unimodal sentiment analysis.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238236114
    type: video
    url: https://vimeo.com/238236114
  author:
  - first: Amir
    full: Amir Zadeh
    id: amir-zadeh
    last: Zadeh
  - first: Minghai
    full: Minghai Chen
    id: minghai-chen
    last: Chen
  - first: Soujanya
    full: Soujanya Poria
    id: soujanya-poria
    last: Poria
  - first: Erik
    full: Erik Cambria
    id: erik-cambria
    last: Cambria
  - first: Louis-Philippe
    full: Louis-Philippe Morency
    id: louis-philippe-morency
    last: Morency
  author_string: Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, Louis-Philippe
    Morency
  bibkey: zadeh-etal-2017-tensor
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1115
  month: September
  page_first: '1103'
  page_last: '1114'
  pages: "1103\u20131114"
  paper_id: '115'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1115.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1115.jpg
  title: Tensor Fusion Network for Multimodal Sentiment Analysis
  title_html: Tensor Fusion Network for Multimodal Sentiment Analysis
  url: https://www.aclweb.org/anthology/D17-1115
  year: '2017'
D17-1116:
  abstract: "Manual annotations are a prerequisite for many applications of machine\
    \ learning. However, weaknesses in the annotation process itself are easy to overlook.\
    \ In particular, scholars often choose what information to give to annotators\
    \ without examining these decisions empirically. For subjective tasks such as\
    \ sentiment analysis, sarcasm, and stance detection, such choices can impact results.\
    \ Here, for the task of political stance detection on Twitter, we show that providing\
    \ too little context can result in noisy and uncertain annotations, whereas providing\
    \ too strong a context may cause it to outweigh other signals. To characterize\
    \ and reduce these biases, we develop ConStance, a general model for reasoning\
    \ about annotations across information conditions. Given conflicting labels produced\
    \ by multiple annotators seeing the same instances with different contexts, ConStance\
    \ simultaneously estimates gold standard labels and also learns a classifier for\
    \ new instances. We show that the classifier learned by ConStance outperforms\
    \ a variety of baselines at predicting political stance, while the model\u2019\
    s interpretable parameters shed light on the effects of each context."
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1116.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1116.Attachment.zip
  - filename: https://vimeo.com/238231805
    type: video
    url: https://vimeo.com/238231805
  author:
  - first: Kenneth
    full: Kenneth Joseph
    id: kenneth-joseph
    last: Joseph
  - first: Lisa
    full: Lisa Friedland
    id: lisa-friedland
    last: Friedland
  - first: William
    full: William Hobbs
    id: william-hobbs
    last: Hobbs
  - first: David
    full: David Lazer
    id: david-lazer
    last: Lazer
  - first: Oren
    full: Oren Tsur
    id: oren-tsur
    last: Tsur
  author_string: Kenneth Joseph, Lisa Friedland, William Hobbs, David Lazer, Oren
    Tsur
  bibkey: joseph-etal-2017-constance
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1116
  month: September
  page_first: '1115'
  page_last: '1124'
  pages: "1115\u20131124"
  paper_id: '116'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1116.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1116.jpg
  title: 'ConStance: Modeling Annotation Contexts to Improve Stance Classification'
  title_html: '<span class="acl-fixed-case">C</span>on<span class="acl-fixed-case">S</span>tance:
    Modeling Annotation Contexts to Improve Stance Classification'
  url: https://www.aclweb.org/anthology/D17-1116
  year: '2017'
D17-1117:
  abstract: Experimenting with a new dataset of 1.6M user comments from a news portal
    and an existing dataset of 115K Wikipedia talk page comments, we show that an
    RNN operating on word embeddings outpeforms the previous state of the art in moderation,
    which used logistic regression or an MLP classifier with character or word n-grams.
    We also compare against a CNN operating on word embeddings, and a word-list baseline.
    A novel, deep, classificationspecific attention mechanism improves the performance
    of the RNN further, and can also highlight suspicious words for free, without
    including highlighted words in the training data. We consider both fully automatic
    and semi-automatic moderation.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238232251
    type: video
    url: https://vimeo.com/238232251
  author:
  - first: John
    full: John Pavlopoulos
    id: john-pavlopoulos
    last: Pavlopoulos
  - first: Prodromos
    full: Prodromos Malakasiotis
    id: prodromos-malakasiotis
    last: Malakasiotis
  - first: Ion
    full: Ion Androutsopoulos
    id: ion-androutsopoulos
    last: Androutsopoulos
  author_string: John Pavlopoulos, Prodromos Malakasiotis, Ion Androutsopoulos
  bibkey: pavlopoulos-etal-2017-deeper
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1117
  month: September
  page_first: '1125'
  page_last: '1135'
  pages: "1125\u20131135"
  paper_id: '117'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1117.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1117.jpg
  title: Deeper Attention to Abusive User Content Moderation
  title_html: Deeper Attention to Abusive User Content Moderation
  url: https://www.aclweb.org/anthology/D17-1117
  year: '2017'
D17-1118:
  abstract: 'This article evaluates three proposed laws of semantic change. Our claim
    is that in order to validate a putative law of semantic change, the effect should
    be observed in the genuine condition but absent or reduced in a suitably matched
    control condition, in which no change can possibly have taken place. Our analysis
    shows that the effects reported in recent literature must be substantially revised:
    (i) the proposed negative correlation between meaning change and word frequency
    is shown to be largely an artefact of the models of word representation used;
    (ii) the proposed negative correlation between meaning change and prototypicality
    is shown to be much weaker than what has been claimed in prior art; and (iii)
    the proposed positive correlation between meaning change and polysemy is largely
    an artefact of word frequency. These empirical observations are corroborated by
    analytical proofs that show that count representations introduce an inherent dependence
    on word frequency, and thus word frequency cannot be evaluated as an independent
    factor with these representations.'
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238235126
    type: video
    url: https://vimeo.com/238235126
  author:
  - first: Haim
    full: Haim Dubossarsky
    id: haim-dubossarsky
    last: Dubossarsky
  - first: Daphna
    full: Daphna Weinshall
    id: daphna-weinshall
    last: Weinshall
  - first: Eitan
    full: Eitan Grossman
    id: eitan-grossman
    last: Grossman
  author_string: Haim Dubossarsky, Daphna Weinshall, Eitan Grossman
  bibkey: dubossarsky-etal-2017-outta
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1118
  month: September
  page_first: '1136'
  page_last: '1145'
  pages: "1136\u20131145"
  paper_id: '118'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1118.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1118.jpg
  title: 'Outta Control: Laws of Semantic Change and Inherent Biases in Word Representation
    Models'
  title_html: 'Outta Control: Laws of Semantic Change and Inherent Biases in Word
    Representation Models'
  url: https://www.aclweb.org/anthology/D17-1118
  year: '2017'
D17-1119:
  abstract: "We pose the general task of user-factor adaptation \u2013 adapting supervised\
    \ learning models to real-valued user factors inferred from a background of their\
    \ language, reflecting the idea that a piece of text should be understood within\
    \ the context of the user that wrote it. We introduce a continuous adaptation\
    \ technique, suited for real-valued user factors that are common in social science\
    \ and bringing us closer to personalized NLP, adapting to each user uniquely.\
    \ We apply this technique with known user factors including age, gender, and personality\
    \ traits, as well as latent factors, evaluating over five tasks: POS tagging,\
    \ PP-attachment, sentiment analysis, sarcasm detection, and stance detection.\
    \ Adaptation provides statistically significant benefits for 3 of the 5 tasks:\
    \ up to +1.2 points for PP-attachment, +3.4 points for sarcasm, and +3.0 points\
    \ for stance."
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238233454
    type: video
    url: https://vimeo.com/238233454
  author:
  - first: Veronica
    full: Veronica Lynn
    id: veronica-lynn
    last: Lynn
  - first: Youngseo
    full: Youngseo Son
    id: youngseo-son
    last: Son
  - first: Vivek
    full: Vivek Kulkarni
    id: vivek-kulkarni
    last: Kulkarni
  - first: Niranjan
    full: Niranjan Balasubramanian
    id: niranjan-balasubramanian
    last: Balasubramanian
  - first: H. Andrew
    full: H. Andrew Schwartz
    id: h-andrew-schwartz
    last: Schwartz
  author_string: Veronica Lynn, Youngseo Son, Vivek Kulkarni, Niranjan Balasubramanian,
    H. Andrew Schwartz
  bibkey: lynn-etal-2017-human
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1119
  month: September
  page_first: '1146'
  page_last: '1155'
  pages: "1146\u20131155"
  paper_id: '119'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1119.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1119.jpg
  title: Human Centered NLP with User-Factor Adaptation
  title_html: Human Centered <span class="acl-fixed-case">NLP</span> with User-Factor
    Adaptation
  url: https://www.aclweb.org/anthology/D17-1119
  year: '2017'
D17-1120:
  abstract: 'Word Sense Disambiguation models exist in many flavors. Even though supervised
    ones tend to perform best in terms of accuracy, they often lose ground to more
    flexible knowledge-based solutions, which do not require training by a word expert
    for every disambiguation target. To bridge this gap we adopt a different perspective
    and rely on sequence learning to frame the disambiguation problem: we propose
    and study in depth a series of end-to-end neural architectures directly tailored
    to the task, from bidirectional Long Short-Term Memory to encoder-decoder models.
    Our extensive evaluation over standard benchmarks and in multiple languages shows
    that sequence learning enables more versatile all-words models that consistently
    lead to state-of-the-art results, even against word experts with engineered features.'
  address: Copenhagen, Denmark
  author:
  - first: Alessandro
    full: Alessandro Raganato
    id: alessandro-raganato
    last: Raganato
  - first: Claudio
    full: Claudio Delli Bovi
    id: claudio-delli-bovi
    last: Delli Bovi
  - first: Roberto
    full: Roberto Navigli
    id: roberto-navigli
    last: Navigli
  author_string: Alessandro Raganato, Claudio Delli Bovi, Roberto Navigli
  bibkey: raganato-etal-2017-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1120
  month: September
  page_first: '1156'
  page_last: '1167'
  pages: "1156\u20131167"
  paper_id: '120'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1120.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1120.jpg
  title: Neural Sequence Learning Models for Word Sense Disambiguation
  title_html: Neural Sequence Learning Models for Word Sense Disambiguation
  url: https://www.aclweb.org/anthology/D17-1120
  year: '2017'
D17-1121:
  abstract: "Search systems are often focused on providing relevant results for the\
    \ \u201Cnow\u201D, assuming both corpora and user needs that focus on the present.\
    \ However, many corpora today reflect significant longitudinal collections ranging\
    \ from 20 years of the Web to hundreds of years of digitized newspapers and books.\
    \ Understanding the temporal intent of the user and retrieving the most relevant\
    \ historical content has become a significant challenge. Common search features,\
    \ such as query expansion, leverage the relationship between terms but cannot\
    \ function well across all times when relationships vary temporally. In this work,\
    \ we introduce a temporal relationship model that is extracted from longitudinal\
    \ data collections. The model supports the task of identifying, given two words,\
    \ when they relate to each other. We present an algorithmic framework for this\
    \ task and show its application for the task of query expansion, achieving high\
    \ gain."
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1121.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1121.Attachment.zip
  author:
  - first: Guy D.
    full: Guy D. Rosin
    id: guy-d-rosin
    last: Rosin
  - first: Eytan
    full: Eytan Adar
    id: eytan-adar
    last: Adar
  - first: Kira
    full: Kira Radinsky
    id: kira-radinsky
    last: Radinsky
  author_string: Guy D. Rosin, Eytan Adar, Kira Radinsky
  bibkey: rosin-etal-2017-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1121
  month: September
  page_first: '1168'
  page_last: '1178'
  pages: "1168\u20131178"
  paper_id: '121'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1121.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1121.jpg
  title: Learning Word Relatedness over Time
  title_html: Learning Word Relatedness over Time
  url: https://www.aclweb.org/anthology/D17-1121
  year: '2017'
D17-1122:
  abstract: Sentence pair modeling is a crucial problem in the field of natural language
    processing. In this paper, we propose a model to measure the similarity of a sentence
    pair focusing on the interaction information. We utilize the word level similarity
    matrix to discover fine-grained alignment of two sentences. It should be emphasized
    that each word in a sentence has a different importance from the perspective of
    semantic composition, so we exploit two novel and efficient strategies to explicitly
    calculate a weight for each word. Although the proposed model only use a sequential
    LSTM for sentence modeling without any external resource such as syntactic parser
    tree and additional lexicon features, experimental results show that our model
    achieves state-of-the-art performance on three datasets of two tasks.
  address: Copenhagen, Denmark
  author:
  - first: Gehui
    full: Gehui Shen
    id: gehui-shen
    last: Shen
  - first: Yunlun
    full: Yunlun Yang
    id: yunlun-yang
    last: Yang
  - first: Zhi-Hong
    full: Zhi-Hong Deng
    id: zhi-hong-deng
    last: Deng
  author_string: Gehui Shen, Yunlun Yang, Zhi-Hong Deng
  bibkey: shen-etal-2017-inter
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1122
  month: September
  page_first: '1179'
  page_last: '1189'
  pages: "1179\u20131189"
  paper_id: '122'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1122.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1122.jpg
  title: Inter-Weighted Alignment Network for Sentence Pair Modeling
  title_html: Inter-Weighted Alignment Network for Sentence Pair Modeling
  url: https://www.aclweb.org/anthology/D17-1122
  year: '2017'
D17-1123:
  abstract: A taxonomy is a semantic hierarchy, consisting of concepts linked by is-a
    relations. While a large number of taxonomies have been constructed from human-compiled
    resources (e.g., Wikipedia), learning taxonomies from text corpora has received
    a growing interest and is essential for long-tailed and domain-specific knowledge
    acquisition. In this paper, we overview recent advances on taxonomy construction
    from free texts, reorganizing relevant subtasks into a complete framework. We
    also overview resources for evaluation and discuss challenges for future research.
  address: Copenhagen, Denmark
  author:
  - first: Chengyu
    full: Chengyu Wang
    id: chengyu-wang
    last: Wang
  - first: Xiaofeng
    full: Xiaofeng He
    id: xiaofeng-he
    last: He
  - first: Aoying
    full: Aoying Zhou
    id: aoying-zhou
    last: Zhou
  author_string: Chengyu Wang, Xiaofeng He, Aoying Zhou
  bibkey: wang-etal-2017-short
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1123
  month: September
  page_first: '1190'
  page_last: '1203'
  pages: "1190\u20131203"
  paper_id: '123'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1123.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1123.jpg
  title: 'A Short Survey on Taxonomy Learning from Text Corpora: Issues, Resources
    and Recent Advances'
  title_html: 'A Short Survey on Taxonomy Learning from Text Corpora: Issues, Resources
    and Recent Advances'
  url: https://www.aclweb.org/anthology/D17-1123
  year: '2017'
D17-1124:
  abstract: Idioms are peculiar linguistic constructions that impose great challenges
    for representing the semantics of language, especially in current prevailing end-to-end
    neural models, which assume that the semantics of a phrase or sentence can be
    literally composed from its constitutive words. In this paper, we propose an idiom-aware
    distributed semantic model to build representation of sentences on the basis of
    understanding their contained idioms. Our models are grounded in the literal-first
    psycholinguistic hypothesis, which can adaptively learn semantic compositionality
    of a phrase literally or idiomatically. To better evaluate our models, we also
    construct an idiom-enriched sentiment classification dataset with considerable
    scale and abundant peculiarities of idioms. The qualitative and quantitative experimental
    analyses demonstrate the efficacy of our models.
  address: Copenhagen, Denmark
  author:
  - first: Pengfei
    full: Pengfei Liu
    id: pengfei-liu
    last: Liu
  - first: Kaiyu
    full: Kaiyu Qian
    id: kaiyu-qian
    last: Qian
  - first: Xipeng
    full: Xipeng Qiu
    id: xipeng-qiu
    last: Qiu
  - first: Xuanjing
    full: Xuanjing Huang
    id: xuan-jing-huang
    last: Huang
  author_string: Pengfei Liu, Kaiyu Qian, Xipeng Qiu, Xuanjing Huang
  bibkey: liu-etal-2017-idiom
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1124
  month: September
  page_first: '1204'
  page_last: '1213'
  pages: "1204\u20131213"
  paper_id: '124'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1124.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1124.jpg
  title: Idiom-Aware Compositional Distributed Semantics
  title_html: Idiom-Aware Compositional Distributed Semantics
  url: https://www.aclweb.org/anthology/D17-1124
  year: '2017'
D17-1125:
  abstract: To learn a semantic parser from denotations, a learning algorithm must
    search over a combinatorially large space of logical forms for ones consistent
    with the annotated denotations. We propose a new online learning algorithm that
    searches faster as training progresses. The two key ideas are using macro grammars
    to cache the abstract patterns of useful logical forms found thus far, and holistic
    triggering to efficiently retrieve the most relevant patterns based on sentence
    similarity. On the WikiTableQuestions dataset, we first expand the search space
    of an existing model to improve the state-of-the-art accuracy from 38.7% to 42.7%,
    and then use macro grammars and holistic triggering to achieve an 11x speedup
    and an accuracy of 43.7%.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1125.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1125.Attachment.zip
  author:
  - first: Yuchen
    full: Yuchen Zhang
    id: yuchen-zhang
    last: Zhang
  - first: Panupong
    full: Panupong Pasupat
    id: panupong-pasupat
    last: Pasupat
  - first: Percy
    full: Percy Liang
    id: percy-liang
    last: Liang
  author_string: Yuchen Zhang, Panupong Pasupat, Percy Liang
  bibkey: zhang-etal-2017-macro
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1125
  month: September
  page_first: '1214'
  page_last: '1223'
  pages: "1214\u20131223"
  paper_id: '125'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1125.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1125.jpg
  title: Macro Grammars and Holistic Triggering for Efficient Semantic Parsing
  title_html: Macro Grammars and Holistic Triggering for Efficient Semantic Parsing
  url: https://www.aclweb.org/anthology/D17-1125
  year: '2017'
D17-1126:
  abstract: A major challenge in paraphrase research is the lack of parallel corpora.
    In this paper, we present a new method to collect large-scale sentential paraphrases
    from Twitter by linking tweets through shared URLs. The main advantage of our
    method is its simplicity, as it gets rid of the classifier or human in the loop
    needed to select data before annotation and subsequent application of paraphrase
    identification algorithms in the previous work. We present the largest human-labeled
    paraphrase corpus to date of 51,524 sentence pairs and the first cross-domain
    benchmarking for automatic paraphrase identification. In addition, we show that
    more than 30,000 new sentential paraphrases can be easily and continuously captured
    every month at ~70% precision, and demonstrate their utility for downstream NLP
    tasks through phrasal paraphrase extraction. We make our code and data freely
    available.
  address: Copenhagen, Denmark
  author:
  - first: Wuwei
    full: Wuwei Lan
    id: wuwei-lan
    last: Lan
  - first: Siyu
    full: Siyu Qiu
    id: siyu-qiu
    last: Qiu
  - first: Hua
    full: Hua He
    id: hua-he
    last: He
  - first: Wei
    full: Wei Xu
    id: wei-xu
    last: Xu
  author_string: Wuwei Lan, Siyu Qiu, Hua He, Wei Xu
  bibkey: lan-etal-2017-continuously
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1126
  month: September
  page_first: '1224'
  page_last: '1234'
  pages: "1224\u20131234"
  paper_id: '126'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1126.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1126.jpg
  title: A Continuously Growing Dataset of Sentential Paraphrases
  title_html: A Continuously Growing Dataset of Sentential Paraphrases
  url: https://www.aclweb.org/anthology/D17-1126
  year: '2017'
D17-1127:
  abstract: 'Existing studies on semantic parsing mainly focus on the in-domain setting.
    We formulate cross-domain semantic parsing as a domain adaptation problem: train
    a semantic parser on some source domains and then adapt it to the target domain.
    Due to the diversity of logical forms in different domains, this problem presents
    unique and intriguing challenges. By converting logical forms into canonical utterances
    in natural language, we reduce semantic parsing to paraphrasing, and develop an
    attentive sequence-to-sequence paraphrase model that is general and flexible to
    adapt to different domains. We discover two problems, small micro variance and
    large macro variance, of pre-trained word embeddings that hinder their direct
    use in neural networks, and propose standardization techniques as a remedy. On
    the popular Overnight dataset, which contains eight domains, we show that both
    cross-domain training and standardized pre-trained word embeddings can bring significant
    improvement.'
  address: Copenhagen, Denmark
  author:
  - first: Yu
    full: Yu Su
    id: yu-su
    last: Su
  - first: Xifeng
    full: Xifeng Yan
    id: xifeng-yan
    last: Yan
  author_string: Yu Su, Xifeng Yan
  bibkey: su-yan-2017-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1127
  month: September
  page_first: '1235'
  page_last: '1246'
  pages: "1235\u20131246"
  paper_id: '127'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1127.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1127.jpg
  title: Cross-domain Semantic Parsing via Paraphrasing
  title_html: Cross-domain Semantic Parsing via Paraphrasing
  url: https://www.aclweb.org/anthology/D17-1127
  year: '2017'
D17-1128:
  abstract: We introduce a new method for frame-semantic parsing that significantly
    improves the prior state of the art. Our model leverages the advantages of a deep
    bidirectional LSTM network which predicts semantic role labels word by word and
    a relational network which predicts semantic roles for individual text expressions
    in relation to a predicate. The two networks are integrated into a single model
    via knowledge distillation, and a unified graphical model is employed to jointly
    decode frames and semantic roles during inference. Experiments on the standard
    FrameNet data show that our model significantly outperforms existing neural and
    non-neural approaches, achieving a 5.7 F1 gain over the current state of the art,
    for full frame structure extraction.
  address: Copenhagen, Denmark
  author:
  - first: Bishan
    full: Bishan Yang
    id: bishan-yang
    last: Yang
  - first: Tom
    full: Tom Mitchell
    id: tom-mitchell
    last: Mitchell
  author_string: Bishan Yang, Tom Mitchell
  bibkey: yang-mitchell-2017-joint
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1128
  month: September
  page_first: '1247'
  page_last: '1256'
  pages: "1247\u20131256"
  paper_id: '128'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1128.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1128.jpg
  title: A Joint Sequential and Relational Model for Frame-Semantic Parsing
  title_html: A Joint Sequential and Relational Model for Frame-Semantic Parsing
  url: https://www.aclweb.org/anthology/D17-1128
  year: '2017'
D17-1129:
  abstract: 'This paper proposes to tackle the AMR parsing bottleneck by improving
    two components of an AMR parser: concept identification and alignment. We first
    build a Bidirectional LSTM based concept identifier that is able to incorporate
    richer contextual information to learn sparse AMR concept labels. We then extend
    an HMM-based word-to-concept alignment model with graph distance distortion and
    a rescoring method during decoding to incorporate the structural information in
    the AMR graph. We show integrating the two components into an existing AMR parser
    results in consistently better performance over the state of the art on various
    datasets.'
  address: Copenhagen, Denmark
  author:
  - first: Chuan
    full: Chuan Wang
    id: chuan-wang
    last: Wang
  - first: Nianwen
    full: Nianwen Xue
    id: nianwen-xue
    last: Xue
  author_string: Chuan Wang, Nianwen Xue
  bibkey: wang-xue-2017-getting
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1129
  month: September
  page_first: '1257'
  page_last: '1268'
  pages: "1257\u20131268"
  paper_id: '129'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1129.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1129.jpg
  title: Getting the Most out of AMR Parsing
  title_html: Getting the Most out of <span class="acl-fixed-case">AMR</span> Parsing
  url: https://www.aclweb.org/anthology/D17-1129
  year: '2017'
D17-1130:
  abstract: We present a transition-based AMR parser that directly generates AMR parses
    from plain text. We use Stack-LSTMs to represent our parser state and make decisions
    greedily. In our experiments, we show that our parser achieves very competitive
    scores on English using only AMR training data. Adding additional information,
    such as POS tags and dependency trees, improves the results further.
  address: Copenhagen, Denmark
  author:
  - first: Miguel
    full: Miguel Ballesteros
    id: miguel-ballesteros
    last: Ballesteros
  - first: Yaser
    full: Yaser Al-Onaizan
    id: yaser-al-onaizan
    last: Al-Onaizan
  author_string: Miguel Ballesteros, Yaser Al-Onaizan
  bibkey: ballesteros-al-onaizan-2017-amr
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1130
  month: September
  page_first: '1269'
  page_last: '1275'
  pages: "1269\u20131275"
  paper_id: '130'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1130.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1130.jpg
  title: AMR Parsing using Stack-LSTMs
  title_html: <span class="acl-fixed-case">AMR</span> Parsing using Stack-<span class="acl-fixed-case">LSTM</span>s
  url: https://www.aclweb.org/anthology/D17-1130
  year: '2017'
D17-1131:
  abstract: Given a question and a set of answer candidates, answer triggering determines
    whether the candidate set contains any correct answers. If yes, it then outputs
    a correct one. In contrast to existing pipeline methods which first consider individual
    candidate answers separately and then make a prediction based on a threshold,
    we propose an end-to-end deep neural network framework, which is trained by a
    novel group-level objective function that directly optimizes the answer triggering
    performance. Our objective function penalizes three potential types of error and
    allows training the framework in an end-to-end manner. Experimental results on
    the WikiQA benchmark show that our framework outperforms the state of the arts
    by a 6.6% absolute gain under F1 measure.
  address: Copenhagen, Denmark
  author:
  - first: Jie
    full: Jie Zhao
    id: jie-zhao
    last: Zhao
  - first: Yu
    full: Yu Su
    id: yu-su
    last: Su
  - first: Ziyu
    full: Ziyu Guan
    id: ziyu-guan
    last: Guan
  - first: Huan
    full: Huan Sun
    id: huan-sun
    last: Sun
  author_string: Jie Zhao, Yu Su, Ziyu Guan, Huan Sun
  bibkey: zhao-etal-2017-end
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1131
  month: September
  page_first: '1276'
  page_last: '1282'
  pages: "1276\u20131282"
  paper_id: '131'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1131.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1131.jpg
  title: An End-to-End Deep Framework for Answer Triggering with a Novel Group-Level
    Objective
  title_html: An End-to-End Deep Framework for Answer Triggering with a Novel Group-Level
    Objective
  url: https://www.aclweb.org/anthology/D17-1131
  year: '2017'
D17-1132:
  abstract: This paper looks at the task of predicting word association strengths
    across three datasets; WordNet Evocation (Boyd-Graber et al., 2006), University
    of Southern Florida Free Association norms (Nelson et al., 2004), and Edinburgh
    Associative Thesaurus (Kiss et al., 1973). We achieve results of r=0.357 and p=0.379,
    r=0.344 and p=0.300, and r=0.292 and p=0.363, respectively. We find Word2Vec (Mikolov
    et al., 2013) and GloVe (Pennington et al., 2014) cosine similarities, as well
    as vector offsets, to be the highest performing features. Furthermore, we examine
    the usefulness of Gaussian embeddings (Vilnis and McCallum, 2014) for predicting
    word association strength, the first work to do so.
  address: Copenhagen, Denmark
  author:
  - first: Andrew
    full: Andrew Cattle
    id: andrew-cattle
    last: Cattle
  - first: Xiaojuan
    full: Xiaojuan Ma
    id: xiaojuan-ma
    last: Ma
  author_string: Andrew Cattle, Xiaojuan Ma
  bibkey: cattle-ma-2017-predicting
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1132
  month: September
  page_first: '1283'
  page_last: '1288'
  pages: "1283\u20131288"
  paper_id: '132'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1132.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1132.jpg
  title: Predicting Word Association Strengths
  title_html: Predicting Word Association Strengths
  url: https://www.aclweb.org/anthology/D17-1132
  year: '2017'
D17-1133:
  abstract: 'Recent advances in RST discourse parsing have focused on two modeling
    paradigms: (a) high order parsers which jointly predict the tree structure of
    the discourse and the relations it encodes; or (b) linear-time parsers which are
    efficient but mostly based on local features. In this work, we propose a linear-time
    parser with a novel way of representing discourse constituents based on neural
    networks which takes into account global contextual information and is able to
    capture long-distance dependencies. Experimental results show that our parser
    obtains state-of-the art performance on benchmark datasets, while being efficient
    (with time complexity linear in the number of sentences in the document) and requiring
    minimal feature engineering.'
  address: Copenhagen, Denmark
  author:
  - first: Yang
    full: Yang Liu
    id: yang-liu-edinburgh
    last: Liu
  - first: Mirella
    full: Mirella Lapata
    id: mirella-lapata
    last: Lapata
  author_string: Yang Liu, Mirella Lapata
  bibkey: liu-lapata-2017-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1133
  month: September
  page_first: '1289'
  page_last: '1298'
  pages: "1289\u20131298"
  paper_id: '133'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1133.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1133.jpg
  title: Learning Contextually Informed Representations for Linear-Time Discourse
    Parsing
  title_html: Learning Contextually Informed Representations for Linear-Time Discourse
    Parsing
  url: https://www.aclweb.org/anthology/D17-1133
  year: '2017'
D17-1134:
  abstract: We present a novel multi-task attention based neural network model to
    address implicit discourse relationship representation and identification through
    two types of representation learning, an attention based neural network for learning
    discourse relationship representation with two arguments and a multi-task framework
    for learning knowledge from annotated and unannotated corpora. The extensive experiments
    have been performed on two benchmark corpora (i.e., PDTB and CoNLL-2016 datasets).
    Experimental results show that our proposed model outperforms the state-of-the-art
    systems on benchmark corpora.
  address: Copenhagen, Denmark
  author:
  - first: Man
    full: Man Lan
    id: man-lan
    last: Lan
  - first: Jianxiang
    full: Jianxiang Wang
    id: jianxiang-wang
    last: Wang
  - first: Yuanbin
    full: Yuanbin Wu
    id: yuanbin-wu
    last: Wu
  - first: Zheng-Yu
    full: Zheng-Yu Niu
    id: zheng-yu-niu
    last: Niu
  - first: Haifeng
    full: Haifeng Wang
    id: haifeng-wang
    last: Wang
  author_string: Man Lan, Jianxiang Wang, Yuanbin Wu, Zheng-Yu Niu, Haifeng Wang
  bibkey: lan-etal-2017-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1134
  month: September
  page_first: '1299'
  page_last: '1308'
  pages: "1299\u20131308"
  paper_id: '134'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1134.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1134.jpg
  title: Multi-task Attention-based Neural Networks for Implicit Discourse Relationship
    Representation and Identification
  title_html: Multi-task Attention-based Neural Networks for Implicit Discourse Relationship
    Representation and Identification
  url: https://www.aclweb.org/anthology/D17-1134
  year: '2017'
D17-1135:
  abstract: Existing approaches for Chinese zero pronoun resolution typically utilize
    only syntactical and lexical features while ignoring semantic information. The
    fundamental reason is that zero pronouns have no descriptive information, which
    brings difficulty in explicitly capturing their semantic similarities with antecedents.
    Meanwhile, representing zero pronouns is challenging since they are merely gaps
    that convey no actual content. In this paper, we address this issue by building
    a deep memory network that is capable of encoding zero pronouns into vector representations
    with information obtained from their contexts and potential antecedents. Consequently,
    our resolver takes advantage of semantic information by using these continuous
    distributed representations. Experiments on the OntoNotes 5.0 dataset show that
    the proposed memory network could substantially outperform the state-of-the-art
    systems in various experimental settings.
  address: Copenhagen, Denmark
  author:
  - first: Qingyu
    full: Qingyu Yin
    id: qingyu-yin
    last: Yin
  - first: Yu
    full: Yu Zhang
    id: yu-zhang
    last: Zhang
  - first: Weinan
    full: Weinan Zhang
    id: weinan-zhang
    last: Zhang
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  author_string: Qingyu Yin, Yu Zhang, Weinan Zhang, Ting Liu
  bibkey: yin-etal-2017-chinese
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1135
  month: September
  page_first: '1309'
  page_last: '1318'
  pages: "1309\u20131318"
  paper_id: '135'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1135.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1135.jpg
  title: Chinese Zero Pronoun Resolution with Deep Memory Network
  title_html: <span class="acl-fixed-case">C</span>hinese Zero Pronoun Resolution
    with Deep Memory Network
  url: https://www.aclweb.org/anthology/D17-1135
  year: '2017'
D17-1136:
  abstract: This article evaluates purported progress over the past years in RST discourse
    parsing. Several studies report a relative error reduction of 24 to 51% on all
    metrics that authors attribute to the introduction of distributed representations
    of discourse units. We replicate the standard evaluation of 9 parsers, 5 of which
    use distributed representations, from 8 studies published between 2013 and 2017,
    using their predictions on the test set of the RST-DT. Our main finding is that
    most recently reported increases in RST discourse parser performance are an artefact
    of differences in implementations of the evaluation procedure. We evaluate all
    these parsers with the standard Parseval procedure to provide a more accurate
    picture of the actual RST discourse parsers performance in standard evaluation
    settings. Under this more stringent procedure, the gains attributable to distributed
    representations represent at most a 16% relative error reduction on fully-labelled
    structures.
  address: Copenhagen, Denmark
  author:
  - first: Mathieu
    full: Mathieu Morey
    id: mathieu-morey
    last: Morey
  - first: Philippe
    full: Philippe Muller
    id: philippe-muller
    last: Muller
  - first: Nicholas
    full: Nicholas Asher
    id: nicholas-asher
    last: Asher
  author_string: Mathieu Morey, Philippe Muller, Nicholas Asher
  bibkey: morey-etal-2017-much
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1136
  month: September
  page_first: '1319'
  page_last: '1324'
  pages: "1319\u20131324"
  paper_id: '136'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1136.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1136.jpg
  title: How much progress have we made on RST discourse parsing? A replication study
    of recent results on the RST-DT
  title_html: How much progress have we made on <span class="acl-fixed-case">RST</span>
    discourse parsing? A replication study of recent results on the <span class="acl-fixed-case">RST</span>-<span
    class="acl-fixed-case">DT</span>
  url: https://www.aclweb.org/anthology/D17-1136
  year: '2017'
D17-1137:
  abstract: "In this paper, we address the problem of predicting one of three functions\
    \ for the English pronoun \u2018it\u2019: anaphoric, event reference or pleonastic.\
    \ This disambiguation is valuable in the context of machine translation and coreference\
    \ resolution. We present experiments using a MAXENT classifier trained on gold-standard\
    \ data and self-training experiments of an RNN trained on silver-standard data,\
    \ annotated using the MAXENT classifier. Lastly, we report on an analysis of the\
    \ strengths of these two models."
  address: Copenhagen, Denmark
  author:
  - first: Sharid
    full: "Sharid Lo\xE1iciga"
    id: sharid-loaiciga
    last: "Lo\xE1iciga"
  - first: Liane
    full: Liane Guillou
    id: liane-guillou
    last: Guillou
  - first: Christian
    full: Christian Hardmeier
    id: christian-hardmeier
    last: Hardmeier
  author_string: "Sharid Lo\xE1iciga, Liane Guillou, Christian Hardmeier"
  bibkey: loaiciga-etal-2017-disambiguating
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1137
  month: September
  page_first: '1325'
  page_last: '1331'
  pages: "1325\u20131331"
  paper_id: '137'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1137.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1137.jpg
  title: "What is it? Disambiguating the different readings of the pronoun \u2018\
    it\u2019"
  title_html: "What is it? Disambiguating the different readings of the pronoun \u2018\
    it\u2019"
  url: https://www.aclweb.org/anthology/D17-1137
  year: '2017'
D17-1138:
  abstract: Selectional preferences have long been claimed to be essential for coreference
    resolution. However, they are modeled only implicitly by current coreference resolvers.
    We propose a dependency-based embedding model of selectional preferences which
    allows fine-grained compatibility judgments with high coverage. Incorporating
    our model improves performance, matching state-of-the-art results of a more complex
    system. However, it comes with a cost that makes it debatable how worthwhile are
    such improvements.
  address: Copenhagen, Denmark
  author:
  - first: Benjamin
    full: Benjamin Heinzerling
    id: benjamin-heinzerling
    last: Heinzerling
  - first: Nafise Sadat
    full: Nafise Sadat Moosavi
    id: nafise-sadat-moosavi
    last: Moosavi
  - first: Michael
    full: Michael Strube
    id: michael-strube
    last: Strube
  author_string: Benjamin Heinzerling, Nafise Sadat Moosavi, Michael Strube
  bibkey: heinzerling-etal-2017-revisiting
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1138
  month: September
  page_first: '1332'
  page_last: '1339'
  pages: "1332\u20131339"
  paper_id: '138'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1138.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1138.jpg
  title: Revisiting Selectional Preferences for Coreference Resolution
  title_html: Revisiting Selectional Preferences for Coreference Resolution
  url: https://www.aclweb.org/anthology/D17-1138
  year: '2017'
D17-1139:
  abstract: "Topic segmentation plays an important role for discourse parsing and\
    \ information retrieval. Due to the absence of training data, previous work mainly\
    \ adopts unsupervised methods to rank semantic coherence between paragraphs for\
    \ topic segmentation. In this paper, we present an intuitive and simple idea to\
    \ automatically create a \u201Cquasi\u201D training dataset, which includes a\
    \ large amount of text pairs from the same or different documents with different\
    \ semantic coherence. With the training corpus, we design a symmetric CNN neural\
    \ network to model text pairs and rank the semantic coherence within the learning\
    \ to rank framework. Experiments show that our algorithm is able to achieve competitive\
    \ performance over strong baselines on several real-world datasets."
  address: Copenhagen, Denmark
  author:
  - first: Liang
    full: Liang Wang
    id: liang-wang
    last: Wang
  - first: Sujian
    full: Sujian Li
    id: sujian-li
    last: Li
  - first: Yajuan
    full: Yajuan Lv
    id: yajuan-lu
    last: Lv
  - first: Houfeng
    full: Houfeng Wang
    id: houfeng-wang
    last: Wang
  author_string: Liang Wang, Sujian Li, Yajuan Lv, Houfeng Wang
  bibkey: wang-etal-2017-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1139
  month: September
  page_first: '1340'
  page_last: '1344'
  pages: "1340\u20131344"
  paper_id: '139'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1139.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1139.jpg
  title: Learning to Rank Semantic Coherence for Topic Segmentation
  title_html: Learning to Rank Semantic Coherence for Topic Segmentation
  url: https://www.aclweb.org/anthology/D17-1139
  year: '2017'
D17-1140:
  abstract: 'GRASP (GReedy Augmented Sequential Patterns) is an algorithm for automatically
    extracting patterns that characterize subtle linguistic phenomena. To that end,
    GRASP augments each term of input text with multiple layers of linguistic information.
    These different facets of the text terms are systematically combined to reveal
    rich patterns. We report highly promising experimental results in several challenging
    text analysis tasks within the field of Argumentation Mining. We believe that
    GRASP is general enough to be useful for other domains too. For example, each
    of the following sentences includes a claim for a [topic]: 1. Opponents often
    argue that the open primary is unconstitutional. [Open Primaries] 2. Prof. Smith
    suggested that affirmative action devalues the accomplishments of the chosen.
    [Affirmative Action] 3. The majority stated that the First Amendment does not
    guarantee the right to offend others. [Freedom of Speech] These sentences share
    almost no words in common, however, they are similar at a more abstract level.
    A human observer may notice the following underlying common structure, or pattern:
    [someone][argue/suggest/state][that][topic term][sentiment term]. GRASP aims to
    automatically capture such underlying structures of the given data. For the above
    examples it finds the pattern [noun][express][that][noun,topic][sentiment], where
    [express] stands for all its (in)direct hyponyms, and [noun,topic] means a noun
    which is also related to the topic.'
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1140.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1140.Attachment.zip
  author:
  - first: Eyal
    full: Eyal Shnarch
    id: eyal-shnarch
    last: Shnarch
  - first: Ran
    full: Ran Levy
    id: ran-levy
    last: Levy
  - first: Vikas
    full: Vikas Raykar
    id: vikas-raykar
    last: Raykar
  - first: Noam
    full: Noam Slonim
    id: noam-slonim
    last: Slonim
  author_string: Eyal Shnarch, Ran Levy, Vikas Raykar, Noam Slonim
  bibkey: shnarch-etal-2017-grasp
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1140
  month: September
  page_first: '1345'
  page_last: '1350'
  pages: "1345\u20131350"
  paper_id: '140'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1140.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1140.jpg
  title: 'GRASP: Rich Patterns for Argumentation Mining'
  title_html: '<span class="acl-fixed-case">GRASP</span>: Rich Patterns for Argumentation
    Mining'
  url: https://www.aclweb.org/anthology/D17-1140
  year: '2017'
D17-1141:
  abstract: "This paper presents an analysis of argumentation strategies in news editorials\
    \ within and across topics. Given nearly 29,000 argumentative editorials from\
    \ the New York Times, we develop two machine learning models, one for determining\
    \ an editorial\u2019s topic, and one for identifying evidence types in the editorial.\
    \ Based on the distribution and structure of the identified types, we analyze\
    \ the usage patterns of argumentation strategies among 12 different topics. We\
    \ detect several common patterns that provide insights into the manifestation\
    \ of argumentation strategies. Also, our experiments reveal clear correlations\
    \ between the topics and the detected patterns."
  address: Copenhagen, Denmark
  author:
  - first: Khalid
    full: Khalid Al-Khatib
    id: khalid-al-khatib
    last: Al-Khatib
  - first: Henning
    full: Henning Wachsmuth
    id: henning-wachsmuth
    last: Wachsmuth
  - first: Matthias
    full: Matthias Hagen
    id: matthias-hagen
    last: Hagen
  - first: Benno
    full: Benno Stein
    id: benno-stein
    last: Stein
  author_string: Khalid Al-Khatib, Henning Wachsmuth, Matthias Hagen, Benno Stein
  bibkey: al-khatib-etal-2017-patterns
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1141
  month: September
  page_first: '1351'
  page_last: '1357'
  pages: "1351\u20131357"
  paper_id: '141'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1141.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1141.jpg
  title: Patterns of Argumentation Strategies across Topics
  title_html: Patterns of Argumentation Strategies across Topics
  url: https://www.aclweb.org/anthology/D17-1141
  year: '2017'
D17-1142:
  abstract: We study the helpful product reviews identification problem in this paper.
    We observe that the evidence-conclusion discourse relations, also known as arguments,
    often appear in product reviews, and we hypothesise that some argument-based features,
    e.g. the percentage of argumentative sentences, the evidences-conclusions ratios,
    are good indicators of helpful reviews. To validate this hypothesis, we manually
    annotate arguments in 110 hotel reviews, and investigate the effectiveness of
    several combinations of argument-based features. Experiments suggest that, when
    being used together with the argument-based features, the state-of-the-art baseline
    features can enjoy a performance boost (in terms of F1) of 11.01% in average.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1142.Attachment.rar
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1142.Attachment.rar
  author:
  - first: Haijing
    full: Haijing Liu
    id: haijing-liu
    last: Liu
  - first: Yang
    full: Yang Gao
    id: yang-gao
    last: Gao
  - first: Pin
    full: Pin Lv
    id: pin-lv
    last: Lv
  - first: Mengxue
    full: Mengxue Li
    id: mengxue-li
    last: Li
  - first: Shiqiang
    full: Shiqiang Geng
    id: shiqiang-geng
    last: Geng
  - first: Minglan
    full: Minglan Li
    id: minglan-li
    last: Li
  - first: Hao
    full: Hao Wang
    id: hao-wang
    last: Wang
  author_string: Haijing Liu, Yang Gao, Pin Lv, Mengxue Li, Shiqiang Geng, Minglan
    Li, Hao Wang
  bibkey: liu-etal-2017-using
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1142
  month: September
  page_first: '1358'
  page_last: '1363'
  pages: "1358\u20131363"
  paper_id: '142'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1142.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1142.jpg
  title: Using Argument-based Features to Predict and Analyse Review Helpfulness
  title_html: Using Argument-based Features to Predict and Analyse Review Helpfulness
  url: https://www.aclweb.org/anthology/D17-1142
  year: '2017'
D17-1143:
  abstract: In order to determine argument structure in text, one must understand
    how individual components of the overall argument are linked. This work presents
    the first neural network-based approach to link extraction in argument mining.
    Specifically, we propose a novel architecture that applies Pointer Network sequence-to-sequence
    attention modeling to structural prediction in discourse parsing tasks. We then
    develop a joint model that extends this architecture to simultaneously address
    the link extraction task and the classification of argument components. The proposed
    joint model achieves state-of-the-art results on two separate evaluation corpora,
    showing far superior performance than the previously proposed corpus-specific
    and heavily feature-engineered models. Furthermore, our results demonstrate that
    jointly optimizing for both tasks is crucial for high performance.
  address: Copenhagen, Denmark
  author:
  - first: Peter
    full: Peter Potash
    id: peter-potash
    last: Potash
  - first: Alexey
    full: Alexey Romanov
    id: alexey-romanov
    last: Romanov
  - first: Anna
    full: Anna Rumshisky
    id: anna-rumshisky
    last: Rumshisky
  author_string: Peter Potash, Alexey Romanov, Anna Rumshisky
  bibkey: potash-etal-2017-heres
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1143
  month: September
  page_first: '1364'
  page_last: '1373'
  pages: "1364\u20131373"
  paper_id: '143'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1143.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1143.jpg
  title: "Here\u2019s My Point: Joint Pointer Architecture for Argument Mining"
  title_html: "Here\u2019s My Point: Joint Pointer Architecture for Argument Mining"
  url: https://www.aclweb.org/anthology/D17-1143
  year: '2017'
D17-1144:
  abstract: We propose a deep learning architecture to capture argumentative relations
    of attack and support from one piece of text to another, of the kind that naturally
    occur in a debate. The architecture uses two (unidirectional or bidirectional)
    Long Short-Term Memory networks and (trained or non-trained) word embeddings,
    and allows to considerably improve upon existing techniques that use syntactic
    features and supervised classifiers for the same form of (relation-based) argument
    mining.
  address: Copenhagen, Denmark
  author:
  - first: Oana
    full: Oana Cocarascu
    id: oana-cocarascu
    last: Cocarascu
  - first: Francesca
    full: Francesca Toni
    id: francesca-toni
    last: Toni
  author_string: Oana Cocarascu, Francesca Toni
  bibkey: cocarascu-toni-2017-identifying
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1144
  month: September
  page_first: '1374'
  page_last: '1379'
  pages: "1374\u20131379"
  paper_id: '144'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1144.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1144.jpg
  title: Identifying attack and support argumentative relations using deep learning
  title_html: Identifying attack and support argumentative relations using deep learning
  url: https://www.aclweb.org/anthology/D17-1144
  year: '2017'
D17-1145:
  abstract: "The input to a neural sequence-to-sequence model is often determined\
    \ by an up-stream system, e.g. a word segmenter, part of speech tagger, or speech\
    \ recognizer. These up-stream models are potentially error-prone. Representing\
    \ inputs through word lattices allows making this uncertainty explicit by capturing\
    \ alternative sequences and their posterior probabilities in a compact form. In\
    \ this work, we extend the TreeLSTM (Tai et al., 2015) into a LatticeLSTM that\
    \ is able to consume word lattices, and can be used as encoder in an attentional\
    \ encoder-decoder model. We integrate lattice posterior scores into this architecture\
    \ by extending the TreeLSTM\u2019s child-sum and forget gates and introducing\
    \ a bias term into the attention mechanism. We experiment with speech translation\
    \ lattices and report consistent improvements over baselines that translate either\
    \ the 1-best hypothesis or the lattice without posterior scores."
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1145.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1145.Attachment.pdf
  author:
  - first: Matthias
    full: Matthias Sperber
    id: matthias-sperber
    last: Sperber
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  - first: Jan
    full: Jan Niehues
    id: jan-niehues
    last: Niehues
  - first: Alex
    full: Alex Waibel
    id: alex-waibel
    last: Waibel
  author_string: Matthias Sperber, Graham Neubig, Jan Niehues, Alex Waibel
  bibkey: sperber-etal-2017-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1145
  month: September
  page_first: '1380'
  page_last: '1389'
  pages: "1380\u20131389"
  paper_id: '145'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1145.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1145.jpg
  title: Neural Lattice-to-Sequence Models for Uncertain Inputs
  title_html: Neural Lattice-to-Sequence Models for Uncertain Inputs
  url: https://www.aclweb.org/anthology/D17-1145
  year: '2017'
D17-1146:
  abstract: Neural machine translation (NMT) has achieved notable success in recent
    times, however it is also widely recognized that this approach has limitations
    with handling infrequent words and word pairs. This paper presents a novel memory-augmented
    NMT (M-NMT) architecture, which stores knowledge about how words (usually infrequently
    encountered ones) should be translated in a memory and then utilizes them to assist
    the neural model. We use this memory mechanism to combine the knowledge learned
    from a conventional statistical machine translation system and the rules learned
    by an NMT system, and also propose a solution for out-of-vocabulary (OOV) words
    based on this framework. Our experiments on two Chinese-English translation tasks
    demonstrated that the M-NMT architecture outperformed the NMT baseline by 9.0
    and 2.7 BLEU points on the two tasks, respectively. Additionally, we found this
    architecture resulted in a much more effective OOV treatment compared to competitive
    methods.
  address: Copenhagen, Denmark
  author:
  - first: Yang
    full: Yang Feng
    id: yang-feng
    last: Feng
  - first: Shiyue
    full: Shiyue Zhang
    id: shiyue-zhang
    last: Zhang
  - first: Andi
    full: Andi Zhang
    id: andi-zhang
    last: Zhang
  - first: Dong
    full: Dong Wang
    id: dong-wang
    last: Wang
  - first: Andrew
    full: Andrew Abel
    id: andrew-abel
    last: Abel
  author_string: Yang Feng, Shiyue Zhang, Andi Zhang, Dong Wang, Andrew Abel
  bibkey: feng-etal-2017-memory
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1146
  month: September
  page_first: '1390'
  page_last: '1399'
  pages: "1390\u20131399"
  paper_id: '146'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1146.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1146.jpg
  title: Memory-augmented Neural Machine Translation
  title_html: Memory-augmented Neural Machine Translation
  url: https://www.aclweb.org/anthology/D17-1146
  year: '2017'
D17-1147:
  abstract: "Intelligent selection of training data has proven a successful technique\
    \ to simultaneously increase training efficiency and translation performance for\
    \ phrase-based machine translation (PBMT). With the recent increase in popularity\
    \ of neural machine translation (NMT), we explore in this paper to what extent\
    \ and how NMT can also benefit from data selection. While state-of-the-art data\
    \ selection (Axelrod et al., 2011) consistently performs well for PBMT, we show\
    \ that gains are substantially lower for NMT. Next, we introduce \u2018dynamic\
    \ data selection\u2019 for NMT, a method in which we vary the selected subset\
    \ of training data between different training epochs. Our experiments show that\
    \ the best results are achieved when applying a technique we call \u2018gradual\
    \ fine-tuning\u2019, with improvements up to +2.6 BLEU over the original data\
    \ selection approach and up to +3.1 BLEU over a general baseline."
  address: Copenhagen, Denmark
  author:
  - first: Marlies
    full: Marlies van der Wees
    id: marlies-van-der-wees
    last: van der Wees
  - first: Arianna
    full: Arianna Bisazza
    id: arianna-bisazza
    last: Bisazza
  - first: Christof
    full: Christof Monz
    id: christof-monz
    last: Monz
  author_string: Marlies van der Wees, Arianna Bisazza, Christof Monz
  bibkey: van-der-wees-etal-2017-dynamic
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1147
  month: September
  page_first: '1400'
  page_last: '1410'
  pages: "1400\u20131410"
  paper_id: '147'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1147.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1147.jpg
  title: Dynamic Data Selection for Neural Machine Translation
  title_html: Dynamic Data Selection for Neural Machine Translation
  url: https://www.aclweb.org/anthology/D17-1147
  year: '2017'
D17-1148:
  abstract: In this paper, we introduce a hybrid search for attention-based neural
    machine translation (NMT). A target phrase learned with statistical MT models
    extends a hypothesis in the NMT beam search when the attention of the NMT model
    focuses on the source words translated by this phrase. Phrases added in this way
    are scored with the NMT model, but also with SMT features including phrase-level
    translation probabilities and a target language model. Experimental results on
    German-to-English news domain and English-to-Russian e-commerce domain translation
    tasks show that using phrase-based models in NMT search improves MT quality by
    up to 2.3% BLEU absolute as compared to a strong NMT baseline.
  address: Copenhagen, Denmark
  author:
  - first: Leonard
    full: Leonard Dahlmann
    id: leonard-dahlmann
    last: Dahlmann
  - first: Evgeny
    full: Evgeny Matusov
    id: evgeny-matusov
    last: Matusov
  - first: Pavel
    full: Pavel Petrushkov
    id: pavel-petrushkov
    last: Petrushkov
  - first: Shahram
    full: Shahram Khadivi
    id: shahram-khadivi
    last: Khadivi
  author_string: Leonard Dahlmann, Evgeny Matusov, Pavel Petrushkov, Shahram Khadivi
  bibkey: dahlmann-etal-2017-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1148
  month: September
  page_first: '1411'
  page_last: '1420'
  pages: "1411\u20131420"
  paper_id: '148'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1148.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1148.jpg
  title: Neural Machine Translation Leveraging Phrase-based Models in a Hybrid Search
  title_html: Neural Machine Translation Leveraging Phrase-based Models in a Hybrid
    Search
  url: https://www.aclweb.org/anthology/D17-1148
  year: '2017'
D17-1149:
  abstract: Phrases play an important role in natural language understanding and machine
    translation (Sag et al., 2002; Villavicencio et al., 2005). However, it is difficult
    to integrate them into current neural machine translation (NMT) which reads and
    generates sentences word by word. In this work, we propose a method to translate
    phrases in NMT by integrating a phrase memory storing target phrases from a phrase-based
    statistical machine translation (SMT) system into the encoder-decoder architecture
    of NMT. At each decoding step, the phrase memory is first re-written by the SMT
    model, which dynamically generates relevant target phrases with contextual information
    provided by the NMT model. Then the proposed model reads the phrase memory to
    make probability estimations for all phrases in the phrase memory. If phrase generation
    is carried on, the NMT decoder selects an appropriate phrase from the memory to
    perform phrase translation and updates its decoding state by consuming the words
    in the selected phrase. Otherwise, the NMT decoder generates a word from the vocabulary
    as the general NMT decoder does. Experiment results on the Chinese to English
    translation show that the proposed model achieves significant improvements over
    the baseline on various test sets.
  address: Copenhagen, Denmark
  author:
  - first: Xing
    full: Xing Wang
    id: xing-wang
    last: Wang
  - first: Zhaopeng
    full: Zhaopeng Tu
    id: zhaopeng-tu
    last: Tu
  - first: Deyi
    full: Deyi Xiong
    id: deyi-xiong
    last: Xiong
  - first: Min
    full: Min Zhang
    id: min-zhang
    last: Zhang
  author_string: Xing Wang, Zhaopeng Tu, Deyi Xiong, Min Zhang
  bibkey: wang-etal-2017-translating
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1149
  month: September
  page_first: '1421'
  page_last: '1431'
  pages: "1421\u20131431"
  paper_id: '149'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1149.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1149.jpg
  title: Translating Phrases in Neural Machine Translation
  title_html: Translating Phrases in Neural Machine Translation
  url: https://www.aclweb.org/anthology/D17-1149
  year: '2017'
D17-1150:
  abstract: This paper proposes a hierarchical attentional neural translation model
    which focuses on enhancing source-side hierarchical representations by covering
    both local and global semantic information using a bidirectional tree-based encoder.
    To maximize the predictive likelihood of target words, a weighted variant of an
    attention mechanism is used to balance the attentive information between lexical
    and phrase vectors. Using a tree-based rare word encoding, the proposed model
    is extended to sub-word level to alleviate the out-of-vocabulary (OOV) problem.
    Empirical results reveal that the proposed model significantly outperforms sequence-to-sequence
    attention-based and tree-based neural translation models in English-Chinese translation
    tasks.
  address: Copenhagen, Denmark
  author:
  - first: Baosong
    full: Baosong Yang
    id: baosong-yang
    last: Yang
  - first: Derek F.
    full: Derek F. Wong
    id: derek-f-wong
    last: Wong
  - first: Tong
    full: Tong Xiao
    id: tong-xiao
    last: Xiao
  - first: Lidia S.
    full: Lidia S. Chao
    id: lidia-s-chao
    last: Chao
  - first: Jingbo
    full: Jingbo Zhu
    id: jingbo-zhu
    last: Zhu
  author_string: Baosong Yang, Derek F. Wong, Tong Xiao, Lidia S. Chao, Jingbo Zhu
  bibkey: yang-etal-2017-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1150
  month: September
  page_first: '1432'
  page_last: '1441'
  pages: "1432\u20131441"
  paper_id: '150'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1150.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1150.jpg
  title: Towards Bidirectional Hierarchical Representations for Attention-based Neural
    Machine Translation
  title_html: Towards Bidirectional Hierarchical Representations for Attention-based
    Neural Machine Translation
  url: https://www.aclweb.org/anthology/D17-1150
  year: '2017'
D17-1151:
  abstract: Neural Machine Translation (NMT) has shown remarkable progress over the
    past few years, with production systems now being deployed to end-users. As the
    field is moving rapidly, it has become unclear which elements of NMT architectures
    have a significant impact on translation quality. In this work, we present a large-scale
    analysis of the sensitivity of NMT architectures to common hyperparameters. We
    report empirical results and variance numbers for several hundred experimental
    runs, corresponding to over 250,000 GPU hours on a WMT English to German translation
    task. Our experiments provide practical insights into the relative importance
    of factors such as embedding size, network depth, RNN cell type, residual connections,
    attention mechanism, and decoding heuristics. As part of this contribution, we
    also release an open-source NMT framework in TensorFlow to make it easy for others
    to reproduce our results and perform their own experiments.
  address: Copenhagen, Denmark
  author:
  - first: Denny
    full: Denny Britz
    id: denny-britz
    last: Britz
  - first: Anna
    full: Anna Goldie
    id: anna-goldie
    last: Goldie
  - first: Minh-Thang
    full: Minh-Thang Luong
    id: minh-thang-luong
    last: Luong
  - first: Quoc
    full: Quoc Le
    id: quoc-le
    last: Le
  author_string: Denny Britz, Anna Goldie, Minh-Thang Luong, Quoc Le
  bibkey: britz-etal-2017-massive
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1151
  month: September
  page_first: '1442'
  page_last: '1451'
  pages: "1442\u20131451"
  paper_id: '151'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1151.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1151.jpg
  title: Massive Exploration of Neural Machine Translation Architectures
  title_html: Massive Exploration of Neural Machine Translation Architectures
  url: https://www.aclweb.org/anthology/D17-1151
  year: '2017'
D17-1152:
  abstract: Bilingual Lexicon Induction is the task of learning word translations
    without bilingual parallel corpora. We model this task as a matrix completion
    problem, and present an effective and extendable framework for completing the
    matrix. This method harnesses diverse bilingual and monolingual signals, each
    of which may be incomplete or noisy. Our model achieves state-of-the-art performance
    for both high and low resource languages.
  address: Copenhagen, Denmark
  author:
  - first: Derry Tanti
    full: Derry Tanti Wijaya
    id: derry-tanti-wijaya
    last: Wijaya
  - first: Brendan
    full: Brendan Callahan
    id: brendan-callahan
    last: Callahan
  - first: John
    full: John Hewitt
    id: john-hewitt
    last: Hewitt
  - first: Jie
    full: Jie Gao
    id: jie-gao
    last: Gao
  - first: Xiao
    full: Xiao Ling
    id: xiao-ling
    last: Ling
  - first: Marianna
    full: Marianna Apidianaki
    id: marianna-apidianaki
    last: Apidianaki
  - first: Chris
    full: Chris Callison-Burch
    id: chris-callison-burch
    last: Callison-Burch
  author_string: Derry Tanti Wijaya, Brendan Callahan, John Hewitt, Jie Gao, Xiao
    Ling, Marianna Apidianaki, Chris Callison-Burch
  bibkey: wijaya-etal-2017-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1152
  month: September
  page_first: '1452'
  page_last: '1463'
  pages: "1452\u20131463"
  paper_id: '152'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1152.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1152.jpg
  title: Learning Translations via Matrix Completion
  title_html: Learning Translations via Matrix Completion
  url: https://www.aclweb.org/anthology/D17-1152
  year: '2017'
D17-1153:
  abstract: 'Machine translation is a natural candidate problem for reinforcement
    learning from human feedback: users provide quick, dirty ratings on candidate
    translations to guide a system to improve. Yet, current neural machine translation
    training focuses on expensive human-generated reference translations. We describe
    a reinforcement learning algorithm that improves neural machine translation systems
    from simulated human feedback. Our algorithm combines the advantage actor-critic
    algorithm (Mnih et al., 2016) with the attention-based neural encoder-decoder
    architecture (Luong et al., 2015). This algorithm (a) is well-designed for problems
    with a large action space and delayed rewards, (b) effectively optimizes traditional
    corpus-level machine translation metrics, and (c) is robust to skewed, high-variance,
    granular feedback modeled after actual human behaviors.'
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1153.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1153.Attachment.zip
  author:
  - first: Khanh
    full: Khanh Nguyen
    id: khanh-nguyen
    last: Nguyen
  - first: Hal
    full: "Hal Daum\xE9 III"
    id: hal-daume-iii
    last: "Daum\xE9 III"
  - first: Jordan
    full: Jordan Boyd-Graber
    id: jordan-boyd-graber
    last: Boyd-Graber
  author_string: "Khanh Nguyen, Hal Daum\xE9 III, Jordan Boyd-Graber"
  bibkey: nguyen-etal-2017-reinforcement
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1153
  month: September
  page_first: '1464'
  page_last: '1474'
  pages: "1464\u20131474"
  paper_id: '153'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1153.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1153.jpg
  title: Reinforcement Learning for Bandit Neural Machine Translation with Simulated
    Human Feedback
  title_html: Reinforcement Learning for Bandit Neural Machine Translation with Simulated
    Human Feedback
  url: https://www.aclweb.org/anthology/D17-1153
  year: '2017'
D17-1154:
  abstract: Neural Machine Translation (NMT) lays intensive burden on computation
    and memory cost. It is a challenge to deploy NMT models on the devices with limited
    computation and memory budgets. This paper presents a four stage pipeline to compress
    model and speed up the decoding for NMT. Our method first introduces a compact
    architecture based on convolutional encoder and weight shared embeddings. Then
    weight pruning is applied to obtain a sparse model. Next, we propose a fast sequence
    interpolation approach which enables the greedy decoding to achieve performance
    on par with the beam search. Hence, the time-consuming beam search can be replaced
    by simple greedy decoding. Finally, vocabulary selection is used to reduce the
    computation of softmax layer. Our final model achieves 10 times speedup, 17 times
    parameters reduction, less than 35MB storage size and comparable performance compared
    to the baseline model.
  address: Copenhagen, Denmark
  author:
  - first: Xiaowei
    full: Xiaowei Zhang
    id: xiaowei-zhang
    last: Zhang
  - first: Wei
    full: Wei Chen
    id: wei-chen
    last: Chen
  - first: Feng
    full: Feng Wang
    id: feng-wang
    last: Wang
  - first: Shuang
    full: Shuang Xu
    id: shuang-xu
    last: Xu
  - first: Bo
    full: Bo Xu
    id: bo-xu
    last: Xu
  author_string: Xiaowei Zhang, Wei Chen, Feng Wang, Shuang Xu, Bo Xu
  bibkey: zhang-etal-2017-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1154
  month: September
  page_first: '1475'
  page_last: '1481'
  pages: "1475\u20131481"
  paper_id: '154'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1154.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1154.jpg
  title: Towards Compact and Fast Neural Machine Translation Using a Combined Method
  title_html: Towards Compact and Fast Neural Machine Translation Using a Combined
    Method
  url: https://www.aclweb.org/anthology/D17-1154
  year: '2017'
D17-1155:
  abstract: Instance weighting has been widely applied to phrase-based machine translation
    domain adaptation. However, it is challenging to be applied to Neural Machine
    Translation (NMT) directly, because NMT is not a linear model. In this paper,
    two instance weighting technologies, i.e., sentence weighting and domain weighting
    with a dynamic weight learning strategy, are proposed for NMT domain adaptation.
    Empirical results on the IWSLT English-German/French tasks show that the proposed
    methods can substantially improve NMT performance by up to 2.7-6.7 BLEU points,
    outperforming the existing baselines by up to 1.6-3.6 BLEU points.
  address: Copenhagen, Denmark
  author:
  - first: Rui
    full: Rui Wang
    id: rui-wang
    last: Wang
  - first: Masao
    full: Masao Utiyama
    id: masao-utiyama
    last: Utiyama
  - first: Lemao
    full: Lemao Liu
    id: lemao-liu
    last: Liu
  - first: Kehai
    full: Kehai Chen
    id: kehai-chen
    last: Chen
  - first: Eiichiro
    full: Eiichiro Sumita
    id: eiichiro-sumita
    last: Sumita
  author_string: Rui Wang, Masao Utiyama, Lemao Liu, Kehai Chen, Eiichiro Sumita
  bibkey: wang-etal-2017-instance
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1155
  month: September
  page_first: '1482'
  page_last: '1488'
  pages: "1482\u20131488"
  paper_id: '155'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1155.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1155.jpg
  title: Instance Weighting for Neural Machine Translation Domain Adaptation
  title_html: Instance Weighting for Neural Machine Translation Domain Adaptation
  url: https://www.aclweb.org/anthology/D17-1155
  year: '2017'
D17-1156:
  abstract: "We investigate techniques for supervised domain adaptation for neural\
    \ machine translation where an existing model trained on a large out-of-domain\
    \ dataset is adapted to a small in-domain dataset. In this scenario, overfitting\
    \ is a major challenge. We investigate a number of techniques to reduce overfitting\
    \ and improve transfer learning, including regularization techniques such as dropout\
    \ and L2-regularization towards an out-of-domain prior. In addition, we introduce\
    \ tuneout, a novel regularization technique inspired by dropout. We apply these\
    \ techniques, alone and in combination, to neural machine translation, obtaining\
    \ improvements on IWSLT datasets for English\u2192German and English\u2192Russian.\
    \ We also investigate the amounts of in-domain training data needed for domain\
    \ adaptation in NMT, and find a logarithmic relationship between the amount of\
    \ training data and gain in BLEU score."
  address: Copenhagen, Denmark
  author:
  - first: Antonio Valerio
    full: Antonio Valerio Miceli Barone
    id: antonio-valerio-miceli-barone
    last: Miceli Barone
  - first: Barry
    full: Barry Haddow
    id: barry-haddow
    last: Haddow
  - first: Ulrich
    full: Ulrich Germann
    id: ulrich-germann
    last: Germann
  - first: Rico
    full: Rico Sennrich
    id: rico-sennrich
    last: Sennrich
  author_string: Antonio Valerio Miceli Barone, Barry Haddow, Ulrich Germann, Rico
    Sennrich
  bibkey: miceli-barone-etal-2017-regularization
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1156
  month: September
  page_first: '1489'
  page_last: '1494'
  pages: "1489\u20131494"
  paper_id: '156'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1156.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1156.jpg
  title: Regularization techniques for fine-tuning in neural machine translation
  title_html: Regularization techniques for fine-tuning in neural machine translation
  url: https://www.aclweb.org/anthology/D17-1156
  year: '2017'
D17-1157:
  abstract: This paper describes an empirical study of the phrase-based decoding algorithm
    proposed by Chang and Collins (2017). The algorithm produces a translation by
    processing the source-language sentence in strictly left-to-right order, differing
    from commonly used approaches that build the target-language sentence in left-to-right
    order. Our results show that the new algorithm is competitive with Moses (Koehn
    et al., 2007) in terms of both speed and BLEU scores.
  address: Copenhagen, Denmark
  author:
  - first: Yin-Wen
    full: Yin-Wen Chang
    id: yin-wen-chang
    last: Chang
  - first: Michael
    full: Michael Collins
    id: michael-collins
    last: Collins
  author_string: Yin-Wen Chang, Michael Collins
  bibkey: chang-collins-2017-source
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1157
  month: September
  page_first: '1495'
  page_last: '1499'
  pages: "1495\u20131499"
  paper_id: '157'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1157.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1157.jpg
  title: Source-Side Left-to-Right or Target-Side Left-to-Right? An Empirical Comparison
    of Two Phrase-Based Decoding Algorithms
  title_html: Source-Side Left-to-Right or Target-Side Left-to-Right? An Empirical
    Comparison of Two Phrase-Based Decoding Algorithms
  url: https://www.aclweb.org/anthology/D17-1157
  year: '2017'
D17-1158:
  abstract: 'The performance of Neural Machine Translation (NMT) models relies heavily
    on the availability of sufficient amounts of parallel data, and an efficient and
    effective way of leveraging the vastly available amounts of monolingual data has
    yet to be found. We propose to modify the decoder in a neural sequence-to-sequence
    model to enable multi-task learning for two strongly related tasks: target-side
    language modeling and translation. The decoder predicts the next target word through
    two channels, a target-side language model on the lowest layer, and an attentional
    recurrent model which is conditioned on the source representation. This architecture
    allows joint training on both large amounts of monolingual and moderate amounts
    of bilingual data to improve NMT performance. Initial results in the news domain
    for three language pairs show moderate but consistent improvements over a baseline
    trained on bilingual data only.'
  address: Copenhagen, Denmark
  author:
  - first: Tobias
    full: Tobias Domhan
    id: tobias-domhan
    last: Domhan
  - first: Felix
    full: Felix Hieber
    id: felix-hieber
    last: Hieber
  author_string: Tobias Domhan, Felix Hieber
  bibkey: domhan-hieber-2017-using
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1158
  month: September
  page_first: '1500'
  page_last: '1505'
  pages: "1500\u20131505"
  paper_id: '158'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1158.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1158.jpg
  title: Using Target-side Monolingual Data for Neural Machine Translation through
    Multi-task Learning
  title_html: Using Target-side Monolingual Data for Neural Machine Translation through
    Multi-task Learning
  url: https://www.aclweb.org/anthology/D17-1158
  year: '2017'
D17-1159:
  abstract: 'Semantic role labeling (SRL) is the task of identifying the predicate-argument
    structure of a sentence. It is typically regarded as an important step in the
    standard NLP pipeline. As the semantic representations are closely related to
    syntactic ones, we exploit syntactic information in our model. We propose a version
    of graph convolutional networks (GCNs), a recent class of neural networks operating
    on graphs, suited to model syntactic dependency graphs. GCNs over syntactic dependency
    trees are used as sentence encoders, producing latent feature representations
    of words in a sentence. We observe that GCN layers are complementary to LSTM ones:
    when we stack both GCN and LSTM layers, we obtain a substantial improvement over
    an already state-of-the-art LSTM SRL model, resulting in the best reported scores
    on the standard benchmark (CoNLL-2009) both for Chinese and English.'
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1159.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1159.Attachment.pdf
  - filename: https://vimeo.com/238232852
    type: video
    url: https://vimeo.com/238232852
  author:
  - first: Diego
    full: Diego Marcheggiani
    id: diego-marcheggiani
    last: Marcheggiani
  - first: Ivan
    full: Ivan Titov
    id: ivan-titov
    last: Titov
  author_string: Diego Marcheggiani, Ivan Titov
  bibkey: marcheggiani-titov-2017-encoding
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1159
  month: September
  page_first: '1506'
  page_last: '1515'
  pages: "1506\u20131515"
  paper_id: '159'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1159.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1159.jpg
  title: Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling
  title_html: Encoding Sentences with Graph Convolutional Networks for Semantic Role
    Labeling
  url: https://www.aclweb.org/anthology/D17-1159
  year: '2017'
D17-1160:
  abstract: 'We present a new semantic parsing model for answering compositional questions
    on semi-structured Wikipedia tables. Our parser is an encoder-decoder neural network
    with two key technical innovations: (1) a grammar for the decoder that only generates
    well-typed logical forms; and (2) an entity embedding and linking module that
    identifies entity mentions while generalizing across tables. We also introduce
    a novel method for training our neural model with question-answer supervision.
    On the WikiTableQuestions data set, our parser achieves a state-of-the-art accuracy
    of 43.3% for a single model and 45.9% for a 5-model ensemble, improving on the
    best prior score of 38.7% set by a 15-model ensemble. These results suggest that
    type constraints and entity linking are valuable components to incorporate in
    neural semantic parsers.'
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238234920
    type: video
    url: https://vimeo.com/238234920
  author:
  - first: Jayant
    full: Jayant Krishnamurthy
    id: jayant-krishnamurthy
    last: Krishnamurthy
  - first: Pradeep
    full: Pradeep Dasigi
    id: pradeep-dasigi
    last: Dasigi
  - first: Matt
    full: Matt Gardner
    id: matt-gardner
    last: Gardner
  author_string: Jayant Krishnamurthy, Pradeep Dasigi, Matt Gardner
  bibkey: krishnamurthy-etal-2017-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1160
  month: September
  page_first: '1516'
  page_last: '1526'
  pages: "1516\u20131526"
  paper_id: '160'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1160.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1160.jpg
  title: Neural Semantic Parsing with Type Constraints for Semi-Structured Tables
  title_html: Neural Semantic Parsing with Type Constraints for Semi-Structured Tables
  url: https://www.aclweb.org/anthology/D17-1160
  year: '2017'
D17-1161:
  abstract: "Natural language constitutes a predominant medium for much of human learning\
    \ and pedagogy. We consider the problem of concept learning from natural language\
    \ explanations, and a small number of labeled examples of the concept. For example,\
    \ in learning the concept of a phishing email, one might say \u2018this is a phishing\
    \ email because it asks for your bank account number\u2019. Solving this problem\
    \ involves both learning to interpret open ended natural language statements,\
    \ and learning the concept itself. We present a joint model for (1) language interpretation\
    \ (semantic parsing) and (2) concept learning (classification) that does not require\
    \ labeling statements with logical forms. Instead, the model prefers discriminative\
    \ interpretations of statements in context of observable features of the data\
    \ as a weak signal for parsing. On a dataset of email-related concepts, our approach\
    \ yields across-the-board improvements in classification performance, with a 30%\
    \ relative improvement in F1 score over competitive methods in the low data regime."
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238233841
    type: video
    url: https://vimeo.com/238233841
  author:
  - first: Shashank
    full: Shashank Srivastava
    id: shashank-srivastava
    last: Srivastava
  - first: Igor
    full: Igor Labutov
    id: igor-labutov
    last: Labutov
  - first: Tom
    full: Tom Mitchell
    id: tom-mitchell
    last: Mitchell
  author_string: Shashank Srivastava, Igor Labutov, Tom Mitchell
  bibkey: srivastava-etal-2017-joint
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1161
  month: September
  page_first: '1527'
  page_last: '1536'
  pages: "1527\u20131536"
  paper_id: '161'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1161.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1161.jpg
  title: Joint Concept Learning and Semantic Parsing from Natural Language Explanations
  title_html: Joint Concept Learning and Semantic Parsing from Natural Language Explanations
  url: https://www.aclweb.org/anthology/D17-1161
  year: '2017'
D17-1162:
  abstract: The ubiquity of metaphor in our everyday communication makes it an important
    problem for natural language understanding. Yet, the majority of metaphor processing
    systems to date rely on hand-engineered features and there is still no consensus
    in the field as to which features are optimal for this task. In this paper, we
    present the first deep learning architecture designed to capture metaphorical
    composition. Our results demonstrate that it outperforms the existing approaches
    in the metaphor identification task.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238233405
    type: video
    url: https://vimeo.com/238233405
  author:
  - first: Marek
    full: Marek Rei
    id: marek-rei
    last: Rei
  - first: Luana
    full: Luana Bulat
    id: luana-bulat
    last: Bulat
  - first: Douwe
    full: Douwe Kiela
    id: douwe-kiela
    last: Kiela
  - first: Ekaterina
    full: Ekaterina Shutova
    id: ekaterina-shutova
    last: Shutova
  author_string: Marek Rei, Luana Bulat, Douwe Kiela, Ekaterina Shutova
  bibkey: rei-etal-2017-grasping
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1162
  month: September
  page_first: '1537'
  page_last: '1546'
  pages: "1537\u20131546"
  paper_id: '162'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1162.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1162.jpg
  title: 'Grasping the Finer Point: A Supervised Similarity Network for Metaphor Detection'
  title_html: 'Grasping the Finer Point: A Supervised Similarity Network for Metaphor
    Detection'
  url: https://www.aclweb.org/anthology/D17-1162
  year: '2017'
D17-1163:
  abstract: 'We propose a new, socially-impactful task for natural language processing:
    from a news corpus, extract names of persons who have been killed by police. We
    present a newly collected police fatality corpus, which we release publicly, and
    present a model to solve this problem that uses EM-based distant supervision with
    logistic regression and convolutional neural network classifiers. Our model outperforms
    two off-the-shelf event extractor systems, and it can suggest candidate victim
    names in some cases faster than one of the major manually-collected police fatality
    databases.'
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238233559
    type: video
    url: https://vimeo.com/238233559
  author:
  - first: Katherine
    full: Katherine Keith
    id: katherine-keith
    last: Keith
  - first: Abram
    full: Abram Handler
    id: abram-handler
    last: Handler
  - first: Michael
    full: Michael Pinkham
    id: michael-pinkham
    last: Pinkham
  - first: Cara
    full: Cara Magliozzi
    id: cara-magliozzi
    last: Magliozzi
  - first: Joshua
    full: Joshua McDuffie
    id: joshua-mcduffie
    last: McDuffie
  - first: Brendan
    full: "Brendan O\u2019Connor"
    id: brendan-oconnor
    last: "O\u2019Connor"
  author_string: "Katherine Keith, Abram Handler, Michael Pinkham, Cara Magliozzi,\
    \ Joshua McDuffie, Brendan O\u2019Connor"
  bibkey: keith-etal-2017-identifying
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1163
  month: September
  page_first: '1547'
  page_last: '1557'
  pages: "1547\u20131557"
  paper_id: '163'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1163.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1163.jpg
  title: Identifying civilians killed by police with distantly supervised entity-event
    extraction
  title_html: Identifying civilians killed by police with distantly supervised entity-event
    extraction
  url: https://www.aclweb.org/anthology/D17-1163
  year: '2017'
D17-1164:
  abstract: "Questions play a prominent role in social interactions, performing rhetorical\
    \ functions that go beyond that of simple informational exchange. The surface\
    \ form of a question can signal the intention and background of the person asking\
    \ it, as well as the nature of their relation with the interlocutor. While the\
    \ informational nature of questions has been extensively examined in the context\
    \ of question-answering applications, their rhetorical aspects have been largely\
    \ understudied. In this work we introduce an unsupervised methodology for extracting\
    \ surface motifs that recur in questions, and for grouping them according to their\
    \ latent rhetorical role. By applying this framework to the setting of question\
    \ sessions in the UK parliament, we show that the resulting typology encodes key\
    \ aspects of the political discourse\u2014such as the bifurcation in questioning\
    \ behavior between government and opposition parties\u2014and reveals new insights\
    \ into the effects of a legislator\u2019s tenure and political career ambitions."
  address: Copenhagen, Denmark
  author:
  - first: Justine
    full: Justine Zhang
    id: justine-zhang
    last: Zhang
  - first: Arthur
    full: Arthur Spirling
    id: arthur-spirling
    last: Spirling
  - first: Cristian
    full: Cristian Danescu-Niculescu-Mizil
    id: cristian-danescu-niculescu-mizil
    last: Danescu-Niculescu-Mizil
  author_string: Justine Zhang, Arthur Spirling, Cristian Danescu-Niculescu-Mizil
  bibkey: zhang-etal-2017-asking
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1164
  month: September
  page_first: '1558'
  page_last: '1572'
  pages: "1558\u20131572"
  paper_id: '164'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1164.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1164.jpg
  title: Asking too much? The rhetorical role of questions in political discourse
  title_html: Asking too much? The rhetorical role of questions in political discourse
  url: https://www.aclweb.org/anthology/D17-1164
  year: '2017'
D17-1165:
  abstract: "We explore how to detect people\u2019s perspectives that occupy a certain\
    \ proposition. We propose a Bayesian modelling approach where topics (or propositions)\
    \ and their associated perspectives (or viewpoints) are modeled as latent variables.\
    \ Words associated with topics or perspectives follow different generative routes.\
    \ Based on the extracted perspectives, we can extract the top associated sentences\
    \ from text to generate a succinct summary which allows a quick glimpse of the\
    \ main viewpoints in a document. The model is evaluated on debates from the House\
    \ of Commons of the UK Parliament, revealing perspectives from the debates without\
    \ the use of labelled data and obtaining better results than previous related\
    \ solutions under a variety of evaluations."
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238232359
    type: video
    url: https://vimeo.com/238232359
  author:
  - first: David
    full: David Vilares
    id: david-vilares
    last: Vilares
  - first: Yulan
    full: Yulan He
    id: yulan-he
    last: He
  author_string: David Vilares, Yulan He
  bibkey: vilares-he-2017-detecting
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1165
  month: September
  page_first: '1573'
  page_last: '1582'
  pages: "1573\u20131582"
  paper_id: '165'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1165.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1165.jpg
  title: Detecting Perspectives in Political Debates
  title_html: Detecting Perspectives in Political Debates
  url: https://www.aclweb.org/anthology/D17-1165
  year: '2017'
D17-1166:
  abstract: "Social media users often make explicit predictions about upcoming events.\
    \ Such statements vary in the degree of certainty the author expresses toward\
    \ the outcome: \u201CLeonardo DiCaprio will win Best Actor\u201D vs. \u201CLeonardo\
    \ DiCaprio may win\u201D or \u201CNo way Leonardo wins!\u201D. Can popular beliefs\
    \ on social media predict who will win? To answer this question, we build a corpus\
    \ of tweets annotated for veridicality on which we train a log-linear classifier\
    \ that detects positive veridicality with high precision. We then forecast uncertain\
    \ outcomes using the wisdom of crowds, by aggregating users\u2019 explicit predictions.\
    \ Our method for forecasting winners is fully automated, relying only on a set\
    \ of contenders as input. It requires no training data of past outcomes and outperforms\
    \ sentiment and tweet volume baselines on a broad range of contest prediction\
    \ tasks. We further demonstrate how our approach can be used to measure the reliability\
    \ of individual accounts\u2019 predictions and retrospectively identify surprise\
    \ outcomes."
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238236876
    type: video
    url: https://vimeo.com/238236876
  author:
  - first: Sandesh
    full: Sandesh Swamy
    id: sandesh-swamy
    last: Swamy
  - first: Alan
    full: Alan Ritter
    id: alan-ritter
    last: Ritter
  - first: Marie-Catherine
    full: Marie-Catherine de Marneffe
    id: marie-catherine-de-marneffe
    last: de Marneffe
  author_string: Sandesh Swamy, Alan Ritter, Marie-Catherine de Marneffe
  bibkey: swamy-etal-2017-feeling
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1166
  month: September
  page_first: '1583'
  page_last: '1592'
  pages: "1583\u20131592"
  paper_id: '166'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1166.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1166.jpg
  title: "\u201Ci have a feeling trump will win..................\u201D: Forecasting\
    \ Winners and Losers from User Predictions on Twitter"
  title_html: "\u201Ci have a feeling trump will win..................\u201D: Forecasting\
    \ Winners and Losers from User Predictions on Twitter"
  url: https://www.aclweb.org/anthology/D17-1166
  year: '2017'
D17-1167:
  abstract: Emotion cause extraction aims to identify the reasons behind a certain
    emotion expressed in text. It is a much more difficult task compared to emotion
    classification. Inspired by recent advances in using deep memory networks for
    question answering (QA), we propose a new approach which considers emotion cause
    identification as a reading comprehension task in QA. Inspired by convolutional
    neural networks, we propose a new mechanism to store relevant context in different
    memory slots to model context information. Our proposed approach can extract both
    word level sequence features and lexical features. Performance evaluation shows
    that our method achieves the state-of-the-art performance on a recently released
    emotion cause dataset, outperforming a number of competitive baselines by at least
    3.01% in F-measure.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238231156
    type: video
    url: https://vimeo.com/238231156
  author:
  - first: Lin
    full: Lin Gui
    id: lin-gui
    last: Gui
  - first: Jiannan
    full: Jiannan Hu
    id: jiannan-hu
    last: Hu
  - first: Yulan
    full: Yulan He
    id: yulan-he
    last: He
  - first: Ruifeng
    full: Ruifeng Xu
    id: ruifeng-xu
    last: Xu
  - first: Qin
    full: Qin Lu
    id: qin-lu
    last: Lu
  - first: Jiachen
    full: Jiachen Du
    id: jiachen-du
    last: Du
  author_string: Lin Gui, Jiannan Hu, Yulan He, Ruifeng Xu, Qin Lu, Jiachen Du
  bibkey: gui-etal-2017-question
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1167
  month: September
  page_first: '1593'
  page_last: '1602'
  pages: "1593\u20131602"
  paper_id: '167'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1167.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1167.jpg
  title: A Question Answering Approach for Emotion Cause Extraction
  title_html: A Question Answering Approach for Emotion Cause Extraction
  url: https://www.aclweb.org/anthology/D17-1167
  year: '2017'
D17-1168:
  abstract: "Automatic story comprehension is a fundamental challenge in Natural Language\
    \ Understanding, and can enable computers to learn about social norms, human behavior\
    \ and commonsense. In this paper, we present a story comprehension model that\
    \ explores three distinct semantic aspects: (i) the sequence of events described\
    \ in the story, (ii) its emotional trajectory, and (iii) its plot consistency.\
    \ We judge the model\u2019s understanding of real-world stories by inquiring if,\
    \ like humans, it can develop an expectation of what will happen next in a given\
    \ story. Specifically, we use it to predict the correct ending of a given short\
    \ story from possible alternatives. The model uses a hidden variable to weigh\
    \ the semantic aspects in the context of the story. Our experiments demonstrate\
    \ the potential of our approach to characterize these semantic aspects, and the\
    \ strength of the hidden variable based approach. The model outperforms the state-of-the-art\
    \ approaches and achieves best results on a publicly available dataset."
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238235959
    type: video
    url: https://vimeo.com/238235959
  author:
  - first: Snigdha
    full: Snigdha Chaturvedi
    id: snigdha-chaturvedi
    last: Chaturvedi
  - first: Haoruo
    full: Haoruo Peng
    id: haoruo-peng
    last: Peng
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Snigdha Chaturvedi, Haoruo Peng, Dan Roth
  bibkey: chaturvedi-etal-2017-story
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1168
  month: September
  page_first: '1603'
  page_last: '1614'
  pages: "1603\u20131614"
  paper_id: '168'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1168.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1168.jpg
  title: Story Comprehension for Predicting What Happens Next
  title_html: Story Comprehension for Predicting What Happens Next
  url: https://www.aclweb.org/anthology/D17-1168
  year: '2017'
D17-1169:
  abstract: NLP tasks are often limited by scarcity of manually annotated data. In
    social media sentiment analysis and related tasks, researchers have therefore
    used binarized emoticons and specific hashtags as forms of distant supervision.
    Our paper shows that by extending the distant supervision to a more diverse set
    of noisy labels, the models can learn richer representations. Through emoji prediction
    on a dataset of 1246 million tweets containing one of 64 common emojis we obtain
    state-of-the-art performance on 8 benchmark datasets within emotion, sentiment
    and sarcasm detection using a single pretrained model. Our analyses confirm that
    the diversity of our emotional labels yield a performance improvement over previous
    distant supervision approaches.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1169.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1169.Attachment.pdf
  - filename: https://vimeo.com/238236688
    type: video
    url: https://vimeo.com/238236688
  author:
  - first: Bjarke
    full: Bjarke Felbo
    id: bjarke-felbo
    last: Felbo
  - first: Alan
    full: Alan Mislove
    id: alan-mislove
    last: Mislove
  - first: Anders
    full: "Anders S\xF8gaard"
    id: anders-sogaard
    last: "S\xF8gaard"
  - first: Iyad
    full: Iyad Rahwan
    id: iyad-rahwan
    last: Rahwan
  - first: Sune
    full: Sune Lehmann
    id: sune-lehmann
    last: Lehmann
  author_string: "Bjarke Felbo, Alan Mislove, Anders S\xF8gaard, Iyad Rahwan, Sune\
    \ Lehmann"
  bibkey: felbo-etal-2017-using
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1169
  month: September
  page_first: '1615'
  page_last: '1625'
  pages: "1615\u20131625"
  paper_id: '169'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1169.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1169.jpg
  title: Using millions of emoji occurrences to learn any-domain representations for
    detecting sentiment, emotion and sarcasm
  title_html: Using millions of emoji occurrences to learn any-domain representations
    for detecting sentiment, emotion and sarcasm
  url: https://www.aclweb.org/anthology/D17-1169
  year: '2017'
D17-1170:
  abstract: "We present opinion recommendation, a novel task of jointly generating\
    \ a review with a rating score that a certain user would give to a certain product\
    \ which is unreviewed by the user, given existing reviews to the product by other\
    \ users, and the reviews that the user has given to other products. A characteristic\
    \ of opinion recommendation is the reliance of multiple data sources for multi-task\
    \ joint learning. We use a single neural network to model users and products,\
    \ generating customised product representations using a deep memory network, from\
    \ which customised ratings and reviews are constructed jointly. Results show that\
    \ our opinion recommendation system gives ratings that are closer to real user\
    \ ratings on Yelp.com data compared with Yelp\u2019s own ratings. our methods\
    \ give better results compared to several pipelines baselines."
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238235088
    type: video
    url: https://vimeo.com/238235088
  author:
  - first: Zhongqing
    full: Zhongqing Wang
    id: zhongqing-wang
    last: Wang
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  author_string: Zhongqing Wang, Yue Zhang
  bibkey: wang-zhang-2017-opinion
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1170
  month: September
  page_first: '1626'
  page_last: '1637'
  pages: "1626\u20131637"
  paper_id: '170'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1170.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1170.jpg
  title: Opinion Recommendation Using A Neural Model
  title_html: Opinion Recommendation Using A Neural Model
  url: https://www.aclweb.org/anthology/D17-1170
  year: '2017'
D17-1171:
  abstract: Unsupervised dependency parsing, which tries to discover linguistic dependency
    structures from unannotated data, is a very challenging task. Almost all previous
    work on this task focuses on learning generative models. In this paper, we develop
    an unsupervised dependency parsing model based on the CRF autoencoder. The encoder
    part of our model is discriminative and globally normalized which allows us to
    use rich features as well as universal linguistic priors. We propose an exact
    algorithm for parsing as well as a tractable learning algorithm. We evaluated
    the performance of our model on eight multilingual treebanks and found that our
    model achieved comparable performance with state-of-the-art approaches.
  address: Copenhagen, Denmark
  author:
  - first: Jiong
    full: Jiong Cai
    id: jiong-cai
    last: Cai
  - first: Yong
    full: Yong Jiang
    id: yong-jiang
    last: Jiang
  - first: Kewei
    full: Kewei Tu
    id: kewei-tu
    last: Tu
  author_string: Jiong Cai, Yong Jiang, Kewei Tu
  bibkey: cai-etal-2017-crf
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1171
  month: September
  page_first: '1638'
  page_last: '1643'
  pages: "1638\u20131643"
  paper_id: '171'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1171.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1171.jpg
  title: CRF Autoencoder for Unsupervised Dependency Parsing
  title_html: <span class="acl-fixed-case">CRF</span> Autoencoder for Unsupervised
    Dependency Parsing
  url: https://www.aclweb.org/anthology/D17-1171
  year: '2017'
D17-1172:
  abstract: We present a new method for the joint task of tagging and non-projective
    dependency parsing. We demonstrate its usefulness with an application to discontinuous
    phrase-structure parsing where decoding lexicalized spines and syntactic derivations
    is performed jointly. The main contributions of this paper are (1) a reduction
    from joint tagging and non-projective dependency parsing to the Generalized Maximum
    Spanning Arborescence problem, and (2) a novel decoding algorithm for this problem
    through Lagrangian relaxation. We evaluate this model and obtain state-of-the-art
    results despite strong independence assumptions.
  address: Copenhagen, Denmark
  author:
  - first: Caio
    full: Caio Corro
    id: caio-corro
    last: Corro
  - first: Joseph
    full: Joseph Le Roux
    id: joseph-le-roux
    last: Le Roux
  - first: Mathieu
    full: Mathieu Lacroix
    id: mathieu-lacroix
    last: Lacroix
  author_string: Caio Corro, Joseph Le Roux, Mathieu Lacroix
  bibkey: corro-etal-2017-efficient
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1172
  month: September
  page_first: '1644'
  page_last: '1654'
  pages: "1644\u20131654"
  paper_id: '172'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1172.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1172.jpg
  title: Efficient Discontinuous Phrase-Structure Parsing via the Generalized Maximum
    Spanning Arborescence
  title_html: Efficient Discontinuous Phrase-Structure Parsing via the Generalized
    Maximum Spanning Arborescence
  url: https://www.aclweb.org/anthology/D17-1172
  year: '2017'
D17-1173:
  abstract: Very recently, some studies on neural dependency parsers have shown advantage
    over the traditional ones on a wide variety of languages. However, for graph-based
    neural dependency parsing systems, they either count on the long-term memory and
    attention mechanism to implicitly capture the high-order features or give up the
    global exhaustive inference algorithms in order to harness the features over a
    rich history of parsing decisions. The former might miss out the important features
    for specific headword predictions without the help of the explicit structural
    information, and the latter may suffer from the error propagation as false early
    structural constraints are used to create features when making future predictions.
    We explore the feasibility of explicitly taking high-order features into account
    while remaining the main advantage of global inference and learning for graph-based
    parsing. The proposed parser first forms an initial parse tree by head-modifier
    predictions based on the first-order factorization. High-order features (such
    as grandparent, sibling, and uncle) then can be defined over the initial tree,
    and used to refine the parse tree in an iterative fashion. Experimental results
    showed that our model (called INDP) archived competitive performance to existing
    benchmark parsers on both English and Chinese datasets.
  address: Copenhagen, Denmark
  author:
  - first: Xiaoqing
    full: Xiaoqing Zheng
    id: xiaoqing-zheng
    last: Zheng
  author_string: Xiaoqing Zheng
  bibkey: zheng-2017-incremental
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1173
  month: September
  page_first: '1655'
  page_last: '1665'
  pages: "1655\u20131665"
  paper_id: '173'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1173.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1173.jpg
  title: Incremental Graph-based Neural Dependency Parsing
  title_html: Incremental Graph-based Neural Dependency Parsing
  url: https://www.aclweb.org/anthology/D17-1173
  year: '2017'
D17-1174:
  abstract: One of the most pressing issues in discontinuous constituency transition-based
    parsing is that the relevant information for parsing decisions could be located
    in any part of the stack or the buffer. In this paper, we propose a solution to
    this problem by replacing the structured perceptron model with a recursive neural
    model that computes a global representation of the configuration, therefore allowing
    even the most remote parts of the configuration to influence the parsing decisions.
    We also provide a detailed analysis of how this representation should be built
    out of sub-representations of its core elements (words, trees and stack). Additionally,
    we investigate how different types of swap oracles influence the results. Our
    model is the first neural discontinuous constituency parser, and it outperforms
    all the previously published models on three out of four datasets while on the
    fourth it obtains second place by a tiny difference.
  address: Copenhagen, Denmark
  author:
  - first: "Milo\u0161"
    full: "Milo\u0161 Stanojevi\u0107"
    id: milos-stanojevic
    last: "Stanojevi\u0107"
  - first: Raquel
    full: Raquel G. Alhama
    id: raquel-g-alhama
    last: G. Alhama
  author_string: "Milo\u0161 Stanojevi\u0107, Raquel G. Alhama"
  bibkey: stanojevic-g-alhama-2017-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1174
  month: September
  page_first: '1666'
  page_last: '1676'
  pages: "1666\u20131676"
  paper_id: '174'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1174.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1174.jpg
  title: Neural Discontinuous Constituency Parsing
  title_html: Neural Discontinuous Constituency Parsing
  url: https://www.aclweb.org/anthology/D17-1174
  year: '2017'
D17-1175:
  abstract: Although sequence-to-sequence (seq2seq) network has achieved significant
    success in many NLP tasks such as machine translation and text summarization,
    simply applying this approach to transition-based dependency parsing cannot yield
    a comparable performance gain as in other state-of-the-art methods, such as stack-LSTM
    and head selection. In this paper, we propose a stack-based multi-layer attention
    model for seq2seq learning to better leverage structural linguistics information.
    In our method, two binary vectors are used to track the decoding stack in transition-based
    parsing, and multi-layer attention is introduced to capture multiple word dependencies
    in partial trees. We conduct experiments on PTB and CTB datasets, and the results
    show that our proposed model achieves state-of-the-art accuracy and significant
    improvement in labeled precision with respect to the baseline seq2seq model.
  address: Copenhagen, Denmark
  author:
  - first: Zhirui
    full: Zhirui Zhang
    id: zhirui-zhang
    last: Zhang
  - first: Shujie
    full: Shujie Liu
    id: shujie-liu
    last: Liu
  - first: Mu
    full: Mu Li
    id: mu-li
    last: Li
  - first: Ming
    full: Ming Zhou
    id: ming-zhou
    last: Zhou
  - first: Enhong
    full: Enhong Chen
    id: enhong-chen
    last: Chen
  author_string: Zhirui Zhang, Shujie Liu, Mu Li, Ming Zhou, Enhong Chen
  bibkey: zhang-etal-2017-stack
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1175
  month: September
  page_first: '1677'
  page_last: '1682'
  pages: "1677\u20131682"
  paper_id: '175'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1175.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1175.jpg
  title: Stack-based Multi-layer Attention for Transition-based Dependency Parsing
  title_html: Stack-based Multi-layer Attention for Transition-based Dependency Parsing
  url: https://www.aclweb.org/anthology/D17-1175
  year: '2017'
D17-1176:
  abstract: We study the impact of big models (in terms of the degree of lexicalization)
    and big data (in terms of the training corpus size) on dependency grammar induction.
    We experimented with L-DMV, a lexicalized version of Dependency Model with Valence
    (Klein and Manning, 2004) and L-NDMV, our lexicalized extension of the Neural
    Dependency Model with Valence (Jiang et al., 2016). We find that L-DMV only benefits
    from very small degrees of lexicalization and moderate sizes of training corpora.
    L-NDMV can benefit from big training data and lexicalization of greater degrees,
    especially when enhanced with good model initialization, and it achieves a result
    that is competitive with the current state-of-the-art.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1176.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1176.Attachment.pdf
  author:
  - first: Wenjuan
    full: Wenjuan Han
    id: wenjuan-han
    last: Han
  - first: Yong
    full: Yong Jiang
    id: yong-jiang
    last: Jiang
  - first: Kewei
    full: Kewei Tu
    id: kewei-tu
    last: Tu
  author_string: Wenjuan Han, Yong Jiang, Kewei Tu
  bibkey: han-etal-2017-dependency
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1176
  month: September
  page_first: '1683'
  page_last: '1688'
  pages: "1683\u20131688"
  paper_id: '176'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1176.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1176.jpg
  title: Dependency Grammar Induction with Neural Lexicalization and Big Training
    Data
  title_html: Dependency Grammar Induction with Neural Lexicalization and Big Training
    Data
  url: https://www.aclweb.org/anthology/D17-1176
  year: '2017'
D17-1177:
  abstract: Unsupervised dependency parsing aims to learn a dependency parser from
    unannotated sentences. Existing work focuses on either learning generative models
    using the expectation-maximization algorithm and its variants, or learning discriminative
    models using the discriminative clustering algorithm. In this paper, we propose
    a new learning strategy that learns a generative model and a discriminative model
    jointly based on the dual decomposition method. Our method is simple and general,
    yet effective to capture the advantages of both models and improve their learning
    results. We tested our method on the UD treebank and achieved a state-of-the-art
    performance on thirty languages.
  address: Copenhagen, Denmark
  author:
  - first: Yong
    full: Yong Jiang
    id: yong-jiang
    last: Jiang
  - first: Wenjuan
    full: Wenjuan Han
    id: wenjuan-han
    last: Han
  - first: Kewei
    full: Kewei Tu
    id: kewei-tu
    last: Tu
  author_string: Yong Jiang, Wenjuan Han, Kewei Tu
  bibkey: jiang-etal-2017-combining
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1177
  month: September
  page_first: '1689'
  page_last: '1694'
  pages: "1689\u20131694"
  paper_id: '177'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1177.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1177.jpg
  title: Combining Generative and Discriminative Approaches to Unsupervised Dependency
    Parsing via Dual Decomposition
  title_html: Combining Generative and Discriminative Approaches to Unsupervised Dependency
    Parsing via Dual Decomposition
  url: https://www.aclweb.org/anthology/D17-1177
  year: '2017'
D17-1178:
  abstract: Generative neural models have recently achieved state-of-the-art results
    for constituency parsing. However, without a feasible search procedure, their
    use has so far been limited to reranking the output of external parsers in which
    decoding is more tractable. We describe an alternative to the conventional action-level
    beam search used for discriminative neural models that enables us to decode directly
    in these generative models. We then show that by improving our basic candidate
    selection strategy and using a coarse pruning function, we can improve accuracy
    while exploring significantly less of the search space. Applied to the model of
    Choe and Charniak (2016), our inference procedure obtains 92.56 F1 on section
    23 of the Penn Treebank, surpassing prior state-of-the-art results for single-model
    systems.
  address: Copenhagen, Denmark
  author:
  - first: Mitchell
    full: Mitchell Stern
    id: mitchell-stern
    last: Stern
  - first: Daniel
    full: Daniel Fried
    id: daniel-fried
    last: Fried
  - first: Dan
    full: Dan Klein
    id: dan-klein
    last: Klein
  author_string: Mitchell Stern, Daniel Fried, Dan Klein
  bibkey: stern-etal-2017-effective
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1178
  month: September
  page_first: '1695'
  page_last: '1700'
  pages: "1695\u20131700"
  paper_id: '178'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1178.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1178.jpg
  title: Effective Inference for Generative Neural Parsing
  title_html: Effective Inference for Generative Neural Parsing
  url: https://www.aclweb.org/anthology/D17-1178
  year: '2017'
D17-1179:
  abstract: 'In this paper we propose an end-to-end neural CRF autoencoder (NCRF-AE)
    model for semi-supervised learning of sequential structured prediction problems.
    Our NCRF-AE consists of two parts: an encoder which is a CRF model enhanced by
    deep neural networks, and a decoder which is a generative model trying to reconstruct
    the input. Our model has a unified structure with different loss functions for
    labeled and unlabeled data with shared parameters. We developed a variation of
    the EM algorithm for optimizing both the encoder and the decoder simultaneously
    by decoupling their parameters. Our Experimental results over the Part-of-Speech
    (POS) tagging task on eight different languages, show that our model can outperform
    competitive systems in both supervised and semi-supervised scenarios.'
  address: Copenhagen, Denmark
  author:
  - first: Xiao
    full: Xiao Zhang
    id: xiao-zhang
    last: Zhang
  - first: Yong
    full: Yong Jiang
    id: yong-jiang
    last: Jiang
  - first: Hao
    full: Hao Peng
    id: hao-peng
    last: Peng
  - first: Kewei
    full: Kewei Tu
    id: kewei-tu
    last: Tu
  - first: Dan
    full: Dan Goldwasser
    id: dan-goldwasser
    last: Goldwasser
  author_string: Xiao Zhang, Yong Jiang, Hao Peng, Kewei Tu, Dan Goldwasser
  bibkey: zhang-etal-2017-semi
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1179
  month: September
  page_first: '1701'
  page_last: '1711'
  pages: "1701\u20131711"
  paper_id: '179'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1179.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1179.jpg
  title: Semi-supervised Structured Prediction with Neural CRF Autoencoder
  title_html: Semi-supervised Structured Prediction with Neural <span class="acl-fixed-case">CRF</span>
    Autoencoder
  url: https://www.aclweb.org/anthology/D17-1179
  year: '2017'
D17-1180:
  abstract: "We present supertagging-based models for Tree Adjoining Grammar parsing\
    \ that use neural network architectures and dense vector representation of supertags\
    \ (elementary trees) to achieve state-of-the-art performance in unlabeled and\
    \ labeled attachment scores. The shift-reduce parsing model eschews lexical information\
    \ entirely, and uses only the 1-best supertags to parse a sentence, providing\
    \ further support for the claim that supertagging is \u201Calmost parsing.\u201D\
    \ We demonstrate that the embedding vector representations the parser induces\
    \ for supertags possess linguistically interpretable structure, supporting analogies\
    \ between grammatical structures like those familiar from recent work in distributional\
    \ semantics. This dense representation of supertags overcomes the drawbacks for\
    \ statistical models of TAG as compared to CCG parsing, raising the possibility\
    \ that TAG is a viable alternative for NLP tasks that require the assignment of\
    \ richer structural descriptions to sentences."
  address: Copenhagen, Denmark
  author:
  - first: Jungo
    full: Jungo Kasai
    id: jungo-kasai
    last: Kasai
  - first: Bob
    full: Bob Frank
    id: bob-frank
    last: Frank
  - first: Tom
    full: Tom McCoy
    id: tom-mccoy
    last: McCoy
  - first: Owen
    full: Owen Rambow
    id: owen-rambow
    last: Rambow
  - first: Alexis
    full: Alexis Nasr
    id: alexis-nasr
    last: Nasr
  author_string: Jungo Kasai, Bob Frank, Tom McCoy, Owen Rambow, Alexis Nasr
  bibkey: kasai-etal-2017-tag
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1180
  month: September
  page_first: '1712'
  page_last: '1722'
  pages: "1712\u20131722"
  paper_id: '180'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1180.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1180.jpg
  title: TAG Parsing with Neural Networks and Vector Representations of Supertags
  title_html: <span class="acl-fixed-case">TAG</span> Parsing with Neural Networks
    and Vector Representations of Supertags
  url: https://www.aclweb.org/anthology/D17-1180
  year: '2017'
D17-1181:
  abstract: We introduce globally normalized convolutional neural networks for joint
    entity classification and relation extraction. In particular, we propose a way
    to utilize a linear-chain conditional random field output layer for predicting
    entity types and relations between entities at the same time. Our experiments
    show that global normalization outperforms a locally normalized softmax layer
    on a benchmark dataset.
  address: Copenhagen, Denmark
  author:
  - first: Heike
    full: Heike Adel
    id: heike-adel
    last: Adel
  - first: Hinrich
    full: "Hinrich Sch\xFCtze"
    id: hinrich-schutze
    last: "Sch\xFCtze"
  author_string: "Heike Adel, Hinrich Sch\xFCtze"
  bibkey: adel-schutze-2017-global
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1181
  month: September
  page_first: '1723'
  page_last: '1729'
  pages: "1723\u20131729"
  paper_id: '181'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1181.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1181.jpg
  title: Global Normalization of Convolutional Neural Networks for Joint Entity and
    Relation Classification
  title_html: Global Normalization of Convolutional Neural Networks for Joint Entity
    and Relation Classification
  url: https://www.aclweb.org/anthology/D17-1181
  year: '2017'
D17-1182:
  abstract: Neural networks have shown promising results for relation extraction.
    State-of-the-art models cast the task as an end-to-end problem, solved incrementally
    using a local classifier. Yet previous work using statistical models have demonstrated
    that global optimization can achieve better performances compared to local classification.
    We build a globally optimized neural model for end-to-end relation extraction,
    proposing novel LSTM features in order to better learn context representations.
    In addition, we present a novel method to integrate syntactic information to facilitate
    global learning, yet requiring little background on syntactic grammars thus being
    easy to extend. Experimental results show that our proposed model is highly effective,
    achieving the best performances on two standard benchmarks.
  address: Copenhagen, Denmark
  author:
  - first: Meishan
    full: Meishan Zhang
    id: meishan-zhang
    last: Zhang
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  - first: Guohong
    full: Guohong Fu
    id: guohong-fu
    last: Fu
  author_string: Meishan Zhang, Yue Zhang, Guohong Fu
  bibkey: zhang-etal-2017-end
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1182
  month: September
  page_first: '1730'
  page_last: '1740'
  pages: "1730\u20131740"
  paper_id: '182'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1182.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1182.jpg
  title: End-to-End Neural Relation Extraction with Global Optimization
  title_html: End-to-End Neural Relation Extraction with Global Optimization
  url: https://www.aclweb.org/anthology/D17-1182
  year: '2017'
D17-1183:
  abstract: "Automatic construction of large knowledge graphs (KG) by mining web-scale\
    \ text datasets has received considerable attention recently. Estimating accuracy\
    \ of such automatically constructed KGs is a challenging problem due to their\
    \ size and diversity. This important problem has largely been ignored in prior\
    \ research \u2013 we fill this gap and propose KGEval. KGEval uses coupling constraints\
    \ to bind facts and crowdsources those few that can infer large parts of the graph.\
    \ We demonstrate that the objective optimized by KGEval is submodular and NP-hard,\
    \ allowing guarantees for our approximation algorithm. Through experiments on\
    \ real-world datasets, we demonstrate that KGEval best estimates KG accuracy compared\
    \ to other baselines, while requiring significantly lesser number of human evaluations."
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1183.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1183.Attachment.zip
  author:
  - first: Prakhar
    full: Prakhar Ojha
    id: prakhar-ojha
    last: Ojha
  - first: Partha
    full: Partha Talukdar
    id: partha-talukdar
    last: Talukdar
  author_string: Prakhar Ojha, Partha Talukdar
  bibkey: ojha-talukdar-2017-kgeval
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1183
  month: September
  page_first: '1741'
  page_last: '1750'
  pages: "1741\u20131750"
  paper_id: '183'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1183.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1183.jpg
  title: 'KGEval: Accuracy Estimation of Automatically Constructed Knowledge Graphs'
  title_html: '<span class="acl-fixed-case">KGE</span>val: Accuracy Estimation of
    Automatically Constructed Knowledge Graphs'
  url: https://www.aclweb.org/anthology/D17-1183
  year: '2017'
D17-1184:
  abstract: Knowledge graph (KG) embedding techniques use structured relationships
    between entities to learn low-dimensional representations of entities and relations.
    One prominent goal of these approaches is to improve the quality of knowledge
    graphs by removing errors and adding missing facts. Surprisingly, most embedding
    techniques have been evaluated on benchmark datasets consisting of dense and reliable
    subsets of human-curated KGs, which tend to be fairly complete and have few errors.
    In this paper, we consider the problem of applying embedding techniques to KGs
    extracted from text, which are often incomplete and contain errors. We compare
    the sparsity and unreliability of different KGs and perform empirical experiments
    demonstrating how embedding approaches degrade as sparsity and unreliability increase.
  address: Copenhagen, Denmark
  author:
  - first: Jay
    full: Jay Pujara
    id: jay-pujara
    last: Pujara
  - first: Eriq
    full: Eriq Augustine
    id: eriq-augustine
    last: Augustine
  - first: Lise
    full: Lise Getoor
    id: lise-getoor
    last: Getoor
  author_string: Jay Pujara, Eriq Augustine, Lise Getoor
  bibkey: pujara-etal-2017-sparsity
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1184
  month: September
  page_first: '1751'
  page_last: '1756'
  pages: "1751\u20131756"
  paper_id: '184'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1184.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1184.jpg
  title: 'Sparsity and Noise: Where Knowledge Graph Embeddings Fall Short'
  title_html: 'Sparsity and Noise: Where Knowledge Graph Embeddings Fall Short'
  url: https://www.aclweb.org/anthology/D17-1184
  year: '2017'
D17-1185:
  abstract: Detection of lexico-semantic relations is one of the central tasks of
    computational semantics. Although some fundamental relations (e.g., hypernymy)
    are asymmetric, most existing models account for asymmetry only implicitly and
    use the same concept representations to support detection of symmetric and asymmetric
    relations alike. In this work, we propose the Dual Tensor model, a neural architecture
    with which we explicitly model the asymmetry and capture the translation between
    unspecialized and specialized word embeddings via a pair of tensors. Although
    our Dual Tensor model needs only unspecialized embeddings as input, our experiments
    on hypernymy and meronymy detection suggest that it can outperform more complex
    and resource-intensive models. We further demonstrate that the model can account
    for polysemy and that it exhibits stable performance across languages.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1185.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1185.Attachment.zip
  author:
  - first: Goran
    full: "Goran Glava\u0161"
    id: goran-glavas
    last: "Glava\u0161"
  - first: Simone Paolo
    full: Simone Paolo Ponzetto
    id: simone-paolo-ponzetto
    last: Ponzetto
  author_string: "Goran Glava\u0161, Simone Paolo Ponzetto"
  bibkey: glavas-ponzetto-2017-dual
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1185
  month: September
  page_first: '1757'
  page_last: '1767'
  pages: "1757\u20131767"
  paper_id: '185'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1185.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1185.jpg
  title: Dual Tensor Model for Detecting Asymmetric Lexico-Semantic Relations
  title_html: Dual Tensor Model for Detecting Asymmetric Lexico-Semantic Relations
  url: https://www.aclweb.org/anthology/D17-1185
  year: '2017'
D17-1186:
  abstract: Distantly supervised relation extraction has been widely used to find
    novel relational facts from plain text. To predict the relation between a pair
    of two target entities, existing methods solely rely on those direct sentences
    containing both entities. In fact, there are also many sentences containing only
    one of the target entities, which also provide rich useful information but not
    yet employed by relation extraction. To address this issue, we build inference
    chains between two target entities via intermediate entities, and propose a path-based
    neural relation extraction model to encode the relational semantics from both
    direct sentences and inference chains. Experimental results on real-world datasets
    show that, our model can make full use of those sentences containing only one
    target entity, and achieves significant and consistent improvements on relation
    extraction as compared with strong baselines. The source code of this paper can
    be obtained from https://github.com/thunlp/PathNRE.
  address: Copenhagen, Denmark
  author:
  - first: Wenyuan
    full: Wenyuan Zeng
    id: wenyuan-zeng
    last: Zeng
  - first: Yankai
    full: Yankai Lin
    id: yankai-lin
    last: Lin
  - first: Zhiyuan
    full: Zhiyuan Liu
    id: zhiyuan-liu
    last: Liu
  - first: Maosong
    full: Maosong Sun
    id: maosong-sun
    last: Sun
  author_string: Wenyuan Zeng, Yankai Lin, Zhiyuan Liu, Maosong Sun
  bibkey: zeng-etal-2017-incorporating
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1186
  month: September
  page_first: '1768'
  page_last: '1777'
  pages: "1768\u20131777"
  paper_id: '186'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1186.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1186.jpg
  title: Incorporating Relation Paths in Neural Relation Extraction
  title_html: Incorporating Relation Paths in Neural Relation Extraction
  url: https://www.aclweb.org/anthology/D17-1186
  year: '2017'
D17-1187:
  abstract: Adversarial training is a mean of regularizing classification algorithms
    by generating adversarial noise to the training data. We apply adversarial training
    in relation extraction within the multi-instance multi-label learning framework.
    We evaluate various neural network architectures on two different datasets. Experimental
    results demonstrate that adversarial training is generally effective for both
    CNN and RNN models and significantly improves the precision of predicted relations.
  address: Copenhagen, Denmark
  author:
  - first: Yi
    full: Yi Wu
    id: yi-wu
    last: Wu
  - first: David
    full: David Bamman
    id: david-bamman
    last: Bamman
  - first: Stuart
    full: Stuart Russell
    id: stuart-russell
    last: Russell
  author_string: Yi Wu, David Bamman, Stuart Russell
  bibkey: wu-etal-2017-adversarial
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1187
  month: September
  page_first: '1778'
  page_last: '1783'
  pages: "1778\u20131783"
  paper_id: '187'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1187.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1187.jpg
  title: Adversarial Training for Relation Extraction
  title_html: Adversarial Training for Relation Extraction
  url: https://www.aclweb.org/anthology/D17-1187
  year: '2017'
D17-1188:
  abstract: We demonstrate that for sentence-level relation extraction it is beneficial
    to consider other relations in the sentential context while predicting the target
    relation. Our architecture uses an LSTM-based encoder to jointly learn representations
    for all relations in a single sentence. We combine the context representations
    with an attention mechanism to make the final prediction. We use the Wikidata
    knowledge base to construct a dataset of multiple relations per sentence and to
    evaluate our approach. Compared to a baseline system, our method results in an
    average error reduction of 24 on a held-out set of relations. The code and the
    dataset to replicate the experiments are made available at https://github.com/ukplab/.
  address: Copenhagen, Denmark
  author:
  - first: Daniil
    full: Daniil Sorokin
    id: daniil-sorokin
    last: Sorokin
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Daniil Sorokin, Iryna Gurevych
  bibkey: sorokin-gurevych-2017-context
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1188
  month: September
  page_first: '1784'
  page_last: '1789'
  pages: "1784\u20131789"
  paper_id: '188'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1188.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1188.jpg
  title: Context-Aware Representations for Knowledge Base Relation Extraction
  title_html: Context-Aware Representations for Knowledge Base Relation Extraction
  url: https://www.aclweb.org/anthology/D17-1188
  year: '2017'
D17-1189:
  abstract: "Distant-supervised relation extraction inevitably suffers from wrong\
    \ labeling problems because it heuristically labels relational facts with knowledge\
    \ bases. Previous sentence level denoise models don\u2019t achieve satisfying\
    \ performances because they use hard labels which are determined by distant supervision\
    \ and immutable during training. To this end, we introduce an entity-pair level\
    \ denoise method which exploits semantic information from correctly labeled entity\
    \ pairs to correct wrong labels dynamically during training. We propose a joint\
    \ score function which combines the relational scores based on the entity-pair\
    \ representation and the confidence of the hard label to obtain a new label, namely\
    \ a soft label, for certain entity pair. During training, soft labels instead\
    \ of hard labels serve as gold labels. Experiments on the benchmark dataset show\
    \ that our method dramatically reduces noisy instances and outperforms other state-of-the-art\
    \ systems."
  address: Copenhagen, Denmark
  author:
  - first: Tianyu
    full: Tianyu Liu
    id: tianyu-liu
    last: Liu
  - first: Kexiang
    full: Kexiang Wang
    id: kexiang-wang
    last: Wang
  - first: Baobao
    full: Baobao Chang
    id: baobao-chang
    last: Chang
  - first: Zhifang
    full: Zhifang Sui
    id: zhifang-sui
    last: Sui
  author_string: Tianyu Liu, Kexiang Wang, Baobao Chang, Zhifang Sui
  bibkey: liu-etal-2017-soft
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1189
  month: September
  page_first: '1790'
  page_last: '1795'
  pages: "1790\u20131795"
  paper_id: '189'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1189.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1189.jpg
  title: A Soft-label Method for Noise-tolerant Distantly Supervised Relation Extraction
  title_html: A Soft-label Method for Noise-tolerant Distantly Supervised Relation
    Extraction
  url: https://www.aclweb.org/anthology/D17-1189
  year: '2017'
D17-1190:
  abstract: We present a sequential model for temporal relation classification between
    intra-sentence events. The key observation is that the overall syntactic structure
    and compositional meanings of the multi-word context between events are important
    for distinguishing among fine-grained temporal relations. Specifically, our approach
    first extracts a sequence of context words that indicates the temporal relation
    between two events, which well align with the dependency path between two event
    mentions. The context word sequence, together with a parts-of-speech tag sequence
    and a dependency relation sequence that are generated corresponding to the word
    sequence, are then provided as input to bidirectional recurrent neural network
    (LSTM) models. The neural nets learn compositional syntactic and semantic representations
    of contexts surrounding the two events and predict the temporal relation between
    them. Evaluation of the proposed approach on TimeBank corpus shows that sequential
    modeling is capable of accurately recognizing temporal relations between events,
    which outperforms a neural net model using various discrete features as input
    that imitates previous feature based models.
  address: Copenhagen, Denmark
  author:
  - first: Prafulla Kumar
    full: Prafulla Kumar Choubey
    id: prafulla-kumar-choubey
    last: Choubey
  - first: Ruihong
    full: Ruihong Huang
    id: ruihong-huang
    last: Huang
  author_string: Prafulla Kumar Choubey, Ruihong Huang
  bibkey: choubey-huang-2017-sequential
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1190
  month: September
  page_first: '1796'
  page_last: '1802'
  pages: "1796\u20131802"
  paper_id: '190'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1190.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1190.jpg
  title: A Sequential Model for Classifying Temporal Relations between Intra-Sentence
    Events
  title_html: A Sequential Model for Classifying Temporal Relations between Intra-Sentence
    Events
  url: https://www.aclweb.org/anthology/D17-1190
  year: '2017'
D17-1191:
  abstract: Deep residual learning (ResNet) is a new method for training very deep
    neural networks using identity mapping for shortcut connections. ResNet has won
    the ImageNet ILSVRC 2015 classification task, and achieved state-of-the-art performances
    in many computer vision tasks. However, the effect of residual learning on noisy
    natural language processing tasks is still not well understood. In this paper,
    we design a novel convolutional neural network (CNN) with residual learning, and
    investigate its impacts on the task of distantly supervised noisy relation extraction.
    In contradictory to popular beliefs that ResNet only works well for very deep
    networks, we found that even with 9 layers of CNNs, using identity mapping could
    significantly improve the performance for distantly-supervised relation extraction.
  address: Copenhagen, Denmark
  author:
  - first: Yi Yao
    full: Yi Yao Huang
    id: yi-yao-huang
    last: Huang
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  author_string: Yi Yao Huang, William Yang Wang
  bibkey: huang-wang-2017-deep
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1191
  month: September
  page_first: '1803'
  page_last: '1807'
  pages: "1803\u20131807"
  paper_id: '191'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1191.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1191.jpg
  title: Deep Residual Learning for Weakly-Supervised Relation Extraction
  title_html: Deep Residual Learning for Weakly-Supervised Relation Extraction
  url: https://www.aclweb.org/anthology/D17-1191
  year: '2017'
D17-1192:
  abstract: For the task of relation extraction, distant supervision is an efficient
    approach to generate labeled data by aligning knowledge base with free texts.
    The essence of it is a challenging incomplete multi-label classification problem
    with sparse and noisy features. To address the challenge, this work presents a
    novel nonparametric Bayesian formulation for the task. Experiment results show
    substantially higher top precision improvements over the traditional state-of-the-art
    approaches.
  address: Copenhagen, Denmark
  author:
  - first: Qing
    full: Qing Zhang
    id: qing-zhang
    last: Zhang
  - first: Houfeng
    full: Houfeng Wang
    id: houfeng-wang
    last: Wang
  author_string: Qing Zhang, Houfeng Wang
  bibkey: zhang-wang-2017-noise
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1192
  month: September
  page_first: '1808'
  page_last: '1813'
  pages: "1808\u20131813"
  paper_id: '192'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1192.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1192.jpg
  title: 'Noise-Clustered Distant Supervision for Relation Extraction: A Nonparametric
    Bayesian Perspective'
  title_html: 'Noise-Clustered Distant Supervision for Relation Extraction: A Nonparametric
    <span class="acl-fixed-case">B</span>ayesian Perspective'
  url: https://www.aclweb.org/anthology/D17-1192
  year: '2017'
D17-1193:
  abstract: "Word embeddings are used with success for a variety of tasks involving\
    \ lexical semantic similarities between individual words. Using unsupervised methods\
    \ and just cosine similarity, encouraging results were obtained for analogical\
    \ similarities. In this paper, we explore the potential of pre-trained word embeddings\
    \ to identify generic types of semantic relations in an unsupervised experiment.\
    \ We propose a new relational similarity measure based on the combination of word2vec\u2019\
    s CBOW input and output vectors which outperforms concurrent vector representations,\
    \ when used for unsupervised clustering on SemEval 2010 Relation Classification\
    \ data."
  address: Copenhagen, Denmark
  author:
  - first: Kata
    full: "Kata G\xE1bor"
    id: kata-gabor
    last: "G\xE1bor"
  - first: "Ha\xEFfa"
    full: "Ha\xEFfa Zargayouna"
    id: haifa-zargayouna
    last: Zargayouna
  - first: Isabelle
    full: Isabelle Tellier
    id: isabelle-tellier
    last: Tellier
  - first: Davide
    full: Davide Buscaldi
    id: davide-buscaldi
    last: Buscaldi
  - first: Thierry
    full: Thierry Charnois
    id: thierry-charnois
    last: Charnois
  author_string: "Kata G\xE1bor, Ha\xEFfa Zargayouna, Isabelle Tellier, Davide Buscaldi,\
    \ Thierry Charnois"
  bibkey: gabor-etal-2017-exploring
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1193
  month: September
  page_first: '1814'
  page_last: '1823'
  pages: "1814\u20131823"
  paper_id: '193'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1193.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1193.jpg
  title: Exploring Vector Spaces for Semantic Relations
  title_html: Exploring Vector Spaces for Semantic Relations
  url: https://www.aclweb.org/anthology/D17-1193
  year: '2017'
D17-1194:
  abstract: "This paper deals with using word embedding models to trace the temporal\
    \ dynamics of semantic relations between pairs of words. The set-up is similar\
    \ to the well-known analogies task, but expanded with a time dimension. To this\
    \ end, we apply incremental updating of the models with new training texts, including\
    \ incremental vocabulary expansion, coupled with learned transformation matrices\
    \ that let us map between members of the relation. The proposed approach is evaluated\
    \ on the task of predicting insurgent armed groups based on geographical locations.\
    \ The gold standard data for the time span 1994\u20132010 is extracted from the\
    \ UCDP Armed Conflicts dataset. The results show that the method is feasible and\
    \ outperforms the baselines, but also that important work still remains to be\
    \ done."
  address: Copenhagen, Denmark
  author:
  - first: Andrey
    full: Andrey Kutuzov
    id: andrey-kutuzov
    last: Kutuzov
  - first: Erik
    full: Erik Velldal
    id: erik-velldal
    last: Velldal
  - first: Lilja
    full: "Lilja \xD8vrelid"
    id: lilja-ovrelid
    last: "\xD8vrelid"
  author_string: "Andrey Kutuzov, Erik Velldal, Lilja \xD8vrelid"
  bibkey: kutuzov-etal-2017-temporal
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1194
  month: September
  page_first: '1824'
  page_last: '1829'
  pages: "1824\u20131829"
  paper_id: '194'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1194.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1194.jpg
  title: 'Temporal dynamics of semantic relations in word embeddings: an application
    to predicting armed conflict participants'
  title_html: 'Temporal dynamics of semantic relations in word embeddings: an application
    to predicting armed conflict participants'
  url: https://www.aclweb.org/anthology/D17-1194
  year: '2017'
D17-1195:
  abstract: Understanding a long document requires tracking how entities are introduced
    and evolve over time. We present a new type of language model, EntityNLM, that
    can explicitly model entities, dynamically update their representations, and contextually
    generate their mentions. Our model is generative and flexible; it can model an
    arbitrary number of entities in context while generating each entity mention at
    an arbitrary length. In addition, it can be used for several different tasks such
    as language modeling, coreference resolution, and entity prediction. Experimental
    results with all these tasks demonstrate that our model consistently outperforms
    strong baselines and prior work.
  address: Copenhagen, Denmark
  author:
  - first: Yangfeng
    full: Yangfeng Ji
    id: yangfeng-ji
    last: Ji
  - first: Chenhao
    full: Chenhao Tan
    id: chenhao-tan
    last: Tan
  - first: Sebastian
    full: Sebastian Martschat
    id: sebastian-martschat
    last: Martschat
  - first: Yejin
    full: Yejin Choi
    id: yejin-choi
    last: Choi
  - first: Noah A.
    full: Noah A. Smith
    id: noah-a-smith
    last: Smith
  author_string: Yangfeng Ji, Chenhao Tan, Sebastian Martschat, Yejin Choi, Noah A.
    Smith
  bibkey: ji-etal-2017-dynamic
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1195
  month: September
  page_first: '1830'
  page_last: '1839'
  pages: "1830\u20131839"
  paper_id: '195'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1195.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1195.jpg
  title: Dynamic Entity Representations in Neural Language Models
  title_html: Dynamic Entity Representations in Neural Language Models
  url: https://www.aclweb.org/anthology/D17-1195
  year: '2017'
D17-1196:
  abstract: This paper presents a new approach for building Language Models using
    the Quantum Probability Theory, a Quantum Language Model (QLM). It mainly shows
    that relying on this probability calculus it is possible to build stochastic models
    able to benefit from quantum correlations due to interference and entanglement.
    We extensively tested our approach showing its superior performances, both in
    terms of model perplexity and inserting it into an automatic speech recognition
    evaluation setting, when compared with state-of-the-art language modelling techniques.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1196.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1196.Attachment.zip
  author:
  - first: Ivano
    full: Ivano Basile
    id: ivano-basile
    last: Basile
  - first: Fabio
    full: Fabio Tamburini
    id: fabio-tamburini
    last: Tamburini
  author_string: Ivano Basile, Fabio Tamburini
  bibkey: basile-tamburini-2017-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1196
  month: September
  page_first: '1840'
  page_last: '1849'
  pages: "1840\u20131849"
  paper_id: '196'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1196.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1196.jpg
  title: Towards Quantum Language Models
  title_html: Towards Quantum Language Models
  url: https://www.aclweb.org/anthology/D17-1196
  year: '2017'
D17-1197:
  abstract: "We propose a general class of language models that treat reference as\
    \ discrete stochastic latent variables. This decision allows for the creation\
    \ of entity mentions by accessing external databases of referents (required by,\
    \ e.g., dialogue generation) or past internal state (required to explicitly model\
    \ coreferentiality). Beyond simple copying, our coreference model can additionally\
    \ refer to a referent using varied mention forms (e.g., a reference to \u201C\
    Jane\u201D can be realized as \u201Cshe\u201D), a characteristic feature of reference\
    \ in natural languages. Experiments on three representative applications show\
    \ our model variants outperform models based on deterministic attention and standard\
    \ language modeling baselines."
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1197.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1197.Attachment.pdf
  author:
  - first: Zichao
    full: Zichao Yang
    id: zichao-yang
    last: Yang
  - first: Phil
    full: Phil Blunsom
    id: phil-blunsom
    last: Blunsom
  - first: Chris
    full: Chris Dyer
    id: chris-dyer
    last: Dyer
  - first: Wang
    full: Wang Ling
    id: wang-ling
    last: Ling
  author_string: Zichao Yang, Phil Blunsom, Chris Dyer, Wang Ling
  bibkey: yang-etal-2017-reference
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1197
  month: September
  page_first: '1850'
  page_last: '1859'
  pages: "1850\u20131859"
  paper_id: '197'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1197.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1197.jpg
  title: Reference-Aware Language Models
  title_html: Reference-Aware Language Models
  url: https://www.aclweb.org/anthology/D17-1197
  year: '2017'
D17-1198:
  abstract: "In this study, we introduce a new approach for learning language models\
    \ by training them to estimate word-context pointwise mutual information (PMI),\
    \ and then deriving the desired conditional probabilities from PMI at test time.\
    \ Specifically, we show that with minor modifications to word2vec\u2019s algorithm,\
    \ we get principled language models that are closely related to the well-established\
    \ Noise Contrastive Estimation (NCE) based language models. A compelling aspect\
    \ of our approach is that our models are trained with the same simple negative\
    \ sampling objective function that is commonly used in word2vec to learn word\
    \ embeddings."
  address: Copenhagen, Denmark
  author:
  - first: Oren
    full: Oren Melamud
    id: oren-melamud
    last: Melamud
  - first: Ido
    full: Ido Dagan
    id: ido-dagan
    last: Dagan
  - first: Jacob
    full: Jacob Goldberger
    id: jacob-goldberger
    last: Goldberger
  author_string: Oren Melamud, Ido Dagan, Jacob Goldberger
  bibkey: melamud-etal-2017-simple
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1198
  month: September
  page_first: '1860'
  page_last: '1865'
  pages: "1860\u20131865"
  paper_id: '198'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1198.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1198.jpg
  title: A Simple Language Model based on PMI Matrix Approximations
  title_html: A Simple Language Model based on <span class="acl-fixed-case">PMI</span>
    Matrix Approximations
  url: https://www.aclweb.org/anthology/D17-1198
  year: '2017'
D17-1199:
  abstract: Syllabification does not seem to improve word-level RNN language modeling
    quality when compared to character-based segmentation. However, our best syllable-aware
    language model, achieving performance comparable to the competitive character-aware
    model, has 18%-33% fewer parameters and is trained 1.2-2.2 times faster.
  address: Copenhagen, Denmark
  author:
  - first: Zhenisbek
    full: Zhenisbek Assylbekov
    id: zhenisbek-assylbekov
    last: Assylbekov
  - first: Rustem
    full: Rustem Takhanov
    id: rustem-takhanov
    last: Takhanov
  - first: Bagdat
    full: Bagdat Myrzakhmetov
    id: bagdat-myrzakhmetov
    last: Myrzakhmetov
  - first: Jonathan N.
    full: Jonathan N. Washington
    id: jonathan-washington
    last: Washington
  author_string: Zhenisbek Assylbekov, Rustem Takhanov, Bagdat Myrzakhmetov, Jonathan
    N. Washington
  bibkey: assylbekov-etal-2017-syllable
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1199
  month: September
  page_first: '1866'
  page_last: '1872'
  pages: "1866\u20131872"
  paper_id: '199'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1199.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1199.jpg
  title: 'Syllable-aware Neural Language Models: A Failure to Beat Character-aware
    Ones'
  title_html: 'Syllable-aware Neural Language Models: A Failure to Beat Character-aware
    Ones'
  url: https://www.aclweb.org/anthology/D17-1199
  year: '2017'
D17-1200:
  abstract: Automatically understanding the plot of novels is important both for informing
    literary scholarship and applications such as summarization or recommendation.
    Various models have addressed this task, but their evaluation has remained largely
    intrinsic and qualitative. Here, we propose a principled and scalable framework
    leveraging expert-provided semantic tags (e.g., mystery, pirates) to evaluate
    plot representations in an extrinsic fashion, assessing their ability to produce
    locally coherent groupings of novels (micro-clusters) in model space. We present
    a deep recurrent autoencoder model that learns richly structured multi-view plot
    representations, and show that they i) yield better micro-clusters than less structured
    representations; and ii) are interpretable, and thus useful for further literary
    analysis or labeling of the emerging micro-clusters.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1200.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1200.Attachment.zip
  author:
  - first: Lea
    full: Lea Frermann
    id: lea-frermann
    last: Frermann
  - first: "Gy\xF6rgy"
    full: "Gy\xF6rgy Szarvas"
    id: gyorgy-szarvas
    last: Szarvas
  author_string: "Lea Frermann, Gy\xF6rgy Szarvas"
  bibkey: frermann-szarvas-2017-inducing
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1200
  month: September
  page_first: '1873'
  page_last: '1883'
  pages: "1873\u20131883"
  paper_id: '200'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1200.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1200.jpg
  title: Inducing Semantic Micro-Clusters from Deep Multi-View Representations of
    Novels
  title_html: Inducing Semantic Micro-Clusters from Deep Multi-View Representations
    of Novels
  url: https://www.aclweb.org/anthology/D17-1200
  year: '2017'
D17-1201:
  abstract: Convolutional Neural Networks (CNNs) are widely used in NLP tasks. This
    paper presents a novel weight initialization method to improve the CNNs for text
    classification. Instead of randomly initializing the convolutional filters, we
    encode semantic features into them, which helps the model focus on learning useful
    features at the beginning of the training. Experiments demonstrate the effectiveness
    of the initialization technique on seven text classification tasks, including
    sentiment analysis and topic classification.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1201.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1201.Attachment.pdf
  author:
  - first: Shen
    full: Shen Li
    id: shen-li
    last: Li
  - first: Zhe
    full: Zhe Zhao
    id: zhe-zhao
    last: Zhao
  - first: Tao
    full: Tao Liu
    id: tao-liu
    last: Liu
  - first: Renfen
    full: Renfen Hu
    id: renfen-hu
    last: Hu
  - first: Xiaoyong
    full: Xiaoyong Du
    id: xiaoyong-du
    last: Du
  author_string: Shen Li, Zhe Zhao, Tao Liu, Renfen Hu, Xiaoyong Du
  bibkey: li-etal-2017-initializing
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1201
  month: September
  page_first: '1884'
  page_last: '1889'
  pages: "1884\u20131889"
  paper_id: '201'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1201.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1201.jpg
  title: Initializing Convolutional Filters with Semantic Features for Text Classification
  title_html: Initializing Convolutional Filters with Semantic Features for Text Classification
  url: https://www.aclweb.org/anthology/D17-1201
  year: '2017'
D17-1202:
  abstract: In this paper, we present a novel document similarity measure based on
    the definition of a graph kernel between pairs of documents. The proposed measure
    takes into account both the terms contained in the documents and the relationships
    between them. By representing each document as a graph-of-words, we are able to
    model these relationships and then determine how similar two documents are by
    using a modified shortest-path graph kernel. We evaluate our approach on two tasks
    and compare it against several baseline approaches using various performance metrics
    such as DET curves and macro-average F1-score. Experimental results on a range
    of datasets showed that our proposed approach outperforms traditional techniques
    and is capable of measuring more accurately the similarity between two documents.
  address: Copenhagen, Denmark
  author:
  - first: Giannis
    full: Giannis Nikolentzos
    id: giannis-nikolentzos
    last: Nikolentzos
  - first: Polykarpos
    full: Polykarpos Meladianos
    id: polykarpos-meladianos
    last: Meladianos
  - first: "Fran\xE7ois"
    full: "Fran\xE7ois Rousseau"
    id: francois-rousseau
    last: Rousseau
  - first: Yannis
    full: Yannis Stavrakas
    id: yannis-stavrakas
    last: Stavrakas
  - first: Michalis
    full: Michalis Vazirgiannis
    id: michalis-vazirgiannis
    last: Vazirgiannis
  author_string: "Giannis Nikolentzos, Polykarpos Meladianos, Fran\xE7ois Rousseau,\
    \ Yannis Stavrakas, Michalis Vazirgiannis"
  bibkey: nikolentzos-etal-2017-shortest
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1202
  month: September
  page_first: '1890'
  page_last: '1900'
  pages: "1890\u20131900"
  paper_id: '202'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1202.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1202.jpg
  title: Shortest-Path Graph Kernels for Document Similarity
  title_html: Shortest-Path Graph Kernels for Document Similarity
  url: https://www.aclweb.org/anthology/D17-1202
  year: '2017'
D17-1203:
  abstract: Models work best when they are optimized taking into account the evaluation
    criteria that people care about. For topic models, people often care about interpretability,
    which can be approximated using measures of lexical association. We integrate
    lexical association into topic optimization using tree priors, which provide a
    flexible framework that can take advantage of both first order word associations
    and the higher-order associations captured by word embeddings. Tree priors improve
    topic interpretability without hurting extrinsic performance.
  address: Copenhagen, Denmark
  author:
  - first: Weiwei
    full: Weiwei Yang
    id: weiwei-yang
    last: Yang
  - first: Jordan
    full: Jordan Boyd-Graber
    id: jordan-boyd-graber
    last: Boyd-Graber
  - first: Philip
    full: Philip Resnik
    id: philip-resnik
    last: Resnik
  author_string: Weiwei Yang, Jordan Boyd-Graber, Philip Resnik
  bibkey: yang-etal-2017-adapting
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1203
  month: September
  page_first: '1901'
  page_last: '1906'
  pages: "1901\u20131906"
  paper_id: '203'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1203.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1203.jpg
  title: Adapting Topic Models using Lexical Associations with Tree Priors
  title_html: Adapting Topic Models using Lexical Associations with Tree Priors
  url: https://www.aclweb.org/anthology/D17-1203
  year: '2017'
D17-1204:
  abstract: Crowdsourcing offers a convenient means of obtaining labeled data quickly
    and inexpensively. However, crowdsourced labels are often noisier than expert-annotated
    data, making it difficult to aggregate them meaningfully. We present an aggregation
    approach that learns a regression model from crowdsourced annotations to predict
    aggregated labels for instances that have no expert adjudications. The predicted
    labels achieve a correlation of 0.594 with expert labels on our data, outperforming
    the best alternative aggregation method by 11.9%. Our approach also outperforms
    the alternatives on third-party datasets.
  address: Copenhagen, Denmark
  author:
  - first: Natalie
    full: Natalie Parde
    id: natalie-parde
    last: Parde
  - first: Rodney
    full: Rodney Nielsen
    id: rodney-nielsen
    last: Nielsen
  author_string: Natalie Parde, Rodney Nielsen
  bibkey: parde-nielsen-2017-finding
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1204
  month: September
  page_first: '1907'
  page_last: '1912'
  pages: "1907\u20131912"
  paper_id: '204'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1204.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1204.jpg
  title: 'Finding Patterns in Noisy Crowds: Regression-based Annotation Aggregation
    for Crowdsourced Data'
  title_html: 'Finding Patterns in Noisy Crowds: Regression-based Annotation Aggregation
    for Crowdsourced Data'
  url: https://www.aclweb.org/anthology/D17-1204
  year: '2017'
D17-1205:
  abstract: Crowdsourcing has proven to be an effective method for generating labeled
    data for a range of NLP tasks. However, multiple recent attempts of using crowdsourcing
    to generate gold-labeled training data for semantic role labeling (SRL) reported
    only modest results, indicating that SRL is perhaps too difficult a task to be
    effectively crowdsourced. In this paper, we postulate that while producing SRL
    annotation does require expert involvement in general, a large subset of SRL labeling
    tasks is in fact appropriate for the crowd. We present a novel workflow in which
    we employ a classifier to identify difficult annotation tasks and route each task
    either to experts or crowd workers according to their difficulties. Our experimental
    evaluation shows that the proposed approach reduces the workload for experts by
    over two-thirds, and thus significantly reduces the cost of producing SRL annotation
    at little loss in quality.
  address: Copenhagen, Denmark
  author:
  - first: Chenguang
    full: Chenguang Wang
    id: chenguang-wang
    last: Wang
  - first: Alan
    full: Alan Akbik
    id: alan-akbik
    last: Akbik
  - first: Laura
    full: Laura Chiticariu
    id: laura-chiticariu
    last: Chiticariu
  - first: Yunyao
    full: Yunyao Li
    id: yunyao-li
    last: Li
  - first: Fei
    full: Fei Xia
    id: fei-xia
    last: Xia
  - first: Anbang
    full: Anbang Xu
    id: anbang-xu
    last: Xu
  author_string: Chenguang Wang, Alan Akbik, Laura Chiticariu, Yunyao Li, Fei Xia,
    Anbang Xu
  bibkey: wang-etal-2017-crowd
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1205
  month: September
  page_first: '1913'
  page_last: '1922'
  pages: "1913\u20131922"
  paper_id: '205'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1205.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1205.jpg
  title: 'CROWD-IN-THE-LOOP: A Hybrid Approach for Annotating Semantic Roles'
  title_html: '<span class="acl-fixed-case">CROWD</span>-<span class="acl-fixed-case">IN</span>-<span
    class="acl-fixed-case">THE</span>-<span class="acl-fixed-case">LOOP</span>: A
    Hybrid Approach for Annotating Semantic Roles'
  url: https://www.aclweb.org/anthology/D17-1205
  year: '2017'
D17-1206:
  abstract: "Transfer and multi-task learning have traditionally focused on either\
    \ a single source-target pair or very few, similar tasks. Ideally, the linguistic\
    \ levels of morphology, syntax and semantics would benefit each other by being\
    \ trained in a single model. We introduce a joint many-task model together with\
    \ a strategy for successively growing its depth to solve increasingly complex\
    \ tasks. Higher layers include shortcut connections to lower-level task predictions\
    \ to reflect linguistic hierarchies. We use a simple regularization term to allow\
    \ for optimizing all model weights to improve one task\u2019s loss without exhibiting\
    \ catastrophic interference of the other tasks. Our single end-to-end model obtains\
    \ state-of-the-art or competitive results on five different tasks from tagging,\
    \ parsing, relatedness, and entailment tasks."
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1206.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1206.Attachment.zip
  author:
  - first: Kazuma
    full: Kazuma Hashimoto
    id: kazuma-hashimoto
    last: Hashimoto
  - first: Caiming
    full: Caiming Xiong
    id: caiming-xiong
    last: Xiong
  - first: Yoshimasa
    full: Yoshimasa Tsuruoka
    id: yoshimasa-tsuruoka
    last: Tsuruoka
  - first: Richard
    full: Richard Socher
    id: richard-socher
    last: Socher
  author_string: Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, Richard Socher
  bibkey: hashimoto-etal-2017-joint
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1206
  month: September
  page_first: '1923'
  page_last: '1933'
  pages: "1923\u20131933"
  paper_id: '206'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1206.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1206.jpg
  title: 'A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks'
  title_html: 'A Joint Many-Task Model: Growing a Neural Network for Multiple <span
    class="acl-fixed-case">NLP</span> Tasks'
  url: https://www.aclweb.org/anthology/D17-1206
  year: '2017'
D17-1207:
  abstract: "Cross-lingual natural language processing hinges on the premise that\
    \ there exists invariance across languages. At the word level, researchers have\
    \ identified such invariance in the word embedding semantic spaces of different\
    \ languages. However, in order to connect the separate spaces, cross-lingual supervision\
    \ encoded in parallel data is typically required. In this paper, we attempt to\
    \ establish the cross-lingual connection without relying on any cross-lingual\
    \ supervision. By viewing word embedding spaces as distributions, we propose to\
    \ minimize their earth mover\u2019s distance, a measure of divergence between\
    \ distributions. We demonstrate the success on the unsupervised bilingual lexicon\
    \ induction task. In addition, we reveal an interesting finding that the earth\
    \ mover\u2019s distance shows potential as a measure of language difference."
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238232779
    type: video
    url: https://vimeo.com/238232779
  author:
  - first: Meng
    full: Meng Zhang
    id: meng-zhang
    last: Zhang
  - first: Yang
    full: Yang Liu
    id: yang-liu-ict
    last: Liu
  - first: Huanbo
    full: Huanbo Luan
    id: huanbo-luan
    last: Luan
  - first: Maosong
    full: Maosong Sun
    id: maosong-sun
    last: Sun
  author_string: Meng Zhang, Yang Liu, Huanbo Luan, Maosong Sun
  bibkey: zhang-etal-2017-earth
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1207
  month: September
  page_first: '1934'
  page_last: '1945'
  pages: "1934\u20131945"
  paper_id: '207'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1207.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1207.jpg
  title: "Earth Mover\u2019s Distance Minimization for Unsupervised Bilingual Lexicon\
    \ Induction"
  title_html: "Earth Mover\u2019s Distance Minimization for Unsupervised Bilingual\
    \ Lexicon Induction"
  url: https://www.aclweb.org/anthology/D17-1207
  year: '2017'
D17-1208:
  abstract: Ensembling is a well-known technique in neural machine translation (NMT)
    to improve system performance. Instead of a single neural net, multiple neural
    nets with the same topology are trained separately, and the decoder generates
    predictions by averaging over the individual models. Ensembling often improves
    the quality of the generated translations drastically. However, it is not suitable
    for production systems because it is cumbersome and slow. This work aims to reduce
    the runtime to be on par with a single system without compromising the translation
    quality. First, we show that the ensemble can be unfolded into a single large
    neural network which imitates the output of the ensemble system. We show that
    unfolding can already improve the runtime in practice since more work can be done
    on the GPU. We proceed by describing a set of techniques to shrink the unfolded
    network by reducing the dimensionality of layers. On Japanese-English we report
    that the resulting network has the size and decoding speed of a single NMT network
    but performs on the level of a 3-ensemble system.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238236553
    type: video
    url: https://vimeo.com/238236553
  author:
  - first: Felix
    full: Felix Stahlberg
    id: felix-stahlberg
    last: Stahlberg
  - first: Bill
    full: Bill Byrne
    id: bill-byrne
    last: Byrne
  author_string: Felix Stahlberg, Bill Byrne
  bibkey: stahlberg-byrne-2017-unfolding
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1208
  month: September
  page_first: '1946'
  page_last: '1956'
  pages: "1946\u20131956"
  paper_id: '208'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1208.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1208.jpg
  title: Unfolding and Shrinking Neural Machine Translation Ensembles
  title_html: Unfolding and Shrinking Neural Machine Translation Ensembles
  url: https://www.aclweb.org/anthology/D17-1208
  year: '2017'
D17-1209:
  abstract: We present a simple and effective approach to incorporating syntactic
    structure into neural attention-based encoder-decoder models for machine translation.
    We rely on graph-convolutional networks (GCNs), a recent class of neural networks
    developed for modeling graph-structured data. Our GCNs use predicted syntactic
    dependency trees of source sentences to produce representations of words (i.e.
    hidden states of the encoder) that are sensitive to their syntactic neighborhoods.
    GCNs take word representations as input and produce word representations as output,
    so they can easily be incorporated as layers into standard encoders (e.g., on
    top of bidirectional RNNs or convolutional neural networks). We evaluate their
    effectiveness with English-German and English-Czech translation experiments for
    different types of encoders and observe substantial improvements over their syntax-agnostic
    versions in all the considered setups.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238233360
    type: video
    url: https://vimeo.com/238233360
  author:
  - first: Joost
    full: Joost Bastings
    id: joost-bastings
    last: Bastings
  - first: Ivan
    full: Ivan Titov
    id: ivan-titov
    last: Titov
  - first: Wilker
    full: Wilker Aziz
    id: wilker-aziz
    last: Aziz
  - first: Diego
    full: Diego Marcheggiani
    id: diego-marcheggiani
    last: Marcheggiani
  - first: Khalil
    full: "Khalil Sima\u2019an"
    id: khalil-simaan
    last: "Sima\u2019an"
  author_string: "Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, Khalil\
    \ Sima\u2019an"
  bibkey: bastings-etal-2017-graph
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1209
  month: September
  page_first: '1957'
  page_last: '1967'
  pages: "1957\u20131967"
  paper_id: '209'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1209.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1209.jpg
  title: Graph Convolutional Encoders for Syntax-aware Neural Machine Translation
  title_html: Graph Convolutional Encoders for Syntax-aware Neural Machine Translation
  url: https://www.aclweb.org/anthology/D17-1209
  year: '2017'
D17-1210:
  abstract: Recent research in neural machine translation has largely focused on two
    aspects; neural network architectures and end-to-end learning algorithms. The
    problem of decoding, however, has received relatively little attention from the
    research community. In this paper, we solely focus on the problem of decoding
    given a trained neural machine translation model. Instead of trying to build a
    new decoding algorithm for any specific decoding objective, we propose the idea
    of trainable decoding algorithm in which we train a decoding algorithm to find
    a translation that maximizes an arbitrary decoding objective. More specifically,
    we design an actor that observes and manipulates the hidden state of the neural
    machine translation decoder and propose to train it using a variant of deterministic
    policy gradient. We extensively evaluate the proposed algorithm using four language
    pairs and two decoding objectives and show that we can indeed train a trainable
    greedy decoder that generates a better translation (in terms of a target decoding
    objective) with minimal computational overhead.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238236435
    type: video
    url: https://vimeo.com/238236435
  author:
  - first: Jiatao
    full: Jiatao Gu
    id: jiatao-gu
    last: Gu
  - first: Kyunghyun
    full: Kyunghyun Cho
    id: kyunghyun-cho
    last: Cho
  - first: Victor O.K.
    full: Victor O.K. Li
    id: victor-o-k-li
    last: Li
  author_string: Jiatao Gu, Kyunghyun Cho, Victor O.K. Li
  bibkey: gu-etal-2017-trainable
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1210
  month: September
  page_first: '1968'
  page_last: '1978'
  pages: "1968\u20131978"
  paper_id: '210'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1210.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1210.jpg
  title: Trainable Greedy Decoding for Neural Machine Translation
  title_html: Trainable Greedy Decoding for Neural Machine Translation
  url: https://www.aclweb.org/anthology/D17-1210
  year: '2017'
D17-1211:
  abstract: Satirical news is considered to be entertainment, but it is potentially
    deceptive and harmful. Despite the embedded genre in the article, not everyone
    can recognize the satirical cues and therefore believe the news as true news.
    We observe that satirical cues are often reflected in certain paragraphs rather
    than the whole document. Existing works only consider document-level features
    to detect the satire, which could be limited. We consider paragraph-level linguistic
    features to unveil the satire by incorporating neural network and attention mechanism.
    We investigate the difference between paragraph-level features and document-level
    features, and analyze them on a large satirical news dataset. The evaluation shows
    that the proposed model detects satirical news effectively and reveals what features
    are important at which level.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238235500
    type: video
    url: https://vimeo.com/238235500
  author:
  - first: Fan
    full: Fan Yang
    id: fan-yang
    last: Yang
  - first: Arjun
    full: Arjun Mukherjee
    id: arjun-mukherjee
    last: Mukherjee
  - first: Eduard
    full: Eduard Dragut
    id: eduard-dragut
    last: Dragut
  author_string: Fan Yang, Arjun Mukherjee, Eduard Dragut
  bibkey: yang-etal-2017-satirical
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1211
  month: September
  page_first: '1979'
  page_last: '1989'
  pages: "1979\u20131989"
  paper_id: '211'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1211.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1211.jpg
  title: Satirical News Detection and Analysis using Attention Mechanism and Linguistic
    Features
  title_html: Satirical News Detection and Analysis using Attention Mechanism and
    Linguistic Features
  url: https://www.aclweb.org/anthology/D17-1211
  year: '2017'
D17-1212:
  abstract: Verifiability is one of the core editing principles in Wikipedia, where
    editors are encouraged to provide citations for the added content. For a Wikipedia
    article determining what content is covered by a citation or the citation span
    is not trivial, an important aspect for automated citation finding for uncovered
    content, or fact assessments. We address the problem of determining the citation
    span in Wikipedia articles. We approach this problem by classifying which textual
    fragments in an article are covered or hold true given a citation. We propose
    a sequence classification approach where for a paragraph and a citation, we determine
    the citation span at a fine-grained level. We provide a thorough experimental
    evaluation and compare our approach against baselines adopted from the scientific
    domain, where we show improvement for all evaluation metrics.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238233167
    type: video
    url: https://vimeo.com/238233167
  author:
  - first: Besnik
    full: Besnik Fetahu
    id: besnik-fetahu
    last: Fetahu
  - first: Katja
    full: Katja Markert
    id: katja-markert
    last: Markert
  - first: Avishek
    full: Avishek Anand
    id: avishek-anand
    last: Anand
  author_string: Besnik Fetahu, Katja Markert, Avishek Anand
  bibkey: fetahu-etal-2017-fine
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1212
  month: September
  page_first: '1990'
  page_last: '1999'
  pages: "1990\u20131999"
  paper_id: '212'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1212.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1212.jpg
  title: Fine Grained Citation Span for References in Wikipedia
  title_html: Fine Grained Citation Span for References in <span class="acl-fixed-case">W</span>ikipedia
  url: https://www.aclweb.org/anthology/D17-1212
  year: '2017'
D17-1213:
  abstract: 'Most studies on human editing focus merely on syntactic revision operations,
    failing to capture the intentions behind revision changes, which are essential
    for facilitating the single and collaborative writing process. In this work, we
    develop in collaboration with Wikipedia editors a 13-category taxonomy of the
    semantic intention behind edits in Wikipedia articles. Using labeled article edits,
    we build a computational classifier of intentions that achieved a micro-averaged
    F1 score of 0.621. We use this model to investigate edit intention effectiveness:
    how different types of edits predict the retention of newcomers and changes in
    the quality of articles, two key concerns for Wikipedia today. Our analysis shows
    that the types of edits that users make in their first session predict their subsequent
    survival as Wikipedia editors, and articles in different stages need different
    types of edits.'
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238233635
    type: video
    url: https://vimeo.com/238233635
  author:
  - first: Diyi
    full: Diyi Yang
    id: diyi-yang
    last: Yang
  - first: Aaron
    full: Aaron Halfaker
    id: aaron-halfaker
    last: Halfaker
  - first: Robert
    full: Robert Kraut
    id: robert-kraut
    last: Kraut
  - first: Eduard
    full: Eduard Hovy
    id: eduard-hovy
    last: Hovy
  author_string: Diyi Yang, Aaron Halfaker, Robert Kraut, Eduard Hovy
  bibkey: yang-etal-2017-identifying-semantic
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1213
  month: September
  page_first: '2000'
  page_last: '2010'
  pages: "2000\u20132010"
  paper_id: '213'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1213.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1213.jpg
  title: Identifying Semantic Edit Intentions from Revisions in Wikipedia
  title_html: Identifying Semantic Edit Intentions from Revisions in <span class="acl-fixed-case">W</span>ikipedia
  url: https://www.aclweb.org/anthology/D17-1213
  year: '2017'
D17-1214:
  abstract: We introduce a hierarchical architecture for machine reading capable of
    extracting precise information from long documents. The model divides the document
    into small, overlapping windows and encodes all windows in parallel with an RNN.
    It then attends over these window encodings, reducing them to a single encoding,
    which is decoded into an answer using a sequence decoder. This hierarchical approach
    allows the model to scale to longer documents without increasing the number of
    sequential steps. In a supervised setting, our model achieves state of the art
    accuracy of 76.8 on the WikiReading dataset. We also evaluate the model in a semi-supervised
    setting by downsampling the WikiReading training set to create increasingly smaller
    amounts of supervision, while leaving the full unlabeled document corpus to train
    a sequence autoencoder on document windows. We evaluate models that can reuse
    autoencoder states and outputs without fine-tuning their weights, allowing for
    more efficient training and inference.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238231359
    type: video
    url: https://vimeo.com/238231359
  author:
  - first: Daniel
    full: Daniel Hewlett
    id: daniel-hewlett
    last: Hewlett
  - first: Llion
    full: Llion Jones
    id: llion-jones
    last: Jones
  - first: Alexandre
    full: Alexandre Lacoste
    id: alexandre-lacoste
    last: Lacoste
  - first: Izzeddin
    full: Izzeddin Gur
    id: izzeddin-gur1
    last: Gur
  author_string: Daniel Hewlett, Llion Jones, Alexandre Lacoste, Izzeddin Gur
  bibkey: hewlett-etal-2017-accurate
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1214
  month: September
  page_first: '2011'
  page_last: '2020'
  pages: "2011\u20132020"
  paper_id: '214'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1214.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1214.jpg
  title: Accurate Supervised and Semi-Supervised Machine Reading for Long Documents
  title_html: Accurate Supervised and Semi-Supervised Machine Reading for Long Documents
  url: https://www.aclweb.org/anthology/D17-1214
  year: '2017'
D17-1215:
  abstract: Standard accuracy metrics indicate that reading comprehension systems
    are making rapid progress, but the extent to which these systems truly understand
    language remains unclear. To reward systems with real language understanding abilities,
    we propose an adversarial evaluation scheme for the Stanford Question Answering
    Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs
    that contain adversarially inserted sentences, which are automatically generated
    to distract computer systems without changing the correct answer or misleading
    humans. In this adversarial setting, the accuracy of sixteen published models
    drops from an average of 75% F1 score to 36%; when the adversary is allowed to
    add ungrammatical sequences of words, average accuracy on four models decreases
    further to 7%. We hope our insights will motivate the development of new models
    that understand language more precisely.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238231419
    type: video
    url: https://vimeo.com/238231419
  author:
  - first: Robin
    full: Robin Jia
    id: robin-jia
    last: Jia
  - first: Percy
    full: Percy Liang
    id: percy-liang
    last: Liang
  author_string: Robin Jia, Percy Liang
  bibkey: jia-liang-2017-adversarial
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1215
  month: September
  page_first: '2021'
  page_last: '2031'
  pages: "2021\u20132031"
  paper_id: '215'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1215.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1215.jpg
  title: Adversarial Examples for Evaluating Reading Comprehension Systems
  title_html: Adversarial Examples for Evaluating Reading Comprehension Systems
  url: https://www.aclweb.org/anthology/D17-1215
  year: '2017'
D17-1216:
  abstract: Reasoning with commonsense knowledge is critical for natural language
    understanding. Traditional methods for commonsense machine comprehension mostly
    only focus on one specific kind of knowledge, neglecting the fact that commonsense
    reasoning requires simultaneously considering different kinds of commonsense knowledge.
    In this paper, we propose a multi-knowledge reasoning method, which can exploit
    heterogeneous knowledge for commonsense machine comprehension. Specifically, we
    first mine different kinds of knowledge (including event narrative knowledge,
    entity semantic knowledge and sentiment coherent knowledge) and encode them as
    inference rules with costs. Then we propose a multi-knowledge reasoning model,
    which selects inference rules for a specific reasoning context using attention
    mechanism, and reasons by summarizing all valid inference rules. Experiments on
    RocStories show that our method outperforms traditional models significantly.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238235420
    type: video
    url: https://vimeo.com/238235420
  author:
  - first: Hongyu
    full: Hongyu Lin
    id: hongyu-lin
    last: Lin
  - first: Le
    full: Le Sun
    id: le-sun
    last: Sun
  - first: Xianpei
    full: Xianpei Han
    id: xianpei-han
    last: Han
  author_string: Hongyu Lin, Le Sun, Xianpei Han
  bibkey: lin-etal-2017-reasoning
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1216
  month: September
  page_first: '2032'
  page_last: '2043'
  pages: "2032\u20132043"
  paper_id: '216'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1216.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1216.jpg
  title: Reasoning with Heterogeneous Knowledge for Commonsense Machine Comprehension
  title_html: Reasoning with Heterogeneous Knowledge for Commonsense Machine Comprehension
  url: https://www.aclweb.org/anthology/D17-1216
  year: '2017'
D17-1217:
  abstract: Document-level multi-aspect sentiment classification is an important task
    for customer relation management. In this paper, we model the task as a machine
    comprehension problem where pseudo question-answer pairs are constructed by a
    small number of aspect-related keywords and aspect ratings. A hierarchical iterative
    attention model is introduced to build aspectspecific representations by frequent
    and repeated interactions between documents and aspect questions. We adopt a hierarchical
    architecture to represent both word level and sentence level information, and
    use the attention operations for aspect questions and documents alternatively
    with the multiple hop mechanism. Experimental results on the TripAdvisor and BeerAdvocate
    datasets show that our model outperforms classical baselines. We will release
    our code and data for the method replicability.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1217.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1217.Attachment.pdf
  - filename: https://vimeo.com/238232532
    type: video
    url: https://vimeo.com/238232532
  author:
  - first: Yichun
    full: Yichun Yin
    id: yichun-yin
    last: Yin
  - first: Yangqiu
    full: Yangqiu Song
    id: yangqiu-song
    last: Song
  - first: Ming
    full: Ming Zhang
    id: ming-zhang
    last: Zhang
  author_string: Yichun Yin, Yangqiu Song, Ming Zhang
  bibkey: yin-etal-2017-document
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1217
  month: September
  page_first: '2044'
  page_last: '2054'
  pages: "2044\u20132054"
  paper_id: '217'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1217.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1217.jpg
  title: Document-Level Multi-Aspect Sentiment Classification as Machine Comprehension
  title_html: Document-Level Multi-Aspect Sentiment Classification as Machine Comprehension
  url: https://www.aclweb.org/anthology/D17-1217
  year: '2017'
D17-1218:
  abstract: Argument mining has become a popular research area in NLP. It typically
    includes the identification of argumentative components, e.g. claims, as the central
    component of an argument. We perform a qualitative analysis across six different
    datasets and show that these appear to conceptualize claims quite differently.
    To learn about the consequences of such different conceptualizations of claim
    for practical applications, we carried out extensive experiments using state-of-the-art
    feature-rich and deep learning systems, to identify claims in a cross-domain fashion.
    While the divergent conceptualization of claims in different datasets is indeed
    harmful to cross-domain classification, we show that there are shared properties
    on the lexical level as well as system configurations that can help to overcome
    these gaps.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1218.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1218.Attachment.zip
  author:
  - first: Johannes
    full: Johannes Daxenberger
    id: johannes-daxenberger
    last: Daxenberger
  - first: Steffen
    full: Steffen Eger
    id: steffen-eger
    last: Eger
  - first: Ivan
    full: Ivan Habernal
    id: ivan-habernal
    last: Habernal
  - first: Christian
    full: Christian Stab
    id: christian-stab
    last: Stab
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Johannes Daxenberger, Steffen Eger, Ivan Habernal, Christian Stab,
    Iryna Gurevych
  bibkey: daxenberger-etal-2017-essence
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1218
  month: September
  page_first: '2055'
  page_last: '2066'
  pages: "2055\u20132066"
  paper_id: '218'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1218.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1218.jpg
  title: What is the Essence of a Claim? Cross-Domain Claim Identification
  title_html: What is the Essence of a Claim? Cross-Domain Claim Identification
  url: https://www.aclweb.org/anthology/D17-1218
  year: '2017'
D17-1219:
  abstract: "A first step in the task of automatically generating questions for testing\
    \ reading comprehension is to identify question-worthy sentences, i.e. sentences\
    \ in a text passage that humans find it worthwhile to ask questions about. We\
    \ propose a hierarchical neural sentence-level sequence tagging model for this\
    \ task, which existing approaches to question generation have ignored. The approach\
    \ is fully data-driven \u2014 with no sophisticated NLP pipelines or any hand-crafted\
    \ rules/features \u2014 and compares favorably to a number of baselines when evaluated\
    \ on the SQuAD data set. When incorporated into an existing neural question generation\
    \ system, the resulting end-to-end system achieves state-of-the-art performance\
    \ for paragraph-level question generation for reading comprehension."
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1219.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1219.Attachment.pdf
  author:
  - first: Xinya
    full: Xinya Du
    id: xinya-du
    last: Du
  - first: Claire
    full: Claire Cardie
    id: claire-cardie
    last: Cardie
  author_string: Xinya Du, Claire Cardie
  bibkey: du-cardie-2017-identifying
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1219
  month: September
  page_first: '2067'
  page_last: '2073'
  pages: "2067\u20132073"
  paper_id: '219'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1219.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1219.jpg
  title: Identifying Where to Focus in Reading Comprehension for Neural Question Generation
  title_html: Identifying Where to Focus in Reading Comprehension for Neural Question
    Generation
  url: https://www.aclweb.org/anthology/D17-1219
  year: '2017'
D17-1220:
  abstract: Comprehending lyrics, as found in songs and poems, can pose a challenge
    to human and machine readers alike. This motivates the need for systems that can
    understand the ambiguity and jargon found in such creative texts, and provide
    commentary to aid readers in reaching the correct interpretation. We introduce
    the task of automated lyric annotation (ALA). Like text simplification, a goal
    of ALA is to rephrase the original text in a more easily understandable manner.
    However, in ALA the system must often include additional information to clarify
    niche terminology and abstract concepts. To stimulate research on this task, we
    release a large collection of crowdsourced annotations for song lyrics. We analyze
    the performance of translation and retrieval models on this task, measuring performance
    with both automated and human evaluation. We find that each model captures a unique
    type of information important to the task.
  address: Copenhagen, Denmark
  author:
  - first: Lucas
    full: Lucas Sterckx
    id: lucas-sterckx
    last: Sterckx
  - first: Jason
    full: Jason Naradowsky
    id: jason-naradowsky
    last: Naradowsky
  - first: Bill
    full: Bill Byrne
    id: bill-byrne
    last: Byrne
  - first: Thomas
    full: Thomas Demeester
    id: thomas-demeester
    last: Demeester
  - first: Chris
    full: Chris Develder
    id: chris-develder
    last: Develder
  author_string: Lucas Sterckx, Jason Naradowsky, Bill Byrne, Thomas Demeester, Chris
    Develder
  bibkey: sterckx-etal-2017-break
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1220
  month: September
  page_first: '2074'
  page_last: '2080'
  pages: "2074\u20132080"
  paper_id: '220'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1220.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1220.jpg
  title: 'Break it Down for Me: A Study in Automated Lyric Annotation'
  title_html: 'Break it Down for Me: A Study in Automated Lyric Annotation'
  url: https://www.aclweb.org/anthology/D17-1220
  year: '2017'
D17-1221:
  abstract: When people recall and digest what they have read for writing summaries,
    the important content is more likely to attract their attention. Inspired by this
    observation, we propose a cascaded attention based unsupervised model to estimate
    the salience information from the text for compressive multi-document summarization.
    The attention weights are learned automatically by an unsupervised data reconstruction
    framework which can capture the sentence salience. By adding sparsity constraints
    on the number of output vectors, we can generate condensed information which can
    be treated as word salience. Fine-grained and coarse-grained sentence compression
    strategies are incorporated to produce compressive summaries. Experiments on some
    benchmark data sets show that our framework achieves better results than the state-of-the-art
    methods.
  address: Copenhagen, Denmark
  author:
  - first: Piji
    full: Piji Li
    id: piji-li
    last: Li
  - first: Wai
    full: Wai Lam
    id: wai-lam
    last: Lam
  - first: Lidong
    full: Lidong Bing
    id: lidong-bing
    last: Bing
  - first: Weiwei
    full: Weiwei Guo
    id: weiwei-guo
    last: Guo
  - first: Hang
    full: Hang Li
    id: hang-li
    last: Li
  author_string: Piji Li, Wai Lam, Lidong Bing, Weiwei Guo, Hang Li
  bibkey: li-etal-2017-cascaded
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1221
  month: September
  page_first: '2081'
  page_last: '2090'
  pages: "2081\u20132090"
  paper_id: '221'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1221.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1221.jpg
  title: Cascaded Attention based Unsupervised Information Distillation for Compressive
    Summarization
  title_html: Cascaded Attention based Unsupervised Information Distillation for Compressive
    Summarization
  url: https://www.aclweb.org/anthology/D17-1221
  year: '2017'
D17-1222:
  abstract: We propose a new framework for abstractive text summarization based on
    a sequence-to-sequence oriented encoder-decoder model equipped with a deep recurrent
    generative decoder (DRGN). Latent structure information implied in the target
    summaries is learned based on a recurrent latent random model for improving the
    summarization quality. Neural variational inference is employed to address the
    intractable posterior inference for the recurrent latent variables. Abstractive
    summaries are generated based on both the generative latent variables and the
    discriminative deterministic states. Extensive experiments on some benchmark datasets
    in different languages show that DRGN achieves improvements over the state-of-the-art
    methods.
  address: Copenhagen, Denmark
  author:
  - first: Piji
    full: Piji Li
    id: piji-li
    last: Li
  - first: Wai
    full: Wai Lam
    id: wai-lam
    last: Lam
  - first: Lidong
    full: Lidong Bing
    id: lidong-bing
    last: Bing
  - first: Zihao
    full: Zihao Wang
    id: zihao-wang
    last: Wang
  author_string: Piji Li, Wai Lam, Lidong Bing, Zihao Wang
  bibkey: li-etal-2017-deep
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1222
  month: September
  page_first: '2091'
  page_last: '2100'
  pages: "2091\u20132100"
  paper_id: '222'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1222.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1222.jpg
  title: Deep Recurrent Generative Decoder for Abstractive Text Summarization
  title_html: Deep Recurrent Generative Decoder for Abstractive Text Summarization
  url: https://www.aclweb.org/anthology/D17-1222
  year: '2017'
D17-1223:
  abstract: 'The need for automatic document summarization that can be used for practical
    applications is increasing rapidly. In this paper, we propose a general framework
    for summarization that extracts sentences from a document using externally related
    information. Our work is aimed at single document summarization using small amounts
    of reference summaries. In particular, we address document summarization in the
    framework of multi-task learning using curriculum learning for sentence extraction
    and document classification. The proposed framework enables us to obtain better
    feature representations to extract sentences from documents. We evaluate our proposed
    summarization method on two datasets: financial report and news corpus. Experimental
    results demonstrate that our summarizers achieve performance that is comparable
    to state-of-the-art systems.'
  address: Copenhagen, Denmark
  author:
  - first: Masaru
    full: Masaru Isonuma
    id: masaru-isonuma
    last: Isonuma
  - first: Toru
    full: Toru Fujino
    id: toru-fujino
    last: Fujino
  - first: Junichiro
    full: Junichiro Mori
    id: junichiro-mori
    last: Mori
  - first: Yutaka
    full: Yutaka Matsuo
    id: yutaka-matsuo
    last: Matsuo
  - first: Ichiro
    full: Ichiro Sakata
    id: ichiro-sakata
    last: Sakata
  author_string: Masaru Isonuma, Toru Fujino, Junichiro Mori, Yutaka Matsuo, Ichiro
    Sakata
  bibkey: isonuma-etal-2017-extractive
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1223
  month: September
  page_first: '2101'
  page_last: '2110'
  pages: "2101\u20132110"
  paper_id: '223'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1223.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1223.jpg
  title: Extractive Summarization Using Multi-Task Learning with Document Classification
  title_html: Extractive Summarization Using Multi-Task Learning with Document Classification
  url: https://www.aclweb.org/anthology/D17-1223
  year: '2017'
D17-1224:
  abstract: In this paper we investigate a new task of automatically constructing
    an overview article from a given set of news articles about a news event. We propose
    a news synthesis approach to address this task based on passage segmentation,
    ranking, selection and merging. Our proposed approach is compared with several
    typical multi-document summarization methods on the Wikinews dataset, and achieves
    the best performance on both automatic evaluation and manual evaluation.
  address: Copenhagen, Denmark
  author:
  - first: Jianmin
    full: Jianmin Zhang
    id: jianmin-zhang
    last: Zhang
  - first: Xiaojun
    full: Xiaojun Wan
    id: xiaojun-wan
    last: Wan
  author_string: Jianmin Zhang, Xiaojun Wan
  bibkey: zhang-wan-2017-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1224
  month: September
  page_first: '2111'
  page_last: '2116'
  pages: "2111\u20132116"
  paper_id: '224'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1224.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1224.jpg
  title: Towards Automatic Construction of News Overview Articles by News Synthesis
  title_html: Towards Automatic Construction of News Overview Articles by News Synthesis
  url: https://www.aclweb.org/anthology/D17-1224
  year: '2017'
D17-1225:
  abstract: Discourse parsing has long been treated as a stand-alone problem independent
    from constituency or dependency parsing. Most attempts at this problem rely on
    annotated text segmentations (Elementary Discourse Units, EDUs) and sophisticated
    sparse or continuous features to extract syntactic information. In this paper
    we propose the first end-to-end discourse parser that jointly parses in both syntax
    and discourse levels, as well as the first syntacto-discourse treebank by integrating
    the Penn Treebank and the RST Treebank. Built upon our recent span-based constituency
    parser, this joint syntacto-discourse parser requires no preprocessing efforts
    such as segmentation or feature extraction, making discourse parsing more convenient.
    Empirically, our parser achieves the state-of-the-art end-to-end discourse parsing
    accuracy.
  address: Copenhagen, Denmark
  author:
  - first: Kai
    full: Kai Zhao
    id: kai-zhao
    last: Zhao
  - first: Liang
    full: Liang Huang
    id: liang-huang
    last: Huang
  author_string: Kai Zhao, Liang Huang
  bibkey: zhao-huang-2017-joint
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1225
  month: September
  page_first: '2117'
  page_last: '2123'
  pages: "2117\u20132123"
  paper_id: '225'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1225.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1225.jpg
  title: Joint Syntacto-Discourse Parsing and the Syntacto-Discourse Treebank
  title_html: Joint Syntacto-Discourse Parsing and the Syntacto-Discourse Treebank
  url: https://www.aclweb.org/anthology/D17-1225
  year: '2017'
D17-1226:
  abstract: We introduce a novel iterative approach for event coreference resolution
    that gradually builds event clusters by exploiting inter-dependencies among event
    mentions within the same chain as well as across event chains. Among event mentions
    in the same chain, we distinguish within- and cross-document event coreference
    links by using two distinct pairwise classifiers, trained separately to capture
    differences in feature distributions of within- and cross-document event clusters.
    Our event coreference approach alternates between WD and CD clustering and combines
    arguments from both event clusters after every merge, continuing till no more
    merge can be made. And then it performs further merging between event chains that
    are both closely related to a set of other chains of events. Experiments on the
    ECB+ corpus show that our model outperforms state-of-the-art methods in joint
    task of WD and CD event coreference resolution.
  address: Copenhagen, Denmark
  author:
  - first: Prafulla Kumar
    full: Prafulla Kumar Choubey
    id: prafulla-kumar-choubey
    last: Choubey
  - first: Ruihong
    full: Ruihong Huang
    id: ruihong-huang
    last: Huang
  author_string: Prafulla Kumar Choubey, Ruihong Huang
  bibkey: choubey-huang-2017-event
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1226
  month: September
  page_first: '2124'
  page_last: '2133'
  pages: "2124\u20132133"
  paper_id: '226'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1226.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1226.jpg
  title: Event Coreference Resolution by Iteratively Unfolding Inter-dependencies
    among Events
  title_html: Event Coreference Resolution by Iteratively Unfolding Inter-dependencies
    among Events
  url: https://www.aclweb.org/anthology/D17-1226
  year: '2017'
D17-1227:
  abstract: "In neural text generation such as neural machine translation, summarization,\
    \ and image captioning, beam search is widely used to improve the output text\
    \ quality. However, in the neural generation setting, hypotheses can finish in\
    \ different steps, which makes it difficult to decide when to end beam search\
    \ to ensure optimality. We propose a provably optimal beam search algorithm that\
    \ will always return the optimal-score complete hypothesis (modulo beam size),\
    \ and finish as soon as the optimality is established. To counter neural generation\u2019\
    s tendency for shorter hypotheses, we also introduce a bounded length reward mechanism\
    \ which allows a modified version of our beam search algorithm to remain optimal.\
    \ Experiments on neural machine translation demonstrate that our principled beam\
    \ search algorithm leads to improvement in BLEU score over previously proposed\
    \ alternatives."
  address: Copenhagen, Denmark
  author:
  - first: Liang
    full: Liang Huang
    id: liang-huang
    last: Huang
  - first: Kai
    full: Kai Zhao
    id: kai-zhao
    last: Zhao
  - first: Mingbo
    full: Mingbo Ma
    id: mingbo-ma
    last: Ma
  author_string: Liang Huang, Kai Zhao, Mingbo Ma
  bibkey: huang-etal-2017-finish
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1227
  month: September
  page_first: '2134'
  page_last: '2139'
  pages: "2134\u20132139"
  paper_id: '227'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1227.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1227.jpg
  title: When to Finish? Optimal Beam Search for Neural Text Generation (modulo beam
    size)
  title_html: When to Finish? Optimal Beam Search for Neural Text Generation (modulo
    beam size)
  url: https://www.aclweb.org/anthology/D17-1227
  year: '2017'
D17-1228:
  abstract: 'We propose simple and flexible training and decoding methods for influencing
    output style and topic in neural encoder-decoder based language generation. This
    capability is desirable in a variety of applications, including conversational
    systems, where successful agents need to produce language in a specific style
    and generate responses steered by a human puppeteer or external knowledge. We
    decompose the neural generation process into empirically easier sub-problems:
    a faithfulness model and a decoding method based on selective-sampling. We also
    describe training and sampling algorithms that bias the generation process with
    a specific language style restriction, or a topic restriction. Human evaluation
    results show that our proposed methods are able to to restrict style and topic
    without degrading output quality in conversational tasks.'
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1228.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1228.Attachment.zip
  author:
  - first: Di
    full: Di Wang
    id: di-wang
    last: Wang
  - first: Nebojsa
    full: Nebojsa Jojic
    id: nebojsa-jojic
    last: Jojic
  - first: Chris
    full: Chris Brockett
    id: chris-brockett
    last: Brockett
  - first: Eric
    full: Eric Nyberg
    id: eric-nyberg
    last: Nyberg
  author_string: Di Wang, Nebojsa Jojic, Chris Brockett, Eric Nyberg
  bibkey: wang-etal-2017-steering
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1228
  month: September
  page_first: '2140'
  page_last: '2150'
  pages: "2140\u20132150"
  paper_id: '228'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1228.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1228.jpg
  title: Steering Output Style and Topic in Neural Response Generation
  title_html: Steering Output Style and Topic in Neural Response Generation
  url: https://www.aclweb.org/anthology/D17-1228
  year: '2017'
D17-1229:
  abstract: This paper introduces a novel training/decoding strategy for sequence
    labeling. Instead of greedily choosing a label at each time step, and using it
    for the next prediction, we retain the probability distribution over the current
    label, and pass this distribution to the next prediction. This approach allows
    us to avoid the effect of label bias and error propagation in sequence learning/decoding.
    Our experiments on dialogue act classification demonstrate the effectiveness of
    this approach. Even though our underlying neural network model is relatively simple,
    it outperforms more complex neural models, achieving state-of-the-art results
    on the MapTask and Switchboard corpora.
  address: Copenhagen, Denmark
  author:
  - first: Quan Hung
    full: Quan Hung Tran
    id: quan-hung-tran
    last: Tran
  - first: Ingrid
    full: Ingrid Zukerman
    id: ingrid-zukerman
    last: Zukerman
  - first: Gholamreza
    full: Gholamreza Haffari
    id: gholamreza-haffari
    last: Haffari
  author_string: Quan Hung Tran, Ingrid Zukerman, Gholamreza Haffari
  bibkey: tran-etal-2017-preserving
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1229
  month: September
  page_first: '2151'
  page_last: '2156'
  pages: "2151\u20132156"
  paper_id: '229'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1229.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1229.jpg
  title: Preserving Distributional Information in Dialogue Act Classification
  title_html: Preserving Distributional Information in Dialogue Act Classification
  url: https://www.aclweb.org/anthology/D17-1229
  year: '2017'
D17-1230:
  abstract: "We apply adversarial training to open-domain dialogue generation, training\
    \ a system to produce sequences that are indistinguishable from human-generated\
    \ dialogue utterances. We cast the task as a reinforcement learning problem where\
    \ we jointly train two systems: a generative model to produce response sequences,\
    \ and a discriminator\u2014analagous to the human evaluator in the Turing test\u2014\
    \ to distinguish between the human-generated dialogues and the machine-generated\
    \ ones. In this generative adversarial network approach, the outputs from the\
    \ discriminator are used to encourage the system towards more human-like dialogue.\
    \ Further, we investigate models for adversarial evaluation that uses success\
    \ in fooling an adversary as a dialogue evaluation metric, while avoiding a number\
    \ of potential pitfalls. Experimental results on several metrics, including adversarial\
    \ evaluation, demonstrate that the adversarially-trained system generates higher-quality\
    \ responses than previous baselines"
  address: Copenhagen, Denmark
  author:
  - first: Jiwei
    full: Jiwei Li
    id: jiwei-li
    last: Li
  - first: Will
    full: Will Monroe
    id: will-monroe
    last: Monroe
  - first: Tianlin
    full: Tianlin Shi
    id: tianlin-shi
    last: Shi
  - first: "S\xE9bastien"
    full: "S\xE9bastien Jean"
    id: sebastien-jean1
    last: Jean
  - first: Alan
    full: Alan Ritter
    id: alan-ritter
    last: Ritter
  - first: Dan
    full: Dan Jurafsky
    id: dan-jurafsky
    last: Jurafsky
  author_string: "Jiwei Li, Will Monroe, Tianlin Shi, S\xE9bastien Jean, Alan Ritter,\
    \ Dan Jurafsky"
  bibkey: li-etal-2017-adversarial
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1230
  month: September
  page_first: '2157'
  page_last: '2169'
  pages: "2157\u20132169"
  paper_id: '230'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1230.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1230.jpg
  title: Adversarial Learning for Neural Dialogue Generation
  title_html: Adversarial Learning for Neural Dialogue Generation
  url: https://www.aclweb.org/anthology/D17-1230
  year: '2017'
D17-1231:
  abstract: Previous work on dialog act (DA) classification has investigated different
    methods, such as hidden Markov models, maximum entropy, conditional random fields,
    graphical models, and support vector machines. A few recent studies explored using
    deep learning neural networks for DA classification, however, it is not clear
    yet what is the best method for using dialog context or DA sequential information,
    and how much gain it brings. This paper proposes several ways of using context
    information for DA classification, all in the deep learning framework. The baseline
    system classifies each utterance using the convolutional neural networks (CNN).
    Our proposed methods include using hierarchical models (recurrent neural networks
    (RNN) or CNN) for DA sequence tagging where the bottom layer takes the sentence
    CNN representation as input, concatenating predictions from the previous utterances
    with the CNN vector for classification, and performing sequence decoding based
    on the predictions from the sentence CNN model. We conduct thorough experiments
    and comparisons on the Switchboard corpus, demonstrate that incorporating context
    information significantly improves DA classification, and show that we achieve
    new state-of-the-art performance for this task.
  address: Copenhagen, Denmark
  author:
  - first: Yang
    full: Yang Liu
    id: yang-liu-icsi
    last: Liu
  - first: Kun
    full: Kun Han
    id: kun-han
    last: Han
  - first: Zhao
    full: Zhao Tan
    id: zhao-tan
    last: Tan
  - first: Yun
    full: Yun Lei
    id: yun-lei
    last: Lei
  author_string: Yang Liu, Kun Han, Zhao Tan, Yun Lei
  bibkey: liu-etal-2017-using-context
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1231
  month: September
  page_first: '2170'
  page_last: '2178'
  pages: "2170\u20132178"
  paper_id: '231'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1231.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1231.jpg
  title: Using Context Information for Dialog Act Classification in DNN Framework
  title_html: Using Context Information for Dialog Act Classification in <span class="acl-fixed-case">DNN</span>
    Framework
  url: https://www.aclweb.org/anthology/D17-1231
  year: '2017'
D17-1232:
  abstract: We present an unsupervised model of dialogue act sequences in conversation.
    By modeling topical themes as transitioning more slowly than dialogue acts in
    conversation, our model de-emphasizes content-related words in order to focus
    on conversational function words that signal dialogue acts. We also incorporate
    speaker tendencies to use some acts more than others as an additional predictor
    of dialogue act prevalence beyond temporal dependencies. According to the evaluation
    presented on two dissimilar corpora, the CNET forum and NPS Chat corpus, the effectiveness
    of each modeling assumption is found to vary depending on characteristics of the
    data. De-emphasizing content-related words yields improvement on the CNET corpus,
    while utilizing speaker tendencies is advantageous on the NPS corpus. The components
    of our model complement one another to achieve robust performance on both corpora
    and outperform state-of-the-art baseline models.
  address: Copenhagen, Denmark
  author:
  - first: Yohan
    full: Yohan Jo
    id: yohan-jo
    last: Jo
  - first: Michael
    full: Michael Yoder
    id: michael-yoder
    last: Yoder
  - first: Hyeju
    full: Hyeju Jang
    id: hyeju-jang
    last: Jang
  - first: Carolyn
    full: "Carolyn Ros\xE9"
    id: carolyn-rose
    last: "Ros\xE9"
  author_string: "Yohan Jo, Michael Yoder, Hyeju Jang, Carolyn Ros\xE9"
  bibkey: jo-etal-2017-modeling
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1232
  month: September
  page_first: '2179'
  page_last: '2189'
  pages: "2179\u20132189"
  paper_id: '232'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1232.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1232.jpg
  title: Modeling Dialogue Acts with Content Word Filtering and Speaker Preferences
  title_html: Modeling Dialogue Acts with Content Word Filtering and Speaker Preferences
  url: https://www.aclweb.org/anthology/D17-1232
  year: '2017'
D17-1233:
  abstract: The study on human-computer conversation systems is a hot research topic
    nowadays. One of the prevailing methods to build the system is using the generative
    Sequence-to-Sequence (Seq2Seq) model through neural networks. However, the standard
    Seq2Seq model is prone to generate trivial responses. In this paper, we aim to
    generate a more meaningful and informative reply when answering a given question.
    We propose an implicit content-introducing method which incorporates additional
    information into the Seq2Seq model in a flexible way. Specifically, we fuse the
    general decoding and the auxiliary cue word information through our proposed hierarchical
    gated fusion unit. Experiments on real-life data demonstrate that our model consistently
    outperforms a set of competitive baselines in terms of BLEU scores and human evaluation.
  address: Copenhagen, Denmark
  author:
  - first: Lili
    full: Lili Yao
    id: lili-yao
    last: Yao
  - first: Yaoyuan
    full: Yaoyuan Zhang
    id: yaoyuan-zhang
    last: Zhang
  - first: Yansong
    full: Yansong Feng
    id: yansong-feng
    last: Feng
  - first: Dongyan
    full: Dongyan Zhao
    id: dongyan-zhao
    last: Zhao
  - first: Rui
    full: Rui Yan
    id: rui-yan
    last: Yan
  author_string: Lili Yao, Yaoyuan Zhang, Yansong Feng, Dongyan Zhao, Rui Yan
  bibkey: yao-etal-2017-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1233
  month: September
  page_first: '2190'
  page_last: '2199'
  pages: "2190\u20132199"
  paper_id: '233'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1233.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1233.jpg
  title: Towards Implicit Content-Introducing for Generative Short-Text Conversation
    Systems
  title_html: Towards Implicit Content-Introducing for Generative Short-Text Conversation
    Systems
  url: https://www.aclweb.org/anthology/D17-1233
  year: '2017'
D17-1234:
  abstract: The key to building an evolvable dialogue system in real-world scenarios
    is to ensure an affordable on-line dialogue policy learning, which requires the
    on-line learning process to be safe, efficient and economical. But in reality,
    due to the scarcity of real interaction data, the dialogue system usually grows
    slowly. Besides, the poor initial dialogue policy easily leads to bad user experience
    and incurs a failure of attracting users to contribute training data, so that
    the learning process is unsustainable. To accurately depict this, two quantitative
    metrics are proposed to assess safety and efficiency issues. For solving the unsustainable
    learning problem, we proposed a complete companion teaching framework incorporating
    the guidance from the human teacher. Since the human teaching is expensive, we
    compared various teaching schemes answering the question how and when to teach,
    to economically utilize teaching budget, so that make the online learning process
    affordable.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1234.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1234.Attachment.pdf
  author:
  - first: Cheng
    full: Cheng Chang
    id: cheng-chang
    last: Chang
  - first: Runzhe
    full: Runzhe Yang
    id: runzhe-yang
    last: Yang
  - first: Lu
    full: Lu Chen
    id: lu-chen
    last: Chen
  - first: Xiang
    full: Xiang Zhou
    id: xiang-zhou
    last: Zhou
  - first: Kai
    full: Kai Yu
    id: kai-yu
    last: Yu
  author_string: Cheng Chang, Runzhe Yang, Lu Chen, Xiang Zhou, Kai Yu
  bibkey: chang-etal-2017-affordable
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1234
  month: September
  page_first: '2200'
  page_last: '2209'
  pages: "2200\u20132209"
  paper_id: '234'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1234.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1234.jpg
  title: Affordable On-line Dialogue Policy Learning
  title_html: Affordable On-line Dialogue Policy Learning
  url: https://www.aclweb.org/anthology/D17-1234
  year: '2017'
D17-1235:
  abstract: Sequence-to-sequence models have been applied to the conversation response
    generation problem where the source sequence is the conversation history and the
    target sequence is the response. Unlike translation, conversation responding is
    inherently creative. The generation of long, informative, coherent, and diverse
    responses remains a hard task. In this work, we focus on the single turn setting.
    We add self-attention to the decoder to maintain coherence in longer responses,
    and we propose a practical approach, called the glimpse-model, for scaling to
    large datasets. We introduce a stochastic beam-search algorithm with segment-by-segment
    reranking which lets us inject diversity earlier in the generation process. We
    trained on a combined data set of over 2.3B conversation messages mined from the
    web. In human evaluation studies, our method produces longer responses overall,
    with a higher proportion rated as acceptable and excellent as length increases,
    compared to baseline sequence-to-sequence models with explicit length-promotion.
    A back-off strategy produces better responses overall, in the full spectrum of
    lengths.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1235.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1235.Attachment.pdf
  author:
  - first: Yuanlong
    full: Yuanlong Shao
    id: yuanlong-shao
    last: Shao
  - first: Stephan
    full: Stephan Gouws
    id: stephan-gouws
    last: Gouws
  - first: Denny
    full: Denny Britz
    id: denny-britz
    last: Britz
  - first: Anna
    full: Anna Goldie
    id: anna-goldie
    last: Goldie
  - first: Brian
    full: Brian Strope
    id: brian-strope
    last: Strope
  - first: Ray
    full: Ray Kurzweil
    id: ray-kurzweil
    last: Kurzweil
  author_string: Yuanlong Shao, Stephan Gouws, Denny Britz, Anna Goldie, Brian Strope,
    Ray Kurzweil
  bibkey: shao-etal-2017-generating
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1235
  month: September
  page_first: '2210'
  page_last: '2219'
  pages: "2210\u20132219"
  paper_id: '235'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1235.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1235.jpg
  title: Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence
    Models
  title_html: Generating High-Quality and Informative Conversation Responses with
    Sequence-to-Sequence Models
  url: https://www.aclweb.org/anthology/D17-1235
  year: '2017'
D17-1236:
  abstract: 'We investigate an end-to-end method for automatically inducing task-based
    dialogue systems from small amounts of unannotated dialogue data. It combines
    an incremental semantic grammar - Dynamic Syntax and Type Theory with Records
    (DS-TTR) - with Reinforcement Learning (RL), where language generation and dialogue
    management are a joint decision problem. The systems thus produced are incremental:
    dialogues are processed word-by-word, shown previously to be essential in supporting
    natural, spontaneous dialogue. We hypothesised that the rich linguistic knowledge
    within the grammar should enable a combinatorially large number of dialogue variations
    to be processed, even when trained on very few dialogues. Our experiments show
    that our model can process 74% of the Facebook AI bAbI dataset even when trained
    on only 0.13% of the data (5 dialogues). It can in addition process 65% of bAbI+,
    a corpus we created by systematically adding incremental dialogue phenomena such
    as restarts and self-corrections to bAbI. We compare our model with a state-of-the-art
    retrieval model, MEMN2N. We find that, in terms of semantic accuracy, the MEMN2N
    model shows very poor robustness to the bAbI+ transformations even when trained
    on the full bAbI dataset.'
  address: Copenhagen, Denmark
  author:
  - first: Arash
    full: Arash Eshghi
    id: arash-eshghi
    last: Eshghi
  - first: Igor
    full: Igor Shalyminov
    id: igor-shalyminov
    last: Shalyminov
  - first: Oliver
    full: Oliver Lemon
    id: oliver-lemon
    last: Lemon
  author_string: Arash Eshghi, Igor Shalyminov, Oliver Lemon
  bibkey: eshghi-etal-2017-bootstrapping
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1236
  month: September
  page_first: '2220'
  page_last: '2230'
  pages: "2220\u20132230"
  paper_id: '236'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1236.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1236.jpg
  title: 'Bootstrapping incremental dialogue systems from minimal data: the generalisation
    power of dialogue grammars'
  title_html: 'Bootstrapping incremental dialogue systems from minimal data: the generalisation
    power of dialogue grammars'
  url: https://www.aclweb.org/anthology/D17-1236
  year: '2017'
D17-1237:
  abstract: 'Building a dialogue agent to fulfill complex tasks, such as travel planning,
    is challenging because the agent has to learn to collectively complete multiple
    subtasks. For example, the agent needs to reserve a hotel and book a flight so
    that there leaves enough time for commute between arrival and hotel check-in.
    This paper addresses this challenge by formulating the task in the mathematical
    framework of options over Markov Decision Processes (MDPs), and proposing a hierarchical
    deep reinforcement learning approach to learning a dialogue manager that operates
    at different temporal scales. The dialogue manager consists of: (1) a top-level
    dialogue policy that selects among subtasks or options, (2) a low-level dialogue
    policy that selects primitive actions to complete the subtask given by the top-level
    policy, and (3) a global state tracker that helps ensure all cross-subtask constraints
    be satisfied. Experiments on a travel planning task with simulated and real users
    show that our approach leads to significant improvements over three baselines,
    two based on handcrafted rules and the other based on flat deep reinforcement
    learning.'
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1237.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1237.Attachment.pdf
  author:
  - first: Baolin
    full: Baolin Peng
    id: baolin-peng
    last: Peng
  - first: Xiujun
    full: Xiujun Li
    id: xiujun-li
    last: Li
  - first: Lihong
    full: Lihong Li
    id: lihong-li
    last: Li
  - first: Jianfeng
    full: Jianfeng Gao
    id: jianfeng-gao
    last: Gao
  - first: Asli
    full: Asli Celikyilmaz
    id: asli-celikyilmaz
    last: Celikyilmaz
  - first: Sungjin
    full: Sungjin Lee
    id: sungjin-lee
    last: Lee
  - first: Kam-Fai
    full: Kam-Fai Wong
    id: kam-fai-wong
    last: Wong
  author_string: Baolin Peng, Xiujun Li, Lihong Li, Jianfeng Gao, Asli Celikyilmaz,
    Sungjin Lee, Kam-Fai Wong
  bibkey: peng-etal-2017-composite
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1237
  month: September
  page_first: '2231'
  page_last: '2240'
  pages: "2231\u20132240"
  paper_id: '237'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1237.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1237.jpg
  title: Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep
    Reinforcement Learning
  title_html: Composite Task-Completion Dialogue Policy Learning via Hierarchical
    Deep Reinforcement Learning
  url: https://www.aclweb.org/anthology/D17-1237
  year: '2017'
D17-1238:
  abstract: 'The majority of NLG evaluation relies on automatic metrics, such as BLEU
    . In this paper, we motivate the need for novel, system- and data-independent
    automatic evaluation methods: We investigate a wide range of metrics, including
    state-of-the-art word-based and novel grammar-based ones, and demonstrate that
    they only weakly reflect human judgements of system outputs as generated by data-driven,
    end-to-end NLG. We also show that metric performance is data- and system-specific.
    Nevertheless, our results also suggest that automatic metrics perform reliably
    at system-level and can support system development by finding cases where a system
    performs poorly.'
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1238.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1238.Attachment.zip
  author:
  - first: Jekaterina
    full: Jekaterina Novikova
    id: jekaterina-novikova
    last: Novikova
  - first: "Ond\u0159ej"
    full: "Ond\u0159ej Du\u0161ek"
    id: ondrej-dusek
    last: "Du\u0161ek"
  - first: Amanda
    full: Amanda Cercas Curry
    id: amanda-cercas-curry
    last: Cercas Curry
  - first: Verena
    full: Verena Rieser
    id: verena-rieser
    last: Rieser
  author_string: "Jekaterina Novikova, Ond\u0159ej Du\u0161ek, Amanda Cercas Curry,\
    \ Verena Rieser"
  bibkey: novikova-etal-2017-need
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1238
  month: September
  page_first: '2241'
  page_last: '2252'
  pages: "2241\u20132252"
  paper_id: '238'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1238.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1238.jpg
  title: Why We Need New Evaluation Metrics for NLG
  title_html: Why We Need New Evaluation Metrics for <span class="acl-fixed-case">NLG</span>
  url: https://www.aclweb.org/anthology/D17-1238
  year: '2017'
D17-1239:
  abstract: Recent neural models have shown significant progress on the problem of
    generating short descriptive texts conditioned on a small number of database records.
    In this work, we suggest a slightly more difficult data-to-text generation task,
    and investigate how effective current approaches are on this task. In particular,
    we introduce a new, large-scale corpus of data records paired with descriptive
    documents, propose a series of extractive evaluation methods for analyzing performance,
    and obtain baseline results using current neural generation methods. Experiments
    show that these models produce fluent text, but fail to convincingly approximate
    human-generated documents. Moreover, even templated baselines exceed the performance
    of these neural models on some metrics, though copy- and reconstruction-based
    extensions lead to noticeable improvements.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1239.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1239.Attachment.pdf
  author:
  - first: Sam
    full: Sam Wiseman
    id: sam-wiseman
    last: Wiseman
  - first: Stuart
    full: Stuart Shieber
    id: stuart-m-shieber
    last: Shieber
  - first: Alexander
    full: Alexander Rush
    id: alexander-m-rush
    last: Rush
  author_string: Sam Wiseman, Stuart Shieber, Alexander Rush
  bibkey: wiseman-etal-2017-challenges
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1239
  month: September
  page_first: '2253'
  page_last: '2263'
  pages: "2253\u20132263"
  paper_id: '239'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1239.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1239.jpg
  title: Challenges in Data-to-Document Generation
  title_html: Challenges in Data-to-Document Generation
  url: https://www.aclweb.org/anthology/D17-1239
  year: '2017'
D17-1240:
  abstract: "n this paper, we present a set of computational methods to identify the\
    \ likeliness of a word being borrowed, based on the signals from social media.\
    \ In terms of Spearman\u2019s correlation values, our methods perform more than\
    \ two times better (\u223C 0.62) in predicting the borrowing likeliness compared\
    \ to the best performing baseline (\u223C 0.26) reported in literature. Based\
    \ on this likeliness estimate we asked annotators to re-annotate the language\
    \ tags of foreign words in predominantly native contexts. In 88% of cases the\
    \ annotators felt that the foreign language tag should be replaced by native language\
    \ tag, thus indicating a huge scope for improvement of automatic language identification\
    \ systems."
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1240.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1240.Attachment.pdf
  author:
  - first: Jasabanta
    full: Jasabanta Patro
    id: jasabanta-patro
    last: Patro
  - first: Bidisha
    full: Bidisha Samanta
    id: bidisha-samanta
    last: Samanta
  - first: Saurabh
    full: Saurabh Singh
    id: saurabh-singh
    last: Singh
  - first: Abhipsa
    full: Abhipsa Basu
    id: abhipsa-basu
    last: Basu
  - first: Prithwish
    full: Prithwish Mukherjee
    id: prithwish-mukherjee
    last: Mukherjee
  - first: Monojit
    full: Monojit Choudhury
    id: monojit-choudhury
    last: Choudhury
  - first: Animesh
    full: Animesh Mukherjee
    id: animesh-mukherjee
    last: Mukherjee
  author_string: Jasabanta Patro, Bidisha Samanta, Saurabh Singh, Abhipsa Basu, Prithwish
    Mukherjee, Monojit Choudhury, Animesh Mukherjee
  bibkey: patro-etal-2017-english
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1240
  month: September
  page_first: '2264'
  page_last: '2274'
  pages: "2264\u20132274"
  paper_id: '240'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1240.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1240.jpg
  title: 'All that is English may be Hindi: Enhancing language identification through
    automatic ranking of the likeliness of word borrowing in social media'
  title_html: 'All that is <span class="acl-fixed-case">E</span>nglish may be <span
    class="acl-fixed-case">H</span>indi: Enhancing language identification through
    automatic ranking of the likeliness of word borrowing in social media'
  url: https://www.aclweb.org/anthology/D17-1240
  year: '2017'
D17-1241:
  abstract: "In this paper, we demonstrate how the state-of-the-art machine learning\
    \ and text mining techniques can be used to build effective social media-based\
    \ substance use detection systems. Since a substance use ground truth is difficult\
    \ to obtain on a large scale, to maximize system performance, we explore different\
    \ unsupervised feature learning methods to take advantage of a large amount of\
    \ unsupervised social media data. We also demonstrate the benefit of using multi-view\
    \ unsupervised feature learning to combine heterogeneous user information such\
    \ as Facebook \u201Clikes\u201D and \u201Cstatus updates\u201D to enhance system\
    \ performance. Based on our evaluation, our best models achieved 86% AUC for predicting\
    \ tobacco use, 81% for alcohol use and 84% for illicit drug use, all of which\
    \ significantly outperformed existing methods. Our investigation has also uncovered\
    \ interesting relations between a user\u2019s social media behavior (e.g., word\
    \ usage) and substance use."
  address: Copenhagen, Denmark
  author:
  - first: Tao
    full: Tao Ding
    id: tao-ding
    last: Ding
  - first: Warren K.
    full: Warren K. Bickel
    id: warren-k-bickel
    last: Bickel
  - first: Shimei
    full: Shimei Pan
    id: shimei-pan
    last: Pan
  author_string: Tao Ding, Warren K. Bickel, Shimei Pan
  bibkey: ding-etal-2017-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1241
  month: September
  page_first: '2275'
  page_last: '2284'
  pages: "2275\u20132284"
  paper_id: '241'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1241.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1241.jpg
  title: Multi-View Unsupervised User Feature Embedding for Social Media-based Substance
    Use Prediction
  title_html: Multi-View Unsupervised User Feature Embedding for Social Media-based
    Substance Use Prediction
  url: https://www.aclweb.org/anthology/D17-1241
  year: '2017'
D17-1242:
  abstract: "Variations of word associations across different groups of people can\
    \ provide insights into people\u2019s psychologies and their world views. To capture\
    \ these variations, we introduce the task of demographic-aware word associations.\
    \ We build a new gold standard dataset consisting of word association responses\
    \ for approximately 300 stimulus words, collected from more than 800 respondents\
    \ of different gender (male/female) and from different locations (India/United\
    \ States), and show that there are significant variations in the word associations\
    \ made by these groups. We also introduce a new demographic-aware word association\
    \ model based on a neural net skip-gram architecture, and show how computational\
    \ methods for measuring word associations that specifically account for writer\
    \ demographics can outperform generic methods that are agnostic to such information."
  address: Copenhagen, Denmark
  author:
  - first: Aparna
    full: Aparna Garimella
    id: aparna-garimella
    last: Garimella
  - first: Carmen
    full: Carmen Banea
    id: carmen-banea
    last: Banea
  - first: Rada
    full: Rada Mihalcea
    id: rada-mihalcea
    last: Mihalcea
  author_string: Aparna Garimella, Carmen Banea, Rada Mihalcea
  bibkey: garimella-etal-2017-demographic
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1242
  month: September
  page_first: '2285'
  page_last: '2295'
  pages: "2285\u20132295"
  paper_id: '242'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1242.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1242.jpg
  title: Demographic-aware word associations
  title_html: Demographic-aware word associations
  url: https://www.aclweb.org/anthology/D17-1242
  year: '2017'
D17-1243:
  abstract: We develop a novel factored neural model that learns comment embeddings
    in an unsupervised way leveraging the structure of distributional context in online
    discussion forums. The model links different context with related language factors
    in the embedding space, providing a way to interpret the factored embeddings.
    Evaluated on a community endorsement prediction task using a large collection
    of topic-varying Reddit discussions, the factored embeddings consistently achieve
    improvement over other text representations. Qualitative analysis shows that the
    model captures community style and topic, as well as response trigger patterns.
  address: Copenhagen, Denmark
  author:
  - first: Hao
    full: Hao Cheng
    id: hao-cheng
    last: Cheng
  - first: Hao
    full: Hao Fang
    id: hao-fang
    last: Fang
  - first: Mari
    full: Mari Ostendorf
    id: mari-ostendorf
    last: Ostendorf
  author_string: Hao Cheng, Hao Fang, Mari Ostendorf
  bibkey: cheng-etal-2017-factored
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1243
  month: September
  page_first: '2296'
  page_last: '2306'
  pages: "2296\u20132306"
  paper_id: '243'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1243.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1243.jpg
  title: A Factored Neural Network Model for Characterizing Online Discussions in
    Vector Space
  title_html: A Factored Neural Network Model for Characterizing Online Discussions
    in Vector Space
  url: https://www.aclweb.org/anthology/D17-1243
  year: '2017'
D17-1244:
  abstract: This paper presents a corpus and experiments to determine dimensions of
    interpersonal relationships. We define a set of dimensions heavily inspired by
    work in social science. We create a corpus by retrieving pairs of people, and
    then annotating dimensions for their relationships. A corpus analysis shows that
    dimensions can be annotated reliably. Experimental results show that given a pair
    of people, values to dimensions can be assigned automatically.
  address: Copenhagen, Denmark
  author:
  - first: Farzana
    full: Farzana Rashid
    id: farzana-rashid
    last: Rashid
  - first: Eduardo
    full: Eduardo Blanco
    id: eduardo-blanco
    last: Blanco
  author_string: Farzana Rashid, Eduardo Blanco
  bibkey: rashid-blanco-2017-dimensions
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1244
  month: September
  page_first: '2307'
  page_last: '2316'
  pages: "2307\u20132316"
  paper_id: '244'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1244.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1244.jpg
  title: 'Dimensions of Interpersonal Relationships: Corpus and Experiments'
  title_html: 'Dimensions of Interpersonal Relationships: Corpus and Experiments'
  url: https://www.aclweb.org/anthology/D17-1244
  year: '2017'
D17-1245:
  abstract: Social media collect and spread on the Web personal opinions, facts, fake
    news and all kind of information users may be interested in. Applying argument
    mining methods to such heterogeneous data sources is a challenging open research
    issue, in particular considering the peculiarities of the language used to write
    textual messages on social media. In addition, new issues emerge when dealing
    with arguments posted on such platforms, such as the need to make a distinction
    between personal opinions and actual facts, and to detect the source disseminating
    information about such facts to allow for provenance verification. In this paper,
    we apply supervised classification to identify arguments on Twitter, and we present
    two new tasks for argument mining, namely facts recognition and source identification.
    We study the feasibility of the approaches proposed to address these tasks on
    a set of tweets related to the Grexit and Brexit news topics.
  address: Copenhagen, Denmark
  author:
  - first: Mihai
    full: Mihai Dusmanu
    id: mihai-dusmanu
    last: Dusmanu
  - first: Elena
    full: Elena Cabrio
    id: elena-cabrio
    last: Cabrio
  - first: Serena
    full: Serena Villata
    id: serena-villata
    last: Villata
  author_string: Mihai Dusmanu, Elena Cabrio, Serena Villata
  bibkey: dusmanu-etal-2017-argument
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1245
  month: September
  page_first: '2317'
  page_last: '2322'
  pages: "2317\u20132322"
  paper_id: '245'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1245.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1245.jpg
  title: 'Argument Mining on Twitter: Arguments, Facts and Sources'
  title_html: 'Argument Mining on Twitter: Arguments, Facts and Sources'
  url: https://www.aclweb.org/anthology/D17-1245
  year: '2017'
D17-1246:
  abstract: We focus on non-standard usages of common words on social media. In the
    context of social media, words sometimes have other usages that are totally different
    from their original. In this study, we attempt to distinguish non-standard usages
    on social media from standard ones in an unsupervised manner. Our basic idea is
    that non-standardness can be measured by the inconsistency between the expected
    meaning of the target word and the given context. For this purpose, we use context
    embeddings derived from word embeddings. Our experimental results show that the
    model leveraging the context embedding outperforms other methods and provide us
    with findings, for example, on how to construct context embeddings and which corpus
    to use.
  address: Copenhagen, Denmark
  author:
  - first: Tatsuya
    full: Tatsuya Aoki
    id: tatsuya-aoki
    last: Aoki
  - first: Ryohei
    full: Ryohei Sasano
    id: ryohei-sasano
    last: Sasano
  - first: Hiroya
    full: Hiroya Takamura
    id: hiroya-takamura
    last: Takamura
  - first: Manabu
    full: Manabu Okumura
    id: manabu-okumura
    last: Okumura
  author_string: Tatsuya Aoki, Ryohei Sasano, Hiroya Takamura, Manabu Okumura
  bibkey: aoki-etal-2017-distinguishing
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1246
  month: September
  page_first: '2323'
  page_last: '2328'
  pages: "2323\u20132328"
  paper_id: '246'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1246.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1246.jpg
  title: Distinguishing Japanese Non-standard Usages from Standard Ones
  title_html: Distinguishing <span class="acl-fixed-case">J</span>apanese Non-standard
    Usages from Standard Ones
  url: https://www.aclweb.org/anthology/D17-1246
  year: '2017'
D17-1247:
  abstract: The framing of an action influences how we perceive its actor. We introduce
    connotation frames of power and agency, a pragmatic formalism organized using
    frame semantic representations, to model how different levels of power and agency
    are implicitly projected on actors through their actions. We use the new power
    and agency frames to measure the subtle, but prevalent, gender bias in the portrayal
    of modern film characters and provide insights that deviate from the well-known
    Bechdel test. Our contributions include an extended lexicon of connotation frames
    along with a web interface that provides a comprehensive analysis through the
    lens of connotation frames.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1247.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1247.Attachment.zip
  author:
  - first: Maarten
    full: Maarten Sap
    id: maarten-sap
    last: Sap
  - first: Marcella Cindy
    full: Marcella Cindy Prasettio
    id: marcella-cindy-prasettio
    last: Prasettio
  - first: Ari
    full: Ari Holtzman
    id: ari-holtzman
    last: Holtzman
  - first: Hannah
    full: Hannah Rashkin
    id: hannah-rashkin
    last: Rashkin
  - first: Yejin
    full: Yejin Choi
    id: yejin-choi
    last: Choi
  author_string: Maarten Sap, Marcella Cindy Prasettio, Ari Holtzman, Hannah Rashkin,
    Yejin Choi
  bibkey: sap-etal-2017-connotation
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1247
  month: September
  page_first: '2329'
  page_last: '2334'
  pages: "2329\u20132334"
  paper_id: '247'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1247.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1247.jpg
  title: Connotation Frames of Power and Agency in Modern Films
  title_html: Connotation Frames of Power and Agency in Modern Films
  url: https://www.aclweb.org/anthology/D17-1247
  year: '2017'
D17-1248:
  abstract: "Much of our online communication is text-mediated and, lately, more common\
    \ with automated agents. Unlike interacting with humans, these agents currently\
    \ do not tailor their language to the type of person they are communicating to.\
    \ In this pilot study, we measure the extent to which human perception of basic\
    \ user trait information \u2013 gender and age \u2013 is controllable through\
    \ text. Using automatic models of gender and age prediction, we estimate which\
    \ tweets posted by a user are more likely to mis-characterize his traits. We perform\
    \ multiple controlled crowdsourcing experiments in which we show that we can reduce\
    \ the human prediction accuracy of gender to almost random \u2013 an over 20%\
    \ drop in accuracy. Our experiments show that it is practically feasible for multiple\
    \ applications such as text generation, text summarization or machine translation\
    \ to be tailored to specific traits and perceived as such."
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1248.Poster.pdf
    type: poster
    url: https://www.aclweb.org/anthology/attachments/D17-1248.Poster.pdf
  author:
  - first: Daniel
    full: "Daniel Preo\u0163iuc-Pietro"
    id: daniel-preotiuc-pietro
    last: "Preo\u0163iuc-Pietro"
  - first: Sharath
    full: Sharath Chandra Guntuku
    id: sharath-chandra-guntuku
    last: Chandra Guntuku
  - first: Lyle
    full: Lyle Ungar
    id: lyle-ungar
    last: Ungar
  author_string: "Daniel Preo\u0163iuc-Pietro, Sharath Chandra Guntuku, Lyle Ungar"
  bibkey: preotiuc-pietro-etal-2017-controlling
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1248
  month: September
  page_first: '2335'
  page_last: '2341'
  pages: "2335\u20132341"
  paper_id: '248'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1248.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1248.jpg
  title: Controlling Human Perception of Basic User Traits
  title_html: Controlling Human Perception of Basic User Traits
  url: https://www.aclweb.org/anthology/D17-1248
  year: '2017'
D17-1249:
  abstract: Highlighting the recurrence of topics usage in candidates speeches is
    a key feature to identify the main ideas of each candidate during a political
    campaign. In this paper, we present a method combining standard topic modeling
    with signature mining for analyzing topic recurrence in speeches of Clinton and
    Trump during the 2016 American presidential campaign. The results show that the
    method extracts automatically the main ideas of each candidate and, in addition,
    provides information about the evolution of these topics during the campaign.
  address: Copenhagen, Denmark
  author:
  - first: "Cl\xE9ment"
    full: "Cl\xE9ment Gautrais"
    id: clement-gautrais
    last: Gautrais
  - first: Peggy
    full: Peggy Cellier
    id: peggy-cellier
    last: Cellier
  - first: "Ren\xE9"
    full: "Ren\xE9 Quiniou"
    id: rene-quiniou
    last: Quiniou
  - first: Alexandre
    full: Alexandre Termier
    id: alexandre-termier
    last: Termier
  author_string: "Cl\xE9ment Gautrais, Peggy Cellier, Ren\xE9 Quiniou, Alexandre Termier"
  bibkey: gautrais-etal-2017-topic
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1249
  month: September
  page_first: '2342'
  page_last: '2347'
  pages: "2342\u20132347"
  paper_id: '249'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1249.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1249.jpg
  title: Topic Signatures in Political Campaign Speeches
  title_html: Topic Signatures in Political Campaign Speeches
  url: https://www.aclweb.org/anthology/D17-1249
  year: '2017'
D17-1250:
  abstract: "Recommendations are often rated for their subjective quality, but few\
    \ researchers have studied comment quality in terms of objective utility. We explore\
    \ recommendation quality assessment with respect to both subjective (i.e. users\u2019\
    \ ratings) and objective (i.e., did it influence? did it improve decisions?) metrics\
    \ in a massive online geopolitical forecasting system, ultimately comparing linguistic\
    \ characteristics of each quality metric. Using a variety of features, we predict\
    \ all types of quality with better accuracy than the simple yet strong baseline\
    \ of comment length. Looking at the most predictive content illustrates rater\
    \ biases; for example, forecasters are subjectively biased in favor of comments\
    \ mentioning business transactions or dealings as well as material things, even\
    \ though such comments do not indeed prove any more useful objectively. Additionally,\
    \ more complex sentence constructions, as evidenced by subordinate conjunctions,\
    \ are characteristic of comments leading to objective improvements in forecasting."
  address: Copenhagen, Denmark
  author:
  - first: H. Andrew
    full: H. Andrew Schwartz
    id: h-andrew-schwartz
    last: Schwartz
  - first: Masoud
    full: Masoud Rouhizadeh
    id: masoud-rouhizadeh
    last: Rouhizadeh
  - first: Michael
    full: Michael Bishop
    id: michael-bishop
    last: Bishop
  - first: Philip
    full: Philip Tetlock
    id: philip-tetlock
    last: Tetlock
  - first: Barbara
    full: Barbara Mellers
    id: barbara-mellers
    last: Mellers
  - first: Lyle
    full: Lyle Ungar
    id: lyle-ungar
    last: Ungar
  author_string: H. Andrew Schwartz, Masoud Rouhizadeh, Michael Bishop, Philip Tetlock,
    Barbara Mellers, Lyle Ungar
  bibkey: schwartz-etal-2017-assessing
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1250
  month: September
  page_first: '2348'
  page_last: '2357'
  pages: "2348\u20132357"
  paper_id: '250'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1250.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1250.jpg
  title: Assessing Objective Recommendation Quality through Political Forecasting
  title_html: Assessing Objective Recommendation Quality through Political Forecasting
  url: https://www.aclweb.org/anthology/D17-1250
  year: '2017'
D17-1251:
  abstract: We propose a language-independent data-driven method to exhaustively extract
    bursty phrases of arbitrary forms (e.g., phrases other than simple noun phrases)
    from microblogs. The burst (i.e., the rapid increase of the occurrence) of a phrase
    causes the burst of overlapping N-grams including incomplete ones. In other words,
    bursty incomplete N-grams inevitably overlap bursty phrases. Thus, the proposed
    method performs the extraction of bursty phrases as the set cover problem in which
    all bursty N-grams are covered by a minimum set of bursty phrases. Experimental
    results using Japanese Twitter data showed that the proposed method outperformed
    word-based, noun phrase-based, and segmentation-based methods both in terms of
    accuracy and coverage.
  address: Copenhagen, Denmark
  author:
  - first: Masumi
    full: Masumi Shirakawa
    id: masumi-shirakawa
    last: Shirakawa
  - first: Takahiro
    full: Takahiro Hara
    id: takahiro-hara
    last: Hara
  - first: Takuya
    full: Takuya Maekawa
    id: takuya-maekawa
    last: Maekawa
  author_string: Masumi Shirakawa, Takahiro Hara, Takuya Maekawa
  bibkey: shirakawa-etal-2017-never
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1251
  month: September
  page_first: '2358'
  page_last: '2367'
  pages: "2358\u20132367"
  paper_id: '251'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1251.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1251.jpg
  title: 'Never Abandon Minorities: Exhaustive Extraction of Bursty Phrases on Microblogs
    Using Set Cover Problem'
  title_html: 'Never Abandon Minorities: Exhaustive Extraction of Bursty Phrases on
    Microblogs Using Set Cover Problem'
  url: https://www.aclweb.org/anthology/D17-1251
  year: '2017'
D17-1252:
  abstract: Neural networks have achieved state-of-the-art performance on several
    structured-output prediction tasks, trained in a fully supervised fashion. However,
    annotated examples in structured domains are often costly to obtain, which thus
    limits the applications of neural networks. In this work, we propose Maximum Margin
    Reward Networks, a neural network-based framework that aims to learn from both
    explicit (full structures) and implicit supervision signals (delayed feedback
    on the correctness of the predicted structure). On named entity recognition and
    semantic parsing, our model outperforms previous systems on the benchmark datasets,
    CoNLL-2003 and WebQuestionsSP.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238234174
    type: video
    url: https://vimeo.com/238234174
  author:
  - first: Haoruo
    full: Haoruo Peng
    id: haoruo-peng
    last: Peng
  - first: Ming-Wei
    full: Ming-Wei Chang
    id: ming-wei-chang
    last: Chang
  - first: Wen-tau
    full: Wen-tau Yih
    id: wen-tau-yih
    last: Yih
  author_string: Haoruo Peng, Ming-Wei Chang, Wen-tau Yih
  bibkey: peng-etal-2017-maximum
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1252
  month: September
  page_first: '2368'
  page_last: '2378'
  pages: "2368\u20132378"
  paper_id: '252'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1252.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1252.jpg
  title: Maximum Margin Reward Networks for Learning from Explicit and Implicit Supervision
  title_html: Maximum Margin Reward Networks for Learning from Explicit and Implicit
    Supervision
  url: https://www.aclweb.org/anthology/D17-1252
  year: '2017'
D17-1253:
  abstract: Several approaches have been proposed to model either the explicit sequential
    structure of an argumentative text or its implicit hierarchical structure. So
    far, the adequacy of these models of overall argumentation remains unclear. This
    paper asks what type of structure is actually important to tackle downstream tasks
    in computational argumentation. We analyze patterns in the overall argumentation
    of texts from three corpora. Then, we adapt the idea of positional tree kernels
    in order to capture sequential and hierarchical argumentative structure together
    for the first time. In systematic experiments for three text classification tasks,
    we find strong evidence for the impact of both types of structure. Our results
    suggest that either of them is necessary while their combination may be beneficial.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238236158
    type: video
    url: https://vimeo.com/238236158
  author:
  - first: Henning
    full: Henning Wachsmuth
    id: henning-wachsmuth
    last: Wachsmuth
  - first: Giovanni
    full: Giovanni Da San Martino
    id: giovanni-da-san-martino
    last: Da San Martino
  - first: Dora
    full: Dora Kiesel
    id: dora-kiesel
    last: Kiesel
  - first: Benno
    full: Benno Stein
    id: benno-stein
    last: Stein
  author_string: Henning Wachsmuth, Giovanni Da San Martino, Dora Kiesel, Benno Stein
  bibkey: wachsmuth-etal-2017-impact
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1253
  month: September
  page_first: '2379'
  page_last: '2389'
  pages: "2379\u20132389"
  paper_id: '253'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1253.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1253.jpg
  title: The Impact of Modeling Overall Argumentation with Tree Kernels
  title_html: The Impact of Modeling Overall Argumentation with Tree Kernels
  url: https://www.aclweb.org/anthology/D17-1253
  year: '2017'
D17-1254:
  abstract: We propose a new encoder-decoder approach to learn distributed sentence
    representations that are applicable to multiple purposes. The model is learned
    by using a convolutional neural network as an encoder to map an input sentence
    into a continuous vector, and using a long short-term memory recurrent neural
    network as a decoder. Several tasks are considered, including sentence reconstruction
    and future sentence prediction. Further, a hierarchical encoder-decoder model
    is proposed to encode a sentence to predict multiple future sentences. By training
    our models on a large collection of novels, we obtain a highly generic convolutional
    sentence encoder that performs well in practice. Experimental results on several
    benchmark datasets, and across a broad range of applications, demonstrate the
    superiority of the proposed model over competing methods.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238233944
    type: video
    url: https://vimeo.com/238233944
  author:
  - first: Zhe
    full: Zhe Gan
    id: zhe-gan
    last: Gan
  - first: Yunchen
    full: Yunchen Pu
    id: yunchen-pu
    last: Pu
  - first: Ricardo
    full: Ricardo Henao
    id: ricardo-henao
    last: Henao
  - first: Chunyuan
    full: Chunyuan Li
    id: chunyuan-li
    last: Li
  - first: Xiaodong
    full: Xiaodong He
    id: xiaodong-he
    last: He
  - first: Lawrence
    full: Lawrence Carin
    id: lawrence-carin
    last: Carin
  author_string: Zhe Gan, Yunchen Pu, Ricardo Henao, Chunyuan Li, Xiaodong He, Lawrence
    Carin
  bibkey: gan-etal-2017-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1254
  month: September
  page_first: '2390'
  page_last: '2400'
  pages: "2390\u20132400"
  paper_id: '254'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1254.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1254.jpg
  title: Learning Generic Sentence Representations Using Convolutional Neural Networks
  title_html: Learning Generic Sentence Representations Using Convolutional Neural
    Networks
  url: https://www.aclweb.org/anthology/D17-1254
  year: '2017'
D17-1255:
  abstract: "We present a novel approach for training artificial neural networks.\
    \ Our approach is inspired by broad evidence in psychology that shows human learners\
    \ can learn efficiently and effectively by increasing intervals of time between\
    \ subsequent reviews of previously learned materials (spaced repetition). We investigate\
    \ the analogy between training neural models and findings in psychology about\
    \ human memory model and develop an efficient and effective algorithm to train\
    \ neural models. The core part of our algorithm is a cognitively-motivated scheduler\
    \ according to which training instances and their \u201Creviews\u201D are spaced\
    \ over time. Our algorithm uses only 34-50% of data per epoch, is 2.9-4.8 times\
    \ faster than standard training, and outperforms competing state-of-the-art baselines.\
    \ Our code is available at scholar.harvard.edu/hadi/RbF/."
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238235456
    type: video
    url: https://vimeo.com/238235456
  author:
  - first: Hadi
    full: Hadi Amiri
    id: hadi-amiri
    last: Amiri
  - first: Timothy
    full: Timothy Miller
    id: timothy-miller
    last: Miller
  - first: Guergana
    full: Guergana Savova
    id: guergana-savova
    last: Savova
  author_string: Hadi Amiri, Timothy Miller, Guergana Savova
  bibkey: amiri-etal-2017-repeat
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1255
  month: September
  page_first: '2401'
  page_last: '2410'
  pages: "2401\u20132410"
  paper_id: '255'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1255.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1255.jpg
  title: 'Repeat before Forgetting: Spaced Repetition for Efficient and Effective
    Training of Neural Networks'
  title_html: 'Repeat before Forgetting: Spaced Repetition for Efficient and Effective
    Training of Neural Networks'
  url: https://www.aclweb.org/anthology/D17-1255
  year: '2017'
D17-1256:
  abstract: In this work, we study the problem of part-of-speech tagging for Tweets.
    In contrast to newswire articles, Tweets are usually informal and contain numerous
    out-of-vocabulary words. Moreover, there is a lack of large scale labeled datasets
    for this domain. To tackle these challenges, we propose a novel neural network
    to make use of out-of-domain labeled data, unlabeled in-domain data, and labeled
    in-domain data. Inspired by adversarial neural networks, the proposed method tries
    to learn common features through adversarial discriminator. In addition, we hypothesize
    that domain-specific features of target domain should be preserved in some degree.
    Hence, the proposed method adopts a sequence-to-sequence autoencoder to perform
    this task. Experimental results on three different datasets show that our method
    achieves better performance than state-of-the-art methods.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238235246
    type: video
    url: https://vimeo.com/238235246
  author:
  - first: Tao
    full: Tao Gui
    id: tao-gui
    last: Gui
  - first: Qi
    full: Qi Zhang
    id: qi-zhang
    last: Zhang
  - first: Haoran
    full: Haoran Huang
    id: haoran-huang
    last: Huang
  - first: Minlong
    full: Minlong Peng
    id: minlong-peng
    last: Peng
  - first: Xuanjing
    full: Xuanjing Huang
    id: xuan-jing-huang
    last: Huang
  author_string: Tao Gui, Qi Zhang, Haoran Huang, Minlong Peng, Xuanjing Huang
  bibkey: gui-etal-2017-part
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1256
  month: September
  page_first: '2411'
  page_last: '2420'
  pages: "2411\u20132420"
  paper_id: '256'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1256.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1256.jpg
  title: Part-of-Speech Tagging for Twitter with Adversarial Neural Networks
  title_html: Part-of-Speech Tagging for Twitter with Adversarial Neural Networks
  url: https://www.aclweb.org/anthology/D17-1256
  year: '2017'
D17-1257:
  abstract: The number of word embedding models is growing every year. Most of them
    are based on the co-occurrence information of words and their contexts. However,
    it is still an open question what is the best definition of context. We provide
    a systematical investigation of 4 different syntactic context types and context
    representations for learning word embeddings. Comprehensive experiments are conducted
    to evaluate their effectiveness on 6 extrinsic and intrinsic tasks. We hope that
    this paper, along with the published code, would be helpful for choosing the best
    context type and representation for a given task.
  address: Copenhagen, Denmark
  author:
  - first: Bofang
    full: Bofang Li
    id: bofang-li
    last: Li
  - first: Tao
    full: Tao Liu
    id: tao-liu
    last: Liu
  - first: Zhe
    full: Zhe Zhao
    id: zhe-zhao
    last: Zhao
  - first: Buzhou
    full: Buzhou Tang
    id: buzhou-tang
    last: Tang
  - first: Aleksandr
    full: Aleksandr Drozd
    id: aleksandr-drozd
    last: Drozd
  - first: Anna
    full: Anna Rogers
    id: anna-rogers
    last: Rogers
  - first: Xiaoyong
    full: Xiaoyong Du
    id: xiaoyong-du
    last: Du
  author_string: Bofang Li, Tao Liu, Zhe Zhao, Buzhou Tang, Aleksandr Drozd, Anna
    Rogers, Xiaoyong Du
  bibkey: li-etal-2017-investigating
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1257
  month: September
  page_first: '2421'
  page_last: '2431'
  pages: "2421\u20132431"
  paper_id: '257'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1257.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1257.jpg
  title: Investigating Different Syntactic Context Types and Context Representations
    for Learning Word Embeddings
  title_html: Investigating Different Syntactic Context Types and Context Representations
    for Learning Word Embeddings
  url: https://www.aclweb.org/anthology/D17-1257
  year: '2017'
D17-1258:
  abstract: 'Discourse segmentation is the first step in building discourse parsers.
    Most work on discourse segmentation does not scale to real-world discourse parsing
    across languages, for two reasons: (i) models rely on constituent trees, and (ii)
    experiments have relied on gold standard identification of sentence and token
    boundaries. We therefore investigate to what extent constituents can be replaced
    with universal dependencies, or left out completely, as well as how state-of-the-art
    segmenters fare in the absence of sentence boundaries. Our results show that dependency
    information is less useful than expected, but we provide a fully scalable, robust
    model that only relies on part-of-speech information, and show that it performs
    well across languages in the absence of any gold-standard annotation.'
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238232586
    type: video
    url: https://vimeo.com/238232586
  author:
  - first: "Chlo\xE9"
    full: "Chlo\xE9 Braud"
    id: chloe-braud
    last: Braud
  - first: "Oph\xE9lie"
    full: "Oph\xE9lie Lacroix"
    id: ophelie-lacroix
    last: Lacroix
  - first: Anders
    full: "Anders S\xF8gaard"
    id: anders-sogaard
    last: "S\xF8gaard"
  author_string: "Chlo\xE9 Braud, Oph\xE9lie Lacroix, Anders S\xF8gaard"
  bibkey: braud-etal-2017-syntax
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1258
  month: September
  page_first: '2432'
  page_last: '2442'
  pages: "2432\u20132442"
  paper_id: '258'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1258.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1258.jpg
  title: Does syntax help discourse segmentation? Not so much
  title_html: Does syntax help discourse segmentation? Not so much
  url: https://www.aclweb.org/anthology/D17-1258
  year: '2017'
D17-1259:
  abstract: "Much of human dialogue occurs in semi-cooperative settings, where agents\
    \ with different goals attempt to agree on common decisions. Negotiations require\
    \ complex communication and reasoning skills, but success is easy to measure,\
    \ making this an interesting task for AI. We gather a large dataset of human-human\
    \ negotiations on a multi-issue bargaining task, where agents who cannot observe\
    \ each other\u2019s reward functions must reach an agreement (or a deal) via natural\
    \ language dialogue. For the first time, we show it is possible to train end-to-end\
    \ models for negotiation, which must learn both linguistic and reasoning skills\
    \ with no annotated dialogue states. We also introduce dialogue rollouts, in which\
    \ the model plans ahead by simulating possible complete continuations of the conversation,\
    \ and find that this technique dramatically improves performance. Our code and\
    \ dataset are publicly available."
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238232142
    type: video
    url: https://vimeo.com/238232142
  author:
  - first: Mike
    full: Mike Lewis
    id: mike-lewis
    last: Lewis
  - first: Denis
    full: Denis Yarats
    id: denis-yarats
    last: Yarats
  - first: Yann
    full: Yann Dauphin
    id: yann-dauphin
    last: Dauphin
  - first: Devi
    full: Devi Parikh
    id: devi-parikh
    last: Parikh
  - first: Dhruv
    full: Dhruv Batra
    id: dhruv-batra
    last: Batra
  author_string: Mike Lewis, Denis Yarats, Yann Dauphin, Devi Parikh, Dhruv Batra
  bibkey: lewis-etal-2017-deal
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1259
  month: September
  page_first: '2443'
  page_last: '2453'
  pages: "2443\u20132453"
  paper_id: '259'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1259.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1259.jpg
  title: Deal or No Deal? End-to-End Learning of Negotiation Dialogues
  title_html: Deal or No Deal? End-to-End Learning of Negotiation Dialogues
  url: https://www.aclweb.org/anthology/D17-1259
  year: '2017'
D17-1260:
  abstract: "Hand-crafted rules and reinforcement learning (RL) are two popular choices\
    \ to obtain dialogue policy. The rule-based policy is often reliable within predefined\
    \ scope but not self-adaptable, whereas RL is evolvable with data but often suffers\
    \ from a bad initial performance. We employ a companion learning framework to\
    \ integrate the two approaches for on-line dialogue policy learning, in which\
    \ a pre-defined rule-based policy acts as a \u201Cteacher\u201D and guides a data-driven\
    \ RL system by giving example actions as well as additional rewards. A novel agent-aware\
    \ dropout Deep Q-Network (AAD-DQN) is proposed to address the problem of when\
    \ to consult the teacher and how to learn from the teacher\u2019s experiences.\
    \ AAD-DQN, as a data-driven student policy, provides (1) two separate experience\
    \ memories for student and teacher, (2) an uncertainty estimated by dropout to\
    \ control the timing of consultation and learning. Simulation experiments showed\
    \ that the proposed approach can significantly improve both safetyand efficiency\
    \ of on-line policy optimization compared to other companion learning approaches\
    \ as well as supervised pre-training using static dialogue corpus."
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1260.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1260.Attachment.zip
  - filename: https://vimeo.com/238231562
    type: video
    url: https://vimeo.com/238231562
  author:
  - first: Lu
    full: Lu Chen
    id: lu-chen
    last: Chen
  - first: Xiang
    full: Xiang Zhou
    id: xiang-zhou
    last: Zhou
  - first: Cheng
    full: Cheng Chang
    id: cheng-chang
    last: Chang
  - first: Runzhe
    full: Runzhe Yang
    id: runzhe-yang
    last: Yang
  - first: Kai
    full: Kai Yu
    id: kai-yu
    last: Yu
  author_string: Lu Chen, Xiang Zhou, Cheng Chang, Runzhe Yang, Kai Yu
  bibkey: chen-etal-2017-agent
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1260
  month: September
  page_first: '2454'
  page_last: '2464'
  pages: "2454\u20132464"
  paper_id: '260'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1260.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1260.jpg
  title: Agent-Aware Dropout DQN for Safe and Efficient On-line Dialogue Policy Learning
  title_html: Agent-Aware Dropout <span class="acl-fixed-case">DQN</span> for Safe
    and Efficient On-line Dialogue Policy Learning
  url: https://www.aclweb.org/anthology/D17-1260
  year: '2017'
D17-1261:
  abstract: 'In this paper we introduce a practical first step towards the creation
    of an automated debate agent: a state-of-the-art recurrent predictive model for
    predicting debate winners. By having an accurate predictive model, we are able
    to objectively rate the quality of a statement made at a specific turn in a debate.
    The model is based on a recurrent neural network architecture with attention,
    which allows the model to effectively account for the entire debate when making
    its prediction. Our model achieves state-of-the-art accuracy on a dataset of debate
    transcripts annotated with audience favorability of the debate teams. Finally,
    we discuss how future work can leverage our proposed model for the creation of
    an automated debate agent. We accomplish this by determining the model input that
    will maximize audience favorability toward a given side of a debate at an arbitrary
    turn.'
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238236302
    type: video
    url: https://vimeo.com/238236302
  author:
  - first: Peter
    full: Peter Potash
    id: peter-potash
    last: Potash
  - first: Anna
    full: Anna Rumshisky
    id: anna-rumshisky
    last: Rumshisky
  author_string: Peter Potash, Anna Rumshisky
  bibkey: potash-rumshisky-2017-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1261
  month: September
  page_first: '2465'
  page_last: '2475'
  pages: "2465\u20132475"
  paper_id: '261'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1261.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1261.jpg
  title: 'Towards Debate Automation: a Recurrent Model for Predicting Debate Winners'
  title_html: 'Towards Debate Automation: a Recurrent Model for Predicting Debate
    Winners'
  url: https://www.aclweb.org/anthology/D17-1261
  year: '2017'
D17-1262:
  abstract: Monolingual evaluation of Machine Translation (MT) aims to simplify human
    assessment by requiring assessors to compare the meaning of the MT output with
    a reference translation, opening up the task to a much larger pool of genuinely
    qualified evaluators. Monolingual evaluation runs the risk, however, of bias in
    favour of MT systems that happen to produce translations superficially similar
    to the reference and, consistent with this intuition, previous investigations
    have concluded monolingual assessment to be strongly biased in this respect. On
    re-examination of past analyses, we identify a series of potential analytical
    errors that force some important questions to be raised about the reliability
    of past conclusions, however. We subsequently carry out further investigation
    into reference bias via direct human assessment of MT adequacy via quality controlled
    crowd-sourcing. Contrary to both intuition and past conclusions, results for show
    no significant evidence of reference bias in monolingual evaluation of MT.
  address: Copenhagen, Denmark
  author:
  - first: Qingsong
    full: Qingsong Ma
    id: qingsong-ma
    last: Ma
  - first: Yvette
    full: Yvette Graham
    id: yvette-graham
    last: Graham
  - first: Timothy
    full: Timothy Baldwin
    id: timothy-baldwin
    last: Baldwin
  - first: Qun
    full: Qun Liu
    id: qun-liu
    last: Liu
  author_string: Qingsong Ma, Yvette Graham, Timothy Baldwin, Qun Liu
  bibkey: ma-etal-2017-investigation
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1262
  month: September
  page_first: '2476'
  page_last: '2485'
  pages: "2476\u20132485"
  paper_id: '262'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1262.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1262.jpg
  title: Further Investigation into Reference Bias in Monolingual Evaluation of Machine
    Translation
  title_html: Further Investigation into Reference Bias in Monolingual Evaluation
    of Machine Translation
  url: https://www.aclweb.org/anthology/D17-1262
  year: '2017'
D17-1263:
  abstract: "Neural machine translation represents an exciting leap forward in translation\
    \ quality. But what longstanding weaknesses does it resolve, and which remain?\
    \ We address these questions with a challenge set approach to translation evaluation\
    \ and error analysis. A challenge set consists of a small set of sentences, each\
    \ hand-designed to probe a system\u2019s capacity to bridge a particular structural\
    \ divergence between languages. To exemplify this approach, we present an English-French\
    \ challenge set, and use it to analyze phrase-based and neural systems. The resulting\
    \ analysis provides not only a more fine-grained picture of the strengths of neural\
    \ systems, but also insight into which linguistic phenomena remain out of reach."
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1263.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1263.Attachment.zip
  author:
  - first: Pierre
    full: Pierre Isabelle
    id: pierre-isabelle
    last: Isabelle
  - first: Colin
    full: Colin Cherry
    id: colin-cherry
    last: Cherry
  - first: George
    full: George Foster
    id: george-foster
    last: Foster
  author_string: Pierre Isabelle, Colin Cherry, George Foster
  bibkey: isabelle-etal-2017-challenge
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1263
  month: September
  page_first: '2486'
  page_last: '2496'
  pages: "2486\u20132496"
  paper_id: '263'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1263.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1263.jpg
  title: A Challenge Set Approach to Evaluating Machine Translation
  title_html: A Challenge Set Approach to Evaluating Machine Translation
  url: https://www.aclweb.org/anthology/D17-1263
  year: '2017'
D17-1264:
  abstract: Leveraging zero-shot learning to learn mapping functions between vector
    spaces of different languages is a promising approach to bilingual dictionary
    induction. However, methods using this approach have not yet achieved high accuracy
    on the task. In this paper, we propose a bridging approach, where our main contribution
    is a knowledge distillation training objective. As teachers, rich resource translation
    paths are exploited in this role. And as learners, translation paths involving
    low resource languages learn from the teachers. Our training objective allows
    seamless addition of teacher translation paths for any given low resource pair.
    Since our approach relies on the quality of monolingual word embeddings, we also
    propose to enhance vector representations of both the source and target language
    with linguistic information. Our experiments on various languages show large performance
    gains from our distillation training objective, obtaining as high as 17% accuracy
    improvements.
  address: Copenhagen, Denmark
  author:
  - first: Ndapandula
    full: Ndapandula Nakashole
    id: ndapandula-nakashole
    last: Nakashole
  - first: Raphael
    full: Raphael Flauger
    id: raphael-flauger
    last: Flauger
  author_string: Ndapandula Nakashole, Raphael Flauger
  bibkey: nakashole-flauger-2017-knowledge
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1264
  month: September
  page_first: '2497'
  page_last: '2506'
  pages: "2497\u20132506"
  paper_id: '264'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1264.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1264.jpg
  title: Knowledge Distillation for Bilingual Dictionary Induction
  title_html: Knowledge Distillation for Bilingual Dictionary Induction
  url: https://www.aclweb.org/anthology/D17-1264
  year: '2017'
D17-1265:
  abstract: "In this paper, we address the problem of generating English tag questions\
    \ (TQs) (e.g. it is, isn\u2019t it?) in Machine Translation (MT). We propose a\
    \ post-edition solution, formulating the problem as a multi-class classification\
    \ task. We present (i) the automatic annotation of English TQs in a parallel corpus\
    \ of subtitles and (ii) an approach using a series of classifiers to predict TQ\
    \ forms, which we use to post-edit state-of-the-art MT outputs. Our method provides\
    \ significant improvements in English TQ translation when translating from Czech,\
    \ French and German, in turn improving the fluidity, naturalness, grammatical\
    \ correctness and pragmatic coherence of MT output."
  address: Copenhagen, Denmark
  author:
  - first: Rachel
    full: Rachel Bawden
    id: rachel-bawden
    last: Bawden
  author_string: Rachel Bawden
  bibkey: bawden-2017-machine
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1265
  month: September
  page_first: '2507'
  page_last: '2512'
  pages: "2507\u20132512"
  paper_id: '265'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1265.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1265.jpg
  title: "Machine Translation, it\u2019s a question of style, innit? The case of English\
    \ tag questions"
  title_html: "Machine Translation, it\u2019s a question of style, innit? The case\
    \ of <span class=\"acl-fixed-case\">E</span>nglish tag questions"
  url: https://www.aclweb.org/anthology/D17-1265
  year: '2017'
D17-1266:
  abstract: 'We present a method for translating texts between close language pairs.
    The method does not require parallel data, and it does not require the languages
    to be written in the same script. We show results for six language pairs: Afrikaans/Dutch,
    Bosnian/Serbian, Danish/Swedish, Macedonian/Bulgarian, Malaysian/Indonesian, and
    Polish/Belorussian. We report BLEU scores showing our method to outperform others
    that do not use parallel data.'
  address: Copenhagen, Denmark
  author:
  - first: Nima
    full: Nima Pourdamghani
    id: nima-pourdamghani
    last: Pourdamghani
  - first: Kevin
    full: Kevin Knight
    id: kevin-knight
    last: Knight
  author_string: Nima Pourdamghani, Kevin Knight
  bibkey: pourdamghani-knight-2017-deciphering
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1266
  month: September
  page_first: '2513'
  page_last: '2518'
  pages: "2513\u20132518"
  paper_id: '266'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1266.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1266.jpg
  title: Deciphering Related Languages
  title_html: Deciphering Related Languages
  url: https://www.aclweb.org/anthology/D17-1266
  year: '2017'
D17-1267:
  abstract: We present a system for identifying cognate sets across dictionaries of
    related languages. The likelihood of a cognate relationship is calculated on the
    basis of a rich set of features that capture both phonetic and semantic similarity,
    as well as the presence of regular sound correspondences. The similarity scores
    are used to cluster words from different languages that may originate from a common
    proto-word. When tested on the Algonquian language family, our system detects
    63% of cognate sets while maintaining cluster purity of 70%.
  address: Copenhagen, Denmark
  author:
  - first: Adam
    full: Adam St Arnaud
    id: adam-st-arnaud
    last: St Arnaud
  - first: David
    full: David Beck
    id: david-beck
    last: Beck
  - first: Grzegorz
    full: Grzegorz Kondrak
    id: grzegorz-kondrak
    last: Kondrak
  author_string: Adam St Arnaud, David Beck, Grzegorz Kondrak
  bibkey: st-arnaud-etal-2017-identifying
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1267
  month: September
  page_first: '2519'
  page_last: '2528'
  pages: "2519\u20132528"
  paper_id: '267'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1267.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1267.jpg
  title: Identifying Cognate Sets Across Dictionaries of Related Languages
  title_html: Identifying Cognate Sets Across Dictionaries of Related Languages
  url: https://www.aclweb.org/anthology/D17-1267
  year: '2017'
D17-1268:
  abstract: "One central mystery of neural NLP is what neural models \u201Cknow\u201D\
    \ about their subject matter. When a neural machine translation system learns\
    \ to translate from one language to another, does it learn the syntax or semantics\
    \ of the languages? Can this knowledge be extracted from the system to fill holes\
    \ in human scientific knowledge? Existing typological databases contain relatively\
    \ full feature specifications for only a few hundred languages. Exploiting the\
    \ existence of parallel texts in more than a thousand languages, we build a massive\
    \ many-to-one NMT system from 1017 languages into English, and use this to predict\
    \ information missing from typological databases. Experiments show that the proposed\
    \ method is able to infer not only syntactic, but also phonological and phonetic\
    \ inventory features, and improves over a baseline that has access to information\
    \ about the languages geographic and phylogenetic neighbors."
  address: Copenhagen, Denmark
  author:
  - first: Chaitanya
    full: Chaitanya Malaviya
    id: chaitanya-malaviya
    last: Malaviya
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  - first: Patrick
    full: Patrick Littell
    id: patrick-littell
    last: Littell
  author_string: Chaitanya Malaviya, Graham Neubig, Patrick Littell
  bibkey: malaviya-etal-2017-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1268
  month: September
  page_first: '2529'
  page_last: '2535'
  pages: "2529\u20132535"
  paper_id: '268'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1268.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1268.jpg
  title: Learning Language Representations for Typology Prediction
  title_html: Learning Language Representations for Typology Prediction
  url: https://www.aclweb.org/anthology/D17-1268
  year: '2017'
D17-1269:
  abstract: "Recent work in NLP has attempted to deal with low-resource languages\
    \ but still assumed a resource level that is not present for most languages, e.g.,\
    \ the availability of Wikipedia in the target language. We propose a simple method\
    \ for cross-lingual named entity recognition (NER) that works well in settings\
    \ with very minimal resources. Our approach makes use of a lexicon to \u201Ctranslate\u201D\
    \ annotated data available in one or several high resource language(s) into the\
    \ target language, and learns a standard monolingual NER model there. Further,\
    \ when Wikipedia is available in the target language, our method can enhance Wikipedia\
    \ based methods to yield state-of-the-art NER results; we evaluate on 7 diverse\
    \ languages, improving the state-of-the-art by an average of 5.5% F1 points. With\
    \ the minimal resources required, this is an extremely portable cross-lingual\
    \ NER approach, as illustrated using a truly low-resource language, Uyghur."
  address: Copenhagen, Denmark
  author:
  - first: Stephen
    full: Stephen Mayhew
    id: stephen-mayhew
    last: Mayhew
  - first: Chen-Tse
    full: Chen-Tse Tsai
    id: chen-tse-tsai
    last: Tsai
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Stephen Mayhew, Chen-Tse Tsai, Dan Roth
  bibkey: mayhew-etal-2017-cheap
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1269
  month: September
  page_first: '2536'
  page_last: '2545'
  pages: "2536\u20132545"
  paper_id: '269'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1269.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1269.jpg
  title: Cheap Translation for Cross-Lingual Named Entity Recognition
  title_html: Cheap Translation for Cross-Lingual Named Entity Recognition
  url: https://www.aclweb.org/anthology/D17-1269
  year: '2017'
D17-1270:
  abstract: Existing approaches to automatic VerbNet-style verb classification are
    heavily dependent on feature engineering and therefore limited to languages with
    mature NLP pipelines. In this work, we propose a novel cross-lingual transfer
    method for inducing VerbNets for multiple languages. To the best of our knowledge,
    this is the first study which demonstrates how the architectures for learning
    word embeddings can be applied to this challenging syntactic-semantic task. Our
    method uses cross-lingual translation pairs to tie each of the six target languages
    into a bilingual vector space with English, jointly specialising the representations
    to encode the relational information from English VerbNet. A standard clustering
    algorithm is then run on top of the VerbNet-specialised representations, using
    vector dimensions as features for learning verb classes. Our results show that
    the proposed cross-lingual transfer approach sets new state-of-the-art verb classification
    performance across all six target languages explored in this work.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1270.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1270.Attachment.zip
  author:
  - first: Ivan
    full: "Ivan Vuli\u0107"
    id: ivan-vulic
    last: "Vuli\u0107"
  - first: Nikola
    full: "Nikola Mrk\u0161i\u0107"
    id: nikola-mrksic
    last: "Mrk\u0161i\u0107"
  - first: Anna
    full: Anna Korhonen
    id: anna-korhonen
    last: Korhonen
  author_string: "Ivan Vuli\u0107, Nikola Mrk\u0161i\u0107, Anna Korhonen"
  bibkey: vulic-etal-2017-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1270
  month: September
  page_first: '2546'
  page_last: '2558'
  pages: "2546\u20132558"
  paper_id: '270'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1270.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1270.jpg
  title: Cross-Lingual Induction and Transfer of Verb Classes Based on Word Vector
    Space Specialisation
  title_html: Cross-Lingual Induction and Transfer of Verb Classes Based on Word Vector
    Space Specialisation
  url: https://www.aclweb.org/anthology/D17-1270
  year: '2017'
D17-1271:
  abstract: "This paper addresses the automatic recognition of telicity, an aspectual\
    \ notion. A telic event includes a natural endpoint (\u201Cshe walked home\u201D\
    ), while an atelic event does not (\u201Cshe walked around\u201D). Recognizing\
    \ this difference is a prerequisite for temporal natural language understanding.\
    \ In English, this classification task is difficult, as telicity is a covert linguistic\
    \ category. In contrast, in Slavic languages, aspect is part of a verb\u2019s\
    \ meaning and even available in machine-readable dictionaries. Our contributions\
    \ are as follows. We successfully leverage additional silver standard training\
    \ data in the form of projected annotations from parallel English-Czech data as\
    \ well as context information, improving automatic telicity classification for\
    \ English significantly compared to previous work. We also create a new data set\
    \ of English texts manually annotated with telicity."
  address: Copenhagen, Denmark
  author:
  - first: Annemarie
    full: Annemarie Friedrich
    id: annemarie-friedrich
    last: Friedrich
  - first: Damyana
    full: Damyana Gateva
    id: damyana-gateva
    last: Gateva
  author_string: Annemarie Friedrich, Damyana Gateva
  bibkey: friedrich-gateva-2017-classification
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1271
  month: September
  page_first: '2559'
  page_last: '2565'
  pages: "2559\u20132565"
  paper_id: '271'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1271.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1271.jpg
  title: Classification of telicity using cross-linguistic annotation projection
  title_html: Classification of telicity using cross-linguistic annotation projection
  url: https://www.aclweb.org/anthology/D17-1271
  year: '2017'
D17-1272:
  abstract: The goal of counterfactual learning for statistical machine translation
    (SMT) is to optimize a target SMT system from logged data that consist of user
    feedback to translations that were predicted by another, historic SMT system.
    A challenge arises by the fact that risk-averse commercial SMT systems deterministically
    log the most probable translation. The lack of sufficient exploration of the SMT
    output space seemingly contradicts the theoretical requirements for counterfactual
    learning. We show that counterfactual learning from deterministic bandit logs
    is possible nevertheless by smoothing out deterministic components in learning.
    This can be achieved by additive and multiplicative control variates that avoid
    degenerate behavior in empirical risk minimization. Our simulation experiments
    show improvements of up to 2 BLEU points by counterfactual learning from deterministic
    bandit feedback.
  address: Copenhagen, Denmark
  author:
  - first: Carolin
    full: Carolin Lawrence
    id: carolin-lawrence
    last: Lawrence
  - first: Artem
    full: Artem Sokolov
    id: artem-sokolov
    last: Sokolov
  - first: Stefan
    full: Stefan Riezler
    id: stefan-riezler
    last: Riezler
  author_string: Carolin Lawrence, Artem Sokolov, Stefan Riezler
  bibkey: lawrence-etal-2017-counterfactual
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1272
  month: September
  page_first: '2566'
  page_last: '2576'
  pages: "2566\u20132576"
  paper_id: '272'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1272.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1272.jpg
  title: 'Counterfactual Learning from Bandit Feedback under Deterministic Logging
    : A Case Study in Statistical Machine Translation'
  title_html: 'Counterfactual Learning from Bandit Feedback under Deterministic Logging
    : A Case Study in Statistical Machine Translation'
  url: https://www.aclweb.org/anthology/D17-1272
  year: '2017'
D17-1273:
  abstract: User generated categories (UGCs) are short texts that reflect how people
    describe and organize entities, expressing rich semantic relations implicitly.
    While most methods on UGC relation extraction are based on pattern matching in
    English circumstances, learning relations from Chinese UGCs poses different challenges
    due to the flexibility of expressions. In this paper, we present a weakly supervised
    learning framework to harvest relations from Chinese UGCs. We identify is-a relations
    via word embedding based projection and inference, extract non-taxonomic relations
    and their category patterns by graph mining. We conduct experiments on Chinese
    Wikipedia and achieve high accuracy, outperforming state-of-the-art methods.
  address: Copenhagen, Denmark
  author:
  - first: Chengyu
    full: Chengyu Wang
    id: chengyu-wang
    last: Wang
  - first: Yan
    full: Yan Fan
    id: yan-fan
    last: Fan
  - first: Xiaofeng
    full: Xiaofeng He
    id: xiaofeng-he
    last: He
  - first: Aoying
    full: Aoying Zhou
    id: aoying-zhou
    last: Zhou
  author_string: Chengyu Wang, Yan Fan, Xiaofeng He, Aoying Zhou
  bibkey: wang-etal-2017-learning-fine
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1273
  month: September
  page_first: '2577'
  page_last: '2587'
  pages: "2577\u20132587"
  paper_id: '273'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1273.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1273.jpg
  title: Learning Fine-grained Relations from Chinese User Generated Categories
  title_html: Learning Fine-grained Relations from <span class="acl-fixed-case">C</span>hinese
    User Generated Categories
  url: https://www.aclweb.org/anthology/D17-1273
  year: '2017'
D17-1274:
  abstract: 'Slot Filling (SF) aims to extract the values of certain types of attributes
    (or slots, such as person:cities_of_residence) for a given entity from a large
    collection of source documents. In this paper we propose an effective DNN architecture
    for SF with the following new strategies: (1). Take a regularized dependency graph
    instead of a raw sentence as input to DNN, to compress the wide contexts between
    query and candidate filler; (2). Incorporate two attention mechanisms: local attention
    learned from query and candidate filler, and global attention learned from external
    knowledge bases, to guide the model to better select indicative contexts to determine
    slot type. Experiments show that this framework outperforms state-of-the-art on
    both relation extraction (16% absolute F-score gain) and slot filling validation
    for each individual system (up to 8.5% absolute F-score gain).'
  address: Copenhagen, Denmark
  author:
  - first: Lifu
    full: Lifu Huang
    id: lifu-huang
    last: Huang
  - first: Avirup
    full: Avirup Sil
    id: avirup-sil
    last: Sil
  - first: Heng
    full: Heng Ji
    id: heng-ji
    last: Ji
  - first: Radu
    full: Radu Florian
    id: radu-florian
    last: Florian
  author_string: Lifu Huang, Avirup Sil, Heng Ji, Radu Florian
  bibkey: huang-etal-2017-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1274
  month: September
  page_first: '2588'
  page_last: '2597'
  pages: "2588\u20132597"
  paper_id: '274'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1274.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1274.jpg
  title: Improving Slot Filling Performance with Attentive Neural Networks on Dependency
    Structures
  title_html: Improving Slot Filling Performance with Attentive Neural Networks on
    Dependency Structures
  url: https://www.aclweb.org/anthology/D17-1274
  year: '2017'
D17-1275:
  abstract: "One weakness of machine-learned NLP models is that they typically perform\
    \ poorly on out-of-domain data. In this work, we study the task of identifying\
    \ products being bought and sold in online cybercrime forums, which exhibits particularly\
    \ challenging cross-domain effects. We formulate a task that represents a hybrid\
    \ of slot-filling information extraction and named entity recognition and annotate\
    \ data from four different forums. Each of these forums constitutes its own \u201C\
    fine-grained domain\u201D in that the forums cover different market sectors with\
    \ different properties, even though all forums are in the broad domain of cybercrime.\
    \ We characterize these domain differences in the context of a learning-based\
    \ system: supervised models see decreased accuracy when applied to new forums,\
    \ and standard techniques for semi-supervised learning and domain adaptation have\
    \ limited effectiveness on this data, which suggests the need to improve these\
    \ techniques. We release a dataset of 1,938 annotated posts from across the four\
    \ forums."
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1275.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1275.Attachment.zip
  author:
  - first: Greg
    full: Greg Durrett
    id: greg-durrett
    last: Durrett
  - first: Jonathan K.
    full: Jonathan K. Kummerfeld
    id: jonathan-k-kummerfeld
    last: Kummerfeld
  - first: Taylor
    full: Taylor Berg-Kirkpatrick
    id: taylor-berg-kirkpatrick
    last: Berg-Kirkpatrick
  - first: Rebecca
    full: Rebecca Portnoff
    id: rebecca-portnoff
    last: Portnoff
  - first: Sadia
    full: Sadia Afroz
    id: sadia-afroz
    last: Afroz
  - first: Damon
    full: Damon McCoy
    id: damon-mccoy
    last: McCoy
  - first: Kirill
    full: Kirill Levchenko
    id: kirill-levchenko
    last: Levchenko
  - first: Vern
    full: Vern Paxson
    id: vern-paxson
    last: Paxson
  author_string: Greg Durrett, Jonathan K. Kummerfeld, Taylor Berg-Kirkpatrick, Rebecca
    Portnoff, Sadia Afroz, Damon McCoy, Kirill Levchenko, Vern Paxson
  bibkey: durrett-etal-2017-identifying
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1275
  month: September
  page_first: '2598'
  page_last: '2607'
  pages: "2598\u20132607"
  paper_id: '275'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1275.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1275.jpg
  title: 'Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained
    Domain Adaptation'
  title_html: 'Identifying Products in Online Cybercrime Marketplaces: A Dataset for
    Fine-grained Domain Adaptation'
  url: https://www.aclweb.org/anthology/D17-1275
  year: '2017'
D17-1276:
  abstract: In this paper, we propose a new model that is capable of recognizing overlapping
    mentions. We introduce a novel notion of mention separators that can be effectively
    used to capture how mentions overlap with one another. On top of a novel multigraph
    representation that we introduce, we show that efficient and exact inference can
    still be performed. We present some theoretical analysis on the differences between
    our model and a recently proposed model for recognizing overlapping mentions,
    and discuss the possible implications of the differences. Through extensive empirical
    analysis on standard datasets, we demonstrate the effectiveness of our approach.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1276.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1276.Attachment.pdf
  author:
  - first: Aldrian Obaja
    full: Aldrian Obaja Muis
    id: aldrian-obaja-muis
    last: Muis
  - first: Wei
    full: Wei Lu
    id: wei-lu
    last: Lu
  author_string: Aldrian Obaja Muis, Wei Lu
  bibkey: muis-lu-2017-labeling
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1276
  month: September
  page_first: '2608'
  page_last: '2618'
  pages: "2608\u20132618"
  paper_id: '276'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1276.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1276.jpg
  title: 'Labeling Gaps Between Words: Recognizing Overlapping Mentions with Mention
    Separators'
  title_html: 'Labeling Gaps Between Words: Recognizing Overlapping Mentions with
    Mention Separators'
  url: https://www.aclweb.org/anthology/D17-1276
  year: '2017'
D17-1277:
  abstract: We propose a novel deep learning model for joint document-level entity
    disambiguation, which leverages learned neural representations. Key components
    are entity embeddings, a neural attention mechanism over local context windows,
    and a differentiable joint inference stage for disambiguation. Our approach thereby
    combines benefits of deep learning with more traditional approaches such as graphical
    models and probabilistic mention-entity maps. Extensive experiments show that
    we are able to obtain competitive or state-of-the-art accuracy at moderate computational
    costs.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1277.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1277.Attachment.pdf
  author:
  - first: Octavian-Eugen
    full: Octavian-Eugen Ganea
    id: octavian-eugen-ganea
    last: Ganea
  - first: Thomas
    full: Thomas Hofmann
    id: thomas-hofmann
    last: Hofmann
  author_string: Octavian-Eugen Ganea, Thomas Hofmann
  bibkey: ganea-hofmann-2017-deep
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1277
  month: September
  page_first: '2619'
  page_last: '2629'
  pages: "2619\u20132629"
  paper_id: '277'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1277.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1277.jpg
  title: Deep Joint Entity Disambiguation with Local Neural Attention
  title_html: Deep Joint Entity Disambiguation with Local Neural Attention
  url: https://www.aclweb.org/anthology/D17-1277
  year: '2017'
D17-1278:
  abstract: The goal of Open Information Extraction (OIE) is to extract surface relations
    and their arguments from natural-language text in an unsupervised, domain-independent
    manner. In this paper, we propose MinIE, an OIE system that aims to provide useful,
    compact extractions with high precision and recall. MinIE approaches these goals
    by (1) representing information about polarity, modality, attribution, and quantities
    with semantic annotations instead of in the actual extraction, and (2) identifying
    and removing parts that are considered overly specific. We conducted an experimental
    study with several real-world datasets and found that MinIE achieves competitive
    or higher precision and recall than most prior systems, while at the same time
    producing shorter, semantically enriched extractions.
  address: Copenhagen, Denmark
  author:
  - first: Kiril
    full: Kiril Gashteovski
    id: kiril-gashteovski
    last: Gashteovski
  - first: Rainer
    full: Rainer Gemulla
    id: rainer-gemulla
    last: Gemulla
  - first: Luciano
    full: Luciano del Corro
    id: luciano-del-corro
    last: del Corro
  author_string: Kiril Gashteovski, Rainer Gemulla, Luciano del Corro
  bibkey: gashteovski-etal-2017-minie
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1278
  month: September
  page_first: '2630'
  page_last: '2640'
  pages: "2630\u20132640"
  paper_id: '278'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1278.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1278.jpg
  title: 'MinIE: Minimizing Facts in Open Information Extraction'
  title_html: '<span class="acl-fixed-case">M</span>in<span class="acl-fixed-case">IE</span>:
    Minimizing Facts in Open Information Extraction'
  url: https://www.aclweb.org/anthology/D17-1278
  year: '2017'
D17-1279:
  abstract: This paper addresses the problem of extracting keyphrases from scientific
    articles and categorizing them as corresponding to a task, process, or material.
    We cast the problem as sequence tagging and introduce semi-supervised methods
    to a neural tagging model, which builds on recent advances in named entity recognition.
    Since annotated training data is scarce in this domain, we introduce a graph-based
    semi-supervised algorithm together with a data selection scheme to leverage unannotated
    articles. Both inductive and transductive semi-supervised learning strategies
    outperform state-of-the-art information extraction performance on the 2017 SemEval
    Task 10 ScienceIE task.
  address: Copenhagen, Denmark
  author:
  - first: Yi
    full: Yi Luan
    id: yi-luan
    last: Luan
  - first: Mari
    full: Mari Ostendorf
    id: mari-ostendorf
    last: Ostendorf
  - first: Hannaneh
    full: Hannaneh Hajishirzi
    id: hannaneh-hajishirzi
    last: Hajishirzi
  author_string: Yi Luan, Mari Ostendorf, Hannaneh Hajishirzi
  bibkey: luan-etal-2017-scientific
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1279
  month: September
  page_first: '2641'
  page_last: '2651'
  pages: "2641\u20132651"
  paper_id: '279'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1279.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1279.jpg
  title: Scientific Information Extraction with Semi-supervised Neural Tagging
  title_html: Scientific Information Extraction with Semi-supervised Neural Tagging
  url: https://www.aclweb.org/anthology/D17-1279
  year: '2017'
D17-1280:
  abstract: In domain-specific NER, due to insufficient labeled training data, deep
    models usually fail to behave normally. In this paper, we proposed a novel Neural
    Inductive TEaching framework (NITE) to transfer knowledge from existing domain-specific
    NER models into an arbitrary deep neural network in a teacher-student training
    manner. NITE is a general framework that builds upon transfer learning and multiple
    instance learning, which collaboratively not only transfers knowledge to a deep
    student network but also reduces the noise from teachers. NITE can help deep learning
    methods to effectively utilize existing resources (i.e., models, labeled and unlabeled
    data) in a small domain. The experiment resulted on Disease NER proved that without
    using any labeled data, NITE can significantly boost the performance of a CNN-bidirectional
    LSTM-CRF NER neural network nearly over 30% in terms of F1-score.
  address: Copenhagen, Denmark
  author:
  - first: Siliang
    full: Siliang Tang
    id: siliang-tang
    last: Tang
  - first: Ning
    full: Ning Zhang
    id: ning-zhang
    last: Zhang
  - first: Jinjiang
    full: Jinjiang Zhang
    id: jinjiang-zhang
    last: Zhang
  - first: Fei
    full: Fei Wu
    id: fei-wu
    last: Wu
  - first: Yueting
    full: Yueting Zhuang
    id: yueting-zhuang
    last: Zhuang
  author_string: Siliang Tang, Ning Zhang, Jinjiang Zhang, Fei Wu, Yueting Zhuang
  bibkey: tang-etal-2017-nite
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1280
  month: September
  page_first: '2652'
  page_last: '2657'
  pages: "2652\u20132657"
  paper_id: '280'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1280.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1280.jpg
  title: 'NITE: A Neural Inductive Teaching Framework for Domain Specific NER'
  title_html: '<span class="acl-fixed-case">NITE</span>: A Neural Inductive Teaching
    Framework for Domain Specific <span class="acl-fixed-case">NER</span>'
  url: https://www.aclweb.org/anthology/D17-1280
  year: '2017'
D17-1281:
  abstract: RLIE-DQN is a recently proposed Reinforcement Learning-based Information
    Extraction (IE) technique which is able to incorporate external evidence during
    the extraction process. RLIE-DQN trains a single agent sequentially, training
    on one instance at a time. This results in significant training slowdown which
    is undesirable. We leverage recent advances in parallel RL training using asynchronous
    methods and propose RLIE-A3C. RLIE-A3C trains multiple agents in parallel and
    is able to achieve upto 6x training speedup over RLIE-DQN, while suffering no
    loss in average accuracy.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1281.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1281.Attachment.zip
  author:
  - first: Aditya
    full: Aditya Sharma
    id: aditya-sharma
    last: Sharma
  - first: Zarana
    full: Zarana Parekh
    id: zarana-parekh
    last: Parekh
  - first: Partha
    full: Partha Talukdar
    id: partha-talukdar
    last: Talukdar
  author_string: Aditya Sharma, Zarana Parekh, Partha Talukdar
  bibkey: sharma-etal-2017-speeding
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1281
  month: September
  page_first: '2658'
  page_last: '2663'
  pages: "2658\u20132663"
  paper_id: '281'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1281.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1281.jpg
  title: Speeding up Reinforcement Learning-based Information Extraction Training
    using Asynchronous Methods
  title_html: Speeding up Reinforcement Learning-based Information Extraction Training
    using Asynchronous Methods
  url: https://www.aclweb.org/anthology/D17-1281
  year: '2017'
D17-1282:
  abstract: In this paper, we utilize the linguistic structures of texts to improve
    named entity recognition by BRNN-CNN, a special bidirectional recursive network
    attached with a convolutional network. Motivated by the observation that named
    entities are highly related to linguistic constituents, we propose a constituent-based
    BRNN-CNN for named entity recognition. In contrast to classical sequential labeling
    methods, the system first identifies which text chunks are possible named entities
    by whether they are linguistic constituents. Then it classifies these chunks with
    a constituency tree structure by recursively propagating syntactic and semantic
    information to each constituent node. This method surpasses current state-of-the-art
    on OntoNotes 5.0 with automatically generated parses.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1282.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1282.Attachment.zip
  author:
  - first: Peng-Hsuan
    full: Peng-Hsuan Li
    id: peng-hsuan-li
    last: Li
  - first: Ruo-Ping
    full: Ruo-Ping Dong
    id: ruo-ping-dong
    last: Dong
  - first: Yu-Siang
    full: Yu-Siang Wang
    id: yu-siang-wang
    last: Wang
  - first: Ju-Chieh
    full: Ju-Chieh Chou
    id: ju-chieh-chou
    last: Chou
  - first: Wei-Yun
    full: Wei-Yun Ma
    id: wei-yun-ma
    last: Ma
  author_string: Peng-Hsuan Li, Ruo-Ping Dong, Yu-Siang Wang, Ju-Chieh Chou, Wei-Yun
    Ma
  bibkey: li-etal-2017-leveraging
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1282
  month: September
  page_first: '2664'
  page_last: '2669'
  pages: "2664\u20132669"
  paper_id: '282'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1282.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1282.jpg
  title: Leveraging Linguistic Structures for Named Entity Recognition with Bidirectional
    Recursive Neural Networks
  title_html: Leveraging Linguistic Structures for Named Entity Recognition with Bidirectional
    Recursive Neural Networks
  url: https://www.aclweb.org/anthology/D17-1282
  year: '2017'
D17-1283:
  abstract: 'Today when many practitioners run basic NLP on the entire web and large-volume
    traffic, faster methods are paramount to saving time and energy costs. Recent
    advances in GPU hardware have led to the emergence of bi-directional LSTMs as
    a standard method for obtaining per-token vector representations serving as input
    to labeling tasks such as NER (often followed by prediction in a linear-chain
    CRF). Though expressive and accurate, these models fail to fully exploit GPU parallelism,
    limiting their computational efficiency. This paper proposes a faster alternative
    to Bi-LSTMs for NER: Iterated Dilated Convolutional Neural Networks (ID-CNNs),
    which have better capacity than traditional CNNs for large context and structured
    prediction. Unlike LSTMs whose sequential processing on sentences of length N
    requires O(N) time even in the face of parallelism, ID-CNNs permit fixed-depth
    convolutions to run in parallel across entire documents. We describe a distinct
    combination of network structure, parameter sharing and training procedures that
    enable dramatic 14-20x test-time speedups while retaining accuracy comparable
    to the Bi-LSTM-CRF. Moreover, ID-CNNs trained to aggregate context from the entire
    document are more accurate than Bi-LSTM-CRFs while attaining 8x faster test time
    speeds.'
  address: Copenhagen, Denmark
  author:
  - first: Emma
    full: Emma Strubell
    id: emma-strubell
    last: Strubell
  - first: Patrick
    full: Patrick Verga
    id: patrick-verga
    last: Verga
  - first: David
    full: David Belanger
    id: david-belanger
    last: Belanger
  - first: Andrew
    full: Andrew McCallum
    id: andrew-mccallum
    last: McCallum
  author_string: Emma Strubell, Patrick Verga, David Belanger, Andrew McCallum
  bibkey: strubell-etal-2017-fast
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1283
  month: September
  page_first: '2670'
  page_last: '2680'
  pages: "2670\u20132680"
  paper_id: '283'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1283.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1283.jpg
  title: Fast and Accurate Entity Recognition with Iterated Dilated Convolutions
  title_html: Fast and Accurate Entity Recognition with Iterated Dilated Convolutions
  url: https://www.aclweb.org/anthology/D17-1283
  year: '2017'
D17-1284:
  abstract: "For accurate entity linking, we need to capture various information aspects\
    \ of an entity, such as its description in a KB, contexts in which it is mentioned,\
    \ and structured knowledge. Additionally, a linking system should work on texts\
    \ from different domains without requiring domain-specific training data or hand-engineered\
    \ features. In this work we present a neural, modular entity linking system that\
    \ learns a unified dense representation for each entity using multiple sources\
    \ of information, such as its description, contexts around its mentions, and its\
    \ fine-grained types. We show that the resulting entity linking system is effective\
    \ at combining these sources, and performs competitively, sometimes out-performing\
    \ current state-of-the-art systems across datasets, without requiring any domain-specific\
    \ training data or hand-engineered features. We also show that our model can effectively\
    \ \u201Cembed\u201D entities that are new to the KB, and is able to link its mentions\
    \ accurately."
  address: Copenhagen, Denmark
  author:
  - first: Nitish
    full: Nitish Gupta
    id: nitish-gupta
    last: Gupta
  - first: Sameer
    full: Sameer Singh
    id: sameer-singh
    last: Singh
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Nitish Gupta, Sameer Singh, Dan Roth
  bibkey: gupta-etal-2017-entity
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1284
  month: September
  page_first: '2681'
  page_last: '2690'
  pages: "2681\u20132690"
  paper_id: '284'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1284.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1284.jpg
  title: Entity Linking via Joint Encoding of Types, Descriptions, and Context
  title_html: Entity Linking via Joint Encoding of Types, Descriptions, and Context
  url: https://www.aclweb.org/anthology/D17-1284
  year: '2017'
D17-1285:
  abstract: Mining biomedical text offers an opportunity to automatically discover
    important facts and infer associations among them. As new scientific findings
    appear across a large collection of biomedical publications, our aim is to tap
    into this literature to automate biomedical knowledge extraction and identify
    important insights from them. Towards that goal, we develop a system with novel
    deep neural networks to extract insights on biomedical literature. Evaluation
    shows our system is able to provide insights with competitive accuracy of human
    acceptance and its relation extraction component outperforms previous work.
  address: Copenhagen, Denmark
  author:
  - first: Hua
    full: Hua He
    id: hua-he
    last: He
  - first: Kris
    full: Kris Ganjam
    id: kris-ganjam
    last: Ganjam
  - first: Navendu
    full: Navendu Jain
    id: navendu-jain
    last: Jain
  - first: Jessica
    full: Jessica Lundin
    id: jessica-lundin
    last: Lundin
  - first: Ryen
    full: Ryen White
    id: ryen-white
    last: White
  - first: Jimmy
    full: Jimmy Lin
    id: jimmy-lin
    last: Lin
  author_string: Hua He, Kris Ganjam, Navendu Jain, Jessica Lundin, Ryen White, Jimmy
    Lin
  bibkey: he-etal-2017-insight
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1285
  month: September
  page_first: '2691'
  page_last: '2701'
  pages: "2691\u20132701"
  paper_id: '285'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1285.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1285.jpg
  title: An Insight Extraction System on BioMedical Literature with Deep Neural Networks
  title_html: An Insight Extraction System on <span class="acl-fixed-case">B</span>io<span
    class="acl-fixed-case">M</span>edical Literature with Deep Neural Networks
  url: https://www.aclweb.org/anthology/D17-1285
  year: '2017'
D17-1286:
  abstract: "We present experiments that show the influence of native language on\
    \ lexical choice when producing text in another language \u2013 in this particular\
    \ case English. We start from the premise that non-native English speakers will\
    \ choose lexical items that are close to words in their native language. This\
    \ leads us to an etymology-based representation of documents written by people\
    \ whose mother tongue is an Indo-European language. Based on this representation\
    \ we grow a language family tree, that matches closely the Indo-European language\
    \ tree."
  address: Copenhagen, Denmark
  author:
  - first: Vivi
    full: Vivi Nastase
    id: vivi-nastase
    last: Nastase
  - first: Carlo
    full: Carlo Strapparava
    id: carlo-strapparava
    last: Strapparava
  author_string: Vivi Nastase, Carlo Strapparava
  bibkey: nastase-strapparava-2017-word
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1286
  month: September
  page_first: '2702'
  page_last: '2707'
  pages: "2702\u20132707"
  paper_id: '286'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1286.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1286.jpg
  title: Word Etymology as Native Language Interference
  title_html: Word Etymology as Native Language Interference
  url: https://www.aclweb.org/anthology/D17-1286
  year: '2017'
D17-1287:
  abstract: "Story detection is the task of determining whether or not a unit of text\
    \ contains a story. Prior approaches achieved a maximum performance of 0.66 F1,\
    \ and did not generalize well across different corpora. We present a new state-of-the-art\
    \ detector that achieves a maximum performance of 0.75 F1 (a 14% improvement),\
    \ with significantly greater generalizability than previous work. In particular,\
    \ our detector achieves performance above 0.70 F1 across a variety of combinations\
    \ of lexically different corpora for training and testing, as well as dramatic\
    \ improvements (up to 4,000%) in performance when trained on a small, disfluent\
    \ data set. The new detector uses two basic types of features\u2013ones related\
    \ to events, and ones related to characters\u2013totaling 283 specific features\
    \ overall; previous detectors used tens of thousands of features, and so this\
    \ detector represents a significant simplification along with increased performance."
  address: Copenhagen, Denmark
  author:
  - first: Joshua
    full: Joshua Eisenberg
    id: joshua-eisenberg
    last: Eisenberg
  - first: Mark
    full: Mark Finlayson
    id: mark-finlayson
    last: Finlayson
  author_string: Joshua Eisenberg, Mark Finlayson
  bibkey: eisenberg-finlayson-2017-simpler
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1287
  month: September
  page_first: '2708'
  page_last: '2715'
  pages: "2708\u20132715"
  paper_id: '287'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1287.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1287.jpg
  title: A Simpler and More Generalizable Story Detector using Verb and Character
    Features
  title_html: A Simpler and More Generalizable Story Detector using Verb and Character
    Features
  url: https://www.aclweb.org/anthology/D17-1287
  year: '2017'
D17-1288:
  abstract: One of the main obstacles for many Digital Humanities projects is the
    low data availability. Texts have to be digitized in an expensive and time consuming
    process whereas Optical Character Recognition (OCR) post-correction is one of
    the time-critical factors. At the example of OCR post-correction, we show the
    adaptation of a generic system to solve a specific problem with little data. The
    system accounts for a diversity of errors encountered in OCRed texts coming from
    different time periods in the domain of literature. We show that the combination
    of different approaches, such as e.g. Statistical Machine Translation and spell
    checking, with the help of a ranking mechanism tremendously improves over single-handed
    approaches. Since we consider the accessibility of the resulting tool as a crucial
    part of Digital Humanities collaborations, we describe the workflow we suggest
    for efficient text recognition and subsequent automatic and manual post-correction
  address: Copenhagen, Denmark
  author:
  - first: Sarah
    full: Sarah Schulz
    id: sarah-schulz
    last: Schulz
  - first: Jonas
    full: Jonas Kuhn
    id: jonas-kuhn
    last: Kuhn
  author_string: Sarah Schulz, Jonas Kuhn
  bibkey: schulz-kuhn-2017-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1288
  month: September
  page_first: '2716'
  page_last: '2726'
  pages: "2716\u20132726"
  paper_id: '288'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1288.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1288.jpg
  title: Multi-modular domain-tailored OCR post-correction
  title_html: Multi-modular domain-tailored <span class="acl-fixed-case">OCR</span>
    post-correction
  url: https://www.aclweb.org/anthology/D17-1288
  year: '2017'
D17-1289:
  abstract: The charge prediction task is to determine appropriate charges for a given
    case, which is helpful for legal assistant systems where the user input is fact
    description. We argue that relevant law articles play an important role in this
    task, and therefore propose an attention-based neural network method to jointly
    model the charge prediction task and the relevant article extraction task in a
    unified framework. The experimental results show that, besides providing legal
    basis, the relevant articles can also clearly improve the charge prediction results,
    and our full model can effectively predict appropriate charges for cases with
    different expression styles.
  address: Copenhagen, Denmark
  author:
  - first: Bingfeng
    full: Bingfeng Luo
    id: bingfeng-luo
    last: Luo
  - first: Yansong
    full: Yansong Feng
    id: yansong-feng
    last: Feng
  - first: Jianbo
    full: Jianbo Xu
    id: jianbo-xu
    last: Xu
  - first: Xiang
    full: Xiang Zhang
    id: xiang-zhang
    last: Zhang
  - first: Dongyan
    full: Dongyan Zhao
    id: dongyan-zhao
    last: Zhao
  author_string: Bingfeng Luo, Yansong Feng, Jianbo Xu, Xiang Zhang, Dongyan Zhao
  bibkey: luo-etal-2017-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1289
  month: September
  page_first: '2727'
  page_last: '2736'
  pages: "2727\u20132736"
  paper_id: '289'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1289.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1289.jpg
  title: Learning to Predict Charges for Criminal Cases with Legal Basis
  title_html: Learning to Predict Charges for Criminal Cases with Legal Basis
  url: https://www.aclweb.org/anthology/D17-1289
  year: '2017'
D17-1290:
  abstract: Duplicate documents are a pervasive problem in text datasets and can have
    a strong effect on unsupervised models. Methods to remove duplicate texts are
    typically heuristic or very expensive, so it is vital to know when and why they
    are needed. We measure the sensitivity of two latent semantic methods to the presence
    of different levels of document repetition. By artificially creating different
    forms of duplicate text we confirm several hypotheses about how repeated text
    impacts models. While a small amount of duplication is tolerable, substantial
    over-representation of subsets of the text may overwhelm meaningful topical patterns.
  address: Copenhagen, Denmark
  author:
  - first: Alexandra
    full: Alexandra Schofield
    id: alexandra-schofield
    last: Schofield
  - first: Laure
    full: Laure Thompson
    id: laure-thompson
    last: Thompson
  - first: David
    full: David Mimno
    id: david-mimno
    last: Mimno
  author_string: Alexandra Schofield, Laure Thompson, David Mimno
  bibkey: schofield-etal-2017-quantifying
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1290
  month: September
  page_first: '2737'
  page_last: '2747'
  pages: "2737\u20132747"
  paper_id: '290'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1290.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1290.jpg
  title: Quantifying the Effects of Text Duplication on Semantic Models
  title_html: Quantifying the Effects of Text Duplication on Semantic Models
  url: https://www.aclweb.org/anthology/D17-1290
  year: '2017'
D17-1291:
  abstract: A document outlier is a document that substantially deviates in semantics
    from the majority ones in a corpus. Automatic identification of document outliers
    can be valuable in many applications, such as screening health records for medical
    mistakes. In this paper, we study the problem of mining semantically deviating
    document outliers in a given corpus. We develop a generative model to identify
    frequent and characteristic semantic regions in the word embedding space to represent
    the given corpus, and a robust outlierness measure which is resistant to noisy
    content in documents. Experiments conducted on two real-world textual data sets
    show that our method can achieve an up to 135% improvement over baselines in terms
    of recall at top-1% of the outlier ranking.
  address: Copenhagen, Denmark
  author:
  - first: Honglei
    full: Honglei Zhuang
    id: honglei-zhuang
    last: Zhuang
  - first: Chi
    full: Chi Wang
    id: chi-wang
    last: Wang
  - first: Fangbo
    full: Fangbo Tao
    id: fangbo-tao
    last: Tao
  - first: Lance
    full: Lance Kaplan
    id: lance-kaplan
    last: Kaplan
  - first: Jiawei
    full: Jiawei Han
    id: jiawei-han
    last: Han
  author_string: Honglei Zhuang, Chi Wang, Fangbo Tao, Lance Kaplan, Jiawei Han
  bibkey: zhuang-etal-2017-identifying
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1291
  month: September
  page_first: '2748'
  page_last: '2757'
  pages: "2748\u20132757"
  paper_id: '291'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1291.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1291.jpg
  title: Identifying Semantically Deviating Outlier Documents
  title_html: Identifying Semantically Deviating Outlier Documents
  url: https://www.aclweb.org/anthology/D17-1291
  year: '2017'
D17-1292:
  abstract: Explaining underlying causes or effects about events is a challenging
    but valuable task. We define a novel problem of generating explanations of a time
    series event by (1) searching cause and effect relationships of the time series
    with textual data and (2) constructing a connecting chain between them to generate
    an explanation. To detect causal features from text, we propose a novel method
    based on the Granger causality of time series between features extracted from
    text such as N-grams, topics, sentiments, and their composition. The generation
    of the sequence of causal entities requires a commonsense causative knowledge
    base with efficient reasoning. To ensure good interpretability and appropriate
    lexical usage we combine symbolic and neural representations, using a neural reasoning
    algorithm trained on commonsense causal tuples to predict the next cause step.
    Our quantitative and human analysis show empirical evidence that our method successfully
    extracts meaningful causality relationships between time series with textual features
    and generates appropriate explanation between them.
  address: Copenhagen, Denmark
  author:
  - first: Dongyeop
    full: Dongyeop Kang
    id: dongyeop-kang
    last: Kang
  - first: Varun
    full: Varun Gangal
    id: varun-gangal
    last: Gangal
  - first: Ang
    full: Ang Lu
    id: ang-lu
    last: Lu
  - first: Zheng
    full: Zheng Chen
    id: zheng-chen
    last: Chen
  - first: Eduard
    full: Eduard Hovy
    id: eduard-hovy
    last: Hovy
  author_string: Dongyeop Kang, Varun Gangal, Ang Lu, Zheng Chen, Eduard Hovy
  bibkey: kang-etal-2017-detecting
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1292
  month: September
  page_first: '2758'
  page_last: '2767'
  pages: "2758\u20132767"
  paper_id: '292'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1292.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1292.jpg
  title: Detecting and Explaining Causes From Text For a Time Series Event
  title_html: Detecting and Explaining Causes From Text For a Time Series Event
  url: https://www.aclweb.org/anthology/D17-1292
  year: '2017'
D17-1293:
  abstract: "Recent years have witnessed the proliferation of Massive Open Online\
    \ Courses (MOOCs). With massive learners being offered MOOCs, there is a demand\
    \ that the forum contents within MOOCs need to be classified in order to facilitate\
    \ both learners and instructors. Therefore we investigate a significant application,\
    \ which is to associate forum threads to subtitles of video clips. This task can\
    \ be regarded as a document ranking problem, and the key is how to learn a distinguishable\
    \ text representation from word sequences and learners\u2019 behavior sequences.\
    \ In this paper, we propose a novel cascade model, which can capture both the\
    \ latent semantics and latent similarity by modeling MOOC data. Experimental results\
    \ on two real-world datasets demonstrate that our textual representation outperforms\
    \ state-of-the-art unsupervised counterparts for the application."
  address: Copenhagen, Denmark
  author:
  - first: Zhuoxuan
    full: Zhuoxuan Jiang
    id: zhuoxuan-jiang
    last: Jiang
  - first: Shanshan
    full: Shanshan Feng
    id: shanshan-feng
    last: Feng
  - first: Gao
    full: Gao Cong
    id: gao-cong
    last: Cong
  - first: Chunyan
    full: Chunyan Miao
    id: chunyan-miao
    last: Miao
  - first: Xiaoming
    full: Xiaoming Li
    id: xiaoming-li
    last: Li
  author_string: Zhuoxuan Jiang, Shanshan Feng, Gao Cong, Chunyan Miao, Xiaoming Li
  bibkey: jiang-etal-2017-novel
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1293
  month: September
  page_first: '2768'
  page_last: '2773'
  pages: "2768\u20132773"
  paper_id: '293'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1293.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1293.jpg
  title: A Novel Cascade Model for Learning Latent Similarity from Heterogeneous Sequential
    Data of MOOC
  title_html: A Novel Cascade Model for Learning Latent Similarity from Heterogeneous
    Sequential Data of <span class="acl-fixed-case">MOOC</span>
  url: https://www.aclweb.org/anthology/D17-1293
  year: '2017'
D17-1294:
  abstract: "Websites\u2019 and mobile apps\u2019 privacy policies, written in natural\
    \ language, tend to be long and difficult to understand. Information privacy revolves\
    \ around the fundamental principle of Notice and choice, namely the idea that\
    \ users should be able to make informed decisions about what information about\
    \ them can be collected and how it can be used. Internet users want control over\
    \ their privacy, but their choices are often hidden in long and convoluted privacy\
    \ policy texts. Moreover, little (if any) prior work has been done to detect the\
    \ provision of choices in text. We address this challenge of enabling user choice\
    \ by automatically identifying and extracting pertinent choice language in privacy\
    \ policies. In particular, we present a two-stage architecture of classification\
    \ models to identify opt-out choices in privacy policy text, labelling common\
    \ varieties of choices with a mean F1 score of 0.735. Our techniques enable the\
    \ creation of systems to help Internet users to learn about their choices, thereby\
    \ effectuating notice and choice and improving Internet privacy."
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1294.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1294.Attachment.zip
  author:
  - first: Kanthashree
    full: Kanthashree Mysore Sathyendra
    id: kanthashree-mysore-sathyendra
    last: Mysore Sathyendra
  - first: Shomir
    full: Shomir Wilson
    id: shomir-wilson
    last: Wilson
  - first: Florian
    full: Florian Schaub
    id: florian-schaub
    last: Schaub
  - first: Sebastian
    full: Sebastian Zimmeck
    id: sebastian-zimmeck
    last: Zimmeck
  - first: Norman
    full: Norman Sadeh
    id: norman-sadeh
    last: Sadeh
  author_string: Kanthashree Mysore Sathyendra, Shomir Wilson, Florian Schaub, Sebastian
    Zimmeck, Norman Sadeh
  bibkey: mysore-sathyendra-etal-2017-identifying
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1294
  month: September
  page_first: '2774'
  page_last: '2779'
  pages: "2774\u20132779"
  paper_id: '294'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1294.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1294.jpg
  title: Identifying the Provision of Choices in Privacy Policy Text
  title_html: Identifying the Provision of Choices in Privacy Policy Text
  url: https://www.aclweb.org/anthology/D17-1294
  year: '2017'
D17-1295:
  abstract: In this paper, we present a novel approach to infer significance of various
    textual edits to documents. An author may make several edits to a document; each
    edit varies in its impact to the content of the document. While some edits are
    surface changes and introduce negligible change, other edits may change the content/tone
    of the document significantly. In this paper, we perform an analysis on the human
    perceptions of edit importance while reviewing documents from one version to the
    next. We identify linguistic features that influence edit importance and model
    it in a regression based setting. We show that the predicted importance by our
    approach is highly correlated with the human perceived importance, established
    by a Mechanical Turk study.
  address: Copenhagen, Denmark
  author:
  - first: Tanya
    full: Tanya Goyal
    id: tanya-goyal
    last: Goyal
  - first: Sachin
    full: Sachin Kelkar
    id: sachin-kelkar
    last: Kelkar
  - first: Manas
    full: Manas Agarwal
    id: manas-agarwal
    last: Agarwal
  - first: Jeenu
    full: Jeenu Grover
    id: jeenu-grover
    last: Grover
  author_string: Tanya Goyal, Sachin Kelkar, Manas Agarwal, Jeenu Grover
  bibkey: goyal-etal-2017-empirical
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1295
  month: September
  page_first: '2780'
  page_last: '2784'
  pages: "2780\u20132784"
  paper_id: '295'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1295.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1295.jpg
  title: An Empirical Analysis of Edit Importance between Document Versions
  title_html: An Empirical Analysis of Edit Importance between Document Versions
  url: https://www.aclweb.org/anthology/D17-1295
  year: '2017'
D17-1296:
  abstract: In this paper, we model the problem of disfluency detection using a transition-based
    framework, which incrementally constructs and labels the disfluency chunk of input
    sentences using a new transition system without syntax information. Compared with
    sequence labeling methods, it can capture non-local chunk-level features; compared
    with joint parsing and disfluency detection methods, it is free for noise in syntax.
    Experiments show that our model achieves state-of-the-art f-score of 87.5% on
    the commonly used English Switchboard test set, and a set of in-house annotated
    Chinese data.
  address: Copenhagen, Denmark
  author:
  - first: Shaolei
    full: Shaolei Wang
    id: shaolei-wang
    last: Wang
  - first: Wanxiang
    full: Wanxiang Che
    id: wanxiang-che
    last: Che
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  - first: Meishan
    full: Meishan Zhang
    id: meishan-zhang
    last: Zhang
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  author_string: Shaolei Wang, Wanxiang Che, Yue Zhang, Meishan Zhang, Ting Liu
  bibkey: wang-etal-2017-transition
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1296
  month: September
  page_first: '2785'
  page_last: '2794'
  pages: "2785\u20132794"
  paper_id: '296'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1296.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1296.jpg
  title: Transition-Based Disfluency Detection using LSTMs
  title_html: Transition-Based Disfluency Detection using <span class="acl-fixed-case">LSTM</span>s
  url: https://www.aclweb.org/anthology/D17-1296
  year: '2017'
D17-1297:
  abstract: We propose an approach to N-best list reranking using neural sequence-labelling
    models. We train a compositional model for error detection that calculates the
    probability of each token in a sentence being correct or incorrect, utilising
    the full sentence as context. Using the error detection model, we then re-rank
    the N best hypotheses generated by statistical machine translation systems. Our
    approach achieves state-of-the-art results on error correction for three different
    datasets, and it has the additional advantage of only using a small set of easily
    computed features that require no linguistic input.
  address: Copenhagen, Denmark
  author:
  - first: Helen
    full: Helen Yannakoudakis
    id: helen-yannakoudakis
    last: Yannakoudakis
  - first: Marek
    full: Marek Rei
    id: marek-rei
    last: Rei
  - first: "\xD8istein E."
    full: "\xD8istein E. Andersen"
    id: oistein-e-andersen
    last: Andersen
  - first: Zheng
    full: Zheng Yuan
    id: zheng-yuan
    last: Yuan
  author_string: "Helen Yannakoudakis, Marek Rei, \xD8istein E. Andersen, Zheng Yuan"
  bibkey: yannakoudakis-etal-2017-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1297
  month: September
  page_first: '2795'
  page_last: '2806'
  pages: "2795\u20132806"
  paper_id: '297'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1297.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1297.jpg
  title: Neural Sequence-Labelling Models for Grammatical Error Correction
  title_html: Neural Sequence-Labelling Models for Grammatical Error Correction
  url: https://www.aclweb.org/anthology/D17-1297
  year: '2017'
D17-1298:
  abstract: In a controlled experiment of sequence-to-sequence approaches for the
    task of sentence correction, we find that character-based models are generally
    more effective than word-based models and models that encode subword information
    via convolutions, and that modeling the output data as a series of diffs improves
    effectiveness over standard approaches. Our strongest sequence-to-sequence model
    improves over our strongest phrase-based statistical machine translation model,
    with access to the same data, by 6 M2 (0.5 GLEU) points. Additionally, in the
    data environment of the standard CoNLL-2014 setup, we demonstrate that modeling
    (and tuning against) diffs yields similar or better M2 scores with simpler models
    and/or significantly less data than previous sequence-to-sequence approaches.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1298.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1298.Attachment.zip
  author:
  - first: Allen
    full: Allen Schmaltz
    id: allen-schmaltz
    last: Schmaltz
  - first: Yoon
    full: Yoon Kim
    id: yoon-kim
    last: Kim
  - first: Alexander
    full: Alexander Rush
    id: alexander-m-rush
    last: Rush
  - first: Stuart
    full: Stuart Shieber
    id: stuart-m-shieber
    last: Shieber
  author_string: Allen Schmaltz, Yoon Kim, Alexander Rush, Stuart Shieber
  bibkey: schmaltz-etal-2017-adapting
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1298
  month: September
  page_first: '2807'
  page_last: '2813'
  pages: "2807\u20132813"
  paper_id: '298'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1298.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1298.jpg
  title: Adapting Sequence Models for Sentence Correction
  title_html: Adapting Sequence Models for Sentence Correction
  url: https://www.aclweb.org/anthology/D17-1298
  year: '2017'
D17-1299:
  abstract: "Stylistic variations of language, such as formality, carry speakers\u2019\
    \ intention beyond literal meaning and should be conveyed adequately in translation.\
    \ We propose to use lexical formality models to control the formality level of\
    \ machine translation output. We demonstrate the effectiveness of our approach\
    \ in empirical evaluations, as measured by automatic metrics and human assessments."
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238231330
    type: video
    url: https://vimeo.com/238231330
  author:
  - first: Xing
    full: Xing Niu
    id: xing-niu
    last: Niu
  - first: Marianna
    full: Marianna Martindale
    id: marianna-martindale
    last: Martindale
  - first: Marine
    full: Marine Carpuat
    id: marine-carpuat
    last: Carpuat
  author_string: Xing Niu, Marianna Martindale, Marine Carpuat
  bibkey: niu-etal-2017-study
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1299
  month: September
  page_first: '2814'
  page_last: '2819'
  pages: "2814\u20132819"
  paper_id: '299'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1299.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1299.jpg
  title: 'A Study of Style in Machine Translation: Controlling the Formality of Machine
    Translation Output'
  title_html: 'A Study of Style in Machine Translation: Controlling the Formality
    of Machine Translation Output'
  url: https://www.aclweb.org/anthology/D17-1299
  year: '2017'
D17-1300:
  abstract: 'Attentional sequence-to-sequence models have become the new standard
    for machine translation, but one challenge of such models is a significant increase
    in training and decoding cost compared to phrase-based systems. In this work we
    focus on efficient decoding, with a goal of achieving accuracy close the state-of-the-art
    in neural machine translation (NMT), while achieving CPU decoding speed/throughput
    close to that of a phrasal decoder. We approach this problem from two angles:
    First, we describe several techniques for speeding up an NMT beam search decoder,
    which obtain a 4.4x speedup over a very efficient baseline decoder without changing
    the decoder output. Second, we propose a simple but powerful network architecture
    which uses an RNN (GRU/LSTM) layer at bottom, followed by a series of stacked
    fully-connected layers applied at every timestep. This architecture achieves similar
    accuracy to a deep recurrent model, at a small fraction of the training and decoding
    cost. By combining these techniques, our best system achieves a very competitive
    accuracy of 38.3 BLEU on WMT English-French NewsTest2014, while decoding at 100
    words/sec on single-threaded CPU. We believe this is the best published accuracy/speed
    trade-off of an NMT system.'
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1300.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1300.Attachment.zip
  - filename: https://vimeo.com/238235696
    type: video
    url: https://vimeo.com/238235696
  author:
  - first: Jacob
    full: Jacob Devlin
    id: jacob-devlin
    last: Devlin
  author_string: Jacob Devlin
  bibkey: devlin-2017-sharp
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1300
  month: September
  page_first: '2820'
  page_last: '2825'
  pages: "2820\u20132825"
  paper_id: '300'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1300.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1300.jpg
  title: 'Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation
    Decoding on the CPU'
  title_html: 'Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation
    Decoding on the <span class="acl-fixed-case">CPU</span>'
  url: https://www.aclweb.org/anthology/D17-1300
  year: '2017'
D17-1301:
  abstract: 'In translation, considering the document as a whole can help to resolve
    ambiguities and inconsistencies. In this paper, we propose a cross-sentence context-aware
    approach and investigate the influence of historical contextual information on
    the performance of neural machine translation (NMT). First, this history is summarized
    in a hierarchical way. We then integrate the historical representation into NMT
    in two strategies: 1) a warm-start of encoder and decoder states, and 2) an auxiliary
    context source for updating decoder states. Experimental results on a large Chinese-English
    translation task show that our approach significantly improves upon a strong attention-based
    NMT system by up to +2.1 BLEU points.'
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238233057
    type: video
    url: https://vimeo.com/238233057
  author:
  - first: Longyue
    full: Longyue Wang
    id: longyue-wang
    last: Wang
  - first: Zhaopeng
    full: Zhaopeng Tu
    id: zhaopeng-tu
    last: Tu
  - first: Andy
    full: Andy Way
    id: andy-way
    last: Way
  - first: Qun
    full: Qun Liu
    id: qun-liu
    last: Liu
  author_string: Longyue Wang, Zhaopeng Tu, Andy Way, Qun Liu
  bibkey: wang-etal-2017-exploiting-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1301
  month: September
  page_first: '2826'
  page_last: '2831'
  pages: "2826\u20132831"
  paper_id: '301'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1301.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1301.jpg
  title: Exploiting Cross-Sentence Context for Neural Machine Translation
  title_html: Exploiting Cross-Sentence Context for Neural Machine Translation
  url: https://www.aclweb.org/anthology/D17-1301
  year: '2017'
D17-1302:
  abstract: Training a POS tagging model with crosslingual transfer learning usually
    requires linguistic knowledge and resources about the relation between the source
    language and the target language. In this paper, we introduce a cross-lingual
    transfer learning model for POS tagging without ancillary resources such as parallel
    corpora. The proposed cross-lingual model utilizes a common BLSTM that enables
    knowledge transfer from other languages, and private BLSTMs for language-specific
    representations. The cross-lingual model is trained with language-adversarial
    training and bidirectional language modeling as auxiliary objectives to better
    represent language-general information while not losing the information about
    a specific target language. Evaluating on POS datasets from 14 languages in the
    Universal Dependencies corpus, we show that the proposed transfer learning model
    improves the POS tagging performance of the target languages without exploiting
    any linguistic knowledge between the source language and the target language.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238231953
    type: video
    url: https://vimeo.com/238231953
  author:
  - first: Joo-Kyung
    full: Joo-Kyung Kim
    id: joo-kyung-kim
    last: Kim
  - first: Young-Bum
    full: Young-Bum Kim
    id: young-bum-kim
    last: Kim
  - first: Ruhi
    full: Ruhi Sarikaya
    id: ruhi-sarikaya
    last: Sarikaya
  - first: Eric
    full: Eric Fosler-Lussier
    id: eric-fosler-lussier
    last: Fosler-Lussier
  author_string: Joo-Kyung Kim, Young-Bum Kim, Ruhi Sarikaya, Eric Fosler-Lussier
  bibkey: kim-etal-2017-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1302
  month: September
  page_first: '2832'
  page_last: '2838'
  pages: "2832\u20132838"
  paper_id: '302'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1302.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1302.jpg
  title: Cross-Lingual Transfer Learning for POS Tagging without Cross-Lingual Resources
  title_html: Cross-Lingual Transfer Learning for <span class="acl-fixed-case">POS</span>
    Tagging without Cross-Lingual Resources
  url: https://www.aclweb.org/anthology/D17-1302
  year: '2017'
D17-1303:
  abstract: In this paper we propose a model to learn multimodal multilingual representations
    for matching images and sentences in different languages, with the aim of advancing
    multilingual versions of image search and image understanding. Our model learns
    a common representation for images and their descriptions in two different languages
    (which need not be parallel) by considering the image as a pivot between two languages.
    We introduce a new pairwise ranking loss function which can handle both symmetric
    and asymmetric similarity between the two modalities. We evaluate our models on
    image-description ranking for German and English, and on semantic textual similarity
    of image descriptions in English. In both cases we achieve state-of-the-art performance.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238233708
    type: video
    url: https://vimeo.com/238233708
  author:
  - first: Spandana
    full: Spandana Gella
    id: spandana-gella
    last: Gella
  - first: Rico
    full: Rico Sennrich
    id: rico-sennrich
    last: Sennrich
  - first: Frank
    full: Frank Keller
    id: frank-keller
    last: Keller
  - first: Mirella
    full: Mirella Lapata
    id: mirella-lapata
    last: Lapata
  author_string: Spandana Gella, Rico Sennrich, Frank Keller, Mirella Lapata
  bibkey: gella-etal-2017-image
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1303
  month: September
  page_first: '2839'
  page_last: '2845'
  pages: "2839\u20132845"
  paper_id: '303'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1303.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1303.jpg
  title: Image Pivoting for Learning Multilingual Multimodal Representations
  title_html: Image Pivoting for Learning Multilingual Multimodal Representations
  url: https://www.aclweb.org/anthology/D17-1303
  year: '2017'
D17-1304:
  abstract: Source dependency information has been successfully introduced into statistical
    machine translation. However, there are only a few preliminary attempts for Neural
    Machine Translation (NMT), such as concatenating representations of source word
    and its dependency label together. In this paper, we propose a novel NMT with
    source dependency representation to improve translation performance of NMT, especially
    long sentences. Empirical results on NIST Chinese-to-English translation task
    show that our method achieves 1.6 BLEU improvements on average over a strong NMT
    system.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238234744
    type: video
    url: https://vimeo.com/238234744
  author:
  - first: Kehai
    full: Kehai Chen
    id: kehai-chen
    last: Chen
  - first: Rui
    full: Rui Wang
    id: rui-wang
    last: Wang
  - first: Masao
    full: Masao Utiyama
    id: masao-utiyama
    last: Utiyama
  - first: Lemao
    full: Lemao Liu
    id: lemao-liu
    last: Liu
  - first: Akihiro
    full: Akihiro Tamura
    id: akihiro-tamura
    last: Tamura
  - first: Eiichiro
    full: Eiichiro Sumita
    id: eiichiro-sumita
    last: Sumita
  - first: Tiejun
    full: Tiejun Zhao
    id: tiejun-zhao
    last: Zhao
  author_string: Kehai Chen, Rui Wang, Masao Utiyama, Lemao Liu, Akihiro Tamura, Eiichiro
    Sumita, Tiejun Zhao
  bibkey: chen-etal-2017-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1304
  month: September
  page_first: '2846'
  page_last: '2852'
  pages: "2846\u20132852"
  paper_id: '304'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1304.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1304.jpg
  title: Neural Machine Translation with Source Dependency Representation
  title_html: Neural Machine Translation with Source Dependency Representation
  url: https://www.aclweb.org/anthology/D17-1304
  year: '2017'
D17-1305:
  abstract: In the logic approach to Recognizing Textual Entailment, identifying phrase-to-phrase
    semantic relations is still an unsolved problem. Resources such as the Paraphrase
    Database offer limited coverage despite their large size whereas unsupervised
    distributional models of meaning often fail to recognize phrasal entailments.
    We propose to map phrases to their visual denotations and compare their meaning
    in terms of their images. We show that our approach is effective in the task of
    Recognizing Textual Entailment when combined with specific linguistic and logic
    features.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238236753
    type: video
    url: https://vimeo.com/238236753
  author:
  - first: Dan
    full: Dan Han
    id: dan-han
    last: Han
  - first: Pascual
    full: "Pascual Mart\xEDnez-G\xF3mez"
    id: pascual-martinez-gomez
    last: "Mart\xEDnez-G\xF3mez"
  - first: Koji
    full: Koji Mineshima
    id: koji-mineshima
    last: Mineshima
  author_string: "Dan Han, Pascual Mart\xEDnez-G\xF3mez, Koji Mineshima"
  bibkey: han-etal-2017-visual
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1305
  month: September
  page_first: '2853'
  page_last: '2859'
  pages: "2853\u20132859"
  paper_id: '305'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1305.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1305.jpg
  title: Visual Denotations for Recognizing Textual Entailment
  title_html: Visual Denotations for Recognizing Textual Entailment
  url: https://www.aclweb.org/anthology/D17-1305
  year: '2017'
D17-1306:
  abstract: Manual data annotation is a vital component of NLP research. When designing
    annotation tasks, properties of the annotation interface can unintentionally lead
    to artefacts in the resulting dataset, biasing the evaluation. In this paper,
    we explore sequence effects where annotations of an item are affected by the preceding
    items. Having assigned one label to an instance, the annotator may be less (or
    more) likely to assign the same label to the next. During rating tasks, seeing
    a low quality item may affect the score given to the next item either positively
    or negatively. We see clear evidence of both types of effects using auto-correlation
    studies over three different crowdsourced datasets. We then recommend a simple
    way to minimise sequence effects.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238235659
    type: video
    url: https://vimeo.com/238235659
  author:
  - first: Nitika
    full: Nitika Mathur
    id: nitika-mathur
    last: Mathur
  - first: Timothy
    full: Timothy Baldwin
    id: timothy-baldwin
    last: Baldwin
  - first: Trevor
    full: Trevor Cohn
    id: trevor-cohn
    last: Cohn
  author_string: Nitika Mathur, Timothy Baldwin, Trevor Cohn
  bibkey: mathur-etal-2017-sequence
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1306
  month: September
  page_first: '2860'
  page_last: '2865'
  pages: "2860\u20132865"
  paper_id: '306'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1306.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1306.jpg
  title: Sequence Effects in Crowdsourced Annotations
  title_html: Sequence Effects in Crowdsourced Annotations
  url: https://www.aclweb.org/anthology/D17-1306
  year: '2017'
D17-1307:
  abstract: "First-order factoid question answering assumes that the question can\
    \ be answered by a single fact in a knowledge base (KB). While this does not seem\
    \ like a challenging task, many recent attempts that apply either complex linguistic\
    \ reasoning or deep neural networks achieve 65%\u201376% accuracy on benchmark\
    \ sets. Our approach formulates the task as two machine learning problems: detecting\
    \ the entities in the question, and classifying the question as one of the relation\
    \ types in the KB. We train a recurrent neural network to solve each problem.\
    \ On the SimpleQuestions dataset, our approach yields substantial improvements\
    \ over previously published results \u2014 even neural networks based on much\
    \ more complex architectures. The simplicity of our approach also has practical\
    \ advantages, such as efficiency and modularity, that are valuable especially\
    \ in an industry setting. In fact, we present a preliminary analysis of the performance\
    \ of our model on real queries from Comcast\u2019s X1 entertainment platform with\
    \ millions of users every day."
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238234982
    type: video
    url: https://vimeo.com/238234982
  author:
  - first: Ferhan
    full: Ferhan Ture
    id: ferhan-ture
    last: Ture
  - first: Oliver
    full: Oliver Jojic
    id: oliver-jojic
    last: Jojic
  author_string: Ferhan Ture, Oliver Jojic
  bibkey: ture-jojic-2017-need
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1307
  month: September
  page_first: '2866'
  page_last: '2872'
  pages: "2866\u20132872"
  paper_id: '307'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1307.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1307.jpg
  title: 'No Need to Pay Attention: Simple Recurrent Neural Networks Work!'
  title_html: 'No Need to Pay Attention: Simple Recurrent Neural Networks Work!'
  url: https://www.aclweb.org/anthology/D17-1307
  year: '2017'
D17-1308:
  abstract: Despite their ubiquity, word embeddings trained with skip-gram negative
    sampling (SGNS) remain poorly understood. We find that vector positions are not
    simply determined by semantic similarity, but rather occupy a narrow cone, diametrically
    opposed to the context vectors. We show that this geometric concentration depends
    on the ratio of positive to negative examples, and that it is neither theoretically
    nor empirically inherent in related embedding algorithms.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238236230
    type: video
    url: https://vimeo.com/238236230
  author:
  - first: David
    full: David Mimno
    id: david-mimno
    last: Mimno
  - first: Laure
    full: Laure Thompson
    id: laure-thompson
    last: Thompson
  author_string: David Mimno, Laure Thompson
  bibkey: mimno-thompson-2017-strange
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1308
  month: September
  page_first: '2873'
  page_last: '2878'
  pages: "2873\u20132878"
  paper_id: '308'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1308.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1308.jpg
  title: The strange geometry of skip-gram with negative sampling
  title_html: The strange geometry of skip-gram with negative sampling
  url: https://www.aclweb.org/anthology/D17-1308
  year: '2017'
D17-1309:
  abstract: We show that small and shallow feed-forward neural networks can achieve
    near state-of-the-art results on a range of unstructured and structured language
    processing tasks while being considerably cheaper in memory and computational
    requirements than deep recurrent models. Motivated by resource-constrained environments
    like mobile phones, we showcase simple techniques for obtaining such small neural
    network models, and investigate different tradeoffs when deciding how to allocate
    a small memory budget.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1309.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1309.Attachment.zip
  - filename: https://vimeo.com/238234701
    type: video
    url: https://vimeo.com/238234701
  author:
  - first: Jan A.
    full: Jan A. Botha
    id: jan-a-botha
    last: Botha
  - first: Emily
    full: Emily Pitler
    id: emily-pitler
    last: Pitler
  - first: Ji
    full: Ji Ma
    id: ji-ma
    last: Ma
  - first: Anton
    full: Anton Bakalov
    id: anton-bakalov
    last: Bakalov
  - first: Alex
    full: Alex Salcianu
    id: alex-salcianu
    last: Salcianu
  - first: David
    full: David Weiss
    id: david-weiss
    last: Weiss
  - first: Ryan
    full: Ryan McDonald
    id: ryan-mcdonald
    last: McDonald
  - first: Slav
    full: Slav Petrov
    id: slav-petrov
    last: Petrov
  author_string: Jan A. Botha, Emily Pitler, Ji Ma, Anton Bakalov, Alex Salcianu,
    David Weiss, Ryan McDonald, Slav Petrov
  bibkey: botha-etal-2017-natural
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1309
  month: September
  page_first: '2879'
  page_last: '2885'
  pages: "2879\u20132885"
  paper_id: '309'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1309.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1309.jpg
  title: Natural Language Processing with Small Feed-Forward Networks
  title_html: Natural Language Processing with Small Feed-Forward Networks
  url: https://www.aclweb.org/anthology/D17-1309
  year: '2017'
D17-1310:
  abstract: We propose a novel LSTM-based deep multi-task learning framework for aspect
    term extraction from user review sentences. Two LSTMs equipped with extended memories
    and neural memory operations are designed for jointly handling the extraction
    tasks of aspects and opinions via memory interactions. Sentimental sentence constraint
    is also added for more accurate prediction via another LSTM. Experiment results
    over two benchmark datasets demonstrate the effectiveness of our framework.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238232213
    type: video
    url: https://vimeo.com/238232213
  author:
  - first: Xin
    full: Xin Li
    id: xin-li
    last: Li
  - first: Wai
    full: Wai Lam
    id: wai-lam
    last: Lam
  author_string: Xin Li, Wai Lam
  bibkey: li-lam-2017-deep
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1310
  month: September
  page_first: '2886'
  page_last: '2892'
  pages: "2886\u20132892"
  paper_id: '310'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1310.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1310.jpg
  title: Deep Multi-Task Learning for Aspect Term Extraction with Memory Interaction
  title_html: Deep Multi-Task Learning for Aspect Term Extraction with Memory Interaction
  url: https://www.aclweb.org/anthology/D17-1310
  year: '2017'
D17-1311:
  abstract: "We investigate the compositional structure of message vectors computed\
    \ by a deep network trained on a communication game. By comparing truth-conditional\
    \ representations of encoder-produced message vectors to human-produced referring\
    \ expressions, we are able to identify aligned (vector, utterance) pairs with\
    \ the same meaning. We then search for structured relationships among these aligned\
    \ pairs to discover simple vector space transformations corresponding to negation,\
    \ conjunction, and disjunction. Our results suggest that neural representations\
    \ are capable of spontaneously developing a \u201Csyntax\u201D with functional\
    \ analogues to qualitative properties of natural language."
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238231647
    type: video
    url: https://vimeo.com/238231647
  author:
  - first: Jacob
    full: Jacob Andreas
    id: jacob-andreas
    last: Andreas
  - first: Dan
    full: Dan Klein
    id: dan-klein
    last: Klein
  author_string: Jacob Andreas, Dan Klein
  bibkey: andreas-klein-2017-analogs
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1311
  month: September
  page_first: '2893'
  page_last: '2897'
  pages: "2893\u20132897"
  paper_id: '311'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1311.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1311.jpg
  title: Analogs of Linguistic Structure in Deep Representations
  title_html: Analogs of Linguistic Structure in Deep Representations
  url: https://www.aclweb.org/anthology/D17-1311
  year: '2017'
D17-1312:
  abstract: Learning word embeddings has received a significant amount of attention
    recently. Often, word embeddings are learned in an unsupervised manner from a
    large collection of text. The genre of the text typically plays an important role
    in the effectiveness of the resulting embeddings. How to effectively train word
    embedding models using data from different domains remains a problem that is less
    explored. In this paper, we present a simple yet effective method for learning
    word embeddings based on text from different domains. We demonstrate the effectiveness
    of our approach through extensive experiments on various down-stream NLP tasks.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238231213
    type: video
    url: https://vimeo.com/238231213
  author:
  - first: Wei
    full: Wei Yang
    id: wei-yang
    last: Yang
  - first: Wei
    full: Wei Lu
    id: wei-lu
    last: Lu
  - first: Vincent
    full: Vincent Zheng
    id: vincent-zheng
    last: Zheng
  author_string: Wei Yang, Wei Lu, Vincent Zheng
  bibkey: yang-etal-2017-simple
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1312
  month: September
  page_first: '2898'
  page_last: '2904'
  pages: "2898\u20132904"
  paper_id: '312'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1312.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1312.jpg
  title: A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings
  title_html: A Simple Regularization-based Algorithm for Learning Cross-Domain Word
    Embeddings
  url: https://www.aclweb.org/anthology/D17-1312
  year: '2017'
D17-1313:
  abstract: "Recent efforts in bioinformatics have achieved tremendous progress in\
    \ the machine reading of biomedical literature, and the assembly of the extracted\
    \ biochemical interactions into large-scale models such as protein signaling pathways.\
    \ However, batch machine reading of literature at today\u2019s scale (PubMed alone\
    \ indexes over 1 million papers per year) is unfeasible due to both cost and processing\
    \ overhead. In this work, we introduce a focused reading approach to guide the\
    \ machine reading of biomedical literature towards what literature should be read\
    \ to answer a biomedical query as efficiently as possible. We introduce a family\
    \ of algorithms for focused reading, including an intuitive, strong baseline,\
    \ and a second approach which uses a reinforcement learning (RL) framework that\
    \ learns when to explore (widen the search) or exploit (narrow it). We demonstrate\
    \ that the RL approach is capable of answering more queries than the baseline,\
    \ while being more efficient, i.e., reading fewer documents."
  address: Copenhagen, Denmark
  author:
  - first: Enrique
    full: Enrique Noriega-Atala
    id: enrique-noriega-atala
    last: Noriega-Atala
  - first: Marco A.
    full: "Marco A. Valenzuela-Esc\xE1rcega"
    id: marco-a-valenzuela-escarcega
    last: "Valenzuela-Esc\xE1rcega"
  - first: Clayton
    full: Clayton Morrison
    id: clayton-morrison
    last: Morrison
  - first: Mihai
    full: Mihai Surdeanu
    id: mihai-surdeanu
    last: Surdeanu
  author_string: "Enrique Noriega-Atala, Marco A. Valenzuela-Esc\xE1rcega, Clayton\
    \ Morrison, Mihai Surdeanu"
  bibkey: noriega-atala-etal-2017-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1313
  month: September
  page_first: '2905'
  page_last: '2910'
  pages: "2905\u20132910"
  paper_id: '313'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1313.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1313.jpg
  title: 'Learning what to read: Focused machine reading'
  title_html: 'Learning what to read: Focused machine reading'
  url: https://www.aclweb.org/anthology/D17-1313
  year: '2017'
D17-1314:
  abstract: Traditional supervised learning makes the closed-world assumption that
    the classes appeared in the test data must have appeared in training. This also
    applies to text learning or text classification. As learning is used increasingly
    in dynamic open environments where some new/test documents may not belong to any
    of the training classes, identifying these novel documents during classification
    presents an important problem. This problem is called open-world classification
    or open classification. This paper proposes a novel deep learning based approach.
    It outperforms existing state-of-the-art techniques dramatically.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238232487
    type: video
    url: https://vimeo.com/238232487
  author:
  - first: Lei
    full: Lei Shu
    id: lei-shu
    last: Shu
  - first: Hu
    full: Hu Xu
    id: hu-xu
    last: Xu
  - first: Bing
    full: Bing Liu
    id: bing-liu
    last: Liu
  author_string: Lei Shu, Hu Xu, Bing Liu
  bibkey: shu-etal-2017-doc
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1314
  month: September
  page_first: '2911'
  page_last: '2916'
  pages: "2911\u20132916"
  paper_id: '314'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1314.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1314.jpg
  title: 'DOC: Deep Open Classification of Text Documents'
  title_html: '<span class="acl-fixed-case">DOC</span>: Deep Open Classification of
    Text Documents'
  url: https://www.aclweb.org/anthology/D17-1314
  year: '2017'
D17-1315:
  abstract: Portmanteaus are a word formation phenomenon where two words combine into
    a new word. We propose character-level neural sequence-to-sequence (S2S) methods
    for the task of portmanteau generation that are end-to-end-trainable, language
    independent, and do not explicitly use additional phonetic information. We propose
    a noisy-channel-style model, which allows for the incorporation of unsupervised
    word lists, improving performance over a standard source-to-target model. This
    model is made possible by an exhaustive candidate generation strategy specifically
    enabled by the features of the portmanteau task. Experiments find our approach
    superior to a state-of-the-art FST-based baseline with respect to ground truth
    accuracy and human evaluation.
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1315.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1315.Attachment.zip
  - filename: https://vimeo.com/238231770
    type: video
    url: https://vimeo.com/238231770
  author:
  - first: Varun
    full: Varun Gangal
    id: varun-gangal
    last: Gangal
  - first: Harsh
    full: Harsh Jhamtani
    id: harsh-jhamtani
    last: Jhamtani
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  - first: Eduard
    full: Eduard Hovy
    id: eduard-hovy
    last: Hovy
  - first: Eric
    full: Eric Nyberg
    id: eric-nyberg
    last: Nyberg
  author_string: Varun Gangal, Harsh Jhamtani, Graham Neubig, Eduard Hovy, Eric Nyberg
  bibkey: gangal-etal-2017-charmanteau
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1315
  month: September
  page_first: '2917'
  page_last: '2922'
  pages: "2917\u20132922"
  paper_id: '315'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1315.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1315.jpg
  title: 'Charmanteau: Character Embedding Models For Portmanteau Creation'
  title_html: '<span class="acl-fixed-case">C</span>harmanteau: Character Embedding
    Models For Portmanteau Creation'
  url: https://www.aclweb.org/anthology/D17-1315
  year: '2017'
D17-1316:
  abstract: The diagnosis of serious mental health conditions such as schizophrenia
    is based on the judgment of clinicians whose training takes several years, and
    cannot be easily formalized into objective measures. However, previous research
    suggests there are disturbances in aspects of the language use of patients with
    schizophrenia. Using metaphor-identification and sentiment-analysis algorithms
    to automatically generate features, we create a classifier, that, with high accuracy,
    can predict which patients will develop (or currently suffer from) schizophrenia.
    To our knowledge, this study is the first to demonstrate the utility of automated
    metaphor identification algorithms for detection or prediction of disease.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238236651
    type: video
    url: https://vimeo.com/238236651
  author:
  - first: "E. Dar\xEDo"
    full: "E. Dar\xEDo Guti\xE9rrez"
    id: e-dario-gutierrez
    last: "Guti\xE9rrez"
  - first: Guillermo
    full: Guillermo Cecchi
    id: guillermo-a-cecchi
    last: Cecchi
  - first: Cheryl
    full: Cheryl Corcoran
    id: cheryl-corcoran
    last: Corcoran
  - first: Philip
    full: Philip Corlett
    id: philip-corlett
    last: Corlett
  author_string: "E. Dar\xEDo Guti\xE9rrez, Guillermo Cecchi, Cheryl Corcoran, Philip\
    \ Corlett"
  bibkey: gutierrez-etal-2017-using
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1316
  month: September
  page_first: '2923'
  page_last: '2930'
  pages: "2923\u20132930"
  paper_id: '316'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1316.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1316.jpg
  title: Using Automated Metaphor Identification to Aid in Detection and Prediction
    of First-Episode Schizophrenia
  title_html: Using Automated Metaphor Identification to Aid in Detection and Prediction
    of First-Episode Schizophrenia
  url: https://www.aclweb.org/anthology/D17-1316
  year: '2017'
D17-1317:
  abstract: We present an analytic study on the language of news media in the context
    of political fact-checking and fake news detection. We compare the language of
    real news with that of satire, hoaxes, and propaganda to find linguistic characteristics
    of untrustworthy text. To probe the feasibility of automatic political fact-checking,
    we also present a case study based on PolitiFact.com using their factuality judgments
    on a 6-point scale. Experiments show that while media fact-checking remains to
    be an open research question, stylistic cues can help determine the truthfulness
    of text.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238236521
    type: video
    url: https://vimeo.com/238236521
  author:
  - first: Hannah
    full: Hannah Rashkin
    id: hannah-rashkin
    last: Rashkin
  - first: Eunsol
    full: Eunsol Choi
    id: eunsol-choi
    last: Choi
  - first: Jin Yea
    full: Jin Yea Jang
    id: jin-yea-jang
    last: Jang
  - first: Svitlana
    full: Svitlana Volkova
    id: svitlana-volkova
    last: Volkova
  - first: Yejin
    full: Yejin Choi
    id: yejin-choi
    last: Choi
  author_string: Hannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana Volkova, Yejin
    Choi
  bibkey: rashkin-etal-2017-truth
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1317
  month: September
  page_first: '2931'
  page_last: '2937'
  pages: "2931\u20132937"
  paper_id: '317'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1317.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1317.jpg
  title: 'Truth of Varying Shades: Analyzing Language in Fake News and Political Fact-Checking'
  title_html: 'Truth of Varying Shades: Analyzing Language in Fake News and Political
    Fact-Checking'
  url: https://www.aclweb.org/anthology/D17-1317
  year: '2017'
D17-1318:
  abstract: We present a topic-based analysis of agreement and disagreement in political
    manifestos, which relies on a new method for topic detection based on key concept
    clustering. Our approach outperforms both standard techniques like LDA and a state-of-the-art
    graph-based method, and provides promising initial results for this new task in
    computational social science.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238236263
    type: video
    url: https://vimeo.com/238236263
  author:
  - first: Stefano
    full: Stefano Menini
    id: stefano-menini
    last: Menini
  - first: Federico
    full: Federico Nanni
    id: federico-nanni
    last: Nanni
  - first: Simone Paolo
    full: Simone Paolo Ponzetto
    id: simone-paolo-ponzetto
    last: Ponzetto
  - first: Sara
    full: Sara Tonelli
    id: sara-tonelli
    last: Tonelli
  author_string: Stefano Menini, Federico Nanni, Simone Paolo Ponzetto, Sara Tonelli
  bibkey: menini-etal-2017-topic
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1318
  month: September
  page_first: '2938'
  page_last: '2944'
  pages: "2938\u20132944"
  paper_id: '318'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1318.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1318.jpg
  title: Topic-Based Agreement and Disagreement in US Electoral Manifestos
  title_html: Topic-Based Agreement and Disagreement in <span class="acl-fixed-case">US</span>
    Electoral Manifestos
  url: https://www.aclweb.org/anthology/D17-1318
  year: '2017'
D17-1319:
  abstract: We introduce Zipporah, a fast and scalable data cleaning system. We propose
    a novel type of bag-of-words translation feature, and train logistic regression
    models to classify good data and synthetic noisy data in the proposed feature
    space. The trained model is used to score parallel sentences in the data pool
    for selection. As shown in experiments, Zipporah selects a high-quality parallel
    corpus from a large, mixed quality data pool. In particular, for one noisy dataset,
    Zipporah achieves a 2.1 BLEU score improvement with using 1/5 of the data over
    using the entire corpus.
  address: Copenhagen, Denmark
  attachment:
  - filename: https://vimeo.com/238236824
    type: video
    url: https://vimeo.com/238236824
  author:
  - first: Hainan
    full: Hainan Xu
    id: hainan-xu
    last: Xu
  - first: Philipp
    full: Philipp Koehn
    id: philipp-koehn
    last: Koehn
  author_string: Hainan Xu, Philipp Koehn
  bibkey: xu-koehn-2017-zipporah
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1319
  month: September
  page_first: '2945'
  page_last: '2950'
  pages: "2945\u20132950"
  paper_id: '319'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1319.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1319.jpg
  title: 'Zipporah: a Fast and Scalable Data Cleaning System for Noisy Web-Crawled
    Parallel Corpora'
  title_html: '<span class="acl-fixed-case">Z</span>ipporah: a Fast and Scalable Data
    Cleaning System for Noisy Web-Crawled Parallel Corpora'
  url: https://www.aclweb.org/anthology/D17-1319
  year: '2017'
D17-1320:
  abstract: Concept maps can be used to concisely represent important information
    and bring structure into large document collections. Therefore, we study a variant
    of multi-document summarization that produces summaries in the form of concept
    maps. However, suitable evaluation datasets for this task are currently missing.
    To close this gap, we present a newly created corpus of concept maps that summarize
    heterogeneous collections of web documents on educational topics. It was created
    using a novel crowdsourcing approach that allows us to efficiently determine important
    elements in large document collections. We release the corpus along with a baseline
    system and proposed evaluation protocol to enable further research on this variant
    of summarization.
  address: Copenhagen, Denmark
  author:
  - first: Tobias
    full: Tobias Falke
    id: tobias-falke
    last: Falke
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Tobias Falke, Iryna Gurevych
  bibkey: falke-gurevych-2017-bringing
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1320
  month: September
  page_first: '2951'
  page_last: '2961'
  pages: "2951\u20132961"
  paper_id: '320'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1320.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1320.jpg
  title: 'Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of Concept
    Maps'
  title_html: 'Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus
    of Concept Maps'
  url: https://www.aclweb.org/anthology/D17-1320
  year: '2017'
D17-1321:
  abstract: "A number of recent works have proposed techniques for end-to-end learning\
    \ of communication protocols among cooperative multi-agent populations, and have\
    \ simultaneously found the emergence of grounded human-interpretable language\
    \ in the protocols developed by the agents, learned without any human supervision!\
    \ In this paper, using a Task & Talk reference game between two agents as a testbed,\
    \ we present a sequence of \u2018negative\u2019 results culminating in a \u2018\
    positive\u2019 one \u2013 showing that while most agent-invented languages are\
    \ effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable\
    \ or compositional. In essence, we find that natural language does not emerge\
    \ \u2018naturally\u2019,despite the semblance of ease of natural-language-emergence\
    \ that one may gather from recent literature. We discuss how it is possible to\
    \ coax the invented languages to become more and more human-like and compositional\
    \ by increasing restrictions on how two agents may communicate."
  address: Copenhagen, Denmark
  attachment:
  - filename: D17-1321.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D17-1321.Attachment.zip
  author:
  - first: Satwik
    full: Satwik Kottur
    id: satwik-kottur
    last: Kottur
  - first: "Jos\xE9"
    full: "Jos\xE9 Moura"
    id: jose-moura
    last: Moura
  - first: Stefan
    full: Stefan Lee
    id: stefan-lee
    last: Lee
  - first: Dhruv
    full: Dhruv Batra
    id: dhruv-batra
    last: Batra
  author_string: "Satwik Kottur, Jos\xE9 Moura, Stefan Lee, Dhruv Batra"
  bibkey: kottur-etal-2017-natural
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1321
  month: September
  page_first: '2962'
  page_last: '2967'
  pages: "2962\u20132967"
  paper_id: '321'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1321.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1321.jpg
  title: "Natural Language Does Not Emerge \u2018Naturally\u2019 in Multi-Agent Dialog"
  title_html: "Natural Language Does Not Emerge \u2018Naturally\u2019 in Multi-Agent\
    \ Dialog"
  url: https://www.aclweb.org/anthology/D17-1321
  year: '2017'
D17-1322:
  abstract: Users suffering from mental health conditions often turn to online resources
    for support, including specialized online support communities or general communities
    such as Twitter and Reddit. In this work, we present a framework for supporting
    and studying users in both types of communities. We propose methods for identifying
    posts in support communities that may indicate a risk of self-harm, and demonstrate
    that our approach outperforms strong previously proposed methods for identifying
    such posts. Self-harm is closely related to depression, which makes identifying
    depressed users on general forums a crucial related task. We introduce a large-scale
    general forum dataset consisting of users with self-reported depression diagnoses
    matched with control users. We show how our method can be applied to effectively
    identify depressed users from their use of language alone. We demonstrate that
    our method outperforms strong baselines on this general forum dataset.
  address: Copenhagen, Denmark
  author:
  - first: Andrew
    full: Andrew Yates
    id: andrew-yates
    last: Yates
  - first: Arman
    full: Arman Cohan
    id: arman-cohan
    last: Cohan
  - first: Nazli
    full: Nazli Goharian
    id: nazli-goharian
    last: Goharian
  author_string: Andrew Yates, Arman Cohan, Nazli Goharian
  bibkey: yates-etal-2017-depression
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1322
  month: September
  page_first: '2968'
  page_last: '2978'
  pages: "2968\u20132978"
  paper_id: '322'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1322.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1322.jpg
  title: Depression and Self-Harm Risk Assessment in Online Forums
  title_html: Depression and Self-Harm Risk Assessment in Online Forums
  url: https://www.aclweb.org/anthology/D17-1322
  year: '2017'
D17-1323:
  abstract: "Language is increasingly being used to de-fine rich visual recognition\
    \ problems with supporting image collections sourced from the web. Structured\
    \ prediction models are used in these tasks to take advantage of correlations\
    \ between co-occurring labels and visual input but risk inadvertently encoding\
    \ social biases found in web corpora. In this work, we study data and models associated\
    \ with multilabel object classification and visual semantic role labeling. We\
    \ find that (a) datasets for these tasks contain significant gender bias and (b)\
    \ models trained on these datasets further amplify existing bias. For example,\
    \ the activity cooking is over 33% more likely to involve females than males in\
    \ a training set, and a trained model further amplifies the disparity to 68% at\
    \ test time. We propose to inject corpus-level constraints for calibrating existing\
    \ structured prediction models and design an algorithm based on Lagrangian relaxation\
    \ for collective inference. Our method results in almost no performance loss for\
    \ the underlying recognition task but decreases the magnitude of bias amplification\
    \ by 47.5% and 40.5% for multilabel classification and visual semantic role labeling,\
    \ respectively\u3002"
  address: Copenhagen, Denmark
  author:
  - first: Jieyu
    full: Jieyu Zhao
    id: jieyu-zhao
    last: Zhao
  - first: Tianlu
    full: Tianlu Wang
    id: tianlu-wang
    last: Wang
  - first: Mark
    full: Mark Yatskar
    id: mark-yatskar
    last: Yatskar
  - first: Vicente
    full: Vicente Ordonez
    id: vicente-ordonez
    last: Ordonez
  - first: Kai-Wei
    full: Kai-Wei Chang
    id: kai-wei-chang
    last: Chang
  author_string: Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, Kai-Wei Chang
  bibkey: zhao-etal-2017-men
  bibtype: inproceedings
  booktitle: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D17-1323
  month: September
  page_first: '2979'
  page_last: '2989'
  pages: "2979\u20132989"
  paper_id: '323'
  parent_volume_id: D17-1
  pdf: https://www.aclweb.org/anthology/D17-1323.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-1323.jpg
  title: 'Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level
    Constraints'
  title_html: 'Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level
    Constraints'
  url: https://www.aclweb.org/anthology/D17-1323
  year: '2017'
D17-2000:
  address: Copenhagen, Denmark
  author:
  - first: Lucia
    full: Lucia Specia
    id: lucia-specia
    last: Specia
  - first: Matt
    full: Matt Post
    id: matt-post
    last: Post
  - first: Michael
    full: Michael Paul
    id: michael-paul
    last: Paul
  author_string: Lucia Specia, Matt Post, Michael Paul
  bibkey: emnlp-2017-2017-empirical
  bibtype: proceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  doi: 10.18653/v1/D17-2
  month: September
  paper_id: '0'
  parent_volume_id: D17-2
  pdf: https://www.aclweb.org/anthology/D17-2000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-2000.jpg
  title: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  title_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  url: https://www.aclweb.org/anthology/D17-2000
  year: '2017'
D17-2001:
  abstract: A new Python API, integrated within the NLTK suite, offers access to the
    FrameNet 1.7 lexical database. The lexicon (structured in terms of frames) as
    well as annotated sentences can be processed programatically, or browsed with
    human-readable displays via the interactive Python prompt.
  address: Copenhagen, Denmark
  author:
  - first: Nathan
    full: Nathan Schneider
    id: nathan-schneider
    last: Schneider
  - first: Chuck
    full: Chuck Wooters
    id: chuck-wooters
    last: Wooters
  author_string: Nathan Schneider, Chuck Wooters
  bibkey: schneider-wooters-2017-nltk
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D17-2001
  month: September
  page_first: '1'
  page_last: '6'
  pages: "1\u20136"
  paper_id: '1'
  parent_volume_id: D17-2
  pdf: https://www.aclweb.org/anthology/D17-2001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-2001.jpg
  title: 'The NLTK FrameNet API: Designing for Discoverability with a Rich Linguistic
    Resource'
  title_html: 'The <span class="acl-fixed-case">NLTK</span> <span class="acl-fixed-case">F</span>rame<span
    class="acl-fixed-case">N</span>et <span class="acl-fixed-case">API</span>: Designing
    for Discoverability with a Rich Linguistic Resource'
  url: https://www.aclweb.org/anthology/D17-2001
  year: '2017'
D17-2002:
  abstract: "An important skill in critical thinking and argumentation is the ability\
    \ to spot and recognize fallacies. Fallacious arguments, omnipresent in argumentative\
    \ discourse, can be deceptive, manipulative, or simply leading to \u2018wrong\
    \ moves\u2019 in a discussion. Despite their importance, argumentation scholars\
    \ and NLP researchers with focus on argumentation quality have not yet investigated\
    \ fallacies empirically. The nonexistence of resources dealing with fallacious\
    \ argumentation calls for scalable approaches to data acquisition and annotation,\
    \ for which the serious games methodology offers an appealing, yet unexplored,\
    \ alternative. We present Argotario, a serious game that deals with fallacies\
    \ in everyday argumentation. Argotario is a multilingual, open-source, platform-independent\
    \ application with strong educational aspects, accessible at www.argotario.net."
  address: Copenhagen, Denmark
  author:
  - first: Ivan
    full: Ivan Habernal
    id: ivan-habernal
    last: Habernal
  - first: Raffael
    full: Raffael Hannemann
    id: raffael-hannemann
    last: Hannemann
  - first: Christian
    full: Christian Pollak
    id: christian-pollak
    last: Pollak
  - first: Christopher
    full: Christopher Klamm
    id: christopher-klamm
    last: Klamm
  - first: Patrick
    full: Patrick Pauli
    id: patrick-pauli
    last: Pauli
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Ivan Habernal, Raffael Hannemann, Christian Pollak, Christopher Klamm,
    Patrick Pauli, Iryna Gurevych
  bibkey: habernal-etal-2017-argotario
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D17-2002
  month: September
  page_first: '7'
  page_last: '12'
  pages: "7\u201312"
  paper_id: '2'
  parent_volume_id: D17-2
  pdf: https://www.aclweb.org/anthology/D17-2002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-2002.jpg
  title: 'Argotario: Computational Argumentation Meets Serious Games'
  title_html: '<span class="acl-fixed-case">A</span>rgotario: Computational Argumentation
    Meets Serious Games'
  url: https://www.aclweb.org/anthology/D17-2002
  year: '2017'
D17-2003:
  abstract: We present an educational tool that integrates computational linguistics
    resources for use in non-technical undergraduate language science courses. By
    using the tool in conjunction with evidence-driven pedagogical case studies, we
    strive to provide opportunities for students to gain an understanding of linguistic
    concepts and analysis through the lens of realistic problems in feasible ways.
    Case studies tend to be used in legal, business, and health education contexts,
    but less in the teaching and learning of linguistics. The approach introduced
    also has potential to encourage students across training backgrounds to continue
    on to computational language analysis coursework.
  address: Copenhagen, Denmark
  author:
  - first: Cecilia
    full: Cecilia Ovesdotter Alm
    id: cecilia-ovesdotter-alm
    last: Ovesdotter Alm
  - first: Benjamin
    full: Benjamin Meyers
    id: benjamin-s-meyers
    last: Meyers
  - first: Emily
    full: "Emily Prud\u2019hommeaux"
    id: emily-prudhommeaux
    last: "Prud\u2019hommeaux"
  author_string: "Cecilia Ovesdotter Alm, Benjamin Meyers, Emily Prud\u2019hommeaux"
  bibkey: ovesdotter-alm-etal-2017-analysis
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D17-2003
  month: September
  page_first: '13'
  page_last: '18'
  pages: "13\u201318"
  paper_id: '3'
  parent_volume_id: D17-2
  pdf: https://www.aclweb.org/anthology/D17-2003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-2003.jpg
  title: An Analysis and Visualization Tool for Case Study Learning of Linguistic
    Concepts
  title_html: An Analysis and Visualization Tool for Case Study Learning of Linguistic
    Concepts
  url: https://www.aclweb.org/anthology/D17-2003
  year: '2017'
D17-2004:
  abstract: Graphs have long been proposed as a tool to browse and navigate in a collection
    of documents in order to support exploratory search. Many techniques to automatically
    extract different types of graphs, showing for example entities or concepts and
    different relationships between them, have been suggested. While experimental
    evidence that they are indeed helpful exists for some of them, it is largely unknown
    which type of graph is most helpful for a specific exploratory task. However,
    carrying out experimental comparisons with human subjects is challenging and time-consuming.
    Towards this end, we present the GraphDocExplore framework. It provides an intuitive
    web interface for graph-based document exploration that is optimized for experimental
    user studies. Through a generic graph interface, different methods to extract
    graphs from text can be plugged into the system. Hence, they can be compared at
    minimal implementation effort in an environment that ensures controlled comparisons.
    The system is publicly available under an open-source license.
  address: Copenhagen, Denmark
  author:
  - first: Tobias
    full: Tobias Falke
    id: tobias-falke
    last: Falke
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Tobias Falke, Iryna Gurevych
  bibkey: falke-gurevych-2017-graphdocexplore
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D17-2004
  month: September
  page_first: '19'
  page_last: '24'
  pages: "19\u201324"
  paper_id: '4'
  parent_volume_id: D17-2
  pdf: https://www.aclweb.org/anthology/D17-2004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-2004.jpg
  title: 'GraphDocExplore: A Framework for the Experimental Comparison of Graph-based
    Document Exploration Techniques'
  title_html: '<span class="acl-fixed-case">G</span>raph<span class="acl-fixed-case">D</span>oc<span
    class="acl-fixed-case">E</span>xplore: A Framework for the Experimental Comparison
    of Graph-based Document Exploration Techniques'
  url: https://www.aclweb.org/anthology/D17-2004
  year: '2017'
D17-2005:
  abstract: This paper introduces SGNMT, our experimental platform for machine translation
    research. SGNMT provides a generic interface to neural and symbolic scoring modules
    (predictors) with left-to-right semantic such as translation models like NMT,
    language models, translation lattices, n-best lists or other kinds of scores and
    constraints. Predictors can be combined with other predictors to form complex
    decoding tasks. SGNMT implements a number of search strategies for traversing
    the space spanned by the predictors which are appropriate for different predictor
    constellations. Adding new predictors or decoding strategies is particularly easy,
    making it a very efficient tool for prototyping new research ideas. SGNMT is actively
    being used by students in the MPhil program in Machine Learning, Speech and Language
    Technology at the University of Cambridge for course work and theses, as well
    as for most of the research work in our group.
  address: Copenhagen, Denmark
  author:
  - first: Felix
    full: Felix Stahlberg
    id: felix-stahlberg
    last: Stahlberg
  - first: Eva
    full: Eva Hasler
    id: eva-hasler
    last: Hasler
  - first: Danielle
    full: Danielle Saunders
    id: danielle-saunders
    last: Saunders
  - first: Bill
    full: Bill Byrne
    id: bill-byrne
    last: Byrne
  author_string: Felix Stahlberg, Eva Hasler, Danielle Saunders, Bill Byrne
  bibkey: stahlberg-etal-2017-sgnmt
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D17-2005
  month: September
  page_first: '25'
  page_last: '30'
  pages: "25\u201330"
  paper_id: '5'
  parent_volume_id: D17-2
  pdf: https://www.aclweb.org/anthology/D17-2005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-2005.jpg
  title: "SGNMT \u2013 A Flexible NMT Decoding Platform for Quick Prototyping of New\
    \ Models and Search Strategies"
  title_html: "<span class=\"acl-fixed-case\">SGNMT</span> \u2013 A Flexible <span\
    \ class=\"acl-fixed-case\">NMT</span> Decoding Platform for Quick Prototyping\
    \ of New Models and Search Strategies"
  url: https://www.aclweb.org/anthology/D17-2005
  year: '2017'
D17-2006:
  abstract: We present a tool for developing tree structure patterns that makes it
    easy to define the relations among textual phrases and create a search index for
    these newly defined relations. By using the proposed tool, users develop tree
    structure patterns through abstracting syntax trees. The tool features (1) intuitive
    pattern syntax, (2) unique functions such as recursive call of patterns and use
    of lexicon dictionaries, and (3) whole workflow support for relation development
    and validation. We report the current implementation of the tool and its effectiveness.
  address: Copenhagen, Denmark
  author:
  - first: Kohsuke
    full: Kohsuke Yanai
    id: kohsuke-yanai
    last: Yanai
  - first: Misa
    full: Misa Sato
    id: misa-sato
    last: Sato
  - first: Toshihiko
    full: Toshihiko Yanase
    id: toshihiko-yanase
    last: Yanase
  - first: Kenzo
    full: Kenzo Kurotsuchi
    id: kenzo-kurotsuchi
    last: Kurotsuchi
  - first: Yuta
    full: Yuta Koreeda
    id: yuta-koreeda
    last: Koreeda
  - first: Yoshiki
    full: Yoshiki Niwa
    id: yoshiki-niwa
    last: Niwa
  author_string: Kohsuke Yanai, Misa Sato, Toshihiko Yanase, Kenzo Kurotsuchi, Yuta
    Koreeda, Yoshiki Niwa
  bibkey: yanai-etal-2017-struap
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D17-2006
  month: September
  page_first: '31'
  page_last: '36'
  pages: "31\u201336"
  paper_id: '6'
  parent_volume_id: D17-2
  pdf: https://www.aclweb.org/anthology/D17-2006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-2006.jpg
  title: 'StruAP: A Tool for Bundling Linguistic Trees through Structure-based Abstract
    Pattern'
  title_html: '<span class="acl-fixed-case">S</span>tru<span class="acl-fixed-case">AP</span>:
    A Tool for Bundling Linguistic Trees through Structure-based Abstract Pattern'
  url: https://www.aclweb.org/anthology/D17-2006
  year: '2017'
D17-2007:
  abstract: "Semantic relation knowledge is crucial for natural language understanding.\
    \ We introduce \u201CKnowYourNyms?\u201D, a web-based game for learning semantic\
    \ relations. While providing users with an engaging experience, the application\
    \ collects large amounts of data that can be used to improve semantic relation\
    \ classifiers. The data also broadly informs us of how people perceive the relationships\
    \ between words, providing useful insights for research in psychology and linguistics."
  address: Copenhagen, Denmark
  author:
  - first: Ross
    full: Ross Mechanic
    id: ross-mechanic
    last: Mechanic
  - first: Dean
    full: Dean Fulgoni
    id: dean-fulgoni
    last: Fulgoni
  - first: Hannah
    full: Hannah Cutler
    id: hannah-cutler
    last: Cutler
  - first: Sneha
    full: Sneha Rajana
    id: sneha-rajana
    last: Rajana
  - first: Zheyuan
    full: Zheyuan Liu
    id: zheyuan-liu
    last: Liu
  - first: Bradley
    full: Bradley Jackson
    id: bradley-jackson
    last: Jackson
  - first: Anne
    full: Anne Cocos
    id: anne-cocos
    last: Cocos
  - first: Chris
    full: Chris Callison-Burch
    id: chris-callison-burch
    last: Callison-Burch
  - first: Marianna
    full: Marianna Apidianaki
    id: marianna-apidianaki
    last: Apidianaki
  author_string: Ross Mechanic, Dean Fulgoni, Hannah Cutler, Sneha Rajana, Zheyuan
    Liu, Bradley Jackson, Anne Cocos, Chris Callison-Burch, Marianna Apidianaki
  bibkey: mechanic-etal-2017-knowyournyms
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D17-2007
  month: September
  page_first: '37'
  page_last: '42'
  pages: "37\u201342"
  paper_id: '7'
  parent_volume_id: D17-2
  pdf: https://www.aclweb.org/anthology/D17-2007.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-2007.jpg
  title: KnowYourNyms? A Game of Semantic Relationships
  title_html: <span class="acl-fixed-case">K</span>now<span class="acl-fixed-case">Y</span>our<span
    class="acl-fixed-case">N</span>yms? A Game of Semantic Relationships
  url: https://www.aclweb.org/anthology/D17-2007
  year: '2017'
D17-2008:
  abstract: 'Previous works proposed annotation projection in parallel corpora to
    inexpensively generate treebanks or propbanks for new languages. In this approach,
    linguistic annotation is automatically transferred from a resource-rich source
    language (SL) to translations in a target language (TL). However, annotation projection
    may be adversely affected by translational divergences between specific language
    pairs. For this reason, previous work often required careful qualitative analysis
    of projectability of specific annotation in order to define strategies to address
    quality and coverage issues. In this demonstration, we present THE PROJECTOR,
    an interactive GUI designed to assist researchers in such analysis: it allows
    users to execute and visually inspect annotation projection in a range of different
    settings. We give an overview of the GUI, discuss use cases and illustrate how
    the tool can facilitate discussions with the research community.'
  address: Copenhagen, Denmark
  author:
  - first: Alan
    full: Alan Akbik
    id: alan-akbik
    last: Akbik
  - first: Roland
    full: Roland Vollgraf
    id: roland-vollgraf
    last: Vollgraf
  author_string: Alan Akbik, Roland Vollgraf
  bibkey: akbik-vollgraf-2017-projector
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D17-2008
  month: September
  page_first: '43'
  page_last: '48'
  pages: "43\u201348"
  paper_id: '8'
  parent_volume_id: D17-2
  pdf: https://www.aclweb.org/anthology/D17-2008.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-2008.jpg
  title: 'The Projector: An Interactive Annotation Projection Visualization Tool'
  title_html: 'The Projector: An Interactive Annotation Projection Visualization Tool'
  url: https://www.aclweb.org/anthology/D17-2008
  year: '2017'
D17-2009:
  abstract: We provide a visualization library and web interface for interactively
    exploring a parse tree or a forest of parses. The library is not tied to any particular
    linguistic representation, but provides a general-purpose API for the interactive
    exploration of hierarchical linguistic structure. To facilitate rapid understanding
    of a complex structure, the API offers several important features, including expand/collapse
    functionality, positional and color cues, explicit visual support for sequential
    structure, and dynamic highlighting to convey node-to-text correspondence.
  address: Copenhagen, Denmark
  author:
  - first: Aaron
    full: Aaron Sarnat
    id: aaron-sarnat
    last: Sarnat
  - first: Vidur
    full: Vidur Joshi
    id: vidur-joshi
    last: Joshi
  - first: Cristian
    full: Cristian Petrescu-Prahova
    id: cristian-petrescu-prahova
    last: Petrescu-Prahova
  - first: Alvaro
    full: Alvaro Herrasti
    id: alvaro-herrasti
    last: Herrasti
  - first: Brandon
    full: Brandon Stilson
    id: brandon-stilson
    last: Stilson
  - first: Mark
    full: Mark Hopkins
    id: mark-hopkins
    last: Hopkins
  author_string: Aaron Sarnat, Vidur Joshi, Cristian Petrescu-Prahova, Alvaro Herrasti,
    Brandon Stilson, Mark Hopkins
  bibkey: sarnat-etal-2017-interactive
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D17-2009
  month: September
  page_first: '49'
  page_last: '54'
  pages: "49\u201354"
  paper_id: '9'
  parent_volume_id: D17-2
  pdf: https://www.aclweb.org/anthology/D17-2009.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-2009.jpg
  title: Interactive Visualization for Linguistic Structure
  title_html: Interactive Visualization for Linguistic Structure
  url: https://www.aclweb.org/anthology/D17-2009
  year: '2017'
D17-2010:
  abstract: 'We present Differential Language Analysis Toolkit (DLATK), an open-source
    python package and command-line tool developed for conducting social-scientific
    language analyses. While DLATK provides standard NLP pipeline steps such as tokenization
    or SVM-classification, its novel strengths lie in analyses useful for psychological,
    health, and social science: (1) incorporation of extra-linguistic structured information,
    (2) specified levels and units of analysis (e.g. document, user, community), (3)
    statistical metrics for continuous outcomes, and (4) robust, proven, and accurate
    pipelines for social-scientific prediction problems. DLATK integrates multiple
    popular packages (SKLearn, Mallet), enables interactive usage (Jupyter Notebooks),
    and generally follows object oriented principles to make it easy to tie in additional
    libraries or storage technologies.'
  address: Copenhagen, Denmark
  author:
  - first: H. Andrew
    full: H. Andrew Schwartz
    id: h-andrew-schwartz
    last: Schwartz
  - first: Salvatore
    full: Salvatore Giorgi
    id: salvatore-giorgi
    last: Giorgi
  - first: Maarten
    full: Maarten Sap
    id: maarten-sap
    last: Sap
  - first: Patrick
    full: Patrick Crutchley
    id: patrick-crutchley
    last: Crutchley
  - first: Lyle
    full: Lyle Ungar
    id: lyle-ungar
    last: Ungar
  - first: Johannes
    full: Johannes Eichstaedt
    id: johannes-eichstaedt
    last: Eichstaedt
  author_string: H. Andrew Schwartz, Salvatore Giorgi, Maarten Sap, Patrick Crutchley,
    Lyle Ungar, Johannes Eichstaedt
  bibkey: schwartz-etal-2017-dlatk
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D17-2010
  month: September
  page_first: '55'
  page_last: '60'
  pages: "55\u201360"
  paper_id: '10'
  parent_volume_id: D17-2
  pdf: https://www.aclweb.org/anthology/D17-2010.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-2010.jpg
  title: 'DLATK: Differential Language Analysis ToolKit'
  title_html: '<span class="acl-fixed-case">DLATK</span>: Differential Language Analysis
    <span class="acl-fixed-case">T</span>ool<span class="acl-fixed-case">K</span>it'
  url: https://www.aclweb.org/anthology/D17-2010
  year: '2017'
D17-2011:
  abstract: We present QUINT, a live system for question answering over knowledge
    bases. QUINT automatically learns role-aligned utterance-query templates from
    user questions paired with their answers. When QUINT answers a question, it visualizes
    the complete derivation sequence from the natural language utterance to the final
    answer. The derivation provides an explanation of how the syntactic structure
    of the question was used to derive the structure of a SPARQL query, and how the
    phrases in the question were used to instantiate different parts of the query.
    When an answer seems unsatisfactory, the derivation provides valuable insights
    towards reformulating the question.
  address: Copenhagen, Denmark
  author:
  - first: Abdalghani
    full: Abdalghani Abujabal
    id: abdalghani-abujabal
    last: Abujabal
  - first: Rishiraj
    full: Rishiraj Saha Roy
    id: rishiraj-saha-roy
    last: Saha Roy
  - first: Mohamed
    full: Mohamed Yahya
    id: mohamed-yahya
    last: Yahya
  - first: Gerhard
    full: Gerhard Weikum
    id: gerhard-weikum
    last: Weikum
  author_string: Abdalghani Abujabal, Rishiraj Saha Roy, Mohamed Yahya, Gerhard Weikum
  bibkey: abujabal-etal-2017-quint
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D17-2011
  month: September
  page_first: '61'
  page_last: '66'
  pages: "61\u201366"
  paper_id: '11'
  parent_volume_id: D17-2
  pdf: https://www.aclweb.org/anthology/D17-2011.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-2011.jpg
  title: 'QUINT: Interpretable Question Answering over Knowledge Bases'
  title_html: '<span class="acl-fixed-case">QUINT</span>: Interpretable Question Answering
    over Knowledge Bases'
  url: https://www.aclweb.org/anthology/D17-2011
  year: '2017'
D17-2012:
  abstract: In this paper, we describe Function Assistant, a lightweight Python-based
    toolkit for querying and exploring source code repositories using natural language.
    The toolkit is designed to help end-users of a target API quickly find information
    about functions through high-level natural language queries, or descriptions.
    For a given text query and background API, the tool finds candidate functions
    by performing a translation from the text to known representations in the API
    using the semantic parsing approach of (Richardson and Kuhn, 2017). Translations
    are automatically learned from example text-code pairs in example APIs. The toolkit
    includes features for building translation pipelines and query engines for arbitrary
    source code projects. To explore this last feature, we perform new experiments
    on 27 well-known Python projects hosted on Github.
  address: Copenhagen, Denmark
  author:
  - first: Kyle
    full: Kyle Richardson
    id: kyle-richardson
    last: Richardson
  - first: Jonas
    full: Jonas Kuhn
    id: jonas-kuhn
    last: Kuhn
  author_string: Kyle Richardson, Jonas Kuhn
  bibkey: richardson-kuhn-2017-function
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D17-2012
  month: September
  page_first: '67'
  page_last: '72'
  pages: "67\u201372"
  paper_id: '12'
  parent_volume_id: D17-2
  pdf: https://www.aclweb.org/anthology/D17-2012.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-2012.jpg
  title: 'Function Assistant: A Tool for NL Querying of APIs'
  title_html: 'Function Assistant: A Tool for <span class="acl-fixed-case">NL</span>
    Querying of <span class="acl-fixed-case">API</span>s'
  url: https://www.aclweb.org/anthology/D17-2012
  year: '2017'
D17-2013:
  abstract: We present MoodSwipe, a soft keyboard that suggests text messages given
    the user-specified emotions utilizing the real dialog data. The aim of MoodSwipe
    is to create a convenient user interface to enjoy the technology of emotion classification
    and text suggestion, and at the same time to collect labeled data automatically
    for developing more advanced technologies. While users select the MoodSwipe keyboard,
    they can type as usual but sense the emotion conveyed by their text and receive
    suggestions for their message as a benefit. In MoodSwipe, the detected emotions
    serve as the medium for suggested texts, where viewing the latter is the incentive
    to correcting the former. We conduct several experiments to show the superiority
    of the emotion classification models trained on the dialog data, and further to
    verify good emotion cues are important context for text suggestion.
  address: Copenhagen, Denmark
  author:
  - first: Chieh-Yang
    full: Chieh-Yang Huang
    id: chieh-yang-huang
    last: Huang
  - first: Tristan
    full: Tristan Labetoulle
    id: tristan-labetoulle
    last: Labetoulle
  - first: Ting-Hao
    full: Ting-Hao Huang
    id: ting-hao-huang
    last: Huang
  - first: Yi-Pei
    full: Yi-Pei Chen
    id: yi-pei-chen
    last: Chen
  - first: Hung-Chen
    full: Hung-Chen Chen
    id: hung-chen-chen
    last: Chen
  - first: Vallari
    full: Vallari Srivastava
    id: vallari-srivastava
    last: Srivastava
  - first: Lun-Wei
    full: Lun-Wei Ku
    id: lun-wei-ku
    last: Ku
  author_string: Chieh-Yang Huang, Tristan Labetoulle, Ting-Hao Huang, Yi-Pei Chen,
    Hung-Chen Chen, Vallari Srivastava, Lun-Wei Ku
  bibkey: huang-etal-2017-moodswipe
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D17-2013
  month: September
  page_first: '73'
  page_last: '78'
  pages: "73\u201378"
  paper_id: '13'
  parent_volume_id: D17-2
  pdf: https://www.aclweb.org/anthology/D17-2013.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-2013.jpg
  title: 'MoodSwipe: A Soft Keyboard that Suggests MessageBased on User-Specified
    Emotions'
  title_html: '<span class="acl-fixed-case">M</span>ood<span class="acl-fixed-case">S</span>wipe:
    A Soft Keyboard that Suggests <span class="acl-fixed-case">M</span>essage<span
    class="acl-fixed-case">B</span>ased on User-Specified Emotions'
  url: https://www.aclweb.org/anthology/D17-2013
  year: '2017'
D17-2014:
  abstract: "We introduce ParlAI (pronounced \u201Cpar-lay\u201D), an open-source\
    \ software platform for dialog research implemented in Python, available at http://parl.ai.\
    \ Its goal is to provide a unified framework for sharing, training and testing\
    \ dialog models; integration of Amazon Mechanical Turk for data collection, human\
    \ evaluation, and online/reinforcement learning; and a repository of machine learning\
    \ models for comparing with others\u2019 models, and improving upon existing architectures.\
    \ Over 20 tasks are supported in the first release, including popular datasets\
    \ such as SQuAD, bAbI tasks, MCTest, WikiQA, QACNN, QADailyMail, CBT, bAbI Dialog,\
    \ Ubuntu, OpenSubtitles and VQA. Several models are integrated, including neural\
    \ models such as memory networks, seq2seq and attentive LSTMs."
  address: Copenhagen, Denmark
  author:
  - first: Alexander
    full: Alexander Miller
    id: alexander-miller
    last: Miller
  - first: Will
    full: Will Feng
    id: will-feng
    last: Feng
  - first: Dhruv
    full: Dhruv Batra
    id: dhruv-batra
    last: Batra
  - first: Antoine
    full: Antoine Bordes
    id: antoine-bordes
    last: Bordes
  - first: Adam
    full: Adam Fisch
    id: adam-fisch
    last: Fisch
  - first: Jiasen
    full: Jiasen Lu
    id: jiasen-lu
    last: Lu
  - first: Devi
    full: Devi Parikh
    id: devi-parikh
    last: Parikh
  - first: Jason
    full: Jason Weston
    id: jason-weston
    last: Weston
  author_string: Alexander Miller, Will Feng, Dhruv Batra, Antoine Bordes, Adam Fisch,
    Jiasen Lu, Devi Parikh, Jason Weston
  bibkey: miller-etal-2017-parlai
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D17-2014
  month: September
  page_first: '79'
  page_last: '84'
  pages: "79\u201384"
  paper_id: '14'
  parent_volume_id: D17-2
  pdf: https://www.aclweb.org/anthology/D17-2014.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-2014.jpg
  title: 'ParlAI: A Dialog Research Software Platform'
  title_html: '<span class="acl-fixed-case">P</span>arl<span class="acl-fixed-case">AI</span>:
    A Dialog Research Software Platform'
  url: https://www.aclweb.org/anthology/D17-2014
  year: '2017'
D17-2015:
  abstract: Geographic information extraction from textual data sources, called geoparsing,
    is a key task in text processing and central to subsequent spatial analysis approaches.
    Several geoparsers are available that support this task, each with its own (often
    limited or specialized) gazetteer and its own approaches to toponym detection
    and resolution. In this demonstration paper, we present HeidelPlace, an extensible
    framework in support of geoparsing. Key features of HeidelPlace include a generic
    gazetteer model that supports the integration of place information from different
    knowledge bases, and a pipeline approach that enables an effective combination
    of diverse modules tailored to specific geoparsing tasks. This makes HeidelPlace
    a valuable tool for testing and evaluating different gazetteer sources and geoparsing
    methods. In the demonstration, we show how to set up a geoparsing workflow with
    HeidelPlace and how it can be used to compare and consolidate the output of different
    geoparsing approaches.
  address: Copenhagen, Denmark
  author:
  - first: Ludwig
    full: Ludwig Richter
    id: ludwig-richter
    last: Richter
  - first: Johanna
    full: "Johanna Gei\xDF"
    id: johanna-geiss
    last: "Gei\xDF"
  - first: Andreas
    full: Andreas Spitz
    id: andreas-spitz
    last: Spitz
  - first: Michael
    full: Michael Gertz
    id: michael-gertz
    last: Gertz
  author_string: "Ludwig Richter, Johanna Gei\xDF, Andreas Spitz, Michael Gertz"
  bibkey: richter-etal-2017-heidelplace
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D17-2015
  month: September
  page_first: '85'
  page_last: '90'
  pages: "85\u201390"
  paper_id: '15'
  parent_volume_id: D17-2
  pdf: https://www.aclweb.org/anthology/D17-2015.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-2015.jpg
  title: 'HeidelPlace: An Extensible Framework for Geoparsing'
  title_html: '<span class="acl-fixed-case">H</span>eidel<span class="acl-fixed-case">P</span>lace:
    An Extensible Framework for Geoparsing'
  url: https://www.aclweb.org/anthology/D17-2015
  year: '2017'
D17-2016:
  abstract: Interpretability of a predictive model is a powerful feature that gains
    the trust of users in the correctness of the predictions. In word sense disambiguation
    (WSD), knowledge-based systems tend to be much more interpretable than knowledge-free
    counterparts as they rely on the wealth of manually-encoded elements representing
    word senses, such as hypernyms, usage examples, and images. We present a WSD system
    that bridges the gap between these two so far disconnected groups of methods.
    Namely, our system, providing access to several state-of-the-art WSD models, aims
    to be interpretable as a knowledge-based system while it remains completely unsupervised
    and knowledge-free. The presented tool features a Web interface for all-word disambiguation
    of texts that makes the sense predictions human readable by providing interpretable
    word sense inventories, sense representations, and disambiguation results. We
    provide a public API, enabling seamless integration.
  address: Copenhagen, Denmark
  author:
  - first: Alexander
    full: Alexander Panchenko
    id: alexander-panchenko
    last: Panchenko
  - first: Fide
    full: Fide Marten
    id: fide-marten
    last: Marten
  - first: Eugen
    full: Eugen Ruppert
    id: eugen-ruppert
    last: Ruppert
  - first: Stefano
    full: Stefano Faralli
    id: stefano-faralli
    last: Faralli
  - first: Dmitry
    full: Dmitry Ustalov
    id: dmitry-ustalov
    last: Ustalov
  - first: Simone Paolo
    full: Simone Paolo Ponzetto
    id: simone-paolo-ponzetto
    last: Ponzetto
  - first: Chris
    full: Chris Biemann
    id: chris-biemann
    last: Biemann
  author_string: Alexander Panchenko, Fide Marten, Eugen Ruppert, Stefano Faralli,
    Dmitry Ustalov, Simone Paolo Ponzetto, Chris Biemann
  bibkey: panchenko-etal-2017-unsupervised-knowledge
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D17-2016
  month: September
  page_first: '91'
  page_last: '96'
  pages: "91\u201396"
  paper_id: '16'
  parent_volume_id: D17-2
  pdf: https://www.aclweb.org/anthology/D17-2016.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-2016.jpg
  title: Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation
  title_html: Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation
  url: https://www.aclweb.org/anthology/D17-2016
  year: '2017'
D17-2017:
  abstract: "Named-entity recognition (NER) aims at identifying entities of interest\
    \ in a text. Artificial neural networks (ANNs) have recently been shown to outperform\
    \ existing NER systems. However, ANNs remain challenging to use for non-expert\
    \ users. In this paper, we present NeuroNER, an easy-to-use named-entity recognition\
    \ tool based on ANNs. Users can annotate entities using a graphical web-based\
    \ user interface (BRAT): the annotations are then used to train an ANN, which\
    \ in turn predict entities\u2019 locations and categories in new texts. NeuroNER\
    \ makes this annotation-training-prediction flow smooth and accessible to anyone."
  address: Copenhagen, Denmark
  author:
  - first: Franck
    full: Franck Dernoncourt
    id: franck-dernoncourt
    last: Dernoncourt
  - first: Ji Young
    full: Ji Young Lee
    id: ji-young-lee
    last: Lee
  - first: Peter
    full: Peter Szolovits
    id: peter-szolovits
    last: Szolovits
  author_string: Franck Dernoncourt, Ji Young Lee, Peter Szolovits
  bibkey: dernoncourt-etal-2017-neuroner
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D17-2017
  month: September
  page_first: '97'
  page_last: '102'
  pages: "97\u2013102"
  paper_id: '17'
  parent_volume_id: D17-2
  pdf: https://www.aclweb.org/anthology/D17-2017.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-2017.jpg
  title: 'NeuroNER: an easy-to-use program for named-entity recognition based on neural
    networks'
  title_html: '<span class="acl-fixed-case">N</span>euro<span class="acl-fixed-case">NER</span>:
    an easy-to-use program for named-entity recognition based on neural networks'
  url: https://www.aclweb.org/anthology/D17-2017
  year: '2017'
D17-2018:
  abstract: In this demonstration we present SupWSD, a Java API for supervised Word
    Sense Disambiguation (WSD). This toolkit includes the implementation of a state-of-the-art
    supervised WSD system, together with a Natural Language Processing pipeline for
    preprocessing and feature extraction. Our aim is to provide an easy-to-use tool
    for the research community, designed to be modular, fast and scalable for training
    and testing on large datasets. The source code of SupWSD is available at http://github.com/SI3P/SupWSD.
  address: Copenhagen, Denmark
  author:
  - first: Simone
    full: Simone Papandrea
    id: simone-papandrea
    last: Papandrea
  - first: Alessandro
    full: Alessandro Raganato
    id: alessandro-raganato
    last: Raganato
  - first: Claudio
    full: Claudio Delli Bovi
    id: claudio-delli-bovi
    last: Delli Bovi
  author_string: Simone Papandrea, Alessandro Raganato, Claudio Delli Bovi
  bibkey: papandrea-etal-2017-supwsd
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D17-2018
  month: September
  page_first: '103'
  page_last: '108'
  pages: "103\u2013108"
  paper_id: '18'
  parent_volume_id: D17-2
  pdf: https://www.aclweb.org/anthology/D17-2018.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-2018.jpg
  title: 'SupWSD: A Flexible Toolkit for Supervised Word Sense Disambiguation'
  title_html: '<span class="acl-fixed-case">S</span>up<span class="acl-fixed-case">WSD</span>:
    A Flexible Toolkit for Supervised Word Sense Disambiguation'
  url: https://www.aclweb.org/anthology/D17-2018
  year: '2017'
D17-2019:
  abstract: We present a novel interactive summarization system that is based on abstractive
    summarization, derived from a recent consolidated knowledge representation for
    multiple texts. We incorporate a couple of interaction mechanisms, providing a
    bullet-style summary while allowing to attain the most important information first
    and interactively drill down to more specific details. A usability study of our
    implementation, for event news tweets, suggests the utility of our approach for
    text exploration.
  address: Copenhagen, Denmark
  author:
  - first: Ori
    full: Ori Shapira
    id: ori-shapira
    last: Shapira
  - first: Hadar
    full: Hadar Ronen
    id: hadar-ronen
    last: Ronen
  - first: Meni
    full: Meni Adler
    id: meni-adler
    last: Adler
  - first: Yael
    full: Yael Amsterdamer
    id: yael-amsterdamer
    last: Amsterdamer
  - first: Judit
    full: Judit Bar-Ilan
    id: judit-bar-ilan
    last: Bar-Ilan
  - first: Ido
    full: Ido Dagan
    id: ido-dagan
    last: Dagan
  author_string: Ori Shapira, Hadar Ronen, Meni Adler, Yael Amsterdamer, Judit Bar-Ilan,
    Ido Dagan
  bibkey: shapira-etal-2017-interactive
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D17-2019
  month: September
  page_first: '109'
  page_last: '114'
  pages: "109\u2013114"
  paper_id: '19'
  parent_volume_id: D17-2
  pdf: https://www.aclweb.org/anthology/D17-2019.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-2019.jpg
  title: Interactive Abstractive Summarization for Event News Tweets
  title_html: Interactive Abstractive Summarization for Event News Tweets
  url: https://www.aclweb.org/anthology/D17-2019
  year: '2017'
D17-2020:
  abstract: LangPro is an automated theorem prover for natural language. Given a set
    of premises and a hypothesis, it is able to prove semantic relations between them.
    The prover is based on a version of analytic tableau method specially designed
    for natural logic. The proof procedure operates on logical forms that preserve
    linguistic expressions to a large extent. %This property makes the logical forms
    easily obtainable from syntactic trees. %, in particular, Combinatory Categorial
    Grammar derivation trees. The nature of proofs is deductive and transparent. On
    the FraCaS and SICK textual entailment datasets, the prover achieves high results
    comparable to state-of-the-art.
  address: Copenhagen, Denmark
  author:
  - first: Lasha
    full: Lasha Abzianidze
    id: lasha-abzianidze
    last: Abzianidze
  author_string: Lasha Abzianidze
  bibkey: abzianidze-2017-langpro
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D17-2020
  month: September
  page_first: '115'
  page_last: '120'
  pages: "115\u2013120"
  paper_id: '20'
  parent_volume_id: D17-2
  pdf: https://www.aclweb.org/anthology/D17-2020.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-2020.jpg
  title: 'LangPro: Natural Language Theorem Prover'
  title_html: '<span class="acl-fixed-case">L</span>ang<span class="acl-fixed-case">P</span>ro:
    Natural Language Theorem Prover'
  url: https://www.aclweb.org/anthology/D17-2020
  year: '2017'
D17-2021:
  abstract: While neural machine translation (NMT) provides high-quality translation,
    it is still hard to interpret and analyze its behavior. We present an interactive
    interface for visualizing and intervening behavior of NMT, specifically concentrating
    on the behavior of beam search mechanism and attention component. The tool (1)
    visualizes search tree and attention and (2) provides interface to adjust search
    tree and attention weight (manually or automatically) at real-time. We show the
    tool gives various methods to understand NMT.
  address: Copenhagen, Denmark
  author:
  - first: Jaesong
    full: Jaesong Lee
    id: jaesong-lee
    last: Lee
  - first: Joong-Hwi
    full: Joong-Hwi Shin
    id: joong-hwi-shin
    last: Shin
  - first: Jun-Seok
    full: Jun-Seok Kim
    id: jun-seok-kim
    last: Kim
  author_string: Jaesong Lee, Joong-Hwi Shin, Jun-Seok Kim
  bibkey: lee-etal-2017-interactive
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D17-2021
  month: September
  page_first: '121'
  page_last: '126'
  pages: "121\u2013126"
  paper_id: '21'
  parent_volume_id: D17-2
  pdf: https://www.aclweb.org/anthology/D17-2021.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-2021.jpg
  title: Interactive Visualization and Manipulation of Attention-based Neural Machine
    Translation
  title_html: Interactive Visualization and Manipulation of Attention-based Neural
    Machine Translation
  url: https://www.aclweb.org/anthology/D17-2021
  year: '2017'
D17-3001:
  abstract: "Through subsumption and instantiation, individual instances (\u201Cartificial\
    \ intelligence\u201D, \u201Cthe spotted pig\u201D) otherwise spanning a wide range\
    \ of domains can be brought together and organized under conceptual hierarchies.\
    \ The hierarchies connect more specific concepts (\u201Ccomputer science subfields\u201D\
    , \u201Cgastropubs\u201D) to more general concepts (\u201Cacademic disciplines\u201D\
    , \u201Crestaurants\u201D) through IsA relations. Explicit or implicit properties\
    \ applicable to, and defining, more general concepts are inherited by their more\
    \ specific concepts, down to the instances connected to the lower parts of the\
    \ hierarchies. Subsumption represents a crisp, universally-applicable principle\
    \ towards consistently representing IsA relations in any knowledge resource. Yet\
    \ knowledge resources often exhibit significant differences in their scope, representation\
    \ choices and intended usage, to cause significant differences in their expected\
    \ usage and impact on various tasks. This tutorial examines the theoretical foundations\
    \ of subsumption, and its practical embodiment through IsA relations compiled\
    \ manually or extracted automatically. It addresses IsA relations from their formal\
    \ definition; through practical choices made in their representation within the\
    \ larger and more widely-used of the available knowledge resources; to their automatic\
    \ acquisition from document repositories, as opposed to their manual compilation\
    \ by human contributors; to their impact in text analysis and information retrieval.\
    \ As search engines move away from returning a set of links and closer to returning\
    \ results that more directly answer queries, IsA relations play an increasingly\
    \ important role towards a better understanding of documents and queries. The\
    \ tutorial teaches the audience about definitions, assumptions and practical choices\
    \ related to modeling and representing IsA relations in existing, human-compiled\
    \ resources of instances, concepts and resulting conceptual hierarchies; methods\
    \ for automatically extracting sets of instances within unlabeled or labeled concepts,\
    \ where the concepts may be considered as a flat set or organized hierarchically;\
    \ and applications of IsA relations in information retrieval."
  address: Copenhagen, Denmark
  author:
  - first: Marius
    full: Marius Pasca
    id: marius-pasca
    last: Pasca
  author_string: Marius Pasca
  bibkey: pasca-2017-acquisition
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: Tutorial Abstracts'
  month: September
  paper_id: '1'
  parent_volume_id: D17-3
  pdf: https://www.aclweb.org/anthology/D17-3001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-3001.jpg
  title: Acquisition, Representation and Usage of Conceptual Hierarchies
  title_html: Acquisition, Representation and Usage of Conceptual Hierarchies
  url: https://www.aclweb.org/anthology/D17-3001
  year: '2017'
D17-3002:
  abstract: "Sarcasm is a form of verbal irony that is intended to express contempt\
    \ or ridicule. Motivated by challenges posed by sarcastic text to sentiment analysis,\
    \ computational approaches to sarcasm have witnessed a growing interest at NLP\
    \ forums in the past decade. Computational sarcasm refers to automatic approaches\
    \ pertaining to sarcasm. The tutorial will provide a bird\u2019s-eye view of the\
    \ research in computational sarcasm for text, while focusing on significant milestones.The\
    \ tutorial begins with linguistic theories of sarcasm, with a focus on incongruity:\
    \ a useful notion that underlies sarcasm and other forms of figurative language.\
    \ Since the most significant work in computational sarcasm is sarcasm detection:\
    \ predicting whether a given piece of text is sarcastic or not, sarcasm detection\
    \ forms the focus hereafter. We begin our discussion on sarcasm detection with\
    \ datasets, touching on strategies, challenges and nature of datasets. Then, we\
    \ describe algorithms for sarcasm detection: rule-based (where a specific evidence\
    \ of sarcasm is utilised as a rule), statistical classifier-based (where features\
    \ are designed for a statistical classifier), a topic model-based technique, and\
    \ deep learning-based algorithms for sarcasm detection. In case of each of these\
    \ algorithms, we refer to our work on sarcasm detection and share our learnings.\
    \ Since information beyond the text to be classified, contextual information is\
    \ useful for sarcasm detection, we then describe approaches that use such information\
    \ through conversational context or author-specific context.We then follow it\
    \ by novel areas in computational sarcasm such as sarcasm generation, sarcasm\
    \ v/s irony classification, etc. We then summarise the tutorial and describe future\
    \ directions based on errors reported in past work. The tutorial will end with\
    \ a demonstration of our work on sarcasm detection.This tutorial will be of interest\
    \ to researchers investigating computational sarcasm and related areas such as\
    \ computational humour, figurative language understanding, emotion and sentiment\
    \ sentiment analysis, etc. The tutorial is motivated by our continually evolving\
    \ survey paper of sarcasm detection, that is available on arXiv at: Joshi, Aditya,\
    \ Pushpak Bhattacharyya, and Mark James Carman. \u201CAutomatic Sarcasm Detection:\
    \ A Survey.\u201D arXiv preprint arXiv:1602.03426 (2016)."
  address: Copenhagen, Denmark
  author:
  - first: Pushpak
    full: Pushpak Bhattacharyya
    id: pushpak-bhattacharyya
    last: Bhattacharyya
  - first: Aditya
    full: Aditya Joshi
    id: aditya-joshi
    last: Joshi
  author_string: Pushpak Bhattacharyya, Aditya Joshi
  bibkey: bhattacharyya-joshi-2017-computational
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: Tutorial Abstracts'
  month: September
  paper_id: '2'
  parent_volume_id: D17-3
  pdf: https://www.aclweb.org/anthology/D17-3002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-3002.jpg
  title: Computational Sarcasm
  title_html: Computational Sarcasm
  url: https://www.aclweb.org/anthology/D17-3002
  year: '2017'
D17-3003:
  abstract: 'Graphs or networks have been widely used as modeling tools in Natural
    Language Processing (NLP), Text Mining (TM) and Information Retrieval (IR). Traditionally,
    the unigram bag-of-words representation is applied; that way, a document is represented
    as a multiset of its terms, disregarding dependencies between the terms. Although
    several variants and extensions of this modeling approach have been proposed (e.g.,
    the n-gram model), the main weakness comes from the underlying term independence
    assumption. The order of the terms within a document is completely disregarded
    and any relationship between terms is not taken into account in the final task
    (e.g., text categorization). Nevertheless, as the heterogeneity of text collections
    is increasing (especially with respect to document length and vocabulary), the
    research community has started exploring different document representations aiming
    to capture more fine-grained contexts of co-occurrence between different terms,
    challenging the well-established unigram bag-of-words model. To this direction,
    graphs constitute a well-developed model that has been adopted for text representation.
    The goal of this tutorial is to offer a comprehensive presentation of recent methods
    that rely on graph-based text representations to deal with various tasks in NLP
    and IR. We will describe basic as well as novel graph theoretic concepts and we
    will examine how they can be applied in a wide range of text-related application
    domains.All the material associated to the tutorial will be available at: http://fragkiskosm.github.io/projects/graph_text_tutorial'
  address: Copenhagen, Denmark
  author:
  - first: Fragkiskos D.
    full: Fragkiskos D. Malliaros
    id: fragkiskos-d-malliaros
    last: Malliaros
  - first: Michalis
    full: Michalis Vazirgiannis
    id: michalis-vazirgiannis
    last: Vazirgiannis
  author_string: Fragkiskos D. Malliaros, Michalis Vazirgiannis
  bibkey: malliaros-vazirgiannis-2017-graph
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: Tutorial Abstracts'
  month: September
  paper_id: '3'
  parent_volume_id: D17-3
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-3003.jpg
  title: 'Graph-based Text Representations: Boosting Text Mining, NLP and Information
    Retrieval with Graphs'
  title_html: 'Graph-based Text Representations: Boosting Text Mining, <span class="acl-fixed-case">NLP</span>
    and Information Retrieval with Graphs'
  year: '2017'
D17-3004:
  abstract: This tutorial describes semantic role labelling (SRL), the task of mapping
    text to shallow semantic representations of eventualities and their participants.
    The tutorial introduces the SRL task and discusses recent research directions
    related to the task. The audience of this tutorial will learn about the linguistic
    background and motivation for semantic roles, and also about a range of computational
    models for this task, from early approaches to the current state-of-the-art. We
    will further discuss recently proposed variations to the traditional SRL task,
    including topics such as semantic proto-role labeling.We also cover techniques
    for reducing required annotation effort, such as methods exploiting unlabeled
    corpora (semi-supervised and unsupervised techniques), model adaptation across
    languages and domains, and methods for crowdsourcing semantic role annotation
    (e.g., question-answer driven SRL). Methods based on different machine learning
    paradigms, including neural networks, generative Bayesian models, graph-based
    algorithms and bootstrapping style techniques.Beyond sentence-level SRL, we discuss
    work that involves semantic roles in discourse. In particular, we cover data sets
    and models related to the task of identifying implicit roles and linking them
    to discourse antecedents. We introduce different approaches to this task from
    the literature, including models based on coreference resolution, centering, and
    selectional preferences. We also review how new insights gained through them can
    be useful for the traditional SRL task.
  address: Copenhagen, Denmark
  author:
  - first: Diego
    full: Diego Marcheggiani
    id: diego-marcheggiani
    last: Marcheggiani
  - first: Michael
    full: Michael Roth
    id: michael-roth
    last: Roth
  - first: Ivan
    full: Ivan Titov
    id: ivan-titov
    last: Titov
  - first: Benjamin
    full: Benjamin Van Durme
    id: benjamin-van-durme
    last: Van Durme
  author_string: Diego Marcheggiani, Michael Roth, Ivan Titov, Benjamin Van Durme
  bibkey: marcheggiani-etal-2017-semantic
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: Tutorial Abstracts'
  month: September
  paper_id: '4'
  parent_volume_id: D17-3
  pdf: https://www.aclweb.org/anthology/D17-3004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-3004.jpg
  title: Semantic Role Labeling
  title_html: Semantic Role Labeling
  url: https://www.aclweb.org/anthology/D17-3004
  year: '2017'
D17-3005:
  abstract: "Designing of general-purpose learning algorithms is a long-standing goal\
    \ of artificial intelligence. A general purpose AI agent should be able to have\
    \ a memory that it can store and retrieve information from. Despite the success\
    \ of deep learning in particular with the introduction of LSTMs and GRUs to this\
    \ area, there are still a set of complex tasks that can be challenging for conventional\
    \ neural networks. Those tasks often require a neural network to be equipped with\
    \ an explicit, external memory in which a larger, potentially unbounded, set of\
    \ facts need to be stored. They include but are not limited to, reasoning, planning,\
    \ episodic question-answering and learning compact algorithms. Recently two promising\
    \ approaches based on neural networks to this type of tasks have been proposed:\
    \ Memory Networks and Neural Turing Machines.In this tutorial, we will give an\
    \ overview of this new paradigm of \u201Cneural networks with memory\u201D. We\
    \ will present a unified architecture for Memory Augmented Neural Networks (MANN)\
    \ and discuss the ways in which one can address the external memory and hence\
    \ read/write from it. Then we will introduce Neural Turing Machines and Memory\
    \ Networks as specific instantiations of this general architecture. In the second\
    \ half of the tutorial, we will focus on recent advances in MANN which focus on\
    \ the following questions: How can we read/write from an extremely large memory\
    \ in a scalable way? How can we design efficient non-linear addressing schemes?\
    \ How can we do efficient reasoning using large scale memory and an episodic memory?\
    \ The answer to any one of these questions introduces a variant of MANN. We will\
    \ conclude the tutorial with several open challenges in MANN and its applications\
    \ to NLP.We will introduce several applications of MANN in NLP throughout the\
    \ tutorial. Few examples include language modeling, question answering, visual\
    \ question answering, and dialogue systems.For updated information and material,\
    \ please refer to our tutorial website: https://sites.google.com/view/mann-emnlp2017/."
  address: Copenhagen, Denmark
  author:
  - first: Caglar
    full: Caglar Gulcehre
    id: caglar-gulcehre
    last: Gulcehre
  - first: Sarath
    full: Sarath Chandar
    id: sarath-chandar
    last: Chandar
  author_string: Caglar Gulcehre, Sarath Chandar
  bibkey: gulcehre-chandar-2017-memory
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: Tutorial Abstracts'
  month: September
  paper_id: '5'
  parent_volume_id: D17-3
  pdf: https://www.aclweb.org/anthology/D17-3005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-3005.jpg
  title: Memory Augmented Neural Networks for Natural Language Processing
  title_html: Memory Augmented Neural Networks for Natural Language Processing
  url: https://www.aclweb.org/anthology/D17-3005
  year: '2017'
D17-3006:
  abstract: "Structured prediction is one of the most important topics in various\
    \ fields, including machine learning, computer vision, natural language processing\
    \ (NLP) and bioinformatics. In this tutorial, we present a novel framework that\
    \ unifies various structured prediction models.The hidden Markov model (HMM) and\
    \ the probabilistic context-free grammars (PCFGs) are two classic generative models\
    \ used for predicting outputs with linear-chain and tree structures, respectively.\
    \ As HMM\u2019s discriminative counterpart, the linear-chain conditional random\
    \ fields (CRFs) (Lafferty et al., 2001) model was later proposed. Such a model\
    \ was shown to yield good performance on standard NLP tasks such as information\
    \ extraction. Several extensions to such a model were then proposed afterward,\
    \ including the semi-Markov CRFs (Sarawagi and Cohen, 2004), tree CRFs (Cohn and\
    \ Blunsom, 2005), as well as discriminative parsing models and their latent variable\
    \ variants (Petrov and Klein, 2007). On the other hand, utilizing a slightly different\
    \ loss function, one could arrive at the structured support vector machines (Tsochantaridis\
    \ et al., 2004) and its latent variable variant (Yu and Joachims, 2009) as well.\
    \ Furthermore, new models that integrate neural networks and graphical models,\
    \ such as neural CRFs (Do et al., 2010) were also proposed.In this tutorial, we\
    \ will be discussing how such a wide spectrum of existing structured prediction\
    \ models can all be implemented under a unified framework (available at here)\
    \ that involves some basic building blocks. Based on such a framework, we show\
    \ how some seemingly complicated structured prediction models such as a semantic\
    \ parsing model (Lu et al., 2008; Lu, 2014) can be implemented conveniently and\
    \ quickly. Furthermore, we also show that the framework can be used to solve certain\
    \ structured prediction problems that otherwise cannot be easily handled by conventional\
    \ structured prediction models. Specifically, we show how to use such a framework\
    \ to construct models that are capable of predicting non-conventional structures,\
    \ such as overlapping structures (Lu and Roth, 2015; Muis and Lu, 2016a). We will\
    \ also discuss how to make use of the framework to build other related models\
    \ such as topic models and highlight its potential applications in some recent\
    \ popular tasks (e.g., AMR parsing (Flanigan et al., 2014)).The framework has\
    \ been extensively used by our research group for developing various structured\
    \ prediction models, including models for information extraction (Lu and Roth,\
    \ 2015; Muis and Lu, 2016a; Jie et al., 2017), noun phrase chunking (Muis and\
    \ Lu, 2016b), semantic parsing (Lu, 2015; Susanto and Lu, 2017), and sentiment\
    \ analysis (Li and Lu, 2017). It is our hope that this tutorial will be helpful\
    \ for many natural language processing researchers who are interested in designing\
    \ their own structured prediction models rapidly. We also hope this tutorial allows\
    \ researchers to strengthen their understandings on the connections between various\
    \ structured prediction models, and that the open release of the framework will\
    \ bring value to the NLP research community and enhance its overall productivity.The\
    \ material associated with this tutorial will be available at the tutorial web\
    \ site: https://web.archive.org/web/20180427113151/http://statnlp.org/tutorials/."
  address: Copenhagen, Denmark
  author:
  - first: Wei
    full: Wei Lu
    id: wei-lu
    last: Lu
  author_string: Wei Lu
  bibkey: lu-2017-unified
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: Tutorial Abstracts'
  month: September
  paper_id: '6'
  parent_volume_id: D17-3
  pdf: https://www.aclweb.org/anthology/D17-3006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-3006.jpg
  title: 'A Unified Framework for Structured Prediction: From Theory to Practice'
  title_html: 'A Unified Framework for Structured Prediction: From Theory to Practice'
  url: https://www.aclweb.org/anthology/D17-3006
  year: '2017'
D17-3007:
  abstract: In recent past, NLP as a field has seen tremendous utility of distributional
    word vector representations as features in downstream tasks. The fact that these
    word vectors can be trained on unlabeled monolingual corpora of a language makes
    them an inexpensive resource in NLP. With the increasing use of monolingual word
    vectors, there is a need for word vectors that can be used as efficiently across
    multiple languages as monolingually. Therefore, learning bilingual and multilingual
    word embeddings/vectors is currently an important research topic. These vectors
    offer an elegant and language-pair independent way to represent content across
    different languages.This tutorial aims to bring NLP researchers up to speed with
    the current techniques in cross-lingual word representation learning. We will
    first discuss how to induce cross-lingual word representations (covering both
    bilingual and multilingual ones) from various data types and resources (e.g.,
    parallel data, comparable data, non-aligned monolingual data in different languages,
    dictionaries and theasuri, or, even, images, eye-tracking data). We will then
    discuss how to evaluate such representations, intrinsically and extrinsically.
    We will introduce researchers to state-of-the-art methods for constructing cross-lingual
    word representations and discuss their applicability in a broad range of downstream
    NLP applications.We will deliver a detailed survey of the current methods, discuss
    best training and evaluation practices and use-cases, and provide links to publicly
    available implementations, datasets, and pre-trained models.
  address: Copenhagen, Denmark
  author:
  - first: Manaal
    full: Manaal Faruqui
    id: manaal-faruqui
    last: Faruqui
  - first: Anders
    full: "Anders S\xF8gaard"
    id: anders-sogaard
    last: "S\xF8gaard"
  - first: Ivan
    full: "Ivan Vuli\u0107"
    id: ivan-vulic
    last: "Vuli\u0107"
  author_string: "Manaal Faruqui, Anders S\xF8gaard, Ivan Vuli\u0107"
  bibkey: faruqui-etal-2017-cross
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing: Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing: Tutorial Abstracts'
  month: September
  paper_id: '7'
  parent_volume_id: D17-3
  pdf: https://www.aclweb.org/anthology/D17-3007.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D17-3007.jpg
  title: 'Cross-Lingual Word Representations: Induction and Evaluation'
  title_html: 'Cross-Lingual Word Representations: Induction and Evaluation'
  url: https://www.aclweb.org/anthology/D17-3007
  year: '2017'
