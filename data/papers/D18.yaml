D18-1000:
  address: Brussels, Belgium
  author:
  - first: Ellen
    full: Ellen Riloff
    id: ellen-riloff
    last: Riloff
  - first: David
    full: David Chiang
    id: david-chiang
    last: Chiang
  - first: Julia
    full: Julia Hockenmaier
    id: julia-hockenmaier
    last: Hockenmaier
  - first: "Jun\u2019ichi"
    full: "Jun\u2019ichi Tsujii"
    id: junichi-tsujii
    last: Tsujii
  author_string: "Ellen Riloff, David Chiang, Julia Hockenmaier, Jun\u2019ichi Tsujii"
  bibkey: emnlp-2018-2018
  bibtype: proceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  month: October-November
  paper_id: '0'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1000.jpg
  title: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  title_html: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  url: https://www.aclweb.org/anthology/D18-1000
  year: '2018'
D18-1001:
  abstract: "This article deals with adversarial attacks towards deep learning systems\
    \ for Natural Language Processing (NLP), in the context of privacy protection.\
    \ We study a specific type of attack: an attacker eavesdrops on the hidden representations\
    \ of a neural text classifier and tries to recover information about the input\
    \ text. Such scenario may arise in situations when the computation of a neural\
    \ network is shared across multiple devices, e.g. some hidden representation is\
    \ computed by a user\u2019s device and sent to a cloud-based model. We measure\
    \ the privacy of a hidden representation by the ability of an attacker to predict\
    \ accurately specific private information from it and characterize the tradeoff\
    \ between the privacy and the utility of neural representations. Finally, we propose\
    \ several defense methods based on modified training objectives and show that\
    \ they improve the privacy of neural representations."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305202770
    type: video
    url: https://vimeo.com/305202770
  author:
  - first: Maximin
    full: Maximin Coavoux
    id: maximin-coavoux
    last: Coavoux
  - first: Shashi
    full: Shashi Narayan
    id: shashi-narayan
    last: Narayan
  - first: Shay B.
    full: Shay B. Cohen
    id: shay-b-cohen
    last: Cohen
  author_string: Maximin Coavoux, Shashi Narayan, Shay B. Cohen
  bibkey: coavoux-etal-2018-privacy
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1001
  month: October-November
  page_first: '1'
  page_last: '10'
  pages: "1\u201310"
  paper_id: '1'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1001.jpg
  title: Privacy-preserving Neural Representations of Text
  title_html: Privacy-preserving Neural Representations of Text
  url: https://www.aclweb.org/anthology/D18-1001
  year: '2018'
D18-1002:
  abstract: "Recent advances in Representation Learning and Adversarial Training seem\
    \ to succeed in removing unwanted features from the learned representation. We\
    \ show that demographic information of authors is encoded in\u2014and can be recovered\
    \ from\u2014the intermediate representations learned by text-based neural classifiers.\
    \ The implication is that decisions of classifiers trained on textual data are\
    \ not agnostic to\u2014and likely condition on\u2014demographic attributes. When\
    \ attempting to remove such demographic information using adversarial training,\
    \ we find that while the adversarial component achieves chance-level development-set\
    \ accuracy during training, a post-hoc classifier, trained on the encoded sentences\
    \ from the first part, still manages to reach substantially higher classification\
    \ accuracies on the same data. This behavior is consistent across several tasks,\
    \ demographic properties and datasets. We explore several techniques to improve\
    \ the effectiveness of the adversarial component. Our main conclusion is a cautionary\
    \ one: do not rely on the adversarial training to achieve invariant representation\
    \ to sensitive features."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1002.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1002.Attachment.pdf
  - filename: https://vimeo.com/305203150
    type: video
    url: https://vimeo.com/305203150
  author:
  - first: Yanai
    full: Yanai Elazar
    id: yanai-elazar
    last: Elazar
  - first: Yoav
    full: Yoav Goldberg
    id: yoav-goldberg
    last: Goldberg
  author_string: Yanai Elazar, Yoav Goldberg
  bibkey: elazar-goldberg-2018-adversarial
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1002
  month: October-November
  page_first: '11'
  page_last: '21'
  pages: "11\u201321"
  paper_id: '2'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1002.jpg
  title: Adversarial Removal of Demographic Attributes from Text Data
  title_html: Adversarial Removal of Demographic Attributes from Text Data
  url: https://www.aclweb.org/anthology/D18-1002
  year: '2018'
D18-1003:
  abstract: Misinformation such as fake news is one of the big challenges of our society.
    Research on automated fact-checking has proposed methods based on supervised learning,
    but these approaches do not consider external evidence apart from labeled training
    instances. Recent approaches counter this deficit by considering external sources
    related to a claim. However, these methods require substantial feature modeling
    and rich lexicons. This paper overcomes these limitations of prior work with an
    end-to-end model for evidence-aware credibility assessment of arbitrary textual
    claims, without any human intervention. It presents a neural network model that
    judiciously aggregates signals from external evidence articles, the language of
    these articles and the trustworthiness of their sources. It also derives informative
    features for generating user-comprehensible explanations that makes the neural
    network predictions transparent to the end-user. Experiments with four datasets
    and ablation studies show the strength of our method.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305203523
    type: video
    url: https://vimeo.com/305203523
  author:
  - first: Kashyap
    full: Kashyap Popat
    id: kashyap-popat
    last: Popat
  - first: Subhabrata
    full: Subhabrata Mukherjee
    id: subhabrata-mukherjee
    last: Mukherjee
  - first: Andrew
    full: Andrew Yates
    id: andrew-yates
    last: Yates
  - first: Gerhard
    full: Gerhard Weikum
    id: gerhard-weikum
    last: Weikum
  author_string: Kashyap Popat, Subhabrata Mukherjee, Andrew Yates, Gerhard Weikum
  bibkey: popat-etal-2018-declare
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1003
  month: October-November
  page_first: '22'
  page_last: '32'
  pages: "22\u201332"
  paper_id: '3'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1003.jpg
  title: 'DeClarE: Debunking Fake News and False Claims using Evidence-Aware Deep
    Learning'
  title_html: '<span class="acl-fixed-case">D</span>e<span class="acl-fixed-case">C</span>lar<span
    class="acl-fixed-case">E</span>: Debunking Fake News and False Claims using Evidence-Aware
    Deep Learning'
  url: https://www.aclweb.org/anthology/D18-1003
  year: '2018'
D18-1004:
  abstract: "People use online platforms to seek out support for their informational\
    \ and emotional needs. Here, we ask what effect does revealing one\u2019s gender\
    \ have on receiving support. To answer this, we create (i) a new dataset and method\
    \ for identifying supportive replies and (ii) new methods for inferring gender\
    \ from text and name. We apply these methods to create a new massive corpus of\
    \ 102M online interactions with gender-labeled users, each rated by degree of\
    \ supportiveness. Our analysis shows wide-spread and consistent disparity in support:\
    \ identifying as a woman is associated with higher rates of support - but also\
    \ higher rates of disparagement."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1004.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1004.Attachment.pdf
  - filename: https://vimeo.com/305203914
    type: video
    url: https://vimeo.com/305203914
  author:
  - first: Zijian
    full: Zijian Wang
    id: zijian-wang
    last: Wang
  - first: David
    full: David Jurgens
    id: david-jurgens
    last: Jurgens
  author_string: Zijian Wang, David Jurgens
  bibkey: wang-jurgens-2018-going
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1004
  month: October-November
  page_first: '33'
  page_last: '45'
  pages: "33\u201345"
  paper_id: '4'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1004.jpg
  title: "It\u2019s going to be okay: Measuring Access to Support in Online Communities"
  title_html: "It\u2019s going to be okay: Measuring Access to Support in Online Communities"
  url: https://www.aclweb.org/anthology/D18-1004
  year: '2018'
D18-1005:
  abstract: "Gang-involved youth in cities such as Chicago have increasingly turned\
    \ to social media to post about their experiences and intents online. In some\
    \ situations, when they experience the loss of a loved one, their online expression\
    \ of emotion may evolve into aggression towards rival gangs and ultimately into\
    \ real-world violence. In this paper, we present a novel system for detecting\
    \ Aggression and Loss in social media. Our system features the use of domain-specific\
    \ resources automatically derived from a large unlabeled corpus, and contextual\
    \ representations of the emotional and semantic content of the user\u2019s recent\
    \ tweets as well as their interactions with other users. Incorporating context\
    \ in our Convolutional Neural Network (CNN) leads to a significant improvement."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1005.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1005.Attachment.zip
  - filename: https://vimeo.com/305204297
    type: video
    url: https://vimeo.com/305204297
  author:
  - first: Serina
    full: Serina Chang
    id: serina-chang
    last: Chang
  - first: Ruiqi
    full: Ruiqi Zhong
    id: ruiqi-zhong
    last: Zhong
  - first: Ethan
    full: Ethan Adams
    id: ethan-adams
    last: Adams
  - first: Fei-Tzin
    full: Fei-Tzin Lee
    id: fei-tzin-lee
    last: Lee
  - first: Siddharth
    full: Siddharth Varia
    id: siddharth-varia
    last: Varia
  - first: Desmond
    full: Desmond Patton
    id: desmond-patton
    last: Patton
  - first: William
    full: William Frey
    id: william-frey
    last: Frey
  - first: Chris
    full: Chris Kedzie
    id: chris-kedzie
    last: Kedzie
  - first: Kathy
    full: Kathy McKeown
    id: kathleen-mckeown
    last: McKeown
  author_string: Serina Chang, Ruiqi Zhong, Ethan Adams, Fei-Tzin Lee, Siddharth Varia,
    Desmond Patton, William Frey, Chris Kedzie, Kathy McKeown
  bibkey: chang-etal-2018-detecting
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1005
  month: October-November
  page_first: '46'
  page_last: '56'
  pages: "46\u201356"
  paper_id: '5'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1005.jpg
  title: Detecting Gang-Involved Escalation on Social Media Using Context
  title_html: Detecting Gang-Involved Escalation on Social Media Using Context
  url: https://www.aclweb.org/anthology/D18-1005
  year: '2018'
D18-1006:
  abstract: 'Comprehending procedural text, e.g., a paragraph describing photosynthesis,
    requires modeling actions and the state changes they produce, so that questions
    about entities at different timepoints can be answered. Although several recent
    systems have shown impressive progress in this task, their predictions can be
    globally inconsistent or highly improbable. In this paper, we show how the predicted
    effects of actions in the context of a paragraph can be improved in two ways:
    (1) by incorporating global, commonsense constraints (e.g., a non-existent entity
    cannot be destroyed), and (2) by biasing reading with preferences from large-scale
    corpora (e.g., trees rarely move). Unlike earlier methods, we treat the problem
    as a neural structured prediction task, allowing hard and soft constraints to
    steer the model away from unlikely predictions. We show that the new model significantly
    outperforms earlier systems on a benchmark dataset for procedural text comprehension
    (+8% relative gain), and that it also avoids some of the nonsensical predictions
    that earlier systems make.'
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305193585
    type: video
    url: https://vimeo.com/305193585
  author:
  - first: Niket
    full: Niket Tandon
    id: niket-tandon
    last: Tandon
  - first: Bhavana
    full: Bhavana Dalvi
    id: bhavana-dalvi
    last: Dalvi
  - first: Joel
    full: Joel Grus
    id: joel-grus
    last: Grus
  - first: Wen-tau
    full: Wen-tau Yih
    id: wen-tau-yih
    last: Yih
  - first: Antoine
    full: Antoine Bosselut
    id: antoine-bosselut
    last: Bosselut
  - first: Peter
    full: Peter Clark
    id: peter-clark
    last: Clark
  author_string: Niket Tandon, Bhavana Dalvi, Joel Grus, Wen-tau Yih, Antoine Bosselut,
    Peter Clark
  bibkey: tandon-etal-2018-reasoning
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1006
  month: October-November
  page_first: '57'
  page_last: '66'
  pages: "57\u201366"
  paper_id: '6'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1006.jpg
  title: Reasoning about Actions and State Changes by Injecting Commonsense Knowledge
  title_html: Reasoning about Actions and State Changes by Injecting Commonsense Knowledge
  url: https://www.aclweb.org/anthology/D18-1006
  year: '2018'
D18-1007:
  abstract: 'We present a large-scale collection of diverse natural language inference
    (NLI) datasets that help provide insight into how well a sentence representation
    captures distinct types of reasoning. The collection results from recasting 13
    existing datasets from 7 semantic phenomena into a common NLI structure, resulting
    in over half a million labeled context-hypothesis pairs in total. We refer to
    our collection as the DNC: Diverse Natural Language Inference Collection. The
    DNC is available online at https://www.decomp.net, and will grow over time as
    additional resources are recast and added from novel sources.'
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305194062
    type: video
    url: https://vimeo.com/305194062
  author:
  - first: Adam
    full: Adam Poliak
    id: adam-poliak
    last: Poliak
  - first: Aparajita
    full: Aparajita Haldar
    id: aparajita-haldar
    last: Haldar
  - first: Rachel
    full: Rachel Rudinger
    id: rachel-rudinger
    last: Rudinger
  - first: J. Edward
    full: J. Edward Hu
    id: j-edward-hu
    last: Hu
  - first: Ellie
    full: Ellie Pavlick
    id: ellie-pavlick
    last: Pavlick
  - first: Aaron Steven
    full: Aaron Steven White
    id: aaron-steven-white
    last: White
  - first: Benjamin
    full: Benjamin Van Durme
    id: benjamin-van-durme
    last: Van Durme
  author_string: Adam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie
    Pavlick, Aaron Steven White, Benjamin Van Durme
  bibkey: poliak-etal-2018-collecting
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1007
  month: October-November
  page_first: '67'
  page_last: '81'
  pages: "67\u201381"
  paper_id: '7'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1007.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1007.jpg
  title: Collecting Diverse Natural Language Inference Problems for Sentence Representation
    Evaluation
  title_html: Collecting Diverse Natural Language Inference Problems for Sentence
    Representation Evaluation
  url: https://www.aclweb.org/anthology/D18-1007
  year: '2018'
D18-1008:
  abstract: "To understand a sentence like \u201Cwhereas only 10% of White Americans\
    \ live at or below the poverty line, 28% of African Americans do\u201D it is important\
    \ not only to identify individual facts, e.g., poverty rates of distinct demographic\
    \ groups, but also the higher-order relations between them, e.g., the disparity\
    \ between them. In this paper, we propose the task of Textual Analogy Parsing\
    \ (TAP) to model this higher-order meaning. Given a sentence such as the one above,\
    \ TAP outputs a frame-style meaning representation which explicitly specifies\
    \ what is shared (e.g., poverty rates) and what is compared (e.g., White Americans\
    \ vs. African Americans, 10% vs. 28%) between its component facts. Such a meaning\
    \ representation can enable new applications that rely on discourse understanding\
    \ such as automated chart generation from quantitative text. We present a new\
    \ dataset for TAP, baselines, and a model that successfully uses an ILP to enforce\
    \ the structural constraints of the problem."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305194870
    type: video
    url: https://vimeo.com/305194870
  author:
  - first: Matthew
    full: Matthew Lamm
    id: matthew-lamm
    last: Lamm
  - first: Arun
    full: Arun Chaganty
    id: arun-chaganty
    last: Chaganty
  - first: Christopher D.
    full: Christopher D. Manning
    id: christopher-d-manning
    last: Manning
  - first: Dan
    full: Dan Jurafsky
    id: dan-jurafsky
    last: Jurafsky
  - first: Percy
    full: Percy Liang
    id: percy-liang
    last: Liang
  author_string: Matthew Lamm, Arun Chaganty, Christopher D. Manning, Dan Jurafsky,
    Percy Liang
  bibkey: lamm-etal-2018-textual
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1008
  month: October-November
  page_first: '82'
  page_last: '92'
  pages: "82\u201392"
  paper_id: '8'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1008.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1008.jpg
  title: "Textual Analogy Parsing: What\u2019s Shared and What\u2019s Compared among\
    \ Analogous Facts"
  title_html: "Textual Analogy Parsing: What\u2019s Shared and What\u2019s Compared\
    \ among Analogous Facts"
  url: https://www.aclweb.org/anthology/D18-1008
  year: '2018'
D18-1009:
  abstract: "Given a partial description like \u201Cshe opened the hood of the car,\u201D\
    \ humans can reason about the situation and anticipate what might come next (\u201D\
    then, she examined the engine\u201D). In this paper, we introduce the task of\
    \ grounded commonsense inference, unifying natural language inference and commonsense\
    \ reasoning. We present SWAG, a new dataset with 113k multiple choice questions\
    \ about a rich spectrum of grounded situations. To address the recurring challenges\
    \ of the annotation artifacts and human biases found in many existing datasets,\
    \ we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased\
    \ dataset by iteratively training an ensemble of stylistic classifiers, and using\
    \ them to filter the data. To account for the aggressive adversarial filtering,\
    \ we use state-of-the-art language models to massively oversample a diverse set\
    \ of potential counterfactuals. Empirical results demonstrate that while humans\
    \ can solve the resulting inference problems with high accuracy (88%), various\
    \ competitive models struggle on our task. We provide comprehensive analysis that\
    \ indicates significant opportunities for future research."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1009.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1009.Attachment.zip
  - filename: https://vimeo.com/305195438
    type: video
    url: https://vimeo.com/305195438
  author:
  - first: Rowan
    full: Rowan Zellers
    id: rowan-zellers
    last: Zellers
  - first: Yonatan
    full: Yonatan Bisk
    id: yonatan-bisk
    last: Bisk
  - first: Roy
    full: Roy Schwartz
    id: roy-schwartz
    last: Schwartz
  - first: Yejin
    full: Yejin Choi
    id: yejin-choi
    last: Choi
  author_string: Rowan Zellers, Yonatan Bisk, Roy Schwartz, Yejin Choi
  bibkey: zellers-etal-2018-swag
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1009
  month: October-November
  page_first: '93'
  page_last: '104'
  pages: "93\u2013104"
  paper_id: '9'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1009.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1009.jpg
  title: 'SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference'
  title_html: '<span class="acl-fixed-case">SWAG</span>: A Large-Scale Adversarial
    Dataset for Grounded Commonsense Inference'
  url: https://www.aclweb.org/anthology/D18-1009
  year: '2018'
D18-1010:
  abstract: 'Determining whether a given claim is supported by evidence is a fundamental
    NLP problem that is best modeled as Textual Entailment. However, given a large
    collection of text, finding evidence that could support or refute a given claim
    is a challenge in itself, amplified by the fact that different evidence might
    be needed to support or refute a claim. Nevertheless, most prior work decouples
    evidence finding from determining the truth value of the claim given the evidence.
    We propose to consider these two aspects jointly. We develop TwoWingOS (two-wing
    optimization strategy), a system that, while identifying appropriate evidence
    for a claim, also determines whether or not the claim is supported by the evidence.
    Given the claim, TwoWingOS attempts to identify a subset of the evidence candidates;
    given the predicted evidence, it then attempts to determine the truth value of
    the corresponding claim entailment problem. We treat this problem as coupled optimization
    problems, training a joint model for it. TwoWingOS offers two advantages: (i)
    Unlike pipeline systems it facilitates flexible-size evidence set, and (ii) Joint
    training improves both the claim entailment and the evidence identification. Experiments
    on a benchmark dataset show state-of-the-art performance.'
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305195990
    type: video
    url: https://vimeo.com/305195990
  author:
  - first: Wenpeng
    full: Wenpeng Yin
    id: wenpeng-yin
    last: Yin
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Wenpeng Yin, Dan Roth
  bibkey: yin-roth-2018-twowingos
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1010
  month: October-November
  page_first: '105'
  page_last: '114'
  pages: "105\u2013114"
  paper_id: '10'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1010.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1010.jpg
  title: 'TwoWingOS: A Two-Wing Optimization Strategy for Evidential Claim Verification'
  title_html: '<span class="acl-fixed-case">T</span>wo<span class="acl-fixed-case">W</span>ing<span
    class="acl-fixed-case">OS</span>: A Two-Wing Optimization Strategy for Evidential
    Claim Verification'
  url: https://www.aclweb.org/anthology/D18-1010
  year: '2018'
D18-1011:
  abstract: In this paper we address the problem of learning multimodal word representations
    by integrating textual, visual and auditory inputs. Inspired by the re-constructive
    and associative nature of human memory, we propose a novel associative multichannel
    autoencoder (AMA). Our model first learns the associations between textual and
    perceptual modalities, so as to predict the missing perceptual information of
    concepts. Then the textual and predicted perceptual representations are fused
    through reconstructing their original and associated embeddings. Using a gating
    mechanism our model assigns different weights to each modality according to the
    different concepts. Results on six benchmark concept similarity tests show that
    the proposed method significantly outperforms strong unimodal baselines and state-of-the-art
    multimodal models.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305209813
    type: video
    url: https://vimeo.com/305209813
  author:
  - first: Shaonan
    full: Shaonan Wang
    id: shaonan-wang
    last: Wang
  - first: Jiajun
    full: Jiajun Zhang
    id: jiajun-zhang
    last: Zhang
  - first: Chengqing
    full: Chengqing Zong
    id: chengqing-zong
    last: Zong
  author_string: Shaonan Wang, Jiajun Zhang, Chengqing Zong
  bibkey: wang-etal-2018-associative
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1011
  month: October-November
  page_first: '115'
  page_last: '124'
  pages: "115\u2013124"
  paper_id: '11'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1011.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1011.jpg
  title: Associative Multichannel Autoencoder for Multimodal Word Representation
  title_html: Associative Multichannel Autoencoder for Multimodal Word Representation
  url: https://www.aclweb.org/anthology/D18-1011
  year: '2018'
D18-1012:
  abstract: Current dialogue systems focus more on textual and speech context knowledge
    and are usually based on two speakers. Some recent work has investigated static
    image-based dialogue. However, several real-world human interactions also involve
    dynamic visual context (similar to videos) as well as dialogue exchanges among
    multiple speakers. To move closer towards such multimodal conversational skills
    and visually-situated applications, we introduce a new video-context, many-speaker
    dialogue dataset based on live-broadcast soccer game videos and chats from Twitch.tv.
    This challenging testbed allows us to develop visually-grounded dialogue models
    that should generate relevant temporal and spatial event language from the live
    video, while also being relevant to the chat history. For strong baselines, we
    also present several discriminative and generative models, e.g., based on tridirectional
    attention flow (TriDAF). We evaluate these models via retrieval ranking-recall,
    automatic phrase-matching metrics, as well as human evaluation studies. We also
    present dataset analyses, model ablations, and visualizations to understand the
    contribution of different modalities and model components.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1012.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1012.Attachment.pdf
  - filename: https://vimeo.com/305210347
    type: video
    url: https://vimeo.com/305210347
  author:
  - first: Ramakanth
    full: Ramakanth Pasunuru
    id: ramakanth-pasunuru
    last: Pasunuru
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  author_string: Ramakanth Pasunuru, Mohit Bansal
  bibkey: pasunuru-bansal-2018-game
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1012
  month: October-November
  page_first: '125'
  page_last: '136'
  pages: "125\u2013136"
  paper_id: '12'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1012.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1012.jpg
  title: Game-Based Video-Context Dialogue
  title_html: Game-Based Video-Context Dialogue
  url: https://www.aclweb.org/anthology/D18-1012
  year: '2018'
D18-1013:
  abstract: The encode-decoder framework has shown recent success in image captioning.
    Visual attention, which is good at detailedness, and semantic attention, which
    is good at comprehensiveness, have been separately proposed to ground the caption
    on the image. In this paper, we propose the Stepwise Image-Topic Merging Network
    (simNet) that makes use of the two kinds of attention at the same time. At each
    time step when generating the caption, the decoder adaptively merges the attentive
    information in the extracted topics and the image according to the generated context,
    so that the visual information and the semantic information can be effectively
    combined. The proposed approach is evaluated on two benchmark datasets and reaches
    the state-of-the-art performances.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305210529
    type: video
    url: https://vimeo.com/305210529
  author:
  - first: Fenglin
    full: Fenglin Liu
    id: fenglin-liu
    last: Liu
  - first: Xuancheng
    full: Xuancheng Ren
    id: xuancheng-ren
    last: Ren
  - first: Yuanxin
    full: Yuanxin Liu
    id: yuanxin-liu
    last: Liu
  - first: Houfeng
    full: Houfeng Wang
    id: houfeng-wang
    last: Wang
  - first: Xu
    full: Xu Sun
    id: xu-sun
    last: Sun
  author_string: Fenglin Liu, Xuancheng Ren, Yuanxin Liu, Houfeng Wang, Xu Sun
  bibkey: liu-etal-2018-simnet
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1013
  month: October-November
  page_first: '137'
  page_last: '149'
  pages: "137\u2013149"
  paper_id: '13'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1013.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1013.jpg
  title: 'simNet: Stepwise Image-Topic Merging Network for Generating Detailed and
    Comprehensive Image Captions'
  title_html: 'sim<span class="acl-fixed-case">N</span>et: Stepwise Image-Topic Merging
    Network for Generating Detailed and Comprehensive Image Captions'
  url: https://www.aclweb.org/anthology/D18-1013
  year: '2018'
D18-1014:
  abstract: Computational modeling of human multimodal language is an emerging research
    area in natural language processing spanning the language, visual and acoustic
    modalities. Comprehending multimodal language requires modeling not only the interactions
    within each modality (intra-modal interactions) but more importantly the interactions
    between modalities (cross-modal interactions). In this paper, we propose the Recurrent
    Multistage Fusion Network (RMFN) which decomposes the fusion problem into multiple
    stages, each of them focused on a subset of multimodal signals for specialized,
    effective fusion. Cross-modal interactions are modeled using this multistage fusion
    approach which builds upon intermediate representations of previous stages. Temporal
    and intra-modal interactions are modeled by integrating our proposed fusion approach
    with a system of recurrent neural networks. The RMFN displays state-of-the-art
    performance in modeling human multimodal language across three public datasets
    relating to multimodal sentiment analysis, emotion recognition, and speaker traits
    recognition. We provide visualizations to show that each stage of fusion focuses
    on a different subset of multimodal signals, learning increasingly discriminative
    multimodal representations.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1014.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1014.Attachment.zip
  - filename: https://vimeo.com/305210831
    type: video
    url: https://vimeo.com/305210831
  author:
  - first: Paul Pu
    full: Paul Pu Liang
    id: paul-pu-liang
    last: Liang
  - first: Ziyin
    full: Ziyin Liu
    id: ziyin-liu
    last: Liu
  - first: AmirAli
    full: AmirAli Bagher Zadeh
    id: amirali-bagher-zadeh
    last: Bagher Zadeh
  - first: Louis-Philippe
    full: Louis-Philippe Morency
    id: louis-philippe-morency
    last: Morency
  author_string: Paul Pu Liang, Ziyin Liu, AmirAli Bagher Zadeh, Louis-Philippe Morency
  bibkey: liang-etal-2018-multimodal
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1014
  month: October-November
  page_first: '150'
  page_last: '161'
  pages: "150\u2013161"
  paper_id: '14'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1014.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1014.jpg
  title: Multimodal Language Analysis with Recurrent Multistage Fusion
  title_html: Multimodal Language Analysis with Recurrent Multistage Fusion
  url: https://www.aclweb.org/anthology/D18-1014
  year: '2018'
D18-1015:
  abstract: We introduce an effective and efficient method that grounds (i.e., localizes)
    natural sentences in long, untrimmed video sequences. Specifically, a novel Temporal
    GroundNet (TGN) is proposed to temporally capture the evolving fine-grained frame-by-word
    interactions between video and sentence. TGN sequentially scores a set of temporal
    candidates ended at each frame based on the exploited frame-by-word interactions,
    and finally grounds the segment corresponding to the sentence. Unlike traditional
    methods treating the overlapping segments separately in a sliding window fashion,
    TGN aggregates the historical information and generates the final grounding result
    in one single pass. We extensively evaluate our proposed TGN on three public datasets
    with significant improvements over the state-of-the-arts. We further show the
    consistent effectiveness and efficiency of TGN through an ablation study and a
    runtime test.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305211326
    type: video
    url: https://vimeo.com/305211326
  author:
  - first: Jingyuan
    full: Jingyuan Chen
    id: jingyuan-chen
    last: Chen
  - first: Xinpeng
    full: Xinpeng Chen
    id: xinpeng-chen
    last: Chen
  - first: Lin
    full: Lin Ma
    id: lin-ma
    last: Ma
  - first: Zequn
    full: Zequn Jie
    id: zequn-jie
    last: Jie
  - first: Tat-Seng
    full: Tat-Seng Chua
    id: tat-seng-chua
    last: Chua
  author_string: Jingyuan Chen, Xinpeng Chen, Lin Ma, Zequn Jie, Tat-Seng Chua
  bibkey: chen-etal-2018-temporally
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1015
  month: October-November
  page_first: '162'
  page_last: '171'
  pages: "162\u2013171"
  paper_id: '15'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1015.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1015.jpg
  title: Temporally Grounding Natural Sentence in Video
  title_html: Temporally Grounding Natural Sentence in Video
  url: https://www.aclweb.org/anthology/D18-1015
  year: '2018'
D18-1016:
  abstract: We introduce PreCo, a large-scale English dataset for coreference resolution.
    The dataset is designed to embody the core challenges in coreference, such as
    entity representation, by alleviating the challenge of low overlap between training
    and test sets and enabling separated analysis of mention detection and mention
    clustering. To strengthen the training-test overlap, we collect a large corpus
    of 38K documents and 12.5M words which are mostly from the vocabulary of English-speaking
    preschoolers. Experiments show that with higher training-test overlap, error analysis
    on PreCo is more efficient than the one on OntoNotes, a popular existing dataset.
    Furthermore, we annotate singleton mentions making it possible for the first time
    to quantify the influence that a mention detector makes on coreference resolution
    performance. The dataset is freely available at https://preschool-lab.github.io/PreCo/.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306353798
    type: video
    url: https://vimeo.com/306353798
  author:
  - first: Hong
    full: Hong Chen
    id: hong-chen
    last: Chen
  - first: Zhenhua
    full: Zhenhua Fan
    id: zhenhua-fan
    last: Fan
  - first: Hao
    full: Hao Lu
    id: hao-lu
    last: Lu
  - first: Alan
    full: Alan Yuille
    id: alan-yuille
    last: Yuille
  - first: Shu
    full: Shu Rong
    id: shu-rong
    last: Rong
  author_string: Hong Chen, Zhenhua Fan, Hao Lu, Alan Yuille, Shu Rong
  bibkey: chen-etal-2018-preco
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1016
  month: October-November
  page_first: '172'
  page_last: '181'
  pages: "172\u2013181"
  paper_id: '16'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1016.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1016.jpg
  title: 'PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution'
  title_html: '<span class="acl-fixed-case">P</span>re<span class="acl-fixed-case">C</span>o:
    A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution'
  url: https://www.aclweb.org/anthology/D18-1016
  year: '2018'
D18-1017:
  abstract: Named entity recognition (NER) is an important task in natural language
    processing area, which needs to determine entities boundaries and classify them
    into pre-defined categories. For Chinese NER task, there is only a very small
    amount of annotated data available. Chinese NER task and Chinese word segmentation
    (CWS) task have many similar word boundaries. There are also specificities in
    each task. However, existing methods for Chinese NER either do not exploit word
    boundary information from CWS or cannot filter the specific information of CWS.
    In this paper, we propose a novel adversarial transfer learning framework to make
    full use of task-shared boundaries information and prevent the task-specific features
    of CWS. Besides, since arbitrary character can provide important cues when predicting
    entity type, we exploit self-attention to explicitly capture long range dependencies
    between two tokens. Experimental results on two different widely used datasets
    show that our proposed model significantly and consistently outperforms other
    state-of-the-art methods.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306354811
    type: video
    url: https://vimeo.com/306354811
  author:
  - first: Pengfei
    full: Pengfei Cao
    id: pengfei-cao
    last: Cao
  - first: Yubo
    full: Yubo Chen
    id: yubo-chen
    last: Chen
  - first: Kang
    full: Kang Liu
    id: kang-liu
    last: Liu
  - first: Jun
    full: Jun Zhao
    id: jun-zhao
    last: Zhao
  - first: Shengping
    full: Shengping Liu
    id: shengping-liu
    last: Liu
  author_string: Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, Shengping Liu
  bibkey: cao-etal-2018-adversarial
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1017
  month: October-November
  page_first: '182'
  page_last: '192'
  pages: "182\u2013192"
  paper_id: '17'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1017.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1017.jpg
  title: Adversarial Transfer Learning for Chinese Named Entity Recognition with Self-Attention
    Mechanism
  title_html: Adversarial Transfer Learning for <span class="acl-fixed-case">C</span>hinese
    Named Entity Recognition with Self-Attention Mechanism
  url: https://www.aclweb.org/anthology/D18-1017
  year: '2018'
D18-1018:
  abstract: Coreference resolution is an intermediate step for text understanding.
    It is used in tasks and domains for which we do not necessarily have coreference
    annotated corpora. Therefore, generalization is of special importance for coreference
    resolution. However, while recent coreference resolvers have notable improvements
    on the CoNLL dataset, they struggle to generalize properly to new domains or datasets.
    In this paper, we investigate the role of linguistic features in building more
    generalizable coreference resolvers. We show that generalization improves only
    slightly by merely using a set of additional linguistic features. However, employing
    features and subsets of their values that are informative for coreference resolution,
    considerably improves generalization. Thanks to better generalization, our system
    achieves state-of-the-art results in out-of-domain evaluations, e.g., on WikiCoref,
    our system, which is trained on CoNLL, achieves on-par performance with a system
    designed for this dataset.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1018.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1018.Attachment.pdf
  - filename: https://vimeo.com/306355512
    type: video
    url: https://vimeo.com/306355512
  author:
  - first: Nafise Sadat
    full: Nafise Sadat Moosavi
    id: nafise-sadat-moosavi
    last: Moosavi
  - first: Michael
    full: Michael Strube
    id: michael-strube
    last: Strube
  author_string: Nafise Sadat Moosavi, Michael Strube
  bibkey: moosavi-strube-2018-using
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1018
  month: October-November
  page_first: '193'
  page_last: '203'
  pages: "193\u2013203"
  paper_id: '18'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1018.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1018.jpg
  title: Using Linguistic Features to Improve the Generalization Capability of Neural
    Coreference Resolvers
  title_html: Using Linguistic Features to Improve the Generalization Capability of
    Neural Coreference Resolvers
  url: https://www.aclweb.org/anthology/D18-1018
  year: '2018'
D18-1019:
  abstract: In this work, we propose a novel segmental hypergraph representation to
    model overlapping entity mentions that are prevalent in many practical datasets.
    We show that our model built on top of such a new representation is able to capture
    features and interactions that cannot be captured by previous models while maintaining
    a low time complexity for inference. We also present a theoretical analysis to
    formally assess how our representation is better than alternative representations
    reported in the literature in terms of representational power. Coupled with neural
    networks for feature learning, our model achieves the state-of-the-art performance
    in three benchmark datasets annotated with overlapping mentions.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1019.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1019.Attachment.zip
  - filename: https://vimeo.com/306356485
    type: video
    url: https://vimeo.com/306356485
  author:
  - first: Bailin
    full: Bailin Wang
    id: bailin-wang
    last: Wang
  - first: Wei
    full: Wei Lu
    id: wei-lu
    last: Lu
  author_string: Bailin Wang, Wei Lu
  bibkey: wang-lu-2018-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1019
  month: October-November
  page_first: '204'
  page_last: '214'
  pages: "204\u2013214"
  paper_id: '19'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1019.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1019.jpg
  title: Neural Segmental Hypergraphs for Overlapping Mention Recognition
  title_html: Neural Segmental Hypergraphs for Overlapping Mention Recognition
  url: https://www.aclweb.org/anthology/D18-1019
  year: '2018'
D18-1020:
  abstract: We introduce a family of multitask variational methods for semi-supervised
    sequence labeling. Our model family consists of a latent-variable generative model
    and a discriminative labeler. The generative models use latent variables to define
    the conditional probability of a word given its context, drawing inspiration from
    word prediction objectives commonly used in learning word embeddings. The labeler
    helps inject discriminative information into the latent space. We explore several
    latent variable configurations, including ones with hierarchical structure, which
    enables the model to account for both label-specific and word-specific information.
    Our models consistently outperform standard sequential baselines on 8 sequence
    labeling datasets, and improve further with unlabeled data.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1020.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1020.Attachment.pdf
  - filename: https://vimeo.com/306357379
    type: video
    url: https://vimeo.com/306357379
  author:
  - first: Mingda
    full: Mingda Chen
    id: mingda-chen
    last: Chen
  - first: Qingming
    full: Qingming Tang
    id: qingming-tang
    last: Tang
  - first: Karen
    full: Karen Livescu
    id: karen-livescu
    last: Livescu
  - first: Kevin
    full: Kevin Gimpel
    id: kevin-gimpel
    last: Gimpel
  author_string: Mingda Chen, Qingming Tang, Karen Livescu, Kevin Gimpel
  bibkey: chen-etal-2018-variational-sequential
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1020
  month: October-November
  page_first: '215'
  page_last: '226'
  pages: "215\u2013226"
  paper_id: '20'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1020.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1020.jpg
  title: Variational Sequential Labelers for Semi-Supervised Learning
  title_html: Variational Sequential Labelers for Semi-Supervised Learning
  url: https://www.aclweb.org/anthology/D18-1020
  year: '2018'
D18-1021:
  abstract: 'Jointly representation learning of words and entities benefits many NLP
    tasks, but has not been well explored in cross-lingual settings. In this paper,
    we propose a novel method for joint representation learning of cross-lingual words
    and entities. It captures mutually complementary knowledge, and enables cross-lingual
    inferences among knowledge bases and texts. Our method does not require parallel
    corpus, and automatically generates comparable data via distant supervision using
    multi-lingual knowledge bases. We utilize two types of regularizers to align cross-lingual
    words and entities, and design knowledge attention and cross-lingual attention
    to further reduce noises. We conducted a series of experiments on three tasks:
    word translation, entity relatedness, and cross-lingual entity linking. The results,
    both qualitative and quantitative, demonstrate the significance of our method.'
  address: Brussels, Belgium
  author:
  - first: Yixin
    full: Yixin Cao
    id: yixin-cao
    last: Cao
  - first: Lei
    full: Lei Hou
    id: lei-hou
    last: Hou
  - first: Juanzi
    full: Juanzi Li
    id: juanzi-li
    last: Li
  - first: Zhiyuan
    full: Zhiyuan Liu
    id: zhiyuan-liu
    last: Liu
  - first: Chengjiang
    full: Chengjiang Li
    id: chengjiang-li
    last: Li
  - first: Xu
    full: Xu Chen
    id: xu-chen
    last: Chen
  - first: Tiansi
    full: Tiansi Dong
    id: tiansi-dong
    last: Dong
  author_string: Yixin Cao, Lei Hou, Juanzi Li, Zhiyuan Liu, Chengjiang Li, Xu Chen,
    Tiansi Dong
  bibkey: cao-etal-2018-joint
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1021
  month: October-November
  page_first: '227'
  page_last: '237'
  pages: "227\u2013237"
  paper_id: '21'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1021.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1021.jpg
  title: Joint Representation Learning of Cross-lingual Words and Entities via Attentive
    Distant Supervision
  title_html: Joint Representation Learning of Cross-lingual Words and Entities via
    Attentive Distant Supervision
  url: https://www.aclweb.org/anthology/D18-1021
  year: '2018'
D18-1022:
  abstract: While cross-domain and cross-language transfer have long been prominent
    topics in NLP research, their combination has hardly been explored. In this work
    we consider this problem, and propose a framework that builds on pivot-based learning,
    structure-aware Deep Neural Networks (particularly LSTMs and CNNs) and bilingual
    word embeddings, with the goal of training a model on labeled data from one (language,
    domain) pair so that it can be effectively applied to another (language, domain)
    pair. We consider two setups, differing with respect to the unlabeled data available
    for model training. In the full setup the model has access to unlabeled data from
    both pairs, while in the lazy setup, which is more realistic for truly resource-poor
    languages, unlabeled data is available for both domains but only for the source
    language. We design our model for the lazy setup so that for a given target domain,
    it can train once on the source language and then be applied to any target language
    without re-training. In experiments with nine English-German and nine English-French
    domain pairs our best model substantially outperforms previous models even when
    it is trained in the lazy setup and previous models are trained in the full setup.
  address: Brussels, Belgium
  author:
  - first: Yftah
    full: Yftah Ziser
    id: yftah-ziser
    last: Ziser
  - first: Roi
    full: Roi Reichart
    id: roi-reichart
    last: Reichart
  author_string: Yftah Ziser, Roi Reichart
  bibkey: ziser-reichart-2018-deep
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1022
  month: October-November
  page_first: '238'
  page_last: '249'
  pages: "238\u2013249"
  paper_id: '22'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1022.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1022.jpg
  title: Deep Pivot-Based Modeling for Cross-language Cross-domain Transfer with Minimal
    Guidance
  title_html: Deep Pivot-Based Modeling for Cross-language Cross-domain Transfer with
    Minimal Guidance
  url: https://www.aclweb.org/anthology/D18-1022
  year: '2018'
D18-1023:
  abstract: 'We construct a multilingual common semantic space based on distributional
    semantics, where words from multiple languages are projected into a shared space
    via which all available resources and knowledge can be shared across multiple
    languages. Beyond word alignment, we introduce multiple cluster-level alignments
    and enforce the word clusters to be consistently distributed across multiple languages.
    We exploit three signals for clustering: (1) neighbor words in the monolingual
    word embedding space; (2) character-level information; and (3) linguistic properties
    (e.g., apposition, locative suffix) derived from linguistic structure knowledge
    bases available for thousands of languages. We introduce a new cluster-consistent
    correlational neural network to construct the common semantic space by aligning
    words as well as clusters. Intrinsic evaluation on monolingual and multilingual
    QVEC tasks shows our approach achieves significantly higher correlation with linguistic
    features which are extracted from manually crafted lexical resources than state-of-the-art
    multi-lingual embedding learning methods do. Using low-resource language name
    tagging as a case study for extrinsic evaluation, our approach achieves up to
    14.6% absolute F-score gain over the state of the art on cross-lingual direct
    transfer. Our approach is also shown to be robust even when the size of bilingual
    dictionary is small.'
  address: Brussels, Belgium
  author:
  - first: Lifu
    full: Lifu Huang
    id: lifu-huang
    last: Huang
  - first: Kyunghyun
    full: Kyunghyun Cho
    id: kyunghyun-cho
    last: Cho
  - first: Boliang
    full: Boliang Zhang
    id: boliang-zhang
    last: Zhang
  - first: Heng
    full: Heng Ji
    id: heng-ji
    last: Ji
  - first: Kevin
    full: Kevin Knight
    id: kevin-knight
    last: Knight
  author_string: Lifu Huang, Kyunghyun Cho, Boliang Zhang, Heng Ji, Kevin Knight
  bibkey: huang-etal-2018-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1023
  month: October-November
  page_first: '250'
  page_last: '260'
  pages: "250\u2013260"
  paper_id: '23'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1023.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1023.jpg
  title: Multi-lingual Common Semantic Space Construction via Cluster-consistent Word
    Embedding
  title_html: Multi-lingual Common Semantic Space Construction via Cluster-consistent
    Word Embedding
  url: https://www.aclweb.org/anthology/D18-1023
  year: '2018'
D18-1024:
  abstract: Multilingual Word Embeddings (MWEs) represent words from multiple languages
    in a single distributional vector space. Unsupervised MWE (UMWE) methods acquire
    multilingual embeddings without cross-lingual supervision, which is a significant
    advantage over traditional supervised approaches and opens many new possibilities
    for low-resource languages. Prior art for learning UMWEs, however, merely relies
    on a number of independently trained Unsupervised Bilingual Word Embeddings (UBWEs)
    to obtain multilingual embeddings. These methods fail to leverage the interdependencies
    that exist among many languages. To address this shortcoming, we propose a fully
    unsupervised framework for learning MWEs that directly exploits the relations
    between all language pairs. Our model substantially outperforms previous approaches
    in the experiments on multilingual word translation and cross-lingual word similarity.
    In addition, our model even beats supervised approaches trained with cross-lingual
    resources.
  address: Brussels, Belgium
  author:
  - first: Xilun
    full: Xilun Chen
    id: xilun-chen
    last: Chen
  - first: Claire
    full: Claire Cardie
    id: claire-cardie
    last: Cardie
  author_string: Xilun Chen, Claire Cardie
  bibkey: chen-cardie-2018-unsupervised
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1024
  month: October-November
  page_first: '261'
  page_last: '270'
  pages: "261\u2013270"
  paper_id: '24'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1024.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1024.jpg
  title: Unsupervised Multilingual Word Embeddings
  title_html: Unsupervised Multilingual Word Embeddings
  url: https://www.aclweb.org/anthology/D18-1024
  year: '2018'
D18-1025:
  abstract: This paper proposes a modularized sense induction and representation learning
    model that jointly learns bilingual sense embeddings that align well in the vector
    space, where the cross-lingual signal in the English-Chinese parallel corpus is
    exploited to capture the collocation and distributed characteristics in the language
    pair. The model is evaluated on the Stanford Contextual Word Similarity (SCWS)
    dataset to ensure the quality of monolingual sense embeddings. In addition, we
    introduce Bilingual Contextual Word Similarity (BCWS), a large and high-quality
    dataset for evaluating cross-lingual sense embeddings, which is the first attempt
    of measuring whether the learned embeddings are indeed aligned well in the vector
    space. The proposed approach shows the superior quality of sense embeddings evaluated
    in both monolingual and bilingual spaces.
  address: Brussels, Belgium
  author:
  - first: Ta-Chung
    full: Ta-Chung Chi
    id: ta-chung-chi
    last: Chi
  - first: Yun-Nung
    full: Yun-Nung Chen
    id: yun-nung-chen
    last: Chen
  author_string: Ta-Chung Chi, Yun-Nung Chen
  bibkey: chi-chen-2018-cluse
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1025
  month: October-November
  page_first: '271'
  page_last: '281'
  pages: "271\u2013281"
  paper_id: '25'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1025.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1025.jpg
  title: 'CLUSE: Cross-Lingual Unsupervised Sense Embeddings'
  title_html: '<span class="acl-fixed-case">CLUSE</span>: Cross-Lingual Unsupervised
    Sense Embeddings'
  url: https://www.aclweb.org/anthology/D18-1025
  year: '2018'
D18-1026:
  abstract: 'Semantic specialization is a process of fine-tuning pre-trained distributional
    word vectors using external lexical knowledge (e.g., WordNet) to accentuate a
    particular semantic relation in the specialized vector space. While post-processing
    specialization methods are applicable to arbitrary distributional vectors, they
    are limited to updating only the vectors of words occurring in external lexicons
    (i.e., seen words), leaving the vectors of all other words unchanged. We propose
    a novel approach to specializing the full distributional vocabulary. Our adversarial
    post-specialization method propagates the external lexical knowledge to the full
    distributional space. We exploit words seen in the resources as training examples
    for learning a global specialization function. This function is learned by combining
    a standard L2-distance loss with a adversarial loss: the adversarial component
    produces more realistic output vectors. We show the effectiveness and robustness
    of the proposed method across three languages and on three tasks: word similarity,
    dialog state tracking, and lexical simplification. We report consistent improvements
    over distributional word vectors and vectors specialized by other state-of-the-art
    specialization frameworks. Finally, we also propose a cross-lingual transfer method
    for zero-shot specialization which successfully specializes a full target distributional
    space without any lexical knowledge in the target language and without any bilingual
    data.'
  address: Brussels, Belgium
  author:
  - first: Edoardo Maria
    full: Edoardo Maria Ponti
    id: edoardo-maria-ponti
    last: Ponti
  - first: Ivan
    full: "Ivan Vuli\u0107"
    id: ivan-vulic
    last: "Vuli\u0107"
  - first: Goran
    full: "Goran Glava\u0161"
    id: goran-glavas
    last: "Glava\u0161"
  - first: Nikola
    full: "Nikola Mrk\u0161i\u0107"
    id: nikola-mrksic
    last: "Mrk\u0161i\u0107"
  - first: Anna
    full: Anna Korhonen
    id: anna-korhonen
    last: Korhonen
  author_string: "Edoardo Maria Ponti, Ivan Vuli\u0107, Goran Glava\u0161, Nikola\
    \ Mrk\u0161i\u0107, Anna Korhonen"
  bibkey: ponti-etal-2018-adversarial
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1026
  month: October-November
  page_first: '282'
  page_last: '293'
  pages: "282\u2013293"
  paper_id: '26'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1026.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1026.jpg
  title: Adversarial Propagation and Zero-Shot Cross-Lingual Transfer of Word Vector
    Specialization
  title_html: Adversarial Propagation and Zero-Shot Cross-Lingual Transfer of Word
    Vector Specialization
  url: https://www.aclweb.org/anthology/D18-1026
  year: '2018'
D18-1027:
  abstract: Cross-lingual word embeddings are becoming increasingly important in multilingual
    NLP. Recently, it has been shown that these embeddings can be effectively learned
    by aligning two disjoint monolingual vector spaces through linear transformations,
    using no more than a small bilingual dictionary as supervision. In this work,
    we propose to apply an additional transformation after the initial alignment step,
    which moves cross-lingual synonyms towards a middle point between them. By applying
    this transformation our aim is to obtain a better cross-lingual integration of
    the vector spaces. In addition, and perhaps surprisingly, the monolingual spaces
    also improve by this transformation. This is in contrast to the original alignment,
    which is typically learned such that the structure of the monolingual spaces is
    preserved. Our experiments confirm that the resulting cross-lingual embeddings
    outperform state-of-the-art models in both monolingual and cross-lingual evaluation
    tasks.
  address: Brussels, Belgium
  author:
  - first: Yerai
    full: Yerai Doval
    id: yerai-doval
    last: Doval
  - first: Jose
    full: Jose Camacho-Collados
    id: jose-camacho-collados
    last: Camacho-Collados
  - first: Luis
    full: Luis Espinosa-Anke
    id: luis-espinosa-anke
    last: Espinosa-Anke
  - first: Steven
    full: Steven Schockaert
    id: steven-schockaert
    last: Schockaert
  author_string: Yerai Doval, Jose Camacho-Collados, Luis Espinosa-Anke, Steven Schockaert
  bibkey: doval-etal-2018-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1027
  month: October-November
  page_first: '294'
  page_last: '304'
  pages: "294\u2013304"
  paper_id: '27'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1027.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1027.jpg
  title: Improving Cross-Lingual Word Embeddings by Meeting in the Middle
  title_html: Improving Cross-Lingual Word Embeddings by Meeting in the Middle
  url: https://www.aclweb.org/anthology/D18-1027
  year: '2018'
D18-1028:
  abstract: We release a corpus of 43 million atomic edits across 8 languages. These
    edits are mined from Wikipedia edit history and consist of instances in which
    a human editor has inserted a single contiguous phrase into, or deleted a single
    contiguous phrase from, an existing sentence. We use the collected data to show
    that the language generated during editing differs from the language that we observe
    in standard corpora, and that models trained on edits encode different aspects
    of semantics and discourse than models trained on raw text. We release the full
    corpus as a resource to aid ongoing research in semantics, discourse, and representation
    learning.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1028.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1028.Attachment.pdf
  author:
  - first: Manaal
    full: Manaal Faruqui
    id: manaal-faruqui
    last: Faruqui
  - first: Ellie
    full: Ellie Pavlick
    id: ellie-pavlick
    last: Pavlick
  - first: Ian
    full: Ian Tenney
    id: ian-tenney
    last: Tenney
  - first: Dipanjan
    full: Dipanjan Das
    id: dipanjan-das
    last: Das
  author_string: Manaal Faruqui, Ellie Pavlick, Ian Tenney, Dipanjan Das
  bibkey: faruqui-etal-2018-wikiatomicedits
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1028
  month: October-November
  page_first: '305'
  page_last: '315'
  pages: "305\u2013315"
  paper_id: '28'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1028.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1028.jpg
  title: 'WikiAtomicEdits: A Multilingual Corpus of Wikipedia Edits for Modeling Language
    and Discourse'
  title_html: '<span class="acl-fixed-case">W</span>iki<span class="acl-fixed-case">A</span>tomic<span
    class="acl-fixed-case">E</span>dits: A Multilingual Corpus of <span class="acl-fixed-case">W</span>ikipedia
    Edits for Modeling Language and Discourse'
  url: https://www.aclweb.org/anthology/D18-1028
  year: '2018'
D18-1029:
  abstract: "A key challenge in cross-lingual NLP is developing general language-independent\
    \ architectures that are equally applicable to any language. However, this ambition\
    \ is largely hampered by the variation in structural and semantic properties,\
    \ i.e. the typological profiles of the world\u2019s languages. In this work, we\
    \ analyse the implications of this variation on the language modeling (LM) task.\
    \ We present a large-scale study of state-of-the art n-gram based and neural language\
    \ models on 50 typologically diverse languages covering a wide variety of morphological\
    \ systems. Operating in the full vocabulary LM setup focused on word-level prediction,\
    \ we demonstrate that a coarse typology of morphological systems is predictive\
    \ of absolute LM performance. Moreover, fine-grained typological features such\
    \ as exponence, flexivity, fusion, and inflectional synthesis are borne out to\
    \ be responsible for the proliferation of low-frequency phenomena which are organically\
    \ difficult to model by statistical architectures, or for the meaning ambiguity\
    \ of character n-grams. Our study strongly suggests that these features have to\
    \ be taken into consideration during the construction of next-level language-agnostic\
    \ LM architectures, capable of handling morphologically complex languages such\
    \ as Tamil or Korean."
  address: Brussels, Belgium
  author:
  - first: Daniela
    full: Daniela Gerz
    id: daniela-gerz
    last: Gerz
  - first: Ivan
    full: "Ivan Vuli\u0107"
    id: ivan-vulic
    last: "Vuli\u0107"
  - first: Edoardo Maria
    full: Edoardo Maria Ponti
    id: edoardo-maria-ponti
    last: Ponti
  - first: Roi
    full: Roi Reichart
    id: roi-reichart
    last: Reichart
  - first: Anna
    full: Anna Korhonen
    id: anna-korhonen
    last: Korhonen
  author_string: "Daniela Gerz, Ivan Vuli\u0107, Edoardo Maria Ponti, Roi Reichart,\
    \ Anna Korhonen"
  bibkey: gerz-etal-2018-relation
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1029
  month: October-November
  page_first: '316'
  page_last: '327'
  pages: "316\u2013327"
  paper_id: '29'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1029.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1029.jpg
  title: On the Relation between Linguistic Typology and (Limitations of) Multilingual
    Language Modeling
  title_html: On the Relation between Linguistic Typology and (Limitations of) Multilingual
    Language Modeling
  url: https://www.aclweb.org/anthology/D18-1029
  year: '2018'
D18-1030:
  abstract: 'We address fine-grained multilingual language identification: providing
    a language code for every token in a sentence, including codemixed text containing
    multiple languages. Such text is prevalent online, in documents, social media,
    and message boards. We show that a feed-forward network with a simple globally
    constrained decoder can accurately and rapidly label both codemixed and monolingual
    text in 100 languages and 100 language pairs. This model outperforms previously
    published multilingual approaches in terms of both accuracy and speed, yielding
    an 800x speed-up and a 19.5% averaged absolute gain on three codemixed datasets.
    It furthermore outperforms several benchmark systems on monolingual language identification.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1030.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1030.Attachment.pdf
  author:
  - first: Yuan
    full: Yuan Zhang
    id: yuan-zhang
    last: Zhang
  - first: Jason
    full: Jason Riesa
    id: jason-riesa
    last: Riesa
  - first: Daniel
    full: Daniel Gillick
    id: dan-gillick
    last: Gillick
  - first: Anton
    full: Anton Bakalov
    id: anton-bakalov
    last: Bakalov
  - first: Jason
    full: Jason Baldridge
    id: jason-baldridge
    last: Baldridge
  - first: David
    full: David Weiss
    id: david-weiss
    last: Weiss
  author_string: Yuan Zhang, Jason Riesa, Daniel Gillick, Anton Bakalov, Jason Baldridge,
    David Weiss
  bibkey: zhang-etal-2018-fast-compact
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1030
  month: October-November
  page_first: '328'
  page_last: '337'
  pages: "328\u2013337"
  paper_id: '30'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1030.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1030.jpg
  title: A Fast, Compact, Accurate Model for Language Identification of Codemixed
    Text
  title_html: A Fast, Compact, Accurate Model for Language Identification of Codemixed
    Text
  url: https://www.aclweb.org/anthology/D18-1030
  year: '2018'
D18-1031:
  abstract: "Sentiment expression in microblog posts can be affected by user\u2019\
    s personal character, opinion bias, political stance and so on. Most of existing\
    \ personalized microblog sentiment classification methods suffer from the insufficiency\
    \ of discriminative tweets for personalization learning. We observed that microblog\
    \ users have consistent individuality and opinion bias in different languages.\
    \ Based on this observation, in this paper we propose a novel user-attention-based\
    \ Convolutional Neural Network (CNN) model with adversarial cross-lingual learning\
    \ framework. The user attention mechanism is leveraged in CNN model to capture\
    \ user\u2019s language-specific individuality from the posts. Then the attention-based\
    \ CNN model is incorporated into a novel adversarial cross-lingual learning framework,\
    \ in which with the help of user properties as bridge between languages, we can\
    \ extract the language-specific features and language-independent features to\
    \ enrich the user post representation so as to alleviate the data insufficiency\
    \ problem. Results on English and Chinese microblog datasets confirm that our\
    \ method outperforms state-of-the-art baseline algorithms with large margins."
  address: Brussels, Belgium
  author:
  - first: Weichao
    full: Weichao Wang
    id: weichao-wang
    last: Wang
  - first: Shi
    full: Shi Feng
    id: shi-feng
    last: Feng
  - first: Wei
    full: Wei Gao
    id: wei-gao
    last: Gao
  - first: Daling
    full: Daling Wang
    id: daling-wang
    last: Wang
  - first: Yifei
    full: Yifei Zhang
    id: yifei-zhang
    last: Zhang
  author_string: Weichao Wang, Shi Feng, Wei Gao, Daling Wang, Yifei Zhang
  bibkey: wang-etal-2018-personalized
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1031
  month: October-November
  page_first: '338'
  page_last: '348'
  pages: "338\u2013348"
  paper_id: '31'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1031.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1031.jpg
  title: Personalized Microblog Sentiment Classification via Adversarial Cross-lingual
    Multi-task Learning
  title_html: Personalized Microblog Sentiment Classification via Adversarial Cross-lingual
    Multi-task Learning
  url: https://www.aclweb.org/anthology/D18-1031
  year: '2018'
D18-1032:
  abstract: Multilingual knowledge graphs (KGs) such as DBpedia and YAGO contain structured
    knowledge of entities in several distinct languages, and they are useful resources
    for cross-lingual AI and NLP applications. Cross-lingual KG alignment is the task
    of matching entities with their counterparts in different languages, which is
    an important way to enrich the cross-lingual links in multilingual KGs. In this
    paper, we propose a novel approach for cross-lingual KG alignment via graph convolutional
    networks (GCNs). Given a set of pre-aligned entities, our approach trains GCNs
    to embed entities of each language into a unified vector space. Entity alignments
    are discovered based on the distances between entities in the embedding space.
    Embeddings can be learned from both the structural and attribute information of
    entities, and the results of structure embedding and attribute embedding are combined
    to get accurate alignments. In the experiments on aligning real multilingual KGs,
    our approach gets the best performance compared with other embedding-based KG
    alignment approaches.
  address: Brussels, Belgium
  author:
  - first: Zhichun
    full: Zhichun Wang
    id: zhichun-wang
    last: Wang
  - first: Qingsong
    full: Qingsong Lv
    id: qingsong-lv
    last: Lv
  - first: Xiaohan
    full: Xiaohan Lan
    id: xiaohan-lan
    last: Lan
  - first: Yu
    full: Yu Zhang
    id: yu-zhang
    last: Zhang
  author_string: Zhichun Wang, Qingsong Lv, Xiaohan Lan, Yu Zhang
  bibkey: wang-etal-2018-cross-lingual
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1032
  month: October-November
  page_first: '349'
  page_last: '357'
  pages: "349\u2013357"
  paper_id: '32'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1032.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1032.jpg
  title: Cross-lingual Knowledge Graph Alignment via Graph Convolutional Networks
  title_html: Cross-lingual Knowledge Graph Alignment via Graph Convolutional Networks
  url: https://www.aclweb.org/anthology/D18-1032
  year: '2018'
D18-1033:
  abstract: Sememes are defined as the minimum semantic units of human languages.
    As important knowledge sources, sememe-based linguistic knowledge bases have been
    widely used in many NLP tasks. However, most languages still do not have sememe-based
    linguistic knowledge bases. Thus we present a task of cross-lingual lexical sememe
    prediction, aiming to automatically predict sememes for words in other languages.
    We propose a novel framework to model correlations between sememes and multi-lingual
    words in low-dimensional semantic space for sememe prediction. Experimental results
    on real-world datasets show that our proposed model achieves consistent and significant
    improvements as compared to baseline methods in cross-lingual sememe prediction.
    The codes and data of this paper are available at https://github.com/thunlp/CL-SP.
  address: Brussels, Belgium
  author:
  - first: Fanchao
    full: Fanchao Qi
    id: fanchao-qi
    last: Qi
  - first: Yankai
    full: Yankai Lin
    id: yankai-lin
    last: Lin
  - first: Maosong
    full: Maosong Sun
    id: maosong-sun
    last: Sun
  - first: Hao
    full: Hao Zhu
    id: hao-zhu
    last: Zhu
  - first: Ruobing
    full: Ruobing Xie
    id: ruobing-xie
    last: Xie
  - first: Zhiyuan
    full: Zhiyuan Liu
    id: zhiyuan-liu
    last: Liu
  author_string: Fanchao Qi, Yankai Lin, Maosong Sun, Hao Zhu, Ruobing Xie, Zhiyuan
    Liu
  bibkey: qi-etal-2018-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1033
  month: October-November
  page_first: '358'
  page_last: '368'
  pages: "358\u2013368"
  paper_id: '33'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1033.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1033.jpg
  title: Cross-lingual Lexical Sememe Prediction
  title_html: Cross-lingual Lexical Sememe Prediction
  url: https://www.aclweb.org/anthology/D18-1033
  year: '2018'
D18-1034:
  abstract: For languages with no annotated resources, unsupervised transfer of natural
    language processing models such as named-entity recognition (NER) from resource-rich
    languages would be an appealing capability. However, differences in words and
    word order across languages make it a challenging problem. To improve mapping
    of lexical items across languages, we propose a method that finds translations
    based on bilingual word embeddings. To improve robustness to word order differences,
    we propose to use self-attention, which allows for a degree of flexibility with
    respect to word order. We demonstrate that these methods achieve state-of-the-art
    or competitive NER performance on commonly tested languages under a cross-lingual
    setting, with much lower resource requirements than past approaches. We also evaluate
    the challenges of applying these methods to Uyghur, a low-resource language.
  address: Brussels, Belgium
  author:
  - first: Jiateng
    full: Jiateng Xie
    id: jiateng-xie
    last: Xie
  - first: Zhilin
    full: Zhilin Yang
    id: zhilin-yang
    last: Yang
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  - first: Noah A.
    full: Noah A. Smith
    id: noah-a-smith
    last: Smith
  - first: Jaime
    full: Jaime Carbonell
    id: jaime-g-carbonell
    last: Carbonell
  author_string: Jiateng Xie, Zhilin Yang, Graham Neubig, Noah A. Smith, Jaime Carbonell
  bibkey: xie-etal-2018-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1034
  month: October-November
  page_first: '369'
  page_last: '379'
  pages: "369\u2013379"
  paper_id: '34'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1034.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1034.jpg
  title: Neural Cross-Lingual Named Entity Recognition with Minimal Resources
  title_html: Neural Cross-Lingual Named Entity Recognition with Minimal Resources
  url: https://www.aclweb.org/anthology/D18-1034
  year: '2018'
D18-1035:
  abstract: Beam search is a widely used approximate search strategy for neural network
    decoders, and it generally outperforms simple greedy decoding on tasks like machine
    translation. However, this improvement comes at substantial computational cost.
    In this paper, we propose a flexible new method that allows us to reap nearly
    the full benefits of beam search with nearly no additional computational cost.
    The method revolves around a small neural network actor that is trained to observe
    and manipulate the hidden state of a previously-trained decoder. To train this
    actor network, we introduce the use of a pseudo-parallel corpus built using the
    output of beam search on a base model, ranked by a target quality metric like
    BLEU. Our method is inspired by earlier work on this problem, but requires no
    reinforcement learning, and can be trained reliably on a range of models. Experiments
    on three parallel corpora and three architectures show that the method yields
    substantial improvements in translation quality and speed over each base system.
  address: Brussels, Belgium
  author:
  - first: Yun
    full: Yun Chen
    id: yun-chen
    last: Chen
  - first: Victor O.K.
    full: Victor O.K. Li
    id: victor-o-k-li
    last: Li
  - first: Kyunghyun
    full: Kyunghyun Cho
    id: kyunghyun-cho
    last: Cho
  - first: Samuel
    full: Samuel Bowman
    id: samuel-bowman
    last: Bowman
  author_string: Yun Chen, Victor O.K. Li, Kyunghyun Cho, Samuel Bowman
  bibkey: chen-etal-2018-stable
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1035
  month: October-November
  page_first: '380'
  page_last: '390'
  pages: "380\u2013390"
  paper_id: '35'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1035.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1035.jpg
  title: A Stable and Effective Learning Strategy for Trainable Greedy Decoding
  title_html: A Stable and Effective Learning Strategy for Trainable Greedy Decoding
  url: https://www.aclweb.org/anthology/D18-1035
  year: '2018'
D18-1036:
  abstract: One of the weaknesses of Neural Machine Translation (NMT) is in handling
    lowfrequency and ambiguous words, which we refer as troublesome words. To address
    this problem, we propose a novel memoryenhanced NMT method. First, we investigate
    different strategies to define and detect the troublesome words. Then, a contextual
    memory is constructed to memorize which target words should be produced in what
    situations. Finally, we design a hybrid model to dynamically access the contextual
    memory so as to correctly translate the troublesome words. The extensive experiments
    on Chinese-to-English and English-to-German translation tasks demonstrate that
    our method significantly outperforms the strong baseline models in translation
    quality, especially in handling troublesome words.
  address: Brussels, Belgium
  author:
  - first: Yang
    full: Yang Zhao
    id: yang-zhao
    last: Zhao
  - first: Jiajun
    full: Jiajun Zhang
    id: jiajun-zhang
    last: Zhang
  - first: Zhongjun
    full: Zhongjun He
    id: zhongjun-he
    last: He
  - first: Chengqing
    full: Chengqing Zong
    id: chengqing-zong
    last: Zong
  - first: Hua
    full: Hua Wu
    id: hua-wu
    last: Wu
  author_string: Yang Zhao, Jiajun Zhang, Zhongjun He, Chengqing Zong, Hua Wu
  bibkey: zhao-etal-2018-addressing
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1036
  month: October-November
  page_first: '391'
  page_last: '400'
  pages: "391\u2013400"
  paper_id: '36'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1036.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1036.jpg
  title: Addressing Troublesome Words in Neural Machine Translation
  title_html: Addressing Troublesome Words in Neural Machine Translation
  url: https://www.aclweb.org/anthology/D18-1036
  year: '2018'
D18-1037:
  abstract: The addition of syntax-aware decoding in Neural Machine Translation (NMT)
    systems requires an effective tree-structured neural network, a syntax-aware attention
    model and a language generation model that is sensitive to sentence structure.
    Recent approaches resort to sequential decoding by adding additional neural network
    units to capture bottom-up structural information, or serialising structured data
    into sequence. We exploit a top-down tree-structured model called DRNN (Doubly-Recurrent
    Neural Networks) first proposed by Alvarez-Melis and Jaakola (2017) to create
    an NMT model called Seq2DRNN that combines a sequential encoder with tree-structured
    decoding augmented with a syntax-aware attention model. Unlike previous approaches
    to syntax-based NMT which use dependency parsing models our method uses constituency
    parsing which we argue provides useful information for translation. In addition,
    we use the syntactic structure of the sentence to add new connections to the tree-structured
    decoder neural network (Seq2DRNN+SynC). We compare our NMT model with sequential
    and state of the art syntax-based NMT models and show that our model produces
    more fluent translations with better reordering. Since our model is capable of
    doing translation and constituency parsing at the same time we also compare our
    parsing accuracy against other neural parsing models.
  address: Brussels, Belgium
  author:
  - first: Jetic
    full: "Jetic G\u016B"
    id: jetic-gu
    last: "G\u016B"
  - first: Hassan S.
    full: Hassan S. Shavarani
    id: hassan-s-shavarani
    last: Shavarani
  - first: Anoop
    full: Anoop Sarkar
    id: anoop-sarkar
    last: Sarkar
  author_string: "Jetic G\u016B, Hassan S. Shavarani, Anoop Sarkar"
  bibkey: gu-etal-2018-top
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1037
  month: October-November
  page_first: '401'
  page_last: '413'
  pages: "401\u2013413"
  paper_id: '37'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1037.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1037.jpg
  title: Top-down Tree Structured Decoding with Syntactic Connections for Neural Machine
    Translation and Parsing
  title_html: Top-down Tree Structured Decoding with Syntactic Connections for Neural
    Machine Translation and Parsing
  url: https://www.aclweb.org/anthology/D18-1037
  year: '2018'
D18-1038:
  abstract: "Task-oriented dialog systems are becoming pervasive, and many companies\
    \ heavily rely on them to complement human agents for customer service in call\
    \ centers. With globalization, the need for providing cross-lingual customer support\
    \ becomes more urgent than ever. However, cross-lingual support poses great challenges\u2014\
    it requires a large amount of additional annotated data from native speakers.\
    \ In order to bypass the expensive human annotation and achieve the first step\
    \ towards the ultimate goal of building a universal dialog system, we set out\
    \ to build a cross-lingual state tracking framework. Specifically, we assume that\
    \ there exists a source language with dialog belief tracking annotations while\
    \ the target languages have no annotated dialog data of any form. Then, we pre-train\
    \ a state tracker for the source language as a teacher, which is able to exploit\
    \ easy-to-access parallel data. We then distill and transfer its own knowledge\
    \ to the student state tracker in target languages. We specifically discuss two\
    \ types of common parallel resources: bilingual corpus and bilingual dictionary,\
    \ and design different transfer learning strategies accordingly. Experimentally,\
    \ we successfully use English state tracker as the teacher to transfer its knowledge\
    \ to both Italian and German trackers and achieve promising results."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1038.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1038.Attachment.pdf
  author:
  - first: Wenhu
    full: Wenhu Chen
    id: wenhu-chen
    last: Chen
  - first: Jianshu
    full: Jianshu Chen
    id: jianshu-chen
    last: Chen
  - first: Yu
    full: Yu Su
    id: yu-su
    last: Su
  - first: Xin
    full: Xin Wang
    id: xin-wang
    last: Wang
  - first: Dong
    full: Dong Yu
    id: dong-yu
    last: Yu
  - first: Xifeng
    full: Xifeng Yan
    id: xifeng-yan
    last: Yan
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  author_string: Wenhu Chen, Jianshu Chen, Yu Su, Xin Wang, Dong Yu, Xifeng Yan, William
    Yang Wang
  bibkey: chen-etal-2018-xl
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1038
  month: October-November
  page_first: '414'
  page_last: '424'
  pages: "414\u2013424"
  paper_id: '38'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1038.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1038.jpg
  title: 'XL-NBT: A Cross-lingual Neural Belief Tracking Framework'
  title_html: '<span class="acl-fixed-case">XL</span>-<span class="acl-fixed-case">NBT</span>:
    A Cross-lingual Neural Belief Tracking Framework'
  url: https://www.aclweb.org/anthology/D18-1038
  year: '2018'
D18-1039:
  abstract: We propose a simple modification to existing neural machine translation
    (NMT) models that enables using a single universal model to translate between
    multiple languages while allowing for language specific parameterization, and
    that can also be used for domain adaptation. Our approach requires no changes
    to the model architecture of a standard NMT system, but instead introduces a new
    component, the contextual parameter generator (CPG), that generates the parameters
    of the system (e.g., weights in a neural network). This parameter generator accepts
    source and target language embeddings as input, and generates the parameters for
    the encoder and the decoder, respectively. The rest of the model remains unchanged
    and is shared across all languages. We show how this simple modification enables
    the system to use monolingual data for training and also perform zero-shot translation.
    We further show it is able to surpass state-of-the-art performance for both the
    IWSLT-15 and IWSLT-17 datasets and that the learned language embeddings are able
    to uncover interesting relationships between languages.
  address: Brussels, Belgium
  author:
  - first: Emmanouil Antonios
    full: Emmanouil Antonios Platanios
    id: emmanouil-antonios-platanios
    last: Platanios
  - first: Mrinmaya
    full: Mrinmaya Sachan
    id: mrinmaya-sachan
    last: Sachan
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  - first: Tom
    full: Tom Mitchell
    id: tom-mitchell
    last: Mitchell
  author_string: Emmanouil Antonios Platanios, Mrinmaya Sachan, Graham Neubig, Tom
    Mitchell
  bibkey: platanios-etal-2018-contextual
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1039
  month: October-November
  page_first: '425'
  page_last: '435'
  pages: "425\u2013435"
  paper_id: '39'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1039.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1039.jpg
  title: Contextual Parameter Generation for Universal Neural Machine Translation
  title_html: Contextual Parameter Generation for Universal Neural Machine Translation
  url: https://www.aclweb.org/anthology/D18-1039
  year: '2018'
D18-1040:
  abstract: Neural Machine Translation has achieved state-of-the-art performance for
    several language pairs using a combination of parallel and synthetic data. Synthetic
    data is often generated by back-translating sentences randomly sampled from monolingual
    data using a reverse translation model. While back-translation has been shown
    to be very effective in many cases, it is not entirely clear why. In this work,
    we explore different aspects of back-translation, and show that words with high
    prediction loss during training benefit most from the addition of synthetic data.
    We introduce several variations of sampling strategies targeting difficult-to-predict
    words using prediction losses and frequencies of words. In addition, we also target
    the contexts of difficult words and sample sentences that are similar in context.
    Experimental results for the WMT news translation task show that our method improves
    translation quality by up to 1.7 and 1.2 Bleu points over back-translation using
    random sampling for German-English and English-German, respectively.
  address: Brussels, Belgium
  author:
  - first: Marzieh
    full: Marzieh Fadaee
    id: marzieh-fadaee
    last: Fadaee
  - first: Christof
    full: Christof Monz
    id: christof-monz
    last: Monz
  author_string: Marzieh Fadaee, Christof Monz
  bibkey: fadaee-monz-2018-back
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1040
  month: October-November
  page_first: '436'
  page_last: '446'
  pages: "436\u2013446"
  paper_id: '40'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1040.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1040.jpg
  title: Back-Translation Sampling by Targeting Difficult Words in Neural Machine
    Translation
  title_html: Back-Translation Sampling by Targeting Difficult Words in Neural Machine
    Translation
  url: https://www.aclweb.org/anthology/D18-1040
  year: '2018'
D18-1041:
  abstract: 'With great practical value, the study of Multi-domain Neural Machine
    Translation (NMT) mainly focuses on using mixed-domain parallel sentences to construct
    a unified model that allows translation to switch between different domains. Intuitively,
    words in a sentence are related to its domain to varying degrees, so that they
    will exert disparate impacts on the multi-domain NMT modeling. Based on this intuition,
    in this paper, we devote to distinguishing and exploiting word-level domain contexts
    for multi-domain NMT. To this end, we jointly model NMT with monolingual attention-based
    domain classification tasks and improve NMT as follows: 1) Based on the sentence
    representations produced by a domain classifier and an adversarial domain classifier,
    we generate two gating vectors and use them to construct domain-specific and domain-shared
    annotations, for later translation predictions via different attention models;
    2) We utilize the attention weights derived from target-side domain classifier
    to adjust the weights of target words in the training objective, enabling domain-related
    words to have greater impacts during model training. Experimental results on Chinese-English
    and English-French multi-domain translation tasks demonstrate the effectiveness
    of the proposed model. Source codes of this paper are available on Github https://github.com/DeepLearnXMU/WDCNMT.'
  address: Brussels, Belgium
  author:
  - first: Jiali
    full: Jiali Zeng
    id: jiali-zeng
    last: Zeng
  - first: Jinsong
    full: Jinsong Su
    id: jinsong-su
    last: Su
  - first: Huating
    full: Huating Wen
    id: huating-wen
    last: Wen
  - first: Yang
    full: Yang Liu
    id: yang-liu-ict
    last: Liu
  - first: Jun
    full: Jun Xie
    id: jun-xie
    last: Xie
  - first: Yongjing
    full: Yongjing Yin
    id: yongjing-yin
    last: Yin
  - first: Jianqiang
    full: Jianqiang Zhao
    id: jianqiang-zhao
    last: Zhao
  author_string: Jiali Zeng, Jinsong Su, Huating Wen, Yang Liu, Jun Xie, Yongjing
    Yin, Jianqiang Zhao
  bibkey: zeng-etal-2018-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1041
  month: October-November
  page_first: '447'
  page_last: '457'
  pages: "447\u2013457"
  paper_id: '41'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1041.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1041.jpg
  title: Multi-Domain Neural Machine Translation with Word-Level Domain Context Discrimination
  title_html: Multi-Domain Neural Machine Translation with Word-Level Domain Context
    Discrimination
  url: https://www.aclweb.org/anthology/D18-1041
  year: '2018'
D18-1042:
  abstract: We introduce a novel discriminative latent-variable model for the task
    of bilingual lexicon induction. Our model combines the bipartite matching dictionary
    prior of Haghighi et al. (2008) with a state-of-the-art embedding-based approach.
    To train the model, we derive an efficient Viterbi EM algorithm. We provide empirical
    improvements on six language pairs under two metrics and show that the prior theoretically
    and empirically helps to mitigate the hubness problem. We also demonstrate how
    previous work may be viewed as a similarly fashioned latent-variable model, albeit
    with a different prior.
  address: Brussels, Belgium
  author:
  - first: Sebastian
    full: Sebastian Ruder
    id: sebastian-ruder
    last: Ruder
  - first: Ryan
    full: Ryan Cotterell
    id: ryan-cotterell
    last: Cotterell
  - first: Yova
    full: Yova Kementchedjhieva
    id: yova-kementchedjhieva
    last: Kementchedjhieva
  - first: Anders
    full: "Anders S\xF8gaard"
    id: anders-sogaard
    last: "S\xF8gaard"
  author_string: "Sebastian Ruder, Ryan Cotterell, Yova Kementchedjhieva, Anders S\xF8\
    gaard"
  bibkey: ruder-etal-2018-discriminative
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1042
  month: October-November
  page_first: '458'
  page_last: '468'
  pages: "458\u2013468"
  paper_id: '42'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1042.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1042.jpg
  title: A Discriminative Latent-Variable Model for Bilingual Lexicon Induction
  title_html: A Discriminative Latent-Variable Model for Bilingual Lexicon Induction
  url: https://www.aclweb.org/anthology/D18-1042
  year: '2018'
D18-1043:
  abstract: 'Unsupervised word translation from non-parallel inter-lingual corpora
    has attracted much research interest. Very recently, neural network methods trained
    with adversarial loss functions achieved high accuracy on this task. Despite the
    impressive success of the recent techniques, they suffer from the typical drawbacks
    of generative adversarial models: sensitivity to hyper-parameters, long training
    time and lack of interpretability. In this paper, we make the observation that
    two sufficiently similar distributions can be aligned correctly with iterative
    matching methods. We present a novel method that first aligns the second moment
    of the word distributions of the two languages and then iteratively refines the
    alignment. Extensive experiments on word translation of European and Non-European
    languages show that our method achieves better performance than recent state-of-the-art
    deep adversarial approaches and is competitive with the supervised baseline. It
    is also efficient, easy to parallelize on CPU and interpretable.'
  address: Brussels, Belgium
  author:
  - first: Yedid
    full: Yedid Hoshen
    id: yedid-hoshen
    last: Hoshen
  - first: Lior
    full: Lior Wolf
    id: lior-wolf
    last: Wolf
  author_string: Yedid Hoshen, Lior Wolf
  bibkey: hoshen-wolf-2018-non
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1043
  month: October-November
  page_first: '469'
  page_last: '478'
  pages: "469\u2013478"
  paper_id: '43'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1043.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1043.jpg
  title: Non-Adversarial Unsupervised Word Translation
  title_html: Non-Adversarial Unsupervised Word Translation
  url: https://www.aclweb.org/anthology/D18-1043
  year: '2018'
D18-1044:
  abstract: "Existing approaches to neural machine translation are typically autoregressive\
    \ models. While these models attain state-of-the-art translation quality, they\
    \ are suffering from low parallelizability and thus slow at decoding long sequences.\
    \ In this paper, we propose a novel model for fast sequence generation \u2014\
    \ the semi-autoregressive Transformer (SAT). The SAT keeps the autoregressive\
    \ property in global but relieves in local and thus are able to produce multiple\
    \ successive words in parallel at each time step. Experiments conducted on English-German\
    \ and Chinese-English translation tasks show that the SAT achieves a good balance\
    \ between translation quality and decoding speed. On WMT\u201914 English-German\
    \ translation, the SAT achieves 5.58\xD7 speedup while maintaining 88% translation\
    \ quality, significantly better than the previous non-autoregressive methods.\
    \ When produces two words at each time step, the SAT is almost lossless (only\
    \ 1% degeneration in BLEU score)."
  address: Brussels, Belgium
  author:
  - first: Chunqi
    full: Chunqi Wang
    id: chunqi-wang
    last: Wang
  - first: Ji
    full: Ji Zhang
    id: ji-zhang
    last: Zhang
  - first: Haiqing
    full: Haiqing Chen
    id: haiqing-chen
    last: Chen
  author_string: Chunqi Wang, Ji Zhang, Haiqing Chen
  bibkey: wang-etal-2018-semi-autoregressive
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1044
  month: October-November
  page_first: '479'
  page_last: '488'
  pages: "479\u2013488"
  paper_id: '44'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1044.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1044.jpg
  title: Semi-Autoregressive Neural Machine Translation
  title_html: Semi-Autoregressive Neural Machine Translation
  url: https://www.aclweb.org/anthology/D18-1044
  year: '2018'
D18-1045:
  abstract: "An effective method to improve neural machine translation with monolingual\
    \ data is to augment the parallel training corpus with back-translations of target\
    \ language sentences. This work broadens the understanding of back-translation\
    \ and investigates a number of methods to generate synthetic source sentences.\
    \ We find that in all but resource poor settings back-translations obtained via\
    \ sampling or noised beam outputs are most effective. Our analysis shows that\
    \ sampling or noisy synthetic data gives a much stronger training signal than\
    \ data generated by beam or greedy search. We also compare how synthetic data\
    \ compares to genuine bitext and study various domain effects. Finally, we scale\
    \ to hundreds of millions of monolingual sentences and achieve a new state of\
    \ the art of 35 BLEU on the WMT\u201914 English-German test set."
  address: Brussels, Belgium
  author:
  - first: Sergey
    full: Sergey Edunov
    id: sergey-edunov
    last: Edunov
  - first: Myle
    full: Myle Ott
    id: myle-ott
    last: Ott
  - first: Michael
    full: Michael Auli
    id: michael-auli
    last: Auli
  - first: David
    full: David Grangier
    id: david-grangier
    last: Grangier
  author_string: Sergey Edunov, Myle Ott, Michael Auli, David Grangier
  bibkey: edunov-etal-2018-understanding
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1045
  month: October-November
  page_first: '489'
  page_last: '500'
  pages: "489\u2013500"
  paper_id: '45'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1045.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1045.jpg
  title: Understanding Back-Translation at Scale
  title_html: Understanding Back-Translation at Scale
  url: https://www.aclweb.org/anthology/D18-1045
  year: '2018'
D18-1046:
  abstract: Generating the English transliteration of a name written in a foreign
    script is an important and challenging step in multilingual knowledge acquisition
    and information extraction. Existing approaches to transliteration generation
    require a large (>5000) number of training examples. This difficulty contrasts
    with transliteration discovery, a somewhat easier task that involves picking a
    plausible transliteration from a given list. In this work, we present a bootstrapping
    algorithm that uses constrained discovery to improve generation, and can be used
    with as few as 500 training examples, which we show can be sourced from annotators
    in a matter of hours. This opens the task to languages for which large number
    of training examples are unavailable. We evaluate transliteration generation performance
    itself, as well the improvement it brings to cross-lingual candidate generation
    for entity linking, a typical downstream task. We present a comprehensive evaluation
    of our approach on nine languages, each written in a unique script.
  address: Brussels, Belgium
  author:
  - first: Shyam
    full: Shyam Upadhyay
    id: shyam-upadhyay
    last: Upadhyay
  - first: Jordan
    full: Jordan Kodner
    id: jordan-kodner
    last: Kodner
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Shyam Upadhyay, Jordan Kodner, Dan Roth
  bibkey: upadhyay-etal-2018-bootstrapping
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1046
  month: October-November
  page_first: '501'
  page_last: '511'
  pages: "501\u2013511"
  paper_id: '46'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1046.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1046.jpg
  title: Bootstrapping Transliteration with Constrained Discovery for Low-Resource
    Languages
  title_html: Bootstrapping Transliteration with Constrained Discovery for Low-Resource
    Languages
  url: https://www.aclweb.org/anthology/D18-1046
  year: '2018'
D18-1047:
  abstract: Inducing multilingual word embeddings by learning a linear map between
    embedding spaces of different languages achieves remarkable accuracy on related
    languages. However, accuracy drops substantially when translating between distant
    languages. Given that languages exhibit differences in vocabulary, grammar, written
    form, or syntax, one would expect that embedding spaces of different languages
    have different structures especially for distant languages. With the goal of capturing
    such differences, we propose a method for learning neighborhood sensitive maps,
    NORMA. Our experiments show that NORMA outperforms current state-of-the-art methods
    for word translation between distant languages.
  address: Brussels, Belgium
  author:
  - first: Ndapa
    full: Ndapa Nakashole
    id: ndapandula-nakashole
    last: Nakashole
  author_string: Ndapa Nakashole
  bibkey: nakashole-2018-norma
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1047
  month: October-November
  page_first: '512'
  page_last: '522'
  pages: "512\u2013522"
  paper_id: '47'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1047.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1047.jpg
  title: 'NORMA: Neighborhood Sensitive Maps for Multilingual Word Embeddings'
  title_html: '<span class="acl-fixed-case">NORMA</span>: Neighborhood Sensitive Maps
    for Multilingual Word Embeddings'
  url: https://www.aclweb.org/anthology/D18-1047
  year: '2018'
D18-1048:
  abstract: Although end-to-end neural machine translation (NMT) has achieved remarkable
    progress in the recent years, the idea of adopting multi-pass decoding mechanism
    into conventional NMT is not well explored. In this paper, we propose a novel
    architecture called adaptive multi-pass decoder, which introduces a flexible multi-pass
    polishing mechanism to extend the capacity of NMT via reinforcement learning.
    More specifically, we adopt an extra policy network to automatically choose a
    suitable and effective number of decoding passes, according to the complexity
    of source sentences and the quality of the generated translations. Extensive experiments
    on Chinese-English translation demonstrate the effectiveness of our proposed adaptive
    multi-pass decoder upon the conventional NMT with a significant improvement about
    1.55 BLEU.
  address: Brussels, Belgium
  author:
  - first: Xinwei
    full: Xinwei Geng
    id: xinwei-geng
    last: Geng
  - first: Xiaocheng
    full: Xiaocheng Feng
    id: xiaocheng-feng
    last: Feng
  - first: Bing
    full: Bing Qin
    id: bing-qin
    last: Qin
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  author_string: Xinwei Geng, Xiaocheng Feng, Bing Qin, Ting Liu
  bibkey: geng-etal-2018-adaptive
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1048
  month: October-November
  page_first: '523'
  page_last: '532'
  pages: "523\u2013532"
  paper_id: '48'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1048.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1048.jpg
  title: Adaptive Multi-pass Decoder for Neural Machine Translation
  title_html: Adaptive Multi-pass Decoder for Neural Machine Translation
  url: https://www.aclweb.org/anthology/D18-1048
  year: '2018'
D18-1049:
  abstract: Although the Transformer translation model (Vaswani et al., 2017) has
    achieved state-of-the-art performance in a variety of translation tasks, how to
    use document-level context to deal with discourse phenomena problematic for Transformer
    still remains a challenge. In this work, we extend the Transformer model with
    a new context encoder to represent document-level context, which is then incorporated
    into the original encoder and decoder. As large-scale document-level parallel
    corpora are usually not available, we introduce a two-step training method to
    take full advantage of abundant sentence-level parallel corpora and limited document-level
    parallel corpora. Experiments on the NIST Chinese-English datasets and the IWSLT
    French-English datasets show that our approach improves over Transformer significantly.
  address: Brussels, Belgium
  author:
  - first: Jiacheng
    full: Jiacheng Zhang
    id: jiacheng-zhang
    last: Zhang
  - first: Huanbo
    full: Huanbo Luan
    id: huanbo-luan
    last: Luan
  - first: Maosong
    full: Maosong Sun
    id: maosong-sun
    last: Sun
  - first: Feifei
    full: Feifei Zhai
    id: feifei-zhai
    last: Zhai
  - first: Jingfang
    full: Jingfang Xu
    id: jingfang-xu
    last: Xu
  - first: Min
    full: Min Zhang
    id: min-zhang
    last: Zhang
  - first: Yang
    full: Yang Liu
    id: yang-liu-ict
    last: Liu
  author_string: Jiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei Zhai, Jingfang Xu,
    Min Zhang, Yang Liu
  bibkey: zhang-etal-2018-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1049
  month: October-November
  page_first: '533'
  page_last: '542'
  pages: "533\u2013542"
  paper_id: '49'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1049.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1049.jpg
  title: Improving the Transformer Translation Model with Document-Level Context
  title_html: Improving the Transformer Translation Model with Document-Level Context
  url: https://www.aclweb.org/anthology/D18-1049
  year: '2018'
D18-1050:
  abstract: Noisy or non-standard input text can cause disastrous mistranslations
    in most modern Machine Translation (MT) systems, and there has been growing research
    interest in creating noise-robust MT systems. However, as of yet there are no
    publicly available parallel corpora of with naturally occurring noisy inputs and
    translations, and thus previous work has resorted to evaluating on synthetically
    created datasets. In this paper, we propose a benchmark dataset for Machine Translation
    of Noisy Text (MTNT), consisting of noisy comments on Reddit (www.reddit.com)
    and professionally sourced translations. We commissioned translations of English
    comments into French and Japanese, as well as French and Japanese comments into
    English, on the order of 7k-37k sentences per language pair. We qualitatively
    and quantitatively examine the types of noise included in this dataset, then demonstrate
    that existing MT models fail badly on a number of noise-related phenomena, even
    after performing adaptation on a small training set of in-domain data. This indicates
    that this dataset can provide an attractive testbed for methods tailored to handling
    noisy text in MT.
  address: Brussels, Belgium
  author:
  - first: Paul
    full: Paul Michel
    id: paul-michel
    last: Michel
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  author_string: Paul Michel, Graham Neubig
  bibkey: michel-neubig-2018-mtnt
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1050
  month: October-November
  page_first: '543'
  page_last: '553'
  pages: "543\u2013553"
  paper_id: '50'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1050.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1050.jpg
  title: 'MTNT: A Testbed for Machine Translation of Noisy Text'
  title_html: '<span class="acl-fixed-case">MTNT</span>: A Testbed for Machine Translation
    of Noisy Text'
  url: https://www.aclweb.org/anthology/D18-1050
  year: '2018'
D18-1051:
  abstract: The SimpleQuestions dataset is one of the most commonly used benchmarks
    for studying single-relation factoid questions. In this paper, we present new
    evidence that this benchmark can be nearly solved by standard methods. First,
    we show that ambiguity in the data bounds performance at 83.4%; many questions
    have more than one equally plausible interpretation. Second, we introduce a baseline
    that sets a new state-of-the-art performance level at 78.1% accuracy, despite
    using standard methods. Finally, we report an empirical analysis showing that
    the upperbound is loose; roughly a quarter of the remaining errors are also not
    resolvable from the linguistic signal. Together, these results suggest that the
    SimpleQuestions dataset is nearly solved.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305204813
    type: video
    url: https://vimeo.com/305204813
  author:
  - first: Michael
    full: Michael Petrochuk
    id: michael-petrochuk
    last: Petrochuk
  - first: Luke
    full: Luke Zettlemoyer
    id: luke-zettlemoyer
    last: Zettlemoyer
  author_string: Michael Petrochuk, Luke Zettlemoyer
  bibkey: petrochuk-zettlemoyer-2018-simplequestions
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1051
  month: October-November
  page_first: '554'
  page_last: '558'
  pages: "554\u2013558"
  paper_id: '51'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1051.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1051.jpg
  title: 'SimpleQuestions Nearly Solved: A New Upperbound and Baseline Approach'
  title_html: '<span class="acl-fixed-case">S</span>imple<span class="acl-fixed-case">Q</span>uestions
    Nearly Solved: A New Upperbound and Baseline Approach'
  url: https://www.aclweb.org/anthology/D18-1051
  year: '2018'
D18-1052:
  abstract: 'We formalize a new modular variant of current question answering tasks
    by enforcing complete independence of the document encoder from the question encoder.
    This formulation addresses a key challenge in machine comprehension by building
    a standalone representation of the document discourse. It additionally leads to
    a significant scalability advantage since the encoding of the answer candidate
    phrases in the document can be pre-computed and indexed offline for efficient
    retrieval. We experiment with baseline models for the new task, which achieve
    a reasonable accuracy but significantly underperform unconstrained QA models.
    We invite the QA research community to engage in Phrase-Indexed Question Answering
    (PIQA, pika) for closing the gap. The leaderboard is at: nlp.cs.washington.edu/piqa'
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305205055
    type: video
    url: https://vimeo.com/305205055
  author:
  - first: Minjoon
    full: Minjoon Seo
    id: minjoon-seo
    last: Seo
  - first: Tom
    full: Tom Kwiatkowski
    id: tom-kwiatkowski
    last: Kwiatkowski
  - first: Ankur
    full: Ankur Parikh
    id: ankur-parikh
    last: Parikh
  - first: Ali
    full: Ali Farhadi
    id: ali-farhadi
    last: Farhadi
  - first: Hannaneh
    full: Hannaneh Hajishirzi
    id: hannaneh-hajishirzi
    last: Hajishirzi
  author_string: Minjoon Seo, Tom Kwiatkowski, Ankur Parikh, Ali Farhadi, Hannaneh
    Hajishirzi
  bibkey: seo-etal-2018-phrase
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1052
  month: October-November
  page_first: '559'
  page_last: '564'
  pages: "559\u2013564"
  paper_id: '52'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1052.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1052.jpg
  title: 'Phrase-Indexed Question Answering: A New Challenge for Scalable Document
    Comprehension'
  title_html: 'Phrase-Indexed Question Answering: A New Challenge for Scalable Document
    Comprehension'
  url: https://www.aclweb.org/anthology/D18-1052
  year: '2018'
D18-1053:
  abstract: Recently, open-domain question answering (QA) has been combined with machine
    comprehension models to find answers in a large knowledge source. As open-domain
    QA requires retrieving relevant documents from text corpora to answer questions,
    its performance largely depends on the performance of document retrievers. However,
    since traditional information retrieval systems are not effective in obtaining
    documents with a high probability of containing answers, they lower the performance
    of QA systems. Simply extracting more documents increases the number of irrelevant
    documents, which also degrades the performance of QA systems. In this paper, we
    introduce Paragraph Ranker which ranks paragraphs of retrieved documents for a
    higher answer recall with less noise. We show that ranking paragraphs and aggregating
    answers using Paragraph Ranker improves performance of open-domain QA pipeline
    on the four open-domain QA datasets by 7.8% on average.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305205289
    type: video
    url: https://vimeo.com/305205289
  author:
  - first: Jinhyuk
    full: Jinhyuk Lee
    id: jinhyuk-lee
    last: Lee
  - first: Seongjun
    full: Seongjun Yun
    id: seongjun-yun
    last: Yun
  - first: Hyunjae
    full: Hyunjae Kim
    id: hyunjae-kim
    last: Kim
  - first: Miyoung
    full: Miyoung Ko
    id: miyoung-ko
    last: Ko
  - first: Jaewoo
    full: Jaewoo Kang
    id: jaewoo-kang
    last: Kang
  author_string: Jinhyuk Lee, Seongjun Yun, Hyunjae Kim, Miyoung Ko, Jaewoo Kang
  bibkey: lee-etal-2018-ranking
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1053
  month: October-November
  page_first: '565'
  page_last: '569'
  pages: "565\u2013569"
  paper_id: '53'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1053.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1053.jpg
  title: Ranking Paragraphs for Improving Answer Recall in Open-Domain Question Answering
  title_html: Ranking Paragraphs for Improving Answer Recall in Open-Domain Question
    Answering
  url: https://www.aclweb.org/anthology/D18-1053
  year: '2018'
D18-1054:
  abstract: "In recent years many deep neural networks have been proposed to solve\
    \ Reading Comprehension (RC) tasks. Most of these models suffer from reasoning\
    \ over long documents and do not trivially generalize to cases where the answer\
    \ is not present as a span in a given document. We present a novel neural-based\
    \ architecture that is capable of extracting relevant regions based on a given\
    \ question-document pair and generating a well-formed answer. To show the effectiveness\
    \ of our architecture, we conducted several experiments on the recently proposed\
    \ and challenging RC dataset \u2018NarrativeQA\u2019. The proposed architecture\
    \ outperforms state-of-the-art results by 12.62% (ROUGE-L) relative improvement."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1054.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1054.Attachment.zip
  - filename: https://vimeo.com/305205548
    type: video
    url: https://vimeo.com/305205548
  author:
  - first: Sathish Reddy
    full: Sathish Reddy Indurthi
    id: sathish-reddy-indurthi
    last: Indurthi
  - first: Seunghak
    full: Seunghak Yu
    id: seunghak-yu
    last: Yu
  - first: Seohyun
    full: Seohyun Back
    id: seohyun-back
    last: Back
  - first: Heriberto
    full: "Heriberto Cuay\xE1huitl"
    id: heriberto-cuayahuitl
    last: "Cuay\xE1huitl"
  author_string: "Sathish Reddy Indurthi, Seunghak Yu, Seohyun Back, Heriberto Cuay\xE1\
    huitl"
  bibkey: indurthi-etal-2018-cut
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1054
  month: October-November
  page_first: '570'
  page_last: '575'
  pages: "570\u2013575"
  paper_id: '54'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1054.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1054.jpg
  title: 'Cut to the Chase: A Context Zoom-in Network for Reading Comprehension'
  title_html: 'Cut to the Chase: A Context Zoom-in Network for Reading Comprehension'
  url: https://www.aclweb.org/anthology/D18-1054
  year: '2018'
D18-1055:
  abstract: 'State-of-the-art systems in deep question answering proceed as follows:
    (1)an initial document retrieval selects relevant documents, which (2) are then
    processed by a neural network in order to extract the final answer. Yet the exact
    interplay between both components is poorly understood, especially concerning
    the number of candidate documents that should be retrieved. We show that choosing
    a static number of documents - as used in prior research - suffers from a noise-information
    trade-off and yields suboptimal results. As a remedy, we propose an adaptive document
    retrieval model. This learns the optimal candidate number for document retrieval,
    conditional on the size of the corpus and the query. We report extensive experimental
    results showing that our adaptive approach outperforms state-of-the-art methods
    on multiple benchmark datasets, as well as in the context of corpora with variable
    sizes.'
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305205847
    type: video
    url: https://vimeo.com/305205847
  author:
  - first: Bernhard
    full: Bernhard Kratzwald
    id: bernhard-kratzwald
    last: Kratzwald
  - first: Stefan
    full: Stefan Feuerriegel
    id: stefan-feuerriegel
    last: Feuerriegel
  author_string: Bernhard Kratzwald, Stefan Feuerriegel
  bibkey: kratzwald-feuerriegel-2018-adaptive
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1055
  month: October-November
  page_first: '576'
  page_last: '581'
  pages: "576\u2013581"
  paper_id: '55'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1055.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1055.jpg
  title: Adaptive Document Retrieval for Deep Question Answering
  title_html: Adaptive Document Retrieval for Deep Question Answering
  url: https://www.aclweb.org/anthology/D18-1055
  year: '2018'
D18-1056:
  abstract: 'This paper presents a challenge to the community: Generative adversarial
    networks (GANs) can perfectly align independent English word embeddings induced
    using the same algorithm, based on distributional information alone; but fails
    to do so, for two different embeddings algorithms. Why is that? We believe understanding
    why, is key to understand both modern word embedding algorithms and the limitations
    and instability dynamics of GANs. This paper shows that (a) in all these cases,
    where alignment fails, there exists a linear transform between the two embeddings
    (so algorithm biases do not lead to non-linear differences), and (b) similar effects
    can not easily be obtained by varying hyper-parameters. One plausible suggestion
    based on our initial experiments is that the differences in the inductive biases
    of the embedding algorithms lead to an optimization landscape that is riddled
    with local optima, leading to a very small basin of convergence, but we present
    this more as a challenge paper than a technical contribution.'
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305196498
    type: video
    url: https://vimeo.com/305196498
  author:
  - first: Mareike
    full: Mareike Hartmann
    id: mareike-hartmann
    last: Hartmann
  - first: Yova
    full: Yova Kementchedjhieva
    id: yova-kementchedjhieva
    last: Kementchedjhieva
  - first: Anders
    full: "Anders S\xF8gaard"
    id: anders-sogaard
    last: "S\xF8gaard"
  author_string: "Mareike Hartmann, Yova Kementchedjhieva, Anders S\xF8gaard"
  bibkey: hartmann-etal-2018-unsupervised
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1056
  month: October-November
  page_first: '582'
  page_last: '586'
  pages: "582\u2013586"
  paper_id: '56'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1056.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1056.jpg
  title: Why is unsupervised alignment of English embeddings from different algorithms
    so hard?
  title_html: Why is unsupervised alignment of <span class="acl-fixed-case">E</span>nglish
    embeddings from different algorithms so hard?
  url: https://www.aclweb.org/anthology/D18-1056
  year: '2018'
D18-1057:
  abstract: Most models for learning word embeddings are trained based on the context
    information of words, more precisely first order co-occurrence relations. In this
    paper, a metric is designed to estimate second order co-occurrence relations based
    on context overlap. The estimated values are further used as the augmented data
    to enhance the learning of word embeddings by joint training with existing neural
    word embedding models. Experimental results show that better word vectors can
    be obtained for word similarity tasks and some downstream NLP tasks by the enhanced
    approach.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305196755
    type: video
    url: https://vimeo.com/305196755
  author:
  - first: Yimeng
    full: Yimeng Zhuang
    id: yimeng-zhuang
    last: Zhuang
  - first: Jinghui
    full: Jinghui Xie
    id: jinghui-xie
    last: Xie
  - first: Yinhe
    full: Yinhe Zheng
    id: yinhe-zheng
    last: Zheng
  - first: Xuan
    full: Xuan Zhu
    id: xuan-zhu
    last: Zhu
  author_string: Yimeng Zhuang, Jinghui Xie, Yinhe Zheng, Xuan Zhu
  bibkey: zhuang-etal-2018-quantifying
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1057
  month: October-November
  page_first: '587'
  page_last: '593'
  pages: "587\u2013593"
  paper_id: '57'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1057.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1057.jpg
  title: Quantifying Context Overlap for Training Word Embeddings
  title_html: Quantifying Context Overlap for Training Word Embeddings
  url: https://www.aclweb.org/anthology/D18-1057
  year: '2018'
D18-1058:
  abstract: Capturing the semantic relations of words in a vector space contributes
    to many natural language processing tasks. One promising approach exploits lexico-syntactic
    patterns as features of word pairs. In this paper, we propose a novel model of
    this pattern-based approach, neural latent relational analysis (NLRA). NLRA can
    generalize co-occurrences of word pairs and lexico-syntactic patterns, and obtain
    embeddings of the word pairs that do not co-occur. This overcomes the critical
    data sparseness problem encountered in previous pattern-based models. Our experimental
    results on measuring relational similarity demonstrate that NLRA outperforms the
    previous pattern-based models. In addition, when combined with a vector offset
    model, NLRA achieves a performance comparable to that of the state-of-the-art
    model that exploits additional semantic relational data.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305197006
    type: video
    url: https://vimeo.com/305197006
  author:
  - first: Koki
    full: Koki Washio
    id: koki-washio
    last: Washio
  - first: Tsuneaki
    full: Tsuneaki Kato
    id: tsuneaki-kato
    last: Kato
  author_string: Koki Washio, Tsuneaki Kato
  bibkey: washio-kato-2018-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1058
  month: October-November
  page_first: '594'
  page_last: '600'
  pages: "594\u2013600"
  paper_id: '58'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1058.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1058.jpg
  title: Neural Latent Relational Analysis to Capture Lexical Semantic Relations in
    a Vector Space
  title_html: Neural Latent Relational Analysis to Capture Lexical Semantic Relations
    in a Vector Space
  url: https://www.aclweb.org/anthology/D18-1058
  year: '2018'
D18-1059:
  abstract: "We approach the problem of generalizing pre-trained word embeddings beyond\
    \ fixed-size vocabularies without using additional contextual information. We\
    \ propose a subword-level word vector generation model that views words as bags\
    \ of character n-grams. The model is simple, fast to train and provides good vectors\
    \ for rare or unseen words. Experiments show that our model achieves state-of-the-art\
    \ performances in English word similarity task and in joint prediction of part-of-speech\
    \ tag and morphosyntactic attributes in 23 languages, suggesting our model\u2019\
    s ability in capturing the relationship between words\u2019 textual representations\
    \ and their embeddings.-grams. The model is simple, fast to train and provides\
    \ good vectors for rare or unseen words. Experiments show that our model achieves\
    \ state-of-the-art performances in English word similarity task and in joint prediction\
    \ of part-of-speech tag and morphosyntactic attributes in 23 languages, suggesting\
    \ our model\u2019s ability in capturing the relationship between words\u2019 textual\
    \ representations and their embeddings."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305197257
    type: video
    url: https://vimeo.com/305197257
  author:
  - first: Jinman
    full: Jinman Zhao
    id: jinman-zhao
    last: Zhao
  - first: Sidharth
    full: Sidharth Mudgal
    id: sidharth-mudgal
    last: Mudgal
  - first: Yingyu
    full: Yingyu Liang
    id: yingyu-liang
    last: Liang
  author_string: Jinman Zhao, Sidharth Mudgal, Yingyu Liang
  bibkey: zhao-etal-2018-generalizing
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1059
  month: October-November
  page_first: '601'
  page_last: '606'
  pages: "601\u2013606"
  paper_id: '59'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1059.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1059.jpg
  title: Generalizing Word Embeddings using Bag of Subwords
  title_html: Generalizing Word Embeddings using Bag of Subwords
  url: https://www.aclweb.org/anthology/D18-1059
  year: '2018'
D18-1060:
  abstract: We present end-to-end neural models for detecting metaphorical word use
    in context. We show that relatively standard BiLSTM models which operate on complete
    sentences work well in this setting, in comparison to previous work that used
    more restricted forms of linguistic context. These models establish a new state-of-the-art
    on existing verb metaphor detection benchmarks, and show strong performance on
    jointly predicting the metaphoricity of all words in a running text.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305197464
    type: video
    url: https://vimeo.com/305197464
  author:
  - first: Ge
    full: Ge Gao
    id: ge-gao
    last: Gao
  - first: Eunsol
    full: Eunsol Choi
    id: eunsol-choi
    last: Choi
  - first: Yejin
    full: Yejin Choi
    id: yejin-choi
    last: Choi
  - first: Luke
    full: Luke Zettlemoyer
    id: luke-zettlemoyer
    last: Zettlemoyer
  author_string: Ge Gao, Eunsol Choi, Yejin Choi, Luke Zettlemoyer
  bibkey: gao-etal-2018-neural-metaphor
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1060
  month: October-November
  page_first: '607'
  page_last: '613'
  pages: "607\u2013613"
  paper_id: '60'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1060.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1060.jpg
  title: Neural Metaphor Detection in Context
  title_html: Neural Metaphor Detection in Context
  url: https://www.aclweb.org/anthology/D18-1060
  year: '2018'
D18-1061:
  abstract: a cross-lingual neural part-of-speech tagger that learns from disparate
    sources of distant supervision, and realistically scales to hundreds of low-resource
    languages. The model exploits annotation projection, instance selection, tag dictionaries,
    morphological lexicons, and distributed representations, all in a uniform framework.
    The approach is simple, yet surprisingly effective, resulting in a new state of
    the art without access to any gold annotated data.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305211701
    type: video
    url: https://vimeo.com/305211701
  author:
  - first: Barbara
    full: Barbara Plank
    id: barbara-plank
    last: Plank
  - first: "\u017Deljko"
    full: "\u017Deljko Agi\u0107"
    id: zeljko-agic
    last: "Agi\u0107"
  author_string: "Barbara Plank, \u017Deljko Agi\u0107"
  bibkey: plank-agic-2018-distant
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1061
  month: October-November
  page_first: '614'
  page_last: '620'
  pages: "614\u2013620"
  paper_id: '61'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1061.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1061.jpg
  title: Distant Supervision from Disparate Sources for Low-Resource Part-of-Speech
    Tagging
  title_html: Distant Supervision from Disparate Sources for Low-Resource Part-of-Speech
    Tagging
  url: https://www.aclweb.org/anthology/D18-1061
  year: '2018'
D18-1062:
  abstract: Bilingual lexicon extraction has been studied for decades and most previous
    methods have relied on parallel corpora or bilingual dictionaries. Recent studies
    have shown that it is possible to build a bilingual dictionary by aligning monolingual
    word embedding spaces in an unsupervised way. With the recent advances in generative
    models, we propose a novel approach which builds cross-lingual dictionaries via
    latent variable models and adversarial training with no parallel corpora. To demonstrate
    the effectiveness of our approach, we evaluate our approach on several language
    pairs and the experimental results show that our model could achieve competitive
    and even superior performance compared with several state-of-the-art models.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305211999
    type: video
    url: https://vimeo.com/305211999
  author:
  - first: Zi-Yi
    full: Zi-Yi Dou
    id: zi-yi-dou
    last: Dou
  - first: Zhi-Hao
    full: Zhi-Hao Zhou
    id: zhi-hao-zhou
    last: Zhou
  - first: Shujian
    full: Shujian Huang
    id: shujian-huang
    last: Huang
  author_string: Zi-Yi Dou, Zhi-Hao Zhou, Shujian Huang
  bibkey: dou-etal-2018-unsupervised
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1062
  month: October-November
  page_first: '621'
  page_last: '626'
  pages: "621\u2013626"
  paper_id: '62'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1062.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1062.jpg
  title: Unsupervised Bilingual Lexicon Induction via Latent Variable Models
  title_html: Unsupervised Bilingual Lexicon Induction via Latent Variable Models
  url: https://www.aclweb.org/anthology/D18-1062
  year: '2018'
D18-1063:
  abstract: "Word translation, or bilingual dictionary induction, is an important\
    \ capability that impacts many multilingual language processing tasks. Recent\
    \ research has shown that word translation can be achieved in an unsupervised\
    \ manner, without parallel seed dictionaries or aligned corpora. However, state\
    \ of the art methods unsupervised bilingual dictionary induction are based on\
    \ generative adversarial models, and as such suffer from their well known problems\
    \ of instability and hyper-parameter sensitivity. We present a statistical dependency-based\
    \ approach to bilingual dictionary induction that is unsupervised \u2013 no seed\
    \ dictionary or parallel corpora required; and introduces no adversary \u2013\
    \ therefore being much easier to train. Our method performs comparably to adversarial\
    \ alternatives and outperforms prior non-adversarial methods."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305212238
    type: video
    url: https://vimeo.com/305212238
  author:
  - first: Tanmoy
    full: Tanmoy Mukherjee
    id: tanmoy-mukherjee
    last: Mukherjee
  - first: Makoto
    full: Makoto Yamada
    id: makoto-yamada
    last: Yamada
  - first: Timothy
    full: Timothy Hospedales
    id: timothy-hospedales
    last: Hospedales
  author_string: Tanmoy Mukherjee, Makoto Yamada, Timothy Hospedales
  bibkey: mukherjee-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1063
  month: October-November
  page_first: '627'
  page_last: '632'
  pages: "627\u2013632"
  paper_id: '63'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1063.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1063.jpg
  title: Learning Unsupervised Word Translations Without Adversaries
  title_html: Learning Unsupervised Word Translations Without Adversaries
  url: https://www.aclweb.org/anthology/D18-1063
  year: '2018'
D18-1064:
  abstract: This paper proposes an adversarial training method for the multi-task
    and multi-lingual joint modeling needed for utterance intent classification. In
    joint modeling, common knowledge can be efficiently utilized among multiple tasks
    or multiple languages. This is achieved by introducing both language-specific
    networks shared among different tasks and task-specific networks shared among
    different languages. However, the shared networks are often specialized in majority
    tasks or languages, so performance degradation must be expected for some minor
    data sets. In order to improve the invariance of shared networks, the proposed
    method introduces both language-specific task adversarial networks and task-specific
    language adversarial networks; both are leveraged for purging the task or language
    dependencies of the shared networks. The effectiveness of the adversarial training
    proposal is demonstrated using Japanese and English data sets for three different
    utterance intent classification tasks.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305212477
    type: video
    url: https://vimeo.com/305212477
  author:
  - first: Ryo
    full: Ryo Masumura
    id: ryo-masumura
    last: Masumura
  - first: Yusuke
    full: Yusuke Shinohara
    id: yusuke-shinohara
    last: Shinohara
  - first: Ryuichiro
    full: Ryuichiro Higashinaka
    id: ryuichiro-higashinaka
    last: Higashinaka
  - first: Yushi
    full: Yushi Aono
    id: yushi-aono
    last: Aono
  author_string: Ryo Masumura, Yusuke Shinohara, Ryuichiro Higashinaka, Yushi Aono
  bibkey: masumura-etal-2018-adversarial
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1064
  month: October-November
  page_first: '633'
  page_last: '639'
  pages: "633\u2013639"
  paper_id: '64'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1064.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1064.jpg
  title: Adversarial Training for Multi-task and Multi-lingual Joint Modeling of Utterance
    Intent Classification
  title_html: Adversarial Training for Multi-task and Multi-lingual Joint Modeling
    of Utterance Intent Classification
  url: https://www.aclweb.org/anthology/D18-1064
  year: '2018'
D18-1065:
  abstract: In this paper we show that a simple beam approximation of the joint distribution
    between attention and output is an easy, accurate, and efficient attention mechanism
    for sequence to sequence learning. The method combines the advantage of sharp
    focus in hard attention and the implementation ease of soft attention. On five
    translation tasks we show effortless and consistent gains in BLEU compared to
    existing attention mechanisms.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305212816
    type: video
    url: https://vimeo.com/305212816
  author:
  - first: Shiv
    full: Shiv Shankar
    id: shiv-shankar
    last: Shankar
  - first: Siddhant
    full: Siddhant Garg
    id: siddhant-garg
    last: Garg
  - first: Sunita
    full: Sunita Sarawagi
    id: sunita-sarawagi
    last: Sarawagi
  author_string: Shiv Shankar, Siddhant Garg, Sunita Sarawagi
  bibkey: shankar-etal-2018-surprisingly
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1065
  month: October-November
  page_first: '640'
  page_last: '645'
  pages: "640\u2013645"
  paper_id: '65'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1065.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1065.jpg
  title: Surprisingly Easy Hard-Attention for Sequence to Sequence Learning
  title_html: Surprisingly Easy Hard-Attention for Sequence to Sequence Learning
  url: https://www.aclweb.org/anthology/D18-1065
  year: '2018'
D18-1066:
  abstract: We present a neural network-based joint approach for emotion classification
    and emotion cause detection, which attempts to capture mutual benefits across
    the two sub-tasks of emotion analysis. Considering that emotion classification
    and emotion cause detection need different kinds of features (affective and event-based
    separately), we propose a joint encoder which uses a unified framework to extract
    features for both sub-tasks and a joint model trainer which simultaneously learns
    two models for the two sub-tasks separately. Our experiments on Chinese microblogs
    show that the joint approach is very promising.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306358514
    type: video
    url: https://vimeo.com/306358514
  author:
  - first: Ying
    full: Ying Chen
    id: ying-chen
    last: Chen
  - first: Wenjun
    full: Wenjun Hou
    id: wenjun-hou
    last: Hou
  - first: Xiyao
    full: Xiyao Cheng
    id: xiyao-cheng
    last: Cheng
  - first: Shoushan
    full: Shoushan Li
    id: shoushan-li
    last: Li
  author_string: Ying Chen, Wenjun Hou, Xiyao Cheng, Shoushan Li
  bibkey: chen-etal-2018-joint
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1066
  month: October-November
  page_first: '646'
  page_last: '651'
  pages: "646\u2013651"
  paper_id: '66'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1066.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1066.jpg
  title: Joint Learning for Emotion Classification and Emotion Cause Detection
  title_html: Joint Learning for Emotion Classification and Emotion Cause Detection
  url: https://www.aclweb.org/anthology/D18-1066
  year: '2018'
D18-1067:
  abstract: Identifying optimistic and pessimistic viewpoints and users from Twitter
    is useful for providing better social support to those who need such support,
    and for minimizing the negative influence among users and maximizing the spread
    of positive attitudes and ideas. In this paper, we explore a range of deep learning
    models to predict optimism and pessimism in Twitter at both tweet and user level
    and show that these models substantially outperform traditional machine learning
    classifiers used in prior work. In addition, we show evidence that a sentiment
    classifier would not be sufficient for accurately predicting optimism and pessimism
    in Twitter. Last, we study the verb tense usage as well as the presence of polarity
    words in optimistic and pessimistic tweets.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306359438
    type: video
    url: https://vimeo.com/306359438
  author:
  - first: Cornelia
    full: Cornelia Caragea
    id: cornelia-caragea
    last: Caragea
  - first: Liviu P.
    full: Liviu P. Dinu
    id: liviu-p-dinu
    last: Dinu
  - first: Bogdan
    full: Bogdan Dumitru
    id: bogdan-dumitru
    last: Dumitru
  author_string: Cornelia Caragea, Liviu P. Dinu, Bogdan Dumitru
  bibkey: caragea-etal-2018-exploring
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1067
  month: October-November
  page_first: '652'
  page_last: '658'
  pages: "652\u2013658"
  paper_id: '67'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1067.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1067.jpg
  title: Exploring Optimism and Pessimism in Twitter Using Deep Learning
  title_html: Exploring Optimism and Pessimism in Twitter Using Deep Learning
  url: https://www.aclweb.org/anthology/D18-1067
  year: '2018'
D18-1068:
  abstract: "Newspapers need to attract readers with headlines, anticipating their\
    \ readers\u2019 preferences. These preferences rely on topical, structural, and\
    \ lexical factors. We model each of these factors in a multi-task GRU network\
    \ to predict headline popularity. We find that pre-trained word embeddings provide\
    \ significant improvements over untrained embeddings, as do the combination of\
    \ two auxiliary tasks, news-section prediction and part-of-speech tagging. However,\
    \ we also find that performance is very similar to that of a simple Logistic Regression\
    \ model over character n-grams. Feature analysis reveals structural patterns of\
    \ headline popularity, including the use of forward-looking deictic expressions\
    \ and second person pronouns."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306360116
    type: video
    url: https://vimeo.com/306360116
  author:
  - first: Sotiris
    full: Sotiris Lamprinidis
    id: sotiris-lamprinidis
    last: Lamprinidis
  - first: Daniel
    full: Daniel Hardt
    id: daniel-hardt
    last: Hardt
  - first: Dirk
    full: Dirk Hovy
    id: dirk-hovy
    last: Hovy
  author_string: Sotiris Lamprinidis, Daniel Hardt, Dirk Hovy
  bibkey: lamprinidis-etal-2018-predicting
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1068
  month: October-November
  page_first: '659'
  page_last: '664'
  pages: "659\u2013664"
  paper_id: '68'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1068.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1068.jpg
  title: Predicting News Headline Popularity with Syntactic and Semantic Knowledge
    Using Multi-Task Learning
  title_html: Predicting News Headline Popularity with Syntactic and Semantic Knowledge
    Using Multi-Task Learning
  url: https://www.aclweb.org/anthology/D18-1068
  year: '2018'
D18-1069:
  abstract: Inferring the agreement/disagreement relation in debates, especially in
    online debates, is one of the fundamental tasks in argumentation mining. The expressions
    of agreement/disagreement usually rely on argumentative expressions in text as
    well as interactions between participants in debates. Previous works usually lack
    the capability of jointly modeling these two factors. To alleviate this problem,
    this paper proposes a hybrid neural attention model which combines self and cross
    attention mechanism to locate salient part from textual context and interaction
    between users. Experimental results on three (dis)agreement inference datasets
    show that our model outperforms the state-of-the-art models.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306360792
    type: video
    url: https://vimeo.com/306360792
  author:
  - first: Di
    full: Di Chen
    id: di-chen
    last: Chen
  - first: Jiachen
    full: Jiachen Du
    id: jiachen-du
    last: Du
  - first: Lidong
    full: Lidong Bing
    id: lidong-bing
    last: Bing
  - first: Ruifeng
    full: Ruifeng Xu
    id: ruifeng-xu
    last: Xu
  author_string: Di Chen, Jiachen Du, Lidong Bing, Ruifeng Xu
  bibkey: chen-etal-2018-hybrid
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1069
  month: October-November
  page_first: '665'
  page_last: '670'
  pages: "665\u2013670"
  paper_id: '69'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1069.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1069.jpg
  title: Hybrid Neural Attention for Agreement/Disagreement Inference in Online Debates
  title_html: Hybrid Neural Attention for Agreement/Disagreement Inference in Online
    Debates
  url: https://www.aclweb.org/anthology/D18-1069
  year: '2018'
D18-1070:
  abstract: 'Most text-classification approaches represent the input based on textual
    features, either feature-based or continuous. However, this ignores strong non-linguistic
    similarities like homophily: people within a demographic group use language more
    similar to each other than to non-group members. We use homophily cues to retrofit
    text-based author representations with non-linguistic information, and introduce
    a trade-off parameter. This approach increases in-class similarity between authors,
    and improves classification performance by making classes more linearly separable.
    We evaluate the effect of our method on two author-attribute prediction tasks
    with various training-set sizes and parameter settings. We find that our method
    can significantly improve classification performance, especially when the number
    of labels is large and limited labeled data is available. It is potentially applicable
    as preprocessing step to any text-classification task.'
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306361301
    type: video
    url: https://vimeo.com/306361301
  author:
  - first: Dirk
    full: Dirk Hovy
    id: dirk-hovy
    last: Hovy
  - first: Tommaso
    full: Tommaso Fornaciari
    id: tommaso-fornaciari
    last: Fornaciari
  author_string: Dirk Hovy, Tommaso Fornaciari
  bibkey: hovy-fornaciari-2018-increasing
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1070
  month: October-November
  page_first: '671'
  page_last: '677'
  pages: "671\u2013677"
  paper_id: '70'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1070.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1070.jpg
  title: Increasing In-Class Similarity by Retrofitting Embeddings with Demographic
    Information
  title_html: Increasing In-Class Similarity by Retrofitting Embeddings with Demographic
    Information
  url: https://www.aclweb.org/anthology/D18-1070
  year: '2018'
D18-1071:
  abstract: Traditional neural language models tend to generate generic replies with
    poor logic and no emotion. In this paper, a syntactically constrained bidirectional-asynchronous
    approach for emotional conversation generation (E-SCBA) is proposed to address
    this issue. In our model, pre-generated emotion keywords and topic keywords are
    asynchronously introduced into the process of decoding. It is much different from
    most existing methods which generate replies from the first word to the last.
    Through experiments, the results indicate that our approach not only improves
    the diversity of replies, but gains a boost on both logic and emotion compared
    with baselines.
  address: Brussels, Belgium
  author:
  - first: Jingyuan
    full: Jingyuan Li
    id: jingyuan-li
    last: Li
  - first: Xiao
    full: Xiao Sun
    id: xiao-sun
    last: Sun
  author_string: Jingyuan Li, Xiao Sun
  bibkey: li-sun-2018-syntactically
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1071
  month: October-November
  page_first: '678'
  page_last: '683'
  pages: "678\u2013683"
  paper_id: '71'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1071.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1071.jpg
  title: A Syntactically Constrained Bidirectional-Asynchronous Approach for Emotional
    Conversation Generation
  title_html: A Syntactically Constrained Bidirectional-Asynchronous Approach for
    Emotional Conversation Generation
  url: https://www.aclweb.org/anthology/D18-1071
  year: '2018'
D18-1072:
  abstract: The lack of labeled data is one of the main challenges when building a
    task-oriented dialogue system. Existing dialogue datasets usually rely on human
    labeling, which is expensive, limited in size, and in low coverage. In this paper,
    we instead propose our framework auto-dialabel to automatically cluster the dialogue
    intents and slots. In this framework, we collect a set of context features, leverage
    an autoencoder for feature assembly, and adapt a dynamic hierarchical clustering
    method for intent and slot labeling. Experimental results show that our framework
    can promote human labeling cost to a great extent, achieve good intent clustering
    accuracy (84.1%), and provide reasonable and instructive slot labeling results.
  address: Brussels, Belgium
  author:
  - first: Chen
    full: Chen Shi
    id: chen-shi
    last: Shi
  - first: Qi
    full: Qi Chen
    id: qi-chen
    last: Chen
  - first: Lei
    full: Lei Sha
    id: lei-sha
    last: Sha
  - first: Sujian
    full: Sujian Li
    id: sujian-li
    last: Li
  - first: Xu
    full: Xu Sun
    id: xu-sun
    last: Sun
  - first: Houfeng
    full: Houfeng Wang
    id: houfeng-wang
    last: Wang
  - first: Lintao
    full: Lintao Zhang
    id: lintao-zhang
    last: Zhang
  author_string: Chen Shi, Qi Chen, Lei Sha, Sujian Li, Xu Sun, Houfeng Wang, Lintao
    Zhang
  bibkey: shi-etal-2018-auto
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1072
  month: October-November
  page_first: '684'
  page_last: '689'
  pages: "684\u2013689"
  paper_id: '72'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1072.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1072.jpg
  title: 'Auto-Dialabel: Labeling Dialogue Data with Unsupervised Learning'
  title_html: 'Auto-Dialabel: Labeling Dialogue Data with Unsupervised Learning'
  url: https://www.aclweb.org/anthology/D18-1072
  year: '2018'
D18-1073:
  abstract: The use of connectionist approaches in conversational agents has been
    progressing rapidly due to the availability of large corpora. However current
    generative dialogue models often lack coherence and are content poor. This work
    proposes an architecture to incorporate unstructured knowledge sources to enhance
    the next utterance prediction in chit-chat type of generative dialogue models.
    We focus on Sequence-to-Sequence (Seq2Seq) conversational agents trained with
    the Reddit News dataset, and consider incorporating external knowledge from Wikipedia
    summaries as well as from the NELL knowledge base. Our experiments show faster
    training time and improved perplexity when leveraging external knowledge.
  address: Brussels, Belgium
  author:
  - first: Prasanna
    full: Prasanna Parthasarathi
    id: prasanna-parthasarathi
    last: Parthasarathi
  - first: Joelle
    full: Joelle Pineau
    id: joelle-pineau
    last: Pineau
  author_string: Prasanna Parthasarathi, Joelle Pineau
  bibkey: parthasarathi-pineau-2018-extending
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1073
  month: October-November
  page_first: '690'
  page_last: '695'
  pages: "690\u2013695"
  paper_id: '73'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1073.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1073.jpg
  title: Extending Neural Generative Conversational Model using External Knowledge
    Sources
  title_html: Extending Neural Generative Conversational Model using External Knowledge
    Sources
  url: https://www.aclweb.org/anthology/D18-1073
  year: '2018'
D18-1074:
  abstract: "Categorizing patient\u2019s intentions in conversational assessment can\
    \ help decision making in clinical treatments. Many conversation corpora span\
    \ broaden a series of time stages. However, it is not clear that how the themes\
    \ shift in the conversation impact on the performance of human intention categorization\
    \ (eg., patients might show different behaviors during the beginning versus the\
    \ end). This paper proposes a method that models the temporal factor by using\
    \ domain adaptation on clinical dialogue corpora, Motivational Interviewing (MI).\
    \ We deploy Bi-LSTM and topic model jointly to learn language usage change across\
    \ different time sessions. We conduct experiments on the MI corpora to show the\
    \ promising improvement after considering temporality in the classification task."
  address: Brussels, Belgium
  author:
  - first: Xiaolei
    full: Xiaolei Huang
    id: xiaolei-huang
    last: Huang
  - first: Lixing
    full: Lixing Liu
    id: lixing-liu
    last: Liu
  - first: Kate
    full: Kate Carey
    id: kate-carey
    last: Carey
  - first: Joshua
    full: Joshua Woolley
    id: joshua-woolley
    last: Woolley
  - first: Stefan
    full: Stefan Scherer
    id: stefan-scherer
    last: Scherer
  - first: Brian
    full: Brian Borsari
    id: brian-borsari
    last: Borsari
  author_string: Xiaolei Huang, Lixing Liu, Kate Carey, Joshua Woolley, Stefan Scherer,
    Brian Borsari
  bibkey: huang-etal-2018-modeling
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1074
  month: October-November
  page_first: '696'
  page_last: '701'
  pages: "696\u2013701"
  paper_id: '74'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1074.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1074.jpg
  title: Modeling Temporality of Human Intentions by Domain Adaptation
  title_html: Modeling Temporality of Human Intentions by Domain Adaptation
  url: https://www.aclweb.org/anthology/D18-1074
  year: '2018'
D18-1075:
  abstract: Generating semantically coherent responses is still a major challenge
    in dialogue generation. Different from conventional text generation tasks, the
    mapping between inputs and responses in conversations is more complicated, which
    highly demands the understanding of utterance-level semantic dependency, a relation
    between the whole meanings of inputs and outputs. To address this problem, we
    propose an Auto-Encoder Matching (AEM) model to learn such dependency. The model
    contains two auto-encoders and one mapping module. The auto-encoders learn the
    semantic representations of inputs and responses, and the mapping module learns
    to connect the utterance-level representations. Experimental results from automatic
    and human evaluations demonstrate that our model is capable of generating responses
    of high coherence and fluency compared to baseline models.
  address: Brussels, Belgium
  author:
  - first: Liangchen
    full: Liangchen Luo
    id: liangchen-luo
    last: Luo
  - first: Jingjing
    full: Jingjing Xu
    id: jingjing-xu
    last: Xu
  - first: Junyang
    full: Junyang Lin
    id: junyang-lin
    last: Lin
  - first: Qi
    full: Qi Zeng
    id: qi-zeng
    last: Zeng
  - first: Xu
    full: Xu Sun
    id: xu-sun
    last: Sun
  author_string: Liangchen Luo, Jingjing Xu, Junyang Lin, Qi Zeng, Xu Sun
  bibkey: luo-etal-2018-auto
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1075
  month: October-November
  page_first: '702'
  page_last: '707'
  pages: "702\u2013707"
  paper_id: '75'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1075.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1075.jpg
  title: An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency
    in Dialogue Generation
  title_html: An Auto-Encoder Matching Model for Learning Utterance-Level Semantic
    Dependency in Dialogue Generation
  url: https://www.aclweb.org/anthology/D18-1075
  year: '2018'
D18-1076:
  abstract: "This paper introduces a document grounded dataset for conversations.\
    \ We define \u201CDocument Grounded Conversations\u201D as conversations that\
    \ are about the contents of a specified document. In this dataset the specified\
    \ documents were Wikipedia articles about popular movies. The dataset contains\
    \ 4112 conversations with an average of 21.43 turns per conversation. This positions\
    \ this dataset to not only provide a relevant chat history while generating responses\
    \ but also provide a source of information that the models could use. We describe\
    \ two neural architectures that provide benchmark performance on the task of generating\
    \ the next response. We also evaluate our models for engagement and fluency, and\
    \ find that the information from the document helps in generating more engaging\
    \ and fluent responses."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1076.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1076.Attachment.zip
  author:
  - first: Kangyan
    full: Kangyan Zhou
    id: kangyan-zhou
    last: Zhou
  - first: Shrimai
    full: Shrimai Prabhumoye
    id: shrimai-prabhumoye
    last: Prabhumoye
  - first: Alan W
    full: Alan W Black
    id: alan-w-black
    last: Black
  author_string: Kangyan Zhou, Shrimai Prabhumoye, Alan W Black
  bibkey: zhou-etal-2018-dataset
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1076
  month: October-November
  page_first: '708'
  page_last: '713'
  pages: "708\u2013713"
  paper_id: '76'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1076.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1076.jpg
  title: A Dataset for Document Grounded Conversations
  title_html: A Dataset for Document Grounded Conversations
  url: https://www.aclweb.org/anthology/D18-1076
  year: '2018'
D18-1077:
  abstract: The main goal of this paper is to develop out-of-domain (OOD) detection
    for dialog systems. We propose to use only in-domain (IND) sentences to build
    a generative adversarial network (GAN) of which the discriminator generates low
    scores for OOD sentences. To improve basic GANs, we apply feature matching loss
    in the discriminator, use domain-category analysis as an additional task in the
    discriminator, and remove the biases in the generator. Thereby, we reduce the
    huge effort of collecting OOD sentences for training OOD detection. For evaluation,
    we experimented OOD detection on a multi-domain dialog system. The experimental
    results showed the proposed method was most accurate compared to the existing
    methods.
  address: Brussels, Belgium
  author:
  - first: Seonghan
    full: Seonghan Ryu
    id: seonghan-ryu
    last: Ryu
  - first: Sangjun
    full: Sangjun Koo
    id: sangjun-koo
    last: Koo
  - first: Hwanjo
    full: Hwanjo Yu
    id: hwanjo-yu
    last: Yu
  - first: Gary Geunbae
    full: Gary Geunbae Lee
    id: gary-geunbae-lee
    last: Lee
  author_string: Seonghan Ryu, Sangjun Koo, Hwanjo Yu, Gary Geunbae Lee
  bibkey: ryu-etal-2018-domain
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1077
  month: October-November
  page_first: '714'
  page_last: '718'
  pages: "714\u2013718"
  paper_id: '77'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1077.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1077.jpg
  title: Out-of-domain Detection based on Generative Adversarial Network
  title_html: Out-of-domain Detection based on Generative Adversarial Network
  url: https://www.aclweb.org/anthology/D18-1077
  year: '2018'
D18-1078:
  abstract: This paper presents a task for machine listening comprehension in the
    argumentation domain and a corresponding dataset in English. We recorded 200 spontaneous
    speeches arguing for or against 50 controversial topics. For each speech, we formulated
    a question, aimed at confirming or rejecting the occurrence of potential arguments
    in the speech. Labels were collected by listening to the speech and marking which
    arguments were mentioned by the speaker. We applied baseline methods addressing
    the task, to be used as a benchmark for future work over this dataset. All data
    used in this work is freely available for research.
  address: Brussels, Belgium
  author:
  - first: Shachar
    full: Shachar Mirkin
    id: shachar-mirkin
    last: Mirkin
  - first: Guy
    full: Guy Moshkowich
    id: guy-moshkowich
    last: Moshkowich
  - first: Matan
    full: Matan Orbach
    id: matan-orbach
    last: Orbach
  - first: Lili
    full: Lili Kotlerman
    id: lili-kotlerman
    last: Kotlerman
  - first: Yoav
    full: Yoav Kantor
    id: yoav-kantor
    last: Kantor
  - first: Tamar
    full: Tamar Lavee
    id: tamar-lavee
    last: Lavee
  - first: Michal
    full: Michal Jacovi
    id: michal-jacovi
    last: Jacovi
  - first: Yonatan
    full: Yonatan Bilu
    id: yonatan-bilu
    last: Bilu
  - first: Ranit
    full: Ranit Aharonov
    id: ranit-aharonov
    last: Aharonov
  - first: Noam
    full: Noam Slonim
    id: noam-slonim
    last: Slonim
  author_string: Shachar Mirkin, Guy Moshkowich, Matan Orbach, Lili Kotlerman, Yoav
    Kantor, Tamar Lavee, Michal Jacovi, Yonatan Bilu, Ranit Aharonov, Noam Slonim
  bibkey: mirkin-etal-2018-listening
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1078
  month: October-November
  page_first: '719'
  page_last: '724'
  pages: "719\u2013724"
  paper_id: '78'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1078.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1078.jpg
  title: Listening Comprehension over Argumentative Content
  title_html: Listening Comprehension over Argumentative Content
  url: https://www.aclweb.org/anthology/D18-1078
  year: '2018'
D18-1079:
  abstract: "We tackle discourse-level relation recognition, a problem of determining\
    \ semantic relations between text spans. Implicit relation recognition is challenging\
    \ due to the lack of explicit relational clues. The increasingly popular neural\
    \ network techniques have been proven effective for semantic encoding, whereby\
    \ widely employed to boost semantic relation discrimination. However, learning\
    \ to predict semantic relations at a deep level heavily relies on a great deal\
    \ of training data, but the scale of the publicly available data in this field\
    \ is limited. In this paper, we follow Rutherford and Xue (2015) to expand the\
    \ training data set using the corpus of explicitly-related arguments, by arbitrarily\
    \ dropping the overtly presented discourse connectives. On the basis, we carry\
    \ out an experiment of sampling, in which a simple active learning approach is\
    \ used, so as to take the informative instances for data expansion. The goal is\
    \ to verify whether the selective use of external data not only reduces the time\
    \ consumption of retraining but also ensures a better system performance. Using\
    \ the expanded training data, we retrain a convolutional neural network (CNN)\
    \ based classifer which is a simplified version of Qin et al. (2016)\u2019s stacking\
    \ gated relation recognizer. Experimental results show that expanding the training\
    \ set with small-scale carefully-selected external data yields substantial performance\
    \ gain, with the improvements of about 4% for accuracy and 3.6% for F-score. This\
    \ allows a weak classifier to achieve a comparable performance against the state-of-the-art\
    \ systems."
  address: Brussels, Belgium
  author:
  - first: Yang
    full: Yang Xu
    id: yang-xu
    last: Xu
  - first: Yu
    full: Yu Hong
    id: yu-hong
    last: Hong
  - first: Huibin
    full: Huibin Ruan
    id: huibin-ruan
    last: Ruan
  - first: Jianmin
    full: Jianmin Yao
    id: jianmin-yao
    last: Yao
  - first: Min
    full: Min Zhang
    id: min-zhang
    last: Zhang
  - first: Guodong
    full: Guodong Zhou
    id: guodong-zhou
    last: Zhou
  author_string: Yang Xu, Yu Hong, Huibin Ruan, Jianmin Yao, Min Zhang, Guodong Zhou
  bibkey: xu-etal-2018-using
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1079
  month: October-November
  page_first: '725'
  page_last: '731'
  pages: "725\u2013731"
  paper_id: '79'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1079.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1079.jpg
  title: Using active learning to expand training data for implicit discourse relation
    recognition
  title_html: Using active learning to expand training data for implicit discourse
    relation recognition
  url: https://www.aclweb.org/anthology/D18-1079
  year: '2018'
D18-1080:
  abstract: "Split and rephrase is the task of breaking down a sentence into shorter\
    \ ones that together convey the same meaning. We extract a rich new dataset for\
    \ this task by mining Wikipedia\u2019s edit history: WikiSplit contains one million\
    \ naturally occurring sentence rewrites, providing sixty times more distinct split\
    \ examples and a ninety times larger vocabulary than the WebSplit corpus introduced\
    \ by Narayan et al. (2017) as a benchmark for this task. Incorporating WikiSplit\
    \ as training data produces a model with qualitatively better predictions that\
    \ score 32 BLEU points above the prior best result on the WebSplit benchmark."
  address: Brussels, Belgium
  author:
  - first: Jan A.
    full: Jan A. Botha
    id: jan-a-botha
    last: Botha
  - first: Manaal
    full: Manaal Faruqui
    id: manaal-faruqui
    last: Faruqui
  - first: John
    full: John Alex
    id: john-alex
    last: Alex
  - first: Jason
    full: Jason Baldridge
    id: jason-baldridge
    last: Baldridge
  - first: Dipanjan
    full: Dipanjan Das
    id: dipanjan-das
    last: Das
  author_string: Jan A. Botha, Manaal Faruqui, John Alex, Jason Baldridge, Dipanjan
    Das
  bibkey: botha-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1080
  month: October-November
  page_first: '732'
  page_last: '737'
  pages: "732\u2013737"
  paper_id: '80'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1080.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1080.jpg
  title: Learning To Split and Rephrase From Wikipedia Edit History
  title_html: Learning To Split and Rephrase From <span class="acl-fixed-case">W</span>ikipedia
    Edit History
  url: https://www.aclweb.org/anthology/D18-1080
  year: '2018'
D18-1081:
  abstract: BLEU is widely considered to be an informative metric for text-to-text
    generation, including Text Simplification (TS). TS includes both lexical and structural
    aspects. In this paper we show that BLEU is not suitable for the evaluation of
    sentence splitting, the major structural simplification operation. We manually
    compiled a sentence splitting gold standard corpus containing multiple structural
    paraphrases, and performed a correlation analysis with human judgments. We find
    low or no correlation between BLEU and the grammaticality and meaning preservation
    parameters where sentence splitting is involved. Moreover, BLEU often negatively
    correlates with simplicity, essentially penalizing simpler sentences.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1081.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1081.Attachment.zip
  author:
  - first: Elior
    full: Elior Sulem
    id: elior-sulem
    last: Sulem
  - first: Omri
    full: Omri Abend
    id: omri-abend
    last: Abend
  - first: Ari
    full: Ari Rappoport
    id: ari-rappoport
    last: Rappoport
  author_string: Elior Sulem, Omri Abend, Ari Rappoport
  bibkey: sulem-etal-2018-bleu
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1081
  month: October-November
  page_first: '738'
  page_last: '744'
  pages: "738\u2013744"
  paper_id: '81'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1081.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1081.jpg
  title: BLEU is Not Suitable for the Evaluation of Text Simplification
  title_html: <span class="acl-fixed-case">BLEU</span> is Not Suitable for the Evaluation
    of Text Simplification
  url: https://www.aclweb.org/anthology/D18-1081
  year: '2018'
D18-1082:
  abstract: How to generate relevant and informative responses is one of the core
    topics in response generation area. Following the task formulation of machine
    translation, previous works mainly consider response generation task as a mapping
    from a source sentence to a target sentence. To realize this mapping, existing
    works tend to design intuitive but complex models. However, the relevant information
    existed in large dialogue corpus is mainly overlooked. In this paper, we propose
    Sequence to Sequence with Prototype Memory Network (S2SPMN) to exploit the relevant
    information provided by the large dialogue corpus to enhance response generation.
    Specifically, we devise two simple approaches in S2SPMN to select the relevant
    information (named prototypes) from the dialogue corpus. These prototypes are
    then saved into prototype memory network (PMN). Furthermore, a hierarchical attention
    mechanism is devised to extract the semantic information from the PMN to assist
    the response generation process. Empirical studies reveal the advantage of our
    model over several classical and strong baselines.
  address: Brussels, Belgium
  author:
  - first: Jiaxin
    full: Jiaxin Pei
    id: jiaxin-pei
    last: Pei
  - first: Chenliang
    full: Chenliang Li
    id: chenliang-li
    last: Li
  author_string: Jiaxin Pei, Chenliang Li
  bibkey: pei-li-2018-s2spmn
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1082
  month: October-November
  page_first: '745'
  page_last: '750'
  pages: "745\u2013750"
  paper_id: '82'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1082.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1082.jpg
  title: 'S2SPMN: A Simple and Effective Framework for Response Generation with Relevant
    Information'
  title_html: '<span class="acl-fixed-case">S</span>2<span class="acl-fixed-case">SPMN</span>:
    A Simple and Effective Framework for Response Generation with Relevant Information'
  url: https://www.aclweb.org/anthology/D18-1082
  year: '2018'
D18-1083:
  abstract: Recently, Reinforcement Learning (RL) approaches have demonstrated advanced
    performance in image captioning by directly optimizing the metric used for testing.
    However, this shaped reward introduces learning biases, which reduces the readability
    of generated text. In addition, the large sample space makes training unstable
    and slow.To alleviate these issues, we propose a simple coherent solution that
    constrains the action space using an n-gram language prior. Quantitative and qualitative
    evaluations on benchmarks show that RL with the simple add-on module performs
    favorably against its counterpart in terms of both readability and speed of convergence.
    Human evaluation results show that our model is more human readable and graceful.
    The implementation will become publicly available upon the acceptance of the paper.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1083.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1083.Attachment.zip
  author:
  - first: Tszhang
    full: Tszhang Guo
    id: tszhang-guo
    last: Guo
  - first: Shiyu
    full: Shiyu Chang
    id: shiyu-chang
    last: Chang
  - first: Mo
    full: Mo Yu
    id: mo-yu
    last: Yu
  - first: Kun
    full: Kun Bai
    id: kun-bai
    last: Bai
  author_string: Tszhang Guo, Shiyu Chang, Mo Yu, Kun Bai
  bibkey: guo-etal-2018-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1083
  month: October-November
  page_first: '751'
  page_last: '756'
  pages: "751\u2013756"
  paper_id: '83'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1083.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1083.jpg
  title: Improving Reinforcement Learning Based Image Captioning with Natural Language
    Prior
  title_html: Improving Reinforcement Learning Based Image Captioning with Natural
    Language Prior
  url: https://www.aclweb.org/anthology/D18-1083
  year: '2018'
D18-1084:
  abstract: Image paragraph captioning models aim to produce detailed descriptions
    of a source image. These models use similar techniques as standard image captioning
    models, but they have encountered issues in text generation, notably a lack of
    diversity between sentences, that have limited their effectiveness. In this work,
    we consider applying sequence-level training for this task. We find that standard
    self-critical training produces poor results, but when combined with an integrated
    penalty on trigram repetition produces much more diverse paragraphs. This simple
    training approach improves on the best result on the Visual Genome paragraph captioning
    dataset from 16.9 to 30.6 CIDEr, with gains on METEOR and BLEU as well, without
    requiring any architectural changes.
  address: Brussels, Belgium
  author:
  - first: Luke
    full: Luke Melas-Kyriazi
    id: luke-melas-kyriazi
    last: Melas-Kyriazi
  - first: Alexander
    full: Alexander Rush
    id: alexander-m-rush
    last: Rush
  - first: George
    full: George Han
    id: george-han
    last: Han
  author_string: Luke Melas-Kyriazi, Alexander Rush, George Han
  bibkey: melas-kyriazi-etal-2018-training
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1084
  month: October-November
  page_first: '757'
  page_last: '761'
  pages: "757\u2013761"
  paper_id: '84'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1084.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1084.jpg
  title: Training for Diversity in Image Paragraph Captioning
  title_html: Training for Diversity in Image Paragraph Captioning
  url: https://www.aclweb.org/anthology/D18-1084
  year: '2018'
D18-1085:
  abstract: ROUGE is one of the first and most widely used evaluation metrics for
    text summarization. However, its assessment merely relies on surface similarities
    between peer and model summaries. Consequently, ROUGE is unable to fairly evaluate
    summaries including lexical variations and paraphrasing. We propose a graph-based
    approach adopted into ROUGE to evaluate summaries based on both lexical and semantic
    similarities. Experiment results over TAC AESOP datasets show that exploiting
    the lexico-semantic similarity of the words used in summaries would significantly
    help ROUGE correlate better with human judgments.
  address: Brussels, Belgium
  author:
  - first: Elaheh
    full: Elaheh ShafieiBavani
    id: elaheh-shafieibavani
    last: ShafieiBavani
  - first: Mohammad
    full: Mohammad Ebrahimi
    id: mohammad-ebrahimi
    last: Ebrahimi
  - first: Raymond
    full: Raymond Wong
    id: raymond-wong
    last: Wong
  - first: Fang
    full: Fang Chen
    id: fang-chen
    last: Chen
  author_string: Elaheh ShafieiBavani, Mohammad Ebrahimi, Raymond Wong, Fang Chen
  bibkey: shafieibavani-etal-2018-graph
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1085
  month: October-November
  page_first: '762'
  page_last: '767'
  pages: "762\u2013767"
  paper_id: '85'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1085.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1085.jpg
  title: A Graph-theoretic Summary Evaluation for ROUGE
  title_html: A Graph-theoretic Summary Evaluation for <span class="acl-fixed-case">ROUGE</span>
  url: https://www.aclweb.org/anthology/D18-1085
  year: '2018'
D18-1086:
  abstract: Recent work on abstractive summarization has made progress with neural
    encoder-decoder architectures. However, such models are often challenged due to
    their lack of explicit semantic modeling of the source document and its summary.
    In this paper, we extend previous work on abstractive summarization using Abstract
    Meaning Representation (AMR) with a neural language generation stage which we
    guide using the source document. We demonstrate that this guidance improves summarization
    results by 7.4 and 10.5 points in ROUGE-2 using gold standard AMR parses and parses
    obtained from an off-the-shelf parser respectively. We also find that the summarization
    performance on later parses is 2 ROUGE-2 points higher than that of a well-established
    neural encoder-decoder approach trained on a larger dataset.
  address: Brussels, Belgium
  author:
  - first: Hardy
    full: Hardy Hardy
    id: hardy-hardy
    last: Hardy
  - first: Andreas
    full: Andreas Vlachos
    id: andreas-vlachos
    last: Vlachos
  author_string: Hardy Hardy, Andreas Vlachos
  bibkey: hardy-vlachos-2018-guided
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1086
  month: October-November
  page_first: '768'
  page_last: '773'
  pages: "768\u2013773"
  paper_id: '86'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1086.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1086.jpg
  title: Guided Neural Language Generation for Abstractive Summarization using Abstract
    Meaning Representation
  title_html: Guided Neural Language Generation for Abstractive Summarization using
    Abstract Meaning Representation
  url: https://www.aclweb.org/anthology/D18-1086
  year: '2018'
D18-1087:
  abstract: Practical summarization systems are expected to produce summaries of varying
    lengths, per user needs. While a couple of early summarization benchmarks tested
    systems across multiple summary lengths, this practice was mostly abandoned due
    to the assumed cost of producing reference summaries of multiple lengths. In this
    paper, we raise the research question of whether reference summaries of a single
    length can be used to reliably evaluate system summaries of multiple lengths.
    For that, we have analyzed a couple of datasets as a case study, using several
    variants of the ROUGE metric that are standard in summarization evaluation. Our
    findings indicate that the evaluation protocol in question is indeed competitive.
    This result paves the way to practically evaluating varying-length summaries with
    simple, possibly existing, summarization benchmarks.
  address: Brussels, Belgium
  author:
  - first: Ori
    full: Ori Shapira
    id: ori-shapira
    last: Shapira
  - first: David
    full: David Gabay
    id: david-gabay
    last: Gabay
  - first: Hadar
    full: Hadar Ronen
    id: hadar-ronen
    last: Ronen
  - first: Judit
    full: Judit Bar-Ilan
    id: judit-bar-ilan
    last: Bar-Ilan
  - first: Yael
    full: Yael Amsterdamer
    id: yael-amsterdamer
    last: Amsterdamer
  - first: Ani
    full: Ani Nenkova
    id: ani-nenkova
    last: Nenkova
  - first: Ido
    full: Ido Dagan
    id: ido-dagan
    last: Dagan
  author_string: Ori Shapira, David Gabay, Hadar Ronen, Judit Bar-Ilan, Yael Amsterdamer,
    Ani Nenkova, Ido Dagan
  bibkey: shapira-etal-2018-evaluating
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1087
  month: October-November
  page_first: '774'
  page_last: '778'
  pages: "774\u2013778"
  paper_id: '87'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1087.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1087.jpg
  title: 'Evaluating Multiple System Summary Lengths: A Case Study'
  title_html: 'Evaluating Multiple System Summary Lengths: A Case Study'
  url: https://www.aclweb.org/anthology/D18-1087
  year: '2018'
D18-1088:
  abstract: Extractive summarization models need sentence level labels, which are
    usually created with rule-based methods since most summarization datasets only
    have document summary pairs. These labels might be suboptimal. We propose a latent
    variable extractive model, where sentences are viewed as latent variables and
    sentences with activated variables are used to infer gold summaries. During training,
    the loss can come directly from gold summaries. Experiments on CNN/Dailymail dataset
    show our latent extractive model outperforms a strong extractive baseline trained
    on rule-based labels and also performs competitively with several recent models.
  address: Brussels, Belgium
  author:
  - first: Xingxing
    full: Xingxing Zhang
    id: xingxing-zhang
    last: Zhang
  - first: Mirella
    full: Mirella Lapata
    id: mirella-lapata
    last: Lapata
  - first: Furu
    full: Furu Wei
    id: furu-wei
    last: Wei
  - first: Ming
    full: Ming Zhou
    id: ming-zhou
    last: Zhou
  author_string: Xingxing Zhang, Mirella Lapata, Furu Wei, Ming Zhou
  bibkey: zhang-etal-2018-neural-latent
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1088
  month: October-November
  page_first: '779'
  page_last: '784'
  pages: "779\u2013784"
  paper_id: '88'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1088.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1088.jpg
  title: Neural Latent Extractive Document Summarization
  title_html: Neural Latent Extractive Document Summarization
  url: https://www.aclweb.org/anthology/D18-1088
  year: '2018'
D18-1089:
  abstract: Many modern neural document summarization systems based on encoder-decoder
    networks are designed to produce abstractive summaries. We attempted to verify
    the degree of abstractiveness of modern neural abstractive summarization systems
    by calculating overlaps in terms of various types of units. Upon the observation
    that many abstractive systems tend to be near-extractive in practice, we also
    implemented a pure copy system, which achieved comparable results as abstractive
    summarizers while being far more computationally efficient. These findings suggest
    the possibility for future efforts towards more efficient systems that could better
    utilize the vocabulary in the original document.
  address: Brussels, Belgium
  author:
  - first: Fangfang
    full: Fangfang Zhang
    id: fang-fang-zhang
    last: Zhang
  - first: Jin-ge
    full: Jin-ge Yao
    id: jin-ge-yao
    last: Yao
  - first: Rui
    full: Rui Yan
    id: rui-yan
    last: Yan
  author_string: Fangfang Zhang, Jin-ge Yao, Rui Yan
  bibkey: zhang-etal-2018-abstractiveness
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1089
  month: October-November
  page_first: '785'
  page_last: '790'
  pages: "785\u2013790"
  paper_id: '89'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1089.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1089.jpg
  title: On the Abstractiveness of Neural Document Summarization
  title_html: On the Abstractiveness of Neural Document Summarization
  url: https://www.aclweb.org/anthology/D18-1089
  year: '2018'
D18-1090:
  abstract: Automatic essay scoring (AES) is the task of assigning grades to essays
    without human interference. Existing systems for AES are typically trained to
    predict the score of each single essay at a time without considering the rating
    schema. In order to address this issue, we propose a reinforcement learning framework
    for essay scoring that incorporates quadratic weighted kappa as guidance to optimize
    the scoring system. Experiment results on benchmark datasets show the effectiveness
    of our framework.
  address: Brussels, Belgium
  author:
  - first: Yucheng
    full: Yucheng Wang
    id: yucheng-wang
    last: Wang
  - first: Zhongyu
    full: Zhongyu Wei
    id: zhongyu-wei
    last: Wei
  - first: Yaqian
    full: Yaqian Zhou
    id: yaqian-zhou
    last: Zhou
  - first: Xuanjing
    full: Xuanjing Huang
    id: xuan-jing-huang
    last: Huang
  author_string: Yucheng Wang, Zhongyu Wei, Yaqian Zhou, Xuanjing Huang
  bibkey: wang-etal-2018-automatic
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1090
  month: October-November
  page_first: '791'
  page_last: '797'
  pages: "791\u2013797"
  paper_id: '90'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1090.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1090.jpg
  title: Automatic Essay Scoring Incorporating Rating Schema via Reinforcement Learning
  title_html: Automatic Essay Scoring Incorporating Rating Schema via Reinforcement
    Learning
  url: https://www.aclweb.org/anthology/D18-1090
  year: '2018'
D18-1091:
  abstract: "Understanding search queries is a hard problem as it involves dealing\
    \ with \u201Cword salad\u201D text ubiquitously issued by users. However, if a\
    \ query resembles a well-formed question, a natural language processing pipeline\
    \ is able to perform more accurate interpretation, thus reducing downstream compounding\
    \ errors. Hence, identifying whether or not a query is well formed can enhance\
    \ query understanding. Here, we introduce a new task of identifying a well-formed\
    \ natural language question. We construct and release a dataset of 25,100 publicly\
    \ available questions classified into well-formed and non-wellformed categories\
    \ and report an accuracy of 70.7% on the test set. We also show that our classifier\
    \ can be used to improve the performance of neural sequence-to-sequence models\
    \ for generating questions for reading comprehension."
  address: Brussels, Belgium
  author:
  - first: Manaal
    full: Manaal Faruqui
    id: manaal-faruqui
    last: Faruqui
  - first: Dipanjan
    full: Dipanjan Das
    id: dipanjan-das
    last: Das
  author_string: Manaal Faruqui, Dipanjan Das
  bibkey: faruqui-das-2018-identifying
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1091
  month: October-November
  page_first: '798'
  page_last: '803'
  pages: "798\u2013803"
  paper_id: '91'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1091.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1091.jpg
  title: Identifying Well-formed Natural Language Questions
  title_html: Identifying Well-formed Natural Language Questions
  url: https://www.aclweb.org/anthology/D18-1091
  year: '2018'
D18-1092:
  abstract: Deep neural networks reach state-of-the-art performance for wide range
    of natural language processing, computer vision and speech applications. Yet,
    one of the biggest challenges is running these complex networks on devices such
    as mobile phones or smart watches with tiny memory footprint and low computational
    capacity. We propose on-device Self-Governing Neural Networks (SGNNs), which learn
    compact projection vectors with local sensitive hashing. The key advantage of
    SGNNs over existing work is that they surmount the need for pre-trained word embeddings
    and complex networks with huge parameters. We conduct extensive evaluation on
    dialog act classification and show significant improvement over state-of-the-art
    results. Our findings show that SGNNs are effective at capturing low-dimensional
    semantic text representations, while maintaining high accuracy.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305197775
    type: video
    url: https://vimeo.com/305197775
  author:
  - first: Sujith
    full: Sujith Ravi
    id: sujith-ravi
    last: Ravi
  - first: Zornitsa
    full: Zornitsa Kozareva
    id: zornitsa-kozareva
    last: Kozareva
  author_string: Sujith Ravi, Zornitsa Kozareva
  bibkey: ravi-kozareva-2018-self
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1092
  month: October-November
  page_first: '804'
  page_last: '810'
  pages: "804\u2013810"
  paper_id: '92'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1092.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1092.jpg
  title: Self-Governing Neural Networks for On-Device Short Text Classification
  title_html: Self-Governing Neural Networks for On-Device Short Text Classification
  url: https://www.aclweb.org/anthology/D18-1092
  year: '2018'
D18-1093:
  abstract: We focus on the multi-label categorization task for short texts and explore
    the use of a hierarchical structure (HS) of categories. In contrast to the existing
    work using non-hierarchical flat model, the method leverages the hierarchical
    relations between the pre-defined categories to tackle the data sparsity problem.
    The lower the HS level, the less the categorization performance. Because the number
    of training data per category in a lower level is much smaller than that in an
    upper level. We propose an approach which can effectively utilize the data in
    the upper levels to contribute the categorization in the lower levels by applying
    the Convolutional Neural Network (CNN) with a fine-tuning technique. The results
    using two benchmark datasets show that proposed method, Hierarchical Fine-Tuning
    based CNN (HFT-CNN) is competitive with the state-of-the-art CNN based methods.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1093.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1093.Attachment.zip
  author:
  - first: Kazuya
    full: Kazuya Shimura
    id: kazuya-shimura
    last: Shimura
  - first: Jiyi
    full: Jiyi Li
    id: jiyi-li
    last: Li
  - first: Fumiyo
    full: Fumiyo Fukumoto
    id: fumiyo-fukumoto
    last: Fukumoto
  author_string: Kazuya Shimura, Jiyi Li, Fumiyo Fukumoto
  bibkey: shimura-etal-2018-hft
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1093
  month: October-November
  page_first: '811'
  page_last: '816'
  pages: "811\u2013816"
  paper_id: '93'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1093.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1093.jpg
  title: 'HFT-CNN: Learning Hierarchical Category Structure for Multi-label Short
    Text Categorization'
  title_html: '<span class="acl-fixed-case">HFT</span>-<span class="acl-fixed-case">CNN</span>:
    Learning Hierarchical Category Structure for Multi-label Short Text Categorization'
  url: https://www.aclweb.org/anthology/D18-1093
  year: '2018'
D18-1094:
  abstract: Deep neural networks have been displaying superior performance over traditional
    supervised classifiers in text classification. They learn to extract useful features
    automatically when sufficient amount of data is presented. However, along with
    the growth in the number of documents comes the increase in the number of categories,
    which often results in poor performance of the multiclass classifiers. In this
    work, we use external knowledge in the form of topic category taxonomies to aide
    the classification by introducing a deep hierarchical neural attention-based classifier.
    Our model performs better than or comparable to state-of-the-art hierarchical
    models at significantly lower computational cost while maintaining high interpretability.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1094.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1094.Attachment.zip
  author:
  - first: Koustuv
    full: Koustuv Sinha
    id: koustuv-sinha
    last: Sinha
  - first: Yue
    full: Yue Dong
    id: yue-dong
    last: Dong
  - first: Jackie Chi Kit
    full: Jackie Chi Kit Cheung
    id: jackie-chi-kit-cheung
    last: Cheung
  - first: Derek
    full: Derek Ruths
    id: derek-ruths
    last: Ruths
  author_string: Koustuv Sinha, Yue Dong, Jackie Chi Kit Cheung, Derek Ruths
  bibkey: sinha-etal-2018-hierarchical
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1094
  month: October-November
  page_first: '817'
  page_last: '823'
  pages: "817\u2013823"
  paper_id: '94'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1094.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1094.jpg
  title: A Hierarchical Neural Attention-based Text Classifier
  title_html: A Hierarchical Neural Attention-based Text Classifier
  url: https://www.aclweb.org/anthology/D18-1094
  year: '2018'
D18-1095:
  abstract: We propose Labeled Anchors, an interactive and supervised topic model
    based on the anchor words algorithm (Arora et al., 2013). Labeled Anchors is similar
    to Supervised Anchors (Nguyen et al., 2014) in that it extends the vector-space
    representation of words to include document labels. However, our formulation also
    admits a classifier which requires no training beyond inferring topics, which
    means our approach is also fast enough to be interactive. We run a small user
    study that demonstrates that untrained users can interactively update topics in
    order to improve classification accuracy.
  address: Brussels, Belgium
  author:
  - first: Jeffrey
    full: Jeffrey Lund
    id: jeffrey-lund
    last: Lund
  - first: Stephen
    full: Stephen Cowley
    id: stephen-cowley
    last: Cowley
  - first: Wilson
    full: Wilson Fearn
    id: wilson-fearn
    last: Fearn
  - first: Emily
    full: Emily Hales
    id: emily-hales
    last: Hales
  - first: Kevin
    full: Kevin Seppi
    id: kevin-seppi
    last: Seppi
  author_string: Jeffrey Lund, Stephen Cowley, Wilson Fearn, Emily Hales, Kevin Seppi
  bibkey: lund-etal-2018-labeled
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1095
  month: October-November
  page_first: '824'
  page_last: '829'
  pages: "824\u2013829"
  paper_id: '95'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1095.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1095.jpg
  title: Labeled Anchors and a Scalable, Transparent, and Interactive Classifier
  title_html: Labeled Anchors and a Scalable, Transparent, and Interactive Classifier
  url: https://www.aclweb.org/anthology/D18-1095
  year: '2018'
D18-1096:
  abstract: Topic models are evaluated based on their ability to describe documents
    well (i.e. low perplexity) and to produce topics that carry coherent semantic
    meaning. In topic modeling so far, perplexity is a direct optimization target.
    However, topic coherence, owing to its challenging computation, is not optimized
    for and is only evaluated after training. In this work, under a neural variational
    inference framework, we propose methods to incorporate a topic coherence objective
    into the training process. We demonstrate that such a coherence-aware topic model
    exhibits a similar level of perplexity as baseline models but achieves substantially
    higher topic coherence.
  address: Brussels, Belgium
  author:
  - first: Ran
    full: Ran Ding
    id: ran-ding
    last: Ding
  - first: Ramesh
    full: Ramesh Nallapati
    id: ramesh-nallapati
    last: Nallapati
  - first: Bing
    full: Bing Xiang
    id: bing-xiang
    last: Xiang
  author_string: Ran Ding, Ramesh Nallapati, Bing Xiang
  bibkey: ding-etal-2018-coherence
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1096
  month: October-November
  page_first: '830'
  page_last: '836'
  pages: "830\u2013836"
  paper_id: '96'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1096.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1096.jpg
  title: Coherence-Aware Neural Topic Modeling
  title_html: Coherence-Aware Neural Topic Modeling
  url: https://www.aclweb.org/anthology/D18-1096
  year: '2018'
D18-1097:
  abstract: Text normalization is an important enabling technology for several NLP
    tasks. Recently, neural-network-based approaches have outperformed well-established
    models in this task. However, in languages other than English, there has been
    little exploration in this direction. Both the scarcity of annotated data and
    the complexity of the language increase the difficulty of the problem. To address
    these challenges, we use a sequence-to-sequence model with character-based attention,
    which in addition to its self-learned character embeddings, uses word embeddings
    pre-trained with an approach that also models subword information. This provides
    the neural model with access to more linguistic information especially suitable
    for text normalization, without large parallel corpora. We show that providing
    the model with word-level features bridges the gap for the neural network approach
    to achieve a state-of-the-art F1 score on a standard Arabic language correction
    shared task dataset.
  address: Brussels, Belgium
  author:
  - first: Daniel
    full: Daniel Watson
    id: daniel-watson
    last: Watson
  - first: Nasser
    full: Nasser Zalmout
    id: nasser-zalmout
    last: Zalmout
  - first: Nizar
    full: Nizar Habash
    id: nizar-habash
    last: Habash
  author_string: Daniel Watson, Nasser Zalmout, Nizar Habash
  bibkey: watson-etal-2018-utilizing
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1097
  month: October-November
  page_first: '837'
  page_last: '843'
  pages: "837\u2013843"
  paper_id: '97'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1097.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1097.jpg
  title: Utilizing Character and Word Embeddings for Text Normalization with Sequence-to-Sequence
    Models
  title_html: Utilizing Character and Word Embeddings for Text Normalization with
    Sequence-to-Sequence Models
  url: https://www.aclweb.org/anthology/D18-1097
  year: '2018'
D18-1098:
  abstract: "Topic coherence is increasingly being used to evaluate topic models and\
    \ filter topics for end-user applications. Topic coherence measures how well topic\
    \ words relate to each other, but offers little insight on the utility of the\
    \ topics in describing the documents. In this paper, we explore the topic intrusion\
    \ task \u2014 the task of guessing an outlier topic given a document and a few\
    \ topics \u2014 and propose a method to automate it. We improve upon the state-of-the-art\
    \ substantially, demonstrating its viability as an alternative method for topic\
    \ model evaluation."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1098.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1098.Attachment.pdf
  author:
  - first: Shraey
    full: Shraey Bhatia
    id: shraey-bhatia
    last: Bhatia
  - first: Jey Han
    full: Jey Han Lau
    id: jey-han-lau
    last: Lau
  - first: Timothy
    full: Timothy Baldwin
    id: timothy-baldwin
    last: Baldwin
  author_string: Shraey Bhatia, Jey Han Lau, Timothy Baldwin
  bibkey: bhatia-etal-2018-topic
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1098
  month: October-November
  page_first: '844'
  page_last: '849'
  pages: "844\u2013849"
  paper_id: '98'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1098.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1098.jpg
  title: Topic Intrusion for Automatic Topic Model Evaluation
  title_html: Topic Intrusion for Automatic Topic Model Evaluation
  url: https://www.aclweb.org/anthology/D18-1098
  year: '2018'
D18-1099:
  abstract: 'The text in many web documents is organized into a hierarchy of section
    titles and corresponding prose content, a structure which provides potentially
    exploitable information on discourse structure and topicality. However, this organization
    is generally discarded during text collection, and collecting it is not straightforward:
    the same visual organization can be implemented in a myriad of different ways
    in the underlying HTML. To remedy this, we present a flexible system for automatically
    extracting the hierarchical section titles and prose organization of web documents
    irrespective of differences in HTML representation. This system uses features
    from syntax, semantics, discourse and markup to build two models which classify
    HTML text into section titles and prose text. When tested on three different domains
    of web text, our domain-independent system achieves an overall precision of 0.82
    and a recall of 0.98. The domain-dependent variation produces very high precision
    (0.99) at the expense of recall (0.75). These results exhibit a robust level of
    accuracy suitable for enhancing question answering, information extraction, and
    summarization.'
  address: Brussels, Belgium
  author:
  - first: Abhijith Athreya
    full: Abhijith Athreya Mysore Gopinath
    id: abhijith-athreya-mysore-gopinath
    last: Mysore Gopinath
  - first: Shomir
    full: Shomir Wilson
    id: shomir-wilson
    last: Wilson
  - first: Norman
    full: Norman Sadeh
    id: norman-sadeh
    last: Sadeh
  author_string: Abhijith Athreya Mysore Gopinath, Shomir Wilson, Norman Sadeh
  bibkey: mysore-gopinath-etal-2018-supervised
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1099
  month: October-November
  page_first: '850'
  page_last: '855'
  pages: "850\u2013855"
  paper_id: '99'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1099.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1099.jpg
  title: Supervised and Unsupervised Methods for Robust Separation of Section Titles
    and Prose Text in Web Documents
  title_html: Supervised and Unsupervised Methods for Robust Separation of Section
    Titles and Prose Text in Web Documents
  url: https://www.aclweb.org/anthology/D18-1099
  year: '2018'
D18-1100:
  abstract: 'In this work, we examine methods for data augmentation for text-based
    tasks such as neural machine translation (NMT). We formulate the design of a data
    augmentation policy with desirable properties as an optimization problem, and
    derive a generic analytic solution. This solution not only subsumes some existing
    augmentation schemes, but also leads to an extremely simple data augmentation
    strategy for NMT: randomly replacing words in both the source sentence and the
    target sentence with other random words from their corresponding vocabularies.
    We name this method SwitchOut. Experiments on three translation datasets of different
    scales show that SwitchOut yields consistent improvements of about 0.5 BLEU, achieving
    better or comparable performances to strong alternatives such as word dropout
    (Sennrich et al., 2016a). Code to implement this method is included in the appendix.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1100.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1100.Attachment.pdf
  - filename: https://vimeo.com/305206127
    type: video
    url: https://vimeo.com/305206127
  author:
  - first: Xinyi
    full: Xinyi Wang
    id: xinyi-wang
    last: Wang
  - first: Hieu
    full: Hieu Pham
    id: hieu-pham
    last: Pham
  - first: Zihang
    full: Zihang Dai
    id: zihang-dai
    last: Dai
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  author_string: Xinyi Wang, Hieu Pham, Zihang Dai, Graham Neubig
  bibkey: wang-etal-2018-switchout
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1100
  month: October-November
  page_first: '856'
  page_last: '861'
  pages: "856\u2013861"
  paper_id: '100'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1100.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1100.jpg
  title: 'SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation'
  title_html: '<span class="acl-fixed-case">S</span>witch<span class="acl-fixed-case">O</span>ut:
    an Efficient Data Augmentation Algorithm for Neural Machine Translation'
  url: https://www.aclweb.org/anthology/D18-1100
  year: '2018'
D18-1101:
  abstract: Unsupervised learning of cross-lingual word embedding offers elegant matching
    of words across languages, but has fundamental limitations in translating sentences.
    In this paper, we propose simple yet effective methods to improve word-by-word
    translation of cross-lingual embeddings, using only monolingual corpora but without
    any back-translation. We integrate a language model for context-aware search,
    and use a novel denoising autoencoder to handle reordering. Our system surpasses
    state-of-the-art unsupervised translation systems without costly iterative training.
    We also analyze the effect of vocabulary size and denoising type on the translation
    performance, which provides better understanding of learning the cross-lingual
    word embedding and its usage in translation.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305206383
    type: video
    url: https://vimeo.com/305206383
  author:
  - first: Yunsu
    full: Yunsu Kim
    id: yunsu-kim
    last: Kim
  - first: Jiahui
    full: Jiahui Geng
    id: jiahui-geng
    last: Geng
  - first: Hermann
    full: Hermann Ney
    id: hermann-ney
    last: Ney
  author_string: Yunsu Kim, Jiahui Geng, Hermann Ney
  bibkey: kim-etal-2018-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1101
  month: October-November
  page_first: '862'
  page_last: '868'
  pages: "862\u2013868"
  paper_id: '101'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1101.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1101.jpg
  title: Improving Unsupervised Word-by-Word Translation with Language Model and Denoising
    Autoencoder
  title_html: Improving Unsupervised Word-by-Word Translation with Language Model
    and Denoising Autoencoder
  url: https://www.aclweb.org/anthology/D18-1101
  year: '2018'
D18-1102:
  abstract: Decipherment of homophonic substitution ciphers using language models
    is a well-studied task in NLP. Previous work in this topic scores short local
    spans of possible plaintext decipherments using n-gram language models. The most
    widely used technique is the use of beam search with n-gram language models proposed
    by Nuhn et al.(2013). We propose a beam search algorithm that scores the entire
    candidate plaintext at each step of the decipherment using a neural language model.
    We augment beam search with a novel rest cost estimation that exploits the prediction
    power of a neural language model. We compare against the state of the art n-gram
    based methods on many different decipherment tasks. On challenging ciphers such
    as the Beale cipher we provide significantly better error rates with much smaller
    beam sizes.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305206655
    type: video
    url: https://vimeo.com/305206655
  author:
  - first: Nishant
    full: Nishant Kambhatla
    id: nishant-kambhatla
    last: Kambhatla
  - first: Anahita
    full: Anahita Mansouri Bigvand
    id: anahita-mansouri-bigvand
    last: Mansouri Bigvand
  - first: Anoop
    full: Anoop Sarkar
    id: anoop-sarkar
    last: Sarkar
  author_string: Nishant Kambhatla, Anahita Mansouri Bigvand, Anoop Sarkar
  bibkey: kambhatla-etal-2018-decipherment
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1102
  month: October-November
  page_first: '869'
  page_last: '874'
  pages: "869\u2013874"
  paper_id: '102'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1102.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1102.jpg
  title: Decipherment of Substitution Ciphers with Neural Language Models
  title_html: Decipherment of Substitution Ciphers with Neural Language Models
  url: https://www.aclweb.org/anthology/D18-1102
  year: '2018'
D18-1103:
  abstract: "This paper examines the problem of adapting neural machine translation\
    \ systems to new, low-resourced languages (LRLs) as effectively and rapidly as\
    \ possible. We propose methods based on starting with massively multilingual \u201C\
    seed models\u201D, which can be trained ahead-of-time, and then continuing training\
    \ on data related to the LRL. We contrast a number of strategies, leading to a\
    \ novel, simple, yet effective method of \u201Csimilar-language regularization\u201D\
    , where we jointly train on both a LRL of interest and a similar high-resourced\
    \ language to prevent over-fitting to small LRL data. Experiments demonstrate\
    \ that massively multilingual models, even without any explicit adaptation, are\
    \ surprisingly effective, achieving BLEU scores of up to 15.5 with no data from\
    \ the LRL, and that the proposed similar-language regularization method improves\
    \ over other adaptation methods by 1.7 BLEU points average over 4 LRL settings."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305207187
    type: video
    url: https://vimeo.com/305207187
  author:
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  - first: Junjie
    full: Junjie Hu
    id: junjie-hu
    last: Hu
  author_string: Graham Neubig, Junjie Hu
  bibkey: neubig-hu-2018-rapid
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1103
  month: October-November
  page_first: '875'
  page_last: '880'
  pages: "875\u2013880"
  paper_id: '103'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1103.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1103.jpg
  title: Rapid Adaptation of Neural Machine Translation to New Languages
  title_html: Rapid Adaptation of Neural Machine Translation to New Languages
  url: https://www.aclweb.org/anthology/D18-1103
  year: '2018'
D18-1104:
  abstract: "We propose and compare methods for gradient-based domain adaptation of\
    \ self-attentive neural machine translation models. We demonstrate that a large\
    \ proportion of model parameters can be frozen during adaptation with minimal\
    \ or no reduction in translation quality by encouraging structured sparsity in\
    \ the set of offset tensors during learning via group lasso regularization. We\
    \ evaluate this technique for both batch and incremental adaptation across multiple\
    \ data sets and language pairs. Our system architecture\u2013combining a state-of-the-art\
    \ self-attentive model with compact domain adaptation\u2013provides high quality\
    \ personalized machine translation that is both space and time efficient."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305207608
    type: video
    url: https://vimeo.com/305207608
  author:
  - first: Joern
    full: Joern Wuebker
    id: joern-wuebker
    last: Wuebker
  - first: Patrick
    full: Patrick Simianer
    id: patrick-simianer
    last: Simianer
  - first: John
    full: John DeNero
    id: john-denero
    last: DeNero
  author_string: Joern Wuebker, Patrick Simianer, John DeNero
  bibkey: wuebker-etal-2018-compact
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1104
  month: October-November
  page_first: '881'
  page_last: '886'
  pages: "881\u2013886"
  paper_id: '104'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1104.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1104.jpg
  title: Compact Personalized Models for Neural Machine Translation
  title_html: Compact Personalized Models for Neural Machine Translation
  url: https://www.aclweb.org/anthology/D18-1104
  year: '2018'
D18-1105:
  abstract: Deep neural networks reach state-of-the-art performance for wide range
    of natural language processing, computer vision and speech applications. Yet,
    one of the biggest challenges is running these complex networks on devices such
    as mobile phones or smart watches with tiny memory footprint and low computational
    capacity. We propose on-device Self-Governing Neural Networks (SGNNs), which learn
    compact projection vectors with local sensitive hashing. The key advantage of
    SGNNs over existing work is that they surmount the need for pre-trained word embeddings
    and complex networks with huge parameters. We conduct extensive evaluation on
    dialog act classification and show significant improvement over state-of-the-art
    results. Our findings show that SGNNs are effective at capturing low-dimensional
    semantic text representations, while maintaining high accuracy.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305197775
    type: video
    url: https://vimeo.com/305197775
  author:
  - first: Sujith
    full: Sujith Ravi
    id: sujith-ravi
    last: Ravi
  - first: Zornitsa
    full: Zornitsa Kozareva
    id: zornitsa-kozareva
    last: Kozareva
  author_string: Sujith Ravi, Zornitsa Kozareva
  bibkey: ravi-kozareva-2018-self-governing
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1105
  month: October-November
  page_first: '887'
  page_last: '893'
  pages: "887\u2013893"
  paper_id: '105'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1105.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1105.jpg
  title: Self-Governing Neural Networks for On-Device Short Text Classification
  title_html: Self-Governing Neural Networks for On-Device Short Text Classification
  url: https://www.aclweb.org/anthology/D18-1105
  year: '2018'
D18-1106:
  abstract: "In large-scale domain classification for natural language understanding,\
    \ leveraging each user\u2019s domain enablement information, which refers to the\
    \ preferred or authenticated domains by the user, with attention mechanism has\
    \ been shown to improve the overall domain classification performance. In this\
    \ paper, we propose a supervised enablement attention mechanism, which utilizes\
    \ sigmoid activation for the attention weighting so that the attention can be\
    \ computed with more expressive power without the weight sum constraint of softmax\
    \ attention. The attention weights are explicitly encouraged to be similar to\
    \ the corresponding elements of the output one-hot vector, and self-distillation\
    \ is used to leverage the attention information of the other enabled domains.\
    \ By evaluating on the actual utterances from a large-scale IPDA, we show that\
    \ our approach significantly improves domain classification performance"
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305198062
    type: video
    url: https://vimeo.com/305198062
  author:
  - first: Joo-Kyung
    full: Joo-Kyung Kim
    id: joo-kyung-kim
    last: Kim
  - first: Young-Bum
    full: Young-Bum Kim
    id: young-bum-kim
    last: Kim
  author_string: Joo-Kyung Kim, Young-Bum Kim
  bibkey: kim-kim-2018-supervised
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1106
  month: October-November
  page_first: '894'
  page_last: '899'
  pages: "894\u2013899"
  paper_id: '106'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1106.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1106.jpg
  title: Supervised Domain Enablement Attention for Personalized Domain Classification
  title_html: Supervised Domain Enablement Attention for Personalized Domain Classification
  url: https://www.aclweb.org/anthology/D18-1106
  year: '2018'
D18-1107:
  abstract: In the sentence classification task, context formed from sentences adjacent
    to the sentence being classified can provide important information for classification.
    This context is, however, often ignored. Where methods do make use of context,
    only small amounts are considered, making it difficult to scale. We present a
    new method for sentence classification, Context-LSTM-CNN, that makes use of potentially
    large contexts. The method also utilizes long-range dependencies within the sentence
    being classified, using an LSTM, and short-span features, using a stacked CNN.
    Our experiments demonstrate that this approach consistently improves over previous
    methods on two different datasets.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305198319
    type: video
    url: https://vimeo.com/305198319
  author:
  - first: Xingyi
    full: Xingyi Song
    id: xingyi-song
    last: Song
  - first: Johann
    full: Johann Petrak
    id: johann-petrak
    last: Petrak
  - first: Angus
    full: Angus Roberts
    id: angus-roberts
    last: Roberts
  author_string: Xingyi Song, Johann Petrak, Angus Roberts
  bibkey: song-etal-2018-deep
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1107
  month: October-November
  page_first: '900'
  page_last: '904'
  pages: "900\u2013904"
  paper_id: '107'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1107.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1107.jpg
  title: A Deep Neural Network Sentence Level Classification Method with Context Information
  title_html: A Deep Neural Network Sentence Level Classification Method with Context
    Information
  url: https://www.aclweb.org/anthology/D18-1107
  year: '2018'
D18-1108:
  abstract: "Deep NLP models benefit from underlying structures in the data\u2014\
    e.g., parse trees\u2014typically extracted using off-the-shelf parsers. Recent\
    \ attempts to jointly learn the latent structure encounter a tradeoff: either\
    \ make factorization assumptions that limit expressiveness, or sacrifice end-to-end\
    \ differentiability. Using the recently proposed SparseMAP inference, which retrieves\
    \ a sparse distribution over latent structures, we propose a novel approach for\
    \ end-to-end learning of latent structure predictors jointly with a downstream\
    \ predictor. To the best of our knowledge, our method is the first to enable unrestricted\
    \ dynamic computation graph construction from the global latent structure, while\
    \ maintaining differentiability."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1108.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1108.Attachment.zip
  - filename: https://vimeo.com/305198410
    type: video
    url: https://vimeo.com/305198410
  author:
  - first: Vlad
    full: Vlad Niculae
    id: vlad-niculae
    last: Niculae
  - first: "Andr\xE9 F. T."
    full: "Andr\xE9 F. T. Martins"
    id: andre-f-t-martins
    last: Martins
  - first: Claire
    full: Claire Cardie
    id: claire-cardie
    last: Cardie
  author_string: "Vlad Niculae, Andr\xE9 F. T. Martins, Claire Cardie"
  bibkey: niculae-etal-2018-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1108
  month: October-November
  page_first: '905'
  page_last: '911'
  pages: "905\u2013911"
  paper_id: '108'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1108.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1108.jpg
  title: Towards Dynamic Computation Graphs via Sparse Latent Structure
  title_html: Towards Dynamic Computation Graphs via Sparse Latent Structure
  url: https://www.aclweb.org/anthology/D18-1108
  year: '2018'
D18-1109:
  abstract: We introduce a class of convolutional neural networks (CNNs) that utilize
    recurrent neural networks (RNNs) as convolution filters. A convolution filter
    is typically implemented as a linear affine transformation followed by a non-linear
    function, which fails to account for language compositionality. As a result, it
    limits the use of high-order filters that are often warranted for natural language
    processing tasks. In this work, we model convolution filters with RNNs that naturally
    capture compositionality and long-term dependencies in language. We show that
    simple CNN architectures equipped with recurrent neural filters (RNFs) achieve
    results that are on par with the best published ones on the Stanford Sentiment
    Treebank and two answer sentence selection datasets.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305198501
    type: video
    url: https://vimeo.com/305198501
  author:
  - first: Yi
    full: Yi Yang
    id: yi-yang
    last: Yang
  author_string: Yi Yang
  bibkey: yang-2018-convolutional
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1109
  month: October-November
  page_first: '912'
  page_last: '917'
  pages: "912\u2013917"
  paper_id: '109'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1109.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1109.jpg
  title: Convolutional Neural Networks with Recurrent Neural Filters
  title_html: Convolutional Neural Networks with Recurrent Neural Filters
  url: https://www.aclweb.org/anthology/D18-1109
  year: '2018'
D18-1110:
  abstract: Existing neural semantic parsers mainly utilize a sequence encoder, i.e.,
    a sequential LSTM, to extract word order features while neglecting other valuable
    syntactic information such as dependency or constituent trees. In this paper,
    we first propose to use the syntactic graph to represent three types of syntactic
    information, i.e., word order, dependency and constituency features; then employ
    a graph-to-sequence model to encode the syntactic graph and decode a logical form.
    Experimental results on benchmark datasets show that our model is comparable to
    the state-of-the-art on Jobs640, ATIS, and Geo880. Experimental results on adversarial
    examples demonstrate the robustness of the model is also improved by encoding
    more syntactic information.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305213182
    type: video
    url: https://vimeo.com/305213182
  author:
  - first: Kun
    full: Kun Xu
    id: kun-xu
    last: Xu
  - first: Lingfei
    full: Lingfei Wu
    id: lingfei-wu
    last: Wu
  - first: Zhiguo
    full: Zhiguo Wang
    id: zhiguo-wang
    last: Wang
  - first: Mo
    full: Mo Yu
    id: mo-yu
    last: Yu
  - first: Liwei
    full: Liwei Chen
    id: liwei-chen
    last: Chen
  - first: Vadim
    full: Vadim Sheinin
    id: vadim-sheinin
    last: Sheinin
  author_string: Kun Xu, Lingfei Wu, Zhiguo Wang, Mo Yu, Liwei Chen, Vadim Sheinin
  bibkey: xu-etal-2018-exploiting
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1110
  month: October-November
  page_first: '918'
  page_last: '924'
  pages: "918\u2013924"
  paper_id: '110'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1110.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1110.jpg
  title: Exploiting Rich Syntactic Information for Semantic Parsing with Graph-to-Sequence
    Model
  title_html: Exploiting Rich Syntactic Information for Semantic Parsing with Graph-to-Sequence
    Model
  url: https://www.aclweb.org/anthology/D18-1110
  year: '2018'
D18-1111:
  abstract: In models to generate program source code from natural language, representing
    this code in a tree structure has been a common approach. However, existing methods
    often fail to generate complex code correctly due to a lack of ability to memorize
    large and complex structures. We introduce RECODE, a method based on subtree retrieval
    that makes it possible to explicitly reference existing code examples within a
    neural code generation model. First, we retrieve sentences that are similar to
    input sentences using a dynamic-programming-based sentence similarity scoring
    method. Next, we extract n-grams of action sequences that build the associated
    abstract syntax tree. Finally, we increase the probability of actions that cause
    the retrieved n-gram action subtree to be in the predicted code. We show that
    our approach improves the performance on two code generation tasks by up to +2.6
    BLEU.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305213468
    type: video
    url: https://vimeo.com/305213468
  author:
  - first: Shirley Anugrah
    full: Shirley Anugrah Hayati
    id: shirley-anugrah-hayati
    last: Hayati
  - first: Raphael
    full: Raphael Olivier
    id: raphael-olivier
    last: Olivier
  - first: Pravalika
    full: Pravalika Avvaru
    id: pravalika-avvaru
    last: Avvaru
  - first: Pengcheng
    full: Pengcheng Yin
    id: pengcheng-yin
    last: Yin
  - first: Anthony
    full: Anthony Tomasic
    id: anthony-tomasic
    last: Tomasic
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  author_string: Shirley Anugrah Hayati, Raphael Olivier, Pravalika Avvaru, Pengcheng
    Yin, Anthony Tomasic, Graham Neubig
  bibkey: hayati-etal-2018-retrieval
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1111
  month: October-November
  page_first: '925'
  page_last: '930'
  pages: "925\u2013930"
  paper_id: '111'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1111.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1111.jpg
  title: Retrieval-Based Neural Code Generation
  title_html: Retrieval-Based Neural Code Generation
  url: https://www.aclweb.org/anthology/D18-1111
  year: '2018'
D18-1112:
  abstract: Previous work approaches the SQL-to-text generation task using vanilla
    Seq2Seq models, which may not fully capture the inherent graph-structured information
    in SQL query. In this paper, we propose a graph-to-sequence model to encode the
    global structure information into node embeddings. This model can effectively
    learn the correlation between the SQL query pattern and its interpretation. Experimental
    results on the WikiSQL dataset and Stackoverflow dataset show that our model outperforms
    the Seq2Seq and Tree2Seq baselines, achieving the state-of-the-art performance.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305213739
    type: video
    url: https://vimeo.com/305213739
  author:
  - first: Kun
    full: Kun Xu
    id: kun-xu
    last: Xu
  - first: Lingfei
    full: Lingfei Wu
    id: lingfei-wu
    last: Wu
  - first: Zhiguo
    full: Zhiguo Wang
    id: zhiguo-wang
    last: Wang
  - first: Yansong
    full: Yansong Feng
    id: yansong-feng
    last: Feng
  - first: Vadim
    full: Vadim Sheinin
    id: vadim-sheinin
    last: Sheinin
  author_string: Kun Xu, Lingfei Wu, Zhiguo Wang, Yansong Feng, Vadim Sheinin
  bibkey: xu-etal-2018-sql
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1112
  month: October-November
  page_first: '931'
  page_last: '936'
  pages: "931\u2013936"
  paper_id: '112'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1112.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1112.jpg
  title: SQL-to-Text Generation with Graph-to-Sequence Model
  title_html: <span class="acl-fixed-case">SQL</span>-to-Text Generation with Graph-to-Sequence
    Model
  url: https://www.aclweb.org/anthology/D18-1112
  year: '2018'
D18-1113:
  abstract: 'We study the automatic generation of syntactic paraphrases using four
    different models for generation: data-to-text generation, text-to-text generation,
    text reduction and text expansion, We derive training data for each of these tasks
    from the WebNLG dataset and we show (i) that conditioning generation on syntactic
    constraints effectively permits the generation of syntactically distinct paraphrases
    for the same input and (ii) that exploiting different types of input (data, text
    or data+text) further increases the number of distinct paraphrases that can be
    generated for a given input.'
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305214075
    type: video
    url: https://vimeo.com/305214075
  author:
  - first: Emilie
    full: Emilie Colin
    id: emilie-colin
    last: Colin
  - first: Claire
    full: Claire Gardent
    id: claire-gardent
    last: Gardent
  author_string: Emilie Colin, Claire Gardent
  bibkey: colin-gardent-2018-generating
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1113
  month: October-November
  page_first: '937'
  page_last: '943'
  pages: "937\u2013943"
  paper_id: '113'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1113.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1113.jpg
  title: Generating Syntactic Paraphrases
  title_html: Generating Syntactic Paraphrases
  url: https://www.aclweb.org/anthology/D18-1113
  year: '2018'
D18-1114:
  abstract: 'We present a model for semantic proto-role labeling (SPRL) using an adapted
    bidirectional LSTM encoding strategy that we call NeuralDavidsonian: predicate-argument
    structure is represented as pairs of hidden states corresponding to predicate
    and argument head tokens of the input sequence. We demonstrate: (1) state-of-the-art
    results in SPRL, and (2) that our network naturally shares parameters between
    attributes, allowing for learning new attribute types with limited added supervision.'
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305214361
    type: video
    url: https://vimeo.com/305214361
  author:
  - first: Rachel
    full: Rachel Rudinger
    id: rachel-rudinger
    last: Rudinger
  - first: Adam
    full: Adam Teichert
    id: adam-teichert
    last: Teichert
  - first: Ryan
    full: Ryan Culkin
    id: ryan-culkin
    last: Culkin
  - first: Sheng
    full: Sheng Zhang
    id: sheng-zhang
    last: Zhang
  - first: Benjamin
    full: Benjamin Van Durme
    id: benjamin-van-durme
    last: Van Durme
  author_string: Rachel Rudinger, Adam Teichert, Ryan Culkin, Sheng Zhang, Benjamin
    Van Durme
  bibkey: rudinger-etal-2018-neural-davidsonian
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1114
  month: October-November
  page_first: '944'
  page_last: '955'
  pages: "944\u2013955"
  paper_id: '114'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1114.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1114.jpg
  title: Neural-Davidsonian Semantic Proto-role Labeling
  title_html: Neural-Davidsonian Semantic Proto-role Labeling
  url: https://www.aclweb.org/anthology/D18-1114
  year: '2018'
D18-1115:
  abstract: "Styles of leaders when they make decisions in groups vary, and the different\
    \ styles affect the performance of the group. To understand the key words and\
    \ speakers associated with decisions, we initially formalize the problem as one\
    \ of predicting leaders\u2019 decisions from discussion with group members. As\
    \ a dataset, we introduce conversational meeting records from a historical corpus,\
    \ and develop a hierarchical RNN structure with attention and pre-trained speaker\
    \ embedding in the form of a, Conversational Decision Making Model (CDMM). The\
    \ CDMM outperforms other baselines to predict leaders\u2019 final decisions from\
    \ the data. We explain why CDMM works better than other methods by showing the\
    \ key words and speakers discovered from the attentions as evidence."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306361321
    type: video
    url: https://vimeo.com/306361321
  author:
  - first: JinYeong
    full: JinYeong Bak
    id: jinyeong-bak
    last: Bak
  - first: Alice
    full: Alice Oh
    id: alice-oh
    last: Oh
  author_string: JinYeong Bak, Alice Oh
  bibkey: bak-oh-2018-conversational
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1115
  month: October-November
  page_first: '956'
  page_last: '961'
  pages: "956\u2013961"
  paper_id: '115'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1115.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1115.jpg
  title: "Conversational Decision-Making Model for Predicting the King\u2019s Decision\
    \ in the Annals of the Joseon Dynasty"
  title_html: "Conversational Decision-Making Model for Predicting the King\u2019\
    s Decision in the Annals of the Joseon Dynasty"
  url: https://www.aclweb.org/anthology/D18-1115
  year: '2018'
D18-1116:
  abstract: Discourse segmentation, which segments texts into Elementary Discourse
    Units, is a fundamental step in discourse analysis. Previous discourse segmenters
    rely on complicated hand-crafted features and are not practical in actual use.
    In this paper, we propose an end-to-end neural segmenter based on BiLSTM-CRF framework.
    To improve its accuracy, we address the problem of data insufficiency by transferring
    a word representation model that is trained on a large corpus. We also propose
    a restricted self-attention mechanism in order to capture useful information within
    a neighborhood. Experiments on the RST-DT corpus show that our model is significantly
    faster than previous methods, while achieving new state-of-the-art performance.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306361340
    type: video
    url: https://vimeo.com/306361340
  author:
  - first: Yizhong
    full: Yizhong Wang
    id: yizhong-wang
    last: Wang
  - first: Sujian
    full: Sujian Li
    id: sujian-li
    last: Li
  - first: Jingfeng
    full: Jingfeng Yang
    id: jingfeng-yang
    last: Yang
  author_string: Yizhong Wang, Sujian Li, Jingfeng Yang
  bibkey: wang-etal-2018-toward
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1116
  month: October-November
  page_first: '962'
  page_last: '967'
  pages: "962\u2013967"
  paper_id: '116'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1116.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1116.jpg
  title: Toward Fast and Accurate Neural Discourse Segmentation
  title_html: Toward Fast and Accurate Neural Discourse Segmentation
  url: https://www.aclweb.org/anthology/D18-1116
  year: '2018'
D18-1117:
  abstract: Video content on social media platforms constitutes a major part of the
    communication between people, as it allows everyone to share their stories. However,
    if someone is unable to consume video, either due to a disability or network bandwidth,
    this severely limits their participation and communication. Automatically telling
    the stories using multi-sentence descriptions of videos would allow bridging this
    gap. To learn and evaluate such models, we introduce VideoStory a new large-scale
    dataset for video description as a new challenge for multi-sentence video description.
    Our VideoStory captions dataset is complementary to prior work and contains 20k
    videos posted publicly on a social media platform amounting to 396 hours of video
    with 123k sentences, temporally aligned to the video.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306361803
    type: video
    url: https://vimeo.com/306361803
  author:
  - first: Spandana
    full: Spandana Gella
    id: spandana-gella
    last: Gella
  - first: Mike
    full: Mike Lewis
    id: mike-lewis
    last: Lewis
  - first: Marcus
    full: Marcus Rohrbach
    id: marcus-rohrbach
    last: Rohrbach
  author_string: Spandana Gella, Mike Lewis, Marcus Rohrbach
  bibkey: gella-etal-2018-dataset
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1117
  month: October-November
  page_first: '968'
  page_last: '974'
  pages: "968\u2013974"
  paper_id: '117'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1117.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1117.jpg
  title: A Dataset for Telling the Stories of Social Media Videos
  title_html: A Dataset for Telling the Stories of Social Media Videos
  url: https://www.aclweb.org/anthology/D18-1117
  year: '2018'
D18-1118:
  abstract: 'Visual reasoning is a special visual question answering problem that
    is multi-step and compositional by nature, and also requires intensive text-vision
    interactions. We propose CMM: Cascaded Mutual Modulation as a novel end-to-end
    visual reasoning model. CMM includes a multi-step comprehension process for both
    question and image. In each step, we use a Feature-wise Linear Modulation (FiLM)
    technique to enable textual/visual pipeline to mutually control each other. Experiments
    show that CMM significantly outperforms most related models, and reach state-of-the-arts
    on two visual reasoning benchmarks: CLEVR and NLVR, collected from both synthetic
    and natural languages. Ablation studies confirm the effectiveness of CMM to comprehend
    natural language logics under the guidence of images. Our code is available at
    https://github.com/FlamingHorizon/CMM-VR.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1118.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1118.Attachment.zip
  - filename: https://vimeo.com/306362249
    type: video
    url: https://vimeo.com/306362249
  author:
  - first: Yiqun
    full: Yiqun Yao
    id: yiqun-yao
    last: Yao
  - first: Jiaming
    full: Jiaming Xu
    id: jiaming-xu
    last: Xu
  - first: Feng
    full: Feng Wang
    id: feng-wang
    last: Wang
  - first: Bo
    full: Bo Xu
    id: bo-xu
    last: Xu
  author_string: Yiqun Yao, Jiaming Xu, Feng Wang, Bo Xu
  bibkey: yao-etal-2018-cascaded
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1118
  month: October-November
  page_first: '975'
  page_last: '980'
  pages: "975\u2013980"
  paper_id: '118'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1118.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1118.jpg
  title: Cascaded Mutual Modulation for Visual Reasoning
  title_html: Cascaded Mutual Modulation for Visual Reasoning
  url: https://www.aclweb.org/anthology/D18-1118
  year: '2018'
D18-1119:
  abstract: "There is growing interest in the language developed by agents interacting\
    \ in emergent-communication settings. Earlier studies have focused on the agents\u2019\
    \ symbol usage, rather than on their representation of visual input. In this paper,\
    \ we consider the referential games of Lazaridou et al. (2017), and investigate\
    \ the representations the agents develop during their evolving interaction. We\
    \ find that the agents establish successful communication by inducing visual representations\
    \ that almost perfectly align with each other, but, surprisingly, do not capture\
    \ the conceptual properties of the objects depicted in the input images. We conclude\
    \ that, if we care about developing language-like communication systems, we must\
    \ pay more attention to the visual semantics agents associate to the symbols they\
    \ use."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306362292
    type: video
    url: https://vimeo.com/306362292
  author:
  - first: Diane
    full: Diane Bouchacourt
    id: diane-bouchacourt
    last: Bouchacourt
  - first: Marco
    full: Marco Baroni
    id: marco-baroni
    last: Baroni
  author_string: Diane Bouchacourt, Marco Baroni
  bibkey: bouchacourt-baroni-2018-agents
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1119
  month: October-November
  page_first: '981'
  page_last: '985'
  pages: "981\u2013985"
  paper_id: '119'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1119.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1119.jpg
  title: 'How agents see things: On visual representations in an emergent language
    game'
  title_html: 'How agents see things: On visual representations in an emergent language
    game'
  url: https://www.aclweb.org/anthology/D18-1119
  year: '2018'
D18-1120:
  abstract: A capsule is a group of neurons, whose activity vector represents the
    instantiation parameters of a specific type of entity. In this paper, we explore
    the capsule networks used for relation extraction in a multi-instance multi-label
    learning framework and propose a novel neural approach based on capsule networks
    with attention mechanisms. We evaluate our method with different benchmarks, and
    it is demonstrated that our method improves the precision of the predicted relations.
    Particularly, we show that capsule networks improve multiple entity pairs relation
    extraction.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1120.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1120.Attachment.zip
  author:
  - first: Ningyu
    full: Ningyu Zhang
    id: ningyu-zhang
    last: Zhang
  - first: Shumin
    full: Shumin Deng
    id: shumin-deng
    last: Deng
  - first: Zhanling
    full: Zhanling Sun
    id: zhanling-sun
    last: Sun
  - first: Xi
    full: Xi Chen
    id: xi-chen
    last: Chen
  - first: Wei
    full: Wei Zhang
    id: wei-zhang
    last: Zhang
  - first: Huajun
    full: Huajun Chen
    id: huajun-chen
    last: Chen
  author_string: Ningyu Zhang, Shumin Deng, Zhanling Sun, Xi Chen, Wei Zhang, Huajun
    Chen
  bibkey: zhang-etal-2018-attention
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1120
  month: October-November
  page_first: '986'
  page_last: '992'
  pages: "986\u2013992"
  paper_id: '120'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1120.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1120.jpg
  title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction
  title_html: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction
  url: https://www.aclweb.org/anthology/D18-1120
  year: '2018'
D18-1121:
  abstract: Entity typing aims to classify semantic types of an entity mention in
    a specific context. Most existing models obtain training data using distant supervision,
    and inevitably suffer from the problem of noisy labels. To address this issue,
    we propose entity typing with language model enhancement. It utilizes a language
    model to measure the compatibility between context sentences and labels, and thereby
    automatically focuses more on context-dependent labels. Experiments on benchmark
    datasets demonstrate that our method is capable of enhancing the entity typing
    model with information from the language model, and significantly outperforms
    the state-of-the-art baseline. Code and data for this paper can be found from
    https://github.com/thunlp/LME.
  address: Brussels, Belgium
  author:
  - first: Ji
    full: Ji Xin
    id: ji-xin
    last: Xin
  - first: Hao
    full: Hao Zhu
    id: hao-zhu
    last: Zhu
  - first: Xu
    full: Xu Han
    id: xu-han
    last: Han
  - first: Zhiyuan
    full: Zhiyuan Liu
    id: zhiyuan-liu
    last: Liu
  - first: Maosong
    full: Maosong Sun
    id: maosong-sun
    last: Sun
  author_string: Ji Xin, Hao Zhu, Xu Han, Zhiyuan Liu, Maosong Sun
  bibkey: xin-etal-2018-put
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1121
  month: October-November
  page_first: '993'
  page_last: '998'
  pages: "993\u2013998"
  paper_id: '121'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1121.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1121.jpg
  title: 'Put It Back: Entity Typing with Language Model Enhancement'
  title_html: 'Put It Back: Entity Typing with Language Model Enhancement'
  url: https://www.aclweb.org/anthology/D18-1121
  year: '2018'
D18-1122:
  abstract: Detecting events and classifying them into predefined types is an important
    step in knowledge extraction from natural language texts. While the neural network
    models have generally led the state-of-the-art, the differences in performance
    between different architectures have not been rigorously studied. In this paper
    we present a novel GRU-based model that combines syntactic information along with
    temporal structure through an attention mechanism. We show that it is competitive
    with other neural network architectures through empirical evaluations under different
    random initializations and training-validation-test splits of ACE2005 dataset.
  address: Brussels, Belgium
  author:
  - first: Walker
    full: Walker Orr
    id: j-walker-orr
    last: Orr
  - first: Prasad
    full: Prasad Tadepalli
    id: prasad-tadepalli
    last: Tadepalli
  - first: Xiaoli
    full: Xiaoli Fern
    id: xiaoli-fern
    last: Fern
  author_string: Walker Orr, Prasad Tadepalli, Xiaoli Fern
  bibkey: orr-etal-2018-event
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1122
  month: October-November
  page_first: '999'
  page_last: '1004'
  pages: "999\u20131004"
  paper_id: '122'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1122.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1122.jpg
  title: 'Event Detection with Neural Networks: A Rigorous Empirical Evaluation'
  title_html: 'Event Detection with Neural Networks: A Rigorous Empirical Evaluation'
  url: https://www.aclweb.org/anthology/D18-1122
  year: '2018'
D18-1123:
  abstract: "Publication information in a researcher\u2019s academic homepage provides\
    \ insights about the researcher\u2019s expertise, research interests, and collaboration\
    \ networks. We aim to extract all the publication strings from a given academic\
    \ homepage. This is a challenging task because the publication strings in different\
    \ academic homepages may be located at different positions with different structures.\
    \ To capture the positional and structural diversity, we propose an end-to-end\
    \ hierarchical model named PubSE based on Bi-LSTM-CRF. We further propose an alternating\
    \ training method for training the model. Experiments on real data show that PubSE\
    \ outperforms the state-of-the-art models by up to 11.8% in F1-score."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1123.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1123.Attachment.pdf
  author:
  - first: Yiqing
    full: Yiqing Zhang
    id: yiqing-zhang
    last: Zhang
  - first: Jianzhong
    full: Jianzhong Qi
    id: jianzhong-qi
    last: Qi
  - first: Rui
    full: Rui Zhang
    id: rui-zhang
    last: Zhang
  - first: Chuandong
    full: Chuandong Yin
    id: chuandong-yin
    last: Yin
  author_string: Yiqing Zhang, Jianzhong Qi, Rui Zhang, Chuandong Yin
  bibkey: zhang-etal-2018-pubse
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1123
  month: October-November
  page_first: '1005'
  page_last: '1010'
  pages: "1005\u20131010"
  paper_id: '123'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1123.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1123.jpg
  title: 'PubSE: A Hierarchical Model for Publication Extraction from Academic Homepages'
  title_html: '<span class="acl-fixed-case">P</span>ub<span class="acl-fixed-case">SE</span>:
    A Hierarchical Model for Publication Extraction from Academic Homepages'
  url: https://www.aclweb.org/anthology/D18-1123
  year: '2018'
D18-1124:
  abstract: It is common that entity mentions can contain other mentions recursively.
    This paper introduces a scalable transition-based method to model the nested structure
    of mentions. We first map a sentence with nested mentions to a designated forest
    where each mention corresponds to a constituent of the forest. Our shift-reduce
    based system then learns to construct the forest structure in a bottom-up manner
    through an action sequence whose maximal length is guaranteed to be three times
    of the sentence length. Based on Stack-LSTM which is employed to efficiently and
    effectively represent the states of the system in a continuous space, our system
    is further incorporated with a character-based component to capture letter-level
    patterns. Our model gets the state-of-the-art performances in ACE datasets, showing
    its effectiveness in detecting nested mentions.
  address: Brussels, Belgium
  author:
  - first: Bailin
    full: Bailin Wang
    id: bailin-wang
    last: Wang
  - first: Wei
    full: Wei Lu
    id: wei-lu
    last: Lu
  - first: Yu
    full: Yu Wang
    id: yu-wang
    last: Wang
  - first: Hongxia
    full: Hongxia Jin
    id: hongxia-jin
    last: Jin
  author_string: Bailin Wang, Wei Lu, Yu Wang, Hongxia Jin
  bibkey: wang-etal-2018-neural-transition
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1124
  month: October-November
  page_first: '1011'
  page_last: '1017'
  pages: "1011\u20131017"
  paper_id: '124'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1124.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1124.jpg
  title: A Neural Transition-based Model for Nested Mention Recognition
  title_html: A Neural Transition-based Model for Nested Mention Recognition
  url: https://www.aclweb.org/anthology/D18-1124
  year: '2018'
D18-1125:
  abstract: Relation Extraction suffers from dramatical performance decrease when
    training a model on one genre and directly applying it to a new genre, due to
    the distinct feature distributions. Previous studies address this problem by discovering
    a shared space across genres using manually crafted features, which requires great
    human effort. To effectively automate this process, we design a genre-separation
    network, which applies two encoders, one genre-independent and one genre-shared,
    to explicitly extract genre-specific and genre-agnostic features. Then we train
    a relation classifier using the genre-agnostic features on the source genre and
    directly apply to the target genre. Experiment results on three distinct genres
    of the ACE dataset show that our approach achieves up to 6.1% absolute F1-score
    gain compared to previous methods. By incorporating a set of external linguistic
    features, our approach outperforms the state-of-the-art by 1.7% absolute F1 gain.
    We make all programs of our model publicly available for research purpose
  address: Brussels, Belgium
  author:
  - first: Ge
    full: Ge Shi
    id: ge-shi
    last: Shi
  - first: Chong
    full: Chong Feng
    id: chong-feng
    last: Feng
  - first: Lifu
    full: Lifu Huang
    id: lifu-huang
    last: Huang
  - first: Boliang
    full: Boliang Zhang
    id: boliang-zhang
    last: Zhang
  - first: Heng
    full: Heng Ji
    id: heng-ji
    last: Ji
  - first: Lejian
    full: Lejian Liao
    id: lejian-liao
    last: Liao
  - first: Heyan
    full: Heyan Huang
    id: he-yan-huang
    last: Huang
  author_string: Ge Shi, Chong Feng, Lifu Huang, Boliang Zhang, Heng Ji, Lejian Liao,
    Heyan Huang
  bibkey: shi-etal-2018-genre
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1125
  month: October-November
  page_first: '1018'
  page_last: '1023'
  pages: "1018\u20131023"
  paper_id: '125'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1125.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1125.jpg
  title: Genre Separation Network with Adversarial Training for Cross-genre Relation
    Extraction
  title_html: Genre Separation Network with Adversarial Training for Cross-genre Relation
    Extraction
  url: https://www.aclweb.org/anthology/D18-1125
  year: '2018'
D18-1126:
  abstract: "To disambiguate between closely related concepts, entity linking systems\
    \ need to effectively distill cues from their context, which may be quite noisy.\
    \ We investigate several techniques for using these cues in the context of noisy\
    \ entity linking on short texts. Our starting point is a state-of-the-art attention-based\
    \ model from prior work; while this model\u2019s attention typically identifies\
    \ context that is topically relevant, it fails to identify some of the most indicative\
    \ surface strings, especially those exhibiting lexical overlap with the true title.\
    \ Augmenting the model with convolutional networks over characters still leaves\
    \ it largely unable to pick up on these cues compared to sparse features that\
    \ target them directly, indicating that automatically learning how to identify\
    \ relevant character-level context features is a hard problem. Our final system\
    \ outperforms past work on the WikilinksNED test set by 2.8% absolute."
  address: Brussels, Belgium
  author:
  - first: David
    full: David Mueller
    id: david-mueller
    last: Mueller
  - first: Greg
    full: Greg Durrett
    id: greg-durrett
    last: Durrett
  author_string: David Mueller, Greg Durrett
  bibkey: mueller-durrett-2018-effective
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1126
  month: October-November
  page_first: '1024'
  page_last: '1029'
  pages: "1024\u20131029"
  paper_id: '126'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1126.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1126.jpg
  title: Effective Use of Context in Noisy Entity Linking
  title_html: Effective Use of Context in Noisy Entity Linking
  url: https://www.aclweb.org/anthology/D18-1126
  year: '2018'
D18-1127:
  abstract: The task of event detection involves identifying and categorizing event
    triggers. Contextual information has been shown effective on the task. However,
    existing methods which utilize contextual information only process the context
    once. We argue that the context can be better exploited by processing the context
    multiple times, allowing the model to perform complex reasoning and to generate
    better context representation, thus improving the overall performance. Meanwhile,
    dynamic memory network (DMN) has demonstrated promising capability in capturing
    contextual information and has been applied successfully to various tasks. In
    light of the multi-hop mechanism of the DMN to model the context, we propose the
    trigger detection dynamic memory network (TD-DMN) to tackle the event detection
    problem. We performed a five-fold cross-validation on the ACE-2005 dataset and
    experimental results show that the multi-hop mechanism does improve the performance
    and the proposed model achieves best F1 score compared to the state-of-the-art
    methods.
  address: Brussels, Belgium
  author:
  - first: Shaobo
    full: Shaobo Liu
    id: shaobo-liu
    last: Liu
  - first: Rui
    full: Rui Cheng
    id: rui-cheng
    last: Cheng
  - first: Xiaoming
    full: Xiaoming Yu
    id: xiaoming-yu
    last: Yu
  - first: Xueqi
    full: Xueqi Cheng
    id: xueqi-cheng
    last: Cheng
  author_string: Shaobo Liu, Rui Cheng, Xiaoming Yu, Xueqi Cheng
  bibkey: liu-etal-2018-exploiting-contextual
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1127
  month: October-November
  page_first: '1030'
  page_last: '1035'
  pages: "1030\u20131035"
  paper_id: '127'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1127.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1127.jpg
  title: Exploiting Contextual Information via Dynamic Memory Network for Event Detection
  title_html: Exploiting Contextual Information via Dynamic Memory Network for Event
    Detection
  url: https://www.aclweb.org/anthology/D18-1127
  year: '2018'
D18-1128:
  abstract: "A rich line of research attempts to make deep neural networks more transparent\
    \ by generating human-interpretable \u2018explanations\u2019 of their decision\
    \ process, especially for interactive tasks like Visual Question Answering (VQA).\
    \ In this work, we analyze if existing explanations indeed make a VQA model \u2014\
    \ its responses as well as failures \u2014 more predictable to a human. Surprisingly,\
    \ we find that they do not. On the other hand, we find that human-in-the-loop\
    \ approaches that treat the model as a black-box do."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1128.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1128.Attachment.zip
  author:
  - first: Arjun
    full: Arjun Chandrasekaran
    id: arjun-chandrasekaran
    last: Chandrasekaran
  - first: Viraj
    full: Viraj Prabhu
    id: viraj-prabhu
    last: Prabhu
  - first: Deshraj
    full: Deshraj Yadav
    id: deshraj-yadav
    last: Yadav
  - first: Prithvijit
    full: Prithvijit Chattopadhyay
    id: prithvijit-chattopadhyay
    last: Chattopadhyay
  - first: Devi
    full: Devi Parikh
    id: devi-parikh
    last: Parikh
  author_string: Arjun Chandrasekaran, Viraj Prabhu, Deshraj Yadav, Prithvijit Chattopadhyay,
    Devi Parikh
  bibkey: chandrasekaran-etal-2018-explanations
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1128
  month: October-November
  page_first: '1036'
  page_last: '1042'
  pages: "1036\u20131042"
  paper_id: '128'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1128.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1128.jpg
  title: Do explanations make VQA models more predictable to a human?
  title_html: Do explanations make <span class="acl-fixed-case">VQA</span> models
    more predictable to a human?
  url: https://www.aclweb.org/anthology/D18-1128
  year: '2018'
D18-1129:
  abstract: 'This work introduces fact salience: The task of generating a machine-readable
    representation of the most prominent information in a text document as a set of
    facts. We also present SalIE, the first fact salience system. SalIE is unsupervised
    and knowledge agnostic, based on open information extraction to detect facts in
    natural language text, PageRank to determine their relevance, and clustering to
    promote diversity. We compare SalIE with several baselines (including positional,
    standard for saliency tasks), and in an extrinsic evaluation, with state-of-the-art
    automatic text summarizers. SalIE outperforms baselines and text summarizers showing
    that facts are an effective way to compress information.'
  address: Brussels, Belgium
  author:
  - first: Marco
    full: Marco Ponza
    id: marco-ponza
    last: Ponza
  - first: Luciano
    full: Luciano Del Corro
    id: luciano-del-corro1
    last: Del Corro
  - first: Gerhard
    full: Gerhard Weikum
    id: gerhard-weikum
    last: Weikum
  author_string: Marco Ponza, Luciano Del Corro, Gerhard Weikum
  bibkey: ponza-etal-2018-facts
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1129
  month: October-November
  page_first: '1043'
  page_last: '1048'
  pages: "1043\u20131048"
  paper_id: '129'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1129.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1129.jpg
  title: Facts That Matter
  title_html: Facts That Matter
  url: https://www.aclweb.org/anthology/D18-1129
  year: '2018'
D18-1130:
  abstract: 'Recent work has improved on modeling for reading comprehension tasks
    with simple approaches such as the Attention Sum-Reader; however, automatic systems
    still significantly trail human performance. Analysis suggests that many of the
    remaining hard instances are related to the inability to track entity-references
    throughout documents. This work focuses on these hard entity tracking cases with
    two extensions: (1) additional entity features, and (2) training with a multi-task
    tracking objective. We show that these simple modifications improve performance
    both independently and in combination, and we outperform the previous state of
    the art on the LAMBADA dataset by 8 pts, particularly on difficult entity examples.
    We also effectively match the performance of more complicated models on the named
    entity portion of the CBT dataset.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1130.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1130.Attachment.pdf
  author:
  - first: Luong
    full: Luong Hoang
    id: luong-hoang
    last: Hoang
  - first: Sam
    full: Sam Wiseman
    id: sam-wiseman
    last: Wiseman
  - first: Alexander
    full: Alexander Rush
    id: alexander-m-rush
    last: Rush
  author_string: Luong Hoang, Sam Wiseman, Alexander Rush
  bibkey: hoang-etal-2018-entity
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1130
  month: October-November
  page_first: '1049'
  page_last: '1055'
  pages: "1049\u20131055"
  paper_id: '130'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1130.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1130.jpg
  title: Entity Tracking Improves Cloze-style Reading Comprehension
  title_html: Entity Tracking Improves Cloze-style Reading Comprehension
  url: https://www.aclweb.org/anthology/D18-1130
  year: '2018'
D18-1131:
  abstract: We address the problem of detecting duplicate questions in forums, which
    is an important step towards automating the process of answering new questions.
    As finding and annotating such potential duplicates manually is very tedious and
    costly, automatic methods based on machine learning are a viable alternative.
    However, many forums do not have annotated data, i.e., questions labeled by experts
    as duplicates, and thus a promising solution is to use domain adaptation from
    another forum that has such annotations. Here we focus on adversarial domain adaptation,
    deriving important findings about when it performs well and what properties of
    the domains are important in this regard. Our experiments with StackExchange data
    show an average improvement of 5.6% over the best baseline across multiple pairs
    of domains.
  address: Brussels, Belgium
  author:
  - first: Darsh
    full: Darsh Shah
    id: darsh-shah
    last: Shah
  - first: Tao
    full: Tao Lei
    id: tao-lei
    last: Lei
  - first: Alessandro
    full: Alessandro Moschitti
    id: alessandro-moschitti
    last: Moschitti
  - first: Salvatore
    full: Salvatore Romeo
    id: salvatore-romeo
    last: Romeo
  - first: Preslav
    full: Preslav Nakov
    id: preslav-nakov
    last: Nakov
  author_string: Darsh Shah, Tao Lei, Alessandro Moschitti, Salvatore Romeo, Preslav
    Nakov
  bibkey: shah-etal-2018-adversarial
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1131
  month: October-November
  page_first: '1056'
  page_last: '1063'
  pages: "1056\u20131063"
  paper_id: '131'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1131.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1131.jpg
  title: Adversarial Domain Adaptation for Duplicate Question Detection
  title_html: Adversarial Domain Adaptation for Duplicate Question Detection
  url: https://www.aclweb.org/anthology/D18-1131
  year: '2018'
D18-1132:
  abstract: 'Sequence-to-sequence (SEQ2SEQ) models have been successfully applied
    to automatic math word problem solving. Despite its simplicity, a drawback still
    remains: a math word problem can be correctly solved by more than one equations.
    This non-deterministic transduction harms the performance of maximum likelihood
    estimation. In this paper, by considering the uniqueness of expression tree, we
    propose an equation normalization method to normalize the duplicated equations.
    Moreover, we analyze the performance of three popular SEQ2SEQ models on the math
    word problem solving. We find that each model has its own specialty in solving
    problems, consequently an ensemble model is then proposed to combine their advantages.
    Experiments on dataset Math23K show that the ensemble model with equation normalization
    significantly outperforms the previous state-of-the-art methods.'
  address: Brussels, Belgium
  author:
  - first: Lei
    full: Lei Wang
    id: lei-wang
    last: Wang
  - first: Yan
    full: Yan Wang
    id: yan-wang
    last: Wang
  - first: Deng
    full: Deng Cai
    id: deng-cai
    last: Cai
  - first: Dongxiang
    full: Dongxiang Zhang
    id: dongxiang-zhang
    last: Zhang
  - first: Xiaojiang
    full: Xiaojiang Liu
    id: xiaojiang-liu
    last: Liu
  author_string: Lei Wang, Yan Wang, Deng Cai, Dongxiang Zhang, Xiaojiang Liu
  bibkey: wang-etal-2018-translating
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1132
  month: October-November
  page_first: '1064'
  page_last: '1069'
  pages: "1064\u20131069"
  paper_id: '132'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1132.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1132.jpg
  title: Translating a Math Word Problem to a Expression Tree
  title_html: Translating a Math Word Problem to a Expression Tree
  url: https://www.aclweb.org/anthology/D18-1132
  year: '2018'
D18-1133:
  abstract: State-of-the-art networks that model relations between two pieces of text
    often use complex architectures and attention. In this paper, instead of focusing
    on architecture engineering, we take advantage of small amounts of labelled data
    that model semantic phenomena in text to encode matching features directly in
    the word representations. This greatly boosts the accuracy of our reference network,
    while keeping the model simple and fast to train. Our approach also beats a tree
    kernel model that uses similar input encodings, and neural models which use advanced
    attention and compare-aggregate mechanisms.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1133.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1133.Attachment.zip
  author:
  - first: Massimo
    full: Massimo Nicosia
    id: massimo-nicosia
    last: Nicosia
  - first: Alessandro
    full: Alessandro Moschitti
    id: alessandro-moschitti
    last: Moschitti
  author_string: Massimo Nicosia, Alessandro Moschitti
  bibkey: nicosia-moschitti-2018-semantic
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1133
  month: October-November
  page_first: '1070'
  page_last: '1076'
  pages: "1070\u20131076"
  paper_id: '133'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1133.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1133.jpg
  title: Semantic Linking in Convolutional Neural Networks for Answer Sentence Selection
  title_html: Semantic Linking in Convolutional Neural Networks for Answer Sentence
    Selection
  url: https://www.aclweb.org/anthology/D18-1133
  year: '2018'
D18-1134:
  abstract: Previous work on question-answering systems mainly focuses on answering
    individual questions, assuming they are independent and devoid of context. Instead,
    we investigate sequential question answering, asking multiple related questions.
    We present QBLink, a new dataset of fully human-authored questions. We extend
    existing strong question answering frameworks to include previous questions to
    improve the overall question-answering accuracy in open-domain question answering.
    The dataset is publicly available at http://sequential.qanta.org.
  address: Brussels, Belgium
  author:
  - first: Ahmed
    full: Ahmed Elgohary
    id: ahmed-elgohary
    last: Elgohary
  - first: Chen
    full: Chen Zhao
    id: chen-zhao
    last: Zhao
  - first: Jordan
    full: Jordan Boyd-Graber
    id: jordan-boyd-graber
    last: Boyd-Graber
  author_string: Ahmed Elgohary, Chen Zhao, Jordan Boyd-Graber
  bibkey: elgohary-etal-2018-dataset
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1134
  month: October-November
  page_first: '1077'
  page_last: '1083'
  pages: "1077\u20131083"
  paper_id: '134'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1134.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1134.jpg
  title: A dataset and baselines for sequential open-domain question answering
  title_html: A dataset and baselines for sequential open-domain question answering
  url: https://www.aclweb.org/anthology/D18-1134
  year: '2018'
D18-1135:
  abstract: Recently, string kernels have obtained state-of-the-art results in various
    text classification tasks such as Arabic dialect identification or native language
    identification. In this paper, we apply two simple yet effective transductive
    learning approaches to further improve the results of string kernels. The first
    approach is based on interpreting the pairwise string kernel similarities between
    samples in the training set and samples in the test set as features. Our second
    approach is a simple self-training method based on two learning iterations. In
    the first iteration, a classifier is trained on the training set and tested on
    the test set, as usual. In the second iteration, a number of test samples (to
    which the classifier associated higher confidence scores) are added to the training
    set for another round of training. However, the ground-truth labels of the added
    test samples are not necessary. Instead, we use the labels predicted by the classifier
    in the first training iteration. By adapting string kernels to the test set, we
    report significantly better accuracy rates in English polarity classification
    and Arabic dialect identification.
  address: Brussels, Belgium
  author:
  - first: Radu Tudor
    full: Radu Tudor Ionescu
    id: radu-tudor-ionescu
    last: Ionescu
  - first: Andrei M.
    full: Andrei M. Butnaru
    id: andrei-butnaru
    last: Butnaru
  author_string: Radu Tudor Ionescu, Andrei M. Butnaru
  bibkey: ionescu-butnaru-2018-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1135
  month: October-November
  page_first: '1084'
  page_last: '1090'
  pages: "1084\u20131090"
  paper_id: '135'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1135.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1135.jpg
  title: Improving the results of string kernels in sentiment analysis and Arabic
    dialect identification by adapting them to your test set
  title_html: Improving the results of string kernels in sentiment analysis and <span
    class="acl-fixed-case">A</span>rabic dialect identification by adapting them to
    your test set
  url: https://www.aclweb.org/anthology/D18-1135
  year: '2018'
D18-1136:
  abstract: We introduce a novel parameterized convolutional neural network for aspect
    level sentiment classification. Using parameterized filters and parameterized
    gates, we incorporate aspect information into convolutional neural networks (CNN).
    Experiments demonstrate that our parameterized filters and parameterized gates
    effectively capture the aspect-specific features, and our CNN-based models achieve
    excellent results on SemEval 2014 datasets.
  address: Brussels, Belgium
  author:
  - first: Binxuan
    full: Binxuan Huang
    id: binxuan-huang
    last: Huang
  - first: Kathleen
    full: Kathleen Carley
    id: kathleen-m-carley
    last: Carley
  author_string: Binxuan Huang, Kathleen Carley
  bibkey: huang-carley-2018-parameterized
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1136
  month: October-November
  page_first: '1091'
  page_last: '1096'
  pages: "1091\u20131096"
  paper_id: '136'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1136.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1136.jpg
  title: Parameterized Convolutional Neural Networks for Aspect Level Sentiment Classification
  title_html: Parameterized Convolutional Neural Networks for Aspect Level Sentiment
    Classification
  url: https://www.aclweb.org/anthology/D18-1136
  year: '2018'
D18-1137:
  abstract: In this paper, we target at improving the performance of multi-label emotion
    classification with the help of sentiment classification. Specifically, we propose
    a new transfer learning architecture to divide the sentence representation into
    two different feature spaces, which are expected to respectively capture the general
    sentiment words and the other important emotion-specific words via a dual attention
    mechanism. Experimental results on two benchmark datasets demonstrate the effectiveness
    of our proposed method.
  address: Brussels, Belgium
  author:
  - first: Jianfei
    full: Jianfei Yu
    id: jianfei-yu
    last: Yu
  - first: "Lu\xEDs"
    full: "Lu\xEDs Marujo"
    id: luis-marujo
    last: Marujo
  - first: Jing
    full: Jing Jiang
    id: jing-jiang
    last: Jiang
  - first: Pradeep
    full: Pradeep Karuturi
    id: pradeep-karuturi
    last: Karuturi
  - first: William
    full: William Brendel
    id: william-brendel
    last: Brendel
  author_string: "Jianfei Yu, Lu\xEDs Marujo, Jing Jiang, Pradeep Karuturi, William\
    \ Brendel"
  bibkey: yu-etal-2018-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1137
  month: October-November
  page_first: '1097'
  page_last: '1102'
  pages: "1097\u20131102"
  paper_id: '137'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1137.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1137.jpg
  title: Improving Multi-label Emotion Classification via Sentiment Classification
    with Dual Attention Transfer Network
  title_html: Improving Multi-label Emotion Classification via Sentiment Classification
    with Dual Attention Transfer Network
  url: https://www.aclweb.org/anthology/D18-1137
  year: '2018'
D18-1138:
  abstract: "The task of sentiment modification requires reversing the sentiment of\
    \ the input and preserving the sentiment-independent content. However, aligned\
    \ sentences with the same content but different sentiments are usually unavailable.\
    \ Due to the lack of such parallel data, it is hard to extract sentiment independent\
    \ content and reverse the sentiment in an unsupervised way. Previous work usually\
    \ can not reconcile sentiment transformation and content preservation. In this\
    \ paper, motivated by the fact the non-emotional context (e.g., \u201Cstaff\u201D\
    ) provides strong cues for the occurrence of emotional words (e.g., \u201Cfriendly\u201D\
    ), we propose a novel method that automatically extracts appropriate sentiment\
    \ information from learned sentiment memories according to the specific context.\
    \ Experiments show that our method substantially improves the content preservation\
    \ degree and achieves the state-of-the-art performance."
  address: Brussels, Belgium
  author:
  - first: Yi
    full: Yi Zhang
    id: yi-zhang
    last: Zhang
  - first: Jingjing
    full: Jingjing Xu
    id: jingjing-xu
    last: Xu
  - first: Pengcheng
    full: Pengcheng Yang
    id: pengcheng-yang
    last: Yang
  - first: Xu
    full: Xu Sun
    id: xu-sun
    last: Sun
  author_string: Yi Zhang, Jingjing Xu, Pengcheng Yang, Xu Sun
  bibkey: zhang-etal-2018-learning-sentiment
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1138
  month: October-November
  page_first: '1103'
  page_last: '1108'
  pages: "1103\u20131108"
  paper_id: '138'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1138.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1138.jpg
  title: Learning Sentiment Memories for Sentiment Modification without Parallel Data
  title_html: Learning Sentiment Memories for Sentiment Modification without Parallel
    Data
  url: https://www.aclweb.org/anthology/D18-1138
  year: '2018'
D18-1139:
  abstract: In this work, we propose a new model for aspect-based sentiment analysis.
    In contrast to previous approaches, we jointly model the detection of aspects
    and the classification of their polarity in an end-to-end trainable neural network.
    We conduct experiments with different neural architectures and word representations
    on the recent GermEval 2017 dataset. We were able to show considerable performance
    gains by using the joint modeling approach in all settings compared to pipeline
    approaches. The combination of a convolutional neural network and fasttext embeddings
    outperformed the best submission of the shared task in 2017, establishing a new
    state of the art.
  address: Brussels, Belgium
  author:
  - first: Martin
    full: Martin Schmitt
    id: martin-schmitt
    last: Schmitt
  - first: Simon
    full: Simon Steinheber
    id: simon-steinheber
    last: Steinheber
  - first: Konrad
    full: Konrad Schreiber
    id: konrad-schreiber
    last: Schreiber
  - first: Benjamin
    full: Benjamin Roth
    id: benjamin-roth
    last: Roth
  author_string: Martin Schmitt, Simon Steinheber, Konrad Schreiber, Benjamin Roth
  bibkey: schmitt-etal-2018-joint
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1139
  month: October-November
  page_first: '1109'
  page_last: '1114'
  pages: "1109\u20131114"
  paper_id: '139'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1139.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1139.jpg
  title: Joint Aspect and Polarity Classification for Aspect-based Sentiment Analysis
    with End-to-End Neural Networks
  title_html: Joint Aspect and Polarity Classification for Aspect-based Sentiment
    Analysis with End-to-End Neural Networks
  url: https://www.aclweb.org/anthology/D18-1139
  year: '2018'
D18-1140:
  abstract: "We explore two methods for representing authors in the context of textual\
    \ sarcasm detection: a Bayesian approach that directly represents authors\u2019\
    \ propensities to be sarcastic, and a dense embedding approach that can learn\
    \ interactions between the author and the text. Using the SARC dataset of Reddit\
    \ comments, we show that augmenting a bidirectional RNN with these representations\
    \ improves performance; the Bayesian approach suffices in homogeneous contexts,\
    \ whereas the added power of the dense embeddings proves valuable in more diverse\
    \ ones."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1140.Attachment.tgz
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1140.Attachment.tgz
  author:
  - first: Y. Alex
    full: Y. Alex Kolchinski
    id: y-alex-kolchinski
    last: Kolchinski
  - first: Christopher
    full: Christopher Potts
    id: christopher-potts
    last: Potts
  author_string: Y. Alex Kolchinski, Christopher Potts
  bibkey: kolchinski-potts-2018-representing
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1140
  month: October-November
  page_first: '1115'
  page_last: '1121'
  pages: "1115\u20131121"
  paper_id: '140'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1140.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1140.jpg
  title: Representing Social Media Users for Sarcasm Detection
  title_html: Representing Social Media Users for Sarcasm Detection
  url: https://www.aclweb.org/anthology/D18-1140
  year: '2018'
D18-1141:
  abstract: 'We carry out a syntactic analysis of two state-of-the-art sentiment analyzers,
    Google Cloud Natural Language and Stanford CoreNLP, to assess their classification
    accuracy on sentences with negative polarity items. We were motivated by the absence
    of studies investigating sentiment analyzer performance on sentences with polarity
    items, a common construct in human language. Our analysis focuses on two sentential
    structures: downward entailment and non-monotone quantifiers; and demonstrates
    weaknesses of Google Natural Language and CoreNLP in capturing polarity item information.
    We describe the particular syntactic phenomenon that these analyzers fail to understand
    that any ideal sentiment analyzer must. We also provide a set of 150 test sentences
    that any ideal sentiment analyzer must be able to understand.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1141.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1141.Attachment.zip
  author:
  - first: Rohil
    full: Rohil Verma
    id: rohil-verma
    last: Verma
  - first: Samuel
    full: Samuel Kim
    id: samuel-kim
    last: Kim
  - first: David
    full: David Walter
    id: david-walter
    last: Walter
  author_string: Rohil Verma, Samuel Kim, David Walter
  bibkey: verma-etal-2018-syntactical
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1141
  month: October-November
  page_first: '1122'
  page_last: '1127'
  pages: "1122\u20131127"
  paper_id: '141'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1141.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1141.jpg
  title: Syntactical Analysis of the Weaknesses of Sentiment Analyzers
  title_html: Syntactical Analysis of the Weaknesses of Sentiment Analyzers
  url: https://www.aclweb.org/anthology/D18-1141
  year: '2018'
D18-1142:
  abstract: "Are brand names such as Nike female or male? Previous research suggests\
    \ that the sound of a person\u2019s first name is associated with the person\u2019\
    s gender, but no research has tried to use this knowledge to assess the gender\
    \ of brand names. We present a simple computational approach that uses sound symbolism\
    \ to address this open issue. Consistent with previous research, a model trained\
    \ on various linguistic features of name endings predicts human gender with high\
    \ accuracy. Applying this model to a data set of over a thousand commercially-traded\
    \ brands in 17 product categories, our results reveal an overall bias toward male\
    \ names, cutting across both male-oriented product categories as well as female-oriented\
    \ categories. In addition, we find variation within categories, suggesting that\
    \ firms might be seeking to imbue their brands with differentiating characteristics\
    \ as part of their competitive strategy."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1142.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1142.Attachment.zip
  author:
  - first: Sridhar
    full: Sridhar Moorthy
    id: sridhar-moorthy
    last: Moorthy
  - first: Ruth
    full: Ruth Pogacar
    id: ruth-pogacar
    last: Pogacar
  - first: Samin
    full: Samin Khan
    id: samin-khan
    last: Khan
  - first: Yang
    full: Yang Xu
    id: yang-xu
    last: Xu
  author_string: Sridhar Moorthy, Ruth Pogacar, Samin Khan, Yang Xu
  bibkey: moorthy-etal-2018-nike
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1142
  month: October-November
  page_first: '1128'
  page_last: '1132'
  pages: "1128\u20131132"
  paper_id: '142'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1142.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1142.jpg
  title: Is Nike female? Exploring the role of sound symbolism in predicting brand
    name gender
  title_html: Is <span class="acl-fixed-case">N</span>ike female? Exploring the role
    of sound symbolism in predicting brand name gender
  url: https://www.aclweb.org/anthology/D18-1142
  year: '2018'
D18-1143:
  abstract: Fact-checking of textual sources needs to effectively extract relevant
    information from large knowledge bases. In this paper, we extend an existing pipeline
    approach to better tackle this problem. We propose a neural ranker using a decomposable
    attention model that dynamically selects sentences to achieve promising improvement
    in evidence retrieval F1 by 38.80%, with (x65) speedup compared to a TF-IDF method.
    Moreover, we incorporate lexical tagging methods into our pipeline framework to
    simplify the tasks and render the model more generalizable. As a result, our framework
    achieves promising performance on a large-scale fact extraction and verification
    dataset with speedup.
  address: Brussels, Belgium
  author:
  - first: Nayeon
    full: Nayeon Lee
    id: nayeon-lee
    last: Lee
  - first: Chien-Sheng
    full: Chien-Sheng Wu
    id: chien-sheng-wu
    last: Wu
  - first: Pascale
    full: Pascale Fung
    id: pascale-fung
    last: Fung
  author_string: Nayeon Lee, Chien-Sheng Wu, Pascale Fung
  bibkey: lee-etal-2018-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1143
  month: October-November
  page_first: '1133'
  page_last: '1138'
  pages: "1133\u20131138"
  paper_id: '143'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1143.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1143.jpg
  title: Improving Large-Scale Fact-Checking using Decomposable Attention Models and
    Lexical Tagging
  title_html: Improving Large-Scale Fact-Checking using Decomposable Attention Models
    and Lexical Tagging
  url: https://www.aclweb.org/anthology/D18-1143
  year: '2018'
D18-1144:
  abstract: We leverage a popularity measure in social media as a distant label for
    extractive summarization of online conversations. In social media, users can vote,
    share, or bookmark a post they prefer. The number of these actions is regarded
    as a measure of popularity. However, popularity is not determined solely by content
    of a post, e.g., a text or an image it contains, but is highly based on its contexts,
    e.g., timing, and authority. We propose Disjunctive model that computes the contribution
    of content and context separately. For evaluation, we build a dataset where the
    informativeness of comments is annotated. We evaluate the results with ranking
    metrics, and show that our model outperforms the baseline models which directly
    use popularity as a measure of informativeness.
  address: Brussels, Belgium
  author:
  - first: Ryuji
    full: Ryuji Kano
    id: ryuji-kano
    last: Kano
  - first: Yasuhide
    full: Yasuhide Miura
    id: yasuhide-miura
    last: Miura
  - first: Motoki
    full: Motoki Taniguchi
    id: motoki-taniguchi
    last: Taniguchi
  - first: Yan-Ying
    full: Yan-Ying Chen
    id: yan-ying-chen
    last: Chen
  - first: Francine
    full: Francine Chen
    id: francine-chen
    last: Chen
  - first: Tomoko
    full: Tomoko Ohkuma
    id: tomoko-ohkuma
    last: Ohkuma
  author_string: Ryuji Kano, Yasuhide Miura, Motoki Taniguchi, Yan-Ying Chen, Francine
    Chen, Tomoko Ohkuma
  bibkey: kano-etal-2018-harnessing
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1144
  month: October-November
  page_first: '1139'
  page_last: '1145'
  pages: "1139\u20131145"
  paper_id: '144'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1144.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1144.jpg
  title: Harnessing Popularity in Social Media for Extractive Summarization of Online
    Conversations
  title_html: Harnessing Popularity in Social Media for Extractive Summarization of
    Online Conversations
  url: https://www.aclweb.org/anthology/D18-1144
  year: '2018'
D18-1145:
  abstract: "Individuals express their locus of control, or \u201Ccontrol\u201D, in\
    \ their language when they identify whether or not they are in control of their\
    \ circumstances. Although control is a core concept underlying rhetorical style,\
    \ it is not clear whether control is expressed by how or by what authors write.\
    \ We explore the roles of syntax and semantics in expressing users\u2019 sense\
    \ of control \u2013i.e. being \u201Ccontrolled by\u201D or \u201Cin control of\u201D\
    \ their circumstances\u2013 in a corpus of annotated Facebook posts. We present\
    \ rich insights into these linguistic aspects and find that while the language\
    \ signaling control is easy to identify, it is more challenging to label it is\
    \ internally or externally controlled, with lexical features outperforming syntactic\
    \ features at the task. Our findings could have important implications for studying\
    \ self-expression in social media."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1145.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1145.Attachment.pdf
  author:
  - first: Masoud
    full: Masoud Rouhizadeh
    id: masoud-rouhizadeh
    last: Rouhizadeh
  - first: Kokil
    full: Kokil Jaidka
    id: kokil-jaidka
    last: Jaidka
  - first: Laura
    full: Laura Smith
    id: laura-smith
    last: Smith
  - first: H. Andrew
    full: H. Andrew Schwartz
    id: h-andrew-schwartz
    last: Schwartz
  - first: Anneke
    full: Anneke Buffone
    id: anneke-buffone
    last: Buffone
  - first: Lyle
    full: Lyle Ungar
    id: lyle-ungar
    last: Ungar
  author_string: Masoud Rouhizadeh, Kokil Jaidka, Laura Smith, H. Andrew Schwartz,
    Anneke Buffone, Lyle Ungar
  bibkey: rouhizadeh-etal-2018-identifying
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1145
  month: October-November
  page_first: '1146'
  page_last: '1152'
  pages: "1146\u20131152"
  paper_id: '145'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1145.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1145.jpg
  title: Identifying Locus of Control in Social Media Language
  title_html: Identifying Locus of Control in Social Media Language
  url: https://www.aclweb.org/anthology/D18-1145
  year: '2018'
D18-1146:
  abstract: "To what extent could the sommelier profession, or wine stewardship, be\
    \ displaced by machine leaning algorithms? There are at least three essential\
    \ skills that make a qualified sommelier: wine theory, blind tasting, and beverage\
    \ service, as exemplified in the rigorous certification processes of certified\
    \ sommeliers and above (advanced and master) with the most authoritative body\
    \ in the industry, the Court of Master Sommelier (hereafter CMS). We propose and\
    \ train corresponding machine learning models that match these skills, and compare\
    \ algorithmic results with real data collected from a large group of wine professionals.\
    \ We find that our machine learning models outperform human sommeliers on most\
    \ tasks \u2014 most notably in the section of blind tasting, where hierarchically\
    \ supervised Latent Dirichlet Allocation outperforms sommeliers\u2019 judgment\
    \ calls by over 6% in terms of F1-score; and in the section of beverage service,\
    \ especially wine and food pairing, a modified Siamese neural network based on\
    \ BiLSTM achieves better results than sommeliers by 2%. This demonstrates, contrary\
    \ to popular opinion in the industry, that the sommelier profession is at least\
    \ to some extent automatable, barring economic (Kleinberg et al., 2017) and psychological\
    \ (Dietvorst et al., 2015) complications."
  address: Brussels, Belgium
  author:
  - first: Shengli
    full: Shengli Hu
    id: shengli-hu
    last: Hu
  author_string: Shengli Hu
  bibkey: hu-2018-somm
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1146
  month: October-November
  page_first: '1153'
  page_last: '1159'
  pages: "1153\u20131159"
  paper_id: '146'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1146.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1146.jpg
  title: 'Somm: Into the Model'
  title_html: '<span class="acl-fixed-case">S</span>omm: Into the Model'
  url: https://www.aclweb.org/anthology/D18-1146
  year: '2018'
D18-1147:
  abstract: "Detecting fine-grained emotions in online health communities provides\
    \ insightful information about patients\u2019 emotional states. However, current\
    \ computational approaches to emotion detection from health-related posts focus\
    \ only on identifying messages that contain emotions, with no emphasis on the\
    \ emotion type, using a set of handcrafted features. In this paper, we take a\
    \ step further and propose to detect fine-grained emotion types from health-related\
    \ posts and show how high-level and abstract features derived from deep neural\
    \ networks combined with lexicon-based features can be employed to detect emotions."
  address: Brussels, Belgium
  author:
  - first: Hamed
    full: Hamed Khanpour
    id: hamed-khanpour
    last: Khanpour
  - first: Cornelia
    full: Cornelia Caragea
    id: cornelia-caragea
    last: Caragea
  author_string: Hamed Khanpour, Cornelia Caragea
  bibkey: khanpour-caragea-2018-fine
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1147
  month: October-November
  page_first: '1160'
  page_last: '1166'
  pages: "1160\u20131166"
  paper_id: '147'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1147.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1147.jpg
  title: Fine-Grained Emotion Detection in Health-Related Online Posts
  title_html: Fine-Grained Emotion Detection in Health-Related Online Posts
  url: https://www.aclweb.org/anthology/D18-1147
  year: '2018'
D18-1148:
  abstract: "Nowcasting based on social media text promises to provide unobtrusive\
    \ and near real-time predictions of community-level outcomes. These outcomes are\
    \ typically regarding people, but the data is often aggregated without regard\
    \ to users in the Twitter populations of each community. This paper describes\
    \ a simple yet effective method for building community-level models using Twitter\
    \ language aggregated by user. Results on four different U.S. county-level tasks,\
    \ spanning demographic, health, and psychological outcomes show large and consistent\
    \ improvements in prediction accuracies (e.g. from Pearson r=.73 to .82 for median\
    \ income prediction or r=.37 to .47 for life satisfaction prediction) over the\
    \ standard approach of aggregating all tweets. We make our aggregated and anonymized\
    \ community-level data, derived from 37 billion tweets \u2013 over 1 billion of\
    \ which were mapped to counties, available for research."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1148.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1148.Attachment.zip
  author:
  - first: Salvatore
    full: Salvatore Giorgi
    id: salvatore-giorgi
    last: Giorgi
  - first: Daniel
    full: "Daniel Preo\u0163iuc-Pietro"
    id: daniel-preotiuc-pietro
    last: "Preo\u0163iuc-Pietro"
  - first: Anneke
    full: Anneke Buffone
    id: anneke-buffone
    last: Buffone
  - first: Daniel
    full: Daniel Rieman
    id: daniel-rieman
    last: Rieman
  - first: Lyle
    full: Lyle Ungar
    id: lyle-ungar
    last: Ungar
  - first: H. Andrew
    full: H. Andrew Schwartz
    id: h-andrew-schwartz
    last: Schwartz
  author_string: "Salvatore Giorgi, Daniel Preo\u0163iuc-Pietro, Anneke Buffone, Daniel\
    \ Rieman, Lyle Ungar, H. Andrew Schwartz"
  bibkey: giorgi-etal-2018-remarkable
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1148
  month: October-November
  page_first: '1167'
  page_last: '1172'
  pages: "1167\u20131172"
  paper_id: '148'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1148.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1148.jpg
  title: The Remarkable Benefit of User-Level Aggregation for Lexical-based Population-Level
    Predictions
  title_html: The Remarkable Benefit of User-Level Aggregation for Lexical-based Population-Level
    Predictions
  url: https://www.aclweb.org/anthology/D18-1148
  year: '2018'
D18-1149:
  abstract: We propose a conditional non-autoregressive neural sequence model based
    on iterative refinement. The proposed model is designed based on the principles
    of latent variable models and denoising autoencoders, and is generally applicable
    to any sequence generation task. We extensively evaluate the proposed model on
    machine translation (En-De and En-Ro) and image caption generation, and observe
    that it significantly speeds up decoding while maintaining the generation quality
    comparable to the autoregressive counterpart.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1149.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1149.Attachment.pdf
  - filename: https://vimeo.com/305207923
    type: video
    url: https://vimeo.com/305207923
  author:
  - first: Jason
    full: Jason Lee
    id: jason-lee
    last: Lee
  - first: Elman
    full: Elman Mansimov
    id: elman-mansimov
    last: Mansimov
  - first: Kyunghyun
    full: Kyunghyun Cho
    id: kyunghyun-cho
    last: Cho
  author_string: Jason Lee, Elman Mansimov, Kyunghyun Cho
  bibkey: lee-etal-2018-deterministic
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1149
  month: October-November
  page_first: '1173'
  page_last: '1182'
  pages: "1173\u20131182"
  paper_id: '149'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1149.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1149.jpg
  title: Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement
  title_html: Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative
    Refinement
  url: https://www.aclweb.org/anthology/D18-1149
  year: '2018'
D18-1150:
  abstract: "We propose a large margin criterion for training neural language models.\
    \ Conventionally, neural language models are trained by minimizing perplexity\
    \ (PPL) on grammatical sentences. However, we demonstrate that PPL may not be\
    \ the best metric to optimize in some tasks, and further propose a large margin\
    \ formulation. The proposed method aims to enlarge the margin between the \u201C\
    good\u201D and \u201Cbad\u201D sentences in a task-specific sense. It is trained\
    \ end-to-end and can be widely applied to tasks that involve re-scoring of generated\
    \ text. Compared with minimum-PPL training, our method gains up to 1.1 WER reduction\
    \ for speech recognition and 1.0 BLEU increase for machine translation."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305208380
    type: video
    url: https://vimeo.com/305208380
  author:
  - first: Jiaji
    full: Jiaji Huang
    id: jiaji-huang
    last: Huang
  - first: Yi
    full: Yi Li
    id: yi-li
    last: Li
  - first: Wei
    full: Wei Ping
    id: wei-ping
    last: Ping
  - first: Liang
    full: Liang Huang
    id: liang-huang
    last: Huang
  author_string: Jiaji Huang, Yi Li, Wei Ping, Liang Huang
  bibkey: huang-etal-2018-large
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1150
  month: October-November
  page_first: '1183'
  page_last: '1191'
  pages: "1183\u20131191"
  paper_id: '150'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1150.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1150.jpg
  title: Large Margin Neural Language Model
  title_html: Large Margin Neural Language Model
  url: https://www.aclweb.org/anthology/D18-1150
  year: '2018'
D18-1151:
  abstract: "We present a data set for evaluating the grammaticality of the predictions\
    \ of a language model. We automatically construct a large number of minimally\
    \ different pairs of English sentences, each consisting of a grammatical and an\
    \ ungrammatical sentence. The sentence pairs represent different variations of\
    \ structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and\
    \ negative polarity items. We expect a language model to assign a higher probability\
    \ to the grammatical sentence than the ungrammatical one. In an experiment using\
    \ this data set, an LSTM language model performed poorly on many of the constructions.\
    \ Multi-task training with a syntactic objective (CCG supertagging) improved the\
    \ LSTM\u2019s accuracy, but a large gap remained between its performance and the\
    \ accuracy of human participants recruited online. This suggests that there is\
    \ considerable room for improvement over LSTMs in capturing syntax in a language\
    \ model."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1151.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1151.Attachment.pdf
  - filename: https://vimeo.com/305208737
    type: video
    url: https://vimeo.com/305208737
  author:
  - first: Rebecca
    full: Rebecca Marvin
    id: rebecca-marvin
    last: Marvin
  - first: Tal
    full: Tal Linzen
    id: tal-linzen
    last: Linzen
  author_string: Rebecca Marvin, Tal Linzen
  bibkey: marvin-linzen-2018-targeted
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1151
  month: October-November
  page_first: '1192'
  page_last: '1202'
  pages: "1192\u20131202"
  paper_id: '151'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1151.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1151.jpg
  title: Targeted Syntactic Evaluation of Language Models
  title_html: Targeted Syntactic Evaluation of Language Models
  url: https://www.aclweb.org/anthology/D18-1151
  year: '2018'
D18-1152:
  abstract: Despite the tremendous empirical success of neural models in natural language
    processing, many of them lack the strong intuitions that accompany classical machine
    learning approaches. Recently, connections have been shown between convolutional
    neural networks (CNNs) and weighted finite state automata (WFSAs), leading to
    new interpretations and insights. In this work, we show that some recurrent neural
    networks also share this connection to WFSAs. We characterize this connection
    formally, defining rational recurrences to be recurrent hidden state update functions
    that can be written as the Forward calculation of a finite set of WFSAs. We show
    that several recent neural models use rational recurrences. Our analysis provides
    a fresh view of these models and facilitates devising new neural architectures
    that draw inspiration from WFSAs. We present one such model, which performs better
    than two recent baselines on language modeling and text classification. Our results
    demonstrate that transferring intuitions from classical models like WFSAs can
    be an effective approach to designing and understanding neural models.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1152.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1152.Attachment.pdf
  - filename: https://vimeo.com/305209046
    type: video
    url: https://vimeo.com/305209046
  author:
  - first: Hao
    full: Hao Peng
    id: hao-peng
    last: Peng
  - first: Roy
    full: Roy Schwartz
    id: roy-schwartz
    last: Schwartz
  - first: Sam
    full: Sam Thomson
    id: sam-thomson
    last: Thomson
  - first: Noah A.
    full: Noah A. Smith
    id: noah-a-smith
    last: Smith
  author_string: Hao Peng, Roy Schwartz, Sam Thomson, Noah A. Smith
  bibkey: peng-etal-2018-rational
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1152
  month: October-November
  page_first: '1203'
  page_last: '1214'
  pages: "1203\u20131214"
  paper_id: '152'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1152.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1152.jpg
  title: Rational Recurrences
  title_html: Rational Recurrences
  url: https://www.aclweb.org/anthology/D18-1152
  year: '2018'
D18-1153:
  abstract: Many efforts have been made to facilitate natural language processing
    tasks with pre-trained language models (LMs), and brought significant improvements
    to various applications. To fully leverage the nearly unlimited corpora and capture
    linguistic information of multifarious levels, large-size LMs are required; but
    for a specific task, only parts of these information are useful. Such large-sized
    LMs, even in the inference stage, may cause heavy computation workloads, making
    them too time-consuming for large-scale applications. Here we propose to compress
    bulky LMs while preserving useful information with regard to a specific task.
    As different layers of the model keep different information, we develop a layer
    selection method for model pruning using sparsity-inducing regularization. By
    introducing the dense connectivity, we can detach any layer without affecting
    others, and stretch shallow and wide LMs to be deep and narrow. In model training,
    LMs are learned with layer-wise dropouts for better robustness. Experiments on
    two benchmark datasets demonstrate the effectiveness of our method.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305209444
    type: video
    url: https://vimeo.com/305209444
  author:
  - first: Liyuan
    full: Liyuan Liu
    id: liyuan-liu
    last: Liu
  - first: Xiang
    full: Xiang Ren
    id: xiang-ren
    last: Ren
  - first: Jingbo
    full: Jingbo Shang
    id: jingbo-shang
    last: Shang
  - first: Xiaotao
    full: Xiaotao Gu
    id: xiaotao-gu
    last: Gu
  - first: Jian
    full: Jian Peng
    id: jian-peng
    last: Peng
  - first: Jiawei
    full: Jiawei Han
    id: jiawei-han
    last: Han
  author_string: Liyuan Liu, Xiang Ren, Jingbo Shang, Xiaotao Gu, Jian Peng, Jiawei
    Han
  bibkey: liu-etal-2018-efficient-contextualized
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1153
  month: October-November
  page_first: '1215'
  page_last: '1225'
  pages: "1215\u20131225"
  paper_id: '153'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1153.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1153.jpg
  title: 'Efficient Contextualized Representation: Language Model Pruning for Sequence
    Labeling'
  title_html: 'Efficient Contextualized Representation: Language Model Pruning for
    Sequence Labeling'
  url: https://www.aclweb.org/anthology/D18-1153
  year: '2018'
D18-1154:
  abstract: Identifying the salience (i.e. importance) of discourse units is an important
    task in language understanding. While events play important roles in text documents,
    little research exists on analyzing their saliency status. This paper empirically
    studies Event Salience and proposes two salience detection models based on discourse
    relations. The first is a feature based salience model that incorporates cohesion
    among discourse units. The second is a neural model that captures more complex
    interactions between discourse units. In our new large-scale event salience corpus,
    both methods significantly outperform the strong frequency baseline, while our
    neural model further improves the feature based one by a large margin. Our analyses
    demonstrate that our neural model captures interesting connections between salience
    and discourse unit relations (e.g., scripts and frame structures).
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305198570
    type: video
    url: https://vimeo.com/305198570
  author:
  - first: Zhengzhong
    full: Zhengzhong Liu
    id: zhengzhong-liu
    last: Liu
  - first: Chenyan
    full: Chenyan Xiong
    id: chenyan-xiong
    last: Xiong
  - first: Teruko
    full: Teruko Mitamura
    id: teruko-mitamura
    last: Mitamura
  - first: Eduard
    full: Eduard Hovy
    id: eduard-hovy
    last: Hovy
  author_string: Zhengzhong Liu, Chenyan Xiong, Teruko Mitamura, Eduard Hovy
  bibkey: liu-etal-2018-automatic-event
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1154
  month: October-November
  page_first: '1226'
  page_last: '1236'
  pages: "1226\u20131236"
  paper_id: '154'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1154.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1154.jpg
  title: Automatic Event Salience Identification
  title_html: Automatic Event Salience Identification
  url: https://www.aclweb.org/anthology/D18-1154
  year: '2018'
D18-1155:
  abstract: 'The current leading paradigm for temporal information extraction from
    text consists of three phases: (1) recognition of events and temporal expressions,
    (2) recognition of temporal relations among them, and (3) time-line construction
    from the temporal relations. In contrast to the first two phases, the last phase,
    time-line construction, received little attention and is the focus of this work.
    In this paper, we propose a new method to construct a linear time-line from a
    set of (extracted) temporal relations. But more importantly, we propose a novel
    paradigm in which we directly predict start and end-points for events from the
    text, constituting a time-line without going through the intermediate step of
    prediction of temporal relations as in earlier work. Within this paradigm, we
    propose two models that predict in linear complexity, and a new training loss
    using TimeML-style annotations, yielding promising results.'
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305198795
    type: video
    url: https://vimeo.com/305198795
  author:
  - first: Artuur
    full: Artuur Leeuwenberg
    id: artuur-leeuwenberg
    last: Leeuwenberg
  - first: Marie-Francine
    full: Marie-Francine Moens
    id: marie-francine-moens
    last: Moens
  author_string: Artuur Leeuwenberg, Marie-Francine Moens
  bibkey: leeuwenberg-moens-2018-temporal
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1155
  month: October-November
  page_first: '1237'
  page_last: '1246'
  pages: "1237\u20131246"
  paper_id: '155'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1155.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1155.jpg
  title: Temporal Information Extraction by Predicting Relative Time-lines
  title_html: Temporal Information Extraction by Predicting Relative Time-lines
  url: https://www.aclweb.org/anthology/D18-1155
  year: '2018'
D18-1156:
  abstract: Event extraction is of practical utility in natural language processing.
    In the real world, it is a common phenomenon that multiple events existing in
    the same sentence, where extracting them are more difficult than extracting a
    single event. Previous works on modeling the associations between events by sequential
    modeling methods suffer a lot from the low efficiency in capturing very long-range
    dependencies. In this paper, we propose a novel Jointly Multiple Events Extraction
    (JMEE) framework to jointly extract multiple event triggers and arguments by introducing
    syntactic shortcut arcs to enhance information flow and attention-based graph
    convolution networks to model graph information. The experiment results demonstrate
    that our proposed framework achieves competitive results compared with state-of-the-art
    methods.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305198933
    type: video
    url: https://vimeo.com/305198933
  author:
  - first: Xiao
    full: Xiao Liu
    id: xiao-liu
    last: Liu
  - first: Zhunchen
    full: Zhunchen Luo
    id: zhunchen-luo
    last: Luo
  - first: Heyan
    full: Heyan Huang
    id: he-yan-huang
    last: Huang
  author_string: Xiao Liu, Zhunchen Luo, Heyan Huang
  bibkey: liu-etal-2018-jointly
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1156
  month: October-November
  page_first: '1247'
  page_last: '1256'
  pages: "1247\u20131256"
  paper_id: '156'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1156.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1156.jpg
  title: Jointly Multiple Events Extraction via Attention-based Graph Information
    Aggregation
  title_html: Jointly Multiple Events Extraction via Attention-based Graph Information
    Aggregation
  url: https://www.aclweb.org/anthology/D18-1156
  year: '2018'
D18-1157:
  abstract: "Distantly-supervised Relation Extraction (RE) methods train an extractor\
    \ by automatically aligning relation instances in a Knowledge Base (KB) with unstructured\
    \ text. In addition to relation instances, KBs often contain other relevant side\
    \ information, such as aliases of relations (e.g., founded and co-founded are\
    \ aliases for the relation founderOfCompany). RE models usually ignore such readily\
    \ available side information. In this paper, we propose RESIDE, a distantly-supervised\
    \ neural relation extraction method which utilizes additional side information\
    \ from KBs for improved relation extraction. It uses entity type and relation\
    \ alias information for imposing soft constraints while predicting relations.\
    \ RESIDE employs Graph Convolution Networks (GCN) to encode syntactic information\
    \ from text and improves performance even when limited side information is available.\
    \ Through extensive experiments on benchmark datasets, we demonstrate RESIDE\u2019\
    s effectiveness. We have made RESIDE\u2019s source code available to encourage\
    \ reproducible research."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1157.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1157.Attachment.pdf
  - filename: https://vimeo.com/305199302
    type: video
    url: https://vimeo.com/305199302
  author:
  - first: Shikhar
    full: Shikhar Vashishth
    id: shikhar-vashishth
    last: Vashishth
  - first: Rishabh
    full: Rishabh Joshi
    id: rishabh-joshi
    last: Joshi
  - first: Sai Suman
    full: Sai Suman Prayaga
    id: sai-suman-prayaga
    last: Prayaga
  - first: Chiranjib
    full: Chiranjib Bhattacharyya
    id: chiranjib-bhattacharyya
    last: Bhattacharyya
  - first: Partha
    full: Partha Talukdar
    id: partha-talukdar
    last: Talukdar
  author_string: Shikhar Vashishth, Rishabh Joshi, Sai Suman Prayaga, Chiranjib Bhattacharyya,
    Partha Talukdar
  bibkey: vashishth-etal-2018-reside
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1157
  month: October-November
  page_first: '1257'
  page_last: '1266'
  pages: "1257\u20131266"
  paper_id: '157'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1157.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1157.jpg
  title: 'RESIDE: Improving Distantly-Supervised Neural Relation Extraction using
    Side Information'
  title_html: '<span class="acl-fixed-case">RESIDE</span>: Improving Distantly-Supervised
    Neural Relation Extraction using Side Information'
  url: https://www.aclweb.org/anthology/D18-1157
  year: '2018'
D18-1158:
  abstract: Traditional approaches to the task of ACE event detection primarily regard
    multiple events in one sentence as independent ones and recognize them separately
    by using sentence-level information. However, events in one sentence are usually
    interdependent and sentence-level information is often insufficient to resolve
    ambiguities for some types of events. This paper proposes a novel framework dubbed
    as Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms
    (HBTNGMA) to solve the two problems simultaneously. Firstly, we propose a hierachical
    and bias tagging networks to detect multiple events in one sentence collectively.
    Then, we devise a gated multi-level attention to automatically extract and dynamically
    fuse the sentence-level and document-level information. The experimental results
    on the widely used ACE 2005 dataset show that our approach significantly outperforms
    other state-of-the-art methods.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305199664
    type: video
    url: https://vimeo.com/305199664
  author:
  - first: Yubo
    full: Yubo Chen
    id: yubo-chen
    last: Chen
  - first: Hang
    full: Hang Yang
    id: hang-yang
    last: Yang
  - first: Kang
    full: Kang Liu
    id: kang-liu
    last: Liu
  - first: Jun
    full: Jun Zhao
    id: jun-zhao
    last: Zhao
  - first: Yantao
    full: Yantao Jia
    id: yantao-jia
    last: Jia
  author_string: Yubo Chen, Hang Yang, Kang Liu, Jun Zhao, Yantao Jia
  bibkey: chen-etal-2018-collective
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1158
  month: October-November
  page_first: '1267'
  page_last: '1276'
  pages: "1267\u20131276"
  paper_id: '158'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1158.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1158.jpg
  title: Collective Event Detection via a Hierarchical and Bias Tagging Networks with
    Gated Multi-level Attention Mechanisms
  title_html: Collective Event Detection via a Hierarchical and Bias Tagging Networks
    with Gated Multi-level Attention Mechanisms
  url: https://www.aclweb.org/anthology/D18-1158
  year: '2018'
D18-1159:
  abstract: "We present a complete, automated, and efficient approach for utilizing\
    \ valency analysis in making dependency parsing decisions. It includes extraction\
    \ of valency patterns, a probabilistic model for tagging these patterns, and a\
    \ joint decoding process that explicitly considers the number and types of each\
    \ token\u2019s syntactic dependents. On 53 treebanks representing 41 languages\
    \ in the Universal Dependencies data, we find that incorporating valency information\
    \ yields higher precision and F1 scores on the core arguments (subjects and complements)\
    \ and functional relations (e.g., auxiliaries) that we employ for valency analysis.\
    \ Precision on core arguments improves from 80.87 to 85.43. We further show that\
    \ our approach can be applied to an ostensibly different formalism and dataset,\
    \ Tree Adjoining Grammar as extracted from the Penn Treebank; there, we outperform\
    \ the previous state-of-the-art labeled attachment score by 0.7. Finally, we explore\
    \ the potential of extending valency patterns beyond their traditional domain\
    \ by confirming their helpfulness in improving PP attachment decisions."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1159.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1159.Attachment.zip
  - filename: https://vimeo.com/305214708
    type: video
    url: https://vimeo.com/305214708
  author:
  - first: Tianze
    full: Tianze Shi
    id: tianze-shi
    last: Shi
  - first: Lillian
    full: Lillian Lee
    id: lillian-lee
    last: Lee
  author_string: Tianze Shi, Lillian Lee
  bibkey: shi-lee-2018-valency
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1159
  month: October-November
  page_first: '1277'
  page_last: '1291'
  pages: "1277\u20131291"
  paper_id: '159'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1159.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1159.jpg
  title: Valency-Augmented Dependency Parsing
  title_html: Valency-Augmented Dependency Parsing
  url: https://www.aclweb.org/anthology/D18-1159
  year: '2018'
D18-1160:
  abstract: 'Unsupervised learning of syntactic structure is typically performed using
    generative models with discrete latent variables and multinomial parameters. In
    most cases, these models have not leveraged continuous word representations. In
    this work, we propose a novel generative model that jointly learns discrete syntactic
    structure and continuous word representations in an unsupervised fashion by cascading
    an invertible neural network with a structured generative prior. We show that
    the invertibility condition allows for efficient exact inference and marginal
    likelihood computation in our model so long as the prior is well-behaved. In experiments
    we instantiate our approach with both Markov and tree-structured priors, evaluating
    on two tasks: part-of-speech (POS) induction, and unsupervised dependency parsing
    without gold POS annotation. On the Penn Treebank, our Markov-structured model
    surpasses state-of-the-art results on POS induction. Similarly, we find that our
    tree-structured model achieves state-of-the-art performance on unsupervised dependency
    parsing for the difficult training condition where neither gold POS annotation
    nor punctuation-based constraints are available.'
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305215139
    type: video
    url: https://vimeo.com/305215139
  author:
  - first: Junxian
    full: Junxian He
    id: junxian-he
    last: He
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  - first: Taylor
    full: Taylor Berg-Kirkpatrick
    id: taylor-berg-kirkpatrick
    last: Berg-Kirkpatrick
  author_string: Junxian He, Graham Neubig, Taylor Berg-Kirkpatrick
  bibkey: he-etal-2018-unsupervised
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1160
  month: October-November
  page_first: '1292'
  page_last: '1302'
  pages: "1292\u20131302"
  paper_id: '160'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1160.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1160.jpg
  title: Unsupervised Learning of Syntactic Structure with Invertible Neural Projections
  title_html: Unsupervised Learning of Syntactic Structure with Invertible Neural
    Projections
  url: https://www.aclweb.org/anthology/D18-1160
  year: '2018'
D18-1161:
  abstract: 'We introduce novel dynamic oracles for training two of the most accurate
    known shift-reduce algorithms for constituent parsing: the top-down and in-order
    transition-based parsers. In both cases, the dynamic oracles manage to notably
    increase their accuracy, in comparison to that obtained by performing classic
    static training. In addition, by improving the performance of the state-of-the-art
    in-order shift-reduce parser, we achieve the best accuracy to date (92.0 F1) obtained
    by a fully-supervised single-model greedy shift-reduce constituent parser on the
    WSJ benchmark.'
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305215645
    type: video
    url: https://vimeo.com/305215645
  author:
  - first: Daniel
    full: "Daniel Fern\xE1ndez-Gonz\xE1lez"
    id: daniel-fernandez-gonzalez
    last: "Fern\xE1ndez-Gonz\xE1lez"
  - first: Carlos
    full: "Carlos G\xF3mez-Rodr\xEDguez"
    id: carlos-gomez-rodriguez
    last: "G\xF3mez-Rodr\xEDguez"
  author_string: "Daniel Fern\xE1ndez-Gonz\xE1lez, Carlos G\xF3mez-Rodr\xEDguez"
  bibkey: fernandez-gonzalez-gomez-rodriguez-2018-dynamic-oracles
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1161
  month: October-November
  page_first: '1303'
  page_last: '1313'
  pages: "1303\u20131313"
  paper_id: '161'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1161.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1161.jpg
  title: Dynamic Oracles for Top-Down and In-Order Shift-Reduce Constituent Parsing
  title_html: Dynamic Oracles for Top-Down and In-Order Shift-Reduce Constituent Parsing
  url: https://www.aclweb.org/anthology/D18-1161
  year: '2018'
D18-1162:
  abstract: 'We introduce a method to reduce constituent parsing to sequence labeling.
    For each word wt, it generates a label that encodes: (1) the number of ancestors
    in the tree that the words , it generates a label that encodes: (1) the number
    of ancestors in the tree that the words wt and and wt+1 have in common, and (2)
    the nonterminal symbol at the lowest common ancestor. We first prove that the
    proposed encoding function is injective for any tree without unary branches. In
    practice, the approach is made extensible to all constituency trees by collapsing
    unary branches. We then use the PTB and CTB treebanks as testbeds and propose
    a set of fast baselines. We achieve 90% F-score on the PTB test set, outperforming
    the Vinyals et al. (2015) sequence-to-sequence parser. In addition, sacrificing
    some accuracy, our approach achieves the fastest constituent parsing speeds reported
    to date on PTB by a wide margin. have in common, and (2) the nonterminal symbol
    at the lowest common ancestor. We first prove that the proposed encoding function
    is injective for any tree without unary branches. In practice, the approach is
    made extensible to all constituency trees by collapsing unary branches. We then
    use the PTB and CTB treebanks as testbeds and propose a set of fast baselines.
    We achieve 90% F-score on the PTB test set, outperforming the Vinyals et al. (2015)
    sequence-to-sequence parser. In addition, sacrificing some accuracy, our approach
    achieves the fastest constituent parsing speeds reported to date on PTB by a wide
    margin.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1162.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1162.Attachment.pdf
  - filename: https://vimeo.com/305216237
    type: video
    url: https://vimeo.com/305216237
  author:
  - first: Carlos
    full: "Carlos G\xF3mez-Rodr\xEDguez"
    id: carlos-gomez-rodriguez
    last: "G\xF3mez-Rodr\xEDguez"
  - first: David
    full: David Vilares
    id: david-vilares
    last: Vilares
  author_string: "Carlos G\xF3mez-Rodr\xEDguez, David Vilares"
  bibkey: gomez-rodriguez-vilares-2018-constituent
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1162
  month: October-November
  page_first: '1314'
  page_last: '1324'
  pages: "1314\u20131324"
  paper_id: '162'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1162.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1162.jpg
  title: Constituent Parsing as Sequence Labeling
  title_html: Constituent Parsing as Sequence Labeling
  url: https://www.aclweb.org/anthology/D18-1162
  year: '2018'
D18-1163:
  abstract: "To approximately parse an unfamiliar language, it helps to have a treebank\
    \ of a similar language. But what if the closest available treebank still has\
    \ the wrong word order? We show how to (stochastically) permute the constituents\
    \ of an existing dependency treebank so that its surface part-of-speech statistics\
    \ approximately match those of the target language. The parameters of the permutation\
    \ model can be evaluated for quality by dynamic programming and tuned by gradient\
    \ descent (up to a local optimum). This optimization procedure yields trees for\
    \ a new artificial language that resembles the target language. We show that delexicalized\
    \ parsers for the target language can be successfully trained using such \u201C\
    made to order\u201D artificial languages."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1163.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1163.Attachment.pdf
  - filename: https://vimeo.com/305216917
    type: video
    url: https://vimeo.com/305216917
  author:
  - first: Dingquan
    full: Dingquan Wang
    id: dingquan-wang
    last: Wang
  - first: Jason
    full: Jason Eisner
    id: jason-eisner
    last: Eisner
  author_string: Dingquan Wang, Jason Eisner
  bibkey: wang-eisner-2018-synthetic
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1163
  month: October-November
  page_first: '1325'
  page_last: '1337'
  pages: "1325\u20131337"
  paper_id: '163'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1163.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1163.jpg
  title: 'Synthetic Data Made to Order: The Case of Parsing'
  title_html: 'Synthetic Data Made to Order: The Case of Parsing'
  url: https://www.aclweb.org/anthology/D18-1163
  year: '2018'
D18-1164:
  abstract: 'In Visual Question Answering, most existing approaches adopt the pipeline
    of representing an image via pre-trained CNNs, and then using the uninterpretable
    CNN features in conjunction with the question to predict the answer. Although
    such end-to-end models might report promising performance, they rarely provide
    any insight, apart from the answer, into the VQA process. In this work, we propose
    to break up the end-to-end VQA into two steps: explaining and reasoning, in an
    attempt towards a more explainable VQA by shedding light on the intermediate results
    between these two steps. To that end, we first extract attributes and generate
    descriptions as explanations for an image. Next, a reasoning module utilizes these
    explanations in place of the image to infer an answer. The advantages of such
    a breakdown include: (1) the attributes and captions can reflect what the system
    extracts from the image, thus can provide some insights for the predicted answer;
    (2) these intermediate results can help identify the inabilities of the image
    understanding or the answer inference part when the predicted answer is wrong.
    We conduct extensive experiments on a popular VQA dataset and our system achieves
    comparable performance with the baselines, yet with added benefits of explanability
    and the inherent ability to further improve with higher quality explanations.'
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306362344
    type: video
    url: https://vimeo.com/306362344
  author:
  - first: Qing
    full: Qing Li
    id: qing-li
    last: Li
  - first: Jianlong
    full: Jianlong Fu
    id: jianlong-fu
    last: Fu
  - first: Dongfei
    full: Dongfei Yu
    id: dongfei-yu
    last: Yu
  - first: Tao
    full: Tao Mei
    id: tao-mei
    last: Mei
  - first: Jiebo
    full: Jiebo Luo
    id: jiebo-luo
    last: Luo
  author_string: Qing Li, Jianlong Fu, Dongfei Yu, Tao Mei, Jiebo Luo
  bibkey: li-etal-2018-tell
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1164
  month: October-November
  page_first: '1338'
  page_last: '1346'
  pages: "1338\u20131346"
  paper_id: '164'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1164.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1164.jpg
  title: 'Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes
    and Captions'
  title_html: 'Tell-and-Answer: Towards Explainable Visual Question Answering using
    Attributes and Captions'
  url: https://www.aclweb.org/anthology/D18-1164
  year: '2018'
D18-1165:
  abstract: Active learning identifies data points to label that are expected to be
    the most useful in improving a supervised model. Opportunistic active learning
    incorporates active learning into interactive tasks that constrain possible queries
    during interactions. Prior work has shown that opportunistic active learning can
    be used to improve grounding of natural language descriptions in an interactive
    object retrieval task. In this work, we use reinforcement learning for such an
    object retrieval task, to learn a policy that effectively trades off task completion
    with model improvement that would benefit future tasks.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1165.Attachment.tgz
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1165.Attachment.tgz
  - filename: https://vimeo.com/306362379
    type: video
    url: https://vimeo.com/306362379
  author:
  - first: Aishwarya
    full: Aishwarya Padmakumar
    id: aishwarya-padmakumar
    last: Padmakumar
  - first: Peter
    full: Peter Stone
    id: peter-stone
    last: Stone
  - first: Raymond
    full: Raymond Mooney
    id: raymond-mooney
    last: Mooney
  author_string: Aishwarya Padmakumar, Peter Stone, Raymond Mooney
  bibkey: padmakumar-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1165
  month: October-November
  page_first: '1347'
  page_last: '1357'
  pages: "1347\u20131357"
  paper_id: '165'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1165.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1165.jpg
  title: Learning a Policy for Opportunistic Active Learning
  title_html: Learning a Policy for Opportunistic Active Learning
  url: https://www.aclweb.org/anthology/D18-1165
  year: '2018'
D18-1166:
  abstract: Understanding and reasoning about cooking recipes is a fruitful research
    direction towards enabling machines to interpret procedural text. In this work,
    we introduce RecipeQA, a dataset for multimodal comprehension of cooking recipes.
    It comprises of approximately 20K instructional recipes with multiple modalities
    such as titles, descriptions and aligned set of images. With over 36K automatically
    generated question-answer pairs, we design a set of comprehension and reasoning
    tasks that require joint understanding of images and text, capturing the temporal
    flow of events and making sense of procedural knowledge. Our preliminary results
    indicate that RecipeQA will serve as a challenging test bed and an ideal benchmark
    for evaluating machine comprehension systems. The data and leaderboard are available
    at http://hucvl.github.io/recipeqa.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1166.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1166.Attachment.pdf
  - filename: https://vimeo.com/306363701
    type: video
    url: https://vimeo.com/306363701
  author:
  - first: Semih
    full: Semih Yagcioglu
    id: semih-yagcioglu
    last: Yagcioglu
  - first: Aykut
    full: Aykut Erdem
    id: aykut-erdem
    last: Erdem
  - first: Erkut
    full: Erkut Erdem
    id: erkut-erdem
    last: Erdem
  - first: Nazli
    full: Nazli Ikizler-Cinbis
    id: nazli-ikizler-cinbis
    last: Ikizler-Cinbis
  author_string: Semih Yagcioglu, Aykut Erdem, Erkut Erdem, Nazli Ikizler-Cinbis
  bibkey: yagcioglu-etal-2018-recipeqa
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1166
  month: October-November
  page_first: '1358'
  page_last: '1368'
  pages: "1358\u20131368"
  paper_id: '166'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1166.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1166.jpg
  title: 'RecipeQA: A Challenge Dataset for Multimodal Comprehension of Cooking Recipes'
  title_html: '<span class="acl-fixed-case">R</span>ecipe<span class="acl-fixed-case">QA</span>:
    A Challenge Dataset for Multimodal Comprehension of Cooking Recipes'
  url: https://www.aclweb.org/anthology/D18-1166
  year: '2018'
D18-1167:
  abstract: Recent years have witnessed an increasing interest in image-based question-answering
    (QA) tasks. However, due to data limitations, there has been much less work on
    video-based QA. In this paper, we present TVQA, a large-scale video QA dataset
    based on 6 popular TV shows. TVQA consists of 152,545 QA pairs from 21,793 clips,
    spanning over 460 hours of video. Questions are designed to be compositional in
    nature, requiring systems to jointly localize relevant moments within a clip,
    comprehend subtitle-based dialogue, and recognize relevant visual concepts. We
    provide analyses of this new dataset as well as several baselines and a multi-stream
    end-to-end trainable neural network framework for the TVQA task. The dataset is
    publicly available at http://tvqa.cs.unc.edu.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1167.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1167.Attachment.pdf
  - filename: https://vimeo.com/306364803
    type: video
    url: https://vimeo.com/306364803
  author:
  - first: Jie
    full: Jie Lei
    id: jie-lei
    last: Lei
  - first: Licheng
    full: Licheng Yu
    id: licheng-yu
    last: Yu
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  - first: Tamara
    full: Tamara Berg
    id: tamara-berg
    last: Berg
  author_string: Jie Lei, Licheng Yu, Mohit Bansal, Tamara Berg
  bibkey: lei-etal-2018-tvqa
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1167
  month: October-November
  page_first: '1369'
  page_last: '1379'
  pages: "1369\u20131379"
  paper_id: '167'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1167.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1167.jpg
  title: 'TVQA: Localized, Compositional Video Question Answering'
  title_html: '<span class="acl-fixed-case">TVQA</span>: Localized, Compositional
    Video Question Answering'
  url: https://www.aclweb.org/anthology/D18-1167
  year: '2018'
D18-1168:
  abstract: 'Localizing moments in a longer video via natural language queries is
    a new, challenging task at the intersection of language and video understanding.
    Though moment localization with natural language is similar to other language
    and vision tasks like natural language object retrieval in images, moment localization
    offers an interesting opportunity to model temporal dependencies and reasoning
    in text. We propose a new model that explicitly reasons about different temporal
    segments in a video, and shows that temporal context is important for localizing
    phrases which include temporal language. To benchmark whether our model, and other
    recent video localization models, can effectively reason about temporal language,
    we collect the novel TEMPOral reasoning in video and language (TEMPO) dataset.
    Our dataset consists of two parts: a dataset with real videos and template sentences
    (TEMPO - Template Language) which allows for controlled studies on temporal language,
    and a human language dataset which consists of temporal sentences annotated by
    humans (TEMPO - Human Language).'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1168.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1168.Attachment.pdf
  - filename: https://vimeo.com/306365884
    type: video
    url: https://vimeo.com/306365884
  author:
  - first: Lisa Anne
    full: Lisa Anne Hendricks
    id: lisa-anne-hendricks
    last: Hendricks
  - first: Oliver
    full: Oliver Wang
    id: oliver-wang
    last: Wang
  - first: Eli
    full: Eli Shechtman
    id: eli-shechtman
    last: Shechtman
  - first: Josef
    full: Josef Sivic
    id: josef-sivic
    last: Sivic
  - first: Trevor
    full: Trevor Darrell
    id: trevor-darrell
    last: Darrell
  - first: Bryan
    full: Bryan Russell
    id: bryan-russell
    last: Russell
  author_string: Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor
    Darrell, Bryan Russell
  bibkey: hendricks-etal-2018-localizing
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1168
  month: October-November
  page_first: '1380'
  page_last: '1390'
  pages: "1380\u20131390"
  paper_id: '168'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1168.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1168.jpg
  title: Localizing Moments in Video with Temporal Language
  title_html: Localizing Moments in Video with Temporal Language
  url: https://www.aclweb.org/anthology/D18-1168
  year: '2018'
D18-1169:
  abstract: Rare word representation has recently enjoyed a surge of interest, owing
    to the crucial role that effective handling of infrequent words can play in accurate
    semantic understanding. However, there is a paucity of reliable benchmarks for
    evaluation and comparison of these techniques. We show in this paper that the
    only existing benchmark (the Stanford Rare Word dataset) suffers from low-confidence
    annotations and limited vocabulary; hence, it does not constitute a solid comparison
    framework. In order to fill this evaluation gap, we propose Cambridge Rare word
    Dataset (Card-660), an expert-annotated word similarity dataset which provides
    a highly reliable, yet challenging, benchmark for rare word representation techniques.
    Through a set of experiments we show that even the best mainstream word embeddings,
    with millions of words in their vocabularies, are unable to achieve performances
    higher than 0.43 (Pearson correlation) on the dataset, compared to a human-level
    upperbound of 0.90. We release the dataset and the annotation materials at https://pilehvar.github.io/card-660/.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1169.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1169.Attachment.zip
  author:
  - first: Mohammad Taher
    full: Mohammad Taher Pilehvar
    id: mohammad-taher-pilehvar
    last: Pilehvar
  - first: Dimitri
    full: Dimitri Kartsaklis
    id: dimitri-kartsaklis
    last: Kartsaklis
  - first: Victor
    full: Victor Prokhorov
    id: victor-prokhorov
    last: Prokhorov
  - first: Nigel
    full: Nigel Collier
    id: nigel-collier
    last: Collier
  author_string: Mohammad Taher Pilehvar, Dimitri Kartsaklis, Victor Prokhorov, Nigel
    Collier
  bibkey: pilehvar-etal-2018-card
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1169
  month: October-November
  page_first: '1391'
  page_last: '1401'
  pages: "1391\u20131401"
  paper_id: '169'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1169.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1169.jpg
  title: 'Card-660: Cambridge Rare Word Dataset - a Reliable Benchmark for Infrequent
    Word Representation Models'
  title_html: 'Card-660: <span class="acl-fixed-case">C</span>ambridge Rare Word Dataset
    - a Reliable Benchmark for Infrequent Word Representation Models'
  url: https://www.aclweb.org/anthology/D18-1169
  year: '2018'
D18-1170:
  abstract: The goal of Word Sense Disambiguation (WSD) is to identify the correct
    meaning of a word in the particular context. Traditional supervised methods only
    use labeled data (context), while missing rich lexical knowledge such as the gloss
    which defines the meaning of a word sense. Recent studies have shown that incorporating
    glosses into neural networks for WSD has made significant improvement. However,
    the previous models usually build the context representation and gloss representation
    separately. In this paper, we find that the learning for the context and gloss
    representation can benefit from each other. Gloss can help to highlight the important
    words in the context, thus building a better context representation. Context can
    also help to locate the key words in the gloss of the correct word sense. Therefore,
    we introduce a co-attention mechanism to generate co-dependent representations
    for the context and gloss. Furthermore, in order to capture both word-level and
    sentence-level information, we extend the attention mechanism in a hierarchical
    fashion. Experimental results show that our model achieves the state-of-the-art
    results on several standard English all-words WSD test datasets.
  address: Brussels, Belgium
  author:
  - first: Fuli
    full: Fuli Luo
    id: fuli-luo
    last: Luo
  - first: Tianyu
    full: Tianyu Liu
    id: tianyu-liu
    last: Liu
  - first: Zexue
    full: Zexue He
    id: zexue-he
    last: He
  - first: Qiaolin
    full: Qiaolin Xia
    id: qiaolin-xia
    last: Xia
  - first: Zhifang
    full: Zhifang Sui
    id: zhifang-sui
    last: Sui
  - first: Baobao
    full: Baobao Chang
    id: baobao-chang
    last: Chang
  author_string: Fuli Luo, Tianyu Liu, Zexue He, Qiaolin Xia, Zhifang Sui, Baobao
    Chang
  bibkey: luo-etal-2018-leveraging
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1170
  month: October-November
  page_first: '1402'
  page_last: '1411'
  pages: "1402\u20131411"
  paper_id: '170'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1170.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1170.jpg
  title: Leveraging Gloss Knowledge in Neural Word Sense Disambiguation by Hierarchical
    Co-Attention
  title_html: Leveraging Gloss Knowledge in Neural Word Sense Disambiguation by Hierarchical
    Co-Attention
  url: https://www.aclweb.org/anthology/D18-1170
  year: '2018'
D18-1171:
  abstract: We encounter metaphors every day, but only a few jump out on us and make
    us stumble. However, little effort has been devoted to investigating more novel
    metaphors in comparison to general metaphor detection efforts. We attribute this
    gap primarily to the lack of larger datasets that distinguish between conventionalized,
    i.e., very common, and novel metaphors. The goal of this paper is to alleviate
    this situation by introducing a crowdsourced novel metaphor annotation layer for
    an existing metaphor corpus. Further, we analyze our corpus and investigate correlations
    between novelty and features that are typically used in metaphor detection, such
    as concreteness ratings and more semantic features like the Potential for Metaphoricity.
    Finally, we present a baseline approach to assess novelty in metaphors based on
    our annotations.
  address: Brussels, Belgium
  author:
  - first: "Erik-L\xE2n"
    full: "Erik-L\xE2n Do Dinh"
    id: erik-lan-do-dinh
    last: Do Dinh
  - first: Hannah
    full: Hannah Wieland
    id: hannah-wieland
    last: Wieland
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: "Erik-L\xE2n Do Dinh, Hannah Wieland, Iryna Gurevych"
  bibkey: do-dinh-etal-2018-weeding
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1171
  month: October-November
  page_first: '1412'
  page_last: '1424'
  pages: "1412\u20131424"
  paper_id: '171'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1171.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1171.jpg
  title: 'Weeding out Conventionalized Metaphors: A Corpus of Novel Metaphor Annotations'
  title_html: 'Weeding out Conventionalized Metaphors: A Corpus of Novel Metaphor
    Annotations'
  url: https://www.aclweb.org/anthology/D18-1171
  year: '2018'
D18-1172:
  abstract: Accurately and efficiently estimating word similarities from text is fundamental
    in natural language processing. In this paper, we propose a fast and lightweight
    method for estimating similarities from streams by explicitly counting second-order
    co-occurrences. The method rests on the observation that words that are highly
    correlated with respect to such counts are also highly similar with respect to
    first-order co-occurrences. Using buffers of co-occurred words per word to count
    second-order co-occurrences, we can then estimate similarities in a single pass
    over data without having to do prohibitively expensive similarity calculations.
    We demonstrate that this approach is scalable, converges rapidly, behaves robustly
    under parameter changes, and that it captures word similarities on par with those
    given by state-of-the-art word embeddings.
  address: Brussels, Belgium
  author:
  - first: Olof
    full: "Olof G\xF6rnerup"
    id: olof-gornerup
    last: "G\xF6rnerup"
  - first: Daniel
    full: Daniel Gillblad
    id: daniel-gillblad
    last: Gillblad
  author_string: "Olof G\xF6rnerup, Daniel Gillblad"
  bibkey: gornerup-gillblad-2018-streaming
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1172
  month: October-November
  page_first: '1425'
  page_last: '1434'
  pages: "1425\u20131434"
  paper_id: '172'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1172.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1172.jpg
  title: Streaming word similarity mining on the cheap
  title_html: Streaming word similarity mining on the cheap
  url: https://www.aclweb.org/anthology/D18-1172
  year: '2018'
D18-1173:
  abstract: Distributional semantic models (DSMs) generally require sufficient examples
    for a word to learn a high quality representation. This is in stark contrast with
    human who can guess the meaning of a word from one or a few referents only. In
    this paper, we propose Mem2Vec, a memory based embedding learning method capable
    of acquiring high quality word representations from fairly limited context. Our
    method directly adapts the representations produced by a DSM with a longterm memory
    to guide its guess of a novel word. Based on a pre-trained embedding space, the
    proposed method delivers impressive performance on two challenging few-shot word
    similarity tasks. Embeddings learned with our method also lead to considerable
    improvements over strong baselines on NER and sentiment classification.
  address: Brussels, Belgium
  author:
  - first: Jingyuan
    full: Jingyuan Sun
    id: jingyuan-sun
    last: Sun
  - first: Shaonan
    full: Shaonan Wang
    id: shaonan-wang
    last: Wang
  - first: Chengqing
    full: Chengqing Zong
    id: chengqing-zong
    last: Zong
  author_string: Jingyuan Sun, Shaonan Wang, Chengqing Zong
  bibkey: sun-etal-2018-memory
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1173
  month: October-November
  page_first: '1435'
  page_last: '1444'
  pages: "1435\u20131444"
  paper_id: '173'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1173.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1173.jpg
  title: 'Memory, Show the Way: Memory Based Few Shot Word Representation Learning'
  title_html: 'Memory, Show the Way: Memory Based Few Shot Word Representation Learning'
  url: https://www.aclweb.org/anthology/D18-1173
  year: '2018'
D18-1174:
  abstract: 'We present disambiguated skip-gram: a neural-probabilistic model for
    learning multi-sense distributed representations of words. Disambiguated skip-gram
    jointly estimates a skip-gram-like context word prediction model and a word sense
    disambiguation model. Unlike previous probabilistic models for learning multi-sense
    word embeddings, disambiguated skip-gram is end-to-end differentiable and can
    be interpreted as a simple feed-forward neural network. We also introduce an effective
    pruning strategy for the embeddings learned by disambiguated skip-gram. This allows
    us to control the granularity of representations learned by our model. In experimental
    evaluation disambiguated skip-gram improves state-of-the are results in several
    word sense induction benchmarks.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1174.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1174.Attachment.zip
  author:
  - first: Karol
    full: Karol Grzegorczyk
    id: karol-grzegorczyk
    last: Grzegorczyk
  - first: Marcin
    full: Marcin Kurdziel
    id: marcin-kurdziel
    last: Kurdziel
  author_string: Karol Grzegorczyk, Marcin Kurdziel
  bibkey: grzegorczyk-kurdziel-2018-disambiguated
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1174
  month: October-November
  page_first: '1445'
  page_last: '1454'
  pages: "1445\u20131454"
  paper_id: '174'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1174.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1174.jpg
  title: Disambiguated skip-gram model
  title_html: Disambiguated skip-gram model
  url: https://www.aclweb.org/anthology/D18-1174
  year: '2018'
D18-1175:
  abstract: During natural disasters and conflicts, information about what happened
    is often confusing and messy, and distributed across many sources. We would like
    to be able to automatically identify relevant information and assemble it into
    coherent narratives of what happened. To make this task accessible to neural models,
    we introduce Story Salads, mixtures of multiple documents that can be generated
    at scale. By exploiting the Wikipedia hierarchy, we can generate salads that exhibit
    challenging inference problems. Story salads give rise to a novel, challenging
    clustering task, where the objective is to group sentences from the same narratives.
    We demonstrate that simple bag-of-words similarity clustering falls short on this
    task, and that it is necessary to take into account global context and coherence.
  address: Brussels, Belgium
  author:
  - first: Su
    full: Su Wang
    id: su-wang
    last: Wang
  - first: Eric
    full: Eric Holgate
    id: eric-holgate
    last: Holgate
  - first: Greg
    full: Greg Durrett
    id: greg-durrett
    last: Durrett
  - first: Katrin
    full: Katrin Erk
    id: katrin-erk
    last: Erk
  author_string: Su Wang, Eric Holgate, Greg Durrett, Katrin Erk
  bibkey: wang-etal-2018-picking
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1175
  month: October-November
  page_first: '1455'
  page_last: '1465'
  pages: "1455\u20131465"
  paper_id: '175'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1175.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1175.jpg
  title: Picking Apart Story Salads
  title_html: Picking Apart Story Salads
  url: https://www.aclweb.org/anthology/D18-1175
  year: '2018'
D18-1176:
  abstract: While one of the first steps in many NLP systems is selecting what pre-trained
    word embeddings to use, we argue that such a step is better left for neural networks
    to figure out by themselves. To that end, we introduce dynamic meta-embeddings,
    a simple yet effective method for the supervised learning of embedding ensembles,
    which leads to state-of-the-art performance within the same model class on a variety
    of tasks. We subsequently show how the technique can be used to shed new light
    on the usage of word embeddings in NLP systems.
  address: Brussels, Belgium
  author:
  - first: Douwe
    full: Douwe Kiela
    id: douwe-kiela
    last: Kiela
  - first: Changhan
    full: Changhan Wang
    id: changhan-wang
    last: Wang
  - first: Kyunghyun
    full: Kyunghyun Cho
    id: kyunghyun-cho
    last: Cho
  author_string: Douwe Kiela, Changhan Wang, Kyunghyun Cho
  bibkey: kiela-etal-2018-dynamic
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1176
  month: October-November
  page_first: '1466'
  page_last: '1477'
  pages: "1466\u20131477"
  paper_id: '176'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1176.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1176.jpg
  title: Dynamic Meta-Embeddings for Improved Sentence Representations
  title_html: Dynamic Meta-Embeddings for Improved Sentence Representations
  url: https://www.aclweb.org/anthology/D18-1176
  year: '2018'
D18-1177:
  abstract: 'Several recent studies have shown the benefits of combining language
    and perception to infer word embeddings. These multimodal approaches either simply
    combine pre-trained textual and visual representations (e.g. features extracted
    from convolutional neural networks), or use the latter to bias the learning of
    textual word embeddings. In this work, we propose a novel probabilistic model
    to formalize how linguistic and perceptual inputs can work in concert to explain
    the observed word-context pairs in a text corpus. Our approach learns textual
    and visual representations jointly: latent visual factors couple together a skip-gram
    model for co-occurrence in linguistic data and a generative latent variable model
    for visual data. Extensive experimental studies validate the proposed model. Concretely,
    on the tasks of assessing pairwise word similarity and image/caption retrieval,
    our approach attains equally competitive or stronger results when compared to
    other state-of-the-art multimodal models.'
  address: Brussels, Belgium
  author:
  - first: Melissa
    full: Melissa Ailem
    id: melissa-ailem
    last: Ailem
  - first: Bowen
    full: Bowen Zhang
    id: bowen-zhang
    last: Zhang
  - first: Aurelien
    full: Aurelien Bellet
    id: aurelien-bellet
    last: Bellet
  - first: Pascal
    full: Pascal Denis
    id: pascal-denis
    last: Denis
  - first: Fei
    full: Fei Sha
    id: fei-sha
    last: Sha
  author_string: Melissa Ailem, Bowen Zhang, Aurelien Bellet, Pascal Denis, Fei Sha
  bibkey: ailem-etal-2018-probabilistic
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1177
  month: October-November
  page_first: '1478'
  page_last: '1487'
  pages: "1478\u20131487"
  paper_id: '177'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1177.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1177.jpg
  title: A Probabilistic Model for Joint Learning of Word Embeddings from Texts and
    Images
  title_html: A Probabilistic Model for Joint Learning of Word Embeddings from Texts
    and Images
  url: https://www.aclweb.org/anthology/D18-1177
  year: '2018'
D18-1178:
  abstract: "In this paper, we empirically evaluate the utility of transfer and multi-task\
    \ learning on a challenging semantic classification task: semantic interpretation\
    \ of noun\u2013noun compounds. Through a comprehensive series of experiments and\
    \ in-depth error analysis, we show that transfer learning via parameter initialization\
    \ and multi-task learning via parameter sharing can help a neural classification\
    \ model generalize over a highly skewed distribution of relations. Further, we\
    \ demonstrate how dual annotation with two distinct sets of relations over the\
    \ same set of compounds can be exploited to improve the overall accuracy of a\
    \ neural classifier and its F1 scores on the less frequent, but more difficult\
    \ relations."
  address: Brussels, Belgium
  author:
  - first: Murhaf
    full: Murhaf Fares
    id: murhaf-fares
    last: Fares
  - first: Stephan
    full: Stephan Oepen
    id: stephan-oepen
    last: Oepen
  - first: Erik
    full: Erik Velldal
    id: erik-velldal
    last: Velldal
  author_string: Murhaf Fares, Stephan Oepen, Erik Velldal
  bibkey: fares-etal-2018-transfer
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1178
  month: October-November
  page_first: '1488'
  page_last: '1498'
  pages: "1488\u20131498"
  paper_id: '178'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1178.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1178.jpg
  title: "Transfer and Multi-Task Learning for Noun\u2013Noun Compound Interpretation"
  title_html: "Transfer and Multi-Task Learning for Noun\u2013Noun Compound Interpretation"
  url: https://www.aclweb.org/anthology/D18-1178
  year: '2018'
D18-1179:
  abstract: Contextual word representations derived from pre-trained bidirectional
    language models (biLMs) have recently been shown to provide significant improvements
    to the state of the art for a wide range of NLP tasks. However, many questions
    remain as to how and why these models are so effective. In this paper, we present
    a detailed empirical study of how the choice of neural architecture (e.g. LSTM,
    CNN, or self attention) influences both end task accuracy and qualitative properties
    of the representations that are learned. We show there is a tradeoff between speed
    and accuracy, but all architectures learn high quality contextual representations
    that outperform word embeddings for four challenging NLP tasks. Additionally,
    all architectures learn representations that vary with network depth, from exclusively
    morphological based at the word embedding layer through local syntax based in
    the lower contextual layers to longer range semantics such coreference at the
    upper layers. Together, these results suggest that unsupervised biLMs, independent
    of architecture, are learning much more about the structure of language than previously
    appreciated.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1179.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1179.Attachment.pdf
  author:
  - first: Matthew
    full: Matthew Peters
    id: matthew-peters
    last: Peters
  - first: Mark
    full: Mark Neumann
    id: mark-neumann
    last: Neumann
  - first: Luke
    full: Luke Zettlemoyer
    id: luke-zettlemoyer
    last: Zettlemoyer
  - first: Wen-tau
    full: Wen-tau Yih
    id: wen-tau-yih
    last: Yih
  author_string: Matthew Peters, Mark Neumann, Luke Zettlemoyer, Wen-tau Yih
  bibkey: peters-etal-2018-dissecting
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1179
  month: October-November
  page_first: '1499'
  page_last: '1509'
  pages: "1499\u20131509"
  paper_id: '179'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1179.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1179.jpg
  title: 'Dissecting Contextual Word Embeddings: Architecture and Representation'
  title_html: 'Dissecting Contextual Word Embeddings: Architecture and Representation'
  url: https://www.aclweb.org/anthology/D18-1179
  year: '2018'
D18-1180:
  abstract: "Prepositions are highly polysemous, and their variegated senses encode\
    \ significant semantic information. In this paper we match each preposition\u2019\
    s left- and right context, and their interplay to the geometry of the word vectors\
    \ to the left and right of the preposition. Extracting these features from a large\
    \ corpus and using them with machine learning models makes for an efficient preposition\
    \ sense disambiguation (PSD) algorithm, which is comparable to and better than\
    \ state-of-the-art on two benchmark datasets. Our reliance on no linguistic tool\
    \ allows us to scale the PSD algorithm to a large corpus and learn sense-specific\
    \ preposition representations. The crucial abstraction of preposition senses as\
    \ word representations permits their use in downstream applications\u2013phrasal\
    \ verb paraphrasing and preposition selection\u2013with new state-of-the-art results."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1180.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1180.Attachment.zip
  author:
  - first: Hongyu
    full: Hongyu Gong
    id: hongyu-gong
    last: Gong
  - first: Jiaqi
    full: Jiaqi Mu
    id: jiaqi-mu
    last: Mu
  - first: Suma
    full: Suma Bhat
    id: suma-bhat
    last: Bhat
  - first: Pramod
    full: Pramod Viswanath
    id: pramod-viswanath
    last: Viswanath
  author_string: Hongyu Gong, Jiaqi Mu, Suma Bhat, Pramod Viswanath
  bibkey: gong-etal-2018-preposition
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1180
  month: October-November
  page_first: '1510'
  page_last: '1521'
  pages: "1510\u20131521"
  paper_id: '180'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1180.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1180.jpg
  title: Preposition Sense Disambiguation and Representation
  title_html: Preposition Sense Disambiguation and Representation
  url: https://www.aclweb.org/anthology/D18-1180
  year: '2018'
D18-1181:
  abstract: Monolingual dictionaries are widespread and semantically rich resources.
    This paper presents a simple model that learns to compute word embeddings by processing
    dictionary definitions and trying to reconstruct them. It exploits the inherent
    recursivity of dictionaries by encouraging consistency between the representations
    it uses as inputs and the representations it produces as outputs. The resulting
    embeddings are shown to capture semantic similarity better than regular distributional
    methods and other dictionary-based methods. In addition, our method shows strong
    performance when trained exclusively on dictionary data and generalizes in one
    shot.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1181.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1181.Attachment.zip
  author:
  - first: Tom
    full: Tom Bosc
    id: tom-bosc
    last: Bosc
  - first: Pascal
    full: Pascal Vincent
    id: pascal-vincent
    last: Vincent
  author_string: Tom Bosc, Pascal Vincent
  bibkey: bosc-vincent-2018-auto
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1181
  month: October-November
  page_first: '1522'
  page_last: '1532'
  pages: "1522\u20131532"
  paper_id: '181'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1181.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1181.jpg
  title: Auto-Encoding Dictionary Definitions into Consistent Word Embeddings
  title_html: Auto-Encoding Dictionary Definitions into Consistent Word Embeddings
  url: https://www.aclweb.org/anthology/D18-1181
  year: '2018'
D18-1182:
  abstract: We propose Odd-Man-Out, a novel task which aims to test different properties
    of word representations. An Odd-Man-Out puzzle is composed of 5 (or more) words,
    and requires the system to choose the one which does not belong with the others.
    We show that this simple setup is capable of teasing out various properties of
    different popular lexical resources (like WordNet and pre-trained word embeddings),
    while being intuitive enough to annotate on a large scale. In addition, we propose
    a novel technique for training multi-prototype word representations, based on
    unsupervised clustering of ELMo embeddings, and show that it surpasses all other
    representations on all Odd-Man-Out collections.
  address: Brussels, Belgium
  author:
  - first: Gabriel
    full: Gabriel Stanovsky
    id: gabriel-stanovsky
    last: Stanovsky
  - first: Mark
    full: Mark Hopkins
    id: mark-hopkins
    last: Hopkins
  author_string: Gabriel Stanovsky, Mark Hopkins
  bibkey: stanovsky-hopkins-2018-spot
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1182
  month: October-November
  page_first: '1533'
  page_last: '1542'
  pages: "1533\u20131542"
  paper_id: '182'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1182.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1182.jpg
  title: 'Spot the Odd Man Out: Exploring the Associative Power of Lexical Resources'
  title_html: 'Spot the Odd Man Out: Exploring the Associative Power of Lexical Resources'
  url: https://www.aclweb.org/anthology/D18-1182
  year: '2018'
D18-1183:
  abstract: 'Simile is a special type of metaphor, where comparators such as like
    and as are used to compare two objects. Simile recognition is to recognize simile
    sentences and extract simile components, i.e., the tenor and the vehicle. This
    paper presents a study of simile recognition in Chinese. We construct an annotated
    corpus for this research, which consists of 11.3k sentences that contain a comparator.
    We propose a neural network framework for jointly optimizing three tasks: simile
    sentence classification, simile component extraction and language modeling. The
    experimental results show that the neural network based approaches can outperform
    all rule-based and feature-based baselines. Both simile sentence classification
    and simile component extraction can benefit from multitask learning. The former
    can be solved very well, while the latter is more difficult.'
  address: Brussels, Belgium
  author:
  - first: Lizhen
    full: Lizhen Liu
    id: lizhen-liu
    last: Liu
  - first: Xiao
    full: Xiao Hu
    id: xiao-hu
    last: Hu
  - first: Wei
    full: Wei Song
    id: wei-song
    last: Song
  - first: Ruiji
    full: Ruiji Fu
    id: ruiji-fu
    last: Fu
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  - first: Guoping
    full: Guoping Hu
    id: guoping-hu
    last: Hu
  author_string: Lizhen Liu, Xiao Hu, Wei Song, Ruiji Fu, Ting Liu, Guoping Hu
  bibkey: liu-etal-2018-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1183
  month: October-November
  page_first: '1543'
  page_last: '1553'
  pages: "1543\u20131553"
  paper_id: '183'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1183.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1183.jpg
  title: Neural Multitask Learning for Simile Recognition
  title_html: Neural Multitask Learning for Simile Recognition
  url: https://www.aclweb.org/anthology/D18-1183
  year: '2018'
D18-1184:
  abstract: Many tasks in natural language processing involve comparing two sentences
    to compute some notion of relevance, entailment, or similarity. Typically this
    comparison is done either at the word level or at the sentence level, with no
    attempt to leverage the inherent structure of the sentence. When sentence structure
    is used for comparison, it is obtained during a non-differentiable pre-processing
    step, leading to propagation of errors. We introduce a model of structured alignments
    between sentences, showing how to compare two sentences by matching their latent
    structures. Using a structured attention mechanism, our model matches candidate
    spans in the first sentence to candidate spans in the second sentence, simultaneously
    discovering the tree structure of each sentence. Our model is fully differentiable
    and trained only on the matching objective. We evaluate this model on two tasks,
    natural entailment detection and answer sentence selection, and find that modeling
    latent tree structures results in superior performance. Analysis of the learned
    sentence structures shows they can reflect some syntactic phenomena.
  address: Brussels, Belgium
  author:
  - first: Yang
    full: Yang Liu
    id: yang-liu-edinburgh
    last: Liu
  - first: Matt
    full: Matt Gardner
    id: matt-gardner
    last: Gardner
  - first: Mirella
    full: Mirella Lapata
    id: mirella-lapata
    last: Lapata
  author_string: Yang Liu, Matt Gardner, Mirella Lapata
  bibkey: liu-etal-2018-structured
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1184
  month: October-November
  page_first: '1554'
  page_last: '1564'
  pages: "1554\u20131564"
  paper_id: '184'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1184.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1184.jpg
  title: Structured Alignment Networks for Matching Sentences
  title_html: Structured Alignment Networks for Matching Sentences
  url: https://www.aclweb.org/anthology/D18-1184
  year: '2018'
D18-1185:
  abstract: This paper presents a new deep learning architecture for Natural Language
    Inference (NLI). Firstly, we introduce a new architecture where alignment pairs
    are compared, compressed and then propagated to upper layers for enhanced representation
    learning. Secondly, we adopt factorization layers for efficient and expressive
    compression of alignment vectors into scalar features, which are then used to
    augment the base word representations. The design of our approach is aimed to
    be conceptually simple, compact and yet powerful. We conduct experiments on three
    popular benchmarks, SNLI, MultiNLI and SciTail, achieving competitive performance
    on all. A lightweight parameterization of our model also enjoys a 3 times reduction
    in parameter size compared to the existing state-of-the-art models, e.g., ESIM
    and DIIN, while maintaining competitive performance. Additionally, visual analysis
    shows that our propagated features are highly interpretable.
  address: Brussels, Belgium
  author:
  - first: Yi
    full: Yi Tay
    id: yi-tay
    last: Tay
  - first: Anh Tuan
    full: Anh Tuan Luu
    id: anh-tuan-luu
    last: Luu
  - first: Siu Cheung
    full: Siu Cheung Hui
    id: siu-cheung-hui
    last: Hui
  author_string: Yi Tay, Anh Tuan Luu, Siu Cheung Hui
  bibkey: tay-etal-2018-compare
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1185
  month: October-November
  page_first: '1565'
  page_last: '1575'
  pages: "1565\u20131575"
  paper_id: '185'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1185.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1185.jpg
  title: 'Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment
    Factorization for Natural Language Inference'
  title_html: 'Compare, Compress and Propagate: Enhancing Neural Architectures with
    Alignment Factorization for Natural Language Inference'
  url: https://www.aclweb.org/anthology/D18-1185
  year: '2018'
D18-1186:
  abstract: "Attention-based neural models have achieved great success in natural\
    \ language inference (NLI). In this paper, we propose the Convolutional Interaction\
    \ Network (CIN), a general model to capture the interaction between two sentences,\
    \ which can be an alternative to the attention mechanism for NLI. Specifically,\
    \ CIN encodes one sentence with the filters dynamically generated based on another\
    \ sentence. Since the filters may be designed to have various numbers and sizes,\
    \ CIN can capture more complicated interaction patterns. Experiments on three\
    \ large datasets demonstrate CIN\u2019s efficacy."
  address: Brussels, Belgium
  author:
  - first: Jingjing
    full: Jingjing Gong
    id: jingjing-gong
    last: Gong
  - first: Xipeng
    full: Xipeng Qiu
    id: xipeng-qiu
    last: Qiu
  - first: Xinchi
    full: Xinchi Chen
    id: xinchi-chen
    last: Chen
  - first: Dong
    full: Dong Liang
    id: dong-liang
    last: Liang
  - first: Xuanjing
    full: Xuanjing Huang
    id: xuan-jing-huang
    last: Huang
  author_string: Jingjing Gong, Xipeng Qiu, Xinchi Chen, Dong Liang, Xuanjing Huang
  bibkey: gong-etal-2018-convolutional
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1186
  month: October-November
  page_first: '1576'
  page_last: '1585'
  pages: "1576\u20131585"
  paper_id: '186'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1186.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1186.jpg
  title: Convolutional Interaction Network for Natural Language Inference
  title_html: Convolutional Interaction Network for Natural Language Inference
  url: https://www.aclweb.org/anthology/D18-1186
  year: '2018'
D18-1187:
  abstract: 'State of the art models using deep neural networks have become very good
    in learning an accurate mapping from inputs to outputs. However, they still lack
    generalization capabilities in conditions that differ from the ones encountered
    during training. This is even more challenging in specialized, and knowledge intensive
    domains, where training data is limited. To address this gap, we introduce MedNLI
    - a dataset annotated by doctors, performing a natural language inference task
    (NLI), grounded in the medical history of patients. We present strategies to:
    1) leverage transfer learning using datasets from the open domain, (e.g. SNLI)
    and 2) incorporate domain knowledge from external data and lexical sources (e.g.
    medical terminologies). Our results demonstrate performance gains using both strategies.'
  address: Brussels, Belgium
  author:
  - first: Alexey
    full: Alexey Romanov
    id: alexey-romanov
    last: Romanov
  - first: Chaitanya
    full: Chaitanya Shivade
    id: chaitanya-shivade
    last: Shivade
  author_string: Alexey Romanov, Chaitanya Shivade
  bibkey: romanov-shivade-2018-lessons
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1187
  month: October-November
  page_first: '1586'
  page_last: '1596'
  pages: "1586\u20131596"
  paper_id: '187'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1187.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1187.jpg
  title: Lessons from Natural Language Inference in the Clinical Domain
  title_html: Lessons from Natural Language Inference in the Clinical Domain
  url: https://www.aclweb.org/anthology/D18-1187
  year: '2018'
D18-1188:
  abstract: In this paper, we study how to learn a semantic parser of state-of-the-art
    accuracy with less supervised training data. We conduct our study on WikiSQL,
    the largest hand-annotated semantic parsing dataset to date. First, we demonstrate
    that question generation is an effective method that empowers us to learn a state-of-the-art
    neural network based semantic parser with thirty percent of the supervised training
    data. Second, we show that applying question generation to the full supervised
    training data further improves the state-of-the-art model. In addition, we observe
    that there is a logarithmic relationship between the accuracy of a semantic parser
    and the amount of training data.
  address: Brussels, Belgium
  author:
  - first: Daya
    full: Daya Guo
    id: daya-guo
    last: Guo
  - first: Yibo
    full: Yibo Sun
    id: yibo-sun
    last: Sun
  - first: Duyu
    full: Duyu Tang
    id: duyu-tang
    last: Tang
  - first: Nan
    full: Nan Duan
    id: nan-duan
    last: Duan
  - first: Jian
    full: Jian Yin
    id: jian-yin
    last: Yin
  - first: Hong
    full: Hong Chi
    id: hong-chi
    last: Chi
  - first: James
    full: James Cao
    id: james-cao
    last: Cao
  - first: Peng
    full: Peng Chen
    id: peng-chen
    last: Chen
  - first: Ming
    full: Ming Zhou
    id: ming-zhou
    last: Zhou
  author_string: Daya Guo, Yibo Sun, Duyu Tang, Nan Duan, Jian Yin, Hong Chi, James
    Cao, Peng Chen, Ming Zhou
  bibkey: guo-etal-2018-question
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1188
  month: October-November
  page_first: '1597'
  page_last: '1607'
  pages: "1597\u20131607"
  paper_id: '188'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1188.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1188.jpg
  title: Question Generation from SQL Queries Improves Neural Semantic Parsing
  title_html: Question Generation from <span class="acl-fixed-case">SQL</span> Queries
    Improves Neural Semantic Parsing
  url: https://www.aclweb.org/anthology/D18-1188
  year: '2018'
D18-1189:
  abstract: 'Recent research proposes syntax-based approaches to address the problem
    of generating programs from natural language specifications. These approaches
    typically train a sequence-to-sequence learning model using a syntax-based objective:
    maximum likelihood estimation (MLE). Such syntax-based approaches do not effectively
    address the goal of generating semantically correct programs, because these approaches
    fail to handle Program Aliasing, i.e., semantically equivalent programs may have
    many syntactically different forms. To address this issue, in this paper, we propose
    a semantics-based approach named SemRegex. SemRegex provides solutions for a subtask
    of the program-synthesis problem: generating regular expressions from natural
    language. Different from the existing syntax-based approaches, SemRegex trains
    the model by maximizing the expected semantic correctness of the generated regular
    expressions. The semantic correctness is measured using the DFA-equivalence oracle,
    random test cases, and distinguishing test cases. The experiments on three public
    datasets demonstrate the superiority of SemRegex over the existing state-of-the-art
    approaches.'
  address: Brussels, Belgium
  author:
  - first: Zexuan
    full: Zexuan Zhong
    id: zexuan-zhong
    last: Zhong
  - first: Jiaqi
    full: Jiaqi Guo
    id: jiaqi-guo
    last: Guo
  - first: Wei
    full: Wei Yang
    id: wei-yang
    last: Yang
  - first: Jian
    full: Jian Peng
    id: jian-peng
    last: Peng
  - first: Tao
    full: Tao Xie
    id: tao-xie
    last: Xie
  - first: Jian-Guang
    full: Jian-Guang Lou
    id: jian-guang-lou
    last: Lou
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  - first: Dongmei
    full: Dongmei Zhang
    id: dongmei-zhang
    last: Zhang
  author_string: Zexuan Zhong, Jiaqi Guo, Wei Yang, Jian Peng, Tao Xie, Jian-Guang
    Lou, Ting Liu, Dongmei Zhang
  bibkey: zhong-etal-2018-semregex
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1189
  month: October-November
  page_first: '1608'
  page_last: '1618'
  pages: "1608\u20131618"
  paper_id: '189'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1189.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1189.jpg
  title: 'SemRegex: A Semantics-Based Approach for Generating Regular Expressions
    from Natural Language Specifications'
  title_html: '<span class="acl-fixed-case">S</span>em<span class="acl-fixed-case">R</span>egex:
    A Semantics-Based Approach for Generating Regular Expressions from Natural Language
    Specifications'
  url: https://www.aclweb.org/anthology/D18-1189
  year: '2018'
D18-1190:
  abstract: Building a semantic parser quickly in a new domain is a fundamental challenge
    for conversational interfaces, as current semantic parsers require expensive supervision
    and lack the ability to generalize to new domains. In this paper, we introduce
    a zero-shot approach to semantic parsing that can parse utterances in unseen domains
    while only being trained on examples in other source domains. First, we map an
    utterance to an abstract, domain independent, logical form that represents the
    structure of the logical form, but contains slots instead of KB constants. Then,
    we replace slots with KB constants via lexical alignment scores and global inference.
    Our model reaches an average accuracy of 53.4% on 7 domains in the OVERNIGHT dataset,
    substantially better than other zero-shot baselines, and performs as good as a
    parser trained on over 30% of the target domain examples.
  address: Brussels, Belgium
  author:
  - first: Jonathan
    full: Jonathan Herzig
    id: jonathan-herzig
    last: Herzig
  - first: Jonathan
    full: Jonathan Berant
    id: jonathan-berant
    last: Berant
  author_string: Jonathan Herzig, Jonathan Berant
  bibkey: herzig-berant-2018-decoupling
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1190
  month: October-November
  page_first: '1619'
  page_last: '1629'
  pages: "1619\u20131629"
  paper_id: '190'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1190.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1190.jpg
  title: Decoupling Structure and Lexicon for Zero-Shot Semantic Parsing
  title_html: Decoupling Structure and Lexicon for Zero-Shot Semantic Parsing
  url: https://www.aclweb.org/anthology/D18-1190
  year: '2018'
D18-1191:
  abstract: We present a simple and accurate span-based model for semantic role labeling
    (SRL). Our model directly takes into account all possible argument spans and scores
    them for each label. At decoding time, we greedily select higher scoring labeled
    spans. One advantage of our model is to allow us to design and use span-level
    features, that are difficult to use in token-based BIO tagging approaches. Experimental
    results demonstrate that our ensemble model achieves the state-of-the-art results,
    87.4 F1 and 87.0 F1 on the CoNLL-2005 and 2012 datasets, respectively.
  address: Brussels, Belgium
  author:
  - first: Hiroki
    full: Hiroki Ouchi
    id: hiroki-ouchi
    last: Ouchi
  - first: Hiroyuki
    full: Hiroyuki Shindo
    id: hiroyuki-shindo
    last: Shindo
  - first: Yuji
    full: Yuji Matsumoto
    id: yuji-matsumoto
    last: Matsumoto
  author_string: Hiroki Ouchi, Hiroyuki Shindo, Yuji Matsumoto
  bibkey: ouchi-etal-2018-span
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1191
  month: October-November
  page_first: '1630'
  page_last: '1642'
  pages: "1630\u20131642"
  paper_id: '191'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1191.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1191.jpg
  title: A Span Selection Model for Semantic Role Labeling
  title_html: A Span Selection Model for Semantic Role Labeling
  url: https://www.aclweb.org/anthology/D18-1191
  year: '2018'
D18-1192:
  abstract: "Source code is rarely written in isolation. It depends significantly\
    \ on the programmatic context, such as the class that the code would reside in.\
    \ To study this phenomenon, we introduce the task of generating class member functions\
    \ given English documentation and the programmatic context provided by the rest\
    \ of the class. This task is challenging because the desired code can vary greatly\
    \ depending on the functionality the class provides (e.g., a sort function may\
    \ or may not be available when we are asked to \u201Creturn the smallest element\u201D\
    \ in a particular member variable list). We introduce CONCODE, a new large dataset\
    \ with over 100,000 examples consisting of Java classes from online code repositories,\
    \ and develop a new encoder-decoder architecture that models the interaction between\
    \ the method documentation and the class environment. We also present a detailed\
    \ error analysis suggesting that there is significant room for future work on\
    \ this task."
  address: Brussels, Belgium
  author:
  - first: Srinivasan
    full: Srinivasan Iyer
    id: srinivasan-iyer
    last: Iyer
  - first: Ioannis
    full: Ioannis Konstas
    id: ioannis-konstas
    last: Konstas
  - first: Alvin
    full: Alvin Cheung
    id: alvin-cheung
    last: Cheung
  - first: Luke
    full: Luke Zettlemoyer
    id: luke-zettlemoyer
    last: Zettlemoyer
  author_string: Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Luke Zettlemoyer
  bibkey: iyer-etal-2018-mapping
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1192
  month: October-November
  page_first: '1643'
  page_last: '1652'
  pages: "1643\u20131652"
  paper_id: '192'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1192.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1192.jpg
  title: Mapping Language to Code in Programmatic Context
  title_html: Mapping Language to Code in Programmatic Context
  url: https://www.aclweb.org/anthology/D18-1192
  year: '2018'
D18-1193:
  abstract: Most existing studies in text-to-SQL tasks do not require generating complex
    SQL queries with multiple clauses or sub-queries, and generalizing to new, unseen
    databases. In this paper we propose SyntaxSQLNet, a syntax tree network to address
    the complex and cross-domain text-to-SQL generation task. SyntaxSQLNet employs
    a SQL specific syntax tree-based decoder with SQL generation path history and
    table-aware column attention encoders. We evaluate SyntaxSQLNet on a new large-scale
    text-to-SQL corpus containing databases with multiple tables and complex SQL queries
    containing multiple SQL clauses and nested queries. We use a database split setting
    where databases in the test set are unseen during training. Experimental results
    show that SyntaxSQLNet can handle a significantly greater number of complex SQL
    examples than prior work, outperforming the previous state-of-the-art model by
    9.5% in exact matching accuracy. To our knowledge, we are the first to study this
    complex text-to-SQL task. Our task and models with the latest updates are available
    at https://yale-lily.github.io/seq2sql/spider.
  address: Brussels, Belgium
  author:
  - first: Tao
    full: Tao Yu
    id: tao-yu
    last: Yu
  - first: Michihiro
    full: Michihiro Yasunaga
    id: michihiro-yasunaga
    last: Yasunaga
  - first: Kai
    full: Kai Yang
    id: kai-yang
    last: Yang
  - first: Rui
    full: Rui Zhang
    id: rui-zhang
    last: Zhang
  - first: Dongxu
    full: Dongxu Wang
    id: dongxu-wang
    last: Wang
  - first: Zifan
    full: Zifan Li
    id: zifan-li
    last: Li
  - first: Dragomir
    full: Dragomir Radev
    id: dragomir-radev
    last: Radev
  author_string: Tao Yu, Michihiro Yasunaga, Kai Yang, Rui Zhang, Dongxu Wang, Zifan
    Li, Dragomir Radev
  bibkey: yu-etal-2018-syntaxsqlnet
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1193
  month: October-November
  page_first: '1653'
  page_last: '1663'
  pages: "1653\u20131663"
  paper_id: '193'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1193.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1193.jpg
  title: 'SyntaxSQLNet: Syntax Tree Networks for Complex and Cross-Domain Text-to-SQL
    Task'
  title_html: '<span class="acl-fixed-case">S</span>yntax<span class="acl-fixed-case">SQLN</span>et:
    Syntax Tree Networks for Complex and Cross-Domain Text-to-<span class="acl-fixed-case">SQL</span>
    Task'
  url: https://www.aclweb.org/anthology/D18-1193
  year: '2018'
D18-1194:
  abstract: 'We introduce the task of cross-lingual decompositional semantic parsing:
    mapping content provided in a source language into a decompositional semantic
    analysis based on a target language. We present: (1) a form of decompositional
    semantic analysis designed to allow systems to target varying levels of structural
    complexity (shallow to deep analysis), (2) an evaluation metric to measure the
    similarity between system output and reference semantic analysis, (3) an end-to-end
    model with a novel annotating mechanism that supports intra-sentential coreference,
    and (4) an evaluation dataset on which our model outperforms strong baselines
    by at least 1.75 F1 score.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1194.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1194.Attachment.pdf
  author:
  - first: Sheng
    full: Sheng Zhang
    id: sheng-zhang
    last: Zhang
  - first: Xutai
    full: Xutai Ma
    id: xutai-ma
    last: Ma
  - first: Rachel
    full: Rachel Rudinger
    id: rachel-rudinger
    last: Rudinger
  - first: Kevin
    full: Kevin Duh
    id: kevin-duh
    last: Duh
  - first: Benjamin
    full: Benjamin Van Durme
    id: benjamin-van-durme
    last: Van Durme
  author_string: Sheng Zhang, Xutai Ma, Rachel Rudinger, Kevin Duh, Benjamin Van Durme
  bibkey: zhang-etal-2018-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1194
  month: October-November
  page_first: '1664'
  page_last: '1675'
  pages: "1664\u20131675"
  paper_id: '194'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1194.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1194.jpg
  title: Cross-lingual Decompositional Semantic Parsing
  title_html: Cross-lingual Decompositional Semantic Parsing
  url: https://www.aclweb.org/anthology/D18-1194
  year: '2018'
D18-1195:
  abstract: "As humans, we often rely on language to learn language. For example,\
    \ when corrected in a conversation, we may learn from that correction, over time\
    \ improving our language fluency. Inspired by this observation, we propose a learning\
    \ algorithm for training semantic parsers from supervision (feedback) expressed\
    \ in natural language. Our algorithm learns a semantic parser from users\u2019\
    \ corrections such as \u201Cno, what I really meant was before his job, not after\u201D\
    , by also simultaneously learning to parse this natural language feedback in order\
    \ to leverage it as a form of supervision. Unlike supervision with gold-standard\
    \ logical forms, our method does not require the user to be familiar with the\
    \ underlying logical formalism, and unlike supervision from denotation, it does\
    \ not require the user to know the correct answer to their query. This makes our\
    \ learning algorithm naturally scalable in settings where existing conversational\
    \ logs are available and can be leveraged as training data. We construct a novel\
    \ dataset of natural language feedback in a conversational setting, and show that\
    \ our method is effective at learning a semantic parser from such natural language\
    \ supervision."
  address: Brussels, Belgium
  author:
  - first: Igor
    full: Igor Labutov
    id: igor-labutov
    last: Labutov
  - first: Bishan
    full: Bishan Yang
    id: bishan-yang
    last: Yang
  - first: Tom
    full: Tom Mitchell
    id: tom-mitchell
    last: Mitchell
  author_string: Igor Labutov, Bishan Yang, Tom Mitchell
  bibkey: labutov-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1195
  month: October-November
  page_first: '1676'
  page_last: '1690'
  pages: "1676\u20131690"
  paper_id: '195'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1195.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1195.jpg
  title: Learning to Learn Semantic Parsers from Natural Language Supervision
  title_html: Learning to Learn Semantic Parsers from Natural Language Supervision
  url: https://www.aclweb.org/anthology/D18-1195
  year: '2018'
D18-1196:
  abstract: This paper introduces the surface construction labeling (SCL) task, which
    expands the coverage of Shallow Semantic Parsing (SSP) to include frames triggered
    by complex constructions. We present DeepCx, a neural, transition-based system
    for SCL. As a test case for the approach, we apply DeepCx to the task of tagging
    causal language in English, which relies on a wider variety of constructions than
    are typically addressed in SSP. We report substantial improvements over previous
    tagging efforts on a causal language dataset. We also propose ways DeepCx could
    be extended to still more difficult constructions and to other semantic domains
    once appropriate datasets become available.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1196.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1196.Attachment.zip
  author:
  - first: Jesse
    full: Jesse Dunietz
    id: jesse-dunietz
    last: Dunietz
  - first: Jaime
    full: Jaime Carbonell
    id: jaime-g-carbonell
    last: Carbonell
  - first: Lori
    full: Lori Levin
    id: lori-levin
    last: Levin
  author_string: Jesse Dunietz, Jaime Carbonell, Lori Levin
  bibkey: dunietz-etal-2018-deepcx
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1196
  month: October-November
  page_first: '1691'
  page_last: '1701'
  pages: "1691\u20131701"
  paper_id: '196'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1196.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1196.jpg
  title: 'DeepCx: A transition-based approach for shallow semantic parsing with complex
    constructional triggers'
  title_html: '<span class="acl-fixed-case">D</span>eep<span class="acl-fixed-case">C</span>x:
    A transition-based approach for shallow semantic parsing with complex constructional
    triggers'
  url: https://www.aclweb.org/anthology/D18-1196
  year: '2018'
D18-1197:
  abstract: "WikiSQL is a newly released dataset for studying the natural language\
    \ sequence to SQL translation problem. The SQL queries in WikiSQL are simple:\
    \ Each involves one relation and does not have any join operation. Despite of\
    \ its simplicity, none of the publicly reported structured query generation models\
    \ can achieve an accuracy beyond 62%, which is still far from enough for practical\
    \ use. In this paper, we ask two questions, \u201CWhy is the accuracy still low\
    \ for such simple queries?\u201D and \u201CWhat does it take to achieve 100% accuracy\
    \ on WikiSQL?\u201D To limit the scope of our study, we focus on the WHERE clause\
    \ in SQL. The answers will help us gain insights about the directions we should\
    \ explore in order to further improve the translation accuracy. We will then investigate\
    \ alternative solutions to realize the potential ceiling performance on WikiSQL.\
    \ Our proposed solution can reach up to 88.6% condition accuracy on the WikiSQL\
    \ dataset."
  address: Brussels, Belgium
  author:
  - first: Semih
    full: Semih Yavuz
    id: semih-yavuz
    last: Yavuz
  - first: Izzeddin
    full: Izzeddin Gur
    id: izzeddin-gur1
    last: Gur
  - first: Yu
    full: Yu Su
    id: yu-su
    last: Su
  - first: Xifeng
    full: Xifeng Yan
    id: xifeng-yan
    last: Yan
  author_string: Semih Yavuz, Izzeddin Gur, Yu Su, Xifeng Yan
  bibkey: yavuz-etal-2018-takes
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1197
  month: October-November
  page_first: '1702'
  page_last: '1711'
  pages: "1702\u20131711"
  paper_id: '197'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1197.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1197.jpg
  title: What It Takes to Achieve 100% Condition Accuracy on WikiSQL
  title_html: What It Takes to Achieve 100% Condition Accuracy on <span class="acl-fixed-case">W</span>iki<span
    class="acl-fixed-case">SQL</span>
  url: https://www.aclweb.org/anthology/D18-1197
  year: '2018'
D18-1198:
  abstract: This paper introduces a simple yet effective transition-based system for
    Abstract Meaning Representation (AMR) parsing. We argue that a well-defined search
    space involved in a transition system is crucial for building an effective parser.
    We propose to conduct the search in a refined search space based on a new compact
    AMR graph and an improved oracle. Our end-to-end parser achieves the state-of-the-art
    performance on various datasets with minimal additional information.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1198.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1198.Attachment.zip
  author:
  - first: Zhijiang
    full: Zhijiang Guo
    id: zhijiang-guo
    last: Guo
  - first: Wei
    full: Wei Lu
    id: wei-lu
    last: Lu
  author_string: Zhijiang Guo, Wei Lu
  bibkey: guo-lu-2018-better
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1198
  month: October-November
  page_first: '1712'
  page_last: '1722'
  pages: "1712\u20131722"
  paper_id: '198'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1198.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1198.jpg
  title: Better Transition-Based AMR Parsing with a Refined Search Space
  title_html: Better Transition-Based <span class="acl-fixed-case">AMR</span> Parsing
    with a Refined Search Space
  url: https://www.aclweb.org/anthology/D18-1198
  year: '2018'
D18-1199:
  abstract: Many idiomatic expressions can be interpreted figuratively or literally
    depending on their contexts. This paper proposes an unsupervised learning method
    for recognizing the intended usages of idioms. We treat the usages as a latent
    variable in probabilistic models and train them in a linguistically motivated
    feature space. Crucially, we show that distributional semantics is a helpful heuristic
    for distinguishing the literal usage of idioms, giving us a way to formulate a
    literal usage metric to estimate the likelihood that the idiom is intended literally.
    This information then serves as a form of distant supervision to guide the unsupervised
    training process for the probabilistic models. Experiments show that our overall
    model performs competitively against supervised methods.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305931117
    type: video
    url: https://vimeo.com/305931117
  author:
  - first: Changsheng
    full: Changsheng Liu
    id: changsheng-liu
    last: Liu
  - first: Rebecca
    full: Rebecca Hwa
    id: rebecca-hwa
    last: Hwa
  author_string: Changsheng Liu, Rebecca Hwa
  bibkey: liu-hwa-2018-heuristically
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1199
  month: October-November
  page_first: '1723'
  page_last: '1731'
  pages: "1723\u20131731"
  paper_id: '199'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1199.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1199.jpg
  title: Heuristically Informed Unsupervised Idiom Usage Recognition
  title_html: Heuristically Informed Unsupervised Idiom Usage Recognition
  url: https://www.aclweb.org/anthology/D18-1199
  year: '2018'
D18-1200:
  abstract: The point of departure of this article is the claim that sense-specific
    vectors provide an advantage over normal vectors due to the polysemy that they
    presumably represent. This claim is based on performance gains observed in gold
    standard evaluation tests such as word similarity tasks. We demonstrate that this
    claim, at least as it is instantiated in prior art, is unfounded in two ways.
    Furthermore, we provide empirical data and an analytic discussion that may account
    for the previously reported improved performance. First, we show that ground-truth
    polysemy degrades performance in word similarity tasks. Therefore word similarity
    tasks are not suitable as an evaluation test for polysemy representation. Second,
    random assignment of words to senses is shown to improve performance in the same
    task. This and additional results point to the conclusion that performance gains
    as reported in previous work may be an artifact of random sense assignment, which
    is equivalent to sub-sampling and multiple estimation of word vector representations.
    Theoretical analysis shows that this may on its own be beneficial for the estimation
    of word similarity, by reducing the bias in the estimation of the cosine distance.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305931948
    type: video
    url: https://vimeo.com/305931948
  author:
  - first: Haim
    full: Haim Dubossarsky
    id: haim-dubossarsky
    last: Dubossarsky
  - first: Eitan
    full: Eitan Grossman
    id: eitan-grossman
    last: Grossman
  - first: Daphna
    full: Daphna Weinshall
    id: daphna-weinshall
    last: Weinshall
  author_string: Haim Dubossarsky, Eitan Grossman, Daphna Weinshall
  bibkey: dubossarsky-etal-2018-coming
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1200
  month: October-November
  page_first: '1732'
  page_last: '1740'
  pages: "1732\u20131740"
  paper_id: '200'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1200.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1200.jpg
  title: 'Coming to Your Senses: on Controls and Evaluation Sets in Polysemy Research'
  title_html: 'Coming to Your Senses: on Controls and Evaluation Sets in Polysemy
    Research'
  url: https://www.aclweb.org/anthology/D18-1200
  year: '2018'
D18-1201:
  abstract: "Semantic graphs, such as WordNet, are resources which curate natural\
    \ language on two distinguishable layers. On the local level, individual relations\
    \ between synsets (semantic building blocks) such as hypernymy and meronymy enhance\
    \ our understanding of the words used to express their meanings. Globally, analysis\
    \ of graph-theoretic properties of the entire net sheds light on the structure\
    \ of human language as a whole. In this paper, we combine global and local properties\
    \ of semantic graphs through the framework of Max-Margin Markov Graph Models (M3GM),\
    \ a novel extension of Exponential Random Graph Model (ERGM) that scales to large\
    \ multi-relational graphs. We demonstrate how such global modeling improves performance\
    \ on the local task of predicting semantic relations between synsets, yielding\
    \ new state-of-the-art results on the WN18RR dataset, a challenging version of\
    \ WordNet link prediction in which \u201Ceasy\u201D reciprocal cases are removed.\
    \ In addition, the M3GM model identifies multirelational motifs that are characteristic\
    \ of well-formed lexical semantic ontologies."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305933339
    type: video
    url: https://vimeo.com/305933339
  - filename: D18-1201.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/D18-1201.Presentation.pdf
  author:
  - first: Yuval
    full: Yuval Pinter
    id: yuval-pinter
    last: Pinter
  - first: Jacob
    full: Jacob Eisenstein
    id: jacob-eisenstein
    last: Eisenstein
  author_string: Yuval Pinter, Jacob Eisenstein
  bibkey: pinter-eisenstein-2018-predicting
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1201
  month: October-November
  page_first: '1741'
  page_last: '1751'
  pages: "1741\u20131751"
  paper_id: '201'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1201.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1201.jpg
  title: Predicting Semantic Relations using Global Graph Properties
  title_html: Predicting Semantic Relations using Global Graph Properties
  url: https://www.aclweb.org/anthology/D18-1201
  year: '2018'
D18-1202:
  abstract: "Adjectives like \u201Cwarm\u201D, \u201Chot\u201D, and \u201Cscalding\u201D\
    \ all describe temperature but differ in intensity. Understanding these differences\
    \ between adjectives is a necessary part of reasoning about natural language.\
    \ We propose a new paraphrase-based method to automatically learn the relative\
    \ intensity relation that holds between a pair of scalar adjectives. Our approach\
    \ analyzes over 36k adjectival pairs from the Paraphrase Database under the assumption\
    \ that, for example, paraphrase pair \u201Creally hot\u201D <\u2013> \u201Cscalding\u201D\
    \ suggests that \u201Chot\u201D < \u201Cscalding\u201D. We show that combining\
    \ this paraphrase evidence with existing, complementary pattern- and lexicon-based\
    \ approaches improves the quality of systems for automatically ordering sets of\
    \ scalar adjectives and inferring the polarity of indirect answers to \u201Cyes/no\u201D\
    \ questions."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1202.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1202.Attachment.pdf
  - filename: https://vimeo.com/305934622
    type: video
    url: https://vimeo.com/305934622
  author:
  - first: Anne
    full: Anne Cocos
    id: anne-cocos
    last: Cocos
  - first: Skyler
    full: Skyler Wharton
    id: skyler-wharton
    last: Wharton
  - first: Ellie
    full: Ellie Pavlick
    id: ellie-pavlick
    last: Pavlick
  - first: Marianna
    full: Marianna Apidianaki
    id: marianna-apidianaki
    last: Apidianaki
  - first: Chris
    full: Chris Callison-Burch
    id: chris-callison-burch
    last: Callison-Burch
  author_string: Anne Cocos, Skyler Wharton, Ellie Pavlick, Marianna Apidianaki, Chris
    Callison-Burch
  bibkey: cocos-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1202
  month: October-November
  page_first: '1752'
  page_last: '1762'
  pages: "1752\u20131762"
  paper_id: '202'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1202.pdf
  publisher: Association for Computational Linguistics
  revision:
  - id: '1'
    url: https://www.aclweb.org/anthology/D18-1202v1.pdf
    value: D18-1202v1
  - explanation: Changed first name of one of the authors.
    id: '2'
    url: https://www.aclweb.org/anthology/D18-1202v2.pdf
    value: D18-1202v2
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1202.jpg
  title: Learning Scalar Adjective Intensity from Paraphrases
  title_html: Learning Scalar Adjective Intensity from Paraphrases
  url: https://www.aclweb.org/anthology/D18-1202
  year: '2018'
D18-1203:
  abstract: "In this paper, we propose a new kernel-based co-occurrence measure that\
    \ can be applied to sparse linguistic expressions (e.g., sentences) with a very\
    \ short learning time, as an alternative to pointwise mutual information (PMI).\
    \ As well as deriving PMI from mutual information, we derive this new measure\
    \ from the Hilbert\u2013Schmidt independence criterion (HSIC); thus, we call the\
    \ new measure the pointwise HSIC (PHSIC). PHSIC can be interpreted as a smoothed\
    \ variant of PMI that allows various similarity metrics (e.g., sentence embeddings)\
    \ to be plugged in as kernels. Moreover, PHSIC can be estimated by simple and\
    \ fast (linear in the size of the data) matrix calculations regardless of whether\
    \ we use linear or nonlinear kernels. Empirically, in a dialogue response selection\
    \ task, PHSIC is learned thousands of times faster than an RNN-based PMI while\
    \ outperforming PMI in accuracy. In addition, we also demonstrate that PHSIC is\
    \ beneficial as a criterion of a data selection task for machine translation owing\
    \ to its ability to give high (low) scores to a consistent (inconsistent) pair\
    \ with other pairs."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305935544
    type: video
    url: https://vimeo.com/305935544
  author:
  - first: Sho
    full: Sho Yokoi
    id: sho-yokoi
    last: Yokoi
  - first: Sosuke
    full: Sosuke Kobayashi
    id: sosuke-kobayashi
    last: Kobayashi
  - first: Kenji
    full: Kenji Fukumizu
    id: kenji-fukumizu
    last: Fukumizu
  - first: Jun
    full: Jun Suzuki
    id: jun-suzuki
    last: Suzuki
  - first: Kentaro
    full: Kentaro Inui
    id: kentaro-inui
    last: Inui
  author_string: Sho Yokoi, Sosuke Kobayashi, Kenji Fukumizu, Jun Suzuki, Kentaro
    Inui
  bibkey: yokoi-etal-2018-pointwise
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1203
  month: October-November
  page_first: '1763'
  page_last: '1775'
  pages: "1763\u20131775"
  paper_id: '203'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1203.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1203.jpg
  title: 'Pointwise HSIC: A Linear-Time Kernelized Co-occurrence Norm for Sparse Linguistic
    Expressions'
  title_html: 'Pointwise <span class="acl-fixed-case">HSIC</span>: A Linear-Time Kernelized
    Co-occurrence Norm for Sparse Linguistic Expressions'
  url: https://www.aclweb.org/anthology/D18-1203
  year: '2018'
D18-1204:
  abstract: Conventional solutions to automatic related work summarization rely heavily
    on human-engineered features. In this paper, we develop a neural data-driven summarizer
    by leveraging the seq2seq paradigm, in which a joint context-driven attention
    mechanism is proposed to measure the contextual relevance within full texts and
    a heterogeneous bibliography graph simultaneously. Our motivation is to maintain
    the topic coherency between a related work section and its target document, where
    both the textual and graphic contexts play a big role in characterizing the relationship
    among scientific publications accurately. Experimental results on a large dataset
    show that our approach achieves a considerable improvement over a typical seq2seq
    summarizer and five classical summarization baselines.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305686976
    type: video
    url: https://vimeo.com/305686976
  author:
  - first: Yongzhen
    full: Yongzhen Wang
    id: yongzhen-wang
    last: Wang
  - first: Xiaozhong
    full: Xiaozhong Liu
    id: xiaozhong-liu
    last: Liu
  - first: Zheng
    full: Zheng Gao
    id: zheng-gao
    last: Gao
  author_string: Yongzhen Wang, Xiaozhong Liu, Zheng Gao
  bibkey: wang-etal-2018-neural-related
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1204
  month: October-November
  page_first: '1776'
  page_last: '1786'
  pages: "1776\u20131786"
  paper_id: '204'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1204.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1204.jpg
  title: Neural Related Work Summarization with a Joint Context-driven Attention Mechanism
  title_html: Neural Related Work Summarization with a Joint Context-driven Attention
    Mechanism
  url: https://www.aclweb.org/anthology/D18-1204
  year: '2018'
D18-1205:
  abstract: 'Information selection is the most important component in document summarization
    task. In this paper, we propose to extend the basic neural encoding-decoding framework
    with an information selection layer to explicitly model and optimize the information
    selection process in abstractive document summarization. Specifically, our information
    selection layer consists of two parts: gated global information filtering and
    local sentence selection. Unnecessary information in the original document is
    first globally filtered, then salient sentences are selected locally while generating
    each summary sentence sequentially. To optimize the information selection process
    directly, distantly-supervised training guided by the golden summary is also imported.
    Experimental results demonstrate that the explicit modeling and optimizing of
    the information selection process improves document summarization performance
    significantly, which enables our model to generate more informative and concise
    summaries, and thus significantly outperform state-of-the-art neural abstractive
    methods.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1205.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1205.Attachment.pdf
  - filename: https://vimeo.com/305885506
    type: video
    url: https://vimeo.com/305885506
  author:
  - first: Wei
    full: Wei Li
    id: wei-li
    last: Li
  - first: Xinyan
    full: Xinyan Xiao
    id: xinyan-xiao
    last: Xiao
  - first: Yajuan
    full: Yajuan Lyu
    id: yajuan-lyu
    last: Lyu
  - first: Yuanzhuo
    full: Yuanzhuo Wang
    id: yuanzhuo-wang
    last: Wang
  author_string: Wei Li, Xinyan Xiao, Yajuan Lyu, Yuanzhuo Wang
  bibkey: li-etal-2018-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1205
  month: October-November
  page_first: '1787'
  page_last: '1796'
  pages: "1787\u20131796"
  paper_id: '205'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1205.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1205.jpg
  title: Improving Neural Abstractive Document Summarization with Explicit Information
    Selection Modeling
  title_html: Improving Neural Abstractive Document Summarization with Explicit Information
    Selection Modeling
  url: https://www.aclweb.org/anthology/D18-1205
  year: '2018'
D18-1206:
  abstract: "We introduce \u201Cextreme summarization\u201D, a new single-document\
    \ summarization task which does not favor extractive strategies and calls for\
    \ an abstractive modeling approach. The idea is to create a short, one-sentence\
    \ news summary answering the question \u201CWhat is the article about?\u201D.\
    \ We collect a real-world, large-scale dataset for this task by harvesting online\
    \ articles from the British Broadcasting Corporation (BBC). We propose a novel\
    \ abstractive model which is conditioned on the article\u2019s topics and based\
    \ entirely on convolutional neural networks. We demonstrate experimentally that\
    \ this architecture captures long-range dependencies in a document and recognizes\
    \ pertinent content, outperforming an oracle extractive system and state-of-the-art\
    \ abstractive approaches when evaluated automatically and by humans."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305885893
    type: video
    url: https://vimeo.com/305885893
  author:
  - first: Shashi
    full: Shashi Narayan
    id: shashi-narayan
    last: Narayan
  - first: Shay B.
    full: Shay B. Cohen
    id: shay-b-cohen
    last: Cohen
  - first: Mirella
    full: Mirella Lapata
    id: mirella-lapata
    last: Lapata
  author_string: Shashi Narayan, Shay B. Cohen, Mirella Lapata
  bibkey: narayan-etal-2018-dont
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1206
  month: October-November
  page_first: '1797'
  page_last: '1807'
  pages: "1797\u20131807"
  paper_id: '206'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1206.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1206.jpg
  title: "Don\u2019t Give Me the Details, Just the Summary! Topic-Aware Convolutional\
    \ Neural Networks for Extreme Summarization"
  title_html: "Don\u2019t Give Me the Details, Just the Summary! Topic-Aware Convolutional\
    \ Neural Networks for Extreme Summarization"
  url: https://www.aclweb.org/anthology/D18-1206
  year: '2018'
D18-1207:
  abstract: Abstractive text summarization aims to shorten long text documents into
    a human readable form that contains the most important facts from the original
    document. However, the level of actual abstraction as measured by novel phrases
    that do not appear in the source document remains low in existing approaches.
    We propose two techniques to improve the level of abstraction of generated summaries.
    First, we decompose the decoder into a contextual network that retrieves relevant
    parts of the source document, and a pretrained language model that incorporates
    prior knowledge about language generation. Second, we propose a novelty metric
    that is optimized directly through policy learning to encourage the generation
    of novel phrases. Our model achieves results comparable to state-of-the-art models,
    as determined by ROUGE scores and human evaluations, while achieving a significantly
    higher level of abstraction as measured by n-gram overlap with the source document.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305886179
    type: video
    url: https://vimeo.com/305886179
  author:
  - first: Wojciech
    full: "Wojciech Kry\u015Bci\u0144ski"
    id: wojciech-kryscinski1
    last: "Kry\u015Bci\u0144ski"
  - first: Romain
    full: Romain Paulus
    id: romain-paulus
    last: Paulus
  - first: Caiming
    full: Caiming Xiong
    id: caiming-xiong
    last: Xiong
  - first: Richard
    full: Richard Socher
    id: richard-socher
    last: Socher
  author_string: "Wojciech Kry\u015Bci\u0144ski, Romain Paulus, Caiming Xiong, Richard\
    \ Socher"
  bibkey: kryscinski-etal-2018-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1207
  month: October-November
  page_first: '1808'
  page_last: '1817'
  pages: "1808\u20131817"
  paper_id: '207'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1207.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1207.jpg
  title: Improving Abstraction in Text Summarization
  title_html: Improving Abstraction in Text Summarization
  url: https://www.aclweb.org/anthology/D18-1207
  year: '2018'
D18-1208:
  abstract: We carry out experiments with deep learning models of summarization across
    the domains of news, personal stories, meetings, and medical articles in order
    to understand how content selection is performed. We find that many sophisticated
    features of state of the art extractive summarizers do not improve performance
    over simpler models. These results suggest that it is easier to create a summarizer
    for a new domain than previous work suggests and bring into question the benefit
    of deep learning models for summarization for those domains that do have massive
    datasets (i.e., news). At the same time, they suggest important questions for
    new research in summarization; namely, new forms of sentence representations or
    external knowledge sources are needed that are better suited to the sumarization
    task.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1208.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1208.Attachment.pdf
  - filename: https://vimeo.com/305886331
    type: video
    url: https://vimeo.com/305886331
  author:
  - first: Chris
    full: Chris Kedzie
    id: chris-kedzie
    last: Kedzie
  - first: Kathleen
    full: Kathleen McKeown
    id: kathleen-mckeown
    last: McKeown
  - first: Hal
    full: "Hal Daum\xE9 III"
    id: hal-daume-iii
    last: "Daum\xE9 III"
  author_string: "Chris Kedzie, Kathleen McKeown, Hal Daum\xE9 III"
  bibkey: kedzie-etal-2018-content
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1208
  month: October-November
  page_first: '1818'
  page_last: '1828'
  pages: "1818\u20131828"
  paper_id: '208'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1208.pdf
  publisher: Association for Computational Linguistics
  revision:
  - id: '1'
    url: https://www.aclweb.org/anthology/D18-1208v1.pdf
    value: D18-1208v1
  - explanation: No description of the changes were recorded.
    id: '2'
    url: https://www.aclweb.org/anthology/D18-1208v2.pdf
    value: D18-1208v2
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1208.jpg
  title: Content Selection in Deep Learning Models of Summarization
  title_html: Content Selection in Deep Learning Models of Summarization
  url: https://www.aclweb.org/anthology/D18-1208
  year: '2018'
D18-1209:
  abstract: Network embeddings, which learns low-dimensional representations for each
    vertex in a large-scale network, have received considerable attention in recent
    years. For a wide range of applications, vertices in a network are typically accompanied
    by rich textual information such as user profiles, paper abstracts, etc. In this
    paper, we propose to incorporate semantic features into network embeddings by
    matching important words between text sequences for all pairs of vertices. We
    introduce an word-by-word alignment framework that measures the compatibility
    of embeddings between word pairs, and then adaptively accumulates these alignment
    features with a simple yet effective aggregation function. In experiments, we
    evaluate the proposed framework on three real-world benchmarks for downstream
    tasks, including link prediction and multi-label vertex classification. The experimental
    results demonstrate that our model outperforms state-of-the-art network embedding
    methods by a large margin.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306030030
    type: video
    url: https://vimeo.com/306030030
  author:
  - first: Dinghan
    full: Dinghan Shen
    id: dinghan-shen
    last: Shen
  - first: Xinyuan
    full: Xinyuan Zhang
    id: xinyuan-zhang
    last: Zhang
  - first: Ricardo
    full: Ricardo Henao
    id: ricardo-henao
    last: Henao
  - first: Lawrence
    full: Lawrence Carin
    id: lawrence-carin
    last: Carin
  author_string: Dinghan Shen, Xinyuan Zhang, Ricardo Henao, Lawrence Carin
  bibkey: shen-etal-2018-improved
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1209
  month: October-November
  page_first: '1829'
  page_last: '1838'
  pages: "1829\u20131838"
  paper_id: '209'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1209.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1209.jpg
  title: Improved Semantic-Aware Network Embedding with Fine-Grained Word Alignment
  title_html: Improved Semantic-Aware Network Embedding with Fine-Grained Word Alignment
  url: https://www.aclweb.org/anthology/D18-1209
  year: '2018'
D18-1210:
  abstract: Convolutional neural networks (CNNs) have recently emerged as a popular
    building block for natural language processing (NLP). Despite their success, most
    existing CNN models employed in NLP share the same learned (and static) set of
    filters for all input sentences. In this paper, we consider an approach of using
    a small meta network to learn context-sensitive convolutional filters for text
    processing. The role of meta network is to abstract the contextual information
    of a sentence or document into a set of input-sensitive filters. We further generalize
    this framework to model sentence pairs, where a bidirectional filter generation
    mechanism is introduced to encapsulate co-dependent sentence representations.
    In our benchmarks on four different tasks, including ontology classification,
    sentiment analysis, answer sentence selection, and paraphrase identification,
    our proposed model, a modified CNN with context-sensitive filters, consistently
    outperforms the standard CNN and attention-based CNN baselines. By visualizing
    the learned context-sensitive filters, we further validate and rationalize the
    effectiveness of proposed framework.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306040551
    type: video
    url: https://vimeo.com/306040551
  author:
  - first: Dinghan
    full: Dinghan Shen
    id: dinghan-shen
    last: Shen
  - first: Martin Renqiang
    full: Martin Renqiang Min
    id: martin-renqiang-min
    last: Min
  - first: Yitong
    full: Yitong Li
    id: yitong-li
    last: Li
  - first: Lawrence
    full: Lawrence Carin
    id: lawrence-carin
    last: Carin
  author_string: Dinghan Shen, Martin Renqiang Min, Yitong Li, Lawrence Carin
  bibkey: shen-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1210
  month: October-November
  page_first: '1839'
  page_last: '1848'
  pages: "1839\u20131848"
  paper_id: '210'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1210.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1210.jpg
  title: Learning Context-Sensitive Convolutional Filters for Text Processing
  title_html: Learning Context-Sensitive Convolutional Filters for Text Processing
  url: https://www.aclweb.org/anthology/D18-1210
  year: '2018'
D18-1211:
  abstract: "We explore several new models for document relevance ranking, building\
    \ upon the Deep Relevance Matching Model (DRMM) of Guo et al. (2016). Unlike DRMM,\
    \ which uses context-insensitive encodings of terms and query-document term interactions,\
    \ we inject rich context-sensitive encodings throughout our models, inspired by\
    \ PACRR\u2019s (Hui et al., 2017) convolutional n-gram matching features, but\
    \ extended in several ways including multiple views of query and document inputs.\
    \ We test our models on datasets from the BIOASQ question answering challenge\
    \ (Tsatsaronis et al., 2015) and TREC ROBUST 2004 (Voorhees, 2005), showing they\
    \ outperform BM25-based baselines, DRMM, and PACRR."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1211.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1211.Attachment.zip
  - filename: https://vimeo.com/306041612
    type: video
    url: https://vimeo.com/306041612
  author:
  - first: Ryan
    full: Ryan McDonald
    id: ryan-mcdonald
    last: McDonald
  - first: George
    full: George Brokos
    id: george-brokos
    last: Brokos
  - first: Ion
    full: Ion Androutsopoulos
    id: ion-androutsopoulos
    last: Androutsopoulos
  author_string: Ryan McDonald, George Brokos, Ion Androutsopoulos
  bibkey: mcdonald-etal-2018-deep
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1211
  month: October-November
  page_first: '1849'
  page_last: '1860'
  pages: "1849\u20131860"
  paper_id: '211'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1211.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1211.jpg
  title: Deep Relevance Ranking Using Enhanced Document-Query Interactions
  title_html: Deep Relevance Ranking Using Enhanced Document-Query Interactions
  url: https://www.aclweb.org/anthology/D18-1211
  year: '2018'
D18-1212:
  abstract: The existing studies in cross-language information retrieval (CLIR) mostly
    rely on general text representation models (e.g., vector space model or latent
    semantic analysis). These models are not optimized for the target retrieval task.
    In this paper, we follow the success of neural representation in natural language
    processing (NLP) and develop a novel text representation model based on adversarial
    learning, which seeks a task-specific embedding space for CLIR. Adversarial learning
    is implemented as an interplay between the generator process and the discriminator
    process. In order to adapt adversarial learning to CLIR, we design three constraints
    to direct representation learning, which are (1) a matching constraint capturing
    essential characteristics of cross-language ranking, (2) a translation constraint
    bridging language gaps, and (3) an adversarial constraint forcing both language
    and media invariant to be reached more efficiently and effectively. Through the
    joint exploitation of these constraints in an adversarial manner, the underlying
    cross-language semantics relevant to retrieval tasks are better preserved in the
    embedding space. Standard CLIR experiments show that our model significantly outperforms
    state-of-the-art continuous space models and is better than the strong machine
    translation baseline.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306042980
    type: video
    url: https://vimeo.com/306042980
  author:
  - first: Bo
    full: Bo Li
    id: bo-li
    last: Li
  - first: Ping
    full: Ping Cheng
    id: ping-cheng
    last: Cheng
  author_string: Bo Li, Ping Cheng
  bibkey: li-cheng-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1212
  month: October-November
  page_first: '1861'
  page_last: '1870'
  pages: "1861\u20131870"
  paper_id: '212'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1212.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1212.jpg
  title: Learning Neural Representation for CLIR with Adversarial Framework
  title_html: Learning Neural Representation for <span class="acl-fixed-case">CLIR</span>
    with Adversarial Framework
  url: https://www.aclweb.org/anthology/D18-1212
  year: '2018'
D18-1213:
  abstract: "Knowledge of the creation date of documents facilitates several tasks\
    \ such as summarization, event extraction, temporally focused information extraction\
    \ etc. Unfortunately, for most of the documents on the Web, the time-stamp metadata\
    \ is either missing or can\u2019t be trusted. Thus, predicting creation time from\
    \ document content itself is an important task. In this paper, we propose Attentive\
    \ Deep Document Dater (AD3), an attention-based neural document dating system\
    \ which utilizes both context and temporal information in documents in a flexible\
    \ and principled manner. We perform extensive experimentation on multiple real-world\
    \ datasets to demonstrate the effectiveness of AD3 over neural and non-neural\
    \ baselines."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306043956
    type: video
    url: https://vimeo.com/306043956
  author:
  - first: Swayambhu Nath
    full: Swayambhu Nath Ray
    id: swayambhu-nath-ray
    last: Ray
  - first: Shib Sankar
    full: Shib Sankar Dasgupta
    id: shib-sankar-dasgupta
    last: Dasgupta
  - first: Partha
    full: Partha Talukdar
    id: partha-talukdar
    last: Talukdar
  author_string: Swayambhu Nath Ray, Shib Sankar Dasgupta, Partha Talukdar
  bibkey: ray-etal-2018-ad3
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1213
  month: October-November
  page_first: '1871'
  page_last: '1880'
  pages: "1871\u20131880"
  paper_id: '213'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1213.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1213.jpg
  title: 'AD3: Attentive Deep Document Dater'
  title_html: '<span class="acl-fixed-case">AD</span>3: Attentive Deep Document Dater'
  url: https://www.aclweb.org/anthology/D18-1213
  year: '2018'
D18-1214:
  abstract: Cross-lingual or cross-domain correspondences play key roles in tasks
    ranging from machine translation to transfer learning. Recently, purely unsupervised
    methods operating on monolingual embeddings have become effective alignment tools.
    Current state-of-the-art methods, however, involve multiple steps, including heuristic
    post-hoc refinement strategies. In this paper, we cast the correspondence problem
    directly as an optimal transport (OT) problem, building on the idea that word
    embeddings arise from metric recovery algorithms. Indeed, we exploit the Gromov-Wasserstein
    distance that measures how similarities between pairs of words relate across languages.
    We show that our OT objective can be estimated efficiently, requires little or
    no tuning, and results in performance comparable with the state-of-the-art in
    various unsupervised word translation tasks.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305660461
    type: video
    url: https://vimeo.com/305660461
  author:
  - first: David
    full: David Alvarez-Melis
    id: david-alvarez-melis
    last: Alvarez-Melis
  - first: Tommi
    full: Tommi Jaakkola
    id: tommi-jaakkola
    last: Jaakkola
  author_string: David Alvarez-Melis, Tommi Jaakkola
  bibkey: alvarez-melis-jaakkola-2018-gromov
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1214
  month: October-November
  page_first: '1881'
  page_last: '1890'
  pages: "1881\u20131890"
  paper_id: '214'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1214.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1214.jpg
  title: Gromov-Wasserstein Alignment of Word Embedding Spaces
  title_html: <span class="acl-fixed-case">G</span>romov-<span class="acl-fixed-case">W</span>asserstein
    Alignment of Word Embedding Spaces
  url: https://www.aclweb.org/anthology/D18-1214
  year: '2018'
D18-1215:
  abstract: Deep learning has emerged as a versatile tool for a wide range of NLP
    tasks, due to its superior capacity in representation learning. But its applicability
    is limited by the reliance on annotated examples, which are difficult to produce
    at scale. Indirect supervision has emerged as a promising direction to address
    this bottleneck, either by introducing labeling functions to automatically generate
    noisy examples from unlabeled text, or by imposing constraints over interdependent
    label decisions. A plethora of methods have been proposed, each with respective
    strengths and limitations. Probabilistic logic offers a unifying language to represent
    indirect supervision, but end-to-end modeling with probabilistic logic is often
    infeasible due to intractable inference and learning. In this paper, we propose
    deep probabilistic logic (DPL) as a general framework for indirect supervision,
    by composing probabilistic logic with deep learning. DPL models label decisions
    as latent variables, represents prior knowledge on their relations using weighted
    first-order logical formulas, and alternates between learning a deep neural network
    for the end task and refining uncertain formula weights for indirect supervision,
    using variational EM. This framework subsumes prior indirect supervision methods
    as special cases, and enables novel combination via infusion of rich domain and
    linguistic knowledge. Experiments on biomedical machine reading demonstrate the
    promise of this approach.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305661315
    type: video
    url: https://vimeo.com/305661315
  author:
  - first: Hai
    full: Hai Wang
    id: hai-wang
    last: Wang
  - first: Hoifung
    full: Hoifung Poon
    id: hoifung-poon
    last: Poon
  author_string: Hai Wang, Hoifung Poon
  bibkey: wang-poon-2018-deep
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1215
  month: October-November
  page_first: '1891'
  page_last: '1902'
  pages: "1891\u20131902"
  paper_id: '215'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1215.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1215.jpg
  title: 'Deep Probabilistic Logic: A Unifying Framework for Indirect Supervision'
  title_html: 'Deep Probabilistic Logic: A Unifying Framework for Indirect Supervision'
  url: https://www.aclweb.org/anthology/D18-1215
  year: '2018'
D18-1216:
  abstract: Attention-based models are successful when trained on large amounts of
    data. In this paper, we demonstrate that even in the low-resource scenario, attention
    can be learned effectively. To this end, we start with discrete human-annotated
    rationales and map them into continuous attention. Our central hypothesis is that
    this mapping is general across domains, and thus can be transferred from resource-rich
    domains to low-resource ones. Our model jointly learns a domain-invariant representation
    and induces the desired mapping between rationales and attention. Our empirical
    results validate this hypothesis and show that our approach delivers significant
    gains over state-of-the-art baselines, yielding over 15% average error reduction
    on benchmark datasets.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1216.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1216.Attachment.zip
  - filename: https://vimeo.com/305661928
    type: video
    url: https://vimeo.com/305661928
  author:
  - first: Yujia
    full: Yujia Bao
    id: yujia-bao
    last: Bao
  - first: Shiyu
    full: Shiyu Chang
    id: shiyu-chang
    last: Chang
  - first: Mo
    full: Mo Yu
    id: mo-yu
    last: Yu
  - first: Regina
    full: Regina Barzilay
    id: regina-barzilay
    last: Barzilay
  author_string: Yujia Bao, Shiyu Chang, Mo Yu, Regina Barzilay
  bibkey: bao-etal-2018-deriving
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1216
  month: October-November
  page_first: '1903'
  page_last: '1913'
  pages: "1903\u20131913"
  paper_id: '216'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1216.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1216.jpg
  title: Deriving Machine Attention from Human Rationales
  title_html: Deriving Machine Attention from Human Rationales
  url: https://www.aclweb.org/anthology/D18-1216
  year: '2018'
D18-1217:
  abstract: Unsupervised representation learning algorithms such as word2vec and ELMo
    improve the accuracy of many supervised NLP models, mainly because they can take
    advantage of large amounts of unlabeled text. However, the supervised models only
    learn from task-specific labeled data during the main training phase. We therefore
    propose Cross-View Training (CVT), a semi-supervised learning algorithm that improves
    the representations of a Bi-LSTM sentence encoder using a mix of labeled and unlabeled
    data. On labeled examples, standard supervised learning is used. On unlabeled
    examples, CVT teaches auxiliary prediction modules that see restricted views of
    the input (e.g., only part of a sentence) to match the predictions of the full
    model seeing the whole input. Since the auxiliary modules and the full model share
    intermediate representations, this in turn improves the full model. Moreover,
    we show that CVT is particularly effective when combined with multi-task learning.
    We evaluate CVT on five sequence tagging tasks, machine translation, and dependency
    parsing, achieving state-of-the-art results.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1217.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1217.Attachment.zip
  - filename: https://vimeo.com/305662403
    type: video
    url: https://vimeo.com/305662403
  author:
  - first: Kevin
    full: Kevin Clark
    id: kevin-clark
    last: Clark
  - first: Minh-Thang
    full: Minh-Thang Luong
    id: minh-thang-luong
    last: Luong
  - first: Christopher D.
    full: Christopher D. Manning
    id: christopher-d-manning
    last: Manning
  - first: Quoc
    full: Quoc Le
    id: quoc-le
    last: Le
  author_string: Kevin Clark, Minh-Thang Luong, Christopher D. Manning, Quoc Le
  bibkey: clark-etal-2018-semi
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1217
  month: October-November
  page_first: '1914'
  page_last: '1925'
  pages: "1914\u20131925"
  paper_id: '217'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1217.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1217.jpg
  title: Semi-Supervised Sequence Modeling with Cross-View Training
  title_html: Semi-Supervised Sequence Modeling with Cross-View Training
  url: https://www.aclweb.org/anthology/D18-1217
  year: '2018'
D18-1218:
  abstract: 'The availability of large scale annotated corpora for coreference is
    essential to the development of the field. However, creating resources at the
    required scale via expert annotation would be too expensive. Crowdsourcing has
    been proposed as an alternative; but this approach has not been widely used for
    coreference. This paper addresses one crucial hurdle on the way to make this possible,
    by introducing a new model of annotation for aggregating crowdsourced anaphoric
    annotations. The model is evaluated along three dimensions: the accuracy of the
    inferred mention pairs, the quality of the post-hoc constructed silver chains,
    and the viability of using the silver chains as an alternative to the expert-annotated
    chains in training a state of the art coreference system. The results suggest
    that our model can extract from crowdsourced annotations coreference chains of
    comparable quality to those obtained with expert annotation.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1218.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1218.Attachment.zip
  author:
  - first: Silviu
    full: Silviu Paun
    id: silviu-paun
    last: Paun
  - first: Jon
    full: Jon Chamberlain
    id: jon-chamberlain
    last: Chamberlain
  - first: Udo
    full: Udo Kruschwitz
    id: udo-kruschwitz
    last: Kruschwitz
  - first: Juntao
    full: Juntao Yu
    id: juntao-yu
    last: Yu
  - first: Massimo
    full: Massimo Poesio
    id: massimo-poesio
    last: Poesio
  author_string: Silviu Paun, Jon Chamberlain, Udo Kruschwitz, Juntao Yu, Massimo
    Poesio
  bibkey: paun-etal-2018-probabilistic
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1218
  month: October-November
  page_first: '1926'
  page_last: '1937'
  pages: "1926\u20131937"
  paper_id: '218'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1218.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1218.jpg
  title: A Probabilistic Annotation Model for Crowdsourcing Coreference
  title_html: A Probabilistic Annotation Model for Crowdsourcing Coreference
  url: https://www.aclweb.org/anthology/D18-1218
  year: '2018'
D18-1219:
  abstract: "Previous work on bridging anaphora resolution (Poesio et al., 2004; Hou\
    \ et al., 2013) use syntactic preposition patterns to calculate word relatedness.\
    \ However, such patterns only consider NPs\u2019 head nouns and hence do not fully\
    \ capture the semantics of NPs. Recently, Hou (2018) created word embeddings (embeddings_PP)\
    \ to capture associative similarity (i.e., relatedness) between nouns by exploring\
    \ the syntactic structure of noun phrases. But embeddings_PP only contains word\
    \ representations for nouns. In this paper, we create new word vectors by combining\
    \ embeddings_PP with GloVe. This new word embeddings (embeddings_bridging) are\
    \ a more general lexical knowledge resource for bridging and allow us to represent\
    \ the meaning of an NP beyond its head easily. We therefore develop a deterministic\
    \ approach for bridging anaphora resolution, which represents the semantics of\
    \ an NP based on its head noun and modifications. We show that this simple approach\
    \ achieves the competitive results compared to the best system in Hou et al. (2013)\
    \ which explores Markov Logic Networks to model the problem. Additionally, we\
    \ further improve the results for bridging anaphora resolution reported in Hou\
    \ (2018) by combining our simple deterministic approach with Hou et al. (2013)\u2019\
    s best system MLN II."
  address: Brussels, Belgium
  author:
  - first: Yufang
    full: Yufang Hou
    id: yufang-hou
    last: Hou
  author_string: Yufang Hou
  bibkey: hou-2018-deterministic
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1219
  month: October-November
  page_first: '1938'
  page_last: '1948'
  pages: "1938\u20131948"
  paper_id: '219'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1219.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1219.jpg
  title: A Deterministic Algorithm for Bridging Anaphora Resolution
  title_html: A Deterministic Algorithm for Bridging Anaphora Resolution
  url: https://www.aclweb.org/anthology/D18-1219
  year: '2018'
D18-1220:
  abstract: We introduce an automatic system that achieves state-of-the-art results
    on the Winograd Schema Challenge (WSC), a common sense reasoning task that requires
    diverse, complex forms of inference and knowledge. Our method uses a knowledge
    hunting module to gather text from the web, which serves as evidence for candidate
    problem resolutions. Given an input problem, our system generates relevant queries
    to send to a search engine, then extracts and classifies knowledge from the returned
    results and weighs them to make a resolution. Our approach improves F1 performance
    on the full WSC by 0.21 over the previous best and represents the first system
    to exceed 0.5 F1. We further demonstrate that the approach is competitive on the
    Choice of Plausible Alternatives (COPA) task, which suggests that it is generally
    applicable.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1220.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1220.Attachment.pdf
  author:
  - first: Ali
    full: Ali Emami
    id: ali-emami
    last: Emami
  - first: Noelia
    full: Noelia De La Cruz
    id: noelia-de-la-cruz
    last: De La Cruz
  - first: Adam
    full: Adam Trischler
    id: adam-trischler
    last: Trischler
  - first: Kaheer
    full: Kaheer Suleman
    id: kaheer-suleman
    last: Suleman
  - first: Jackie Chi Kit
    full: Jackie Chi Kit Cheung
    id: jackie-chi-kit-cheung
    last: Cheung
  author_string: Ali Emami, Noelia De La Cruz, Adam Trischler, Kaheer Suleman, Jackie
    Chi Kit Cheung
  bibkey: emami-etal-2018-knowledge
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1220
  month: October-November
  page_first: '1949'
  page_last: '1958'
  pages: "1949\u20131958"
  paper_id: '220'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1220.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1220.jpg
  title: A Knowledge Hunting Framework for Common Sense Reasoning
  title_html: A Knowledge Hunting Framework for Common Sense Reasoning
  url: https://www.aclweb.org/anthology/D18-1220
  year: '2018'
D18-1221:
  abstract: This paper addresses the problem of mapping natural language text to knowledge
    base entities. The mapping process is approached as a composition of a phrase
    or a sentence into a point in a multi-dimensional entity space obtained from a
    knowledge graph. The compositional model is an LSTM equipped with a dynamic disambiguation
    mechanism on the input word embeddings (a Multi-Sense LSTM), addressing polysemy
    issues. Further, the knowledge base space is prepared by collecting random walks
    from a graph enhanced with textual features, which act as a set of semantic bridges
    between text and knowledge base entities. The ideas of this work are demonstrated
    on large-scale text-to-entity mapping and entity classification tasks, with state
    of the art results.
  address: Brussels, Belgium
  author:
  - first: Dimitri
    full: Dimitri Kartsaklis
    id: dimitri-kartsaklis
    last: Kartsaklis
  - first: Mohammad Taher
    full: Mohammad Taher Pilehvar
    id: mohammad-taher-pilehvar
    last: Pilehvar
  - first: Nigel
    full: Nigel Collier
    id: nigel-collier
    last: Collier
  author_string: Dimitri Kartsaklis, Mohammad Taher Pilehvar, Nigel Collier
  bibkey: kartsaklis-etal-2018-mapping
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1221
  month: October-November
  page_first: '1959'
  page_last: '1970'
  pages: "1959\u20131970"
  paper_id: '221'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1221.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1221.jpg
  title: Mapping Text to Knowledge Graph Entities using Multi-Sense LSTMs
  title_html: Mapping Text to Knowledge Graph Entities using Multi-Sense <span class="acl-fixed-case">LSTM</span>s
  url: https://www.aclweb.org/anthology/D18-1221
  year: '2018'
D18-1222:
  abstract: Concepts, which represent a group of different instances sharing common
    properties, are essential information in knowledge representation. Most conventional
    knowledge embedding methods encode both entities (concepts and instances) and
    relations as vectors in a low dimensional semantic space equally, ignoring the
    difference between concepts and instances. In this paper, we propose a novel knowledge
    graph embedding model named TransC by differentiating concepts and instances.
    Specifically, TransC encodes each concept in knowledge graph as a sphere and each
    instance as a vector in the same semantic space. We use the relative positions
    to model the relations between concepts and instances (i.e.,instanceOf), and the
    relations between concepts and sub-concepts (i.e., subClassOf). We evaluate our
    model on both link prediction and triple classification tasks on the dataset based
    on YAGO. Experimental results show that TransC outperforms state-of-the-art methods,
    and captures the semantic transitivity for instanceOf and subClassOf relation.
    Our codes and datasets can be obtained from https://github.com/davidlvxin/TransC.
  address: Brussels, Belgium
  author:
  - first: Xin
    full: Xin Lv
    id: xin-lv
    last: Lv
  - first: Lei
    full: Lei Hou
    id: lei-hou
    last: Hou
  - first: Juanzi
    full: Juanzi Li
    id: juanzi-li
    last: Li
  - first: Zhiyuan
    full: Zhiyuan Liu
    id: zhiyuan-liu
    last: Liu
  author_string: Xin Lv, Lei Hou, Juanzi Li, Zhiyuan Liu
  bibkey: lv-etal-2018-differentiating
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1222
  month: October-November
  page_first: '1971'
  page_last: '1979'
  pages: "1971\u20131979"
  paper_id: '222'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1222.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1222.jpg
  title: Differentiating Concepts and Instances for Knowledge Graph Embedding
  title_html: Differentiating Concepts and Instances for Knowledge Graph Embedding
  url: https://www.aclweb.org/anthology/D18-1222
  year: '2018'
D18-1223:
  abstract: "Knowledge graphs (KG) are the key components of various natural language\
    \ processing applications. To further expand KGs\u2019 coverage, previous studies\
    \ on knowledge graph completion usually require a large number of positive examples\
    \ for each relation. However, we observe long-tail relations are actually more\
    \ common in KGs and those newly added relations often do not have many known triples\
    \ for training. In this work, we aim at predicting new facts under a challenging\
    \ setting where only one training instance is available. We propose a one-shot\
    \ relational learning framework, which utilizes the knowledge distilled by embedding\
    \ models and learns a matching metric by considering both the learned embeddings\
    \ and one-hop graph structures. Empirically, our model yields considerable performance\
    \ improvements over existing embedding models, and also eliminates the need of\
    \ re-training the embedding models when dealing with newly added relations."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1223.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1223.Attachment.pdf
  author:
  - first: Wenhan
    full: Wenhan Xiong
    id: wenhan-xiong
    last: Xiong
  - first: Mo
    full: Mo Yu
    id: mo-yu
    last: Yu
  - first: Shiyu
    full: Shiyu Chang
    id: shiyu-chang
    last: Chang
  - first: Xiaoxiao
    full: Xiaoxiao Guo
    id: xiaoxiao-guo
    last: Guo
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  author_string: Wenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo, William Yang Wang
  bibkey: xiong-etal-2018-one
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1223
  month: October-November
  page_first: '1980'
  page_last: '1990'
  pages: "1980\u20131990"
  paper_id: '223'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1223.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1223.jpg
  title: One-Shot Relational Learning for Knowledge Graphs
  title_html: One-Shot Relational Learning for Knowledge Graphs
  url: https://www.aclweb.org/anthology/D18-1223
  year: '2018'
D18-1224:
  abstract: Many important entity types in web documents, such as dates, times, email
    addresses, and course numbers, follow or closely resemble patterns that can be
    described by Regular Expressions (REs). Due to a vast diversity of web documents
    and ways in which they are being generated, even seemingly straightforward tasks
    such as identifying mentions of date in a document become very challenging. It
    is reasonable to claim that it is impossible to create a RE that is capable of
    identifying such entities from web documents with perfect precision and recall.
    Rather than abandoning REs as a go-to approach for entity detection, this paper
    explores ways to combine the expressive power of REs, ability of deep learning
    to learn from large data, and human-in-the loop approach into a new integrated
    framework for entity identification from web data. The framework starts by creating
    or collecting the existing REs for a particular type of an entity. Those REs are
    then used over a large document corpus to collect weak labels for the entity mentions
    and a neural network is trained to predict those RE-generated weak labels. Finally,
    a human expert is asked to label a small set of documents and the neural network
    is fine tuned on those documents. The experimental evaluation on several entity
    identification problems shows that the proposed framework achieves impressive
    accuracy, while requiring very modest human effort.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1224.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1224.Attachment.zip
  author:
  - first: Shanshan
    full: Shanshan Zhang
    id: shanshan-zhang
    last: Zhang
  - first: Lihong
    full: Lihong He
    id: lihong-he
    last: He
  - first: Slobodan
    full: Slobodan Vucetic
    id: slobodan-vucetic
    last: Vucetic
  - first: Eduard
    full: Eduard Dragut
    id: eduard-dragut
    last: Dragut
  author_string: Shanshan Zhang, Lihong He, Slobodan Vucetic, Eduard Dragut
  bibkey: zhang-etal-2018-regular
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1224
  month: October-November
  page_first: '1991'
  page_last: '2000'
  pages: "1991\u20132000"
  paper_id: '224'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1224.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1224.jpg
  title: Regular Expression Guided Entity Mention Mining from Noisy Web Data
  title_html: Regular Expression Guided Entity Mention Mining from Noisy Web Data
  url: https://www.aclweb.org/anthology/D18-1224
  year: '2018'
D18-1225:
  abstract: Knowledge Graph (KG) embedding has emerged as an active area of research
    resulting in the development of several KG embedding methods. Relational facts
    in KG often show temporal dynamics, e.g., the fact (Cristiano_Ronaldo, playsFor,
    Manchester_United) is valid only from 2003 to 2009. Most of the existing KG embedding
    methods ignore this temporal dimension while learning embeddings of the KG elements.
    In this paper, we propose HyTE, a temporally aware KG embedding method which explicitly
    incorporates time in the entity-relation space by associating each timestamp with
    a corresponding hyperplane. HyTE not only performs KG inference using temporal
    guidance, but also predicts temporal scopes for relational facts with missing
    time annotations. Through extensive experimentation on temporal datasets extracted
    from real-world KGs, we demonstrate the effectiveness of our model over both traditional
    as well as temporal KG embedding methods.
  address: Brussels, Belgium
  author:
  - first: Shib Sankar
    full: Shib Sankar Dasgupta
    id: shib-sankar-dasgupta
    last: Dasgupta
  - first: Swayambhu Nath
    full: Swayambhu Nath Ray
    id: swayambhu-nath-ray
    last: Ray
  - first: Partha
    full: Partha Talukdar
    id: partha-talukdar
    last: Talukdar
  author_string: Shib Sankar Dasgupta, Swayambhu Nath Ray, Partha Talukdar
  bibkey: dasgupta-etal-2018-hyte
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1225
  month: October-November
  page_first: '2001'
  page_last: '2011'
  pages: "2001\u20132011"
  paper_id: '225'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1225.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1225.jpg
  title: 'HyTE: Hyperplane-based Temporally aware Knowledge Graph Embedding'
  title_html: '<span class="acl-fixed-case">H</span>y<span class="acl-fixed-case">TE</span>:
    Hyperplane-based Temporally aware Knowledge Graph Embedding'
  url: https://www.aclweb.org/anthology/D18-1225
  year: '2018'
D18-1226:
  abstract: Recent research efforts have shown that neural architectures can be effective
    in conventional information extraction tasks such as named entity recognition,
    yielding state-of-the-art results on standard newswire datasets. However, despite
    significant resources required for training such models, the performance of a
    model trained on one domain typically degrades dramatically when applied to a
    different domain, yet extracting entities from new emerging domains such as social
    media can be of significant interest. In this paper, we empirically investigate
    effective methods for conveniently adapting an existing, well-trained neural NER
    model for a new domain. Unlike existing approaches, we propose lightweight yet
    effective methods for performing domain adaptation for neural models. Specifically,
    we introduce adaptation layers on top of existing neural architectures, where
    no re-training using the source domain data is required. We conduct extensive
    empirical studies and show that our approach significantly outperforms state-of-the-art
    methods.
  address: Brussels, Belgium
  author:
  - first: Bill Yuchen
    full: Bill Yuchen Lin
    id: bill-yuchen-lin
    last: Lin
  - first: Wei
    full: Wei Lu
    id: wei-lu
    last: Lu
  author_string: Bill Yuchen Lin, Wei Lu
  bibkey: lin-lu-2018-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1226
  month: October-November
  page_first: '2012'
  page_last: '2022'
  pages: "2012\u20132022"
  paper_id: '226'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1226.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1226.jpg
  title: Neural Adaptation Layers for Cross-domain Named Entity Recognition
  title_html: Neural Adaptation Layers for Cross-domain Named Entity Recognition
  url: https://www.aclweb.org/anthology/D18-1226
  year: '2018'
D18-1227:
  abstract: 'In this paper, we study a new entity linking problem where both the entity
    mentions and the target entities are within a same social media platform. Compared
    with traditional entity linking problems that link mentions to a knowledge base,
    this new problem have less information about the target entities. However, if
    we can successfully link mentions to entities within a social media platform,
    we can improve a lot of applications such as comparative study in business intelligence
    and opinion leader finding. To study this problem, we constructed a dataset called
    Yelp-EL, where the business mentions in Yelp reviews are linked to their corresponding
    businesses on the platform. We conducted comprehensive experiments and analysis
    on this dataset with a learning to rank model that takes different types of features
    as input, as well as a few state-of-the-art entity linking approaches. Our experimental
    results show that two types of features that are not available in traditional
    entity linking: social features and location features, can be very helpful for
    this task.'
  address: Brussels, Belgium
  author:
  - first: Hongliang
    full: Hongliang Dai
    id: hongliang-dai
    last: Dai
  - first: Yangqiu
    full: Yangqiu Song
    id: yangqiu-song
    last: Song
  - first: Liwei
    full: Liwei Qiu
    id: liwei-qiu
    last: Qiu
  - first: Rijia
    full: Rijia Liu
    id: rijia-liu
    last: Liu
  author_string: Hongliang Dai, Yangqiu Song, Liwei Qiu, Rijia Liu
  bibkey: dai-etal-2018-entity
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1227
  month: October-November
  page_first: '2023'
  page_last: '2032'
  pages: "2023\u20132032"
  paper_id: '227'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1227.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1227.jpg
  title: 'Entity Linking within a Social Media Platform: A Case Study on Yelp'
  title_html: 'Entity Linking within a Social Media Platform: A Case Study on Yelp'
  url: https://www.aclweb.org/anthology/D18-1227
  year: '2018'
D18-1228:
  abstract: Having an entity annotated corpus of the clinical domain is one of the
    basic requirements for detection of clinical entities using machine learning (ML)
    approaches. Past researches have shown the superiority of statistical/ML approaches
    over the rule based approaches. But in order to take full advantage of the ML
    approaches, an accurately annotated corpus becomes an essential requirement. Though
    there are a few annotated corpora available either on a small data set, or covering
    a narrower domain (like cancer patients records, lab reports), annotation of a
    large data set representing the entire clinical domain has not been created yet.
    In this paper, we have described in detail the annotation guidelines, annotation
    process and our approaches in creating a CER (clinical entity recognition) corpus
    of 5,160 clinical documents from forty different clinical specialities. The clinical
    entities range across various types such as diseases, procedures, medications,
    medical devices and so on. We have classified them into eleven categories for
    annotation. Our annotation also reflects the relations among the group of entities
    that constitute larger concepts altogether.
  address: Brussels, Belgium
  author:
  - first: Pinal
    full: Pinal Patel
    id: pinal-patel
    last: Patel
  - first: Disha
    full: Disha Davey
    id: disha-davey
    last: Davey
  - first: Vishal
    full: Vishal Panchal
    id: vishal-panchal
    last: Panchal
  - first: Parth
    full: Parth Pathak
    id: parth-pathak
    last: Pathak
  author_string: Pinal Patel, Disha Davey, Vishal Panchal, Parth Pathak
  bibkey: patel-etal-2018-annotation
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1228
  month: October-November
  page_first: '2033'
  page_last: '2042'
  pages: "2033\u20132042"
  paper_id: '228'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1228.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1228.jpg
  title: Annotation of a Large Clinical Entity Corpus
  title_html: Annotation of a Large Clinical Entity Corpus
  url: https://www.aclweb.org/anthology/D18-1228
  year: '2018'
D18-1229:
  abstract: We challenge a common assumption in active learning, that a list-based
    interface populated by informative samples provides for efficient and effective
    data annotation. We show how a 2D scatterplot populated with diverse and representative
    samples can yield improved models given the same time budget. We consider this
    for bootstrapping-based information extraction, in particular named entity classification,
    where human and machine jointly label data. To enable effective data annotation
    in a scatterplot, we have developed an embedding-based bootstrapping model that
    learns the distributional similarity of entities through the patterns that match
    them in a large data corpus, while being discriminative with respect to human-labeled
    and machine-promoted entities. We conducted a user study to assess the effectiveness
    of these different interfaces, and analyze bootstrapping performance in terms
    of human labeling accuracy, label quantity, and labeling consensus across multiple
    users. Our results suggest that supervision acquired from the scatterplot interface,
    despite being noisier, yields improvements in classification performance compared
    with the list interface, due to a larger quantity of supervision acquired.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1229.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1229.Attachment.zip
  author:
  - first: Matthew
    full: Matthew Berger
    id: matthew-berger
    last: Berger
  - first: Ajay
    full: Ajay Nagesh
    id: ajay-nagesh
    last: Nagesh
  - first: Joshua
    full: Joshua Levine
    id: joshua-levine
    last: Levine
  - first: Mihai
    full: Mihai Surdeanu
    id: mihai-surdeanu
    last: Surdeanu
  - first: Helen
    full: Helen Zhang
    id: helen-zhang
    last: Zhang
  author_string: Matthew Berger, Ajay Nagesh, Joshua Levine, Mihai Surdeanu, Helen
    Zhang
  bibkey: berger-etal-2018-visual
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1229
  month: October-November
  page_first: '2043'
  page_last: '2053'
  pages: "2043\u20132053"
  paper_id: '229'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1229.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1229.jpg
  title: Visual Supervision in Bootstrapped Information Extraction
  title_html: Visual Supervision in Bootstrapped Information Extraction
  url: https://www.aclweb.org/anthology/D18-1229
  year: '2018'
D18-1230:
  abstract: Recent advances in deep neural models allow us to build reliable named
    entity recognition (NER) systems without handcrafting features. However, such
    methods require large amounts of manually-labeled training data. There have been
    efforts on replacing human annotations with distant supervision (in conjunction
    with external dictionaries), but the generated noisy labels pose significant challenges
    on learning effective neural models. Here we propose two neural models to suit
    noisy distant supervision from the dictionary. First, under the traditional sequence
    labeling framework, we propose a revised fuzzy CRF layer to handle tokens with
    multiple possible labels. After identifying the nature of noisy labels in distant
    supervision, we go beyond the traditional framework and propose a novel, more
    effective neural model AutoNER with a new Tie or Break scheme. In addition, we
    discuss how to refine distant supervision for better NER performance. Extensive
    experiments on three benchmark datasets demonstrate that AutoNER achieves the
    best performance when only using dictionaries with no additional human effort,
    and delivers competitive results with state-of-the-art supervised benchmarks.
  address: Brussels, Belgium
  author:
  - first: Jingbo
    full: Jingbo Shang
    id: jingbo-shang
    last: Shang
  - first: Liyuan
    full: Liyuan Liu
    id: liyuan-liu
    last: Liu
  - first: Xiaotao
    full: Xiaotao Gu
    id: xiaotao-gu
    last: Gu
  - first: Xiang
    full: Xiang Ren
    id: xiang-ren
    last: Ren
  - first: Teng
    full: Teng Ren
    id: teng-ren
    last: Ren
  - first: Jiawei
    full: Jiawei Han
    id: jiawei-han
    last: Han
  author_string: Jingbo Shang, Liyuan Liu, Xiaotao Gu, Xiang Ren, Teng Ren, Jiawei
    Han
  bibkey: shang-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1230
  month: October-November
  page_first: '2054'
  page_last: '2064'
  pages: "2054\u20132064"
  paper_id: '230'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1230.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1230.jpg
  title: Learning Named Entity Tagger using Domain-Specific Dictionary
  title_html: Learning Named Entity Tagger using Domain-Specific Dictionary
  url: https://www.aclweb.org/anthology/D18-1230
  year: '2018'
D18-1231:
  abstract: "The problem of entity-typing has been studied predominantly as a supervised\
    \ learning problems, mostly with task-specific annotations (for coarse types)\
    \ and sometimes with distant supervision (for fine types). While such approaches\
    \ have strong performance within datasets they often lack the flexibility to transfer\
    \ across text genres and to generalize to new type taxonomies. In this work we\
    \ propose a zero-shot entity typing approach that requires no annotated data and\
    \ can flexibly identify newly defined types. Given a type taxonomy, the entries\
    \ of which we define as Boolean functions of freebase \u201Ctypes,\u201D we ground\
    \ a given mention to a set of type-compatible Wikipedia entries, and then infer\
    \ the target mention\u2019s type using an inference algorithm that makes use of\
    \ the types of these entries. We evaluate our system on a broad range of datasets,\
    \ including standard fine-grained and coarse-grained entity typing datasets, and\
    \ on a dataset in the biological domain. Our system is shown to be competitive\
    \ with state-of-the-art supervised NER systems, and to outperform them on out-of-training\
    \ datasets. We also show that our system significantly outperforms other zero-shot\
    \ fine typing systems."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1231.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1231.Attachment.pdf
  author:
  - first: Ben
    full: Ben Zhou
    id: ben-zhou
    last: Zhou
  - first: Daniel
    full: Daniel Khashabi
    id: daniel-khashabi
    last: Khashabi
  - first: Chen-Tse
    full: Chen-Tse Tsai
    id: chen-tse-tsai
    last: Tsai
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Ben Zhou, Daniel Khashabi, Chen-Tse Tsai, Dan Roth
  bibkey: zhou-etal-2018-zero
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1231
  month: October-November
  page_first: '2065'
  page_last: '2076'
  pages: "2065\u20132076"
  paper_id: '231'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1231.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1231.jpg
  title: Zero-Shot Open Entity Typing as Type-Compatible Grounding
  title_html: Zero-Shot Open Entity Typing as Type-Compatible Grounding
  url: https://www.aclweb.org/anthology/D18-1231
  year: '2018'
D18-1232:
  abstract: Despite that current reading comprehension systems have achieved significant
    advancements, their promising performances are often obtained at the cost of making
    an ensemble of numerous models. Besides, existing approaches are also vulnerable
    to adversarial attacks. This paper tackles these problems by leveraging knowledge
    distillation, which aims to transfer knowledge from an ensemble model to a single
    model. We first demonstrate that vanilla knowledge distillation applied to answer
    span prediction is effective for reading comprehension systems. We then propose
    two novel approaches that not only penalize the prediction on confusing answers
    but also guide the training with alignment information distilled from the ensemble.
    Experiments show that our best student model has only a slight drop of 0.4% F1
    on the SQuAD test set compared to the ensemble teacher, while running 12x faster
    during inference. It even outperforms the teacher on adversarial SQuAD datasets
    and NarrativeQA benchmark.
  address: Brussels, Belgium
  author:
  - first: Minghao
    full: Minghao Hu
    id: minghao-hu
    last: Hu
  - first: Yuxing
    full: Yuxing Peng
    id: yuxing-peng
    last: Peng
  - first: Furu
    full: Furu Wei
    id: furu-wei
    last: Wei
  - first: Zhen
    full: Zhen Huang
    id: zhen-huang
    last: Huang
  - first: Dongsheng
    full: Dongsheng Li
    id: dongsheng-li
    last: Li
  - first: Nan
    full: Nan Yang
    id: nan-yang
    last: Yang
  - first: Ming
    full: Ming Zhou
    id: ming-zhou
    last: Zhou
  author_string: Minghao Hu, Yuxing Peng, Furu Wei, Zhen Huang, Dongsheng Li, Nan
    Yang, Ming Zhou
  bibkey: hu-etal-2018-attention
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1232
  month: October-November
  page_first: '2077'
  page_last: '2086'
  pages: "2077\u20132086"
  paper_id: '232'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1232.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1232.jpg
  title: Attention-Guided Answer Distillation for Machine Reading Comprehension
  title_html: Attention-Guided Answer Distillation for Machine Reading Comprehension
  url: https://www.aclweb.org/anthology/D18-1232
  year: '2018'
D18-1233:
  abstract: "Most work in machine reading focuses on question answering problems where\
    \ the answer is directly expressed in the text to read. However, many real-world\
    \ question answering problems require the reading of text not because it contains\
    \ the literal answer, but because it contains a recipe to derive an answer together\
    \ with the reader\u2019s background knowledge. One example is the task of interpreting\
    \ regulations to answer \u201CCan I...?\u201D or \u201CDo I have to...?\u201D\
    \ questions such as \u201CI am working in Canada. Do I have to carry on paying\
    \ UK National Insurance?\u201D after reading a UK government website about this\
    \ topic. This task requires both the interpretation of rules and the application\
    \ of background knowledge. It is further complicated due to the fact that, in\
    \ practice, most questions are underspecified, and a human assistant will regularly\
    \ have to ask clarification questions such as \u201CHow long have you been working\
    \ abroad?\u201D when the answer cannot be directly derived from the question and\
    \ text. In this paper, we formalise this task and develop a crowd-sourcing strategy\
    \ to collect 37k task instances based on real-world rules and crowd-generated\
    \ questions and scenarios. We analyse the challenges of this task and assess its\
    \ difficulty by evaluating the performance of rule-based and machine-learning\
    \ baselines. We observe promising results when no background knowledge is necessary,\
    \ and substantial room for improvement whenever background knowledge is needed."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1233.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1233.Attachment.zip
  author:
  - first: Marzieh
    full: Marzieh Saeidi
    id: marzieh-saeidi
    last: Saeidi
  - first: Max
    full: Max Bartolo
    id: max-bartolo
    last: Bartolo
  - first: Patrick
    full: Patrick Lewis
    id: patrick-lewis
    last: Lewis
  - first: Sameer
    full: Sameer Singh
    id: sameer-singh
    last: Singh
  - first: Tim
    full: "Tim Rockt\xE4schel"
    id: tim-rocktaschel
    last: "Rockt\xE4schel"
  - first: Mike
    full: Mike Sheldon
    id: mike-sheldon
    last: Sheldon
  - first: Guillaume
    full: Guillaume Bouchard
    id: guillaume-bouchard
    last: Bouchard
  - first: Sebastian
    full: Sebastian Riedel
    id: sebastian-riedel
    last: Riedel
  author_string: "Marzieh Saeidi, Max Bartolo, Patrick Lewis, Sameer Singh, Tim Rockt\xE4\
    schel, Mike Sheldon, Guillaume Bouchard, Sebastian Riedel"
  bibkey: saeidi-etal-2018-interpretation
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1233
  month: October-November
  page_first: '2087'
  page_last: '2097'
  pages: "2087\u20132097"
  paper_id: '233'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1233.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1233.jpg
  title: Interpretation of Natural Language Rules in Conversational Machine Reading
  title_html: Interpretation of Natural Language Rules in Conversational Machine Reading
  url: https://www.aclweb.org/anthology/D18-1233
  year: '2018'
D18-1234:
  abstract: Although natural language question answering over knowledge graphs have
    been studied in the literature, existing methods have some limitations in answering
    complex questions. To address that, in this paper, we propose a State Transition-based
    approach to translate a complex natural language question N to a semantic query
    graph (SQG), which is used to match the underlying knowledge graph to find the
    answers to question N. In order to generate SQG, we propose four primitive operations
    (expand, fold, connect and merge) and a learning-based state transition approach.
    Extensive experiments on several benchmarks (such as QALD, WebQuestions and ComplexQuestions)
    with two knowledge bases (DBpedia and Freebase) confirm the superiority of our
    approach compared with state-of-the-arts.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1234.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1234.Attachment.pdf
  author:
  - first: Sen
    full: Sen Hu
    id: sen-hu
    last: Hu
  - first: Lei
    full: Lei Zou
    id: lei-zou
    last: Zou
  - first: Xinbo
    full: Xinbo Zhang
    id: xinbo-zhang
    last: Zhang
  author_string: Sen Hu, Lei Zou, Xinbo Zhang
  bibkey: hu-etal-2018-state
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1234
  month: October-November
  page_first: '2098'
  page_last: '2108'
  pages: "2098\u20132108"
  paper_id: '234'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1234.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1234.jpg
  title: A State-transition Framework to Answer Complex Questions over Knowledge Base
  title_html: A State-transition Framework to Answer Complex Questions over Knowledge
    Base
  url: https://www.aclweb.org/anthology/D18-1234
  year: '2018'
D18-1235:
  abstract: The task of machine reading comprehension (MRC) has evolved from answering
    simple questions from well-edited text to answering real questions from users
    out of web data. In the real-world setting, full-body text from multiple relevant
    documents in the top search results are provided as context for questions from
    user queries, including not only questions with a single, short, and factual answer,
    but also questions about reasons, procedures, and opinions. In this case, multiple
    answers could be equally valid for a single question and each answer may occur
    multiple times in the context, which should be taken into consideration when we
    build MRC system. We propose a multi-answer multi-task framework, in which different
    loss functions are used for multiple reference answers. Minimum Risk Training
    is applied to solve the multi-occurrence problem of a single answer. Combined
    with a simple heuristic passage extraction strategy for overlong documents, our
    model increases the ROUGE-L score on the DuReader dataset from 44.18, the previous
    state-of-the-art, to 51.09.
  address: Brussels, Belgium
  author:
  - first: Jiahua
    full: Jiahua Liu
    id: jiahua-liu
    last: Liu
  - first: Wan
    full: Wan Wei
    id: wan-wei
    last: Wei
  - first: Maosong
    full: Maosong Sun
    id: maosong-sun
    last: Sun
  - first: Hao
    full: Hao Chen
    id: hao-chen
    last: Chen
  - first: Yantao
    full: Yantao Du
    id: yantao-du
    last: Du
  - first: Dekang
    full: Dekang Lin
    id: dekang-lin
    last: Lin
  author_string: Jiahua Liu, Wan Wei, Maosong Sun, Hao Chen, Yantao Du, Dekang Lin
  bibkey: liu-etal-2018-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1235
  month: October-November
  page_first: '2109'
  page_last: '2118'
  pages: "2109\u20132118"
  paper_id: '235'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1235.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1235.jpg
  title: A Multi-answer Multi-task Framework for Real-world Machine Reading Comprehension
  title_html: A Multi-answer Multi-task Framework for Real-world Machine Reading Comprehension
  url: https://www.aclweb.org/anthology/D18-1235
  year: '2018'
D18-1236:
  abstract: "We propose the task of Open-Domain Information Narration (OIN) as the\
    \ reverse task of Open Information Extraction (OIE), to implement the dual structure\
    \ between language and knowledge in the open domain. Then, we develop an agent,\
    \ called Orator, to accomplish the OIN task, and assemble the Orator and the recently\
    \ proposed OIE agent \u2014 Logician into a dual system to utilize the duality\
    \ structure with a reinforcement learning paradigm. Experimental results reveal\
    \ the dual structure between OIE and OIN tasks helps to build better both OIE\
    \ agents and OIN agents."
  address: Brussels, Belgium
  author:
  - first: Mingming
    full: Mingming Sun
    id: mingming-sun
    last: Sun
  - first: Xu
    full: Xu Li
    id: xu-li
    last: Li
  - first: Ping
    full: Ping Li
    id: ping-li
    last: Li
  author_string: Mingming Sun, Xu Li, Ping Li
  bibkey: sun-etal-2018-logician
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1236
  month: October-November
  page_first: '2119'
  page_last: '2130'
  pages: "2119\u20132130"
  paper_id: '236'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1236.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1236.jpg
  title: 'Logician and Orator: Learning from the Duality between Language and Knowledge
    in Open Domain'
  title_html: 'Logician and Orator: Learning from the Duality between Language and
    Knowledge in Open Domain'
  url: https://www.aclweb.org/anthology/D18-1236
  year: '2018'
D18-1237:
  abstract: 'Machine reading comprehension helps machines learn to utilize most of
    the human knowledge written in the form of text. Existing approaches made a significant
    progress comparable to human-level performance, but they are still limited in
    understanding, up to a few paragraphs, failing to properly comprehend lengthy
    document. In this paper, we propose a novel deep neural network architecture to
    handle a long-range dependency in RC tasks. In detail, our method has two novel
    aspects: (1) an advanced memory-augmented architecture and (2) an expanded gated
    recurrent unit with dense connections that mitigate potential information distortion
    occurring in the memory. Our proposed architecture is widely applicable to other
    models. We have performed extensive experiments with well-known benchmark datasets
    such as TriviaQA, QUASAR-T, and SQuAD. The experimental results demonstrate that
    the proposed method outperforms existing methods, especially for lengthy documents.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1237.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1237.Attachment.pdf
  author:
  - first: Seohyun
    full: Seohyun Back
    id: seohyun-back
    last: Back
  - first: Seunghak
    full: Seunghak Yu
    id: seunghak-yu
    last: Yu
  - first: Sathish Reddy
    full: Sathish Reddy Indurthi
    id: sathish-reddy-indurthi
    last: Indurthi
  - first: Jihie
    full: Jihie Kim
    id: jihie-kim
    last: Kim
  - first: Jaegul
    full: Jaegul Choo
    id: jaegul-choo
    last: Choo
  author_string: Seohyun Back, Seunghak Yu, Sathish Reddy Indurthi, Jihie Kim, Jaegul
    Choo
  bibkey: back-etal-2018-memoreader
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1237
  month: October-November
  page_first: '2131'
  page_last: '2140'
  pages: "2131\u20132140"
  paper_id: '237'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1237.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1237.jpg
  title: 'MemoReader: Large-Scale Reading Comprehension through Neural Memory Controller'
  title_html: '<span class="acl-fixed-case">M</span>emo<span class="acl-fixed-case">R</span>eader:
    Large-Scale Reading Comprehension through Neural Memory Controller'
  url: https://www.aclweb.org/anthology/D18-1237
  year: '2018'
D18-1238:
  abstract: Sequence encoders are crucial components in many neural architectures
    for learning to read and comprehend. This paper presents a new compositional encoder
    for reading comprehension (RC). Our proposed encoder is not only aimed at being
    fast but also expressive. Specifically, the key novelty behind our encoder is
    that it explicitly models across multiple granularities using a new dilated composition
    mechanism. In our approach, gating functions are learned by modeling relationships
    and reasoning over multi-granular sequence information, enabling compositional
    learning that is aware of both long and short term information. We conduct experiments
    on three RC datasets, showing that our proposed encoder demonstrates very promising
    results both as a standalone encoder as well as a complementary building block.
    Empirical results show that simple Bi-Attentive architectures augmented with our
    proposed encoder not only achieves state-of-the-art / highly competitive results
    but is also considerably faster than other published works.
  address: Brussels, Belgium
  author:
  - first: Yi
    full: Yi Tay
    id: yi-tay
    last: Tay
  - first: Anh Tuan
    full: Anh Tuan Luu
    id: anh-tuan-luu
    last: Luu
  - first: Siu Cheung
    full: Siu Cheung Hui
    id: siu-cheung-hui
    last: Hui
  author_string: Yi Tay, Anh Tuan Luu, Siu Cheung Hui
  bibkey: tay-etal-2018-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1238
  month: October-November
  page_first: '2141'
  page_last: '2151'
  pages: "2141\u20132151"
  paper_id: '238'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1238.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1238.jpg
  title: Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading
    Comprehension
  title_html: Multi-Granular Sequence Encoding via Dilated Compositional Units for
    Reading Comprehension
  url: https://www.aclweb.org/anthology/D18-1238
  year: '2018'
D18-1239:
  abstract: "Answering compositional questions requiring multi-step reasoning is challenging.\
    \ We introduce an end-to-end differentiable model for interpreting questions about\
    \ a knowledge graph (KG), which is inspired by formal approaches to semantics.\
    \ Each span of text is represented by a denotation in a KG and a vector that captures\
    \ ungrounded aspects of meaning. Learned composition modules recursively combine\
    \ constituent spans, culminating in a grounding for the complete sentence which\
    \ answers the question. For example, to interpret \u201Cnot green\u201D, the model\
    \ represents \u201Cgreen\u201D as a set of KG entities and \u201Cnot\u201D as\
    \ a trainable ungrounded vector\u2014and then uses this vector to parameterize\
    \ a composition function that performs a complement operation. For each sentence,\
    \ we build a parse chart subsuming all possible parses, allowing the model to\
    \ jointly learn both the composition operators and output structure by gradient\
    \ descent from end-task supervision. The model learns a variety of challenging\
    \ semantic operators, such as quantifiers, disjunctions and composed relations,\
    \ and infers latent syntactic structure. It also generalizes well to longer questions\
    \ than seen in its training data, in contrast to RNN, its tree-based variants,\
    \ and semantic parsing baselines."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1239.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1239.Attachment.pdf
  author:
  - first: Nitish
    full: Nitish Gupta
    id: nitish-gupta
    last: Gupta
  - first: Mike
    full: Mike Lewis
    id: mike-lewis
    last: Lewis
  author_string: Nitish Gupta, Mike Lewis
  bibkey: gupta-lewis-2018-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1239
  month: October-November
  page_first: '2152'
  page_last: '2161'
  pages: "2152\u20132161"
  paper_id: '239'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1239.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1239.jpg
  title: Neural Compositional Denotational Semantics for Question Answering
  title_html: Neural Compositional Denotational Semantics for Question Answering
  url: https://www.aclweb.org/anthology/D18-1239
  year: '2018'
D18-1240:
  abstract: High-level semantics tasks, e.g., paraphrasing, textual entailment or
    question answering, involve modeling of text pairs. Before the emergence of neural
    networks, this has been mostly performed using intra-pair features, which incorporate
    similarity scores or rewrite rules computed between the members within the same
    pair. In this paper, we compute scalar products between vectors representing similarity
    between members of different pairs, in place of simply using a single vector for
    each pair. This allows us to obtain a representation specific to any pair of pairs,
    which delivers the state of the art in answer sentence selection. Most importantly,
    our approach can outperform much more complex algorithms based on neural networks.
  address: Brussels, Belgium
  author:
  - first: Kateryna
    full: Kateryna Tymoshenko
    id: kateryna-tymoshenko
    last: Tymoshenko
  - first: Alessandro
    full: Alessandro Moschitti
    id: alessandro-moschitti
    last: Moschitti
  author_string: Kateryna Tymoshenko, Alessandro Moschitti
  bibkey: tymoshenko-moschitti-2018-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1240
  month: October-November
  page_first: '2162'
  page_last: '2173'
  pages: "2162\u20132173"
  paper_id: '240'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1240.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1240.jpg
  title: Cross-Pair Text Representations for Answer Sentence Selection
  title_html: Cross-Pair Text Representations for Answer Sentence Selection
  url: https://www.aclweb.org/anthology/D18-1240
  year: '2018'
D18-1241:
  abstract: 'We present QuAC, a dataset for Question Answering in Context that contains
    14K information-seeking QA dialogs (100K questions in total). The dialogs involve
    two crowd workers: (1) a student who poses a sequence of freeform questions to
    learn as much as possible about a hidden Wikipedia text, and (2) a teacher who
    answers the questions by providing short excerpts from the text. QuAC introduces
    challenges not found in existing machine comprehension datasets: its questions
    are often more open-ended, unanswerable, or only meaningful within the dialog
    context, as we show in a detailed qualitative evaluation. We also report results
    for a number of reference models, including a recently state-of-the-art reading
    comprehension architecture extended to model dialog context. Our best model underperforms
    humans by 20 F1, suggesting that there is significant room for future work on
    this data. Dataset, baseline, and leaderboard available at http://quac.ai.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1241.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1241.Attachment.zip
  - filename: D18-1241.Poster.pdf
    type: poster
    url: https://www.aclweb.org/anthology/attachments/D18-1241.Poster.pdf
  author:
  - first: Eunsol
    full: Eunsol Choi
    id: eunsol-choi
    last: Choi
  - first: He
    full: He He
    id: he-he
    last: He
  - first: Mohit
    full: Mohit Iyyer
    id: mohit-iyyer
    last: Iyyer
  - first: Mark
    full: Mark Yatskar
    id: mark-yatskar
    last: Yatskar
  - first: Wen-tau
    full: Wen-tau Yih
    id: wen-tau-yih
    last: Yih
  - first: Yejin
    full: Yejin Choi
    id: yejin-choi
    last: Choi
  - first: Percy
    full: Percy Liang
    id: percy-liang
    last: Liang
  - first: Luke
    full: Luke Zettlemoyer
    id: luke-zettlemoyer
    last: Zettlemoyer
  author_string: Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin
    Choi, Percy Liang, Luke Zettlemoyer
  bibkey: choi-etal-2018-quac
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1241
  month: October-November
  page_first: '2174'
  page_last: '2184'
  pages: "2174\u20132184"
  paper_id: '241'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1241.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1241.jpg
  title: 'QuAC: Question Answering in Context'
  title_html: '<span class="acl-fixed-case">Q</span>u<span class="acl-fixed-case">AC</span>:
    Question Answering in Context'
  url: https://www.aclweb.org/anthology/D18-1241
  year: '2018'
D18-1242:
  abstract: Answering complex questions that involve multiple entities and multiple
    relations using a standard knowledge base is an open and challenging task. Most
    existing KBQA approaches focus on simpler questions and do not work very well
    on complex questions because they were not able to simultaneously represent the
    question and the corresponding complex query structure. In this work, we encode
    such complex query structure into a uniform vector representation, and thus successfully
    capture the interactions between individual semantic components within a complex
    question. This approach consistently outperforms existing methods on complex questions
    while staying competitive on simple questions.
  address: Brussels, Belgium
  author:
  - first: Kangqi
    full: Kangqi Luo
    id: kangqi-luo
    last: Luo
  - first: Fengli
    full: Fengli Lin
    id: fengli-lin
    last: Lin
  - first: Xusheng
    full: Xusheng Luo
    id: xusheng-luo
    last: Luo
  - first: Kenny
    full: Kenny Zhu
    id: kenny-zhu
    last: Zhu
  author_string: Kangqi Luo, Fengli Lin, Xusheng Luo, Kenny Zhu
  bibkey: luo-etal-2018-knowledge
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1242
  month: October-November
  page_first: '2185'
  page_last: '2194'
  pages: "2185\u20132194"
  paper_id: '242'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1242.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1242.jpg
  title: Knowledge Base Question Answering via Encoding of Complex Query Graphs
  title_html: Knowledge Base Question Answering via Encoding of Complex Query Graphs
  url: https://www.aclweb.org/anthology/D18-1242
  year: '2018'
D18-1243:
  abstract: Extracting relations is critical for knowledge base completion and construction
    in which distant supervised methods are widely used to extract relational facts
    automatically with the existing knowledge bases. However, the automatically constructed
    datasets comprise amounts of low-quality sentences containing noisy words, which
    is neglected by current distant supervised methods resulting in unacceptable precisions.
    To mitigate this problem, we propose a novel word-level distant supervised approach
    for relation extraction. We first build Sub-Tree Parse(STP) to remove noisy words
    that are irrelevant to relations. Then we construct a neural network inputting
    the sub-tree while applying the entity-wise attention to identify the important
    semantic features of relational words in each instance. To make our model more
    robust against noisy words, we initialize our network with a priori knowledge
    learned from the relevant task of entity classification by transfer learning.
    We conduct extensive experiments using the corpora of New York Times(NYT) and
    Freebase. Experiments show that our approach is effective and improves the area
    of Precision/Recall(PR) from 0.35 to 0.39 over the state-of-the-art work.
  address: Brussels, Belgium
  author:
  - first: Tianyi
    full: Tianyi Liu
    id: tianyi-liu
    last: Liu
  - first: Xinsong
    full: Xinsong Zhang
    id: xinsong-zhang
    last: Zhang
  - first: Wanhao
    full: Wanhao Zhou
    id: wanhao-zhou
    last: Zhou
  - first: Weijia
    full: Weijia Jia
    id: weijia-jia
    last: Jia
  author_string: Tianyi Liu, Xinsong Zhang, Wanhao Zhou, Weijia Jia
  bibkey: liu-etal-2018-neural-relation
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1243
  month: October-November
  page_first: '2195'
  page_last: '2204'
  pages: "2195\u20132204"
  paper_id: '243'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1243.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1243.jpg
  title: Neural Relation Extraction via Inner-Sentence Noise Reduction and Transfer
    Learning
  title_html: Neural Relation Extraction via Inner-Sentence Noise Reduction and Transfer
    Learning
  url: https://www.aclweb.org/anthology/D18-1243
  year: '2018'
D18-1244:
  abstract: Dependency trees help relation extraction models capture long-range relations
    between words. However, existing dependency-based models either neglect crucial
    information (e.g., negation) by pruning the dependency trees too aggressively,
    or are computationally inefficient because it is difficult to parallelize over
    different tree structures. We propose an extension of graph convolutional networks
    that is tailored for relation extraction, which pools information over arbitrary
    dependency structures efficiently in parallel. To incorporate relevant information
    while maximally removing irrelevant content, we further apply a novel pruning
    strategy to the input trees by keeping words immediately around the shortest path
    between the two entities among which a relation might hold. The resulting model
    achieves state-of-the-art performance on the large-scale TACRED dataset, outperforming
    existing sequence and dependency-based neural models. We also show through detailed
    analysis that this model has complementary strengths to sequence models, and combining
    them further improves the state of the art.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1244.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1244.Attachment.pdf
  author:
  - first: Yuhao
    full: Yuhao Zhang
    id: yuhao-zhang
    last: Zhang
  - first: Peng
    full: Peng Qi
    id: peng-qi
    last: Qi
  - first: Christopher D.
    full: Christopher D. Manning
    id: christopher-d-manning
    last: Manning
  author_string: Yuhao Zhang, Peng Qi, Christopher D. Manning
  bibkey: zhang-etal-2018-graph
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1244
  month: October-November
  page_first: '2205'
  page_last: '2215'
  pages: "2205\u20132215"
  paper_id: '244'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1244.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1244.jpg
  title: Graph Convolution over Pruned Dependency Trees Improves Relation Extraction
  title_html: Graph Convolution over Pruned Dependency Trees Improves Relation Extraction
  url: https://www.aclweb.org/anthology/D18-1244
  year: '2018'
D18-1245:
  abstract: Attention mechanism is often used in deep neural networks for distantly
    supervised relation extraction (DS-RE) to distinguish valid from noisy instances.
    However, traditional 1-D vector attention model is insufficient for learning of
    different contexts in the selection of valid instances to predict the relationship
    for an entity pair. To alleviate this issue, we propose a novel multi-level structured
    (2-D matrix) self-attention mechanism for DS-RE in a multi-instance learning (MIL)
    framework using bidirectional recurrent neural networks (BiRNN). In the proposed
    method, a structured word-level self-attention learns a 2-D matrix where each
    row vector represents a weight distribution for different aspects of an instance
    regarding two entities. Targeting the MIL issue, the structured sentence-level
    attention learns a 2-D matrix where each row vector represents a weight distribution
    on selection of different valid instances. Experiments conducted on two publicly
    available DS-RE datasets show that the proposed framework with multi-level structured
    self-attention mechanism significantly outperform baselines in terms of PR curves,
    P@N and F1 measures.
  address: Brussels, Belgium
  author:
  - first: Jinhua
    full: Jinhua Du
    id: jinhua-du
    last: Du
  - first: Jingguang
    full: Jingguang Han
    id: jingguang-han
    last: Han
  - first: Andy
    full: Andy Way
    id: andy-way
    last: Way
  - first: Dadong
    full: Dadong Wan
    id: dadong-wan
    last: Wan
  author_string: Jinhua Du, Jingguang Han, Andy Way, Dadong Wan
  bibkey: du-etal-2018-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1245
  month: October-November
  page_first: '2216'
  page_last: '2225'
  pages: "2216\u20132225"
  paper_id: '245'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1245.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1245.jpg
  title: Multi-Level Structured Self-Attentions for Distantly Supervised Relation
    Extraction
  title_html: Multi-Level Structured Self-Attentions for Distantly Supervised Relation
    Extraction
  url: https://www.aclweb.org/anthology/D18-1245
  year: '2018'
D18-1246:
  abstract: Cross-sentence n-ary relation extraction detects relations among -ary
    relation extraction detects relations among n entities across multiple sentences.
    Typical methods formulate an input as a entities across multiple sentences. Typical
    methods formulate an input as a document graph, integrating various intra-sentential
    and inter-sentential dependencies. The current state-of-the-art method splits
    the input graph into two DAGs, adopting a DAG-structured LSTM for each. Though
    being able to model rich linguistic knowledge by leveraging graph edges, important
    information can be lost in the splitting procedure. We propose a graph-state LSTM
    model, which uses a parallel state to model each word, recurrently enriching state
    values via message passing. Compared with DAG LSTMs, our graph LSTM keeps the
    original graph structure, and speeds up computation by allowing more parallelization.
    On a standard benchmark, our model shows the best result in the literature.
  address: Brussels, Belgium
  author:
  - first: Linfeng
    full: Linfeng Song
    id: linfeng-song
    last: Song
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  - first: Zhiguo
    full: Zhiguo Wang
    id: zhiguo-wang
    last: Wang
  - first: Daniel
    full: Daniel Gildea
    id: daniel-gildea
    last: Gildea
  author_string: Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel Gildea
  bibkey: song-etal-2018-n
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1246
  month: October-November
  page_first: '2226'
  page_last: '2235'
  pages: "2226\u20132235"
  paper_id: '246'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1246.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1246.jpg
  title: N-ary Relation Extraction using Graph-State LSTM
  title_html: N-ary Relation Extraction using Graph-State <span class="acl-fixed-case">LSTM</span>
  url: https://www.aclweb.org/anthology/D18-1246
  year: '2018'
D18-1247:
  abstract: Distantly supervised relation extraction employs existing knowledge graphs
    to automatically collect training data. While distant supervision is effective
    to scale relation extraction up to large-scale corpora, it inevitably suffers
    from the wrong labeling problem. Many efforts have been devoted to identifying
    valid instances from noisy data. However, most existing methods handle each relation
    in isolation, regardless of rich semantic correlations located in relation hierarchies.
    In this paper, we aim to incorporate the hierarchical information of relations
    for distantly supervised relation extraction and propose a novel hierarchical
    attention scheme. The multiple layers of our hierarchical attention scheme provide
    coarse-to-fine granularity to better identify valid instances, which is especially
    effective for extracting those long-tail relations. The experimental results on
    a large-scale benchmark dataset demonstrate that our models are capable of modeling
    the hierarchical information of relations and significantly outperform other baselines.
    The source code of this paper can be obtained from https://github.com/thunlp/HNRE.
  address: Brussels, Belgium
  author:
  - first: Xu
    full: Xu Han
    id: xu-han
    last: Han
  - first: Pengfei
    full: Pengfei Yu
    id: pengfei-yu
    last: Yu
  - first: Zhiyuan
    full: Zhiyuan Liu
    id: zhiyuan-liu
    last: Liu
  - first: Maosong
    full: Maosong Sun
    id: maosong-sun
    last: Sun
  - first: Peng
    full: Peng Li
    id: peng-li
    last: Li
  author_string: Xu Han, Pengfei Yu, Zhiyuan Liu, Maosong Sun, Peng Li
  bibkey: han-etal-2018-hierarchical
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1247
  month: October-November
  page_first: '2236'
  page_last: '2245'
  pages: "2236\u20132245"
  paper_id: '247'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1247.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1247.jpg
  title: Hierarchical Relation Extraction with Coarse-to-Fine Grained Attention
  title_html: Hierarchical Relation Extraction with Coarse-to-Fine Grained Attention
  url: https://www.aclweb.org/anthology/D18-1247
  year: '2018'
D18-1248:
  abstract: Distant supervision is an effective method to generate large scale labeled
    data for relation extraction, which assumes that if a pair of entities appears
    in some relation of a Knowledge Graph (KG), all sentences containing those entities
    in a large unlabeled corpus are then labeled with that relation to train a relation
    classifier. However, when the pair of entities has multiple relationships in the
    KG, this assumption may produce noisy relation labels. This paper proposes a label-free
    distant supervision method, which makes no use of the relation labels under this
    inadequate assumption, but only uses the prior knowledge derived from the KG to
    supervise the learning of the classifier directly and softly. Specifically, we
    make use of the type information and the translation law derived from typical
    KG embedding model to learn embeddings for certain sentence patterns. As the supervision
    signal is only determined by the two aligned entities, neither hard relation labels
    nor extra noise-reduction model for the bag of sentences is needed in this way.
    The experiments show that the approach performs well in current distant supervision
    dataset.
  address: Brussels, Belgium
  author:
  - first: Guanying
    full: Guanying Wang
    id: guanying-wang
    last: Wang
  - first: Wen
    full: Wen Zhang
    id: wen-zhang
    last: Zhang
  - first: Ruoxu
    full: Ruoxu Wang
    id: ruoxu-wang
    last: Wang
  - first: Yalin
    full: Yalin Zhou
    id: yalin-zhou
    last: Zhou
  - first: Xi
    full: Xi Chen
    id: xi-chen
    last: Chen
  - first: Wei
    full: Wei Zhang
    id: wei-zhang
    last: Zhang
  - first: Hai
    full: Hai Zhu
    id: hai-zhu
    last: Zhu
  - first: Huajun
    full: Huajun Chen
    id: huajun-chen
    last: Chen
  author_string: Guanying Wang, Wen Zhang, Ruoxu Wang, Yalin Zhou, Xi Chen, Wei Zhang,
    Hai Zhu, Huajun Chen
  bibkey: wang-etal-2018-label-free
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1248
  month: October-November
  page_first: '2246'
  page_last: '2255'
  pages: "2246\u20132255"
  paper_id: '248'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1248.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1248.jpg
  title: Label-Free Distant Supervision for Relation Extraction via Knowledge Graph
    Embedding
  title_html: Label-Free Distant Supervision for Relation Extraction via Knowledge
    Graph Embedding
  url: https://www.aclweb.org/anthology/D18-1248
  year: '2018'
D18-1249:
  abstract: We investigate the task of joint entity relation extraction. Unlike prior
    efforts, we propose a new lightweight joint learning paradigm based on minimum
    risk training (MRT). Specifically, our algorithm optimizes a global loss function
    which is flexible and effective to explore interactions between the entity model
    and the relation model. We implement a strong and simple neural network where
    the MRT is executed. Experiment results on the benchmark ACE05 and NYT datasets
    show that our model is able to achieve state-of-the-art joint extraction performances.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1249.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1249.Attachment.zip
  author:
  - first: Changzhi
    full: Changzhi Sun
    id: changzhi-sun
    last: Sun
  - first: Yuanbin
    full: Yuanbin Wu
    id: yuanbin-wu
    last: Wu
  - first: Man
    full: Man Lan
    id: man-lan
    last: Lan
  - first: Shiliang
    full: Shiliang Sun
    id: shiliang-sun
    last: Sun
  - first: Wenting
    full: Wenting Wang
    id: wenting-wang
    last: Wang
  - first: Kuang-Chih
    full: Kuang-Chih Lee
    id: kuang-chih-lee
    last: Lee
  - first: Kewen
    full: Kewen Wu
    id: kewen-wu
    last: Wu
  author_string: Changzhi Sun, Yuanbin Wu, Man Lan, Shiliang Sun, Wenting Wang, Kuang-Chih
    Lee, Kewen Wu
  bibkey: sun-etal-2018-extracting
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1249
  month: October-November
  page_first: '2256'
  page_last: '2265'
  pages: "2256\u20132265"
  paper_id: '249'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1249.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1249.jpg
  title: Extracting Entities and Relations with Joint Minimum Risk Training
  title_html: Extracting Entities and Relations with Joint Minimum Risk Training
  url: https://www.aclweb.org/anthology/D18-1249
  year: '2018'
D18-1250:
  abstract: "Experimental performance on the task of relation classification has generally\
    \ improved using deep neural network architectures. One major drawback of reported\
    \ studies is that individual models have been evaluated on a very narrow range\
    \ of datasets, raising questions about the adaptability of the architectures,\
    \ while making comparisons between approaches difficult. In this work, we present\
    \ a systematic large-scale analysis of neural relation classification architectures\
    \ on six benchmark datasets with widely varying characteristics. We propose a\
    \ novel multi-channel LSTM model combined with a CNN that takes advantage of all\
    \ currently popular linguistic and architectural features. Our \u2018Man for All\
    \ Seasons\u2019 approach achieves state-of-the-art performance on two datasets.\
    \ More importantly, in our view, the model allowed us to obtain direct insights\
    \ into the continued challenges faced by neural language models on this task."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1250.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1250.Attachment.pdf
  author:
  - first: Hoang-Quynh
    full: Hoang-Quynh Le
    id: hoang-quynh-le
    last: Le
  - first: Duy-Cat
    full: Duy-Cat Can
    id: duy-cat-can
    last: Can
  - first: Sinh T.
    full: Sinh T. Vu
    id: sinh-t-vu
    last: Vu
  - first: Thanh Hai
    full: Thanh Hai Dang
    id: thanh-hai-dang
    last: Dang
  - first: Mohammad Taher
    full: Mohammad Taher Pilehvar
    id: mohammad-taher-pilehvar
    last: Pilehvar
  - first: Nigel
    full: Nigel Collier
    id: nigel-collier
    last: Collier
  author_string: Hoang-Quynh Le, Duy-Cat Can, Sinh T. Vu, Thanh Hai Dang, Mohammad
    Taher Pilehvar, Nigel Collier
  bibkey: le-etal-2018-large
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1250
  month: October-November
  page_first: '2266'
  page_last: '2277'
  pages: "2266\u20132277"
  paper_id: '250'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1250.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1250.jpg
  title: Large-scale Exploration of Neural Relation Classification Architectures
  title_html: Large-scale Exploration of Neural Relation Classification Architectures
  url: https://www.aclweb.org/anthology/D18-1250
  year: '2018'
D18-1251:
  abstract: This paper presents a corpus and experimental results to extract possession
    relations over time. We work with Wikipedia articles about artworks, and extract
    possession relations along with temporal information indicating when these relations
    are true. The annotation scheme yields many possessors over time for a given artwork,
    and experimental results show that an LSTM ensemble can automate the task.
  address: Brussels, Belgium
  author:
  - first: Dhivya
    full: Dhivya Chinnappa
    id: dhivya-chinnappa
    last: Chinnappa
  - first: Eduardo
    full: Eduardo Blanco
    id: eduardo-blanco
    last: Blanco
  author_string: Dhivya Chinnappa, Eduardo Blanco
  bibkey: chinnappa-blanco-2018-possessors
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1251
  month: October-November
  page_first: '2278'
  page_last: '2287'
  pages: "2278\u20132287"
  paper_id: '251'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1251.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1251.jpg
  title: 'Possessors Change Over Time: A Case Study with Artworks'
  title_html: 'Possessors Change Over Time: A Case Study with Artworks'
  url: https://www.aclweb.org/anthology/D18-1251
  year: '2018'
D18-1252:
  abstract: 'Referring to entities in situated dialog is a collaborative process,
    whereby interlocutors often expand, repair and/or replace referring expressions
    in an iterative process, converging on conceptual pacts of referring language
    use in doing so. Nevertheless, much work on exophoric reference resolution (i.e.
    resolution of references to entities outside of a given text) follows a literary
    model, whereby individual referring expressions are interpreted as unique identifiers
    of their referents given the state of the dialog the referring expression is initiated.
    In this paper, we address this collaborative nature to improve dialogic reference
    resolution in two ways: First, we trained a words-as-classifiers logistic regression
    model of word semantics and incrementally adapt the model to idiosyncratic language
    between dyad partners during evaluation of the dialog. We then used these semantic
    models to learn the general referring ability of each word, which is independent
    of referent features. These methods facilitate accurate automatic reference resolution
    in situated dialog without annotation of referring expressions, even with little
    background data.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1252.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1252.Attachment.pdf
  - filename: https://vimeo.com/305936322
    type: video
    url: https://vimeo.com/305936322
  author:
  - first: Todd
    full: Todd Shore
    id: todd-shore
    last: Shore
  - first: Gabriel
    full: Gabriel Skantze
    id: gabriel-skantze
    last: Skantze
  author_string: Todd Shore, Gabriel Skantze
  bibkey: shore-skantze-2018-using
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1252
  month: October-November
  page_first: '2288'
  page_last: '2297'
  pages: "2288\u20132297"
  paper_id: '252'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1252.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1252.jpg
  title: Using Lexical Alignment and Referring Ability to Address Data Sparsity in
    Situated Dialog Reference Resolution
  title_html: Using Lexical Alignment and Referring Ability to Address Data Sparsity
    in Situated Dialog Reference Resolution
  url: https://www.aclweb.org/anthology/D18-1252
  year: '2018'
D18-1253:
  abstract: Developing agents to engage in complex goal-oriented dialogues is challenging
    partly because the main learning signals are very sparse in long conversations.
    In this paper, we propose a divide-and-conquer approach that discovers and exploits
    the hidden structure of the task to enable efficient policy learning. First, given
    successful example dialogues, we propose the Subgoal Discovery Network (SDN) to
    divide a complex goal-oriented task into a set of simpler subgoals in an unsupervised
    fashion. We then use these subgoals to learn a multi-level policy by hierarchical
    reinforcement learning. We demonstrate our method by building a dialogue agent
    for the composite task of travel planning. Experiments with simulated and real
    users show that our approach performs competitively against a state-of-the-art
    method that requires human-defined subgoals. Moreover, we show that the learned
    subgoals are often human comprehensible.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305937184
    type: video
    url: https://vimeo.com/305937184
  author:
  - first: Da
    full: Da Tang
    id: da-tang
    last: Tang
  - first: Xiujun
    full: Xiujun Li
    id: xiujun-li
    last: Li
  - first: Jianfeng
    full: Jianfeng Gao
    id: jianfeng-gao
    last: Gao
  - first: Chong
    full: Chong Wang
    id: chong-wang
    last: Wang
  - first: Lihong
    full: Lihong Li
    id: lihong-li
    last: Li
  - first: Tony
    full: Tony Jebara
    id: tony-jebara
    last: Jebara
  author_string: Da Tang, Xiujun Li, Jianfeng Gao, Chong Wang, Lihong Li, Tony Jebara
  bibkey: tang-etal-2018-subgoal
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1253
  month: October-November
  page_first: '2298'
  page_last: '2309'
  pages: "2298\u20132309"
  paper_id: '253'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1253.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1253.jpg
  title: Subgoal Discovery for Hierarchical Dialogue Policy Learning
  title_html: Subgoal Discovery for Hierarchical Dialogue Policy Learning
  url: https://www.aclweb.org/anthology/D18-1253
  year: '2018'
D18-1254:
  abstract: Modern automated dialog systems require complex dialog managers able to
    deal with user intent triggered by high-level semantic questions. In this paper,
    we propose a model for automatically clustering questions into user intents to
    help the design tasks. Since questions are short texts, uncovering their semantics
    to group them together can be very challenging. We approach the problem by using
    powerful semantic classifiers from question duplicate/matching research along
    with a novel idea of supervised clustering methods based on structured output.
    We test our approach on two intent clustering corpora, showing an impressive improvement
    over previous methods for two languages/domains.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305938531
    type: video
    url: https://vimeo.com/305938531
  author:
  - first: Iryna
    full: Iryna Haponchyk
    id: iryna-haponchyk
    last: Haponchyk
  - first: Antonio
    full: Antonio Uva
    id: antonio-uva
    last: Uva
  - first: Seunghak
    full: Seunghak Yu
    id: seunghak-yu
    last: Yu
  - first: Olga
    full: Olga Uryupina
    id: olga-uryupina
    last: Uryupina
  - first: Alessandro
    full: Alessandro Moschitti
    id: alessandro-moschitti
    last: Moschitti
  author_string: Iryna Haponchyk, Antonio Uva, Seunghak Yu, Olga Uryupina, Alessandro
    Moschitti
  bibkey: haponchyk-etal-2018-supervised
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1254
  month: October-November
  page_first: '2310'
  page_last: '2321'
  pages: "2310\u20132321"
  paper_id: '254'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1254.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1254.jpg
  title: Supervised Clustering of Questions into Intents for Dialog System Applications
  title_html: Supervised Clustering of Questions into Intents for Dialog System Applications
  url: https://www.aclweb.org/anthology/D18-1254
  year: '2018'
D18-1255:
  abstract: 'Existing dialog datasets contain a sequence of utterances and responses
    without any explicit background knowledge associated with them. This has resulted
    in the development of models which treat conversation as a sequence-to-sequence
    generation task (i.e., given a sequence of utterances generate the response sequence).
    This is not only an overly simplistic view of conversation but it is also emphatically
    different from the way humans converse by heavily relying on their background
    knowledge about the topic (as opposed to simply relying on the previous sequence
    of utterances). For example, it is common for humans to (involuntarily) produce
    utterances which are copied or suitably modified from background articles they
    have read about the topic. To facilitate the development of such natural conversation
    models which mimic the human process of conversing, we create a new dataset containing
    movie chats wherein each response is explicitly generated by copying and/or modifying
    sentences from unstructured background knowledge such as plots, comments and reviews
    about the movie. We establish baseline results on this dataset (90K utterances
    from 9K conversations) using three different models: (i) pure generation based
    models which ignore the background knowledge (ii) generation based models which
    learn to copy information from the background knowledge when required and (iii)
    span prediction based models which predict the appropriate response span in the
    background knowledge.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1255.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1255.Attachment.zip
  - filename: https://vimeo.com/305939688
    type: video
    url: https://vimeo.com/305939688
  author:
  - first: Nikita
    full: Nikita Moghe
    id: nikita-moghe
    last: Moghe
  - first: Siddhartha
    full: Siddhartha Arora
    id: siddhartha-arora
    last: Arora
  - first: Suman
    full: Suman Banerjee
    id: suman-banerjee
    last: Banerjee
  - first: Mitesh M.
    full: Mitesh M. Khapra
    id: mitesh-m-khapra
    last: Khapra
  author_string: Nikita Moghe, Siddhartha Arora, Suman Banerjee, Mitesh M. Khapra
  bibkey: moghe-etal-2018-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1255
  month: October-November
  page_first: '2322'
  page_last: '2332'
  pages: "2322\u20132332"
  paper_id: '255'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1255.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1255.jpg
  title: Towards Exploiting Background Knowledge for Building Conversation Systems
  title_html: Towards Exploiting Background Knowledge for Building Conversation Systems
  url: https://www.aclweb.org/anthology/D18-1255
  year: '2018'
D18-1256:
  abstract: "We consider negotiation settings in which two agents use natural language\
    \ to bargain on goods. Agents need to decide on both high-level strategy (e.g.,\
    \ proposing $50) and the execution of that strategy (e.g., generating \u201CThe\
    \ bike is brand new. Selling for just $50!\u201D). Recent work on negotiation\
    \ trains neural models, but their end-to-end nature makes it hard to control their\
    \ strategy, and reinforcement learning tends to lead to degenerate solutions.\
    \ In this paper, we propose a modular approach based on coarse dialogue acts (e.g.,\
    \ propose(price=50)) that decouples strategy and generation. We show that we can\
    \ flexibly set the strategy using supervised learning, reinforcement learning,\
    \ or domain-specific knowledge without degeneracy, while our retrieval-based generation\
    \ can maintain context-awareness and produce diverse utterances. We test our approach\
    \ on the recently proposed DEALORNODEAL game, and we also collect a richer dataset\
    \ based on real items on Craigslist. Human evaluation shows that our systems achieve\
    \ higher task success rate and more human-like negotiation behavior than previous\
    \ approaches."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1256.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1256.Attachment.pdf
  - filename: https://vimeo.com/305940786
    type: video
    url: https://vimeo.com/305940786
  author:
  - first: He
    full: He He
    id: he-he
    last: He
  - first: Derek
    full: Derek Chen
    id: derek-chen
    last: Chen
  - first: Anusha
    full: Anusha Balakrishnan
    id: anusha-balakrishnan
    last: Balakrishnan
  - first: Percy
    full: Percy Liang
    id: percy-liang
    last: Liang
  author_string: He He, Derek Chen, Anusha Balakrishnan, Percy Liang
  bibkey: he-etal-2018-decoupling
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1256
  month: October-November
  page_first: '2333'
  page_last: '2343'
  pages: "2333\u20132343"
  paper_id: '256'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1256.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1256.jpg
  title: Decoupling Strategy and Generation in Negotiation Dialogues
  title_html: Decoupling Strategy and Generation in Negotiation Dialogues
  url: https://www.aclweb.org/anthology/D18-1256
  year: '2018'
D18-1257:
  abstract: "Cloze tests are widely adopted in language exams to evaluate students\u2019\
    \ language proficiency. In this paper, we propose the first large-scale human-created\
    \ cloze test dataset CLOTH, containing questions used in middle-school and high-school\
    \ language exams. With missing blanks carefully created by teachers and candidate\
    \ choices purposely designed to be nuanced, CLOTH requires a deeper language understanding\
    \ and a wider attention span than previously automatically-generated cloze datasets.\
    \ We test the performance of dedicatedly designed baseline models including a\
    \ language model trained on the One Billion Word Corpus and show humans outperform\
    \ them by a significant margin. We investigate the source of the performance gap,\
    \ trace model deficiencies to some distinct properties of CLOTH, and identify\
    \ the limited ability of comprehending the long-term context to be the key bottleneck."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305886563
    type: video
    url: https://vimeo.com/305886563
  author:
  - first: Qizhe
    full: Qizhe Xie
    id: qizhe-xie
    last: Xie
  - first: Guokun
    full: Guokun Lai
    id: guokun-lai
    last: Lai
  - first: Zihang
    full: Zihang Dai
    id: zihang-dai
    last: Dai
  - first: Eduard
    full: Eduard Hovy
    id: eduard-hovy
    last: Hovy
  author_string: Qizhe Xie, Guokun Lai, Zihang Dai, Eduard Hovy
  bibkey: xie-etal-2018-large
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1257
  month: October-November
  page_first: '2344'
  page_last: '2356'
  pages: "2344\u20132356"
  paper_id: '257'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1257.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1257.jpg
  title: Large-scale Cloze Test Dataset Created by Teachers
  title_html: Large-scale Cloze Test Dataset Created by Teachers
  url: https://www.aclweb.org/anthology/D18-1257
  year: '2018'
D18-1258:
  abstract: We propose a novel methodology to generate domain-specific large-scale
    question answering (QA) datasets by re-purposing existing annotations for other
    NLP tasks. We demonstrate an instance of this methodology in generating a large-scale
    QA dataset for electronic medical records by leveraging existing expert annotations
    on clinical notes for various NLP tasks from the community shared i2b2 datasets.
    The resulting corpus (emrQA) has 1 million questions-logical form and 400,000+
    question-answer evidence pairs. We characterize the dataset and explore its learning
    potential by training baseline models for question to logical form and question
    to answer mapping.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305887077
    type: video
    url: https://vimeo.com/305887077
  author:
  - first: Anusri
    full: Anusri Pampari
    id: anusri-pampari
    last: Pampari
  - first: Preethi
    full: Preethi Raghavan
    id: preethi-raghavan
    last: Raghavan
  - first: Jennifer
    full: Jennifer Liang
    id: jennifer-liang
    last: Liang
  - first: Jian
    full: Jian Peng
    id: jian-peng
    last: Peng
  author_string: Anusri Pampari, Preethi Raghavan, Jennifer Liang, Jian Peng
  bibkey: pampari-etal-2018-emrqa
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1258
  month: October-November
  page_first: '2357'
  page_last: '2368'
  pages: "2357\u20132368"
  paper_id: '258'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1258.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1258.jpg
  title: 'emrQA: A Large Corpus for Question Answering on Electronic Medical Records'
  title_html: 'emr<span class="acl-fixed-case">QA</span>: A Large Corpus for Question
    Answering on Electronic Medical Records'
  url: https://www.aclweb.org/anthology/D18-1258
  year: '2018'
D18-1259:
  abstract: "Existing question answering (QA) datasets fail to train QA systems to\
    \ perform complex reasoning and provide explanations for answers. We introduce\
    \ HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with\
    \ four key features: (1) the questions require finding and reasoning over multiple\
    \ supporting documents to answer; (2) the questions are diverse and not constrained\
    \ to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level\
    \ supporting facts required for reasoning, allowing QA systems to reason with\
    \ strong supervision and explain the predictions; (4) we offer a new type of factoid\
    \ comparison questions to test QA systems\u2019 ability to extract relevant facts\
    \ and perform necessary comparison. We show that HotpotQA is challenging for the\
    \ latest QA systems, and the supporting facts enable models to improve performance\
    \ and make explainable predictions."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1259.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1259.Attachment.pdf
  - filename: https://vimeo.com/305887533
    type: video
    url: https://vimeo.com/305887533
  author:
  - first: Zhilin
    full: Zhilin Yang
    id: zhilin-yang
    last: Yang
  - first: Peng
    full: Peng Qi
    id: peng-qi
    last: Qi
  - first: Saizheng
    full: Saizheng Zhang
    id: saizheng-zhang
    last: Zhang
  - first: Yoshua
    full: Yoshua Bengio
    id: yoshua-bengio
    last: Bengio
  - first: William
    full: William Cohen
    id: william-cohen
    last: Cohen
  - first: Ruslan
    full: Ruslan Salakhutdinov
    id: ruslan-salakhutdinov
    last: Salakhutdinov
  - first: Christopher D.
    full: Christopher D. Manning
    id: christopher-d-manning
    last: Manning
  author_string: Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen,
    Ruslan Salakhutdinov, Christopher D. Manning
  bibkey: yang-etal-2018-hotpotqa
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1259
  month: October-November
  page_first: '2369'
  page_last: '2380'
  pages: "2369\u20132380"
  paper_id: '259'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1259.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1259.jpg
  title: 'HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering'
  title_html: '<span class="acl-fixed-case">H</span>otpot<span class="acl-fixed-case">QA</span>:
    A Dataset for Diverse, Explainable Multi-hop Question Answering'
  url: https://www.aclweb.org/anthology/D18-1259
  year: '2018'
D18-1260:
  abstract: "We present a new kind of question answering dataset, OpenBookQA, modeled\
    \ after open book exams for assessing human understanding of a subject. The open\
    \ book that comes with our questions is a set of 1326 elementary level science\
    \ facts. Roughly 6000 questions probe an understanding of these facts and their\
    \ application to novel situations. This requires combining an open book fact (e.g.,\
    \ metals conduct electricity) with broad common knowledge (e.g., a suit of armor\
    \ is made of metal) obtained from other sources. While existing QA datasets over\
    \ documents or knowledge bases, being generally self-contained, focus on linguistic\
    \ understanding, OpenBookQA probes a deeper understanding of both the topic\u2014\
    in the context of common knowledge\u2014and the language it is expressed in. Human\
    \ performance on OpenBookQA is close to 92%, but many state-of-the-art pre-trained\
    \ QA methods perform surprisingly poorly, worse than several simple neural baselines\
    \ we develop. Our oracle experiments designed to circumvent the knowledge retrieval\
    \ bottleneck demonstrate the value of both the open book and additional facts.\
    \ We leave it as a challenge to solve the retrieval problem in this multi-hop\
    \ setting and to close the large gap to human performance."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1260.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1260.Attachment.pdf
  - filename: https://vimeo.com/305887978
    type: video
    url: https://vimeo.com/305887978
  author:
  - first: Todor
    full: Todor Mihaylov
    id: todor-mihaylov
    last: Mihaylov
  - first: Peter
    full: Peter Clark
    id: peter-clark
    last: Clark
  - first: Tushar
    full: Tushar Khot
    id: tushar-khot
    last: Khot
  - first: Ashish
    full: Ashish Sabharwal
    id: ashish-sabharwal
    last: Sabharwal
  author_string: Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal
  bibkey: mihaylov-etal-2018-suit
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1260
  month: October-November
  page_first: '2381'
  page_last: '2391'
  pages: "2381\u20132391"
  paper_id: '260'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1260.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1260.jpg
  title: Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question
    Answering
  title_html: Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book
    Question Answering
  url: https://www.aclweb.org/anthology/D18-1260
  year: '2018'
D18-1261:
  abstract: "We propose a new dataset for evaluating question answering models with\
    \ respect to their capacity to reason about beliefs. Our tasks are inspired by\
    \ theory-of-mind experiments that examine whether children are able to reason\
    \ about the beliefs of others, in particular when those beliefs differ from reality.\
    \ We evaluate a number of recent neural models with memory augmentation. We find\
    \ that all fail on our tasks, which require keeping track of inconsistent states\
    \ of the world; moreover, the models\u2019 accuracy decreases notably when random\
    \ sentences are introduced to the tasks at test."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305888739
    type: video
    url: https://vimeo.com/305888739
  author:
  - first: Aida
    full: Aida Nematzadeh
    id: aida-nematzadeh
    last: Nematzadeh
  - first: Kaylee
    full: Kaylee Burns
    id: kaylee-burns
    last: Burns
  - first: Erin
    full: Erin Grant
    id: erin-grant
    last: Grant
  - first: Alison
    full: Alison Gopnik
    id: alison-gopnik
    last: Gopnik
  - first: Tom
    full: Tom Griffiths
    id: tom-griffiths
    last: Griffiths
  author_string: Aida Nematzadeh, Kaylee Burns, Erin Grant, Alison Gopnik, Tom Griffiths
  bibkey: nematzadeh-etal-2018-evaluating
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1261
  month: October-November
  page_first: '2392'
  page_last: '2400'
  pages: "2392\u20132400"
  paper_id: '261'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1261.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1261.jpg
  title: Evaluating Theory of Mind in Question Answering
  title_html: Evaluating Theory of Mind in Question Answering
  url: https://www.aclweb.org/anthology/D18-1261
  year: '2018'
D18-1262:
  abstract: Semantic role labeling (SRL) aims to recognize the predicate-argument
    structure of a sentence. Syntactic information has been paid a great attention
    over the role of enhancing SRL. However, the latest advance shows that syntax
    would not be so important for SRL with the emerging much smaller gap between syntax-aware
    and syntax-agnostic SRL. To comprehensively explore the role of syntax for SRL
    task, we extend existing models and propose a unified framework to investigate
    more effective and more diverse ways of incorporating syntax into sequential neural
    networks. Exploring the effect of syntactic input quality on SRL performance,
    we confirm that high-quality syntactic parse could still effectively enhance syntactically-driven
    SRL. Using empirically optimized integration strategy, we even enlarge the gap
    between syntax-aware and syntax-agnostic SRL. Our framework achieves state-of-the-art
    results on CoNLL-2009 benchmarks both for English and Chinese, substantially outperforming
    all previous models.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306044975
    type: video
    url: https://vimeo.com/306044975
  author:
  - first: Zuchao
    full: Zuchao Li
    id: zuchao-li
    last: Li
  - first: Shexia
    full: Shexia He
    id: shexia-he
    last: He
  - first: Jiaxun
    full: Jiaxun Cai
    id: jiaxun-cai
    last: Cai
  - first: Zhuosheng
    full: Zhuosheng Zhang
    id: zhuosheng-zhang
    last: Zhang
  - first: Hai
    full: Hai Zhao
    id: hai-zhao
    last: Zhao
  - first: Gongshen
    full: Gongshen Liu
    id: gongshen-liu
    last: Liu
  - first: Linlin
    full: Linlin Li
    id: linlin-li
    last: Li
  - first: Luo
    full: Luo Si
    id: luo-si
    last: Si
  author_string: Zuchao Li, Shexia He, Jiaxun Cai, Zhuosheng Zhang, Hai Zhao, Gongshen
    Liu, Linlin Li, Luo Si
  bibkey: li-etal-2018-unified
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1262
  month: October-November
  page_first: '2401'
  page_last: '2411'
  pages: "2401\u20132411"
  paper_id: '262'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1262.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1262.jpg
  title: A Unified Syntax-aware Framework for Semantic Role Labeling
  title_html: A Unified Syntax-aware Framework for Semantic Role Labeling
  url: https://www.aclweb.org/anthology/D18-1262
  year: '2018'
D18-1263:
  abstract: We propose a novel approach to semantic dependency parsing (SDP) by casting
    the task as an instance of multi-lingual machine translation, where each semantic
    representation is a different foreign dialect. To that end, we first generalize
    syntactic linearization techniques to account for the richer semantic dependency
    graph structure. Following, we design a neural sequence-to-sequence framework
    which can effectively recover our graph linearizations, performing almost on-par
    with previous SDP state-of-the-art while requiring less parallel training annotations.
    Beyond SDP, our linearization technique opens the door to integration of graph-based
    semantic representations as features in neural models for downstream applications.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306045906
    type: video
    url: https://vimeo.com/306045906
  author:
  - first: Gabriel
    full: Gabriel Stanovsky
    id: gabriel-stanovsky
    last: Stanovsky
  - first: Ido
    full: Ido Dagan
    id: ido-dagan
    last: Dagan
  author_string: Gabriel Stanovsky, Ido Dagan
  bibkey: stanovsky-dagan-2018-semantics
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1263
  month: October-November
  page_first: '2412'
  page_last: '2421'
  pages: "2412\u20132421"
  paper_id: '263'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1263.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1263.jpg
  title: Semantics as a Foreign Language
  title_html: Semantics as a Foreign Language
  url: https://www.aclweb.org/anthology/D18-1263
  year: '2018'
D18-1264:
  abstract: In this paper, we propose a new rich resource enhanced AMR aligner which
    produces multiple alignments and a new transition system for AMR parsing along
    with its oracle parser. Our aligner is further tuned by our oracle parser via
    picking the alignment that leads to the highest-scored achievable AMR graph. Experimental
    results show that our aligner outperforms the rule-based aligner in previous work
    by achieving higher alignment F1 score and consistently improving two open-sourced
    AMR parsers. Based on our aligner and transition system, we develop a transition-based
    AMR parser that parses a sentence into its AMR graph directly. An ensemble of
    our parsers with only words and POS tags as input leads to 68.4 Smatch F1 score,
    which outperforms the current state-of-the-art parser.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306049123
    type: video
    url: https://vimeo.com/306049123
  author:
  - first: Yijia
    full: Yijia Liu
    id: yijia-liu
    last: Liu
  - first: Wanxiang
    full: Wanxiang Che
    id: wanxiang-che
    last: Che
  - first: Bo
    full: Bo Zheng
    id: bo-zheng
    last: Zheng
  - first: Bing
    full: Bing Qin
    id: bing-qin
    last: Qin
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  author_string: Yijia Liu, Wanxiang Che, Bo Zheng, Bing Qin, Ting Liu
  bibkey: liu-etal-2018-amr
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1264
  month: October-November
  page_first: '2422'
  page_last: '2430'
  pages: "2422\u20132430"
  paper_id: '264'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1264.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1264.jpg
  title: An AMR Aligner Tuned by Transition-based Parser
  title_html: An <span class="acl-fixed-case">AMR</span> Aligner Tuned by Transition-based
    Parser
  url: https://www.aclweb.org/anthology/D18-1264
  year: '2018'
D18-1265:
  abstract: We propose a novel dependency-based hybrid tree model for semantic parsing,
    which converts natural language utterance into machine interpretable meaning representations.
    Unlike previous state-of-the-art models, the semantic information is interpreted
    as the latent dependency between the natural language words in our joint representation.
    Such dependency information can capture the interactions between the semantics
    and natural language words. We integrate a neural component into our model and
    propose an efficient dynamic-programming algorithm to perform tractable inference.
    Through extensive experiments on the standard multilingual GeoQuery dataset with
    eight languages, we demonstrate that our proposed approach is able to achieve
    state-of-the-art performance across several languages. Analysis also justifies
    the effectiveness of using our new dependency-based representation.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1265.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1265.Attachment.zip
  - filename: https://vimeo.com/306052219
    type: video
    url: https://vimeo.com/306052219
  author:
  - first: Zhanming
    full: Zhanming Jie
    id: zhanming-jie
    last: Jie
  - first: Wei
    full: Wei Lu
    id: wei-lu
    last: Lu
  author_string: Zhanming Jie, Wei Lu
  bibkey: jie-lu-2018-dependency
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1265
  month: October-November
  page_first: '2431'
  page_last: '2441'
  pages: "2431\u20132441"
  paper_id: '265'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1265.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1265.jpg
  title: Dependency-based Hybrid Trees for Semantic Parsing
  title_html: Dependency-based Hybrid Trees for Semantic Parsing
  url: https://www.aclweb.org/anthology/D18-1265
  year: '2018'
D18-1266:
  abstract: 'Semantic parsing from denotations faces two key challenges in model training:
    (1) given only the denotations (e.g., answers), search for good candidate semantic
    parses, and (2) choose the best model update algorithm. We propose effective and
    general solutions to each of them. Using policy shaping, we bias the search procedure
    towards semantic parses that are more compatible to the text, which provide better
    supervision signals for training. In addition, we propose an update equation that
    generalizes three different families of learning algorithms, which enables fast
    model exploration. When experimented on a recently proposed sequential question
    answering dataset, our framework leads to a new state-of-the-art model that outperforms
    previous work by 5.0% absolute on exact match accuracy.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1266.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1266.Attachment.pdf
  - filename: https://vimeo.com/306053202
    type: video
    url: https://vimeo.com/306053202
  - filename: D18-1266.Presentation.pdf
    type: presentation
    url: https://www.aclweb.org/anthology/attachments/D18-1266.Presentation.pdf
  author:
  - first: Dipendra
    full: Dipendra Misra
    id: dipendra-misra
    last: Misra
  - first: Ming-Wei
    full: Ming-Wei Chang
    id: ming-wei-chang
    last: Chang
  - first: Xiaodong
    full: Xiaodong He
    id: xiaodong-he
    last: He
  - first: Wen-tau
    full: Wen-tau Yih
    id: wen-tau-yih
    last: Yih
  author_string: Dipendra Misra, Ming-Wei Chang, Xiaodong He, Wen-tau Yih
  bibkey: misra-etal-2018-policy
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1266
  month: October-November
  page_first: '2442'
  page_last: '2452'
  pages: "2442\u20132452"
  paper_id: '266'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1266.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1266.jpg
  title: Policy Shaping and Generalized Update Equations for Semantic Parsing from
    Denotations
  title_html: Policy Shaping and Generalized Update Equations for Semantic Parsing
    from Denotations
  url: https://www.aclweb.org/anthology/D18-1266
  year: '2018'
D18-1267:
  abstract: 'In this paper we advocate the use of bilingual corpora which are abundantly
    available for training sentence compression models. Our approach borrows much
    of its machinery from neural machine translation and leverages bilingual pivoting:
    compressions are obtained by translating a source string into a foreign language
    and then back-translating it into the source while controlling the translation
    length. Our model can be trained for any language as long as a bilingual corpus
    is available and performs arbitrary rewrites without access to compression specific
    data. We release. Moss, a new parallel Multilingual Compression dataset for English,
    German, and French which can be used to evaluate compression models across languages
    and genres.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1267.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1267.Attachment.zip
  - filename: https://vimeo.com/305663630
    type: video
    url: https://vimeo.com/305663630
  author:
  - first: Jonathan
    full: Jonathan Mallinson
    id: jonathan-mallinson
    last: Mallinson
  - first: Rico
    full: Rico Sennrich
    id: rico-sennrich
    last: Sennrich
  - first: Mirella
    full: Mirella Lapata
    id: mirella-lapata
    last: Lapata
  author_string: Jonathan Mallinson, Rico Sennrich, Mirella Lapata
  bibkey: mallinson-etal-2018-sentence
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1267
  month: October-November
  page_first: '2453'
  page_last: '2464'
  pages: "2453\u20132464"
  paper_id: '267'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1267.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1267.jpg
  title: Sentence Compression for Arbitrary Languages via Multilingual Pivoting
  title_html: Sentence Compression for Arbitrary Languages via Multilingual Pivoting
  url: https://www.aclweb.org/anthology/D18-1267
  year: '2018'
D18-1268:
  abstract: Cross-lingual transfer of word embeddings aims to establish the semantic
    mappings among words in different languages by learning the transformation functions
    over the corresponding word embedding spaces. Successfully solving this problem
    would benefit many downstream tasks such as to translate text classification models
    from resource-rich languages (e.g. English) to low-resource languages. Supervised
    methods for this problem rely on the availability of cross-lingual supervision,
    either using parallel corpora or bilingual lexicons as the labeled data for training,
    which may not be available for many low resource languages. This paper proposes
    an unsupervised learning approach that does not require any cross-lingual labeled
    data. Given two monolingual word embedding spaces for any language pair, our algorithm
    optimizes the transformation functions in both directions simultaneously based
    on distributional matching as well as minimizing the back-translation losses.
    We use a neural network implementation to calculate the Sinkhorn distance, a well-defined
    distributional similarity measure, and optimize our objective through back-propagation.
    Our evaluation on benchmark datasets for bilingual lexicon induction and cross-lingual
    word similarity prediction shows stronger or competitive performance of the proposed
    method compared to other state-of-the-art supervised and unsupervised baseline
    methods over many language pairs.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1268.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1268.Attachment.pdf
  - filename: https://vimeo.com/305664457
    type: video
    url: https://vimeo.com/305664457
  author:
  - first: Ruochen
    full: Ruochen Xu
    id: ruochen-xu
    last: Xu
  - first: Yiming
    full: Yiming Yang
    id: yiming-yang
    last: Yang
  - first: Naoki
    full: Naoki Otani
    id: naoki-otani
    last: Otani
  - first: Yuexin
    full: Yuexin Wu
    id: yuexin-wu
    last: Wu
  author_string: Ruochen Xu, Yiming Yang, Naoki Otani, Yuexin Wu
  bibkey: xu-etal-2018-unsupervised-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1268
  month: October-November
  page_first: '2465'
  page_last: '2474'
  pages: "2465\u20132474"
  paper_id: '268'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1268.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1268.jpg
  title: Unsupervised Cross-lingual Transfer of Word Embedding Spaces
  title_html: Unsupervised Cross-lingual Transfer of Word Embedding Spaces
  url: https://www.aclweb.org/anthology/D18-1268
  year: '2018'
D18-1269:
  abstract: State-of-the-art natural language processing systems rely on supervision
    in the form of annotated data to learn competent models. These models are generally
    trained on data in a single language (usually English), and cannot be directly
    used beyond that language. Since collecting data in every language is not realistic,
    there has been a growing interest in cross-lingual language understanding (XLU)
    and low-resource cross-language transfer. In this work, we construct an evaluation
    set for XLU by extending the development and test sets of the Multi-Genre Natural
    Language Inference Corpus (MultiNLI) to 14 languages, including low-resource languages
    such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze
    research in cross-lingual sentence understanding by providing an informative standard
    evaluation task. In addition, we provide several baselines for multilingual sentence
    understanding, including two based on machine translation systems, and two that
    use parallel data to train aligned multilingual bag-of-words and LSTM encoders.
    We find that XNLI represents a practical and challenging evaluation suite, and
    that directly translating the test data yields the best performance among available
    baselines.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305665271
    type: video
    url: https://vimeo.com/305665271
  author:
  - first: Alexis
    full: Alexis Conneau
    id: alexis-conneau
    last: Conneau
  - first: Ruty
    full: Ruty Rinott
    id: ruty-rinott
    last: Rinott
  - first: Guillaume
    full: Guillaume Lample
    id: guillaume-lample
    last: Lample
  - first: Adina
    full: Adina Williams
    id: adina-williams
    last: Williams
  - first: Samuel
    full: Samuel Bowman
    id: samuel-bowman
    last: Bowman
  - first: Holger
    full: Holger Schwenk
    id: holger-schwenk
    last: Schwenk
  - first: Veselin
    full: Veselin Stoyanov
    id: veselin-stoyanov
    last: Stoyanov
  author_string: Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel
    Bowman, Holger Schwenk, Veselin Stoyanov
  bibkey: conneau-etal-2018-xnli
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1269
  month: October-November
  page_first: '2475'
  page_last: '2485'
  pages: "2475\u20132485"
  paper_id: '269'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1269.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1269.jpg
  title: 'XNLI: Evaluating Cross-lingual Sentence Representations'
  title_html: '<span class="acl-fixed-case">XNLI</span>: Evaluating Cross-lingual
    Sentence Representations'
  url: https://www.aclweb.org/anthology/D18-1269
  year: '2018'
D18-1270:
  abstract: 'Cross-lingual Entity Linking (XEL) aims to ground entity mentions written
    in any language to an English Knowledge Base (KB), such as Wikipedia. XEL for
    most languages is challenging, owing to limited availability of resources as supervision.
    We address this challenge by developing the first XEL approach that combines supervision
    from multiple languages jointly. This enables our approach to: (a) augment the
    limited supervision in the target language with additional supervision from a
    high-resource language (like English), and (b) train a single entity linking model
    for multiple languages, improving upon individually trained models for each language.
    Extensive evaluation on three benchmark datasets across 8 languages shows that
    our approach significantly improves over the current state-of-the-art. We also
    provide analyses in two limited resource settings: (a) zero-shot setting, when
    no supervision in the target language is available, and in (b) low-resource setting,
    when some supervision in the target language is available. Our analysis provides
    insights into the limitations of zero-shot XEL approaches in realistic scenarios,
    and shows the value of joint supervision in low-resource settings.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1270.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1270.Attachment.pdf
  - filename: https://vimeo.com/305666173
    type: video
    url: https://vimeo.com/305666173
  author:
  - first: Shyam
    full: Shyam Upadhyay
    id: shyam-upadhyay
    last: Upadhyay
  - first: Nitish
    full: Nitish Gupta
    id: nitish-gupta
    last: Gupta
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Shyam Upadhyay, Nitish Gupta, Dan Roth
  bibkey: upadhyay-etal-2018-joint
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1270
  month: October-November
  page_first: '2486'
  page_last: '2495'
  pages: "2486\u20132495"
  paper_id: '270'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1270.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1270.jpg
  title: Joint Multilingual Supervision for Cross-lingual Entity Linking
  title_html: Joint Multilingual Supervision for Cross-lingual Entity Linking
  url: https://www.aclweb.org/anthology/D18-1270
  year: '2018'
D18-1271:
  abstract: This paper proposes to study fine-grained coordinated cross-lingual text
    stream alignment through a novel information network decipherment paradigm. We
    use Burst Information Networks as media to represent text streams and present
    a simple yet effective network decipherment algorithm with diverse clues to decipher
    the networks for accurate text stream alignment. Experiments on Chinese-English
    news streams show our approach not only outperforms previous approaches on bilingual
    lexicon extraction from coordinated text streams but also can harvest high-quality
    alignments from large amounts of streaming data for endless language knowledge
    mining, which makes it promising to be a new paradigm for automatic language knowledge
    acquisition.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1271.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1271.Attachment.zip
  - filename: https://vimeo.com/305667349
    type: video
    url: https://vimeo.com/305667349
  author:
  - first: Tao
    full: Tao Ge
    id: tao-ge
    last: Ge
  - first: Qing
    full: Qing Dou
    id: qing-dou
    last: Dou
  - first: Heng
    full: Heng Ji
    id: heng-ji
    last: Ji
  - first: Lei
    full: Lei Cui
    id: lei-cui
    last: Cui
  - first: Baobao
    full: Baobao Chang
    id: baobao-chang
    last: Chang
  - first: Zhifang
    full: Zhifang Sui
    id: zhifang-sui
    last: Sui
  - first: Furu
    full: Furu Wei
    id: furu-wei
    last: Wei
  - first: Ming
    full: Ming Zhou
    id: ming-zhou
    last: Zhou
  author_string: Tao Ge, Qing Dou, Heng Ji, Lei Cui, Baobao Chang, Zhifang Sui, Furu
    Wei, Ming Zhou
  bibkey: ge-etal-2018-fine
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1271
  month: October-November
  page_first: '2496'
  page_last: '2506'
  pages: "2496\u20132506"
  paper_id: '271'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1271.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1271.jpg
  title: Fine-grained Coordinated Cross-lingual Text Stream Alignment for Endless
    Language Knowledge Acquisition
  title_html: Fine-grained Coordinated Cross-lingual Text Stream Alignment for Endless
    Language Knowledge Acquisition
  url: https://www.aclweb.org/anthology/D18-1271
  year: '2018'
D18-1272:
  abstract: Homographic puns have a long history in human writing, widely used in
    written and spoken literature, which usually occur in a certain syntactic or stylistic
    structure. How to recognize homographic puns is an important research. However,
    homographic pun recognition does not solve very well in existing work. In this
    work, we first use WordNet to understand and expand word embedding for settling
    the polysemy of homographic puns, and then propose a WordNet-Encoded Collocation-Attention
    network model (WECA) which combined with the context weights for recognizing the
    puns. Our experiments on the SemEval2017 Task7 and Pun of the Day demonstrate
    that the proposed model is able to distinguish between homographic pun and non-homographic
    pun texts. We show the effectiveness of the model to present the capability of
    choosing qualitatively informative words. The results show that our model achieves
    the state-of-the-art performance on homographic puns recognition.
  address: Brussels, Belgium
  author:
  - first: Yufeng
    full: Yufeng Diao
    id: yufeng-diao
    last: Diao
  - first: Hongfei
    full: Hongfei Lin
    id: hongfei-lin
    last: Lin
  - first: Di
    full: Di Wu
    id: di-wu
    last: Wu
  - first: Liang
    full: Liang Yang
    id: liang-yang
    last: Yang
  - first: Kan
    full: Kan Xu
    id: kan-xu
    last: Xu
  - first: Zhihao
    full: Zhihao Yang
    id: zhihao-yang
    last: Yang
  - first: Jian
    full: Jian Wang
    id: jian-wang
    last: Wang
  - first: Shaowu
    full: Shaowu Zhang
    id: shaowu-zhang
    last: Zhang
  - first: Bo
    full: Bo Xu
    id: bo-xu
    last: Xu
  - first: Dongyu
    full: Dongyu Zhang
    id: dongyu-zhang
    last: Zhang
  author_string: Yufeng Diao, Hongfei Lin, Di Wu, Liang Yang, Kan Xu, Zhihao Yang,
    Jian Wang, Shaowu Zhang, Bo Xu, Dongyu Zhang
  bibkey: diao-etal-2018-weca
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1272
  month: October-November
  page_first: '2507'
  page_last: '2516'
  pages: "2507\u20132516"
  paper_id: '272'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1272.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1272.jpg
  title: 'WECA: A WordNet-Encoded Collocation-Attention Network for Homographic Pun
    Recognition'
  title_html: '<span class="acl-fixed-case">WECA</span>: A <span class="acl-fixed-case">W</span>ord<span
    class="acl-fixed-case">N</span>et-Encoded Collocation-Attention Network for Homographic
    Pun Recognition'
  url: https://www.aclweb.org/anthology/D18-1272
  year: '2018'
D18-1273:
  abstract: "Chinese spelling check (CSC) is a challenging yet meaningful task, which\
    \ not only serves as a preprocessing in many natural language processing(NLP)\
    \ applications, but also facilitates reading and understanding of running texts\
    \ in peoples\u2019 daily lives. However, to utilize data-driven approaches for\
    \ CSC, there is one major limitation that annotated corpora are not enough in\
    \ applying algorithms and building models. In this paper, we propose a novel approach\
    \ of constructing CSC corpus with automatically generated spelling errors, which\
    \ are either visually or phonologically resembled characters, corresponding to\
    \ the OCR- and ASR-based methods, respectively. Upon the constructed corpus, different\
    \ models are trained and evaluated for CSC with respect to three standard test\
    \ sets. Experimental results demonstrate the effectiveness of the corpus, therefore\
    \ confirm the validity of our approach."
  address: Brussels, Belgium
  author:
  - first: Dingmin
    full: Dingmin Wang
    id: dingmin-wang
    last: Wang
  - first: Yan
    full: Yan Song
    id: yan-song
    last: Song
  - first: Jing
    full: Jing Li
    id: jing-li
    last: Li
  - first: Jialong
    full: Jialong Han
    id: jialong-han
    last: Han
  - first: Haisong
    full: Haisong Zhang
    id: haisong-zhang
    last: Zhang
  author_string: Dingmin Wang, Yan Song, Jing Li, Jialong Han, Haisong Zhang
  bibkey: wang-etal-2018-hybrid
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1273
  month: October-November
  page_first: '2517'
  page_last: '2527'
  pages: "2517\u20132527"
  paper_id: '273'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1273.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1273.jpg
  title: A Hybrid Approach to Automatic Corpus Generation for Chinese Spelling Check
  title_html: A Hybrid Approach to Automatic Corpus Generation for <span class="acl-fixed-case">C</span>hinese
    Spelling Check
  url: https://www.aclweb.org/anthology/D18-1273
  year: '2018'
D18-1274:
  abstract: "Grammatical error correction (GEC) systems deployed in language learning\
    \ environments are expected to accurately correct errors in learners\u2019 writing.\
    \ However, in practice, they often produce spurious corrections and fail to correct\
    \ many errors, thereby misleading learners. This necessitates the estimation of\
    \ the quality of output sentences produced by GEC systems so that instructors\
    \ can selectively intervene and re-correct the sentences which are poorly corrected\
    \ by the system and ensure that learners get accurate feedback. We propose the\
    \ first neural approach to automatic quality estimation of GEC output sentences\
    \ that does not employ any hand-crafted features. Our system is trained in a supervised\
    \ manner on learner sentences and corresponding GEC system outputs with quality\
    \ score labels computed using human-annotated references. Our neural quality estimation\
    \ models for GEC show significant improvements over a strong feature-based baseline.\
    \ We also show that a state-of-the-art GEC system can be improved when quality\
    \ scores are used as features for re-ranking the N-best candidates."
  address: Brussels, Belgium
  author:
  - first: Shamil
    full: Shamil Chollampatt
    id: shamil-chollampatt
    last: Chollampatt
  - first: Hwee Tou
    full: Hwee Tou Ng
    id: hwee-tou-ng
    last: Ng
  author_string: Shamil Chollampatt, Hwee Tou Ng
  bibkey: chollampatt-ng-2018-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1274
  month: October-November
  page_first: '2528'
  page_last: '2539'
  pages: "2528\u20132539"
  paper_id: '274'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1274.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1274.jpg
  title: Neural Quality Estimation of Grammatical Error Correction
  title_html: Neural Quality Estimation of Grammatical Error Correction
  url: https://www.aclweb.org/anthology/D18-1274
  year: '2018'
D18-1275:
  abstract: Part-of-Speech (POS) tagging for Twitter has received considerable attention
    in recent years. Because most POS tagging methods are based on supervised models,
    they usually require a large amount of labeled data for training. However, the
    existing labeled datasets for Twitter are much smaller than those for newswire
    text. Hence, to help POS tagging for Twitter, most domain adaptation methods try
    to leverage newswire datasets by learning the shared features between the two
    domains. However, from a linguistic perspective, Twitter users not only tend to
    mimic the formal expressions of traditional media, like news, but they also appear
    to be developing linguistically informal styles. Therefore, POS tagging for the
    formal Twitter context can be learned together with the newswire dataset, while
    POS tagging for the informal Twitter context should be learned separately. To
    achieve this task, in this work, we propose a hypernetwork-based method to generate
    different parameters to separately model contexts with different expression styles.
    Experimental results on three different datasets show that our approach achieves
    better performance than state-of-the-art methods in most cases.
  address: Brussels, Belgium
  author:
  - first: Tao
    full: Tao Gui
    id: tao-gui
    last: Gui
  - first: Qi
    full: Qi Zhang
    id: qi-zhang
    last: Zhang
  - first: Jingjing
    full: Jingjing Gong
    id: jingjing-gong
    last: Gong
  - first: Minlong
    full: Minlong Peng
    id: minlong-peng
    last: Peng
  - first: Di
    full: Di Liang
    id: di-liang
    last: Liang
  - first: Keyu
    full: Keyu Ding
    id: keyu-ding
    last: Ding
  - first: Xuanjing
    full: Xuanjing Huang
    id: xuan-jing-huang
    last: Huang
  author_string: Tao Gui, Qi Zhang, Jingjing Gong, Minlong Peng, Di Liang, Keyu Ding,
    Xuanjing Huang
  bibkey: gui-etal-2018-transferring
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1275
  month: October-November
  page_first: '2540'
  page_last: '2549'
  pages: "2540\u20132549"
  paper_id: '275'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1275.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1275.jpg
  title: Transferring from Formal Newswire Domain with Hypernet for Twitter POS Tagging
  title_html: Transferring from Formal Newswire Domain with Hypernet for Twitter <span
    class="acl-fixed-case">POS</span> Tagging
  url: https://www.aclweb.org/anthology/D18-1275
  year: '2018'
D18-1276:
  abstract: The configurational information in sentences of a free word order language
    such as Sanskrit is of limited use. Thus, the context of the entire sentence will
    be desirable even for basic processing tasks such as word segmentation. We propose
    a structured prediction framework that jointly solves the word segmentation and
    morphological tagging tasks in Sanskrit. We build an energy based model where
    we adopt approaches generally employed in graph based parsing techniques (McDonald
    et al., 2005a; Carreras, 2007). Our model outperforms the state of the art with
    an F-Score of 96.92 (percentage improvement of 7.06%) while using less than one
    tenth of the task-specific training data. We find that the use of a graph based
    approach instead of a traditional lattice-based sequential labelling approach
    leads to a percentage gain of 12.6% in F-Score for the segmentation task.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1276.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1276.Attachment.zip
  author:
  - first: Amrith
    full: Amrith Krishna
    id: amrith-krishna
    last: Krishna
  - first: Bishal
    full: Bishal Santra
    id: bishal-santra
    last: Santra
  - first: Sasi Prasanth
    full: Sasi Prasanth Bandaru
    id: sasi-prasanth-bandaru
    last: Bandaru
  - first: Gaurav
    full: Gaurav Sahu
    id: gaurav-sahu
    last: Sahu
  - first: Vishnu Dutt
    full: Vishnu Dutt Sharma
    id: vishnu-dutt-sharma
    last: Sharma
  - first: Pavankumar
    full: Pavankumar Satuluri
    id: pavankumar-satuluri
    last: Satuluri
  - first: Pawan
    full: Pawan Goyal
    id: pawan-goyal
    last: Goyal
  author_string: Amrith Krishna, Bishal Santra, Sasi Prasanth Bandaru, Gaurav Sahu,
    Vishnu Dutt Sharma, Pavankumar Satuluri, Pawan Goyal
  bibkey: krishna-etal-2018-free
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1276
  month: October-November
  page_first: '2550'
  page_last: '2561'
  pages: "2550\u20132561"
  paper_id: '276'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1276.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1276.jpg
  title: 'Free as in Free Word Order: An Energy Based Model for Word Segmentation
    and Morphological Tagging in Sanskrit'
  title_html: 'Free as in Free Word Order: An Energy Based Model for Word Segmentation
    and Morphological Tagging in <span class="acl-fixed-case">S</span>anskrit'
  url: https://www.aclweb.org/anthology/D18-1276
  year: '2018'
D18-1277:
  abstract: English part-of-speech taggers regularly make egregious errors related
    to noun-verb ambiguity, despite having achieved 97%+ accuracy on the WSJ Penn
    Treebank since 2002. These mistakes have been difficult to quantify and make taggers
    less useful to downstream tasks such as translation and text-to-speech synthesis.
    This paper creates a new dataset of over 30,000 naturally-occurring non-trivial
    examples of noun-verb ambiguity. Taggers within 1% of each other when measured
    on the WSJ have accuracies ranging from 57% to 75% accuracy on this challenge
    set. Enhancing the strongest existing tagger with contextual word embeddings and
    targeted training data improves its accuracy to 89%, a 14% absolute (52% relative)
    improvement. Downstream, using just this enhanced tagger yields a 28% reduction
    in error over the prior best learned model for homograph disambiguation for textto-speech
    synthesis.
  address: Brussels, Belgium
  author:
  - first: Ali
    full: Ali Elkahky
    id: ali-elkahky
    last: Elkahky
  - first: Kellie
    full: Kellie Webster
    id: kellie-webster
    last: Webster
  - first: Daniel
    full: Daniel Andor
    id: daniel-andor
    last: Andor
  - first: Emily
    full: Emily Pitler
    id: emily-pitler
    last: Pitler
  author_string: Ali Elkahky, Kellie Webster, Daniel Andor, Emily Pitler
  bibkey: elkahky-etal-2018-challenge
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1277
  month: October-November
  page_first: '2562'
  page_last: '2572'
  pages: "2562\u20132572"
  paper_id: '277'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1277.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1277.jpg
  title: A Challenge Set and Methods for Noun-Verb Ambiguity
  title_html: A Challenge Set and Methods for Noun-Verb Ambiguity
  url: https://www.aclweb.org/anthology/D18-1277
  year: '2018'
D18-1278:
  abstract: When parsing morphologically-rich languages with neural models, it is
    beneficial to model input at the character level, and it has been claimed that
    this is because character-level models learn morphology. We test these claims
    by comparing character-level models to an oracle with access to explicit morphological
    analysis on twelve languages with varying morphological typologies. Our results
    highlight many strengths of character-level models, but also show that they are
    poor at disambiguating some words, particularly in the face of case syncretism.
    We then demonstrate that explicitly modeling morphological case improves our best
    model, showing that character-level models can benefit from targeted forms of
    explicit morphological modeling.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1278.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1278.Attachment.zip
  author:
  - first: Clara
    full: Clara Vania
    id: clara-vania
    last: Vania
  - first: Andreas
    full: Andreas Grivas
    id: andreas-grivas
    last: Grivas
  - first: Adam
    full: Adam Lopez
    id: adam-lopez
    last: Lopez
  author_string: Clara Vania, Andreas Grivas, Adam Lopez
  bibkey: vania-etal-2018-character
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1278
  month: October-November
  page_first: '2573'
  page_last: '2583'
  pages: "2573\u20132583"
  paper_id: '278'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1278.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1278.jpg
  title: What do character-level models learn about morphology? The case of dependency
    parsing
  title_html: What do character-level models learn about morphology? The case of dependency
    parsing
  url: https://www.aclweb.org/anthology/D18-1278
  year: '2018'
D18-1279:
  abstract: Character-based neural models have recently proven very useful for many
    NLP tasks. However, there is a gap of sophistication between methods for learning
    representations of sentences and words. While, most character models for learning
    representations of sentences are deep and complex, models for learning representations
    of words are shallow and simple. Also, in spite of considerable research on learning
    character embeddings, it is still not clear which kind of architecture is the
    best for capturing character-to-word representations. To address these questions,
    we first investigate the gaps between methods for learning word and sentence representations.
    We conduct detailed experiments and comparisons on different state-of-the-art
    convolutional models, and also investigate the advantages and disadvantages of
    their constituents. Furthermore, we propose IntNet, a funnel-shaped wide convolutional
    neural architecture with no down-sampling for learning representations of the
    internal structure of words by composing their characters from limited, supervised
    training corpora. We evaluate our proposed model on six sequence labeling datasets,
    including named entity recognition, part-of-speech tagging, and syntactic chunking.
    Our in-depth analysis shows that IntNet significantly outperforms other character
    embedding models and obtains new state-of-the-art performance without relying
    on any external knowledge or resources.
  address: Brussels, Belgium
  author:
  - first: Yingwei
    full: Yingwei Xin
    id: yingwei-xin
    last: Xin
  - first: Ethan
    full: Ethan Hart
    id: ethan-hart
    last: Hart
  - first: Vibhuti
    full: Vibhuti Mahajan
    id: vibhuti-mahajan
    last: Mahajan
  - first: Jean-David
    full: Jean-David Ruvini
    id: jean-david-ruvini
    last: Ruvini
  author_string: Yingwei Xin, Ethan Hart, Vibhuti Mahajan, Jean-David Ruvini
  bibkey: xin-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1279
  month: October-November
  page_first: '2584'
  page_last: '2593'
  pages: "2584\u20132593"
  paper_id: '279'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1279.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1279.jpg
  title: Learning Better Internal Structure of Words for Sequence Labeling
  title_html: Learning Better Internal Structure of Words for Sequence Labeling
  url: https://www.aclweb.org/anthology/D18-1279
  year: '2018'
D18-1280:
  abstract: Emotion recognition in conversations is crucial for building empathetic
    machines. Present works in this domain do not explicitly consider the inter-personal
    influences that thrive in the emotional dynamics of dialogues. To this end, we
    propose Interactive COnversational memory Network (ICON), a multimodal emotion
    detection framework that extracts multimodal features from conversational videos
    and hierarchically models the self- and inter-speaker emotional influences into
    global memories. Such memories generate contextual summaries which aid in predicting
    the emotional orientation of utterance-videos. Our model outperforms state-of-the-art
    networks on multiple classification and regression tasks in two benchmark datasets.
  address: Brussels, Belgium
  author:
  - first: Devamanyu
    full: Devamanyu Hazarika
    id: devamanyu-hazarika
    last: Hazarika
  - first: Soujanya
    full: Soujanya Poria
    id: soujanya-poria
    last: Poria
  - first: Rada
    full: Rada Mihalcea
    id: rada-mihalcea
    last: Mihalcea
  - first: Erik
    full: Erik Cambria
    id: erik-cambria
    last: Cambria
  - first: Roger
    full: Roger Zimmermann
    id: roger-zimmermann
    last: Zimmermann
  author_string: Devamanyu Hazarika, Soujanya Poria, Rada Mihalcea, Erik Cambria,
    Roger Zimmermann
  bibkey: hazarika-etal-2018-icon
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1280
  month: October-November
  page_first: '2594'
  page_last: '2604'
  pages: "2594\u20132604"
  paper_id: '280'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1280.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1280.jpg
  title: 'ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection'
  title_html: '<span class="acl-fixed-case">ICON</span>: Interactive Conversational
    Memory Network for Multimodal Emotion Detection'
  url: https://www.aclweb.org/anthology/D18-1280
  year: '2018'
D18-1281:
  abstract: Thanks to the success of object detection technology, we can retrieve
    objects of the specified classes even from huge image collections. However, the
    current state-of-the-art object detectors (such as Faster R-CNN) can only handle
    pre-specified classes. In addition, large amounts of positive and negative visual
    samples are required for training. In this paper, we address the problem of open-vocabulary
    object retrieval and localization, where the target object is specified by a textual
    query (e.g., a word or phrase). We first propose Query-Adaptive R-CNN, a simple
    extension of Faster R-CNN adapted to open-vocabulary queries, by transforming
    the text embedding vector into an object classifier and localization regressor.
    Then, for discriminative training, we then propose negative phrase augmentation
    (NPA) to mine hard negative samples which are visually similar to the query and
    at the same time semantically mutually exclusive of the query. The proposed method
    can retrieve and localize objects specified by a textual query from one million
    images in only 0.5 seconds with high precision.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1281.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1281.Attachment.pdf
  author:
  - first: Ryota
    full: Ryota Hinami
    id: ryota-hinami
    last: Hinami
  - first: "Shin\u2019ichi"
    full: "Shin\u2019ichi Satoh"
    id: shinichi-satoh
    last: Satoh
  author_string: "Ryota Hinami, Shin\u2019ichi Satoh"
  bibkey: hinami-satoh-2018-discriminative
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1281
  month: October-November
  page_first: '2605'
  page_last: '2615'
  pages: "2605\u20132615"
  paper_id: '281'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1281.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1281.jpg
  title: Discriminative Learning of Open-Vocabulary Object Retrieval and Localization
    by Negative Phrase Augmentation
  title_html: Discriminative Learning of Open-Vocabulary Object Retrieval and Localization
    by Negative Phrase Augmentation
  url: https://www.aclweb.org/anthology/D18-1281
  year: '2018'
D18-1282:
  abstract: "We address the task of visual semantic role labeling (vSRL), the identification\
    \ of the participants of a situation or event in a visual scene, and their labeling\
    \ with their semantic relations to the event or situation. We render candidate\
    \ participants as image regions of objects, and train a model which learns to\
    \ ground roles in the regions which depict the corresponding participant. Experimental\
    \ results demonstrate that we can train a vSRL model without reliance on prohibitive\
    \ image-based role annotations, by utilizing noisy data which we extract automatically\
    \ from image captions using a linguistic SRL system. Furthermore, our model induces\
    \ frame\u2014semantic visual representations, and their comparison to previous\
    \ work on supervised visual verb sense disambiguation yields overall better results."
  address: Brussels, Belgium
  author:
  - first: Carina
    full: Carina Silberer
    id: carina-silberer
    last: Silberer
  - first: Manfred
    full: Manfred Pinkal
    id: manfred-pinkal
    last: Pinkal
  author_string: Carina Silberer, Manfred Pinkal
  bibkey: silberer-pinkal-2018-grounding
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1282
  month: October-November
  page_first: '2616'
  page_last: '2626'
  pages: "2616\u20132626"
  paper_id: '282'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1282.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1282.jpg
  title: Grounding Semantic Roles in Images
  title_html: Grounding Semantic Roles in Images
  url: https://www.aclweb.org/anthology/D18-1282
  year: '2018'
D18-1283:
  abstract: To enable collaboration and communication between humans and agents, this
    paper investigates learning to acquire commonsense evidence for action justification.
    In particular, we have developed an approach based on the generative Conditional
    Variational Autoencoder(CVAE) that models object relations/attributes of the world
    as latent variables and jointly learns a performer that predicts actions and an
    explainer that gathers commonsense evidence to justify the action. Our empirical
    results have shown that, compared to a typical attention-based model, CVAE achieves
    significantly higher performance in both action prediction and justification.
    A human subject study further shows that the commonsense evidence gathered by
    CVAE can be communicated to humans to achieve a significantly higher common ground
    between humans and agents.
  address: Brussels, Belgium
  author:
  - first: Shaohua
    full: Shaohua Yang
    id: shaohua-yang
    last: Yang
  - first: Qiaozi
    full: Qiaozi Gao
    id: qiaozi-gao
    last: Gao
  - first: Sari
    full: Sari Sadiya
    id: sari-saba-sadiya
    last: Sadiya
  - first: Joyce
    full: Joyce Chai
    id: joyce-chai
    last: Chai
  author_string: Shaohua Yang, Qiaozi Gao, Sari Sadiya, Joyce Chai
  bibkey: yang-etal-2018-commonsense
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1283
  month: October-November
  page_first: '2627'
  page_last: '2637'
  pages: "2627\u20132637"
  paper_id: '283'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1283.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1283.jpg
  title: Commonsense Justification for Action Explanation
  title_html: Commonsense Justification for Action Explanation
  url: https://www.aclweb.org/anthology/D18-1283
  year: '2018'
D18-1284:
  abstract: The ability to infer persona from dialogue can have applications in areas
    ranging from computational narrative analysis to personalized dialogue generation.
    We introduce neural models to learn persona embeddings in a supervised character
    trope classification task. The models encode dialogue snippets from IMDB into
    representations that can capture the various categories of film characters. The
    best-performing models use a multi-level attention mechanism over a set of utterances.
    We also utilize prior knowledge in the form of textual descriptions of the different
    tropes. We apply the learned embeddings to find similar characters across different
    movies, and cluster movies according to the distribution of the embeddings. The
    use of short conversational text as input, and the ability to learn from prior
    knowledge using memory, suggests these methods could be applied to other domains.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1284.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1284.Attachment.zip
  author:
  - first: Eric
    full: Eric Chu
    id: eric-chu
    last: Chu
  - first: Prashanth
    full: Prashanth Vijayaraghavan
    id: prashanth-vijayaraghavan
    last: Vijayaraghavan
  - first: Deb
    full: Deb Roy
    id: deb-roy
    last: Roy
  author_string: Eric Chu, Prashanth Vijayaraghavan, Deb Roy
  bibkey: chu-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1284
  month: October-November
  page_first: '2638'
  page_last: '2646'
  pages: "2638\u20132646"
  paper_id: '284'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1284.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1284.jpg
  title: Learning Personas from Dialogue with Attentive Memory Networks
  title_html: Learning Personas from Dialogue with Attentive Memory Networks
  url: https://www.aclweb.org/anthology/D18-1284
  year: '2018'
D18-1285:
  abstract: "We develop a semantic parser that is trained in a grounded setting using\
    \ pairs of videos captioned with sentences. This setting is both data-efficient,\
    \ requiring little annotation, and similar to the experience of children where\
    \ they observe their environment and listen to speakers. The semantic parser recovers\
    \ the meaning of English sentences despite not having access to any annotated\
    \ sentences. It does so despite the ambiguity inherent in vision where a sentence\
    \ may refer to any combination of objects, object properties, relations or actions\
    \ taken by any agent in a video. For this task, we collected a new dataset for\
    \ grounded language acquisition. Learning a grounded semantic parser \u2014 turning\
    \ sentences into logical forms using captioned videos \u2014 can significantly\
    \ expand the range of data that parsers can be trained on, lower the effort of\
    \ training a semantic parser, and ultimately lead to a better understanding of\
    \ child language acquisition."
  address: Brussels, Belgium
  author:
  - first: Candace
    full: Candace Ross
    id: candace-ross
    last: Ross
  - first: Andrei
    full: Andrei Barbu
    id: andrei-barbu
    last: Barbu
  - first: Yevgeni
    full: Yevgeni Berzak
    id: yevgeni-berzak
    last: Berzak
  - first: Battushig
    full: Battushig Myanganbayar
    id: battushig-myanganbayar
    last: Myanganbayar
  - first: Boris
    full: Boris Katz
    id: boris-katz
    last: Katz
  author_string: Candace Ross, Andrei Barbu, Yevgeni Berzak, Battushig Myanganbayar,
    Boris Katz
  bibkey: ross-etal-2018-grounding
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1285
  month: October-November
  page_first: '2647'
  page_last: '2656'
  pages: "2647\u20132656"
  paper_id: '285'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1285.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1285.jpg
  title: Grounding language acquisition by training semantic parsers using captioned
    videos
  title_html: Grounding language acquisition by training semantic parsers using captioned
    videos
  url: https://www.aclweb.org/anthology/D18-1285
  year: '2018'
D18-1286:
  abstract: "We propose an end-to-end deep learning model for translating free-form\
    \ natural language instructions to a high-level plan for behavioral robot navigation.\
    \ We use attention models to connect information from both the user instructions\
    \ and a topological representation of the environment. We evaluate our model\u2019\
    s performance on a new dataset containing 10,050 pairs of navigation instructions.\
    \ Our model significantly outperforms baseline approaches. Furthermore, our results\
    \ suggest that it is possible to leverage the environment map as a relevant knowledge\
    \ base to facilitate the translation of free-form navigational instruction."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1286.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1286.Attachment.zip
  author:
  - first: Xiaoxue
    full: Xiaoxue Zang
    id: xiaoxue-zang
    last: Zang
  - first: Ashwini
    full: Ashwini Pokle
    id: ashwini-pokle
    last: Pokle
  - first: Marynel
    full: "Marynel V\xE1zquez"
    id: marynel-vazquez
    last: "V\xE1zquez"
  - first: Kevin
    full: Kevin Chen
    id: kevin-chen
    last: Chen
  - first: Juan Carlos
    full: Juan Carlos Niebles
    id: juan-carlos-niebles
    last: Niebles
  - first: Alvaro
    full: Alvaro Soto
    id: alvaro-soto
    last: Soto
  - first: Silvio
    full: Silvio Savarese
    id: silvio-savarese
    last: Savarese
  author_string: "Xiaoxue Zang, Ashwini Pokle, Marynel V\xE1zquez, Kevin Chen, Juan\
    \ Carlos Niebles, Alvaro Soto, Silvio Savarese"
  bibkey: zang-etal-2018-translating
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1286
  month: October-November
  page_first: '2657'
  page_last: '2666'
  pages: "2657\u20132666"
  paper_id: '286'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1286.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1286.jpg
  title: Translating Navigation Instructions in Natural Language to a High-Level Plan
    for Behavioral Robot Navigation
  title_html: Translating Navigation Instructions in Natural Language to a High-Level
    Plan for Behavioral Robot Navigation
  url: https://www.aclweb.org/anthology/D18-1286
  year: '2018'
D18-1287:
  abstract: 'We propose to decompose instruction execution to goal prediction and
    action generation. We design a model that maps raw visual observations to goals
    using LINGUNET, a language-conditioned image generation network, and then generates
    the actions required to complete them. Our model is trained from demonstration
    only without external resources. To evaluate our approach, we introduce two benchmarks
    for instruction following: LANI, a navigation task; and CHAI, where an agent executes
    household instructions. Our evaluation demonstrates the advantages of our model
    decomposition, and illustrates the challenges posed by our new benchmarks.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1287.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1287.Attachment.zip
  author:
  - first: Dipendra
    full: Dipendra Misra
    id: dipendra-misra
    last: Misra
  - first: Andrew
    full: Andrew Bennett
    id: andrew-bennett
    last: Bennett
  - first: Valts
    full: Valts Blukis
    id: valts-blukis
    last: Blukis
  - first: Eyvind
    full: Eyvind Niklasson
    id: eyvind-niklasson
    last: Niklasson
  - first: Max
    full: Max Shatkhin
    id: max-shatkhin
    last: Shatkhin
  - first: Yoav
    full: Yoav Artzi
    id: yoav-artzi
    last: Artzi
  author_string: Dipendra Misra, Andrew Bennett, Valts Blukis, Eyvind Niklasson, Max
    Shatkhin, Yoav Artzi
  bibkey: misra-etal-2018-mapping
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1287
  month: October-November
  page_first: '2667'
  page_last: '2678'
  pages: "2667\u20132678"
  paper_id: '287'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1287.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1287.jpg
  title: Mapping Instructions to Actions in 3D Environments with Visual Goal Prediction
  title_html: Mapping Instructions to Actions in 3<span class="acl-fixed-case">D</span>
    Environments with Visual Goal Prediction
  url: https://www.aclweb.org/anthology/D18-1287
  year: '2018'
D18-1288:
  abstract: Researchers in computational psycholinguistics frequently use linear models
    to study time series data generated by human subjects. However, time series may
    violate the assumptions of these models through temporal diffusion, where stimulus
    presentation has a lingering influence on the response as the rest of the experiment
    unfolds. This paper proposes a new statistical model that borrows from digital
    signal processing by recasting the predictors and response as convolutionally-related
    signals, using recent advances in machine learning to fit latent impulse response
    functions (IRFs) of arbitrary shape. A synthetic experiment shows successful recovery
    of true latent IRFs, and psycholinguistic experiments reveal plausible, replicable,
    and fine-grained estimates of latent temporal dynamics, with comparable or improved
    prediction quality to widely-used alternatives.
  address: Brussels, Belgium
  author:
  - first: Cory
    full: Cory Shain
    id: cory-shain
    last: Shain
  - first: William
    full: William Schuler
    id: william-schuler
    last: Schuler
  author_string: Cory Shain, William Schuler
  bibkey: shain-schuler-2018-deconvolutional
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1288
  month: October-November
  page_first: '2679'
  page_last: '2689'
  pages: "2679\u20132689"
  paper_id: '288'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1288.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1288.jpg
  title: 'Deconvolutional Time Series Regression: A Technique for Modeling Temporally
    Diffuse Effects'
  title_html: 'Deconvolutional Time Series Regression: A Technique for Modeling Temporally
    Diffuse Effects'
  url: https://www.aclweb.org/anthology/D18-1288
  year: '2018'
D18-1289:
  abstract: In this paper, we present a crowdsourcing-based approach to model the
    human perception of sentence complexity. We collect a large corpus of sentences
    rated with judgments of complexity for two typologically-different languages,
    Italian and English. We test our approach in two experimental scenarios aimed
    to investigate the contribution of a wide set of lexical, morpho-syntactic and
    syntactic phenomena in predicting i) the degree of agreement among annotators
    independently from the assigned judgment and ii) the perception of sentence complexity.
  address: Brussels, Belgium
  author:
  - first: Dominique
    full: Dominique Brunato
    id: dominique-brunato
    last: Brunato
  - first: Lorenzo
    full: Lorenzo De Mattei
    id: lorenzo-de-mattei
    last: De Mattei
  - first: Felice
    full: "Felice Dell\u2019Orletta"
    id: felice-dellorletta
    last: "Dell\u2019Orletta"
  - first: Benedetta
    full: Benedetta Iavarone
    id: benedetta-iavarone
    last: Iavarone
  - first: Giulia
    full: Giulia Venturi
    id: giulia-venturi
    last: Venturi
  author_string: "Dominique Brunato, Lorenzo De Mattei, Felice Dell\u2019Orletta,\
    \ Benedetta Iavarone, Giulia Venturi"
  bibkey: brunato-etal-2018-sentence
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1289
  month: October-November
  page_first: '2690'
  page_last: '2699'
  pages: "2690\u20132699"
  paper_id: '289'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1289.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1289.jpg
  title: Is this Sentence Difficult? Do you Agree?
  title_html: Is this Sentence Difficult? Do you Agree?
  url: https://www.aclweb.org/anthology/D18-1289
  year: '2018'
D18-1290:
  abstract: 'Web queries with question intent manifest a complex syntactic structure
    and the processing of this structure is important for their interpretation. Pinter
    et al. (2016) has formalized the grammar of these queries and proposed semi-supervised
    algorithms for the adaptation of parsers originally designed to parse according
    to the standard dependency grammar, so that they can account for the unique forest
    grammar of queries. However, their algorithms rely on resources typically not
    available outside of big web corporates. We propose a new BiLSTM query parser
    that: (1) Explicitly accounts for the unique grammar of web queries; and (2) Utilizes
    named entity (NE) information from a BiLSTM NE tagger, that can be jointly trained
    with the parser. In order to train our model we annotate the query treebank of
    Pinter et al. (2016) with NEs. When trained on 2500 annotated queries our parser
    achieves UAS of 83.5% and segmentation F1-score of 84.5, substantially outperforming
    existing state-of-the-art parsers.'
  address: Brussels, Belgium
  author:
  - first: Rivka
    full: Rivka Malca
    id: rivka-malca
    last: Malca
  - first: Roi
    full: Roi Reichart
    id: roi-reichart
    last: Reichart
  author_string: Rivka Malca, Roi Reichart
  bibkey: malca-reichart-2018-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1290
  month: October-November
  page_first: '2700'
  page_last: '2710'
  pages: "2700\u20132710"
  paper_id: '290'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1290.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1290.jpg
  title: 'Neural Transition Based Parsing of Web Queries: An Entity Based Approach'
  title_html: 'Neural Transition Based Parsing of Web Queries: An Entity Based Approach'
  url: https://www.aclweb.org/anthology/D18-1290
  year: '2018'
D18-1291:
  abstract: We provide a comprehensive analysis of the interactions between pre-trained
    word embeddings, character models and POS tags in a transition-based dependency
    parser. While previous studies have shown POS information to be less important
    in the presence of character models, we show that in fact there are complex interactions
    between all three techniques. In isolation each produces large improvements over
    a baseline system using randomly initialised word embeddings only, but combining
    them quickly leads to diminishing returns. We categorise words by frequency, POS
    tag and language in order to systematically investigate how each of the techniques
    affects parsing quality. For many word categories, applying any two of the three
    techniques is almost as good as the full combined system. Character models tend
    to be more important for low-frequency open-class words, especially in morphologically
    rich languages, while POS tags can help disambiguate high-frequency function words.
    We also show that large character embedding sizes help even for languages with
    small character sets, especially in morphologically rich languages.
  address: Brussels, Belgium
  author:
  - first: Aaron
    full: Aaron Smith
    id: aaron-smith
    last: Smith
  - first: Miryam
    full: Miryam de Lhoneux
    id: miryam-de-lhoneux
    last: de Lhoneux
  - first: Sara
    full: Sara Stymne
    id: sara-stymne
    last: Stymne
  - first: Joakim
    full: Joakim Nivre
    id: joakim-nivre
    last: Nivre
  author_string: Aaron Smith, Miryam de Lhoneux, Sara Stymne, Joakim Nivre
  bibkey: smith-etal-2018-investigation
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1291
  month: October-November
  page_first: '2711'
  page_last: '2720'
  pages: "2711\u20132720"
  paper_id: '291'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1291.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1291.jpg
  title: An Investigation of the Interactions Between Pre-Trained Word Embeddings,
    Character Models and POS Tags in Dependency Parsing
  title_html: An Investigation of the Interactions Between Pre-Trained Word Embeddings,
    Character Models and <span class="acl-fixed-case">POS</span> Tags in Dependency
    Parsing
  url: https://www.aclweb.org/anthology/D18-1291
  year: '2018'
D18-1292:
  abstract: There have been several recent attempts to improve the accuracy of grammar
    induction systems by bounding the recursive complexity of the induction model.
    Modern depth-bounded grammar inducers have been shown to be more accurate than
    early unbounded PCFG inducers, but this technique has never been compared against
    unbounded induction within the same system, in part because most previous depth-bounding
    models are built around sequence models, the complexity of which grows exponentially
    with the maximum allowed depth. The present work instead applies depth bounds
    within a chart-based Bayesian PCFG inducer, where bounding can be switched on
    and off, and then samples trees with or without bounding. Results show that depth-bounding
    is indeed significantly effective in limiting the search space of the inducer
    and thereby increasing accuracy of resulting parsing model, independent of the
    contribution of modern Bayesian induction techniques. Moreover, parsing results
    on English, Chinese and German show that this bounded model is able to produce
    parse trees more accurately than or competitively with state-of-the-art constituency
    grammar induction models.
  address: Brussels, Belgium
  author:
  - first: Lifeng
    full: Lifeng Jin
    id: lifeng-jin
    last: Jin
  - first: Finale
    full: Finale Doshi-Velez
    id: finale-doshi-velez
    last: Doshi-Velez
  - first: Timothy
    full: Timothy Miller
    id: timothy-miller
    last: Miller
  - first: William
    full: William Schuler
    id: william-schuler
    last: Schuler
  - first: Lane
    full: Lane Schwartz
    id: lane-schwartz
    last: Schwartz
  author_string: Lifeng Jin, Finale Doshi-Velez, Timothy Miller, William Schuler,
    Lane Schwartz
  bibkey: jin-etal-2018-depth
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1292
  month: October-November
  page_first: '2721'
  page_last: '2731'
  pages: "2721\u20132731"
  paper_id: '292'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1292.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1292.jpg
  title: 'Depth-bounding is effective: Improvements and evaluation of unsupervised
    PCFG induction'
  title_html: 'Depth-bounding is effective: Improvements and evaluation of unsupervised
    <span class="acl-fixed-case">PCFG</span> induction'
  url: https://www.aclweb.org/anthology/D18-1292
  year: '2018'
D18-1293:
  abstract: In natural language processing, a common task is to compute the probability
    of a phrase appearing in a document or to calculate the probability of all phrases
    matching a given pattern. For instance, one computes affix (prefix, suffix, infix,
    etc.) probabilities of a string or a set of strings with respect to a probability
    distribution of patterns. The problem of computing infix probabilities of strings
    when the pattern distribution is given by a probabilistic context-free grammar
    or by a probabilistic finite automaton is already solved, yet it was open to compute
    the infix probabilities in an incremental manner. The incremental computation
    is crucial when a new query is built from a previous query. We tackle this problem
    and suggest a method that computes infix probabilities incrementally for probabilistic
    finite automata by representing all the probabilities of matching strings as a
    series of transition matrix calculations. We show that the proposed approach is
    theoretically faster than the previous method and, using real world data, demonstrate
    that our approach has vastly better performance in practice.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1293.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1293.Attachment.zip
  author:
  - first: Marco
    full: Marco Cognetta
    id: marco-cognetta
    last: Cognetta
  - first: Yo-Sub
    full: Yo-Sub Han
    id: yo-sub-han
    last: Han
  - first: Soon Chan
    full: Soon Chan Kwon
    id: soon-chan-kwon
    last: Kwon
  author_string: Marco Cognetta, Yo-Sub Han, Soon Chan Kwon
  bibkey: cognetta-etal-2018-incremental
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1293
  month: October-November
  page_first: '2732'
  page_last: '2741'
  pages: "2732\u20132741"
  paper_id: '293'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1293.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1293.jpg
  title: Incremental Computation of Infix Probabilities for Probabilistic Finite Automata
  title_html: Incremental Computation of Infix Probabilities for Probabilistic Finite
    Automata
  url: https://www.aclweb.org/anthology/D18-1293
  year: '2018'
D18-1294:
  abstract: "We propose a novel strategy to encode the syntax parse tree of sentence\
    \ into a learnable distributed representation. The proposed syntax encoding scheme\
    \ is provably information-lossless. In specific, an embedding vector is constructed\
    \ for each word in the sentence, encoding the path in the syntax tree corresponding\
    \ to the word. The one-to-one correspondence between these \u201Csyntax-embedding\u201D\
    \ vectors and the words (hence their embedding vectors) in the sentence makes\
    \ it easy to integrate such a representation with all word-level NLP models. We\
    \ empirically show the benefits of the syntax embeddings on the Authorship Attribution\
    \ domain, where our approach improves upon the prior art and achieves new performance\
    \ records on five benchmarking data sets."
  address: Brussels, Belgium
  author:
  - first: Richong
    full: Richong Zhang
    id: richong-zhang
    last: Zhang
  - first: Zhiyuan
    full: Zhiyuan Hu
    id: zhiyuan-hu
    last: Hu
  - first: Hongyu
    full: Hongyu Guo
    id: hongyu-guo
    last: Guo
  - first: Yongyi
    full: Yongyi Mao
    id: yongyi-mao
    last: Mao
  author_string: Richong Zhang, Zhiyuan Hu, Hongyu Guo, Yongyi Mao
  bibkey: zhang-etal-2018-syntax
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1294
  month: October-November
  page_first: '2742'
  page_last: '2753'
  pages: "2742\u20132753"
  paper_id: '294'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1294.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1294.jpg
  title: Syntax Encoding with Application in Authorship Attribution
  title_html: Syntax Encoding with Application in Authorship Attribution
  url: https://www.aclweb.org/anthology/D18-1294
  year: '2018'
D18-1295:
  abstract: The paper introduces end-to-end neural network models that tokenize Sanskrit
    by jointly splitting compounds and resolving phonetic merges (Sandhi). Tokenization
    of Sanskrit depends on local phonetic and distant semantic features that are incorporated
    using convolutional and recurrent elements. Contrary to most previous systems,
    our models do not require feature engineering or extern linguistic resources,
    but operate solely on parallel versions of raw and segmented text. The models
    discussed in this paper clearly improve over previous approaches to Sanskrit word
    segmentation. As they are language agnostic, we will demonstrate that they also
    outperform the state of the art for the related task of German compound splitting.
  address: Brussels, Belgium
  author:
  - first: Oliver
    full: Oliver Hellwig
    id: oliver-hellwig
    last: Hellwig
  - first: Sebastian
    full: Sebastian Nehrdich
    id: sebastian-nehrdich
    last: Nehrdich
  author_string: Oliver Hellwig, Sebastian Nehrdich
  bibkey: hellwig-nehrdich-2018-sanskrit
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1295
  month: October-November
  page_first: '2754'
  page_last: '2763'
  pages: "2754\u20132763"
  paper_id: '295'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1295.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1295.jpg
  title: Sanskrit Word Segmentation Using Character-level Recurrent and Convolutional
    Neural Networks
  title_html: <span class="acl-fixed-case">S</span>anskrit Word Segmentation Using
    Character-level Recurrent and Convolutional Neural Networks
  url: https://www.aclweb.org/anthology/D18-1295
  year: '2018'
D18-1296:
  abstract: We propose to generalize language models for conversational speech recognition
    to allow them to operate across utterance boundaries and speaker changes, thereby
    capturing conversation-level phenomena such as adjacency pairs, lexical entrainment,
    and topical coherence. The model consists of a long-short-term memory (LSTM) recurrent
    network that reads the entire word-level history of a conversation, as well as
    information about turn taking and speaker overlap, in order to predict each next
    word. The model is applied in a rescoring framework, where the word history prior
    to the current utterance is approximated with preliminary recognition results.
    In experiments in the conversational telephone speech domain (Switchboard) we
    find that such a model gives substantial perplexity reductions over a standard
    LSTM-LM with utterance scope, as well as improvements in word error rate.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305942129
    type: video
    url: https://vimeo.com/305942129
  author:
  - first: Wayne
    full: Wayne Xiong
    id: wayne-xiong
    last: Xiong
  - first: Lingfeng
    full: Lingfeng Wu
    id: lingfeng-wu
    last: Wu
  - first: Jun
    full: Jun Zhang
    id: jun-zhang
    last: Zhang
  - first: Andreas
    full: Andreas Stolcke
    id: andreas-stolcke
    last: Stolcke
  author_string: Wayne Xiong, Lingfeng Wu, Jun Zhang, Andreas Stolcke
  bibkey: xiong-etal-2018-session
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1296
  month: October-November
  page_first: '2764'
  page_last: '2768'
  pages: "2764\u20132768"
  paper_id: '296'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1296.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1296.jpg
  title: Session-level Language Modeling for Conversational Speech
  title_html: Session-level Language Modeling for Conversational Speech
  url: https://www.aclweb.org/anthology/D18-1296
  year: '2018'
D18-1297:
  abstract: Sequence-to-sequence neural generation models have achieved promising
    performance on short text conversation tasks. However, they tend to generate generic/dull
    responses, leading to unsatisfying dialogue experience. We observe that in the
    conversation tasks, each query could have multiple responses, which forms a 1-to-n
    or m-to-n relationship in the view of the total corpus. The objective function
    used in standard sequence-to-sequence models will be dominated by loss terms with
    generic patterns. Inspired by this observation, we introduce a statistical re-weighting
    method that assigns different weights for the multiple responses of the same query,
    and trains the common neural generation model with the weights. Experimental results
    on a large Chinese dialogue corpus show that our method improves the acceptance
    rate of generated responses compared with several baseline models and significantly
    reduces the number of generated generic responses.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305942945
    type: video
    url: https://vimeo.com/305942945
  author:
  - first: Yahui
    full: Yahui Liu
    id: yahui-liu
    last: Liu
  - first: Wei
    full: Wei Bi
    id: wei-bi
    last: Bi
  - first: Jun
    full: Jun Gao
    id: jun-gao
    last: Gao
  - first: Xiaojiang
    full: Xiaojiang Liu
    id: xiaojiang-liu
    last: Liu
  - first: Jian
    full: Jian Yao
    id: jian-yao
    last: Yao
  - first: Shuming
    full: Shuming Shi
    id: shuming-shi
    last: Shi
  author_string: Yahui Liu, Wei Bi, Jun Gao, Xiaojiang Liu, Jian Yao, Shuming Shi
  bibkey: liu-etal-2018-towards-less
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1297
  month: October-November
  page_first: '2769'
  page_last: '2774'
  pages: "2769\u20132774"
  paper_id: '297'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1297.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1297.jpg
  title: 'Towards Less Generic Responses in Neural Conversation Models: A Statistical
    Re-weighting Method'
  title_html: 'Towards Less Generic Responses in Neural Conversation Models: A Statistical
    Re-weighting Method'
  url: https://www.aclweb.org/anthology/D18-1297
  year: '2018'
D18-1298:
  abstract: Current dialogue systems fail at being engaging for users, especially
    when trained end-to-end without relying on proactive reengaging scripted strategies.
    Zhang et al. (2018) showed that the engagement level of end-to-end dialogue models
    increases when conditioning them on text personas providing some personalized
    back-story to the model. However, the dataset used in Zhang et al. (2018) is synthetic
    and only contains around 1k different personas. In this paper we introduce a new
    dataset providing 5 million personas and 700 million persona-based dialogues.
    Our experiments show that, at this scale, training using personas still improves
    the performance of end-to-end systems. In addition, we show that other tasks benefit
    from the wide coverage of our dataset by fine-tuning our model on the data from
    Zhang et al. (2018) and achieving state-of-the-art results.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305943582
    type: video
    url: https://vimeo.com/305943582
  author:
  - first: Pierre-Emmanuel
    full: "Pierre-Emmanuel Mazar\xE9"
    id: pierre-emmanuel-mazare
    last: "Mazar\xE9"
  - first: Samuel
    full: Samuel Humeau
    id: samuel-humeau
    last: Humeau
  - first: Martin
    full: Martin Raison
    id: martin-raison
    last: Raison
  - first: Antoine
    full: Antoine Bordes
    id: antoine-bordes
    last: Bordes
  author_string: "Pierre-Emmanuel Mazar\xE9, Samuel Humeau, Martin Raison, Antoine\
    \ Bordes"
  bibkey: mazare-etal-2018-training
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1298
  month: October-November
  page_first: '2775'
  page_last: '2779'
  pages: "2775\u20132779"
  paper_id: '298'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1298.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1298.jpg
  title: Training Millions of Personalized Dialogue Agents
  title_html: Training Millions of Personalized Dialogue Agents
  url: https://www.aclweb.org/anthology/D18-1298
  year: '2018'
D18-1299:
  abstract: "Dialogue state tracker is the core part of a spoken dialogue system.\
    \ It estimates the beliefs of possible user\u2019s goals at every dialogue turn.\
    \ However, for most current approaches, it\u2019s difficult to scale to large\
    \ dialogue domains. They have one or more of following limitations: (a) Some models\
    \ don\u2019t work in the situation where slot values in ontology changes dynamically;\
    \ (b) The number of model parameters is proportional to the number of slots; (c)\
    \ Some models extract features based on hand-crafted lexicons. To tackle these\
    \ challenges, we propose StateNet, a universal dialogue state tracker. It is independent\
    \ of the number of values, shares parameters across all slots, and uses pre-trained\
    \ word vectors instead of explicit semantic dictionaries. Our experiments on two\
    \ datasets show that our approach not only overcomes the limitations, but also\
    \ significantly outperforms the performance of state-of-the-art approaches."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305944406
    type: video
    url: https://vimeo.com/305944406
  author:
  - first: Liliang
    full: Liliang Ren
    id: liliang-ren
    last: Ren
  - first: Kaige
    full: Kaige Xie
    id: kaige-xie
    last: Xie
  - first: Lu
    full: Lu Chen
    id: lu-chen
    last: Chen
  - first: Kai
    full: Kai Yu
    id: kai-yu
    last: Yu
  author_string: Liliang Ren, Kaige Xie, Lu Chen, Kai Yu
  bibkey: ren-etal-2018-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1299
  month: October-November
  page_first: '2780'
  page_last: '2786'
  pages: "2780\u20132786"
  paper_id: '299'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1299.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1299.jpg
  title: Towards Universal Dialogue State Tracking
  title_html: Towards Universal Dialogue State Tracking
  url: https://www.aclweb.org/anthology/D18-1299
  year: '2018'
D18-1300:
  abstract: Task oriented dialog systems typically first parse user utterances to
    semantic frames comprised of intents and slots. Previous work on task oriented
    intent and slot-filling work has been restricted to one intent per query and one
    slot label per token, and thus cannot model complex compositional requests. Alternative
    semantic parsing systems have represented queries as logical forms, but these
    are challenging to annotate and parse. We propose a hierarchical annotation scheme
    for semantic parsing that allows the representation of compositional queries,
    and can be efficiently and accurately parsed by standard constituency parsing
    models. We release a dataset of 44k annotated queries (http://fb.me/semanticparsingdialog),
    and show that parsing models outperform sequence-to-sequence approaches on this
    dataset.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305945055
    type: video
    url: https://vimeo.com/305945055
  author:
  - first: Sonal
    full: Sonal Gupta
    id: sonal-gupta
    last: Gupta
  - first: Rushin
    full: Rushin Shah
    id: rushin-shah
    last: Shah
  - first: Mrinal
    full: Mrinal Mohit
    id: mrinal-mohit
    last: Mohit
  - first: Anuj
    full: Anuj Kumar
    id: anuj-kumar
    last: Kumar
  - first: Mike
    full: Mike Lewis
    id: mike-lewis
    last: Lewis
  author_string: Sonal Gupta, Rushin Shah, Mrinal Mohit, Anuj Kumar, Mike Lewis
  bibkey: gupta-etal-2018-semantic-parsing
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1300
  month: October-November
  page_first: '2787'
  page_last: '2792'
  pages: "2787\u20132792"
  paper_id: '300'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1300.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1300.jpg
  title: Semantic Parsing for Task Oriented Dialog using Hierarchical Representations
  title_html: Semantic Parsing for Task Oriented Dialog using Hierarchical Representations
  url: https://www.aclweb.org/anthology/D18-1300
  year: '2018'
D18-1301:
  abstract: In this paper, we provide empirical evidence based on a rigourously studied
    mathematical model for bi-populated networks, that a glass ceiling within the
    field of NLP has developed since the mid 2000s.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305889955
    type: video
    url: https://vimeo.com/305889955
  author:
  - first: Natalie
    full: Natalie Schluter
    id: natalie-schluter
    last: Schluter
  author_string: Natalie Schluter
  bibkey: schluter-2018-glass
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1301
  month: October-November
  page_first: '2793'
  page_last: '2798'
  pages: "2793\u20132798"
  paper_id: '301'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1301.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1301.jpg
  title: The glass ceiling in NLP
  title_html: The glass ceiling in <span class="acl-fixed-case">NLP</span>
  url: https://www.aclweb.org/anthology/D18-1301
  year: '2018'
D18-1302:
  abstract: "Abusive language detection models tend to have a problem of being biased\
    \ toward identity words of a certain group of people because of imbalanced training\
    \ datasets. For example, \u201CYou are a good woman\u201D was considered \u201C\
    sexist\u201D when trained on an existing dataset. Such model bias is an obstacle\
    \ for models to be robust enough for practical use. In this work, we measure them\
    \ on models trained with different datasets, while analyzing the effect of different\
    \ pre-trained word embeddings and model architectures. We also experiment with\
    \ three mitigation methods: (1) debiased word embeddings, (2) gender swap data\
    \ augmentation, and (3) fine-tuning with a larger corpus. These methods can effectively\
    \ reduce model bias by 90-98% and can be extended to correct model bias in other\
    \ scenarios."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305891047
    type: video
    url: https://vimeo.com/305891047
  author:
  - first: Ji Ho
    full: Ji Ho Park
    id: ji-ho-park
    last: Park
  - first: Jamin
    full: Jamin Shin
    id: jamin-shin
    last: Shin
  - first: Pascale
    full: Pascale Fung
    id: pascale-fung
    last: Fung
  author_string: Ji Ho Park, Jamin Shin, Pascale Fung
  bibkey: park-etal-2018-reducing
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1302
  month: October-November
  page_first: '2799'
  page_last: '2804'
  pages: "2799\u20132804"
  paper_id: '302'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1302.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1302.jpg
  title: Reducing Gender Bias in Abusive Language Detection
  title_html: Reducing Gender Bias in Abusive Language Detection
  url: https://www.aclweb.org/anthology/D18-1302
  year: '2018'
D18-1303:
  abstract: "With the recent rise of #MeToo, an increasing number of personal stories\
    \ about sexual harassment and sexual abuse have been shared online. In order to\
    \ push forward the fight against such harassment and abuse, we present the task\
    \ of automatically categorizing and analyzing various forms of sexual harassment,\
    \ based on stories shared on the online forum SafeCity. For the labels of groping,\
    \ ogling, and commenting, our single-label CNN-RNN model achieves an accuracy\
    \ of 86.5%, and our multi-label model achieves a Hamming score of 82.5%. Furthermore,\
    \ we present analysis using LIME, first-derivative saliency heatmaps, activation\
    \ clustering, and embedding visualization to interpret neural model predictions\
    \ and demonstrate how this helps extract features that can help automatically\
    \ fill out incident reports, identify unsafe areas, avoid unsafe practices, and\
    \ \u2018pin the creeps\u2019."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305891772
    type: video
    url: https://vimeo.com/305891772
  author:
  - first: Sweta
    full: Sweta Karlekar
    id: sweta-karlekar
    last: Karlekar
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  author_string: Sweta Karlekar, Mohit Bansal
  bibkey: karlekar-bansal-2018-safecity
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1303
  month: October-November
  page_first: '2805'
  page_last: '2811'
  pages: "2805\u20132811"
  paper_id: '303'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1303.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1303.jpg
  title: 'SafeCity: Understanding Diverse Forms of Sexual Harassment Personal Stories'
  title_html: '<span class="acl-fixed-case">S</span>afe<span class="acl-fixed-case">C</span>ity:
    Understanding Diverse Forms of Sexual Harassment Personal Stories'
  url: https://www.aclweb.org/anthology/D18-1303
  year: '2018'
D18-1304:
  abstract: "As the incidence of Alzheimer\u2019s Disease (AD) increases, early detection\
    \ becomes crucial. Unfortunately, datasets for AD assessment are often sparse\
    \ and incomplete. In this work, we leverage the multiview nature of a small AD\
    \ dataset, DementiaBank, to learn an embedding that captures different modes of\
    \ cognitive impairment. We apply generalized canonical correlation analysis (GCCA)\
    \ to our dataset and demonstrate the added benefit of using multiview embeddings\
    \ in two downstream tasks: identifying AD and predicting clinical scores. By including\
    \ multiview embeddings, we obtain an F1 score of 0.82 in the classification task\
    \ and a mean absolute error of 3.42 in the regression task. Furthermore, we show\
    \ that multiview embeddings can be obtained from other datasets as well."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305892345
    type: video
    url: https://vimeo.com/305892345
  author:
  - first: "Chlo\xE9"
    full: "Chlo\xE9 Pou-Prom"
    id: chloe-pou-prom
    last: Pou-Prom
  - first: Frank
    full: Frank Rudzicz
    id: frank-rudzicz
    last: Rudzicz
  author_string: "Chlo\xE9 Pou-Prom, Frank Rudzicz"
  bibkey: pou-prom-rudzicz-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1304
  month: October-November
  page_first: '2812'
  page_last: '2817'
  pages: "2812\u20132817"
  paper_id: '304'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1304.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1304.jpg
  title: Learning multiview embeddings for assessing dementia
  title_html: Learning multiview embeddings for assessing dementia
  url: https://www.aclweb.org/anthology/D18-1304
  year: '2018'
D18-1305:
  abstract: "We present a corpus that encompasses the complete history of conversations\
    \ between contributors to Wikipedia, one of the largest online collaborative communities.\
    \ By recording the intermediate states of conversations - including not only comments\
    \ and replies, but also their modifications, deletions and restorations - this\
    \ data offers an unprecedented view of online conversation. Our framework is designed\
    \ to be language agnostic, and we show that it extracts high quality data in both\
    \ Chinese and English. This level of detail supports new research questions pertaining\
    \ to the process (and challenges) of large-scale online collaboration. We illustrate\
    \ the corpus\u2019 potential with two case studies on English Wikipedia that highlight\
    \ new perspectives on earlier work. First, we explore how a person\u2019s conversational\
    \ behavior depends on how they relate to the discussion\u2019s venue. Second,\
    \ we show that community moderation of toxic behavior happens at a higher rate\
    \ than previously estimated."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305892676
    type: video
    url: https://vimeo.com/305892676
  author:
  - first: Yiqing
    full: Yiqing Hua
    id: yiqing-hua
    last: Hua
  - first: Cristian
    full: Cristian Danescu-Niculescu-Mizil
    id: cristian-danescu-niculescu-mizil
    last: Danescu-Niculescu-Mizil
  - first: Dario
    full: Dario Taraborelli
    id: dario-taraborelli
    last: Taraborelli
  - first: Nithum
    full: Nithum Thain
    id: nithum-thain
    last: Thain
  - first: Jeffery
    full: Jeffery Sorensen
    id: jeffery-sorensen
    last: Sorensen
  - first: Lucas
    full: Lucas Dixon
    id: lucas-dixon
    last: Dixon
  author_string: Yiqing Hua, Cristian Danescu-Niculescu-Mizil, Dario Taraborelli,
    Nithum Thain, Jeffery Sorensen, Lucas Dixon
  bibkey: hua-etal-2018-wikiconv
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1305
  month: October-November
  page_first: '2818'
  page_last: '2823'
  pages: "2818\u20132823"
  paper_id: '305'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1305.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1305.jpg
  title: 'WikiConv: A Corpus of the Complete Conversational History of a Large Online
    Collaborative Community'
  title_html: '<span class="acl-fixed-case">W</span>iki<span class="acl-fixed-case">C</span>onv:
    A Corpus of the Complete Conversational History of a Large Online Collaborative
    Community'
  url: https://www.aclweb.org/anthology/D18-1305
  year: '2018'
D18-1306:
  abstract: "Extracting typed entity mentions from text is a fundamental component\
    \ to language understanding and reasoning. While there exist substantial labeled\
    \ text datasets for multiple subsets of biomedical entity types\u2014such as genes\
    \ and proteins, or chemicals and diseases\u2014it is rare to find large labeled\
    \ datasets containing labels for all desired entity types together. This paper\
    \ presents a method for training a single CRF extractor from multiple datasets\
    \ with disjoint or partially overlapping sets of entity types. Our approach employs\
    \ marginal likelihood training to insist on labels that are present in the data,\
    \ while filling in \u201Cmissing labels\u201D. This allows us to leverage all\
    \ the available data within a single model. In experimental results on the Biocreative\
    \ V CDR (chemicals/diseases), Biocreative VI ChemProt (chemicals/proteins) and\
    \ MedMentions (19 entity types) datasets, we show that joint training on multiple\
    \ datasets improves NER F1 over training in isolation, and our methods achieve\
    \ state-of-the-art results."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1306.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1306.Attachment.pdf
  - filename: https://vimeo.com/306054703
    type: video
    url: https://vimeo.com/306054703
  author:
  - first: Nathan
    full: Nathan Greenberg
    id: nathan-greenberg
    last: Greenberg
  - first: Trapit
    full: Trapit Bansal
    id: trapit-bansal
    last: Bansal
  - first: Patrick
    full: Patrick Verga
    id: patrick-verga
    last: Verga
  - first: Andrew
    full: Andrew McCallum
    id: andrew-mccallum
    last: McCallum
  author_string: Nathan Greenberg, Trapit Bansal, Patrick Verga, Andrew McCallum
  bibkey: greenberg-etal-2018-marginal
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1306
  month: October-November
  page_first: '2824'
  page_last: '2829'
  pages: "2824\u20132829"
  paper_id: '306'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1306.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1306.jpg
  title: Marginal Likelihood Training of BiLSTM-CRF for Biomedical Named Entity Recognition
    from Disjoint Label Sets
  title_html: Marginal Likelihood Training of <span class="acl-fixed-case">B</span>i<span
    class="acl-fixed-case">LSTM</span>-<span class="acl-fixed-case">CRF</span> for
    Biomedical Named Entity Recognition from Disjoint Label Sets
  url: https://www.aclweb.org/anthology/D18-1306
  year: '2018'
D18-1307:
  abstract: Adversarial training (AT) is a regularization method that can be used
    to improve the robustness of neural network methods by adding small perturbations
    in the training data. We show how to use AT for the tasks of entity recognition
    and relation extraction. In particular, we demonstrate that applying AT to a general
    purpose baseline model for jointly extracting entities and relations, allows improving
    the state-of-the-art effectiveness on several datasets in different contexts (i.e.,
    news, biomedical, and real estate data) and for different languages (English and
    Dutch).
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306055549
    type: video
    url: https://vimeo.com/306055549
  author:
  - first: Giannis
    full: Giannis Bekoulis
    id: giannis-bekoulis
    last: Bekoulis
  - first: Johannes
    full: Johannes Deleu
    id: johannes-deleu
    last: Deleu
  - first: Thomas
    full: Thomas Demeester
    id: thomas-demeester
    last: Demeester
  - first: Chris
    full: Chris Develder
    id: chris-develder
    last: Develder
  author_string: Giannis Bekoulis, Johannes Deleu, Thomas Demeester, Chris Develder
  bibkey: bekoulis-etal-2018-adversarial
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1307
  month: October-November
  page_first: '2830'
  page_last: '2836'
  pages: "2830\u20132836"
  paper_id: '307'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1307.pdf
  publisher: Association for Computational Linguistics
  revision:
  - id: '1'
    url: https://www.aclweb.org/anthology/D18-1307v1.pdf
    value: D18-1307v1
  - explanation: 'The changes are: (i) remove the curly braces from e-mails, (ii)
      fix the issue that whole page five is an image and (iii) remove balancing of
      the columns at the end of the document.'
    id: '2'
    url: https://www.aclweb.org/anthology/D18-1307v2.pdf
    value: D18-1307v2
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1307.jpg
  title: Adversarial training for multi-context joint entity and relation extraction
  title_html: Adversarial training for multi-context joint entity and relation extraction
  url: https://www.aclweb.org/anthology/D18-1307
  year: '2018'
D18-1308:
  abstract: We propose a model for tagging unstructured texts with an arbitrary number
    of terms drawn from a tree-structured vocabulary (i.e., an ontology). We treat
    this as a special case of sequence-to-sequence learning in which the decoder begins
    at the root node of an ontological tree and recursively elects to expand child
    nodes as a function of the input text, the current node, and the latent decoder
    state. We demonstrate that this method yields state-of-the-art results on the
    important task of assigning MeSH terms to biomedical abstracts.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1308.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1308.Attachment.pdf
  - filename: https://vimeo.com/306056257
    type: video
    url: https://vimeo.com/306056257
  author:
  - first: Gaurav
    full: Gaurav Singh
    id: gaurav-singh-tomar
    last: Singh
  - first: James
    full: James Thomas
    id: james-thomas
    last: Thomas
  - first: Iain
    full: Iain Marshall
    id: iain-marshall
    last: Marshall
  - first: John
    full: John Shawe-Taylor
    id: john-shawe-taylor
    last: Shawe-Taylor
  - first: Byron C.
    full: Byron C. Wallace
    id: byron-c-wallace
    last: Wallace
  author_string: Gaurav Singh, James Thomas, Iain Marshall, John Shawe-Taylor, Byron
    C. Wallace
  bibkey: singh-etal-2018-structured
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1308
  month: October-November
  page_first: '2837'
  page_last: '2842'
  pages: "2837\u20132842"
  paper_id: '308'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1308.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1308.jpg
  title: Structured Multi-Label Biomedical Text Tagging via Attentive Neural Tree
    Decoding
  title_html: Structured Multi-Label Biomedical Text Tagging via Attentive Neural
    Tree Decoding
  url: https://www.aclweb.org/anthology/D18-1308
  year: '2018'
D18-1309:
  abstract: We propose a simple deep neural model for nested named entity recognition
    (NER). Most NER models focused on flat entities and ignored nested entities, which
    failed to fully capture underlying semantic information in texts. The key idea
    of our model is to enumerate all possible regions or spans as potential entity
    mentions and classify them with deep neural networks. To reduce the computational
    costs and capture the information of the contexts around the regions, the model
    represents the regions using the outputs of shared underlying bidirectional long
    short-term memory. We evaluate our exhaustive model on the GENIA and JNLPBA corpora
    in biomedical domain, and the results show that our model outperforms state-of-the-art
    models on nested and flat NER, achieving 77.1% and 78.4% respectively in terms
    of F-score, without any external knowledge resources.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306057139
    type: video
    url: https://vimeo.com/306057139
  author:
  - first: Mohammad Golam
    full: Mohammad Golam Sohrab
    id: mohammad-golam-sohrab
    last: Sohrab
  - first: Makoto
    full: Makoto Miwa
    id: makoto-miwa
    last: Miwa
  author_string: Mohammad Golam Sohrab, Makoto Miwa
  bibkey: sohrab-miwa-2018-deep
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1309
  month: October-November
  page_first: '2843'
  page_last: '2849'
  pages: "2843\u20132849"
  paper_id: '309'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1309.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1309.jpg
  title: Deep Exhaustive Model for Nested Named Entity Recognition
  title_html: Deep Exhaustive Model for Nested Named Entity Recognition
  url: https://www.aclweb.org/anthology/D18-1309
  year: '2018'
D18-1310:
  abstract: Conventional wisdom is that hand-crafted features are redundant for deep
    learning models, as they already learn adequate representations of text automatically
    from corpora. In this work, we test this claim by proposing a new method for exploiting
    handcrafted features as part of a novel hybrid learning approach, incorporating
    a feature auto-encoder loss component. We evaluate on the task of named entity
    recognition (NER), where we show that including manual features for part-of-speech,
    word shapes and gazetteers can improve the performance of a neural CRF model.
    We obtain a F 1 of 91.89 for the CoNLL-2003 English shared task, which significantly
    outperforms a collection of highly competitive baseline models. We also present
    an ablation study showing the importance of auto-encoding, over using features
    as either inputs or outputs alone, and moreover, show including the autoencoder
    components reduces training requirements to 60%, while retaining the same predictive
    accuracy.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306112516
    type: video
    url: https://vimeo.com/306112516
  author:
  - first: Minghao
    full: Minghao Wu
    id: minghao-wu
    last: Wu
  - first: Fei
    full: Fei Liu
    id: fei-liu-unimelb
    last: Liu
  - first: Trevor
    full: Trevor Cohn
    id: trevor-cohn
    last: Cohn
  author_string: Minghao Wu, Fei Liu, Trevor Cohn
  bibkey: wu-etal-2018-evaluating
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1310
  month: October-November
  page_first: '2850'
  page_last: '2856'
  pages: "2850\u20132856"
  paper_id: '310'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1310.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1310.jpg
  title: Evaluating the Utility of Hand-crafted Features in Sequence Labelling
  title_html: Evaluating the Utility of Hand-crafted Features in Sequence Labelling
  url: https://www.aclweb.org/anthology/D18-1310
  year: '2018'
D18-1311:
  abstract: Pre-trained word embeddings and language model have been shown useful
    in a lot of tasks. However, both of them cannot directly capture word connections
    in a sentence, which is important for dependency parsing given its goal is to
    establish dependency relations between words. In this paper, we propose to implicitly
    capture word connections from unlabeled data by a word ordering model with self-attention
    mechanism. Experiments show that these implicit word connections do improve our
    parsing model. Furthermore, by combining with a pre-trained language model, our
    model gets state-of-the-art performance on the English PTB dataset, achieving
    96.35% UAS and 95.25% LAS.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305667813
    type: video
    url: https://vimeo.com/305667813
  author:
  - first: Wenhui
    full: Wenhui Wang
    id: wenhui-wang
    last: Wang
  - first: Baobao
    full: Baobao Chang
    id: baobao-chang
    last: Chang
  - first: Mairgup
    full: Mairgup Mansur
    id: mairgup-mansur
    last: Mansur
  author_string: Wenhui Wang, Baobao Chang, Mairgup Mansur
  bibkey: wang-etal-2018-improved
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1311
  month: October-November
  page_first: '2857'
  page_last: '2863'
  pages: "2857\u20132863"
  paper_id: '311'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1311.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1311.jpg
  title: Improved Dependency Parsing using Implicit Word Connections Learned from
    Unlabeled Data
  title_html: Improved Dependency Parsing using Implicit Word Connections Learned
    from Unlabeled Data
  url: https://www.aclweb.org/anthology/D18-1311
  year: '2018'
D18-1312:
  abstract: This paper presents a simple framework for characterizing morphological
    complexity and how it encodes syntactic information. In particular, we propose
    a new measure of morpho-syntactic complexity in terms of governor-dependent preferential
    attachment that explains parsing performance. Through experiments on dependency
    parsing with data from Universal Dependencies (UD), we show that representations
    derived from morphological attributes deliver important parsing performance improvements
    over standard word form embeddings when trained on the same datasets. We also
    show that the new morpho-syntactic complexity measure is predictive of the gains
    provided by using morphological attributes over plain forms on parsing scores,
    making it a tool to distinguish languages using morphology as a syntactic marker
    from others.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1312.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1312.Attachment.pdf
  - filename: https://vimeo.com/305925065
    type: video
    url: https://vimeo.com/305925065
  author:
  - first: Mathieu
    full: Mathieu Dehouck
    id: mathieu-dehouck
    last: Dehouck
  - first: Pascal
    full: Pascal Denis
    id: pascal-denis
    last: Denis
  author_string: Mathieu Dehouck, Pascal Denis
  bibkey: dehouck-denis-2018-framework
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1312
  month: October-November
  page_first: '2864'
  page_last: '2870'
  pages: "2864\u20132870"
  paper_id: '312'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1312.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1312.jpg
  title: A Framework for Understanding the Role of Morphology in Universal Dependency
    Parsing
  title_html: A Framework for Understanding the Role of Morphology in Universal Dependency
    Parsing
  url: https://www.aclweb.org/anthology/D18-1312
  year: '2018'
D18-1313:
  abstract: Neural sequence-to-sequence models have proven very effective for machine
    translation, but at the expense of model interpretability. To shed more light
    into the role played by linguistic structure in the process of neural machine
    translation, we perform a fine-grained analysis of how various source-side morphological
    features are captured at different levels of the NMT encoder while varying the
    target language. Differently from previous work, we find no correlation between
    the accuracy of source morphology encoding and translation quality. We do find
    that morphological features are only captured in context and only to the extent
    that they are directly transferable to the target words.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305670159
    type: video
    url: https://vimeo.com/305670159
  author:
  - first: Arianna
    full: Arianna Bisazza
    id: arianna-bisazza
    last: Bisazza
  - first: Clara
    full: Clara Tump
    id: clara-tump
    last: Tump
  author_string: Arianna Bisazza, Clara Tump
  bibkey: bisazza-tump-2018-lazy
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1313
  month: October-November
  page_first: '2871'
  page_last: '2876'
  pages: "2871\u20132876"
  paper_id: '313'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1313.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1313.jpg
  title: 'The Lazy Encoder: A Fine-Grained Analysis of the Role of Morphology in Neural
    Machine Translation'
  title_html: 'The Lazy Encoder: A Fine-Grained Analysis of the Role of Morphology
    in Neural Machine Translation'
  url: https://www.aclweb.org/anthology/D18-1313
  year: '2018'
D18-1314:
  abstract: We employ imitation learning to train a neural transition-based string
    transducer for morphological tasks such as inflection generation and lemmatization.
    Previous approaches to training this type of model either rely on an external
    character aligner for the production of gold action sequences, which results in
    a suboptimal model due to the unwarranted dependence on a single gold action sequence
    despite spurious ambiguity, or require warm starting with an MLE model. Our approach
    only requires a simple expert policy, eliminating the need for a character aligner
    or warm start. It also addresses familiar MLE training biases and leads to strong
    and state-of-the-art performance on several benchmarks.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305671668
    type: video
    url: https://vimeo.com/305671668
  author:
  - first: Peter
    full: Peter Makarov
    id: peter-makarov
    last: Makarov
  - first: Simon
    full: Simon Clematide
    id: simon-clematide
    last: Clematide
  author_string: Peter Makarov, Simon Clematide
  bibkey: makarov-clematide-2018-imitation
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1314
  month: October-November
  page_first: '2877'
  page_last: '2882'
  pages: "2877\u20132882"
  paper_id: '314'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1314.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1314.jpg
  title: Imitation Learning for Neural Morphological String Transduction
  title_html: Imitation Learning for Neural Morphological String Transduction
  url: https://www.aclweb.org/anthology/D18-1314
  year: '2018'
D18-1315:
  abstract: The Paradigm Cell Filling Problem in morphology asks to complete word
    inflection tables from partial ones. We implement novel neural models for this
    task, evaluating them on 18 data sets in 8 languages, showing performance that
    is comparable with previous work with far less training data. We also publish
    a new dataset for this task and code implementing the system described in this
    paper.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305674091
    type: video
    url: https://vimeo.com/305674091
  author:
  - first: Miikka
    full: Miikka Silfverberg
    id: miikka-silfverberg
    last: Silfverberg
  - first: Mans
    full: Mans Hulden
    id: mans-hulden
    last: Hulden
  author_string: Miikka Silfverberg, Mans Hulden
  bibkey: silfverberg-hulden-2018-encoder
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1315
  month: October-November
  page_first: '2883'
  page_last: '2889'
  pages: "2883\u20132889"
  paper_id: '315'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1315.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1315.jpg
  title: An Encoder-Decoder Approach to the Paradigm Cell Filling Problem
  title_html: An Encoder-Decoder Approach to the Paradigm Cell Filling Problem
  url: https://www.aclweb.org/anthology/D18-1315
  year: '2018'
D18-1316:
  abstract: Deep neural networks (DNNs) are vulnerable to adversarial examples, perturbations
    to correctly classified examples which can cause the model to misclassify. In
    the image domain, these perturbations can often be made virtually indistinguishable
    to human perception, causing humans and state-of-the-art models to disagree. However,
    in the natural language domain, small perturbations are clearly perceptible, and
    the replacement of a single word can drastically alter the semantics of the document.
    Given these challenges, we use a black-box population-based optimization algorithm
    to generate semantically and syntactically similar adversarial examples that fool
    well-trained sentiment analysis and textual entailment models with success rates
    of 97% and 70%, respectively. We additionally demonstrate that 92.3% of the successful
    sentiment analysis adversarial examples are classified to their original label
    by 20 human annotators, and that the examples are perceptibly quite similar. Finally,
    we discuss an attempt to use adversarial training as a defense, but fail to yield
    improvement, demonstrating the strength and diversity of our adversarial examples.
    We hope our findings encourage researchers to pursue improving the robustness
    of DNNs in the natural language domain.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1316.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1316.Attachment.pdf
  author:
  - first: Moustafa
    full: Moustafa Alzantot
    id: moustafa-alzantot
    last: Alzantot
  - first: Yash
    full: Yash Sharma
    id: yash-sharma
    last: Sharma
  - first: Ahmed
    full: Ahmed Elgohary
    id: ahmed-elgohary
    last: Elgohary
  - first: Bo-Jhang
    full: Bo-Jhang Ho
    id: bo-jhang-ho
    last: Ho
  - first: Mani
    full: Mani Srivastava
    id: mani-srivastava
    last: Srivastava
  - first: Kai-Wei
    full: Kai-Wei Chang
    id: kai-wei-chang
    last: Chang
  author_string: Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani
    Srivastava, Kai-Wei Chang
  bibkey: alzantot-etal-2018-generating
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1316
  month: October-November
  page_first: '2890'
  page_last: '2896'
  pages: "2890\u20132896"
  paper_id: '316'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1316.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1316.jpg
  title: Generating Natural Language Adversarial Examples
  title_html: Generating Natural Language Adversarial Examples
  url: https://www.aclweb.org/anthology/D18-1316
  year: '2018'
D18-1317:
  abstract: Multi-head attention is appealing for the ability to jointly attend to
    information from different representation subspaces at different positions. In
    this work, we introduce a disagreement regularization to explicitly encourage
    the diversity among multiple attention heads. Specifically, we propose three types
    of disagreement regularization, which respectively encourage the subspace, the
    attended positions, and the output representation associated with each attention
    head to be different from other heads. Experimental results on widely-used WMT14
    English-German and WMT17 Chinese-English translation tasks demonstrate the effectiveness
    and universality of the proposed approach.
  address: Brussels, Belgium
  author:
  - first: Jian
    full: Jian Li
    id: jian-li
    last: Li
  - first: Zhaopeng
    full: Zhaopeng Tu
    id: zhaopeng-tu
    last: Tu
  - first: Baosong
    full: Baosong Yang
    id: baosong-yang
    last: Yang
  - first: Michael R.
    full: Michael R. Lyu
    id: michael-r-lyu
    last: Lyu
  - first: Tong
    full: Tong Zhang
    id: tong-zhang
    last: Zhang
  author_string: Jian Li, Zhaopeng Tu, Baosong Yang, Michael R. Lyu, Tong Zhang
  bibkey: li-etal-2018-multi-head
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1317
  month: October-November
  page_first: '2897'
  page_last: '2903'
  pages: "2897\u20132903"
  paper_id: '317'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1317.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1317.jpg
  title: Multi-Head Attention with Disagreement Regularization
  title_html: Multi-Head Attention with Disagreement Regularization
  url: https://www.aclweb.org/anthology/D18-1317
  year: '2018'
D18-1318:
  abstract: Several recent papers investigate Active Learning (AL) for mitigating
    the data dependence of deep learning for natural language processing. However,
    the applicability of AL to real-world problems remains an open question. While
    in supervised learning, practitioners can try many different methods, evaluating
    each against a validation set before selecting a model, AL affords no such luxury.
    Over the course of one AL run, an agent annotates its dataset exhausting its labeling
    budget. Thus, given a new task, we have no opportunity to compare models and acquisition
    functions. This paper provides a large-scale empirical study of deep active learning,
    addressing multiple tasks and, for each, multiple datasets, multiple models, and
    a full suite of acquisition functions. We find that across all settings, Bayesian
    active learning by disagreement, using uncertainty estimates provided either by
    Dropout or Bayes-by-Backprop significantly improves over i.i.d. baselines and
    usually outperforms classic uncertainty sampling.
  address: Brussels, Belgium
  author:
  - first: Aditya
    full: Aditya Siddhant
    id: aditya-siddhant
    last: Siddhant
  - first: Zachary C.
    full: Zachary C. Lipton
    id: zachary-c-lipton
    last: Lipton
  author_string: Aditya Siddhant, Zachary C. Lipton
  bibkey: siddhant-lipton-2018-deep
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1318
  month: October-November
  page_first: '2904'
  page_last: '2909'
  pages: "2904\u20132909"
  paper_id: '318'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1318.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1318.jpg
  title: 'Deep Bayesian Active Learning for Natural Language Processing: Results of
    a Large-Scale Empirical Study'
  title_html: 'Deep <span class="acl-fixed-case">B</span>ayesian Active Learning for
    Natural Language Processing: Results of a Large-Scale Empirical Study'
  url: https://www.aclweb.org/anthology/D18-1318
  year: '2018'
D18-1319:
  abstract: In natural language processing, a lot of the tasks are successfully solved
    with recurrent neural networks, but such models have a huge number of parameters.
    The majority of these parameters are often concentrated in the embedding layer,
    which size grows proportionally to the vocabulary length. We propose a Bayesian
    sparsification technique for RNNs which allows compressing the RNN dozens or hundreds
    of times without time-consuming hyperparameters tuning. We also generalize the
    model for vocabulary sparsification to filter out unnecessary words and compress
    the RNN even further. We show that the choice of the kept words is interpretable.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1319.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1319.Attachment.zip
  author:
  - first: Nadezhda
    full: Nadezhda Chirkova
    id: nadezhda-chirkova
    last: Chirkova
  - first: Ekaterina
    full: Ekaterina Lobacheva
    id: ekaterina-lobacheva
    last: Lobacheva
  - first: Dmitry
    full: Dmitry Vetrov
    id: dmitry-vetrov
    last: Vetrov
  author_string: Nadezhda Chirkova, Ekaterina Lobacheva, Dmitry Vetrov
  bibkey: chirkova-etal-2018-bayesian
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1319
  month: October-November
  page_first: '2910'
  page_last: '2915'
  pages: "2910\u20132915"
  paper_id: '319'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1319.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1319.jpg
  title: Bayesian Compression for Natural Language Processing
  title_html: <span class="acl-fixed-case">B</span>ayesian Compression for Natural
    Language Processing
  url: https://www.aclweb.org/anthology/D18-1319
  year: '2018'
D18-1320:
  abstract: 'Graphemes of most languages encode pronunciation, though some are more
    explicit than others. Languages like Spanish have a straightforward mapping between
    its graphemes and phonemes, while this mapping is more convoluted for languages
    like English. Spoken languages such as Cantonese present even more challenges
    in pronunciation modeling: (1) they do not have a standard written form, (2) the
    closest graphemic origins are logographic Han characters, of which only a subset
    of these logographic characters implicitly encodes pronunciation. In this work,
    we propose a multimodal approach to predict the pronunciation of Cantonese logographic
    characters, using neural networks with a geometric representation of logographs
    and pronunciation of cognates in historically related languages. The proposed
    framework improves performance by 18.1% and 25.0% respective to unimodal and multimodal
    baselines.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1320.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1320.Attachment.zip
  author:
  - first: Minh
    full: Minh Nguyen
    id: minh-nguyen
    last: Nguyen
  - first: Gia H.
    full: Gia H. Ngo
    id: gia-h-ngo1
    last: Ngo
  - first: Nancy
    full: Nancy Chen
    id: nancy-chen
    last: Chen
  author_string: Minh Nguyen, Gia H. Ngo, Nancy Chen
  bibkey: nguyen-etal-2018-multimodal
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1320
  month: October-November
  page_first: '2916'
  page_last: '2922'
  pages: "2916\u20132922"
  paper_id: '320'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1320.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1320.jpg
  title: Multimodal neural pronunciation modeling for spoken languages with logographic
    origin
  title_html: Multimodal neural pronunciation modeling for spoken languages with logographic
    origin
  url: https://www.aclweb.org/anthology/D18-1320
  year: '2018'
D18-1321:
  abstract: Chinese pinyin input method engine (IME) converts pinyin into character
    so that Chinese characters can be conveniently inputted into computer through
    common keyboard. IMEs work relying on its core component, pinyin-to-character
    conversion (P2C). Usually Chinese IMEs simply predict a list of character sequences
    for user choice only according to user pinyin input at each turn. However, Chinese
    inputting is a multi-turn online procedure, which can be supposed to be exploited
    for further user experience promoting. This paper thus for the first time introduces
    a sequence-to-sequence model with gated-attention mechanism for the core task
    in IMEs. The proposed neural P2C model is learned by encoding previous input utterance
    as extra context to enable our IME capable of predicting character sequence with
    incomplete pinyin input. Our model is evaluated in different benchmark datasets
    showing great user experience improvement compared to traditional models, which
    demonstrates the first engineering practice of building Chinese aided IME.
  address: Brussels, Belgium
  author:
  - first: Yafang
    full: Yafang Huang
    id: yafang-huang
    last: Huang
  - first: Hai
    full: Hai Zhao
    id: hai-zhao
    last: Zhao
  author_string: Yafang Huang, Hai Zhao
  bibkey: huang-zhao-2018-chinese
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1321
  month: October-November
  page_first: '2923'
  page_last: '2929'
  pages: "2923\u20132929"
  paper_id: '321'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1321.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1321.jpg
  title: Chinese Pinyin Aided IME, Input What You Have Not Keystroked Yet
  title_html: <span class="acl-fixed-case">C</span>hinese Pinyin Aided <span class="acl-fixed-case">IME</span>,
    Input What You Have Not Keystroked Yet
  url: https://www.aclweb.org/anthology/D18-1321
  year: '2018'
D18-1322:
  abstract: "Recurrent neural network language models (RNNLMs) are the current standard-bearer\
    \ for statistical language modeling. However, RNNLMs only estimate probabilities\
    \ for complete sequences of text, whereas some applications require context-independent\
    \ phrase probabilities instead. In this paper, we study how to compute an RNNLM\u2019\
    s em marginal probability: the probability that the model assigns to a short sequence\
    \ of text when the preceding context is not known. We introduce a simple method\
    \ of altering the RNNLM training to make the model more accurate at marginal estimation.\
    \ Our experiments demonstrate that the technique is effective compared to baselines\
    \ including the traditional RNNLM probability and an importance sampling approach.\
    \ Finally, we show how we can use the marginal estimation to improve an RNNLM\
    \ by training the marginals to match n-gram probabilities from a larger corpus."
  address: Brussels, Belgium
  author:
  - first: Thanapon
    full: Thanapon Noraset
    id: thanapon-noraset
    last: Noraset
  - first: Doug
    full: Doug Downey
    id: doug-downey
    last: Downey
  - first: Lidong
    full: Lidong Bing
    id: lidong-bing
    last: Bing
  author_string: Thanapon Noraset, Doug Downey, Lidong Bing
  bibkey: noraset-etal-2018-estimating
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1322
  month: October-November
  page_first: '2930'
  page_last: '2935'
  pages: "2930\u20132935"
  paper_id: '322'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1322.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1322.jpg
  title: Estimating Marginal Probabilities of n-grams for Recurrent Neural Language
    Models
  title_html: Estimating Marginal Probabilities of n-grams for Recurrent Neural Language
    Models
  url: https://www.aclweb.org/anthology/D18-1322
  year: '2018'
D18-1323:
  abstract: Recent state-of-the-art neural language models share the representations
    of words given by the input and output mappings. We propose a simple modification
    to these architectures that decouples the hidden state from the word embedding
    prediction. Our architecture leads to comparable or better results compared to
    previous tied models and models without tying, with a much smaller number of parameters.
    We also extend our proposal to word2vec models, showing that tying is appropriate
    for general word prediction tasks.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1323.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1323.Attachment.pdf
  author:
  - first: Kristina
    full: Kristina Gulordava
    id: kristina-gulordava
    last: Gulordava
  - first: Laura
    full: Laura Aina
    id: laura-aina
    last: Aina
  - first: Gemma
    full: Gemma Boleda
    id: gemma-boleda
    last: Boleda
  author_string: Kristina Gulordava, Laura Aina, Gemma Boleda
  bibkey: gulordava-etal-2018-represent
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1323
  month: October-November
  page_first: '2936'
  page_last: '2941'
  pages: "2936\u20132941"
  paper_id: '323'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1323.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1323.jpg
  title: 'How to represent a word and predict it, too: Improving tied architectures
    for language modelling'
  title_html: 'How to represent a word and predict it, too: Improving tied architectures
    for language modelling'
  url: https://www.aclweb.org/anthology/D18-1323
  year: '2018'
D18-1324:
  abstract: "Neural language models are a critical component of state-of-the-art systems\
    \ for machine translation, summarization, audio transcription, and other tasks.\
    \ These language models are almost universally autoregressive in nature, generating\
    \ sentences one token at a time from left to right. This paper studies the influence\
    \ of token generation order on model quality via a novel two-pass language model\
    \ that produces partially-filled sentence \u201Ctemplates\u201D and then fills\
    \ in missing tokens. We compare various strategies for structuring these two passes\
    \ and observe a surprisingly large variation in model quality. We find the most\
    \ effective strategy generates function words in the first pass followed by content\
    \ words in the second. We believe these experimental results justify a more extensive\
    \ investigation of the generation order for neural language models."
  address: Brussels, Belgium
  author:
  - first: Nicolas
    full: Nicolas Ford
    id: nicolas-ford
    last: Ford
  - first: Daniel
    full: Daniel Duckworth
    id: daniel-duckworth
    last: Duckworth
  - first: Mohammad
    full: Mohammad Norouzi
    id: mohammad-norouzi
    last: Norouzi
  - first: George
    full: George Dahl
    id: george-dahl
    last: Dahl
  author_string: Nicolas Ford, Daniel Duckworth, Mohammad Norouzi, George Dahl
  bibkey: ford-etal-2018-importance
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1324
  month: October-November
  page_first: '2942'
  page_last: '2946'
  pages: "2942\u20132946"
  paper_id: '324'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1324.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1324.jpg
  title: The Importance of Generation Order in Language Modeling
  title_html: The Importance of Generation Order in Language Modeling
  url: https://www.aclweb.org/anthology/D18-1324
  year: '2018'
D18-1325:
  abstract: "Neural Machine Translation (NMT) can be improved by including document-level\
    \ contextual information. For this purpose, we propose a hierarchical attention\
    \ model to capture the context in a structured and dynamic manner. The model is\
    \ integrated in the original NMT architecture as another level of abstraction,\
    \ conditioning on the NMT model\u2019s own previous hidden states. Experiments\
    \ show that hierarchical attention significantly improves the BLEU score over\
    \ a strong NMT baseline with the state-of-the-art in context-aware methods, and\
    \ that both the encoder and decoder benefit from context in complementary ways."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1325.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1325.Attachment.pdf
  author:
  - first: Lesly
    full: Lesly Miculicich
    id: lesly-miculicich-werlen
    last: Miculicich
  - first: Dhananjay
    full: Dhananjay Ram
    id: dhananjay-ram
    last: Ram
  - first: Nikolaos
    full: Nikolaos Pappas
    id: nikolaos-pappas
    last: Pappas
  - first: James
    full: James Henderson
    id: james-henderson
    last: Henderson
  author_string: Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas, James Henderson
  bibkey: miculicich-etal-2018-document
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1325
  month: October-November
  page_first: '2947'
  page_last: '2954'
  pages: "2947\u20132954"
  paper_id: '325'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1325.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1325.jpg
  title: Document-Level Neural Machine Translation with Hierarchical Attention Networks
  title_html: Document-Level Neural Machine Translation with Hierarchical Attention
    Networks
  url: https://www.aclweb.org/anthology/D18-1325
  year: '2018'
D18-1326:
  abstract: Due to the benefits of model compactness, multilingual translation (including
    many-to-one, many-to-many and one-to-many) based on a universal encoder-decoder
    architecture attracts more and more attention. However, previous studies show
    that one-to-many translation based on this framework cannot perform on par with
    the individually trained models. In this work, we introduce three strategies to
    improve one-to-many multilingual translation by balancing the shared and unique
    features. Within the architecture of one decoder for all target languages, we
    first exploit the use of unique initial states for different target languages.
    Then, we employ language-dependent positional embeddings. Finally and especially,
    we propose to divide the hidden cells of the decoder into shared and language-dependent
    ones. The extensive experiments demonstrate that our proposed methods can obtain
    remarkable improvements over the strong baselines. Moreover, our strategies can
    achieve comparable or even better performance than the individually trained translation
    models.
  address: Brussels, Belgium
  author:
  - first: Yining
    full: Yining Wang
    id: yining-wang
    last: Wang
  - first: Jiajun
    full: Jiajun Zhang
    id: jiajun-zhang
    last: Zhang
  - first: Feifei
    full: Feifei Zhai
    id: feifei-zhai
    last: Zhai
  - first: Jingfang
    full: Jingfang Xu
    id: jingfang-xu
    last: Xu
  - first: Chengqing
    full: Chengqing Zong
    id: chengqing-zong
    last: Zong
  author_string: Yining Wang, Jiajun Zhang, Feifei Zhai, Jingfang Xu, Chengqing Zong
  bibkey: wang-etal-2018-three
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1326
  month: October-November
  page_first: '2955'
  page_last: '2960'
  pages: "2955\u20132960"
  paper_id: '326'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1326.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1326.jpg
  title: Three Strategies to Improve One-to-Many Multilingual Translation
  title_html: Three Strategies to Improve One-to-Many Multilingual Translation
  url: https://www.aclweb.org/anthology/D18-1326
  year: '2018'
D18-1327:
  abstract: We introduce a novel multi-source technique for incorporating source syntax
    into neural machine translation using linearized parses. This is achieved by employing
    separate encoders for the sequential and parsed versions of the same source sentence;
    the resulting representations are then combined using a hierarchical attention
    mechanism. The proposed model improves over both seq2seq and parsed baselines
    by over 1 BLEU on the WMT17 English-German task. Further analysis shows that our
    multi-source syntactic model is able to translate successfully without any parsed
    input, unlike standard parsed methods. In addition, performance does not deteriorate
    as much on long sentences as for the baselines.
  address: Brussels, Belgium
  author:
  - first: Anna
    full: Anna Currey
    id: anna-currey
    last: Currey
  - first: Kenneth
    full: Kenneth Heafield
    id: kenneth-heafield
    last: Heafield
  author_string: Anna Currey, Kenneth Heafield
  bibkey: currey-heafield-2018-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1327
  month: October-November
  page_first: '2961'
  page_last: '2966'
  pages: "2961\u20132966"
  paper_id: '327'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1327.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1327.jpg
  title: Multi-Source Syntactic Neural Machine Translation
  title_html: Multi-Source Syntactic Neural Machine Translation
  url: https://www.aclweb.org/anthology/D18-1327
  year: '2018'
D18-1328:
  abstract: Corpus-based approaches to machine translation rely on the availability
    of clean parallel corpora. Such resources are scarce, and because of the automatic
    processes involved in their preparation, they are often noisy. This paper describes
    an unsupervised method for detecting translation divergences in parallel sentences.
    We rely on a neural network that computes cross-lingual sentence similarity scores,
    which are then used to effectively filter out divergent translations. Furthermore,
    similarity scores predicted by the network are used to identify and fix some partial
    divergences, yielding additional parallel segments. We evaluate these methods
    for English-French and English-German machine translation tasks, and show that
    using filtered/corrected corpora actually improves MT performance.
  address: Brussels, Belgium
  author:
  - first: MinhQuang
    full: MinhQuang Pham
    id: minh-quang-pham
    last: Pham
  - first: Josep
    full: Josep Crego
    id: josep-m-crego
    last: Crego
  - first: Jean
    full: Jean Senellart
    id: jean-senellart
    last: Senellart
  - first: "Fran\xE7ois"
    full: "Fran\xE7ois Yvon"
    id: francois-yvon
    last: Yvon
  author_string: "MinhQuang Pham, Josep Crego, Jean Senellart, Fran\xE7ois Yvon"
  bibkey: pham-etal-2018-fixing
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1328
  month: October-November
  page_first: '2967'
  page_last: '2973'
  pages: "2967\u20132973"
  paper_id: '328'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1328.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1328.jpg
  title: Fixing Translation Divergences in Parallel Corpora for Neural MT
  title_html: Fixing Translation Divergences in Parallel Corpora for Neural <span
    class="acl-fixed-case">MT</span>
  url: https://www.aclweb.org/anthology/D18-1328
  year: '2018'
D18-1329:
  abstract: The promise of combining language and vision in multimodal machine translation
    is that systems will produce better translations by leveraging the image data.
    However, the evidence surrounding whether the images are useful is unconvincing
    due to inconsistencies between text-similarity metrics and human judgements. We
    present an adversarial evaluation to directly examine the utility of the image
    data in this task. Our evaluation tests whether systems perform better when paired
    with congruent images or incongruent images. This evaluation shows that only one
    out of three publicly available systems is sensitive to this perturbation of the
    data. We recommend that multimodal translation systems should be able to pass
    this sanity check in the future.
  address: Brussels, Belgium
  author:
  - first: Desmond
    full: Desmond Elliott
    id: desmond-elliott
    last: Elliott
  author_string: Desmond Elliott
  bibkey: elliott-2018-adversarial
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1329
  month: October-November
  page_first: '2974'
  page_last: '2978'
  pages: "2974\u20132978"
  paper_id: '329'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1329.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1329.jpg
  title: Adversarial Evaluation of Multimodal Machine Translation
  title_html: Adversarial Evaluation of Multimodal Machine Translation
  url: https://www.aclweb.org/anthology/D18-1329
  year: '2018'
D18-1330:
  abstract: Continuous word representations learned separately on distinct languages
    can be aligned so that their words become comparable in a common space. Existing
    works typically solve a quadratic problem to learn a orthogonal matrix aligning
    a bilingual lexicon, and use a retrieval criterion for inference. In this paper,
    we propose an unified formulation that directly optimizes a retrieval criterion
    in an end-to-end fashion. Our experiments on standard benchmarks show that our
    approach outperforms the state of the art on word translation, with the biggest
    improvements observed for distant language pairs such as English-Chinese.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1330.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1330.Attachment.zip
  author:
  - first: Armand
    full: Armand Joulin
    id: armand-joulin
    last: Joulin
  - first: Piotr
    full: Piotr Bojanowski
    id: piotr-bojanowski
    last: Bojanowski
  - first: Tomas
    full: Tomas Mikolov
    id: tomas-mikolov
    last: Mikolov
  - first: "Herv\xE9"
    full: "Herv\xE9 J\xE9gou"
    id: herve-jegou
    last: "J\xE9gou"
  - first: Edouard
    full: Edouard Grave
    id: edouard-grave
    last: Grave
  author_string: "Armand Joulin, Piotr Bojanowski, Tomas Mikolov, Herv\xE9 J\xE9gou,\
    \ Edouard Grave"
  bibkey: joulin-etal-2018-loss
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1330
  month: October-November
  page_first: '2979'
  page_last: '2984'
  pages: "2979\u20132984"
  paper_id: '330'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1330.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1330.jpg
  title: 'Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion'
  title_html: 'Loss in Translation: Learning Bilingual Word Mapping with a Retrieval
    Criterion'
  url: https://www.aclweb.org/anthology/D18-1330
  year: '2018'
D18-1331:
  abstract: Most of the Neural Machine Translation (NMT) models are based on the sequence-to-sequence
    (Seq2Seq) model with an encoder-decoder framework equipped with the attention
    mechanism. However, the conventional attention mechanism treats the decoding at
    each time step equally with the same matrix, which is problematic since the softness
    of the attention for different types of words (e.g. content words and function
    words) should differ. Therefore, we propose a new model with a mechanism called
    Self-Adaptive Control of Temperature (SACT) to control the softness of attention
    by means of an attention temperature. Experimental results on the Chinese-English
    translation and English-Vietnamese translation demonstrate that our model outperforms
    the baseline models, and the analysis and the case study show that our model can
    attend to the most relevant elements in the source-side contexts and generate
    the translation of high quality.
  address: Brussels, Belgium
  author:
  - first: Junyang
    full: Junyang Lin
    id: junyang-lin
    last: Lin
  - first: Xu
    full: Xu Sun
    id: xu-sun
    last: Sun
  - first: Xuancheng
    full: Xuancheng Ren
    id: xuancheng-ren
    last: Ren
  - first: Muyu
    full: Muyu Li
    id: muyu-li
    last: Li
  - first: Qi
    full: Qi Su
    id: qi-su
    last: Su
  author_string: Junyang Lin, Xu Sun, Xuancheng Ren, Muyu Li, Qi Su
  bibkey: lin-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1331
  month: October-November
  page_first: '2985'
  page_last: '2990'
  pages: "2985\u20132990"
  paper_id: '331'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1331.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1331.jpg
  title: 'Learning When to Concentrate or Divert Attention: Self-Adaptive Attention
    Temperature for Neural Machine Translation'
  title_html: 'Learning When to Concentrate or Divert Attention: Self-Adaptive Attention
    Temperature for Neural Machine Translation'
  url: https://www.aclweb.org/anthology/D18-1331
  year: '2018'
D18-1332:
  abstract: In order to extract the best possible performance from asynchronous stochastic
    gradient descent one must increase the mini-batch size and scale the learning
    rate accordingly. In order to achieve further speedup we introduce a technique
    that delays gradient updates effectively increasing the mini-batch size. Unfortunately
    with the increase of mini-batch size we worsen the stale gradient problem in asynchronous
    stochastic gradient descent (SGD) which makes the model convergence poor. We introduce
    local optimizers which mitigate the stale gradient problem and together with fine
    tuning our momentum we are able to train a shallow machine translation system
    27% faster than an optimized baseline with negligible penalty in BLEU.
  address: Brussels, Belgium
  author:
  - first: Nikolay
    full: Nikolay Bogoychev
    id: nikolay-bogoychev
    last: Bogoychev
  - first: Kenneth
    full: Kenneth Heafield
    id: kenneth-heafield
    last: Heafield
  - first: Alham Fikri
    full: Alham Fikri Aji
    id: alham-fikri-aji
    last: Aji
  - first: Marcin
    full: Marcin Junczys-Dowmunt
    id: marcin-junczys-dowmunt
    last: Junczys-Dowmunt
  author_string: Nikolay Bogoychev, Kenneth Heafield, Alham Fikri Aji, Marcin Junczys-Dowmunt
  bibkey: bogoychev-etal-2018-accelerating
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1332
  month: October-November
  page_first: '2991'
  page_last: '2996'
  pages: "2991\u20132996"
  paper_id: '332'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1332.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1332.jpg
  title: Accelerating Asynchronous Stochastic Gradient Descent for Neural Machine
    Translation
  title_html: Accelerating Asynchronous Stochastic Gradient Descent for Neural Machine
    Translation
  url: https://www.aclweb.org/anthology/D18-1332
  year: '2018'
D18-1333:
  abstract: Pronouns are frequently omitted in pro-drop languages, such as Chinese,
    generally leading to significant challenges with respect to the production of
    complete translations. Recently, Wang et al. (2018) proposed a novel reconstruction-based
    approach to alleviating dropped pronoun (DP) translation problems for neural machine
    translation models. In this work, we improve the original model from two perspectives.
    First, we employ a shared reconstructor to better exploit encoder and decoder
    representations. Second, we jointly learn to translate and predict DPs in an end-to-end
    manner, to avoid the errors propagated from an external DP prediction model. Experimental
    results show that our approach significantly improves both translation performance
    and DP prediction accuracy.
  address: Brussels, Belgium
  author:
  - first: Longyue
    full: Longyue Wang
    id: longyue-wang
    last: Wang
  - first: Zhaopeng
    full: Zhaopeng Tu
    id: zhaopeng-tu
    last: Tu
  - first: Andy
    full: Andy Way
    id: andy-way
    last: Way
  - first: Qun
    full: Qun Liu
    id: qun-liu
    last: Liu
  author_string: Longyue Wang, Zhaopeng Tu, Andy Way, Qun Liu
  bibkey: wang-etal-2018-learning-jointly
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1333
  month: October-November
  page_first: '2997'
  page_last: '3002'
  pages: "2997\u20133002"
  paper_id: '333'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1333.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1333.jpg
  title: Learning to Jointly Translate and Predict Dropped Pronouns with a Shared
    Reconstruction Mechanism
  title_html: Learning to Jointly Translate and Predict Dropped Pronouns with a Shared
    Reconstruction Mechanism
  url: https://www.aclweb.org/anthology/D18-1333
  year: '2018'
D18-1334:
  abstract: "Speakers of different languages must attend to and encode strikingly\
    \ different aspects of the world in order to use their language correctly (Sapir,\
    \ 1921; Slobin, 1996). One such difference is related to the way gender is expressed\
    \ in a language. Saying \u201CI am happy\u201D in English, does not encode any\
    \ additional knowledge of the speaker that uttered the sentence. However, many\
    \ other languages do have grammatical gender systems and so such knowledge would\
    \ be encoded. In order to correctly translate such a sentence into, say, French,\
    \ the inherent gender information needs to be retained/recovered. The same sentence\
    \ would become either \u201CJe suis heureux\u201D, for a male speaker or \u201C\
    Je suis heureuse\u201D for a female one. Apart from morphological agreement, demographic\
    \ factors (gender, age, etc.) also influence our use of language in terms of word\
    \ choices or syntactic constructions (Tannen, 1991; Pennebaker et al., 2003).\
    \ We integrate gender information into NMT systems. Our contribution is two-fold:\
    \ (1) the compilation of large datasets with speaker information for 20 language\
    \ pairs, and (2) a simple set of experiments that incorporate gender information\
    \ into NMT for multiple language pairs. Our experiments show that adding a gender\
    \ feature to an NMT system significantly improves the translation quality for\
    \ some language pairs."
  address: Brussels, Belgium
  author:
  - first: Eva
    full: Eva Vanmassenhove
    id: eva-vanmassenhove
    last: Vanmassenhove
  - first: Christian
    full: Christian Hardmeier
    id: christian-hardmeier
    last: Hardmeier
  - first: Andy
    full: Andy Way
    id: andy-way
    last: Way
  author_string: Eva Vanmassenhove, Christian Hardmeier, Andy Way
  bibkey: vanmassenhove-etal-2018-getting
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1334
  month: October-November
  page_first: '3003'
  page_last: '3008'
  pages: "3003\u20133008"
  paper_id: '334'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1334.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1334.jpg
  title: Getting Gender Right in Neural Machine Translation
  title_html: Getting Gender Right in Neural Machine Translation
  url: https://www.aclweb.org/anthology/D18-1334
  year: '2018'
D18-1335:
  abstract: This work investigates an alternative model for neural machine translation
    (NMT) and proposes a novel architecture, where we employ a multi-dimensional long
    short-term memory (MDLSTM) for translation modelling. In the state-of-the-art
    methods, source and target sentences are treated as one-dimensional sequences
    over time, while we view translation as a two-dimensional (2D) mapping using an
    MDLSTM layer to define the correspondence between source and target words. We
    extend beyond the current sequence to sequence backbone NMT models to a 2D structure
    in which the source and target sentences are aligned with each other in a 2D grid.
    Our proposed topology shows consistent improvements over attention-based sequence
    to sequence model on two WMT 2017 tasks, German<->English.
  address: Brussels, Belgium
  author:
  - first: Parnia
    full: Parnia Bahar
    id: parnia-bahar
    last: Bahar
  - first: Christopher
    full: Christopher Brix
    id: christopher-brix
    last: Brix
  - first: Hermann
    full: Hermann Ney
    id: hermann-ney
    last: Ney
  author_string: Parnia Bahar, Christopher Brix, Hermann Ney
  bibkey: bahar-etal-2018-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1335
  month: October-November
  page_first: '3009'
  page_last: '3015'
  pages: "3009\u20133015"
  paper_id: '335'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1335.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1335.jpg
  title: Towards Two-Dimensional Sequence to Sequence Model in Neural Machine Translation
  title_html: Towards Two-Dimensional Sequence to Sequence Model in Neural Machine
    Translation
  url: https://www.aclweb.org/anthology/D18-1335
  year: '2018'
D18-1336:
  abstract: Autoregressive decoding is the only part of sequence-to-sequence models
    that prevents them from massive parallelization at inference time. Non-autoregressive
    models enable the decoder to generate all output symbols independently in parallel.
    We present a novel non-autoregressive architecture based on connectionist temporal
    classification and evaluate it on the task of neural machine translation. Unlike
    other non-autoregressive methods which operate in several steps, our model can
    be trained end-to-end. We conduct experiments on the WMT English-Romanian and
    English-German datasets. Our models achieve a significant speedup over the autoregressive
    models, keeping the translation quality comparable to other non-autoregressive
    models.
  address: Brussels, Belgium
  author:
  - first: "Jind\u0159ich"
    full: "Jind\u0159ich Libovick\xFD"
    id: jindrich-libovicky
    last: "Libovick\xFD"
  - first: "Jind\u0159ich"
    full: "Jind\u0159ich Helcl"
    id: jindrich-helcl
    last: Helcl
  author_string: "Jind\u0159ich Libovick\xFD, Jind\u0159ich Helcl"
  bibkey: libovicky-helcl-2018-end
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1336
  month: October-November
  page_first: '3016'
  page_last: '3021'
  pages: "3016\u20133021"
  paper_id: '336'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1336.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1336.jpg
  title: End-to-End Non-Autoregressive Neural Machine Translation with Connectionist
    Temporal Classification
  title_html: End-to-End Non-Autoregressive Neural Machine Translation with Connectionist
    Temporal Classification
  url: https://www.aclweb.org/anthology/D18-1336
  year: '2018'
D18-1337:
  abstract: Simultaneous speech translation aims to maintain translation quality while
    minimizing the delay between reading input and incrementally producing the output.
    We propose a new general-purpose prediction action which predicts future words
    in the input to improve quality and minimize delay in simultaneous translation.
    We train this agent using reinforcement learning with a novel reward function.
    Our agent with prediction has better translation quality and less delay compared
    to an agent-based simultaneous translation system without prediction.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1337.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1337.Attachment.zip
  author:
  - first: Ashkan
    full: Ashkan Alinejad
    id: ashkan-alinejad
    last: Alinejad
  - first: Maryam
    full: Maryam Siahbani
    id: maryam-siahbani
    last: Siahbani
  - first: Anoop
    full: Anoop Sarkar
    id: anoop-sarkar
    last: Sarkar
  author_string: Ashkan Alinejad, Maryam Siahbani, Anoop Sarkar
  bibkey: alinejad-etal-2018-prediction
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1337
  month: October-November
  page_first: '3022'
  page_last: '3027'
  pages: "3022\u20133027"
  paper_id: '337'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1337.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1337.jpg
  title: Prediction Improves Simultaneous Neural Machine Translation
  title_html: Prediction Improves Simultaneous Neural Machine Translation
  url: https://www.aclweb.org/anthology/D18-1337
  year: '2018'
D18-1338:
  abstract: "While current state-of-the-art NMT models, such as RNN seq2seq and Transformers,\
    \ possess a large number of parameters, they are still shallow in comparison to\
    \ convolutional models used for both text and vision applications. In this work\
    \ we attempt to train significantly (2-3x) deeper Transformer and Bi-RNN encoders\
    \ for machine translation. We propose a simple modification to the attention mechanism\
    \ that eases the optimization of deeper models, and results in consistent gains\
    \ of 0.7-1.1 BLEU on the benchmark WMT\u201914 English-German and WMT\u201915\
    \ Czech-English tasks for both architectures."
  address: Brussels, Belgium
  author:
  - first: Ankur
    full: Ankur Bapna
    id: ankur-bapna
    last: Bapna
  - first: Mia
    full: Mia Chen
    id: mia-xu-chen
    last: Chen
  - first: Orhan
    full: Orhan Firat
    id: orhan-firat
    last: Firat
  - first: Yuan
    full: Yuan Cao
    id: yuan-cao
    last: Cao
  - first: Yonghui
    full: Yonghui Wu
    id: yonghui-wu
    last: Wu
  author_string: Ankur Bapna, Mia Chen, Orhan Firat, Yuan Cao, Yonghui Wu
  bibkey: bapna-etal-2018-training
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1338
  month: October-November
  page_first: '3028'
  page_last: '3033'
  pages: "3028\u20133033"
  paper_id: '338'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1338.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1338.jpg
  title: Training Deeper Neural Machine Translation Models with Transparent Attention
  title_html: Training Deeper Neural Machine Translation Models with Transparent Attention
  url: https://www.aclweb.org/anthology/D18-1338
  year: '2018'
D18-1339:
  abstract: Neural machine translation systems with subword vocabularies are capable
    of translating or copying unknown words. In this work, we show that they learn
    to copy words based on both the context in which the words appear as well as features
    of the words themselves. In contexts that are particularly copy-prone, they even
    copy words that they have already learned they should translate. We examine the
    influence of context and subword features on this and other types of copying behavior.
  address: Brussels, Belgium
  author:
  - first: Rebecca
    full: Rebecca Knowles
    id: rebecca-knowles
    last: Knowles
  - first: Philipp
    full: Philipp Koehn
    id: philipp-koehn
    last: Koehn
  author_string: Rebecca Knowles, Philipp Koehn
  bibkey: knowles-koehn-2018-context
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1339
  month: October-November
  page_first: '3034'
  page_last: '3041'
  pages: "3034\u20133041"
  paper_id: '339'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1339.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1339.jpg
  title: Context and Copying in Neural Machine Translation
  title_html: Context and Copying in Neural Machine Translation
  url: https://www.aclweb.org/anthology/D18-1339
  year: '2018'
D18-1340:
  abstract: Translation memories (TM) facilitate human translators to reuse existing
    repetitive translation fragments. In this paper, we propose a novel method to
    combine the strengths of both TM and neural machine translation (NMT) for high-quality
    translation. We treat the target translation of a TM match as an additional reference
    input and encode it into NMT with an extra encoder. A gating mechanism is further
    used to balance the impact of the TM match on the NMT decoder. Experiment results
    on the UN corpus demonstrate that when fuzzy matches are higher than 50%, the
    quality of NMT translation can be significantly improved by over 10 BLEU points.
  address: Brussels, Belgium
  author:
  - first: Qian
    full: Qian Cao
    id: qian-cao
    last: Cao
  - first: Deyi
    full: Deyi Xiong
    id: deyi-xiong
    last: Xiong
  author_string: Qian Cao, Deyi Xiong
  bibkey: cao-xiong-2018-encoding
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1340
  month: October-November
  page_first: '3042'
  page_last: '3047'
  pages: "3042\u20133047"
  paper_id: '340'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1340.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1340.jpg
  title: Encoding Gated Translation Memory into Neural Machine Translation
  title_html: Encoding Gated Translation Memory into Neural Machine Translation
  url: https://www.aclweb.org/anthology/D18-1340
  year: '2018'
D18-1341:
  abstract: Automated Post-Editing (PE) is the task of automatically correct common
    and repetitive errors found in machine translation (MT) output. In this paper,
    we present a neural programmer-interpreter approach to this task, resembling the
    way that human perform post-editing using discrete edit operations, wich we refer
    to as programs. Our model outperforms previous neural models for inducing PE programs
    on the WMT17 APE task for German-English up to +1 BLEU score and -0.7 TER scores.
  address: Brussels, Belgium
  author:
  - first: Thuy-Trang
    full: Thuy-Trang Vu
    id: thuy-vu
    last: Vu
  - first: Gholamreza
    full: Gholamreza Haffari
    id: gholamreza-haffari
    last: Haffari
  author_string: Thuy-Trang Vu, Gholamreza Haffari
  bibkey: vu-haffari-2018-automatic
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1341
  month: October-November
  page_first: '3048'
  page_last: '3053'
  pages: "3048\u20133053"
  paper_id: '341'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1341.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1341.jpg
  title: 'Automatic Post-Editing of Machine Translation: A Neural Programmer-Interpreter
    Approach'
  title_html: 'Automatic Post-Editing of Machine Translation: A Neural Programmer-Interpreter
    Approach'
  url: https://www.aclweb.org/anthology/D18-1341
  year: '2018'
D18-1342:
  abstract: Beam search is widely used in neural machine translation, and usually
    improves translation quality compared to greedy search. It has been widely observed
    that, however, beam sizes larger than 5 hurt translation quality. We explain why
    this happens, and propose several methods to address this problem. Furthermore,
    we discuss the optimal stopping criteria for these methods. Results show that
    our hyperparameter-free methods outperform the widely-used hyperparameter-free
    heuristic of length normalization by +2.0 BLEU, and achieve the best results among
    all methods on Chinese-to-English translation.
  address: Brussels, Belgium
  author:
  - first: Yilin
    full: Yilin Yang
    id: yilin-yang
    last: Yang
  - first: Liang
    full: Liang Huang
    id: liang-huang
    last: Huang
  - first: Mingbo
    full: Mingbo Ma
    id: mingbo-ma
    last: Ma
  author_string: Yilin Yang, Liang Huang, Mingbo Ma
  bibkey: yang-etal-2018-breaking
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1342
  month: October-November
  page_first: '3054'
  page_last: '3059'
  pages: "3054\u20133059"
  paper_id: '342'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1342.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1342.jpg
  title: 'Breaking the Beam Search Curse: A Study of (Re-)Scoring Methods and Stopping
    Criteria for Neural Machine Translation'
  title_html: 'Breaking the Beam Search Curse: A Study of (Re-)Scoring Methods and
    Stopping Criteria for Neural Machine Translation'
  url: https://www.aclweb.org/anthology/D18-1342
  year: '2018'
D18-1343:
  abstract: "Accurate and complete knowledge bases (KBs) are paramount in NLP. We\
    \ employ mul-itiview learning for increasing the accuracy and coverage of entity\
    \ type information in KBs. We rely on two metaviews: language and representation.\
    \ For language, we consider high-resource and low-resource languages from Wikipedia.\
    \ For representation, we consider representations based on the context distribution\
    \ of the entity (i.e., on its embedding), on the entity\u2019s name (i.e., on\
    \ its surface form) and on its description in Wikipedia. The two metaviews language\
    \ and representation can be freely combined: each pair of language and representation\
    \ (e.g., German embedding, English description, Spanish name) is a distinct view.\
    \ Our experiments on entity typing with fine-grained classes demonstrate the effectiveness\
    \ of multiview learning. We release MVET, a large multiview \u2014 and, in particular,\
    \ multilingual \u2014 entity typing dataset we created. Mono- and multilingual\
    \ fine-grained entity typing systems can be evaluated on this dataset."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1343.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1343.Attachment.zip
  author:
  - first: Yadollah
    full: Yadollah Yaghoobzadeh
    id: yadollah-yaghoobzadeh
    last: Yaghoobzadeh
  - first: Hinrich
    full: "Hinrich Sch\xFCtze"
    id: hinrich-schutze
    last: "Sch\xFCtze"
  author_string: "Yadollah Yaghoobzadeh, Hinrich Sch\xFCtze"
  bibkey: yaghoobzadeh-schutze-2018-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1343
  month: October-November
  page_first: '3060'
  page_last: '3066'
  pages: "3060\u20133066"
  paper_id: '343'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1343.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1343.jpg
  title: 'Multi-Multi-View Learning: Multilingual and Multi-Representation Entity
    Typing'
  title_html: 'Multi-Multi-View Learning: Multilingual and Multi-Representation Entity
    Typing'
  url: https://www.aclweb.org/anthology/D18-1343
  year: '2018'
D18-1344:
  abstract: We compare three existing bilingual word embedding approaches, and a novel
    approach of training skip-grams on synthetic code-mixed text generated through
    linguistic models of code-mixing, on two tasks - sentiment analysis and POS tagging
    for code-mixed text. Our results show that while CVM and CCA based embeddings
    perform as well as the proposed embedding technique on semantic and syntactic
    tasks respectively, the proposed approach provides the best performance for both
    tasks overall. Thus, this study demonstrates that existing bilingual embedding
    techniques are not ideal for code-mixed text processing and there is a need for
    learning multilingual word embedding from the code-mixed text.
  address: Brussels, Belgium
  author:
  - first: Adithya
    full: Adithya Pratapa
    id: adithya-pratapa
    last: Pratapa
  - first: Monojit
    full: Monojit Choudhury
    id: monojit-choudhury
    last: Choudhury
  - first: Sunayana
    full: Sunayana Sitaram
    id: sunayana-sitaram
    last: Sitaram
  author_string: Adithya Pratapa, Monojit Choudhury, Sunayana Sitaram
  bibkey: pratapa-etal-2018-word
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1344
  month: October-November
  page_first: '3067'
  page_last: '3072'
  pages: "3067\u20133072"
  paper_id: '344'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1344.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1344.jpg
  title: Word Embeddings for Code-Mixed Language Processing
  title_html: Word Embeddings for Code-Mixed Language Processing
  url: https://www.aclweb.org/anthology/D18-1344
  year: '2018'
D18-1345:
  abstract: Character-level patterns have been widely used as features in English
    Named Entity Recognition (NER) systems. However, to date there has been no direct
    investigation of the inherent differences between name and nonname tokens in text,
    nor whether this property holds across multiple languages. This paper analyzes
    the capabilities of corpus-agnostic Character-level Language Models (CLMs) in
    the binary task of distinguishing name tokens from non-name tokens. We demonstrate
    that CLMs provide a simple and powerful model for capturing these differences,
    identifying named entity tokens in a diverse set of languages at close to the
    performance of full NER systems. Moreover, by adding very simple CLM-based features
    we can significantly improve the performance of an off-the-shelf NER system for
    multiple languages.
  address: Brussels, Belgium
  author:
  - first: Xiaodong
    full: Xiaodong Yu
    id: xiaodong-yu
    last: Yu
  - first: Stephen
    full: Stephen Mayhew
    id: stephen-mayhew
    last: Mayhew
  - first: Mark
    full: Mark Sammons
    id: mark-sammons
    last: Sammons
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Xiaodong Yu, Stephen Mayhew, Mark Sammons, Dan Roth
  bibkey: yu-etal-2018-strength
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1345
  month: October-November
  page_first: '3073'
  page_last: '3077'
  pages: "3073\u20133077"
  paper_id: '345'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1345.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1345.jpg
  title: On the Strength of Character Language Models for Multilingual Named Entity
    Recognition
  title_html: On the Strength of Character Language Models for Multilingual Named
    Entity Recognition
  url: https://www.aclweb.org/anthology/D18-1345
  year: '2018'
D18-1346:
  abstract: 'This work focuses on building language models (LMs) for code-switched
    text. We propose two techniques that significantly improve these LMs: 1) A novel
    recurrent neural network unit with dual components that focus on each language
    in the code-switched text separately 2) Pretraining the LM using synthetic text
    from a generative model estimated using the training data. We demonstrate the
    effectiveness of our proposed techniques by reporting perplexities on a Mandarin-English
    task and derive significant reductions in perplexity.'
  address: Brussels, Belgium
  author:
  - first: Saurabh
    full: Saurabh Garg
    id: saurabh-garg
    last: Garg
  - first: Tanmay
    full: Tanmay Parekh
    id: tanmay-parekh
    last: Parekh
  - first: Preethi
    full: Preethi Jyothi
    id: preethi-jyothi
    last: Jyothi
  author_string: Saurabh Garg, Tanmay Parekh, Preethi Jyothi
  bibkey: garg-etal-2018-code
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1346
  month: October-November
  page_first: '3078'
  page_last: '3083'
  pages: "3078\u20133083"
  paper_id: '346'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1346.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1346.jpg
  title: Code-switched Language Models Using Dual RNNs and Same-Source Pretraining
  title_html: Code-switched Language Models Using Dual <span class="acl-fixed-case">RNN</span>s
    and Same-Source Pretraining
  url: https://www.aclweb.org/anthology/D18-1346
  year: '2018'
D18-1347:
  abstract: Code-switching, the use of more than one language within a single utterance,
    is ubiquitous in much of the world, but remains a challenge for NLP largely due
    to the lack of representative data for training models. In this paper, we present
    a novel model architecture that is trained exclusively on monolingual resources,
    but can be applied to unseen code-switched text at inference time. The model accomplishes
    this by jointly maintaining separate word representations for each of the possible
    languages, or scripts in the case of transliteration, allowing each to contribute
    to inferences without forcing the model to commit to a language. Experiments on
    Hindi-English part-of-speech tagging demonstrate that our approach outperforms
    standard models when training on monolingual text without transliteration, and
    testing on code-switched text with alternate scripts.
  address: Brussels, Belgium
  author:
  - first: Kelsey
    full: Kelsey Ball
    id: kelsey-ball
    last: Ball
  - first: Dan
    full: Dan Garrette
    id: dan-garrette
    last: Garrette
  author_string: Kelsey Ball, Dan Garrette
  bibkey: ball-garrette-2018-part
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1347
  month: October-November
  page_first: '3084'
  page_last: '3089'
  pages: "3084\u20133089"
  paper_id: '347'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1347.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1347.jpg
  title: Part-of-Speech Tagging for Code-Switched, Transliterated Texts without Explicit
    Language Identification
  title_html: Part-of-Speech Tagging for Code-Switched, Transliterated Texts without
    Explicit Language Identification
  url: https://www.aclweb.org/anthology/D18-1347
  year: '2018'
D18-1348:
  abstract: "User intent detection plays a critical role in question-answering and\
    \ dialog systems. Most previous works treat intent detection as a classification\
    \ problem where utterances are labeled with predefined intents. However, it is\
    \ labor-intensive and time-consuming to label users\u2019 utterances as intents\
    \ are diversely expressed and novel intents will continually be involved. Instead,\
    \ we study the zero-shot intent detection problem, which aims to detect emerging\
    \ user intents where no labeled utterances are currently available. We propose\
    \ two capsule-based architectures: IntentCapsNet that extracts semantic features\
    \ from utterances and aggregates them to discriminate existing intents, and IntentCapsNet-ZSL\
    \ which gives IntentCapsNet the zero-shot learning ability to discriminate emerging\
    \ intents via knowledge transfer from existing intents. Experiments on two real-world\
    \ datasets show that our model not only can better discriminate diversely expressed\
    \ existing intents, but is also able to discriminate emerging intents when no\
    \ labeled utterances are available."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305945714
    type: video
    url: https://vimeo.com/305945714
  author:
  - first: Congying
    full: Congying Xia
    id: congying-xia
    last: Xia
  - first: Chenwei
    full: Chenwei Zhang
    id: chenwei-zhang
    last: Zhang
  - first: Xiaohui
    full: Xiaohui Yan
    id: xiaohui-yan
    last: Yan
  - first: Yi
    full: Yi Chang
    id: yi-chang
    last: Chang
  - first: Philip
    full: Philip Yu
    id: philip-s-yu
    last: Yu
  author_string: Congying Xia, Chenwei Zhang, Xiaohui Yan, Yi Chang, Philip Yu
  bibkey: xia-etal-2018-zero
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1348
  month: October-November
  page_first: '3090'
  page_last: '3099'
  pages: "3090\u20133099"
  paper_id: '348'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1348.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1348.jpg
  title: Zero-shot User Intent Detection via Capsule Neural Networks
  title_html: Zero-shot User Intent Detection via Capsule Neural Networks
  url: https://www.aclweb.org/anthology/D18-1348
  year: '2018'
D18-1349:
  abstract: Prevalent models based on artificial neural network (ANN) for sentence
    classification often classify sentences in isolation without considering the context
    in which sentences appear. This hampers the traditional sentence classification
    approaches to the problem of sequential sentence classification, where structured
    prediction is needed for better overall classification performance. In this work,
    we present a hierarchical sequential labeling network to make use of the contextual
    information within surrounding sentences to help classify the current sentence.
    Our model outperforms the state-of-the-art results by 2%-3% on two benchmarking
    datasets for sequential sentence classification in medical scientific abstracts.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305946571
    type: video
    url: https://vimeo.com/305946571
  author:
  - first: Di
    full: Di Jin
    id: di-jin
    last: Jin
  - first: Peter
    full: Peter Szolovits
    id: peter-szolovits
    last: Szolovits
  author_string: Di Jin, Peter Szolovits
  bibkey: jin-szolovits-2018-hierarchical
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1349
  month: October-November
  page_first: '3100'
  page_last: '3109'
  pages: "3100\u20133109"
  paper_id: '349'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1349.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1349.jpg
  title: Hierarchical Neural Networks for Sequential Sentence Classification in Medical
    Scientific Abstracts
  title_html: Hierarchical Neural Networks for Sequential Sentence Classification
    in Medical Scientific Abstracts
  url: https://www.aclweb.org/anthology/D18-1349
  year: '2018'
D18-1350:
  abstract: "In this study, we explore capsule networks with dynamic routing for text\
    \ classification. We propose three strategies to stabilize the dynamic routing\
    \ process to alleviate the disturbance of some noise capsules which may contain\
    \ \u201Cbackground\u201D information or have not been successfully trained. A\
    \ series of experiments are conducted with capsule networks on six text classification\
    \ benchmarks. Capsule networks achieve state of the art on 4 out of 6 datasets,\
    \ which shows the effectiveness of capsule networks for text classification. We\
    \ additionally show that capsule networks exhibit significant improvement when\
    \ transfer single-label to multi-label text classification over strong baseline\
    \ methods. To the best of our knowledge, this is the first work that capsule networks\
    \ have been empirically investigated for text modeling."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305947408
    type: video
    url: https://vimeo.com/305947408
  author:
  - first: Min
    full: Min Yang
    id: min-yang
    last: Yang
  - first: Wei
    full: Wei Zhao
    id: wei-zhao
    last: Zhao
  - first: Jianbo
    full: Jianbo Ye
    id: jianbo-ye
    last: Ye
  - first: Zeyang
    full: Zeyang Lei
    id: zeyang-lei
    last: Lei
  - first: Zhou
    full: Zhou Zhao
    id: zhou-zhao
    last: Zhao
  - first: Soufei
    full: Soufei Zhang
    id: soufei-zhang
    last: Zhang
  author_string: Min Yang, Wei Zhao, Jianbo Ye, Zeyang Lei, Zhou Zhao, Soufei Zhang
  bibkey: yang-etal-2018-investigating
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1350
  month: October-November
  page_first: '3110'
  page_last: '3119'
  pages: "3110\u20133119"
  paper_id: '350'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1350.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1350.jpg
  title: Investigating Capsule Networks with Dynamic Routing for Text Classification
  title_html: Investigating Capsule Networks with Dynamic Routing for Text Classification
  url: https://www.aclweb.org/anthology/D18-1350
  year: '2018'
D18-1351:
  abstract: Many classification models work poorly on short texts due to data sparsity.
    To address this issue, we propose topic memory networks for short text classification
    with a novel topic memory mechanism to encode latent topic representations indicative
    of class labels. Different from most prior work that focuses on extending features
    with external knowledge or pre-trained topics, our model jointly explores topic
    inference and text classification with memory networks in an end-to-end manner.
    Experimental results on four benchmark datasets show that our model outperforms
    state-of-the-art models on short text classification, meanwhile generates coherent
    topics.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305947994
    type: video
    url: https://vimeo.com/305947994
  author:
  - first: Jichuan
    full: Jichuan Zeng
    id: jichuan-zeng
    last: Zeng
  - first: Jing
    full: Jing Li
    id: jing-li
    last: Li
  - first: Yan
    full: Yan Song
    id: yan-song
    last: Song
  - first: Cuiyun
    full: Cuiyun Gao
    id: cuiyun-gao
    last: Gao
  - first: Michael R.
    full: Michael R. Lyu
    id: michael-r-lyu
    last: Lyu
  - first: Irwin
    full: Irwin King
    id: irwin-king
    last: King
  author_string: Jichuan Zeng, Jing Li, Yan Song, Cuiyun Gao, Michael R. Lyu, Irwin
    King
  bibkey: zeng-etal-2018-topic
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1351
  month: October-November
  page_first: '3120'
  page_last: '3131'
  pages: "3120\u20133131"
  paper_id: '351'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1351.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1351.jpg
  title: Topic Memory Networks for Short Text Classification
  title_html: Topic Memory Networks for Short Text Classification
  url: https://www.aclweb.org/anthology/D18-1351
  year: '2018'
D18-1352:
  abstract: 'Large multi-label datasets contain labels that occur thousands of times
    (frequent group), those that occur only a few times (few-shot group), and labels
    that never appear in the training dataset (zero-shot group). Multi-label few-
    and zero-shot label prediction is mostly unexplored on datasets with large label
    spaces, especially for text classification. In this paper, we perform a fine-grained
    evaluation to understand how state-of-the-art methods perform on infrequent labels.
    Furthermore, we develop few- and zero-shot methods for multi-label text classification
    when there is a known structure over the label space, and evaluate them on two
    publicly available medical text datasets: MIMIC II and MIMIC III. For few-shot
    labels we achieve improvements of 6.2% and 4.8% in R@10 for MIMIC II and MIMIC
    III, respectively, over prior efforts; the corresponding R@10 improvements for
    zero-shot labels are 17.3% and 19%.'
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305948835
    type: video
    url: https://vimeo.com/305948835
  author:
  - first: Anthony
    full: Anthony Rios
    id: anthony-rios
    last: Rios
  - first: Ramakanth
    full: Ramakanth Kavuluru
    id: ramakanth-kavuluru
    last: Kavuluru
  author_string: Anthony Rios, Ramakanth Kavuluru
  bibkey: rios-kavuluru-2018-shot
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1352
  month: October-November
  page_first: '3132'
  page_last: '3142'
  pages: "3132\u20133142"
  paper_id: '352'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1352.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1352.jpg
  title: Few-Shot and Zero-Shot Multi-Label Learning for Structured Label Spaces
  title_html: Few-Shot and Zero-Shot Multi-Label Learning for Structured Label Spaces
  url: https://www.aclweb.org/anthology/D18-1352
  year: '2018'
D18-1353:
  abstract: "Poetry is one of the most beautiful forms of human language art. As a\
    \ crucial step towards computer creativity, automatic poetry generation has drawn\
    \ researchers\u2019 attention for decades. In recent years, some neural models\
    \ have made remarkable progress in this task. However, they are all based on maximum\
    \ likelihood estimation, which only learns common patterns of the corpus and results\
    \ in loss-evaluation mismatch. Human experts evaluate poetry in terms of some\
    \ specific criteria, instead of word-level likelihood. To handle this problem,\
    \ we directly model the criteria and use them as explicit rewards to guide gradient\
    \ update by reinforcement learning, so as to motivate the model to pursue higher\
    \ scores. Besides, inspired by writing theories, we propose a novel mutual reinforcement\
    \ learning schema. We simultaneously train two learners (generators) which learn\
    \ not only from the teacher (rewarder) but also from each other to further improve\
    \ performance. We experiment on Chinese poetry. Based on a strong basic model,\
    \ our method achieves better results and outperforms the current state-of-the-art\
    \ method."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305925622
    type: video
    url: https://vimeo.com/305925622
  author:
  - first: Xiaoyuan
    full: Xiaoyuan Yi
    id: xiaoyuan-yi
    last: Yi
  - first: Maosong
    full: Maosong Sun
    id: maosong-sun
    last: Sun
  - first: Ruoyu
    full: Ruoyu Li
    id: ruoyu-li
    last: Li
  - first: Wenhao
    full: Wenhao Li
    id: wenhao-li
    last: Li
  author_string: Xiaoyuan Yi, Maosong Sun, Ruoyu Li, Wenhao Li
  bibkey: yi-etal-2018-automatic
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1353
  month: October-November
  page_first: '3143'
  page_last: '3153'
  pages: "3143\u20133153"
  paper_id: '353'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1353.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1353.jpg
  title: Automatic Poetry Generation with Mutual Reinforcement Learning
  title_html: Automatic Poetry Generation with Mutual Reinforcement Learning
  url: https://www.aclweb.org/anthology/D18-1353
  year: '2018'
D18-1354:
  abstract: Combining the virtues of probability graphic models and neural networks,
    Conditional Variational Auto-encoder (CVAE) has shown promising performance in
    applications such as response generation. However, existing CVAE-based models
    often generate responses from a single latent variable which may not be sufficient
    to model high variability in responses. To solve this problem, we propose a novel
    model that sequentially introduces a series of latent variables to condition the
    generation of each word in the response sequence. In addition, the approximate
    posteriors of these latent variables are augmented with a backward Recurrent Neural
    Network (RNN), which allows the latent variables to capture long-term dependencies
    of future tokens in generation. To facilitate training, we supplement our model
    with an auxiliary objective that predicts the subsequent bag of words. Empirical
    experiments conducted on Opensubtitle and Reddit datasets show that the proposed
    model leads to significant improvement on both relevance and diversity over state-of-the-art
    baselines.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305926196
    type: video
    url: https://vimeo.com/305926196
  author:
  - first: Jiachen
    full: Jiachen Du
    id: jiachen-du
    last: Du
  - first: Wenjie
    full: Wenjie Li
    id: wenjie-li
    last: Li
  - first: Yulan
    full: Yulan He
    id: yulan-he
    last: He
  - first: Ruifeng
    full: Ruifeng Xu
    id: ruifeng-xu
    last: Xu
  - first: Lidong
    full: Lidong Bing
    id: lidong-bing
    last: Bing
  - first: Xuan
    full: Xuan Wang
    id: xuan-wang
    last: Wang
  author_string: Jiachen Du, Wenjie Li, Yulan He, Ruifeng Xu, Lidong Bing, Xuan Wang
  bibkey: du-etal-2018-variational
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1354
  month: October-November
  page_first: '3154'
  page_last: '3163'
  pages: "3154\u20133163"
  paper_id: '354'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1354.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1354.jpg
  title: Variational Autoregressive Decoder for Neural Response Generation
  title_html: Variational Autoregressive Decoder for Neural Response Generation
  url: https://www.aclweb.org/anthology/D18-1354
  year: '2018'
D18-1355:
  abstract: 'Sentence simplification aims to reduce the complexity of a sentence while
    retaining its original meaning. Current models for sentence simplification adopted
    ideas from machine translation studies and implicitly learned simplification mapping
    rules from normal-simple sentence pairs. In this paper, we explore a novel model
    based on a multi-layer and multi-head attention architecture and we propose two
    innovative approaches to integrate the Simple PPDB (A Paraphrase Database for
    Simplification), an external paraphrase knowledge base for simplification that
    covers a wide range of real-world simplification rules. The experiments show that
    the integration provides two major benefits: (1) the integrated model outperforms
    multiple state-of-the-art baseline models for sentence simplification in the literature
    (2) through analysis of the rule utilization, the model seeks to select more accurate
    simplification rules. The code and models used in the paper are available at https://github.com/Sanqiang/text_simplification.'
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305927122
    type: video
    url: https://vimeo.com/305927122
  author:
  - first: Sanqiang
    full: Sanqiang Zhao
    id: sanqiang-zhao
    last: Zhao
  - first: Rui
    full: Rui Meng
    id: rui-meng
    last: Meng
  - first: Daqing
    full: Daqing He
    id: daqing-he
    last: He
  - first: Andi
    full: Andi Saptono
    id: andi-saptono
    last: Saptono
  - first: Bambang
    full: Bambang Parmanto
    id: bambang-parmanto
    last: Parmanto
  author_string: Sanqiang Zhao, Rui Meng, Daqing He, Andi Saptono, Bambang Parmanto
  bibkey: zhao-etal-2018-integrating
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1355
  month: October-November
  page_first: '3164'
  page_last: '3173'
  pages: "3164\u20133173"
  paper_id: '355'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1355.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1355.jpg
  title: Integrating Transformer and Paraphrase Rules for Sentence Simplification
  title_html: Integrating Transformer and Paraphrase Rules for Sentence Simplification
  url: https://www.aclweb.org/anthology/D18-1355
  year: '2018'
D18-1356:
  abstract: While neural, encoder-decoder models have had significant empirical success
    in text generation, there remain several unaddressed problems with this style
    of generation. Encoder-decoder models are largely (a) uninterpretable, and (b)
    difficult to control in terms of their phrasing or content. This work proposes
    a neural generation system using a hidden semi-markov model (HSMM) decoder, which
    learns latent, discrete templates jointly with learning to generate. We show that
    this model learns useful templates, and that these templates make generation both
    more interpretable and controllable. Furthermore, we show that this approach scales
    to real data sets and achieves strong performance nearing that of encoder-decoder
    text generation models.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305928599
    type: video
    url: https://vimeo.com/305928599
  author:
  - first: Sam
    full: Sam Wiseman
    id: sam-wiseman
    last: Wiseman
  - first: Stuart
    full: Stuart Shieber
    id: stuart-m-shieber
    last: Shieber
  - first: Alexander
    full: Alexander Rush
    id: alexander-m-rush
    last: Rush
  author_string: Sam Wiseman, Stuart Shieber, Alexander Rush
  bibkey: wiseman-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1356
  month: October-November
  page_first: '3174'
  page_last: '3187'
  pages: "3174\u20133187"
  paper_id: '356'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1356.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1356.jpg
  title: Learning Neural Templates for Text Generation
  title_html: Learning Neural Templates for Text Generation
  url: https://www.aclweb.org/anthology/D18-1356
  year: '2018'
D18-1357:
  abstract: Neural text generation, including neural machine translation, image captioning,
    and summarization, has been quite successful recently. However, during training
    time, typically only one reference is considered for each example, even though
    there are often multiple references available, e.g., 4 references in NIST MT evaluations,
    and 5 references in image captioning data. We first investigate several different
    ways of utilizing multiple human references during training. But more importantly,
    we then propose an algorithm to generate exponentially many pseudo-references
    by first compressing existing human references into lattices and then traversing
    them to generate new pseudo-references. These approaches lead to substantial improvements
    over strong baselines in both machine translation (+1.5 BLEU) and image captioning
    (+3.1 BLEU / +11.7 CIDEr).
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305929800
    type: video
    url: https://vimeo.com/305929800
  author:
  - first: Renjie
    full: Renjie Zheng
    id: renjie-zheng
    last: Zheng
  - first: Mingbo
    full: Mingbo Ma
    id: mingbo-ma
    last: Ma
  - first: Liang
    full: Liang Huang
    id: liang-huang
    last: Huang
  author_string: Renjie Zheng, Mingbo Ma, Liang Huang
  bibkey: zheng-etal-2018-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1357
  month: October-November
  page_first: '3188'
  page_last: '3197'
  pages: "3188\u20133197"
  paper_id: '357'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1357.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1357.jpg
  title: Multi-Reference Training with Pseudo-References for Neural Translation and
    Text Generation
  title_html: Multi-Reference Training with Pseudo-References for Neural Translation
    and Text Generation
  url: https://www.aclweb.org/anthology/D18-1357
  year: '2018'
D18-1358:
  abstract: The rapid development of knowledge graphs (KGs), such as Freebase and
    WordNet, has changed the paradigm for AI-related applications. However, even though
    these KGs are impressively large, most of them are suffering from incompleteness,
    which leads to performance degradation of AI applications. Most existing researches
    are focusing on knowledge graph embedding (KGE) models. Nevertheless, those models
    simply embed entities and relations into latent vectors without leveraging the
    rich information from the relation structure. Indeed, relations in KGs conform
    to a three-layer hierarchical relation structure (HRS), i.e., semantically similar
    relations can make up relation clusters and some relations can be further split
    into several fine-grained sub-relations. Relation clusters, relations and sub-relations
    can fit in the top, the middle and the bottom layer of three-layer HRS respectively.
    To this end, in this paper, we extend existing KGE models TransE, TransH and DistMult,
    to learn knowledge representations by leveraging the information from the HRS.
    Particularly, our approach is capable to extend other KGE models. Finally, the
    experiment results clearly validate the effectiveness of the proposed approach
    against baselines.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306112939
    type: video
    url: https://vimeo.com/306112939
  author:
  - first: Zhao
    full: Zhao Zhang
    id: zhao-zhang
    last: Zhang
  - first: Fuzhen
    full: Fuzhen Zhuang
    id: fuzhen-zhuang
    last: Zhuang
  - first: Meng
    full: Meng Qu
    id: meng-qu
    last: Qu
  - first: Fen
    full: Fen Lin
    id: fen-lin
    last: Lin
  - first: Qing
    full: Qing He
    id: qing-he
    last: He
  author_string: Zhao Zhang, Fuzhen Zhuang, Meng Qu, Fen Lin, Qing He
  bibkey: zhang-etal-2018-knowledge
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1358
  month: October-November
  page_first: '3198'
  page_last: '3207'
  pages: "3198\u20133207"
  paper_id: '358'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1358.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1358.jpg
  title: Knowledge Graph Embedding with Hierarchical Relation Structure
  title_html: Knowledge Graph Embedding with Hierarchical Relation Structure
  url: https://www.aclweb.org/anthology/D18-1358
  year: '2018'
D18-1359:
  abstract: Representing entities and relations in an embedding space is a well-studied
    approach for machine learning on relational data. Existing approaches, however,
    primarily focus on simple link structure between a finite set of entities, ignoring
    the variety of data types that are often used in knowledge bases, such as text,
    images, and numerical values. In this paper, we propose multimodal knowledge base
    embeddings (MKBE) that use different neural encoders for this variety of observed
    data, and combine them with existing relational models to learn embeddings of
    the entities and multimodal data. Further, using these learned embedings and different
    neural decoders, we introduce a novel multimodal imputation model to generate
    missing multimodal values, like text and images, from information in the knowledge
    base. We enrich existing relational datasets to create two novel benchmarks that
    contain additional information such as textual descriptions and images of the
    original entities. We demonstrate that our models utilize this additional information
    effectively to provide more accurate link prediction, achieving state-of-the-art
    results with a considerable gap of 5-7% over existing methods. Further, we evaluate
    the quality of our generated multimodal values via a user study.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1359.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1359.Attachment.zip
  - filename: https://vimeo.com/306113486
    type: video
    url: https://vimeo.com/306113486
  author:
  - first: Pouya
    full: Pouya Pezeshkpour
    id: pouya-pezeshkpour
    last: Pezeshkpour
  - first: Liyan
    full: Liyan Chen
    id: liyan-chen
    last: Chen
  - first: Sameer
    full: Sameer Singh
    id: sameer-singh
    last: Singh
  author_string: Pouya Pezeshkpour, Liyan Chen, Sameer Singh
  bibkey: pezeshkpour-etal-2018-embedding
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1359
  month: October-November
  page_first: '3208'
  page_last: '3218'
  pages: "3208\u20133218"
  paper_id: '359'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1359.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1359.jpg
  title: Embedding Multimodal Relational Data for Knowledge Base Completion
  title_html: Embedding Multimodal Relational Data for Knowledge Base Completion
  url: https://www.aclweb.org/anthology/D18-1359
  year: '2018'
D18-1360:
  abstract: We introduce a multi-task setup of identifying entities, relations, and
    coreference clusters in scientific articles. We create SciERC, a dataset that
    includes annotations for all three tasks and develop a unified framework called
    SciIE with shared span representations. The multi-task setup reduces cascading
    errors between tasks and leverages cross-sentence relations through coreference
    links. Experiments show that our multi-task model outperforms previous models
    in scientific information extraction without using any domain-specific features.
    We further show that the framework supports construction of a scientific knowledge
    graph, which we use to analyze information in scientific literature.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306113930
    type: video
    url: https://vimeo.com/306113930
  author:
  - first: Yi
    full: Yi Luan
    id: yi-luan
    last: Luan
  - first: Luheng
    full: Luheng He
    id: luheng-he
    last: He
  - first: Mari
    full: Mari Ostendorf
    id: mari-ostendorf
    last: Ostendorf
  - first: Hannaneh
    full: Hannaneh Hajishirzi
    id: hannaneh-hajishirzi
    last: Hajishirzi
  author_string: Yi Luan, Luheng He, Mari Ostendorf, Hannaneh Hajishirzi
  bibkey: luan-etal-2018-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1360
  month: October-November
  page_first: '3219'
  page_last: '3232'
  pages: "3219\u20133232"
  paper_id: '360'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1360.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1360.jpg
  title: Multi-Task Identification of Entities, Relations, and Coreference for Scientific
    Knowledge Graph Construction
  title_html: Multi-Task Identification of Entities, Relations, and Coreference for
    Scientific Knowledge Graph Construction
  url: https://www.aclweb.org/anthology/D18-1360
  year: '2018'
D18-1361:
  abstract: The 20 Questions (Q20) game is a well known game which encourages deductive
    reasoning and creativity. In the game, the answerer first thinks of an object
    such as a famous person or a kind of animal. Then the questioner tries to guess
    the object by asking 20 questions. In a Q20 game system, the user is considered
    as the answerer while the system itself acts as the questioner which requires
    a good strategy of question selection to figure out the correct object and win
    the game. However, the optimal policy of question selection is hard to be derived
    due to the complexity and volatility of the game environment. In this paper, we
    propose a novel policy-based Reinforcement Learning (RL) method, which enables
    the questioner agent to learn the optimal policy of question selection through
    continuous interactions with users. To facilitate training, we also propose to
    use a reward network to estimate the more informative reward. Compared to previous
    methods, our RL method is robust to noisy answers and does not rely on the Knowledge
    Base of objects. Experimental results show that our RL method clearly outperforms
    an entropy-based engineering system and has competitive performance in a noisy-free
    simulation environment.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1361.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1361.Attachment.zip
  - filename: https://vimeo.com/306114592
    type: video
    url: https://vimeo.com/306114592
  author:
  - first: Huang
    full: Huang Hu
    id: huang-hu
    last: Hu
  - first: Xianchao
    full: Xianchao Wu
    id: xianchao-wu
    last: Wu
  - first: Bingfeng
    full: Bingfeng Luo
    id: bingfeng-luo
    last: Luo
  - first: Chongyang
    full: Chongyang Tao
    id: chongyang-tao
    last: Tao
  - first: Can
    full: Can Xu
    id: can-xu
    last: Xu
  - first: Wei
    full: Wei Wu
    id: wei-wu
    last: Wu
  - first: Zhan
    full: Zhan Chen
    id: zhan-chen
    last: Chen
  author_string: Huang Hu, Xianchao Wu, Bingfeng Luo, Chongyang Tao, Can Xu, Wei Wu,
    Zhan Chen
  bibkey: hu-etal-2018-playing
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1361
  month: October-November
  page_first: '3233'
  page_last: '3242'
  pages: "3233\u20133242"
  paper_id: '361'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1361.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1361.jpg
  title: Playing 20 Question Game with Policy-Based Reinforcement Learning
  title_html: Playing 20 Question Game with Policy-Based Reinforcement Learning
  url: https://www.aclweb.org/anthology/D18-1361
  year: '2018'
D18-1362:
  abstract: 'Multi-hop reasoning is an effective approach for query answering (QA)
    over incomplete knowledge graphs (KGs). The problem can be formulated in a reinforcement
    learning (RL) setup, where a policy-based agent sequentially extends its inference
    path until it reaches a target. However, in an incomplete KG environment, the
    agent receives low-quality rewards corrupted by false negatives in the training
    data, which harms generalization at test time. Furthermore, since no golden action
    sequence is used for training, the agent can be misled by spurious search trajectories
    that incidentally lead to the correct answer. We propose two modeling advances
    to address both issues: (1) we reduce the impact of false negative supervision
    by adopting a pretrained one-hop embedding model to estimate the reward of unobserved
    facts; (2) we counter the sensitivity to spurious paths of on-policy RL by forcing
    the agent to explore a diverse set of paths using randomly generated edge masks.
    Our approach significantly improves over existing path-based KGQA models on several
    benchmark datasets and is comparable or better than embedding-based models.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1362.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1362.Attachment.pdf
  - filename: https://vimeo.com/306115211
    type: video
    url: https://vimeo.com/306115211
  author:
  - first: Xi Victoria
    full: Xi Victoria Lin
    id: xi-victoria-lin
    last: Lin
  - first: Richard
    full: Richard Socher
    id: richard-socher
    last: Socher
  - first: Caiming
    full: Caiming Xiong
    id: caiming-xiong
    last: Xiong
  author_string: Xi Victoria Lin, Richard Socher, Caiming Xiong
  bibkey: lin-etal-2018-multi-hop
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1362
  month: October-November
  page_first: '3243'
  page_last: '3253'
  pages: "3243\u20133253"
  paper_id: '362'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1362.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1362.jpg
  title: Multi-Hop Knowledge Graph Reasoning with Reward Shaping
  title_html: Multi-Hop Knowledge Graph Reasoning with Reward Shaping
  url: https://www.aclweb.org/anthology/D18-1362
  year: '2018'
D18-1363:
  abstract: 'Neural state-of-the-art sequence-to-sequence (seq2seq) models often do
    not perform well for small training sets. We address paradigm completion, the
    morphological task of, given a partial paradigm, generating all missing forms.
    We propose two new methods for the minimal-resource setting: (i) Paradigm transduction:
    Since we assume only few paradigms available for training, neural seq2seq models
    are able to capture relationships between paradigm cells, but are tied to the
    idiosyncracies of the training set. Paradigm transduction mitigates this problem
    by exploiting the input subset of inflected forms at test time. (ii) Source selection
    with high precision (SHIP): Multi-source models which learn to automatically select
    one or multiple sources to predict a target inflection do not perform well in
    the minimal-resource setting. SHIP is an alternative to identify a reliable source
    if training data is limited. On a 52-language benchmark dataset, we outperform
    the previous state of the art by up to 9.71% absolute accuracy.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1363.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1363.Attachment.zip
  - filename: https://vimeo.com/305676641
    type: video
    url: https://vimeo.com/305676641
  author:
  - first: Katharina
    full: Katharina Kann
    id: katharina-kann
    last: Kann
  - first: Hinrich
    full: "Hinrich Sch\xFCtze"
    id: hinrich-schutze
    last: "Sch\xFCtze"
  author_string: "Katharina Kann, Hinrich Sch\xFCtze"
  bibkey: kann-schutze-2018-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1363
  month: October-November
  page_first: '3254'
  page_last: '3264'
  pages: "3254\u20133264"
  paper_id: '363'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1363.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1363.jpg
  title: 'Neural Transductive Learning and Beyond: Morphological Generation in the
    Minimal-Resource Setting'
  title_html: 'Neural Transductive Learning and Beyond: Morphological Generation in
    the Minimal-Resource Setting'
  url: https://www.aclweb.org/anthology/D18-1363
  year: '2018'
D18-1364:
  abstract: This paper focuses on the most basic implicational universals in phonological
    theory, called T-orders after Anttila and Andrus (2006). It shows that the T-orders
    predicted by stochastic (and partial order) Optimality Theory coincide with those
    predicted by categorical OT. Analogously, the T-orders predicted by stochastic
    Harmonic Grammar coincide with those predicted by categorical HG. In other words,
    these stochastic constraint-based frameworks do not tamper with the typological
    structure induced by the original categorical frameworks.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/305679809
    type: video
    url: https://vimeo.com/305679809
  author:
  - first: Giorgio
    full: Giorgio Magri
    id: giorgio-magri
    last: Magri
  author_string: Giorgio Magri
  bibkey: magri-2018-implicational
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1364
  month: October-November
  page_first: '3265'
  page_last: '3274'
  pages: "3265\u20133274"
  paper_id: '364'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1364.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1364.jpg
  title: Implicational Universals in Stochastic Constraint-Based Phonology
  title_html: Implicational Universals in Stochastic Constraint-Based Phonology
  url: https://www.aclweb.org/anthology/D18-1364
  year: '2018'
D18-1365:
  abstract: Character-level features are currently used in different neural network-based
    natural language processing algorithms. However, little is known about the character-level
    patterns those models learn. Moreover, models are often compared only quantitatively
    while a qualitative analysis is missing. In this paper, we investigate which character-level
    patterns neural networks learn and if those patterns coincide with manually-defined
    word segmentations and annotations. To that end, we extend the contextual decomposition
    technique (Murdoch et al. 2018) to convolutional neural networks which allows
    us to compare convolutional neural networks and bidirectional long short-term
    memory networks. We evaluate and compare these models for the task of morphological
    tagging on three morphologically different languages and show that these models
    implicitly discover understandable linguistic rules.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1365.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1365.Attachment.zip
  - filename: https://vimeo.com/305681577
    type: video
    url: https://vimeo.com/305681577
  author:
  - first: "Fr\xE9deric"
    full: "Fr\xE9deric Godin"
    id: frederic-godin
    last: Godin
  - first: Kris
    full: Kris Demuynck
    id: kris-demuynck
    last: Demuynck
  - first: Joni
    full: Joni Dambre
    id: joni-dambre
    last: Dambre
  - first: Wesley
    full: Wesley De Neve
    id: wesley-de-neve
    last: De Neve
  - first: Thomas
    full: Thomas Demeester
    id: thomas-demeester
    last: Demeester
  author_string: "Fr\xE9deric Godin, Kris Demuynck, Joni Dambre, Wesley De Neve, Thomas\
    \ Demeester"
  bibkey: godin-etal-2018-explaining
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1365
  month: October-November
  page_first: '3275'
  page_last: '3284'
  pages: "3275\u20133284"
  paper_id: '365'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1365.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1365.jpg
  title: 'Explaining Character-Aware Neural Networks for Word-Level Prediction: Do
    They Discover Linguistic Rules?'
  title_html: 'Explaining Character-Aware Neural Networks for Word-Level Prediction:
    Do They Discover Linguistic Rules?'
  url: https://www.aclweb.org/anthology/D18-1365
  year: '2018'
D18-1366:
  abstract: 'Much work in Natural Language Processing (NLP) has been for resource-rich
    languages, making generalization to new, less-resourced languages challenging.
    We present two approaches for improving generalization to low-resourced languages
    by adapting continuous word representations using linguistically motivated subword
    units: phonemes, morphemes and graphemes. Our method requires neither parallel
    corpora nor bilingual dictionaries and provides a significant gain in performance
    over previous methods relying on these resources. We demonstrate the effectiveness
    of our approaches on Named Entity Recognition for four languages, namely Uyghur,
    Turkish, Bengali and Hindi, of which Uyghur and Bengali are low resource languages,
    and also perform experiments on Machine Translation. Exploiting subwords with
    transfer learning gives us a boost of +15.2 NER F1 for Uyghur and +9.7 F1 for
    Bengali. We also show improvements in the monolingual setting where we achieve
    (avg.) +3 F1 and (avg.) +1.35 BLEU.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1366.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1366.Attachment.zip
  - filename: https://vimeo.com/305683572
    type: video
    url: https://vimeo.com/305683572
  author:
  - first: Aditi
    full: Aditi Chaudhary
    id: aditi-chaudhary
    last: Chaudhary
  - first: Chunting
    full: Chunting Zhou
    id: chunting-zhou
    last: Zhou
  - first: Lori
    full: Lori Levin
    id: lori-levin
    last: Levin
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  - first: David R.
    full: David R. Mortensen
    id: david-r-mortensen
    last: Mortensen
  - first: Jaime
    full: Jaime Carbonell
    id: jaime-g-carbonell
    last: Carbonell
  author_string: Aditi Chaudhary, Chunting Zhou, Lori Levin, Graham Neubig, David
    R. Mortensen, Jaime Carbonell
  bibkey: chaudhary-etal-2018-adapting
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1366
  month: October-November
  page_first: '3285'
  page_last: '3295'
  pages: "3285\u20133295"
  paper_id: '366'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1366.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1366.jpg
  title: Adapting Word Embeddings to New Languages with Morphological and Phonological
    Subword Representations
  title_html: Adapting Word Embeddings to New Languages with Morphological and Phonological
    Subword Representations
  url: https://www.aclweb.org/anthology/D18-1366
  year: '2018'
D18-1367:
  abstract: Several NLP studies address the problem of figurative language, but among
    non-literal phenomena, they have neglected exaggeration. This paper presents a
    first computational approach to this figure of speech. We explore the possibility
    to automatically detect exaggerated sentences. First, we introduce HYPO, a corpus
    containing overstatements (or hyperboles) collected on the web and validated via
    crowdsourcing. Then, we evaluate a number of models trained on HYPO, and bring
    evidence that the task of hyperbole identification can be successfully performed
    based on a small set of semantic features.
  address: Brussels, Belgium
  author:
  - first: Enrica
    full: Enrica Troiano
    id: enrica-troiano
    last: Troiano
  - first: Carlo
    full: Carlo Strapparava
    id: carlo-strapparava
    last: Strapparava
  - first: "G\xF6zde"
    full: "G\xF6zde \xD6zbal"
    id: gozde-ozbal
    last: "\xD6zbal"
  - first: Serra Sinem
    full: "Serra Sinem Tekiro\u011Flu"
    id: serra-sinem-tekiroglu
    last: "Tekiro\u011Flu"
  author_string: "Enrica Troiano, Carlo Strapparava, G\xF6zde \xD6zbal, Serra Sinem\
    \ Tekiro\u011Flu"
  bibkey: troiano-etal-2018-computational
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1367
  month: October-November
  page_first: '3296'
  page_last: '3304'
  pages: "3296\u20133304"
  paper_id: '367'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1367.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1367.jpg
  title: A Computational Exploration of Exaggeration
  title_html: A Computational Exploration of Exaggeration
  url: https://www.aclweb.org/anthology/D18-1367
  year: '2018'
D18-1368:
  abstract: Capabilities to categorize a clause based on the type of situation entity
    (e.g., events, states and generic statements) the clause introduces to the discourse
    can benefit many NLP applications. Observing that the situation entity type of
    a clause depends on discourse functions the clause plays in a paragraph and the
    interpretation of discourse functions depends heavily on paragraph-wide contexts,
    we propose to build context-aware clause representations for predicting situation
    entity types of clauses. Specifically, we propose a hierarchical recurrent neural
    network model to read a whole paragraph at a time and jointly learn representations
    for all the clauses in the paragraph by extensively modeling context influences
    and inter-dependencies of clauses. Experimental results show that our model achieves
    the state-of-the-art performance for clause-level situation entity classification
    on the genre-rich MASC+Wiki corpus, which approaches human-level performance.
  address: Brussels, Belgium
  author:
  - first: Zeyu
    full: Zeyu Dai
    id: zeyu-dai
    last: Dai
  - first: Ruihong
    full: Ruihong Huang
    id: ruihong-huang
    last: Huang
  author_string: Zeyu Dai, Ruihong Huang
  bibkey: dai-huang-2018-building
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1368
  month: October-November
  page_first: '3305'
  page_last: '3315'
  pages: "3305\u20133315"
  paper_id: '368'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1368.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1368.jpg
  title: Building Context-aware Clause Representations for Situation Entity Type Classification
  title_html: Building Context-aware Clause Representations for Situation Entity Type
    Classification
  url: https://www.aclweb.org/anthology/D18-1368
  year: '2018'
D18-1369:
  abstract: 'In news and discussions, many articles and posts are provided without
    their related previous articles or posts. Hence, it is difficult to understand
    the context from which the articles and posts have occurred. In this paper, we
    propose the Hierarchical Dirichlet Gaussian Marked Hawkes process (HD-GMHP) for
    reconstructing the narratives and thread structures of news articles and discussion
    posts. HD-GMHP unifies three modeling strategies in previous research: temporal
    characteristics, triggering event relations, and meta information of text in news
    articles and discussion threads. To show the effectiveness of the model, we perform
    experiments in narrative reconstruction and thread reconstruction with real world
    datasets: articles from the New York Times and a corpus of Wikipedia conversations.
    The experimental results show that HD-GMHP outperforms the baselines of LDA, HDP,
    and HDHP for both tasks.'
  address: Brussels, Belgium
  author:
  - first: Yeon
    full: Yeon Seonwoo
    id: yeon-seonwoo
    last: Seonwoo
  - first: Alice
    full: Alice Oh
    id: alice-oh
    last: Oh
  - first: Sungjoon
    full: Sungjoon Park
    id: sungjoon-park
    last: Park
  author_string: Yeon Seonwoo, Alice Oh, Sungjoon Park
  bibkey: seonwoo-etal-2018-hierarchical
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1369
  month: October-November
  page_first: '3316'
  page_last: '3325'
  pages: "3316\u20133325"
  paper_id: '369'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1369.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1369.jpg
  title: Hierarchical Dirichlet Gaussian Marked Hawkes Process for Narrative Reconstruction
    in Continuous Time Domain
  title_html: Hierarchical <span class="acl-fixed-case">D</span>irichlet <span class="acl-fixed-case">G</span>aussian
    Marked <span class="acl-fixed-case">H</span>awkes Process for Narrative Reconstruction
    in Continuous Time Domain
  url: https://www.aclweb.org/anthology/D18-1369
  year: '2018'
D18-1370:
  abstract: Exponential growth in the number of scientific publications yields the
    need for effective automatic analysis of rhetorical aspects of scientific writing.
    Acknowledging the argumentative nature of scientific text, in this work we investigate
    the link between the argumentative structure of scientific publications and rhetorical
    aspects such as discourse categories or citation contexts. To this end, we (1)
    augment a corpus of scientific publications annotated with four layers of rhetoric
    annotations with argumentation annotations and (2) investigate neural multi-task
    learning architectures combining argument extraction with a set of rhetorical
    classification tasks. By coupling rhetorical classifiers with the extraction of
    argumentative components in a joint multi-task learning setting, we obtain significant
    performance gains for different rhetorical analysis tasks.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1370.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1370.Attachment.zip
  author:
  - first: Anne
    full: Anne Lauscher
    id: anne-lauscher
    last: Lauscher
  - first: Goran
    full: "Goran Glava\u0161"
    id: goran-glavas
    last: "Glava\u0161"
  - first: Simone Paolo
    full: Simone Paolo Ponzetto
    id: simone-paolo-ponzetto
    last: Ponzetto
  - first: Kai
    full: Kai Eckert
    id: kai-eckert
    last: Eckert
  author_string: "Anne Lauscher, Goran Glava\u0161, Simone Paolo Ponzetto, Kai Eckert"
  bibkey: lauscher-etal-2018-investigating
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1370
  month: October-November
  page_first: '3326'
  page_last: '3338'
  pages: "3326\u20133338"
  paper_id: '370'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1370.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1370.jpg
  title: Investigating the Role of Argumentation in the Rhetorical Analysis of Scientific
    Publications with Neural Multi-Task Learning Models
  title_html: Investigating the Role of Argumentation in the Rhetorical Analysis of
    Scientific Publications with Neural Multi-Task Learning Models
  url: https://www.aclweb.org/anthology/D18-1370
  year: '2018'
D18-1371:
  abstract: 'We design and build the first neural temporal dependency parser. It utilizes
    a neural ranking model with minimal feature engineering, and parses time expressions
    and events in a text into a temporal dependency tree structure. We evaluate our
    parser on two domains: news reports and narrative stories. In a parsing-only evaluation
    setup where gold time expressions and events are provided, our parser reaches
    0.81 and 0.70 f-score on unlabeled and labeled parsing respectively, a result
    that is very competitive against alternative approaches. In an end-to-end evaluation
    setup where time expressions and events are automatically recognized, our parser
    beats two strong baselines on both data domains. Our experimental results and
    discussions shed light on the nature of temporal dependency structures in different
    domains and provide insights that we believe will be valuable to future research
    in this area.'
  address: Brussels, Belgium
  author:
  - first: Yuchen
    full: Yuchen Zhang
    id: yuchen-zhang
    last: Zhang
  - first: Nianwen
    full: Nianwen Xue
    id: nianwen-xue
    last: Xue
  author_string: Yuchen Zhang, Nianwen Xue
  bibkey: zhang-xue-2018-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1371
  month: October-November
  page_first: '3339'
  page_last: '3349'
  pages: "3339\u20133349"
  paper_id: '371'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1371.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1371.jpg
  title: Neural Ranking Models for Temporal Dependency Structure Parsing
  title_html: Neural Ranking Models for Temporal Dependency Structure Parsing
  url: https://www.aclweb.org/anthology/D18-1371
  year: '2018'
D18-1372:
  abstract: "Understanding causal explanations - reasons given for happenings in one\u2019\
    s life - has been found to be an important psychological factor linked to physical\
    \ and mental health. Causal explanations are often studied through manual identification\
    \ of phrases over limited samples of personal writing. Automatic identification\
    \ of causal explanations in social media, while challenging in relying on contextual\
    \ and sequential cues, offers a larger-scale alternative to expensive manual ratings\
    \ and opens the door for new applications (e.g. studying prevailing beliefs about\
    \ causes, such as climate change). Here, we explore automating causal explanation\
    \ analysis, building on discourse parsing, and presenting two novel subtasks:\
    \ causality detection (determining whether a causal explanation exists at all)\
    \ and causal explanation identification (identifying the specific phrase that\
    \ is the explanation). We achieve strong accuracies for both tasks but find different\
    \ approaches best: an SVM for causality prediction (F1 = 0.791) and a hierarchy\
    \ of Bidirectional LSTMs for causal explanation identification (F1 = 0.853). Finally,\
    \ we explore applications of our complete pipeline (F1 = 0.868), showing demographic\
    \ differences in mentions of causal explanation and that the association between\
    \ a word and sentiment can change when it is used within a causal explanation."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1372.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1372.Attachment.zip
  author:
  - first: Youngseo
    full: Youngseo Son
    id: youngseo-son
    last: Son
  - first: Nipun
    full: Nipun Bayas
    id: nipun-bayas
    last: Bayas
  - first: H. Andrew
    full: H. Andrew Schwartz
    id: h-andrew-schwartz
    last: Schwartz
  author_string: Youngseo Son, Nipun Bayas, H. Andrew Schwartz
  bibkey: son-etal-2018-causal
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1372
  month: October-November
  page_first: '3350'
  page_last: '3359'
  pages: "3350\u20133359"
  paper_id: '372'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1372.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1372.jpg
  title: Causal Explanation Analysis on Social Media
  title_html: Causal Explanation Analysis on Social Media
  url: https://www.aclweb.org/anthology/D18-1372
  year: '2018'
D18-1373:
  abstract: Multimodal learning has shown promising performance in content-based recommendation
    due to the auxiliary user and item information of multiple modalities such as
    text and images. However, the problem of incomplete and missing modality is rarely
    explored and most existing methods fail in learning a recommendation model with
    missing or corrupted modalities. In this paper, we propose LRMM, a novel framework
    that mitigates not only the problem of missing modalities but also more generally
    the cold-start problem of recommender systems. We propose modality dropout (m-drop)
    and a multimodal sequential autoencoder (m-auto) to learn multimodal representations
    for complementing and imputing missing modalities. Extensive experiments on real-world
    Amazon data show that LRMM achieves state-of-the-art performance on rating prediction
    tasks. More importantly, LRMM is more robust to previous methods in alleviating
    data-sparsity and the cold-start problem.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1373.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1373.Attachment.pdf
  author:
  - first: Cheng
    full: Cheng Wang
    id: cheng-wang
    last: Wang
  - first: Mathias
    full: Mathias Niepert
    id: mathias-niepert
    last: Niepert
  - first: Hui
    full: Hui Li
    id: hui-li
    last: Li
  author_string: Cheng Wang, Mathias Niepert, Hui Li
  bibkey: wang-etal-2018-lrmm
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1373
  month: October-November
  page_first: '3360'
  page_last: '3370'
  pages: "3360\u20133370"
  paper_id: '373'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1373.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1373.jpg
  title: 'LRMM: Learning to Recommend with Missing Modalities'
  title_html: '<span class="acl-fixed-case">LRMM</span>: Learning to Recommend with
    Missing Modalities'
  url: https://www.aclweb.org/anthology/D18-1373
  year: '2018'
D18-1374:
  abstract: Background research is an essential part of document writing. Search engines
    are great for retrieving information once we know what to look for. However, the
    bigger challenge is often identifying topics for further research. Automated tools
    could help significantly in this discovery process and increase the productivity
    of the writer. In this paper, we formulate the problem of recommending topics
    to a writer. We consider this as a supervised learning problem and run a user
    study to validate this approach. We propose an evaluation metric and perform an
    empirical comparison of state-of-the-art models for extreme multi-label classification
    on a large data set. We demonstrate how a simple modification of the cross-entropy
    loss function leads to improved results of the deep learning models.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1374.Attachment.tgz
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1374.Attachment.tgz
  author:
  - first: Michal
    full: Michal Lukasik
    id: michal-lukasik
    last: Lukasik
  - first: Richard
    full: Richard Zens
    id: richard-zens
    last: Zens
  author_string: Michal Lukasik, Richard Zens
  bibkey: lukasik-zens-2018-content
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1374
  month: October-November
  page_first: '3371'
  page_last: '3380'
  pages: "3371\u20133380"
  paper_id: '374'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1374.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1374.jpg
  title: 'Content Explorer: Recommending Novel Entities for a Document Writer'
  title_html: 'Content Explorer: Recommending Novel Entities for a Document Writer'
  url: https://www.aclweb.org/anthology/D18-1374
  year: '2018'
D18-1375:
  abstract: Likability prediction of books has many uses. Readers, writers, as well
    as the publishing industry, can all benefit from automatic book likability prediction
    systems. In order to make reliable decisions, these systems need to assimilate
    information from different aspects of a book in a sensible way. We propose a novel
    multimodal neural architecture that incorporates genre supervision to assign weights
    to individual feature types. Our proposed method is capable of dynamically tailoring
    weights given to feature types based on the characteristics of each book. Our
    architecture achieves competitive results and even outperforms state-of-the-art
    for this task.
  address: Brussels, Belgium
  author:
  - first: Suraj
    full: Suraj Maharjan
    id: suraj-maharjan
    last: Maharjan
  - first: Manuel
    full: Manuel Montes
    id: manuel-montes
    last: Montes
  - first: Fabio A.
    full: "Fabio A. Gonz\xE1lez"
    id: fabio-a-gonzalez
    last: "Gonz\xE1lez"
  - first: Thamar
    full: Thamar Solorio
    id: thamar-solorio
    last: Solorio
  author_string: "Suraj Maharjan, Manuel Montes, Fabio A. Gonz\xE1lez, Thamar Solorio"
  bibkey: maharjan-etal-2018-genre
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1375
  month: October-November
  page_first: '3381'
  page_last: '3391'
  pages: "3381\u20133391"
  paper_id: '375'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1375.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1375.jpg
  title: A Genre-Aware Attention Model to Improve the Likability Prediction of Books
  title_html: A Genre-Aware Attention Model to Improve the Likability Prediction of
    Books
  url: https://www.aclweb.org/anthology/D18-1375
  year: '2018'
D18-1376:
  abstract: The task of thread popularity prediction and tracking aims to recommend
    a few popular comments to subscribed users when a batch of new comments arrive
    in a discussion thread. This task has been formulated as a reinforcement learning
    problem, in which the reward of the agent is the sum of positive responses received
    by the recommended comments. In this work, we propose a novel approach to tackle
    this problem. First, we propose a deep neural network architecture to model the
    expected cumulative reward (Q-value) of a recommendation (action). Unlike the
    state-of-the-art approach, which treats an action as a sequence, our model uses
    an attention mechanism to integrate information from a set of comments. Thus,
    the prediction of Q-value is invariant to the permutation of the comments, which
    leads to a more consistent agent behavior. Second, we employ a greedy procedure
    to approximate the action that maximizes the predicted Q-value from a combinatorial
    action space. Different from the state-of-the-art approach, this procedure does
    not require an additional pre-trained model to generate candidate actions. Experiments
    on five real-world datasets show that our approach outperforms the state-of-the-art.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1376.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1376.Attachment.zip
  author:
  - first: Hou Pong
    full: Hou Pong Chan
    id: hou-pong-chan
    last: Chan
  - first: Irwin
    full: Irwin King
    id: irwin-king
    last: King
  author_string: Hou Pong Chan, Irwin King
  bibkey: chan-king-2018-thread
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1376
  month: October-November
  page_first: '3392'
  page_last: '3401'
  pages: "3392\u20133401"
  paper_id: '376'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1376.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1376.jpg
  title: Thread Popularity Prediction and Tracking with a Permutation-invariant Model
  title_html: Thread Popularity Prediction and Tracking with a Permutation-invariant
    Model
  url: https://www.aclweb.org/anthology/D18-1376
  year: '2018'
D18-1377:
  abstract: 'Sentiment analysis has immense implications in e-commerce through user
    feedback mining. Aspect-based sentiment analysis takes this one step further by
    enabling businesses to extract aspect specific sentimental information. In this
    paper, we present a novel approach of incorporating the neighboring aspects related
    information into the sentiment classification of the target aspect using memory
    networks. We show that our method outperforms the state of the art by 1.6% on
    average in two distinct domains: restaurant and laptop.'
  address: Brussels, Belgium
  author:
  - first: Navonil
    full: Navonil Majumder
    id: navonil-majumder
    last: Majumder
  - first: Soujanya
    full: Soujanya Poria
    id: soujanya-poria
    last: Poria
  - first: Alexander
    full: Alexander Gelbukh
    id: alexander-gelbukh
    last: Gelbukh
  - first: Md. Shad
    full: Md. Shad Akhtar
    id: md-shad-akhtar1
    last: Akhtar
  - first: Erik
    full: Erik Cambria
    id: erik-cambria
    last: Cambria
  - first: Asif
    full: Asif Ekbal
    id: asif-ekbal
    last: Ekbal
  author_string: Navonil Majumder, Soujanya Poria, Alexander Gelbukh, Md. Shad Akhtar,
    Erik Cambria, Asif Ekbal
  bibkey: majumder-etal-2018-iarm
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1377
  month: October-November
  page_first: '3402'
  page_last: '3411'
  pages: "3402\u20133411"
  paper_id: '377'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1377.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1377.jpg
  title: 'IARM: Inter-Aspect Relation Modeling with Memory Networks in Aspect-Based
    Sentiment Analysis'
  title_html: '<span class="acl-fixed-case">IARM</span>: Inter-Aspect Relation Modeling
    with Memory Networks in Aspect-Based Sentiment Analysis'
  url: https://www.aclweb.org/anthology/D18-1377
  year: '2018'
D18-1378:
  abstract: "We propose Limbic, an unsupervised probabilistic model that addresses\
    \ the problem of discovering aspects and sentiments and associating them with\
    \ authors of opinionated texts. Limbic combines three ideas, incorporating authors,\
    \ discourse relations, and word embeddings. For discourse relations, Limbic adopts\
    \ a generative process regularized by a Markov Random Field. To promote words\
    \ with high semantic similarity into the same topic, Limbic captures semantic\
    \ regularities from word embeddings via a generalized P\xF3lya Urn process. We\
    \ demonstrate that Limbic (1) discovers aspects associated with sentiments with\
    \ high lexical diversity; (2) outperforms state-of-the-art models by a substantial\
    \ margin in topic cohesion and sentiment classification."
  address: Brussels, Belgium
  author:
  - first: Zhe
    full: Zhe Zhang
    id: zhe-zhang
    last: Zhang
  - first: Munindar
    full: Munindar Singh
    id: munindar-p-singh
    last: Singh
  author_string: Zhe Zhang, Munindar Singh
  bibkey: zhang-singh-2018-limbic
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1378
  month: October-November
  page_first: '3412'
  page_last: '3422'
  pages: "3412\u20133422"
  paper_id: '378'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1378.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1378.jpg
  title: 'Limbic: Author-Based Sentiment Aspect Modeling Regularized with Word Embeddings
    and Discourse Relations'
  title_html: '<span class="acl-fixed-case">L</span>imbic: Author-Based Sentiment
    Aspect Modeling Regularized with Word Embeddings and Discourse Relations'
  url: https://www.aclweb.org/anthology/D18-1378
  year: '2018'
D18-1379:
  abstract: Text might express or evoke multiple emotions with varying intensities.
    As such, it is crucial to predict and rank multiple relevant emotions by their
    intensities. Moreover, as emotions might be evoked by hidden topics, it is important
    to unveil and incorporate such topical information to understand how the emotions
    are evoked. We proposed a novel interpretable neural network approach for relevant
    emotion ranking. Specifically, motivated by transfer learning, the neural network
    is initialized to make the hidden layer approximate the behavior of topic models.
    Moreover, a novel error function is defined to optimize the whole neural network
    for relevant emotion ranking. Experimental results on three real-world corpora
    show that the proposed approach performs remarkably better than the state-of-the-art
    emotion detection approaches and multi-label learning methods. Moreover, the extracted
    emotion-associated topic words indeed represent emotion-evoking events and are
    in line with our common-sense knowledge.
  address: Brussels, Belgium
  author:
  - first: Yang
    full: Yang Yang
    id: yang-yang
    last: Yang
  - first: Deyu
    full: Deyu Zhou
    id: deyu-zhou
    last: Zhou
  - first: Yulan
    full: Yulan He
    id: yulan-he
    last: He
  author_string: Yang Yang, Deyu Zhou, Yulan He
  bibkey: yang-etal-2018-interpretable
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1379
  month: October-November
  page_first: '3423'
  page_last: '3432'
  pages: "3423\u20133432"
  paper_id: '379'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1379.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1379.jpg
  title: An Interpretable Neural Network with Topical Information for Relevant Emotion
    Ranking
  title_html: An Interpretable Neural Network with Topical Information for Relevant
    Emotion Ranking
  url: https://www.aclweb.org/anthology/D18-1379
  year: '2018'
D18-1380:
  abstract: 'We propose a novel multi-grained attention network (MGAN) model for aspect
    level sentiment classification. Existing approaches mostly adopt coarse-grained
    attention mechanism, which may bring information loss if the aspect has multiple
    words or larger context. We propose a fine-grained attention mechanism, which
    can capture the word-level interaction between aspect and context. And then we
    leverage the fine-grained and coarse-grained attention mechanisms to compose the
    MGAN framework. Moreover, unlike previous works which train each aspect with its
    context separately, we design an aspect alignment loss to depict the aspect-level
    interactions among the aspects that have the same context. We evaluate the proposed
    approach on three datasets: laptop and restaurant are from SemEval 2014, and the
    last one is a twitter dataset. Experimental results show that the multi-grained
    attention network consistently outperforms the state-of-the-art methods on all
    three datasets. We also conduct experiments to evaluate the effectiveness of aspect
    alignment loss, which indicates the aspect-level interactions can bring extra
    useful information and further improve the performance.'
  address: Brussels, Belgium
  author:
  - first: Feifan
    full: Feifan Fan
    id: feifan-fan
    last: Fan
  - first: Yansong
    full: Yansong Feng
    id: yansong-feng
    last: Feng
  - first: Dongyan
    full: Dongyan Zhao
    id: dongyan-zhao
    last: Zhao
  author_string: Feifan Fan, Yansong Feng, Dongyan Zhao
  bibkey: fan-etal-2018-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1380
  month: October-November
  page_first: '3433'
  page_last: '3442'
  pages: "3433\u20133442"
  paper_id: '380'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1380.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1380.jpg
  title: Multi-grained Attention Network for Aspect-Level Sentiment Classification
  title_html: Multi-grained Attention Network for Aspect-Level Sentiment Classification
  url: https://www.aclweb.org/anthology/D18-1380
  year: '2018'
D18-1381:
  abstract: This paper proposes a new neural architecture that exploits readily available
    sentiment lexicon resources. The key idea is that that incorporating a word-level
    prior can aid in the representation learning process, eventually improving model
    performance. To this end, our model employs two distinctly unique components,
    i.e., (1) we introduce a lexicon-driven contextual attention mechanism to imbue
    lexicon words with long-range contextual information and (2), we introduce a contrastive
    co-attention mechanism that models contrasting polarities between all positive
    and negative words in a sentence. Via extensive experiments, we show that our
    approach outperforms many other neural baselines on sentiment classification tasks
    on multiple benchmark datasets.
  address: Brussels, Belgium
  author:
  - first: Yi
    full: Yi Tay
    id: yi-tay
    last: Tay
  - first: Anh Tuan
    full: Anh Tuan Luu
    id: anh-tuan-luu
    last: Luu
  - first: Siu Cheung
    full: Siu Cheung Hui
    id: siu-cheung-hui
    last: Hui
  - first: Jian
    full: Jian Su
    id: jian-su
    last: Su
  author_string: Yi Tay, Anh Tuan Luu, Siu Cheung Hui, Jian Su
  bibkey: tay-etal-2018-attentive
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1381
  month: October-November
  page_first: '3443'
  page_last: '3453'
  pages: "3443\u20133453"
  paper_id: '381'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1381.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1381.jpg
  title: Attentive Gated Lexicon Reader with Contrastive Contextual Co-Attention for
    Sentiment Classification
  title_html: Attentive Gated Lexicon Reader with Contrastive Contextual Co-Attention
    for Sentiment Classification
  url: https://www.aclweb.org/anthology/D18-1381
  year: '2018'
D18-1382:
  abstract: Multi-modal sentiment analysis offers various challenges, one being the
    effective combination of different input modalities, namely text, visual and acoustic.
    In this paper, we propose a recurrent neural network based multi-modal attention
    framework that leverages the contextual information for utterance-level sentiment
    prediction. The proposed approach applies attention on multi-modal multi-utterance
    representations and tries to learn the contributing features amongst them. We
    evaluate our proposed approach on two multi-modal sentiment analysis benchmark
    datasets, viz. CMU Multi-modal Opinion-level Sentiment Intensity (CMU-MOSI) corpus
    and the recently released CMU Multi-modal Opinion Sentiment and Emotion Intensity
    (CMU-MOSEI) corpus. Evaluation results show the effectiveness of our proposed
    approach with the accuracies of 82.31% and 79.80% for the MOSI and MOSEI datasets,
    respectively. These are approximately 2 and 1 points performance improvement over
    the state-of-the-art models for the datasets.
  address: Brussels, Belgium
  author:
  - first: Deepanway
    full: Deepanway Ghosal
    id: deepanway-ghosal
    last: Ghosal
  - first: Md Shad
    full: Md Shad Akhtar
    id: md-shad-akhtar
    last: Akhtar
  - first: Dushyant
    full: Dushyant Chauhan
    id: dushyant-chauhan
    last: Chauhan
  - first: Soujanya
    full: Soujanya Poria
    id: soujanya-poria
    last: Poria
  - first: Asif
    full: Asif Ekbal
    id: asif-ekbal
    last: Ekbal
  - first: Pushpak
    full: Pushpak Bhattacharyya
    id: pushpak-bhattacharyya
    last: Bhattacharyya
  author_string: Deepanway Ghosal, Md Shad Akhtar, Dushyant Chauhan, Soujanya Poria,
    Asif Ekbal, Pushpak Bhattacharyya
  bibkey: ghosal-etal-2018-contextual
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1382
  month: October-November
  page_first: '3454'
  page_last: '3466'
  pages: "3454\u20133466"
  paper_id: '382'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1382.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1382.jpg
  title: Contextual Inter-modal Attention for Multi-modal Sentiment Analysis
  title_html: Contextual Inter-modal Attention for Multi-modal Sentiment Analysis
  url: https://www.aclweb.org/anthology/D18-1382
  year: '2018'
D18-1383:
  abstract: "We consider the cross-domain sentiment classification problem, where\
    \ a sentiment classifier is to be learned from a source domain and to be generalized\
    \ to a target domain. Our approach explicitly minimizes the distance between the\
    \ source and the target instances in an embedded feature space. With the difference\
    \ between source and target minimized, we then exploit additional information\
    \ from the target domain by consolidating the idea of semi-supervised learning,\
    \ for which, we jointly employ two regularizations \u2014 entropy minimization\
    \ and self-ensemble bootstrapping \u2014 to incorporate the unlabeled target data\
    \ for classifier refinement. Our experimental results demonstrate that the proposed\
    \ approach can better leverage unlabeled data from the target domain and achieve\
    \ substantial improvements over baseline methods in various experimental settings."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1383.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1383.Attachment.pdf
  author:
  - first: Ruidan
    full: Ruidan He
    id: ruidan-he
    last: He
  - first: Wee Sun
    full: Wee Sun Lee
    id: wee-sun-lee
    last: Lee
  - first: Hwee Tou
    full: Hwee Tou Ng
    id: hwee-tou-ng
    last: Ng
  - first: Daniel
    full: Daniel Dahlmeier
    id: daniel-dahlmeier
    last: Dahlmeier
  author_string: Ruidan He, Wee Sun Lee, Hwee Tou Ng, Daniel Dahlmeier
  bibkey: he-etal-2018-adaptive
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1383
  month: October-November
  page_first: '3467'
  page_last: '3476'
  pages: "3467\u20133476"
  paper_id: '383'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1383.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1383.jpg
  title: Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification
  title_html: Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification
  url: https://www.aclweb.org/anthology/D18-1383
  year: '2018'
D18-1384:
  abstract: Many existing systems for analyzing and summarizing customer reviews about
    products or service are based on a number of prominent review aspects. Conventionally,
    the prominent review aspects of a product type are determined manually. This costly
    approach cannot scale to large and cross-domain services such as Amazon.com, Taobao.com
    or Yelp.com where there are a large number of product types and new products emerge
    almost every day. In this paper, we propose a novel framework, for extracting
    the most prominent aspects of a given product type from textual reviews. The proposed
    framework, ExtRA, extracts K most prominent aspect terms or phrases which do not
    overlap semantically automatically without supervision. Extensive experiments
    show that ExtRA is effective and achieves the state-of-the-art performance on
    a dataset consisting of different product types.
  address: Brussels, Belgium
  author:
  - first: Zhiyi
    full: Zhiyi Luo
    id: zhiyi-luo
    last: Luo
  - first: Shanshan
    full: Shanshan Huang
    id: shanshan-huang
    last: Huang
  - first: Frank F.
    full: Frank F. Xu
    id: frank-f-xu
    last: Xu
  - first: Bill Yuchen
    full: Bill Yuchen Lin
    id: bill-yuchen-lin
    last: Lin
  - first: Hanyuan
    full: Hanyuan Shi
    id: hanyuan-shi
    last: Shi
  - first: Kenny
    full: Kenny Zhu
    id: kenny-zhu
    last: Zhu
  author_string: Zhiyi Luo, Shanshan Huang, Frank F. Xu, Bill Yuchen Lin, Hanyuan
    Shi, Kenny Zhu
  bibkey: luo-etal-2018-extra
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1384
  month: October-November
  page_first: '3477'
  page_last: '3486'
  pages: "3477\u20133486"
  paper_id: '384'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1384.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1384.jpg
  title: 'ExtRA: Extracting Prominent Review Aspects from Customer Feedback'
  title_html: '<span class="acl-fixed-case">E</span>xt<span class="acl-fixed-case">RA</span>:
    Extracting Prominent Review Aspects from Customer Feedback'
  url: https://www.aclweb.org/anthology/D18-1384
  year: '2018'
D18-1385:
  abstract: With the increasing popularity of smart devices, rumors with multimedia
    content become more and more common on social networks. The multimedia information
    usually makes rumors look more convincing. Therefore, finding an automatic approach
    to verify rumors with multimedia content is a pressing task. Previous rumor verification
    research only utilizes multimedia as input features. We propose not to use the
    multimedia content but to find external information in other news platforms pivoting
    on it. We introduce a new features set, cross-lingual cross-platform features
    that leverage the semantic similarity between the rumors and the external information.
    When implemented, machine learning methods utilizing such features achieved the
    state-of-the-art rumor verification results.
  address: Brussels, Belgium
  author:
  - first: Weiming
    full: Weiming Wen
    id: weiming-wen
    last: Wen
  - first: Songwen
    full: Songwen Su
    id: songwen-su
    last: Su
  - first: Zhou
    full: Zhou Yu
    id: zhou-yu
    last: Yu
  author_string: Weiming Wen, Songwen Su, Zhou Yu
  bibkey: wen-etal-2018-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1385
  month: October-November
  page_first: '3487'
  page_last: '3496'
  pages: "3487\u20133496"
  paper_id: '385'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1385.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1385.jpg
  title: Cross-Lingual Cross-Platform Rumor Verification Pivoting on Multimedia Content
  title_html: Cross-Lingual Cross-Platform Rumor Verification Pivoting on Multimedia
    Content
  url: https://www.aclweb.org/anthology/D18-1385
  year: '2018'
D18-1386:
  abstract: "We introduce an adversarial method for producing high-recall explanations\
    \ of neural text classifier decisions. Building on an existing architecture for\
    \ extractive explanations via hard attention, we add an adversarial layer which\
    \ scans the residual of the attention for remaining predictive signal. Motivated\
    \ by the important domain of detecting personal attacks in social media comments,\
    \ we additionally demonstrate the importance of manually setting a semantically\
    \ appropriate \u201Cdefault\u201D behavior for the model by explicitly manipulating\
    \ its bias term. We develop a validation set of human-annotated personal attacks\
    \ to evaluate the impact of these changes."
  address: Brussels, Belgium
  author:
  - first: Samuel
    full: Samuel Carton
    id: samuel-carton
    last: Carton
  - first: Qiaozhu
    full: Qiaozhu Mei
    id: qiaozhu-mei
    last: Mei
  - first: Paul
    full: Paul Resnick
    id: paul-resnick
    last: Resnick
  author_string: Samuel Carton, Qiaozhu Mei, Paul Resnick
  bibkey: carton-etal-2018-extractive
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1386
  month: October-November
  page_first: '3497'
  page_last: '3507'
  pages: "3497\u20133507"
  paper_id: '386'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1386.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1386.jpg
  title: 'Extractive Adversarial Networks: High-Recall Explanations for Identifying
    Personal Attacks in Social Media Posts'
  title_html: 'Extractive Adversarial Networks: High-Recall Explanations for Identifying
    Personal Attacks in Social Media Posts'
  url: https://www.aclweb.org/anthology/D18-1386
  year: '2018'
D18-1387:
  abstract: Website privacy policies represent the single most important source of
    information for users to gauge how their personal data are collected, used and
    shared by companies. However, privacy policies are often vague and people struggle
    to understand the content. Their opaqueness poses a significant challenge to both
    users and policy regulators. In this paper, we seek to identify vague content
    in privacy policies. We construct the first corpus of human-annotated vague words
    and sentences and present empirical studies on automatic vagueness detection.
    In particular, we investigate context-aware and context-agnostic models for predicting
    vague words, and explore auxiliary-classifier generative adversarial networks
    for characterizing sentence vagueness. Our experimental results demonstrate the
    effectiveness of proposed approaches. Finally, we provide suggestions for resolving
    vagueness and improving the usability of privacy policies.
  address: Brussels, Belgium
  author:
  - first: Logan
    full: Logan Lebanoff
    id: logan-lebanoff
    last: Lebanoff
  - first: Fei
    full: Fei Liu
    id: fei-liu-utdallas
    last: Liu
  author_string: Logan Lebanoff, Fei Liu
  bibkey: lebanoff-liu-2018-automatic
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1387
  month: October-November
  page_first: '3508'
  page_last: '3517'
  pages: "3508\u20133517"
  paper_id: '387'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1387.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1387.jpg
  title: Automatic Detection of Vague Words and Sentences in Privacy Policies
  title_html: Automatic Detection of Vague Words and Sentences in Privacy Policies
  url: https://www.aclweb.org/anthology/D18-1387
  year: '2018'
D18-1388:
  abstract: "A news article\u2019s title, content and link structure often reveal\
    \ its political ideology. However, most existing works on automatic political\
    \ ideology detection only leverage textual cues. Drawing inspiration from recent\
    \ advances in neural inference, we propose a novel attention based multi-view\
    \ model to leverage cues from all of the above views to identify the ideology\
    \ evinced by a news article. Our model draws on advances in representation learning\
    \ in natural language processing and network science to capture cues from both\
    \ textual content and the network structure of news articles. We empirically evaluate\
    \ our model against a battery of baselines and show that our model outperforms\
    \ state of the art by 10 percentage points F1 score."
  address: Brussels, Belgium
  author:
  - first: Vivek
    full: Vivek Kulkarni
    id: vivek-kulkarni
    last: Kulkarni
  - first: Junting
    full: Junting Ye
    id: junting-ye
    last: Ye
  - first: Steve
    full: Steve Skiena
    id: steven-skiena
    last: Skiena
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  author_string: Vivek Kulkarni, Junting Ye, Steve Skiena, William Yang Wang
  bibkey: kulkarni-etal-2018-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1388
  month: October-November
  page_first: '3518'
  page_last: '3527'
  pages: "3518\u20133527"
  paper_id: '388'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1388.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1388.jpg
  title: Multi-view Models for Political Ideology Detection of News Articles
  title_html: Multi-view Models for Political Ideology Detection of News Articles
  url: https://www.aclweb.org/anthology/D18-1388
  year: '2018'
D18-1389:
  abstract: We present a study on predicting the factuality of reporting and bias
    of news media. While previous work has focused on studying the veracity of claims
    or documents, here we are interested in characterizing entire news media. This
    is an under-studied, but arguably important research problem, both in its own
    right and as a prior for fact-checking systems. We experiment with a large list
    of news websites and with a rich set of features derived from (i) a sample of
    articles from the target news media, (ii) its Wikipedia page, (iii) its Twitter
    account, (iv) the structure of its URL, and (v) information about the Web traffic
    it attracts. The experimental results show sizable performance gains over the
    baseline, and reveal the importance of each feature type.
  address: Brussels, Belgium
  author:
  - first: Ramy
    full: Ramy Baly
    id: ramy-baly
    last: Baly
  - first: Georgi
    full: Georgi Karadzhov
    id: georgi-karadzhov
    last: Karadzhov
  - first: Dimitar
    full: Dimitar Alexandrov
    id: dimitar-alexandrov
    last: Alexandrov
  - first: James
    full: James Glass
    id: james-glass
    last: Glass
  - first: Preslav
    full: Preslav Nakov
    id: preslav-nakov
    last: Nakov
  author_string: Ramy Baly, Georgi Karadzhov, Dimitar Alexandrov, James Glass, Preslav
    Nakov
  bibkey: baly-etal-2018-predicting
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1389
  month: October-November
  page_first: '3528'
  page_last: '3539'
  pages: "3528\u20133539"
  paper_id: '389'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1389.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1389.jpg
  title: Predicting Factuality of Reporting and Bias of News Media Sources
  title_html: Predicting Factuality of Reporting and Bias of News Media Sources
  url: https://www.aclweb.org/anthology/D18-1389
  year: '2018'
D18-1390:
  abstract: Legal Judgment Prediction (LJP) aims to predict the judgment result based
    on the facts of a case and becomes a promising application of artificial intelligence
    techniques in the legal field. In real-world scenarios, legal judgment usually
    consists of multiple subtasks, such as the decisions of applicable law articles,
    charges, fines, and the term of penalty. Moreover, there exist topological dependencies
    among these subtasks. While most existing works only focus on a specific subtask
    of judgment prediction and ignore the dependencies among subtasks, we formalize
    the dependencies among subtasks as a Directed Acyclic Graph (DAG) and propose
    a topological multi-task learning framework, TopJudge, which incorporates multiple
    subtasks and DAG dependencies into judgment prediction. We conduct experiments
    on several real-world large-scale datasets of criminal cases in the civil law
    system. Experimental results show that our model achieves consistent and significant
    improvements over baselines on all judgment prediction tasks. The source code
    can be obtained from https://github.com/thunlp/TopJudge.
  address: Brussels, Belgium
  author:
  - first: Haoxi
    full: Haoxi Zhong
    id: haoxi-zhong
    last: Zhong
  - first: Zhipeng
    full: Zhipeng Guo
    id: zhipeng-guo
    last: Guo
  - first: Cunchao
    full: Cunchao Tu
    id: cunchao-tu
    last: Tu
  - first: Chaojun
    full: Chaojun Xiao
    id: chaojun-xiao
    last: Xiao
  - first: Zhiyuan
    full: Zhiyuan Liu
    id: zhiyuan-liu
    last: Liu
  - first: Maosong
    full: Maosong Sun
    id: maosong-sun
    last: Sun
  author_string: Haoxi Zhong, Zhipeng Guo, Cunchao Tu, Chaojun Xiao, Zhiyuan Liu,
    Maosong Sun
  bibkey: zhong-etal-2018-legal
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1390
  month: October-November
  page_first: '3540'
  page_last: '3549'
  pages: "3540\u20133549"
  paper_id: '390'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1390.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1390.jpg
  title: Legal Judgment Prediction via Topological Learning
  title_html: Legal Judgment Prediction via Topological Learning
  url: https://www.aclweb.org/anthology/D18-1390
  year: '2018'
D18-1391:
  abstract: Existing work on automated hate speech detection typically focuses on
    binary classification or on differentiating among a small set of categories. In
    this paper, we propose a novel method on a fine-grained hate speech classification
    task, which focuses on differentiating among 40 hate groups of 13 different hate
    group categories. We first explore the Conditional Variational Autoencoder (CVAE)
    as a discriminative model and then extend it to a hierarchical architecture to
    utilize the additional hate category information for more accurate prediction.
    Experimentally, we show that incorporating the hate category information for training
    can significantly improve the classification performance and our proposed model
    outperforms commonly-used discriminative models.
  address: Brussels, Belgium
  author:
  - first: Jing
    full: Jing Qian
    id: jing-qian
    last: Qian
  - first: Mai
    full: Mai ElSherief
    id: mai-elsherief
    last: ElSherief
  - first: Elizabeth
    full: Elizabeth Belding
    id: elizabeth-belding
    last: Belding
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  author_string: Jing Qian, Mai ElSherief, Elizabeth Belding, William Yang Wang
  bibkey: qian-etal-2018-hierarchical
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1391
  month: October-November
  page_first: '3550'
  page_last: '3559'
  pages: "3550\u20133559"
  paper_id: '391'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1391.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1391.jpg
  title: Hierarchical CVAE for Fine-Grained Hate Speech Classification
  title_html: Hierarchical <span class="acl-fixed-case">CVAE</span> for Fine-Grained
    Hate Speech Classification
  url: https://www.aclweb.org/anthology/D18-1391
  year: '2018'
D18-1392:
  abstract: Predictive models over social media language have shown promise in capturing
    community outcomes, but approaches thus far largely neglect the socio-demographic
    context (e.g. age, education rates, race) of the community from which the language
    originates. For example, it may be inaccurate to assume people in Mobile, Alabama,
    where the population is relatively older, will use words the same way as those
    from San Francisco, where the median age is younger with a higher rate of college
    education. In this paper, we present residualized factor adaptation, a novel approach
    to community prediction tasks which both (a) effectively integrates community
    attributes, as well as (b) adapts linguistic features to community attributes
    (factors). We use eleven demographic and socioeconomic attributes, and evaluate
    our approach over five different community-level predictive tasks, spanning health
    (heart disease mortality, percent fair/poor health), psychology (life satisfaction),
    and economics (percent housing price increase, foreclosure rate). Our evaluation
    shows that residualized factor adaptation significantly improves 4 out of 5 community-level
    outcome predictions over prior state-of-the-art for incorporating socio-demographic
    contexts.
  address: Brussels, Belgium
  author:
  - first: Mohammadzaman
    full: Mohammadzaman Zamani
    id: mohammadzaman-zamani
    last: Zamani
  - first: H. Andrew
    full: H. Andrew Schwartz
    id: h-andrew-schwartz
    last: Schwartz
  - first: Veronica
    full: Veronica Lynn
    id: veronica-lynn
    last: Lynn
  - first: Salvatore
    full: Salvatore Giorgi
    id: salvatore-giorgi
    last: Giorgi
  - first: Niranjan
    full: Niranjan Balasubramanian
    id: niranjan-balasubramanian
    last: Balasubramanian
  author_string: Mohammadzaman Zamani, H. Andrew Schwartz, Veronica Lynn, Salvatore
    Giorgi, Niranjan Balasubramanian
  bibkey: zamani-etal-2018-residualized
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1392
  month: October-November
  page_first: '3560'
  page_last: '3569'
  pages: "3560\u20133569"
  paper_id: '392'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1392.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1392.jpg
  title: Residualized Factor Adaptation for Community Social Media Prediction Tasks
  title_html: Residualized Factor Adaptation for Community Social Media Prediction
    Tasks
  url: https://www.aclweb.org/anthology/D18-1392
  year: '2018'
D18-1393:
  abstract: "Amidst growing concern over media manipulation, NLP attention has focused\
    \ on overt strategies like censorship and \u201Cfake news\u201D. Here, we draw\
    \ on two concepts from political science literature to explore subtler strategies\
    \ for government media manipulation: agenda-setting (selecting what topics to\
    \ cover) and framing (deciding how topics are covered). We analyze 13 years (100K\
    \ articles) of the Russian newspaper Izvestia and identify a strategy of distraction:\
    \ articles mention the U.S. more frequently in the month directly following an\
    \ economic downturn in Russia. We introduce embedding-based methods for cross-lingually\
    \ projecting English frames to Russian, and discover that these articles emphasize\
    \ U.S. moral failings and threats to the U.S. Our work offers new ways to identify\
    \ subtle media manipulation strategies at the intersection of agenda-setting and\
    \ framing."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1393.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1393.Attachment.pdf
  author:
  - first: Anjalie
    full: Anjalie Field
    id: anjalie-field
    last: Field
  - first: Doron
    full: Doron Kliger
    id: doron-kliger
    last: Kliger
  - first: Shuly
    full: Shuly Wintner
    id: shuly-wintner
    last: Wintner
  - first: Jennifer
    full: Jennifer Pan
    id: jennifer-pan
    last: Pan
  - first: Dan
    full: Dan Jurafsky
    id: dan-jurafsky
    last: Jurafsky
  - first: Yulia
    full: Yulia Tsvetkov
    id: yulia-tsvetkov
    last: Tsvetkov
  author_string: Anjalie Field, Doron Kliger, Shuly Wintner, Jennifer Pan, Dan Jurafsky,
    Yulia Tsvetkov
  bibkey: field-etal-2018-framing
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1393
  month: October-November
  page_first: '3570'
  page_last: '3580'
  pages: "3570\u20133580"
  paper_id: '393'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1393.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1393.jpg
  title: 'Framing and Agenda-setting in Russian News: a Computational Analysis of
    Intricate Political Strategies'
  title_html: 'Framing and Agenda-setting in <span class="acl-fixed-case">R</span>ussian
    News: a Computational Analysis of Intricate Political Strategies'
  url: https://www.aclweb.org/anthology/D18-1393
  year: '2018'
D18-1394:
  abstract: "Vlogs provide a rich public source of data in a novel setting. This paper\
    \ examined the continuous sentiment styles employed in 27,333 vlogs using a dynamic\
    \ intra-textual approach to sentiment analysis. Using unsupervised clustering,\
    \ we identified seven distinct continuous sentiment trajectories characterized\
    \ by fluctuations of sentiment throughout a vlog\u2019s narrative time. We provide\
    \ a taxonomy of these seven continuous sentiment styles and found that vlogs whose\
    \ sentiment builds up towards a positive ending are the most prevalent in our\
    \ sample. Gender was associated with preferences for different continuous sentiment\
    \ trajectories. This paper discusses the findings with respect to previous work\
    \ and concludes with an outlook towards possible uses of the corpus, method and\
    \ findings of this paper for related areas of research."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1394.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1394.Attachment.pdf
  author:
  - first: Bennett
    full: Bennett Kleinberg
    id: bennett-kleinberg
    last: Kleinberg
  - first: Maximilian
    full: Maximilian Mozes
    id: maximilian-mozes
    last: Mozes
  - first: Isabelle
    full: Isabelle van der Vegt
    id: isabelle-van-der-vegt
    last: van der Vegt
  author_string: Bennett Kleinberg, Maximilian Mozes, Isabelle van der Vegt
  bibkey: kleinberg-etal-2018-identifying
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1394
  month: October-November
  page_first: '3581'
  page_last: '3590'
  pages: "3581\u20133590"
  paper_id: '394'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1394.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1394.jpg
  title: "Identifying the sentiment styles of YouTube\u2019s vloggers"
  title_html: "Identifying the sentiment styles of <span class=\"acl-fixed-case\"\
    >Y</span>ou<span class=\"acl-fixed-case\">T</span>ube\u2019s vloggers"
  url: https://www.aclweb.org/anthology/D18-1394
  year: '2018'
D18-1395:
  abstract: We address the task of native language identification in the context of
    social media content, where authors are highly-fluent, advanced nonnative speakers
    (of English). Using both linguistically-motivated features and the characteristics
    of the social media outlet, we obtain high accuracy on this challenging task.
    We provide a detailed analysis of the features that sheds light on differences
    between native and nonnative speakers, and among nonnative speakers with different
    backgrounds.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1395.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1395.Attachment.pdf
  author:
  - first: Gili
    full: Gili Goldin
    id: gili-goldin
    last: Goldin
  - first: Ella
    full: Ella Rabinovich
    id: ella-rabinovich
    last: Rabinovich
  - first: Shuly
    full: Shuly Wintner
    id: shuly-wintner
    last: Wintner
  author_string: Gili Goldin, Ella Rabinovich, Shuly Wintner
  bibkey: goldin-etal-2018-native
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1395
  month: October-November
  page_first: '3591'
  page_last: '3601'
  pages: "3591\u20133601"
  paper_id: '395'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1395.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1395.jpg
  title: Native Language Identification with User Generated Content
  title_html: Native Language Identification with User Generated Content
  url: https://www.aclweb.org/anthology/D18-1395
  year: '2018'
D18-1396:
  abstract: 'Neural machine translation usually adopts autoregressive models and suffers
    from exposure bias as well as the consequent error propagation problem. Many previous
    works have discussed the relationship between error propagation and the accuracy
    drop (i.e., the left part of the translated sentence is often better than its
    right part in left-to-right decoding models) problem. In this paper, we conduct
    a series of analyses to deeply understand this problem and get several interesting
    findings. (1) The role of error propagation on accuracy drop is overstated in
    the literature, although it indeed contributes to the accuracy drop problem. (2)
    Characteristics of a language play a more important role in causing the accuracy
    drop: the left part of the translation result in a right-branching language (e.g.,
    English) is more likely to be more accurate than its right part, while the right
    part is more accurate for a left-branching language (e.g., Japanese). Our discoveries
    are confirmed on different model structures including Transformer and RNN, and
    in other sequence generation tasks such as text summarization.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1396.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1396.Attachment.pdf
  - filename: https://vimeo.com/306146050
    type: video
    url: https://vimeo.com/306146050
  author:
  - first: Lijun
    full: Lijun Wu
    id: lijun-wu
    last: Wu
  - first: Xu
    full: Xu Tan
    id: xu-tan
    last: Tan
  - first: Di
    full: Di He
    id: di-he
    last: He
  - first: Fei
    full: Fei Tian
    id: fei-tian
    last: Tian
  - first: Tao
    full: Tao Qin
    id: tao-qin
    last: Qin
  - first: Jianhuang
    full: Jianhuang Lai
    id: jianhuang-lai
    last: Lai
  - first: Tie-Yan
    full: Tie-Yan Liu
    id: tie-yan-liu
    last: Liu
  author_string: Lijun Wu, Xu Tan, Di He, Fei Tian, Tao Qin, Jianhuang Lai, Tie-Yan
    Liu
  bibkey: wu-etal-2018-beyond
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1396
  month: October-November
  page_first: '3602'
  page_last: '3611'
  pages: "3602\u20133611"
  paper_id: '396'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1396.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1396.jpg
  title: 'Beyond Error Propagation in Neural Machine Translation: Characteristics
    of Language Also Matter'
  title_html: 'Beyond Error Propagation in Neural Machine Translation: Characteristics
    of Language Also Matter'
  url: https://www.aclweb.org/anthology/D18-1396
  year: '2018'
D18-1397:
  abstract: Recent studies have shown that reinforcement learning (RL) is an effective
    approach for improving the performance of neural machine translation (NMT) system.
    However, due to its instability, successfully RL training is challenging, especially
    in real-world systems where deep models and large datasets are leveraged. In this
    paper, taking several large-scale translation tasks as testbeds, we conduct a
    systematic study on how to train better NMT models using reinforcement learning.
    We provide a comprehensive comparison of several important factors (e.g., baseline
    reward, reward shaping) in RL training. Furthermore, to fill in the gap that it
    remains unclear whether RL is still beneficial when monolingual data is used,
    we propose a new method to leverage RL to further boost the performance of NMT
    systems trained with source/target monolingual data. By integrating all our findings,
    we obtain competitive results on WMT14 English-German, WMT17 English-Chinese,
    and WMT17 Chinese-English translation tasks, especially setting a state-of-the-art
    performance on WMT17 Chinese-English translation task.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306147010
    type: video
    url: https://vimeo.com/306147010
  author:
  - first: Lijun
    full: Lijun Wu
    id: lijun-wu
    last: Wu
  - first: Fei
    full: Fei Tian
    id: fei-tian
    last: Tian
  - first: Tao
    full: Tao Qin
    id: tao-qin
    last: Qin
  - first: Jianhuang
    full: Jianhuang Lai
    id: jianhuang-lai
    last: Lai
  - first: Tie-Yan
    full: Tie-Yan Liu
    id: tie-yan-liu
    last: Liu
  author_string: Lijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, Tie-Yan Liu
  bibkey: wu-etal-2018-study
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1397
  month: October-November
  page_first: '3612'
  page_last: '3621'
  pages: "3612\u20133621"
  paper_id: '397'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1397.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1397.jpg
  title: A Study of Reinforcement Learning for Neural Machine Translation
  title_html: A Study of Reinforcement Learning for Neural Machine Translation
  url: https://www.aclweb.org/anthology/D18-1397
  year: '2018'
D18-1398:
  abstract: "In this paper, we propose to extend the recently introduced model-agnostic\
    \ meta-learning algorithm (MAML, Finn, et al., 2017) for low-resource neural machine\
    \ translation (NMT). We frame low-resource translation as a meta-learning problem\
    \ where we learn to adapt to low-resource languages based on multilingual high-resource\
    \ language tasks. We use the universal lexical representation (Gu et al., 2018b)\
    \ to overcome the input-output mismatch across different languages. We evaluate\
    \ the proposed meta-learning strategy using eighteen European languages (Bg, Cs,\
    \ Da, De, El, Es, Et, Fr, Hu, It, Lt, Nl, Pl, Pt, Sk, Sl, Sv and Ru) as source\
    \ tasks and five diverse languages (Ro,Lv, Fi, Tr and Ko) as target tasks. We\
    \ show that the proposed approach significantly outperforms the multilingual,\
    \ transfer learning based approach (Zoph et al., 2016) and enables us to train\
    \ a competitive NMT system with only a fraction of training examples. For instance,\
    \ the proposed approach can achieve as high as 22.04 BLEU on Romanian-English\
    \ WMT\u201916 by seeing only 16,000 translated words (~600 parallel sentences)"
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306147573
    type: video
    url: https://vimeo.com/306147573
  author:
  - first: Jiatao
    full: Jiatao Gu
    id: jiatao-gu
    last: Gu
  - first: Yong
    full: Yong Wang
    id: yong-wang
    last: Wang
  - first: Yun
    full: Yun Chen
    id: yun-chen
    last: Chen
  - first: Victor O. K.
    full: Victor O. K. Li
    id: victor-o-k-li
    last: Li
  - first: Kyunghyun
    full: Kyunghyun Cho
    id: kyunghyun-cho
    last: Cho
  author_string: Jiatao Gu, Yong Wang, Yun Chen, Victor O. K. Li, Kyunghyun Cho
  bibkey: gu-etal-2018-meta
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1398
  month: October-November
  page_first: '3622'
  page_last: '3631'
  pages: "3622\u20133631"
  paper_id: '398'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1398.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1398.jpg
  title: Meta-Learning for Low-Resource Neural Machine Translation
  title_html: Meta-Learning for Low-Resource Neural Machine Translation
  url: https://www.aclweb.org/anthology/D18-1398
  year: '2018'
D18-1399:
  abstract: 'While modern machine translation has relied on large parallel corpora,
    a recent line of work has managed to train Neural Machine Translation (NMT) systems
    from monolingual corpora only (Artetxe et al., 2018c; Lample et al., 2018). Despite
    the potential of this approach for low-resource settings, existing systems are
    far behind their supervised counterparts, limiting their practical interest. In
    this paper, we propose an alternative approach based on phrase-based Statistical
    Machine Translation (SMT) that significantly closes the gap with supervised systems.
    Our method profits from the modular architecture of SMT: we first induce a phrase
    table from monolingual corpora through cross-lingual embedding mappings, combine
    it with an n-gram language model, and fine-tune hyperparameters through an unsupervised
    MERT variant. In addition, iterative backtranslation improves results further,
    yielding, for instance, 14.08 and 26.22 BLEU points in WMT 2014 English-German
    and English-French, respectively, an improvement of more than 7-10 BLEU points
    over previous unsupervised systems, and closing the gap with supervised SMT (Moses
    trained on Europarl) down to 2-5 BLEU points. Our implementation is available
    at https://github.com/artetxem/monoses.'
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306148376
    type: video
    url: https://vimeo.com/306148376
  author:
  - first: Mikel
    full: Mikel Artetxe
    id: mikel-artetxe
    last: Artetxe
  - first: Gorka
    full: Gorka Labaka
    id: gorka-labaka
    last: Labaka
  - first: Eneko
    full: Eneko Agirre
    id: eneko-agirre
    last: Agirre
  author_string: Mikel Artetxe, Gorka Labaka, Eneko Agirre
  bibkey: artetxe-etal-2018-unsupervised
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1399
  month: October-November
  page_first: '3632'
  page_last: '3642'
  pages: "3632\u20133642"
  paper_id: '399'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1399.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1399.jpg
  title: Unsupervised Statistical Machine Translation
  title_html: Unsupervised Statistical Machine Translation
  url: https://www.aclweb.org/anthology/D18-1399
  year: '2018'
D18-1400:
  abstract: We introduce a novel multimodal machine translation model that utilizes
    parallel visual and textual information. Our model jointly optimizes the learning
    of a shared visual-language embedding and a translator. The model leverages a
    visual attention grounding mechanism that links the visual semantics with the
    corresponding textual semantics. Our approach achieves competitive state-of-the-art
    results on the Multi30K and the Ambiguous COCO datasets. We also collected a new
    multilingual multimodal product description dataset to simulate a real-world international
    online shopping scenario. On this dataset, our visual attention grounding model
    outperforms other methods by a large margin.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1400.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1400.Attachment.zip
  - filename: https://vimeo.com/306149028
    type: video
    url: https://vimeo.com/306149028
  author:
  - first: Mingyang
    full: Mingyang Zhou
    id: mingyang-zhou
    last: Zhou
  - first: Runxiang
    full: Runxiang Cheng
    id: runxiang-cheng
    last: Cheng
  - first: Yong Jae
    full: Yong Jae Lee
    id: yong-jae-lee
    last: Lee
  - first: Zhou
    full: Zhou Yu
    id: zhou-yu
    last: Yu
  author_string: Mingyang Zhou, Runxiang Cheng, Yong Jae Lee, Zhou Yu
  bibkey: zhou-etal-2018-visual
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1400
  month: October-November
  page_first: '3643'
  page_last: '3653'
  pages: "3643\u20133653"
  paper_id: '400'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1400.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1400.jpg
  title: A Visual Attention Grounding Neural Model for Multimodal Machine Translation
  title_html: A Visual Attention Grounding Neural Model for Multimodal Machine Translation
  url: https://www.aclweb.org/anthology/D18-1400
  year: '2018'
D18-1401:
  abstract: In an e-commerce environment, user-oriented question-answering (QA) text
    pair could carry rich sentiment information. In this study, we propose a novel
    task/method to address QA sentiment analysis. In particular, we create a high-quality
    annotated corpus with specially-designed annotation guidelines for QA-style sentiment
    classification. On the basis, we propose a three-stage hierarchical matching network
    to explore deep sentiment information in a QA text pair. First, we segment both
    the question and answer text into sentences and construct a number of [Q-sentence,
    A-sentence] units in each QA text pair. Then, by leveraging a QA bidirectional
    matching layer, the proposed approach can learn the matching vectors of each [Q-sentence,
    A-sentence] unit. Finally, we characterize the importance of the generated matching
    vectors via a self-matching attention layer. Experimental results, comparing with
    a number of state-of-the-art baselines, demonstrate the impressive effectiveness
    of the proposed approach for QA-style sentiment classification.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306126825
    type: video
    url: https://vimeo.com/306126825
  author:
  - first: Chenlin
    full: Chenlin Shen
    id: chenlin-shen
    last: Shen
  - first: Changlong
    full: Changlong Sun
    id: changlong-sun
    last: Sun
  - first: Jingjing
    full: Jingjing Wang
    id: jingjing-wang
    last: Wang
  - first: Yangyang
    full: Yangyang Kang
    id: yangyang-kang
    last: Kang
  - first: Shoushan
    full: Shoushan Li
    id: shoushan-li
    last: Li
  - first: Xiaozhong
    full: Xiaozhong Liu
    id: xiaozhong-liu
    last: Liu
  - first: Luo
    full: Luo Si
    id: luo-si
    last: Si
  - first: Min
    full: Min Zhang
    id: min-zhang
    last: Zhang
  - first: Guodong
    full: Guodong Zhou
    id: guodong-zhou
    last: Zhou
  author_string: Chenlin Shen, Changlong Sun, Jingjing Wang, Yangyang Kang, Shoushan
    Li, Xiaozhong Liu, Luo Si, Min Zhang, Guodong Zhou
  bibkey: shen-etal-2018-sentiment
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1401
  month: October-November
  page_first: '3654'
  page_last: '3663'
  pages: "3654\u20133663"
  paper_id: '401'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1401.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1401.jpg
  title: Sentiment Classification towards Question-Answering with Hierarchical Matching
    Network
  title_html: Sentiment Classification towards Question-Answering with Hierarchical
    Matching Network
  url: https://www.aclweb.org/anthology/D18-1401
  year: '2018'
D18-1402:
  abstract: Argument mining is a core technology for automating argument search in
    large document collections. Despite its usefulness for this task, most current
    approaches are designed for use only with specific text types and fall short when
    applied to heterogeneous texts. In this paper, we propose a new sentential annotation
    scheme that is reliably applicable by crowd workers to arbitrary Web texts. We
    source annotations for over 25,000 instances covering eight controversial topics.
    We show that integrating topic information into bidirectional long short-term
    memory networks outperforms vanilla BiLSTMs by more than 3 percentage points in
    F1 in two- and three-label cross-topic settings. We also show that these results
    can be further improved by leveraging additional data for topic relevance using
    multi-task learning.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1402.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1402.Attachment.zip
  - filename: https://vimeo.com/306127543
    type: video
    url: https://vimeo.com/306127543
  author:
  - first: Christian
    full: Christian Stab
    id: christian-stab
    last: Stab
  - first: Tristan
    full: Tristan Miller
    id: tristan-miller
    last: Miller
  - first: Benjamin
    full: Benjamin Schiller
    id: benjamin-schiller
    last: Schiller
  - first: Pranav
    full: Pranav Rai
    id: pranav-rai
    last: Rai
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Christian Stab, Tristan Miller, Benjamin Schiller, Pranav Rai, Iryna
    Gurevych
  bibkey: stab-etal-2018-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1402
  month: October-November
  page_first: '3664'
  page_last: '3674'
  pages: "3664\u20133674"
  paper_id: '402'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1402.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1402.jpg
  title: Cross-topic Argument Mining from Heterogeneous Sources
  title_html: Cross-topic Argument Mining from Heterogeneous Sources
  url: https://www.aclweb.org/anthology/D18-1402
  year: '2018'
D18-1403:
  abstract: 'We present a neural framework for opinion summarization from online product
    reviews which is knowledge-lean and only requires light supervision (e.g., in
    the form of product domain labels and user-provided ratings). Our method combines
    two weakly supervised components to identify salient opinions and form extractive
    summaries from multiple reviews: an aspect extractor trained under a multi-task
    objective, and a sentiment predictor based on multiple instance learning. We introduce
    an opinion summarization dataset that includes a training set of product reviews
    from six diverse domains and human-annotated development and test sets with gold
    standard aspect annotations, salience labels, and opinion summaries. Automatic
    evaluation shows significant improvements over baselines, and a large-scale study
    indicates that our opinion summaries are preferred by human judges according to
    multiple criteria.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1403.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1403.Attachment.zip
  - filename: https://vimeo.com/306128219
    type: video
    url: https://vimeo.com/306128219
  author:
  - first: Stefanos
    full: Stefanos Angelidis
    id: stefanos-angelidis
    last: Angelidis
  - first: Mirella
    full: Mirella Lapata
    id: mirella-lapata
    last: Lapata
  author_string: Stefanos Angelidis, Mirella Lapata
  bibkey: angelidis-lapata-2018-summarizing
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1403
  month: October-November
  page_first: '3675'
  page_last: '3686'
  pages: "3675\u20133686"
  paper_id: '403'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1403.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1403.jpg
  title: 'Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and They
    Are Both Weakly Supervised'
  title_html: 'Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction
    and They Are Both Weakly Supervised'
  url: https://www.aclweb.org/anthology/D18-1403
  year: '2018'
D18-1404:
  abstract: Emotions are expressed in nuanced ways, which varies by collective or
    individual experiences, knowledge, and beliefs. Therefore, to understand emotion,
    as conveyed through text, a robust mechanism capable of capturing and modeling
    different linguistic nuances and phenomena is needed. We propose a semi-supervised,
    graph-based algorithm to produce rich structural descriptors which serve as the
    building blocks for constructing contextualized affect representations from text.
    The pattern-based representations are further enriched with word embeddings and
    evaluated through several emotion recognition tasks. Our experimental results
    demonstrate that the proposed method outperforms state-of-the-art techniques on
    emotion recognition tasks.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1404.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1404.Attachment.zip
  - filename: https://vimeo.com/306129121
    type: video
    url: https://vimeo.com/306129121
  author:
  - first: Elvis
    full: Elvis Saravia
    id: elvis-saravia
    last: Saravia
  - first: Hsien-Chi Toby
    full: Hsien-Chi Toby Liu
    id: hsien-chi-toby-liu
    last: Liu
  - first: Yen-Hao
    full: Yen-Hao Huang
    id: yen-hao-huang
    last: Huang
  - first: Junlin
    full: Junlin Wu
    id: junlin-wu
    last: Wu
  - first: Yi-Shin
    full: Yi-Shin Chen
    id: yi-shin-chen
    last: Chen
  author_string: Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, Yi-Shin
    Chen
  bibkey: saravia-etal-2018-carer
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1404
  month: October-November
  page_first: '3687'
  page_last: '3697'
  pages: "3687\u20133697"
  paper_id: '404'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1404.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1404.jpg
  title: 'CARER: Contextualized Affect Representations for Emotion Recognition'
  title_html: '<span class="acl-fixed-case">CARER</span>: Contextualized Affect Representations
    for Emotion Recognition'
  url: https://www.aclweb.org/anthology/D18-1404
  year: '2018'
D18-1405:
  abstract: 'Noise Contrastive Estimation (NCE) is a powerful parameter estimation
    method for log-linear models, which avoids calculation of the partition function
    or its derivatives at each training step, a computationally demanding step in
    many cases. It is closely related to negative sampling methods, now widely used
    in NLP. This paper considers NCE-based estimation of conditional models. Conditional
    models are frequently encountered in practice; however there has not been a rigorous
    theoretical analysis of NCE in this setting, and we will argue there are subtle
    but important questions when generalizing NCE to the conditional case. In particular,
    we analyze two variants of NCE for conditional models: one based on a classification
    objective, the other based on a ranking objective. We show that the ranking-based
    variant of NCE gives consistent parameter estimates under weaker assumptions than
    the classification-based method; we analyze the statistical efficiency of the
    ranking-based and classification-based variants of NCE; finally we describe experiments
    on synthetic data and language modeling showing the effectiveness and tradeoffs
    of both methods.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1405.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1405.Attachment.pdf
  - filename: https://vimeo.com/306156327
    type: video
    url: https://vimeo.com/306156327
  author:
  - first: Zhuang
    full: Zhuang Ma
    id: zhuang-ma
    last: Ma
  - first: Michael
    full: Michael Collins
    id: michael-collins
    last: Collins
  author_string: Zhuang Ma, Michael Collins
  bibkey: ma-collins-2018-noise
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1405
  month: October-November
  page_first: '3698'
  page_last: '3707'
  pages: "3698\u20133707"
  paper_id: '405'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1405.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1405.jpg
  title: 'Noise Contrastive Estimation and Negative Sampling for Conditional Models:
    Consistency and Statistical Efficiency'
  title_html: 'Noise Contrastive Estimation and Negative Sampling for Conditional
    Models: Consistency and Statistical Efficiency'
  url: https://www.aclweb.org/anthology/D18-1405
  year: '2018'
D18-1406:
  abstract: Maximum-likelihood estimation (MLE) is one of the most widely used approaches
    for training structured prediction models for text-generation based natural language
    processing applications. However, besides exposure bias, models trained with MLE
    suffer from wrong objective problem where they are trained to maximize the word-level
    correct next step prediction, but are evaluated with respect to sequence-level
    discrete metrics such as ROUGE and BLEU. Several variants of policy-gradient methods
    address some of these problems by optimizing for final discrete evaluation metrics
    and showing improvements over MLE training for downstream tasks like text summarization
    and machine translation. However, policy-gradient methods suffers from high sample
    variance, making the training process very difficult and unstable. In this paper,
    we present an alternative direction towards mitigating this problem by introducing
    a new objective (CaLcs) based on a differentiable surrogate of longest common
    subsequence (LCS) measure that captures sequence-level structure similarity. Experimental
    results on abstractive summarization and machine translation validate the effectiveness
    of the proposed approach.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306157322
    type: video
    url: https://vimeo.com/306157322
  author:
  - first: Semih
    full: Semih Yavuz
    id: semih-yavuz
    last: Yavuz
  - first: Chung-Cheng
    full: Chung-Cheng Chiu
    id: chung-cheng-chiu
    last: Chiu
  - first: Patrick
    full: Patrick Nguyen
    id: patrick-nguyen
    last: Nguyen
  - first: Yonghui
    full: Yonghui Wu
    id: yonghui-wu
    last: Wu
  author_string: Semih Yavuz, Chung-Cheng Chiu, Patrick Nguyen, Yonghui Wu
  bibkey: yavuz-etal-2018-calcs
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1406
  month: October-November
  page_first: '3708'
  page_last: '3718'
  pages: "3708\u20133718"
  paper_id: '406'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1406.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1406.jpg
  title: 'CaLcs: Continuously Approximating Longest Common Subsequence for Sequence
    Level Optimization'
  title_html: '<span class="acl-fixed-case">C</span>a<span class="acl-fixed-case">L</span>cs:
    Continuously Approximating Longest Common Subsequence for Sequence Level Optimization'
  url: https://www.aclweb.org/anthology/D18-1406
  year: '2018'
D18-1407:
  abstract: "One way to interpret neural model predictions is to highlight the most\
    \ important input features\u2014for example, a heatmap visualization over the\
    \ words in an input sentence. In existing interpretation methods for NLP, a word\u2019\
    s importance is determined by either input perturbation\u2014measuring the decrease\
    \ in model confidence when that word is removed\u2014or by the gradient with respect\
    \ to that word. To understand the limitations of these methods, we use input reduction,\
    \ which iteratively removes the least important word from the input. This exposes\
    \ pathological behaviors of neural models: the remaining words appear nonsensical\
    \ to humans and are not the ones determined as important by interpretation methods.\
    \ As we confirm with human experiments, the reduced examples lack information\
    \ to support the prediction of any label, but models still make the same predictions\
    \ with high confidence. To explain these counterintuitive results, we draw connections\
    \ to adversarial examples and confidence calibration: pathological behaviors reveal\
    \ difficulties in interpreting neural models trained with maximum likelihood.\
    \ To mitigate their deficiencies, we fine-tune the models by encouraging high\
    \ entropy outputs on reduced examples. Fine-tuned models become more interpretable\
    \ under input reduction, without accuracy loss on regular examples."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1407.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1407.Attachment.zip
  - filename: https://vimeo.com/306158589
    type: video
    url: https://vimeo.com/306158589
  author:
  - first: Shi
    full: Shi Feng
    id: shi-feng
    last: Feng
  - first: Eric
    full: Eric Wallace
    id: eric-wallace
    last: Wallace
  - first: Alvin
    full: Alvin Grissom II
    id: alvin-grissom-ii
    last: Grissom II
  - first: Mohit
    full: Mohit Iyyer
    id: mohit-iyyer
    last: Iyyer
  - first: Pedro
    full: Pedro Rodriguez
    id: pedro-rodriguez
    last: Rodriguez
  - first: Jordan
    full: Jordan Boyd-Graber
    id: jordan-boyd-graber
    last: Boyd-Graber
  author_string: Shi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer, Pedro Rodriguez,
    Jordan Boyd-Graber
  bibkey: feng-etal-2018-pathologies
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1407
  month: October-November
  page_first: '3719'
  page_last: '3728'
  pages: "3719\u20133728"
  paper_id: '407'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1407.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1407.jpg
  title: Pathologies of Neural Models Make Interpretations Difficult
  title_html: Pathologies of Neural Models Make Interpretations Difficult
  url: https://www.aclweb.org/anthology/D18-1407
  year: '2018'
D18-1408:
  abstract: "Universal sentence encoding is a hot topic in recent NLP research. Attention\
    \ mechanism has been an integral part in many sentence encoding models, allowing\
    \ the models to capture context dependencies regardless of the distance between\
    \ the elements in the sequence. Fully attention-based models have recently attracted\
    \ enormous interest due to their highly parallelizable computation and significantly\
    \ less training time. However, the memory consumption of their models grows quadratically\
    \ with the sentence length, and the syntactic information is neglected. To this\
    \ end, we propose Phrase-level Self-Attention Networks (PSAN) that perform self-attention\
    \ across words inside a phrase to capture context dependencies at the phrase level,\
    \ and use the gated memory updating mechanism to refine each word\u2019s representation\
    \ hierarchically with longer-term context dependencies captured in a larger phrase.\
    \ As a result, the memory consumption can be reduced because the self-attention\
    \ is performed at the phrase level instead of the sentence level. At the same\
    \ time, syntactic information can be easily integrated in the model. Experiment\
    \ results show that PSAN can achieve the state-of-the-art performance across a\
    \ plethora of NLP tasks including binary and multi-class classification, natural\
    \ language inference and sentence similarity."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306159624
    type: video
    url: https://vimeo.com/306159624
  author:
  - first: Wei
    full: Wei Wu
    id: wei-wu
    last: Wu
  - first: Houfeng
    full: Houfeng Wang
    id: houfeng-wang
    last: Wang
  - first: Tianyu
    full: Tianyu Liu
    id: tianyu-liu
    last: Liu
  - first: Shuming
    full: Shuming Ma
    id: shuming-ma
    last: Ma
  author_string: Wei Wu, Houfeng Wang, Tianyu Liu, Shuming Ma
  bibkey: wu-etal-2018-phrase
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1408
  month: October-November
  page_first: '3729'
  page_last: '3738'
  pages: "3729\u20133738"
  paper_id: '408'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1408.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1408.jpg
  title: Phrase-level Self-Attention Networks for Universal Sentence Encoding
  title_html: Phrase-level Self-Attention Networks for Universal Sentence Encoding
  url: https://www.aclweb.org/anthology/D18-1408
  year: '2018'
D18-1409:
  abstract: In this work, we propose a novel method for training neural networks to
    perform single-document extractive summarization without heuristically-generated
    extractive labels. We call our approach BanditSum as it treats extractive summarization
    as a contextual bandit (CB) problem, where the model receives a document to summarize
    (the context), and chooses a sequence of sentences to include in the summary (the
    action). A policy gradient reinforcement learning algorithm is used to train the
    model to select sequences of sentences that maximize ROUGE score. We perform a
    series of experiments demonstrating that BanditSum is able to achieve ROUGE scores
    that are better than or comparable to the state-of-the-art for extractive summarization,
    and converges using significantly fewer update steps than competing approaches.
    In addition, we show empirically that BanditSum performs significantly better
    than competing approaches when good summary sentences appear late in the source
    document.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1409.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1409.Attachment.pdf
  - filename: https://vimeo.com/306160623
    type: video
    url: https://vimeo.com/306160623
  author:
  - first: Yue
    full: Yue Dong
    id: yue-dong
    last: Dong
  - first: Yikang
    full: Yikang Shen
    id: yikang-shen
    last: Shen
  - first: Eric
    full: Eric Crawford
    id: eric-crawford
    last: Crawford
  - first: Herke
    full: Herke van Hoof
    id: herke-van-hoof
    last: van Hoof
  - first: Jackie Chi Kit
    full: Jackie Chi Kit Cheung
    id: jackie-chi-kit-cheung
    last: Cheung
  author_string: Yue Dong, Yikang Shen, Eric Crawford, Herke van Hoof, Jackie Chi
    Kit Cheung
  bibkey: dong-etal-2018-banditsum
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1409
  month: October-November
  page_first: '3739'
  page_last: '3748'
  pages: "3739\u20133748"
  paper_id: '409'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1409.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1409.jpg
  title: 'BanditSum: Extractive Summarization as a Contextual Bandit'
  title_html: '<span class="acl-fixed-case">B</span>andit<span class="acl-fixed-case">S</span>um:
    Extractive Summarization as a Contextual Bandit'
  url: https://www.aclweb.org/anthology/D18-1409
  year: '2018'
D18-1410:
  abstract: Current lexical simplification approaches rely heavily on heuristics and
    corpus level features that do not always align with human judgment. We create
    a human-rated word-complexity lexicon of 15,000 English words and propose a novel
    neural readability ranking model with a Gaussian-based feature vectorization layer
    that utilizes these human ratings to measure the complexity of any given word
    or phrase. Our model performs better than the state-of-the-art systems for different
    lexical simplification tasks and evaluation datasets. Additionally, we also produce
    SimplePPDB++, a lexical resource of over 10 million simplifying paraphrase rules,
    by applying our model to the Paraphrase Database (PPDB).
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306116474
    type: video
    url: https://vimeo.com/306116474
  author:
  - first: Mounica
    full: Mounica Maddela
    id: mounica-maddela
    last: Maddela
  - first: Wei
    full: Wei Xu
    id: wei-xu
    last: Xu
  author_string: Mounica Maddela, Wei Xu
  bibkey: maddela-xu-2018-word
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1410
  month: October-November
  page_first: '3749'
  page_last: '3760'
  pages: "3749\u20133760"
  paper_id: '410'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1410.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1410.jpg
  title: A Word-Complexity Lexicon and A Neural Readability Ranking Model for Lexical
    Simplification
  title_html: A Word-Complexity Lexicon and A Neural Readability Ranking Model for
    Lexical Simplification
  url: https://www.aclweb.org/anthology/D18-1410
  year: '2018'
D18-1411:
  abstract: Previous work on grounded language learning did not fully capture the
    semantics underlying the correspondences between structured world state representations
    and texts, especially those between numerical values and lexical terms. In this
    paper, we attempt at learning explicit latent semantic annotations from paired
    structured tables and texts, establishing correspondences between various types
    of values and texts. We model the joint probability of data fields, texts, phrasal
    spans, and latent annotations with an adapted semi-hidden Markov model, and impose
    a soft statistical constraint to further improve the performance. As a by-product,
    we leverage the induced annotations to extract templates for language generation.
    Experimental results suggest the feasibility of the setting in this study, as
    well as the effectiveness of our proposed framework.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1411.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1411.Attachment.pdf
  - filename: https://vimeo.com/306117499
    type: video
    url: https://vimeo.com/306117499
  author:
  - first: Guanghui
    full: Guanghui Qin
    id: guanghui-qin
    last: Qin
  - first: Jin-Ge
    full: Jin-Ge Yao
    id: jin-ge-yao
    last: Yao
  - first: Xuening
    full: Xuening Wang
    id: xuening-wang
    last: Wang
  - first: Jinpeng
    full: Jinpeng Wang
    id: jinpeng-wang
    last: Wang
  - first: Chin-Yew
    full: Chin-Yew Lin
    id: chin-yew-lin
    last: Lin
  author_string: Guanghui Qin, Jin-Ge Yao, Xuening Wang, Jinpeng Wang, Chin-Yew Lin
  bibkey: qin-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1411
  month: October-November
  page_first: '3761'
  page_last: '3771'
  pages: "3761\u20133771"
  paper_id: '411'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1411.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1411.jpg
  title: Learning Latent Semantic Annotations for Grounding Natural Language to Structured
    Data
  title_html: Learning Latent Semantic Annotations for Grounding Natural Language
    to Structured Data
  url: https://www.aclweb.org/anthology/D18-1411
  year: '2018'
D18-1412:
  abstract: We introduce the syntactic scaffold, an approach to incorporating syntactic
    information into semantic tasks. Syntactic scaffolds avoid expensive syntactic
    processing at runtime, only making use of a treebank during training, through
    a multitask objective. We improve over strong baselines on PropBank semantics,
    frame semantics, and coreference resolution, achieving competitive performance
    on all three tasks.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1412.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1412.Attachment.zip
  - filename: https://vimeo.com/306118515
    type: video
    url: https://vimeo.com/306118515
  author:
  - first: Swabha
    full: Swabha Swayamdipta
    id: swabha-swayamdipta
    last: Swayamdipta
  - first: Sam
    full: Sam Thomson
    id: sam-thomson
    last: Thomson
  - first: Kenton
    full: Kenton Lee
    id: kenton-lee
    last: Lee
  - first: Luke
    full: Luke Zettlemoyer
    id: luke-zettlemoyer
    last: Zettlemoyer
  - first: Chris
    full: Chris Dyer
    id: chris-dyer
    last: Dyer
  - first: Noah A.
    full: Noah A. Smith
    id: noah-a-smith
    last: Smith
  author_string: Swabha Swayamdipta, Sam Thomson, Kenton Lee, Luke Zettlemoyer, Chris
    Dyer, Noah A. Smith
  bibkey: swayamdipta-etal-2018-syntactic
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1412
  month: October-November
  page_first: '3772'
  page_last: '3782'
  pages: "3772\u20133782"
  paper_id: '412'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1412.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1412.jpg
  title: Syntactic Scaffolds for Semantic Structures
  title_html: Syntactic Scaffolds for Semantic Structures
  url: https://www.aclweb.org/anthology/D18-1412
  year: '2018'
D18-1413:
  abstract: Scripts define knowledge about how everyday scenarios (such as going to
    a restaurant) are expected to unfold. One of the challenges to learning scripts
    is the hierarchical nature of the knowledge. For example, a suspect arrested might
    plead innocent or guilty, and a very different track of events is then expected
    to happen. To capture this type of information, we propose an autoencoder model
    with a latent space defined by a hierarchy of categorical variables. We utilize
    a recently proposed vector quantization based approach, which allows continuous
    embeddings to be associated with each latent variable value. This permits the
    decoder to softly decide what portions of the latent hierarchy to condition on
    by attending over the value embeddings for a given setting. Our model effectively
    encodes and generates scripts, outperforming a recent language modeling-based
    method on several standard tasks, and allowing the autoencoder model to achieve
    substantially lower perplexity scores compared to the previous language modeling-based
    method.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306119229
    type: video
    url: https://vimeo.com/306119229
  author:
  - first: Noah
    full: Noah Weber
    id: noah-weber
    last: Weber
  - first: Leena
    full: Leena Shekhar
    id: leena-shekhar
    last: Shekhar
  - first: Niranjan
    full: Niranjan Balasubramanian
    id: niranjan-balasubramanian
    last: Balasubramanian
  - first: Nathanael
    full: Nathanael Chambers
    id: nathanael-chambers
    last: Chambers
  author_string: Noah Weber, Leena Shekhar, Niranjan Balasubramanian, Nathanael Chambers
  bibkey: weber-etal-2018-hierarchical
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1413
  month: October-November
  page_first: '3783'
  page_last: '3792'
  pages: "3783\u20133792"
  paper_id: '413'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1413.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1413.jpg
  title: Hierarchical Quantized Representations for Script Generation
  title_html: Hierarchical Quantized Representations for Script Generation
  url: https://www.aclweb.org/anthology/D18-1413
  year: '2018'
D18-1414:
  abstract: 'This paper studies semantic parsing for interlanguage (L2), taking semantic
    role labeling (SRL) as a case task and learner Chinese as a case language. We
    first manually annotate the semantic roles for a set of learner texts to derive
    a gold standard for automatic SRL. Based on the new data, we then evaluate three
    off-the-shelf SRL systems, i.e., the PCFGLA-parser-based, neural-parser-based
    and neural-syntax-agnostic systems, to gauge how successful SRL for learner Chinese
    can be. We find two non-obvious facts: 1) the L1-sentence-trained systems performs
    rather badly on the L2 data; 2) the performance drop from the L1 data to the L2
    data of the two parser-based systems is much smaller, indicating the importance
    of syntactic parsing in SRL for interlanguages. Finally, the paper introduces
    a new agreement-based model to explore the semantic coherency information in the
    large-scale L2-L1 parallel data. We then show such information is very effective
    to enhance SRL for learner texts. Our model achieves an F-score of 72.06, which
    is a 2.02 point improvement over the best baseline.'
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306119942
    type: video
    url: https://vimeo.com/306119942
  author:
  - first: Zi
    full: Zi Lin
    id: zi-lin
    last: Lin
  - first: Yuguang
    full: Yuguang Duan
    id: yuguang-duan
    last: Duan
  - first: Yuanyuan
    full: Yuanyuan Zhao
    id: yuanyuan-zhao
    last: Zhao
  - first: Weiwei
    full: Weiwei Sun
    id: weiwei-sun
    last: Sun
  - first: Xiaojun
    full: Xiaojun Wan
    id: xiaojun-wan
    last: Wan
  author_string: Zi Lin, Yuguang Duan, Yuanyuan Zhao, Weiwei Sun, Xiaojun Wan
  bibkey: lin-etal-2018-semantic
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1414
  month: October-November
  page_first: '3793'
  page_last: '3802'
  pages: "3793\u20133802"
  paper_id: '414'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1414.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1414.jpg
  title: 'Semantic Role Labeling for Learner Chinese: the Importance of Syntactic
    Parsing and L2-L1 Parallel Data'
  title_html: 'Semantic Role Labeling for Learner <span class="acl-fixed-case">C</span>hinese:
    the Importance of Syntactic Parsing and <span class="acl-fixed-case">L</span>2-<span
    class="acl-fixed-case">L</span>1 Parallel Data'
  url: https://www.aclweb.org/anthology/D18-1414
  year: '2018'
D18-1415:
  abstract: "Reinforcement learning (RL) is an attractive solution for task-oriented\
    \ dialog systems. However, extending RL-based systems to handle new intents and\
    \ slots requires a system redesign. The high maintenance cost makes it difficult\
    \ to apply RL methods to practical systems on a large scale. To address this issue,\
    \ we propose a practical teacher-student framework to extend RL-based dialog systems\
    \ without retraining from scratch. Specifically, the \u201Cstudent\u201D is an\
    \ extended dialog manager based on a new ontology, and the \u201Cteacher\u201D\
    \ is existing resources used for guiding the learning process of the \u201Cstudent\u201D\
    . By specifying constraints held in the new dialog manager, we transfer knowledge\
    \ of the \u201Cteacher\u201D to the \u201Cstudent\u201D without additional resources.\
    \ Experiments show that the performance of the extended system is comparable to\
    \ the system trained from scratch. More importantly, the proposed framework makes\
    \ no assumption about the unsupported intents and slots, which makes it possible\
    \ to improve RL-based systems incrementally."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1415.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1415.Attachment.zip
  author:
  - first: Weikang
    full: Weikang Wang
    id: weikang-wang
    last: Wang
  - first: Jiajun
    full: Jiajun Zhang
    id: jiajun-zhang
    last: Zhang
  - first: Han
    full: Han Zhang
    id: han-zhang
    last: Zhang
  - first: Mei-Yuh
    full: Mei-Yuh Hwang
    id: mei-yuh-hwang
    last: Hwang
  - first: Chengqing
    full: Chengqing Zong
    id: chengqing-zong
    last: Zong
  - first: Zhifei
    full: Zhifei Li
    id: zhifei-li
    last: Li
  author_string: Weikang Wang, Jiajun Zhang, Han Zhang, Mei-Yuh Hwang, Chengqing Zong,
    Zhifei Li
  bibkey: wang-etal-2018-teacher
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1415
  month: October-November
  page_first: '3803'
  page_last: '3812'
  pages: "3803\u20133812"
  paper_id: '415'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1415.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1415.jpg
  title: A Teacher-Student Framework for Maintainable Dialog Manager
  title_html: A Teacher-Student Framework for Maintainable Dialog Manager
  url: https://www.aclweb.org/anthology/D18-1415
  year: '2018'
D18-1416:
  abstract: "This paper presents a Discriminative Deep Dyna-Q (D3Q) approach to improving\
    \ the effectiveness and robustness of Deep Dyna-Q (DDQ), a recently proposed framework\
    \ that extends the Dyna-Q algorithm to integrate planning for task-completion\
    \ dialogue policy learning. To obviate DDQ\u2019s high dependency on the quality\
    \ of simulated experiences, we incorporate an RNN-based discriminator in D3Q to\
    \ differentiate simulated experience from real user experience in order to control\
    \ the quality of training data. Experiments show that D3Q significantly outperforms\
    \ DDQ by controlling the quality of simulated experience used for planning. The\
    \ effectiveness and robustness of D3Q is further demonstrated in a domain extension\
    \ setting, where the agent\u2019s capability of adapting to a changing environment\
    \ is tested."
  address: Brussels, Belgium
  author:
  - first: Shang-Yu
    full: Shang-Yu Su
    id: shang-yu-su
    last: Su
  - first: Xiujun
    full: Xiujun Li
    id: xiujun-li
    last: Li
  - first: Jianfeng
    full: Jianfeng Gao
    id: jianfeng-gao
    last: Gao
  - first: Jingjing
    full: Jingjing Liu
    id: jingjing-liu
    last: Liu
  - first: Yun-Nung
    full: Yun-Nung Chen
    id: yun-nung-chen
    last: Chen
  author_string: Shang-Yu Su, Xiujun Li, Jianfeng Gao, Jingjing Liu, Yun-Nung Chen
  bibkey: su-etal-2018-discriminative
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1416
  month: October-November
  page_first: '3813'
  page_last: '3823'
  pages: "3813\u20133823"
  paper_id: '416'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1416.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1416.jpg
  title: 'Discriminative Deep Dyna-Q: Robust Planning for Dialogue Policy Learning'
  title_html: 'Discriminative Deep <span class="acl-fixed-case">D</span>yna-Q: Robust
    Planning for Dialogue Policy Learning'
  url: https://www.aclweb.org/anthology/D18-1416
  year: '2018'
D18-1417:
  abstract: Spoken Language Understanding (SLU), which typically involves intent determination
    and slot filling, is a core component of spoken dialogue systems. Joint learning
    has shown to be effective in SLU given that slot tags and intents are supposed
    to share knowledge with each other. However, most existing joint learning methods
    only consider joint learning by sharing parameters on surface level rather than
    semantic level. In this work, we propose a novel self-attentive model with gate
    mechanism to fully utilize the semantic correlation between slot and intent. Our
    model first obtains intent-augmented embeddings based on neural network with self-attention
    mechanism. And then the intent semantic representation is utilized as the gate
    for labelling slot tags. The objectives of both tasks are optimized simultaneously
    via joint learning in an end-to-end way. We conduct experiment on popular benchmark
    ATIS. The results show that our model achieves state-of-the-art and outperforms
    other popular methods by a large margin in terms of both intent detection error
    rate and slot filling F1-score. This paper gives a new perspective for research
    on SLU.
  address: Brussels, Belgium
  author:
  - first: Changliang
    full: Changliang Li
    id: changliang-li
    last: Li
  - first: Liang
    full: Liang Li
    id: liang-li
    last: Li
  - first: Ji
    full: Ji Qi
    id: ji-qi
    last: Qi
  author_string: Changliang Li, Liang Li, Ji Qi
  bibkey: li-etal-2018-self
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1417
  month: October-November
  page_first: '3824'
  page_last: '3833'
  pages: "3824\u20133833"
  paper_id: '417'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1417.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1417.jpg
  title: A Self-Attentive Model with Gate Mechanism for Spoken Language Understanding
  title_html: A Self-Attentive Model with Gate Mechanism for Spoken Language Understanding
  url: https://www.aclweb.org/anthology/D18-1417
  year: '2018'
D18-1418:
  abstract: In a dialog, there could be multiple valid next utterances at any point.
    The present end-to-end neural methods for dialog do not take this into account.
    They learn with the assumption that at any time there is only one correct next
    utterance. In this work, we focus on this problem in the goal-oriented dialog
    setting where there are different paths to reach a goal. We propose a new method,
    that uses a combination of supervised learning and reinforcement learning approaches
    to address this issue. We also propose a new and more effective testbed, permuted-bAbI
    dialog tasks, by introducing multiple valid next utterances to the original-bAbI
    dialog tasks, which allows evaluation of end-to-end goal-oriented dialog systems
    in a more realistic setting. We show that there is a significant drop in performance
    of existing end-to-end neural methods from 81.5% per-dialog accuracy on original-bAbI
    dialog tasks to 30.3% on permuted-bAbI dialog tasks. We also show that our proposed
    method improves the performance and achieves 47.3% per-dialog accuracy on permuted-bAbI
    dialog tasks. We also release permuted-bAbI dialog tasks, our proposed testbed,
    to the community for evaluating dialog systems in a goal-oriented setting.
  address: Brussels, Belgium
  author:
  - first: Janarthanan
    full: Janarthanan Rajendran
    id: janarthanan-rajendran
    last: Rajendran
  - first: Jatin
    full: Jatin Ganhotra
    id: jatin-ganhotra
    last: Ganhotra
  - first: Satinder
    full: Satinder Singh
    id: satinder-singh
    last: Singh
  - first: Lazaros
    full: Lazaros Polymenakos
    id: lazaros-polymenakos
    last: Polymenakos
  author_string: Janarthanan Rajendran, Jatin Ganhotra, Satinder Singh, Lazaros Polymenakos
  bibkey: rajendran-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1418
  month: October-November
  page_first: '3834'
  page_last: '3843'
  pages: "3834\u20133843"
  paper_id: '418'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1418.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1418.jpg
  title: Learning End-to-End Goal-Oriented Dialog with Multiple Answers
  title_html: Learning End-to-End Goal-Oriented Dialog with Multiple Answers
  url: https://www.aclweb.org/anthology/D18-1418
  year: '2018'
D18-1419:
  abstract: Recent progress in dialogue generation has inspired a number of studies
    on dialogue systems that are capable of accomplishing tasks through natural language
    interactions. A promising direction among these studies is the use of reinforcement
    learning techniques, such as self-play, for training dialogue agents. However,
    current datasets are limited in size, and the environment for training agents
    and evaluating progress is relatively unsophisticated. We present AirDialogue,
    a large dataset that contains 301,427 goal-oriented conversations. To collect
    this dataset, we create a context-generator which provides travel and flight restrictions.
    We then ask human annotators to play the role of a customer or an agent and interact
    with the goal of successfully booking a trip given the restrictions. Key to our
    environment is the ease of evaluating the success of the dialogue, which is achieved
    by using ground-truth states (e.g., the flight being booked) generated by the
    restrictions. Any dialogue agent that does not generate the correct states is
    considered to fail. Our experimental results indicate that state-of-the-art dialogue
    models can only achieve a score of 0.17 while humans can reach a score of 0.91,
    which suggests significant opportunities for future improvement.
  address: Brussels, Belgium
  author:
  - first: Wei
    full: Wei Wei
    id: wei-wei
    last: Wei
  - first: Quoc
    full: Quoc Le
    id: quoc-le
    last: Le
  - first: Andrew
    full: Andrew Dai
    id: andrew-dai
    last: Dai
  - first: Jia
    full: Jia Li
    id: jia-li
    last: Li
  author_string: Wei Wei, Quoc Le, Andrew Dai, Jia Li
  bibkey: wei-etal-2018-airdialogue
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1419
  month: October-November
  page_first: '3844'
  page_last: '3854'
  pages: "3844\u20133854"
  paper_id: '419'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1419.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1419.jpg
  title: 'AirDialogue: An Environment for Goal-Oriented Dialogue Research'
  title_html: '<span class="acl-fixed-case">A</span>ir<span class="acl-fixed-case">D</span>ialogue:
    An Environment for Goal-Oriented Dialogue Research'
  url: https://www.aclweb.org/anthology/D18-1419
  year: '2018'
D18-1420:
  abstract: 'We propose the task of Quantifiable Sequence Editing (QuaSE): editing
    an input sequence to generate an output sequence that satisfies a given numerical
    outcome value measuring a certain property of the sequence, with the requirement
    of keeping the main content of the input sequence. For example, an input sequence
    could be a word sequence, such as review sentence and advertisement text. For
    a review sentence, the outcome could be the review rating; for an advertisement,
    the outcome could be the click-through rate. The major challenge in performing
    QuaSE is how to perceive the outcome-related wordings, and only edit them to change
    the outcome. In this paper, the proposed framework contains two latent factors,
    namely, outcome factor and content factor, disentangled from the input sentence
    to allow convenient editing to change the outcome and keep the content. Our framework
    explores the pseudo-parallel sentences by modeling their content similarity and
    outcome differences to enable a better disentanglement of the latent factors,
    which allows generating an output to better satisfy the desired outcome and keep
    the content. The dual reconstruction structure further enhances the capability
    of generating expected output by exploiting the couplings of latent factors of
    pseudo-parallel sentences. For evaluation, we prepared a dataset of Yelp review
    sentences with the ratings as outcome. Extensive experimental results are reported
    and discussed to elaborate the peculiarities of our framework.'
  address: Brussels, Belgium
  author:
  - first: Yi
    full: Yi Liao
    id: yi-liao
    last: Liao
  - first: Lidong
    full: Lidong Bing
    id: lidong-bing
    last: Bing
  - first: Piji
    full: Piji Li
    id: piji-li
    last: Li
  - first: Shuming
    full: Shuming Shi
    id: shuming-shi
    last: Shi
  - first: Wai
    full: Wai Lam
    id: wai-lam
    last: Lam
  - first: Tong
    full: Tong Zhang
    id: tong-zhang
    last: Zhang
  author_string: Yi Liao, Lidong Bing, Piji Li, Shuming Shi, Wai Lam, Tong Zhang
  bibkey: liao-etal-2018-quase
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1420
  month: October-November
  page_first: '3855'
  page_last: '3864'
  pages: "3855\u20133864"
  paper_id: '420'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1420.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1420.jpg
  title: 'QuaSE: Sequence Editing under Quantifiable Guidance'
  title_html: '<span class="acl-fixed-case">Q</span>ua<span class="acl-fixed-case">SE</span>:
    Sequence Editing under Quantifiable Guidance'
  url: https://www.aclweb.org/anthology/D18-1420
  year: '2018'
D18-1421:
  abstract: Automatic generation of paraphrases from a given sentence is an important
    yet challenging task in natural language processing (NLP). In this paper, we present
    a deep reinforcement learning approach to paraphrase generation. Specifically,
    we propose a new framework for the task, which consists of a generator and an
    evaluator, both of which are learned from data. The generator, built as a sequence-to-sequence
    learning model, can produce paraphrases given a sentence. The evaluator, constructed
    as a deep matching model, can judge whether two sentences are paraphrases of each
    other. The generator is first trained by deep learning and then further fine-tuned
    by reinforcement learning in which the reward is given by the evaluator. For the
    learning of the evaluator, we propose two methods based on supervised learning
    and inverse reinforcement learning respectively, depending on the type of available
    training data. Experimental results on two datasets demonstrate the proposed models
    (the generators) can produce more accurate paraphrases and outperform the state-of-the-art
    methods in paraphrase generation in both automatic evaluation and human evaluation.
  address: Brussels, Belgium
  author:
  - first: Zichao
    full: Zichao Li
    id: zichao-li
    last: Li
  - first: Xin
    full: Xin Jiang
    id: xin-jiang
    last: Jiang
  - first: Lifeng
    full: Lifeng Shang
    id: lifeng-shang
    last: Shang
  - first: Hang
    full: Hang Li
    id: hang-li
    last: Li
  author_string: Zichao Li, Xin Jiang, Lifeng Shang, Hang Li
  bibkey: li-etal-2018-paraphrase
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1421
  month: October-November
  page_first: '3865'
  page_last: '3878'
  pages: "3865\u20133878"
  paper_id: '421'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1421.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1421.jpg
  title: Paraphrase Generation with Deep Reinforcement Learning
  title_html: Paraphrase Generation with Deep Reinforcement Learning
  url: https://www.aclweb.org/anthology/D18-1421
  year: '2018'
D18-1422:
  abstract: Recent neural models for data-to-text generation are mostly based on data-driven
    end-to-end training over encoder-decoder networks. Even though the generated texts
    are mostly fluent and informative, they often generate descriptions that are not
    consistent with the input structured data. This is a critical issue especially
    in domains that require inference or calculations over raw data. In this paper,
    we attempt to improve the fidelity of neural data-to-text generation by utilizing
    pre-executed symbolic operations. We propose a framework called Operation-guided
    Attention-based sequence-to-sequence network (OpAtt), with a specifically designed
    gating mechanism as well as a quantization module for operation results to utilize
    information from pre-executed operations. Experiments on two sports datasets show
    our proposed method clearly improves the fidelity of the generated texts to the
    input structured data.
  address: Brussels, Belgium
  author:
  - first: Feng
    full: Feng Nie
    id: feng-nie
    last: Nie
  - first: Jinpeng
    full: Jinpeng Wang
    id: jinpeng-wang
    last: Wang
  - first: Jin-Ge
    full: Jin-Ge Yao
    id: jin-ge-yao
    last: Yao
  - first: Rong
    full: Rong Pan
    id: rong-pan
    last: Pan
  - first: Chin-Yew
    full: Chin-Yew Lin
    id: chin-yew-lin
    last: Lin
  author_string: Feng Nie, Jinpeng Wang, Jin-Ge Yao, Rong Pan, Chin-Yew Lin
  bibkey: nie-etal-2018-operation
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1422
  month: October-November
  page_first: '3879'
  page_last: '3889'
  pages: "3879\u20133889"
  paper_id: '422'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1422.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1422.jpg
  title: Operation-guided Neural Networks for High Fidelity Data-To-Text Generation
  title_html: Operation-guided Neural Networks for High Fidelity Data-To-Text Generation
  url: https://www.aclweb.org/anthology/D18-1422
  year: '2018'
D18-1423:
  abstract: It is a challenging task to automatically compose poems with not only
    fluent expressions but also aesthetic wording. Although much attention has been
    paid to this task and promising progress is made, there exist notable gaps between
    automatically generated ones with those created by humans, especially on the aspects
    of term novelty and thematic consistency. Towards filling the gap, in this paper,
    we propose a conditional variational autoencoder with adversarial training for
    classical Chinese poem generation, where the autoencoder part generates poems
    with novel terms and a discriminator is applied to adversarially learn their thematic
    consistency with their titles. Experimental results on a large poetry corpus confirm
    the validity and effectiveness of our model, where its automatic and human evaluation
    scores outperform existing models.
  address: Brussels, Belgium
  author:
  - first: Juntao
    full: Juntao Li
    id: juntao-li
    last: Li
  - first: Yan
    full: Yan Song
    id: yan-song
    last: Song
  - first: Haisong
    full: Haisong Zhang
    id: haisong-zhang
    last: Zhang
  - first: Dongmin
    full: Dongmin Chen
    id: dongmin-chen
    last: Chen
  - first: Shuming
    full: Shuming Shi
    id: shuming-shi
    last: Shi
  - first: Dongyan
    full: Dongyan Zhao
    id: dongyan-zhao
    last: Zhao
  - first: Rui
    full: Rui Yan
    id: rui-yan
    last: Yan
  author_string: Juntao Li, Yan Song, Haisong Zhang, Dongmin Chen, Shuming Shi, Dongyan
    Zhao, Rui Yan
  bibkey: li-etal-2018-generating-classical
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1423
  month: October-November
  page_first: '3890'
  page_last: '3900'
  pages: "3890\u20133900"
  paper_id: '423'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1423.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1423.jpg
  title: Generating Classical Chinese Poems via Conditional Variational Autoencoder
    and Adversarial Training
  title_html: Generating Classical <span class="acl-fixed-case">C</span>hinese Poems
    via Conditional Variational Autoencoder and Adversarial Training
  url: https://www.aclweb.org/anthology/D18-1423
  year: '2018'
D18-1424:
  abstract: "Question generation, the task of automatically creating questions that\
    \ can be answered by a certain span of text within a given passage, is important\
    \ for question-answering and conversational systems in digital assistants such\
    \ as Alexa, Cortana, Google Assistant and Siri. Recent sequence to sequence neural\
    \ models have outperformed previous rule-based systems. Existing models mainly\
    \ focused on using one or two sentences as the input. Long text has posed challenges\
    \ for sequence to sequence neural models in question generation \u2013 worse performances\
    \ were reported if using the whole paragraph (with multiple sentences) as the\
    \ input. In reality, however, it often requires the whole paragraph as context\
    \ in order to generate high quality questions. In this paper, we propose a maxout\
    \ pointer mechanism with gated self-attention encoder to address the challenges\
    \ of processing long text inputs for question generation. With sentence-level\
    \ inputs, our model outperforms previous approaches with either sentence-level\
    \ or paragraph-level inputs. Furthermore, our model can effectively utilize paragraphs\
    \ as inputs, pushing the state-of-the-art result from 13.9 to 16.3 (BLEU_4)."
  address: Brussels, Belgium
  author:
  - first: Yao
    full: Yao Zhao
    id: yao-zhao
    last: Zhao
  - first: Xiaochuan
    full: Xiaochuan Ni
    id: xiaochuan-ni
    last: Ni
  - first: Yuanyuan
    full: Yuanyuan Ding
    id: yuanyuan-ding
    last: Ding
  - first: Qifa
    full: Qifa Ke
    id: qifa-ke
    last: Ke
  author_string: Yao Zhao, Xiaochuan Ni, Yuanyuan Ding, Qifa Ke
  bibkey: zhao-etal-2018-paragraph
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1424
  month: October-November
  page_first: '3901'
  page_last: '3910'
  pages: "3901\u20133910"
  paper_id: '424'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1424.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1424.jpg
  title: Paragraph-level Neural Question Generation with Maxout Pointer and Gated
    Self-attention Networks
  title_html: Paragraph-level Neural Question Generation with Maxout Pointer and Gated
    Self-attention Networks
  url: https://www.aclweb.org/anthology/D18-1424
  year: '2018'
D18-1425:
  abstract: We present Spider, a large-scale complex and cross-domain semantic parsing
    and text-to-SQL dataset annotated by 11 college students. It consists of 10,181
    questions and 5,693 unique complex SQL queries on 200 databases with multiple
    tables covering 138 different domains. We define a new complex and cross-domain
    semantic parsing and text-to-SQL task so that different complicated SQL queries
    and databases appear in train and test sets. In this way, the task requires the
    model to generalize well to both new SQL queries and new database schemas. Therefore,
    Spider is distinct from most of the previous semantic parsing tasks because they
    all use a single database and have the exact same program in the train set and
    the test set. We experiment with various state-of-the-art models and the best
    model achieves only 9.7% exact matching accuracy on a database split setting.
    This shows that Spider presents a strong challenge for future research. Our dataset
    and task with the most recent updates are publicly available at https://yale-lily.github.io/seq2sql/spider.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1425.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1425.Attachment.zip
  author:
  - first: Tao
    full: Tao Yu
    id: tao-yu
    last: Yu
  - first: Rui
    full: Rui Zhang
    id: rui-zhang
    last: Zhang
  - first: Kai
    full: Kai Yang
    id: kai-yang
    last: Yang
  - first: Michihiro
    full: Michihiro Yasunaga
    id: michihiro-yasunaga
    last: Yasunaga
  - first: Dongxu
    full: Dongxu Wang
    id: dongxu-wang
    last: Wang
  - first: Zifan
    full: Zifan Li
    id: zifan-li
    last: Li
  - first: James
    full: James Ma
    id: james-ma
    last: Ma
  - first: Irene
    full: Irene Li
    id: irene-li
    last: Li
  - first: Qingning
    full: Qingning Yao
    id: qingning-yao
    last: Yao
  - first: Shanelle
    full: Shanelle Roman
    id: shanelle-roman
    last: Roman
  - first: Zilin
    full: Zilin Zhang
    id: zilin-zhang
    last: Zhang
  - first: Dragomir
    full: Dragomir Radev
    id: dragomir-radev
    last: Radev
  author_string: Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan
    Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, Dragomir Radev
  bibkey: yu-etal-2018-spider
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1425
  month: October-November
  page_first: '3911'
  page_last: '3921'
  pages: "3911\u20133921"
  paper_id: '425'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1425.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1425.jpg
  title: 'Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain
    Semantic Parsing and Text-to-SQL Task'
  title_html: '<span class="acl-fixed-case">S</span>pider: A Large-Scale Human-Labeled
    Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-<span class="acl-fixed-case">SQL</span>
    Task'
  url: https://www.aclweb.org/anthology/D18-1425
  year: '2018'
D18-1426:
  abstract: Generating text from structured data is important for various tasks such
    as question answering and dialog systems. We show that in at least one domain,
    without any supervision and only based on unlabeled text, we are able to build
    a Natural Language Generation (NLG) system with higher performance than supervised
    approaches. In our approach, we interpret the structured data as a corrupt representation
    of the desired output and use a denoising auto-encoder to reconstruct the sentence.
    We show how to introduce noise into training examples that do not contain structured
    data, and that the resulting denoising auto-encoder generalizes to generate correct
    sentences when given structured data.
  address: Brussels, Belgium
  author:
  - first: Markus
    full: Markus Freitag
    id: markus-freitag
    last: Freitag
  - first: Scott
    full: Scott Roy
    id: scott-roy
    last: Roy
  author_string: Markus Freitag, Scott Roy
  bibkey: freitag-roy-2018-unsupervised
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1426
  month: October-November
  page_first: '3922'
  page_last: '3929'
  pages: "3922\u20133929"
  paper_id: '426'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1426.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1426.jpg
  title: Unsupervised Natural Language Generation with Denoising Autoencoders
  title_html: Unsupervised Natural Language Generation with Denoising Autoencoders
  url: https://www.aclweb.org/anthology/D18-1426
  year: '2018'
D18-1427:
  abstract: 'In this paper, we focus on the problem of question generation (QG). Recent
    neural network-based approaches employ the sequence-to-sequence model which takes
    an answer and its context as input and generates a relevant question as output.
    However, we observe two major issues with these approaches: (1) The generated
    interrogative words (or question words) do not match the answer type. (2) The
    model copies the context words that are far from and irrelevant to the answer,
    instead of the words that are close and relevant to the answer. To address these
    two issues, we propose an answer-focused and position-aware neural question generation
    model. (1) By answer-focused, we mean that we explicitly model question word generation
    by incorporating the answer embedding, which can help generate an interrogative
    word matching the answer type. (2) By position-aware, we mean that we model the
    relative distance between the context words and the answer. Hence the model can
    be aware of the position of the context words when copying them to generate a
    question. We conduct extensive experiments to examine the effectiveness of our
    model. The experimental results show that our model significantly improves the
    baseline and outperforms the state-of-the-art system.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1427.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1427.Attachment.zip
  author:
  - first: Xingwu
    full: Xingwu Sun
    id: xingwu-sun
    last: Sun
  - first: Jing
    full: Jing Liu
    id: jing-liu
    last: Liu
  - first: Yajuan
    full: Yajuan Lyu
    id: yajuan-lyu
    last: Lyu
  - first: Wei
    full: Wei He
    id: wei-he
    last: He
  - first: Yanjun
    full: Yanjun Ma
    id: yanjun-ma
    last: Ma
  - first: Shi
    full: Shi Wang
    id: shi-wang
    last: Wang
  author_string: Xingwu Sun, Jing Liu, Yajuan Lyu, Wei He, Yanjun Ma, Shi Wang
  bibkey: sun-etal-2018-answer
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1427
  month: October-November
  page_first: '3930'
  page_last: '3939'
  pages: "3930\u20133939"
  paper_id: '427'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1427.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1427.jpg
  title: Answer-focused and Position-aware Neural Question Generation
  title_html: Answer-focused and Position-aware Neural Question Generation
  url: https://www.aclweb.org/anthology/D18-1427
  year: '2018'
D18-1428:
  abstract: "Existing text generation methods tend to produce repeated and \u201D\
    boring\u201D expressions. To tackle this problem, we propose a new text generation\
    \ model, called Diversity-Promoting Generative Adversarial Network (DP-GAN). The\
    \ proposed model assigns low reward for repeatedly generated text and high reward\
    \ for \u201Dnovel\u201D and fluent text, encouraging the generator to produce\
    \ diverse and informative text. Moreover, we propose a novel language-model based\
    \ discriminator, which can better distinguish novel text from repeated text without\
    \ the saturation problem compared with existing classifier-based discriminators.\
    \ The experimental results on review generation and dialogue generation tasks\
    \ demonstrate that our model can generate substantially more diverse and informative\
    \ text than existing baselines."
  address: Brussels, Belgium
  author:
  - first: Jingjing
    full: Jingjing Xu
    id: jingjing-xu
    last: Xu
  - first: Xuancheng
    full: Xuancheng Ren
    id: xuancheng-ren
    last: Ren
  - first: Junyang
    full: Junyang Lin
    id: junyang-lin
    last: Lin
  - first: Xu
    full: Xu Sun
    id: xu-sun
    last: Sun
  author_string: Jingjing Xu, Xuancheng Ren, Junyang Lin, Xu Sun
  bibkey: xu-etal-2018-diversity
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1428
  month: October-November
  page_first: '3940'
  page_last: '3949'
  pages: "3940\u20133949"
  paper_id: '428'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1428.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1428.jpg
  title: 'Diversity-Promoting GAN: A Cross-Entropy Based Generative Adversarial Network
    for Diversified Text Generation'
  title_html: 'Diversity-Promoting <span class="acl-fixed-case">GAN</span>: A Cross-Entropy
    Based Generative Adversarial Network for Diversified Text Generation'
  url: https://www.aclweb.org/anthology/D18-1428
  year: '2018'
D18-1429:
  abstract: There has always been criticism for using n-gram based similarity metrics,
    such as BLEU, NIST, -gram based similarity metrics, such as BLEU, NIST, etc, for
    evaluating the performance of NLG systems. However, these metrics continue to
    remain popular and are recently being used for evaluating the performance of systems
    which automatically generate questions from documents, knowledge graphs, images,
    etc. Given the rising interest in such automatic question generation (AQG) systems,
    it is important to objectively examine whether these metrics are suitable for
    this task. In particular, it is important to verify whether such metrics used
    for evaluating AQG systems focus on answerability of the generated question by
    preferring questions which contain all relevant information such as question type
    (Wh-types), entities, relations, etc. In this work, we show that current automatic
    evaluation metrics based on n-gram similarity do not always correlate well with
    human judgments about -gram similarity do not always correlate well with human
    judgments about answerability of a question. To alleviate this problem and as
    a first step towards better evaluation metrics for AQG, we introduce a scoring
    function to capture answerability and show that when this scoring function is
    integrated with existing metrics, they correlate significantly better with human
    judgments. The scripts and data developed as a part of this work are made publicly
    available.
  address: Brussels, Belgium
  author:
  - first: Preksha
    full: Preksha Nema
    id: preksha-nema
    last: Nema
  - first: Mitesh M.
    full: Mitesh M. Khapra
    id: mitesh-m-khapra
    last: Khapra
  author_string: Preksha Nema, Mitesh M. Khapra
  bibkey: nema-khapra-2018-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1429
  month: October-November
  page_first: '3950'
  page_last: '3959'
  pages: "3950\u20133959"
  paper_id: '429'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1429.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1429.jpg
  title: Towards a Better Metric for Evaluating Question Generation Systems
  title_html: Towards a Better Metric for Evaluating Question Generation Systems
  url: https://www.aclweb.org/anthology/D18-1429
  year: '2018'
D18-1430:
  abstract: The ability to write diverse poems in different styles under the same
    poetic imagery is an important characteristic of human poetry writing. Most previous
    works on automatic Chinese poetry generation focused on improving the coherency
    among lines. Some work explored style transfer but suffered from expensive expert
    labeling of poem styles. In this paper, we target on stylistic poetry generation
    in a fully unsupervised manner for the first time. We propose a novel model which
    requires no supervised style labeling by incorporating mutual information, a concept
    in information theory, into modeling. Experimental results show that our model
    is able to generate stylistic poems without losing fluency and coherency.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1430.Attachment.txt
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1430.Attachment.txt
  author:
  - first: Cheng
    full: Cheng Yang
    id: cheng-yang
    last: Yang
  - first: Maosong
    full: Maosong Sun
    id: maosong-sun
    last: Sun
  - first: Xiaoyuan
    full: Xiaoyuan Yi
    id: xiaoyuan-yi
    last: Yi
  - first: Wenhao
    full: Wenhao Li
    id: wenhao-li
    last: Li
  author_string: Cheng Yang, Maosong Sun, Xiaoyuan Yi, Wenhao Li
  bibkey: yang-etal-2018-stylistic
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1430
  month: October-November
  page_first: '3960'
  page_last: '3969'
  pages: "3960\u20133969"
  paper_id: '430'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1430.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1430.jpg
  title: Stylistic Chinese Poetry Generation via Unsupervised Style Disentanglement
  title_html: Stylistic <span class="acl-fixed-case">C</span>hinese Poetry Generation
    via Unsupervised Style Disentanglement
  url: https://www.aclweb.org/anthology/D18-1430
  year: '2018'
D18-1431:
  abstract: Neural conversation models tend to generate safe, generic responses for
    most inputs. This is due to the limitations of likelihood-based decoding objectives
    in generation tasks with diverse outputs, such as conversation. To address this
    challenge, we propose a simple yet effective approach for incorporating side information
    in the form of distributional constraints over the generated responses. We propose
    two constraints that help generate more content rich responses that are based
    on a model of syntax and topics (Griffiths et al., 2005) and semantic similarity
    (Arora et al., 2016). We evaluate our approach against a variety of competitive
    baselines, using both automatic metrics and human judgments, showing that our
    proposed approach generates responses that are much less generic without sacrificing
    plausibility. A working demo of our code can be found at https://github.com/abaheti95/DC-NeuralConversation.
  address: Brussels, Belgium
  author:
  - first: Ashutosh
    full: Ashutosh Baheti
    id: ashutosh-baheti
    last: Baheti
  - first: Alan
    full: Alan Ritter
    id: alan-ritter
    last: Ritter
  - first: Jiwei
    full: Jiwei Li
    id: jiwei-li
    last: Li
  - first: Bill
    full: Bill Dolan
    id: bill-dolan
    last: Dolan
  author_string: Ashutosh Baheti, Alan Ritter, Jiwei Li, Bill Dolan
  bibkey: baheti-etal-2018-generating
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1431
  month: October-November
  page_first: '3970'
  page_last: '3980'
  pages: "3970\u20133980"
  paper_id: '431'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1431.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1431.jpg
  title: Generating More Interesting Responses in Neural Conversation Models with
    Distributional Constraints
  title_html: Generating More Interesting Responses in Neural Conversation Models
    with Distributional Constraints
  url: https://www.aclweb.org/anthology/D18-1431
  year: '2018'
D18-1432:
  abstract: 'We present three enhancements to existing encoder-decoder models for
    open-domain conversational agents, aimed at effectively modeling coherence and
    promoting output diversity: (1) We introduce a measure of coherence as the GloVe
    embedding similarity between the dialogue context and the generated response,
    (2) we filter our training corpora based on the measure of coherence to obtain
    topically coherent and lexically diverse context-response pairs, (3) we then train
    a response generator using a conditional variational autoencoder model that incorporates
    the measure of coherence as a latent variable and uses a context gate to guarantee
    topical consistency with the context and promote lexical diversity. Experiments
    on the OpenSubtitles corpus show a substantial improvement over competitive neural
    models in terms of BLEU score as well as metrics of coherence and diversity.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1432.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1432.Attachment.pdf
  author:
  - first: Xinnuo
    full: Xinnuo Xu
    id: xinnuo-xu
    last: Xu
  - first: "Ond\u0159ej"
    full: "Ond\u0159ej Du\u0161ek"
    id: ondrej-dusek
    last: "Du\u0161ek"
  - first: Ioannis
    full: Ioannis Konstas
    id: ioannis-konstas
    last: Konstas
  - first: Verena
    full: Verena Rieser
    id: verena-rieser
    last: Rieser
  author_string: "Xinnuo Xu, Ond\u0159ej Du\u0161ek, Ioannis Konstas, Verena Rieser"
  bibkey: xu-etal-2018-better
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1432
  month: October-November
  page_first: '3981'
  page_last: '3991'
  pages: "3981\u20133991"
  paper_id: '432'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1432.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1432.jpg
  title: Better Conversations by Modeling, Filtering, and Optimizing for Coherence
    and Diversity
  title_html: Better Conversations by Modeling, Filtering, and Optimizing for Coherence
    and Diversity
  url: https://www.aclweb.org/anthology/D18-1432
  year: '2018'
D18-1433:
  abstract: "Most previous efforts toward video captioning focus on generating generic\
    \ descriptions, such as, \u201CA man is talking.\u201D We collect a news video\
    \ dataset to generate enriched descriptions that include important background\
    \ knowledge, such as named entities and related events, which allows the user\
    \ to fully understand the video content. We develop an approach that uses video\
    \ meta-data to retrieve topically related news documents for a video and extracts\
    \ the events and named entities from these documents. Then, given the video as\
    \ well as the extracted events and entities, we generate a description using a\
    \ Knowledge-aware Video Description network. The model learns to incorporate entities\
    \ found in the topically related documents into the description via an entity\
    \ pointer network and the generation procedure is guided by the event and entity\
    \ types from the topically related documents through a knowledge gate, which is\
    \ a gating mechanism added to the model\u2019s decoder that takes a one-hot vector\
    \ of these types. We evaluate our approach on the new dataset of news videos we\
    \ have collected, establishing the first benchmark for this dataset as well as\
    \ proposing a new metric to evaluate these descriptions."
  address: Brussels, Belgium
  author:
  - first: Spencer
    full: Spencer Whitehead
    id: spencer-whitehead
    last: Whitehead
  - first: Heng
    full: Heng Ji
    id: heng-ji
    last: Ji
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  - first: Shih-Fu
    full: Shih-Fu Chang
    id: shih-fu-chang
    last: Chang
  - first: Clare
    full: Clare Voss
    id: clare-voss
    last: Voss
  author_string: Spencer Whitehead, Heng Ji, Mohit Bansal, Shih-Fu Chang, Clare Voss
  bibkey: whitehead-etal-2018-incorporating
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1433
  month: October-November
  page_first: '3992'
  page_last: '4001'
  pages: "3992\u20134001"
  paper_id: '433'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1433.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1433.jpg
  title: Incorporating Background Knowledge into Video Description Generation
  title_html: Incorporating Background Knowledge into Video Description Generation
  url: https://www.aclweb.org/anthology/D18-1433
  year: '2018'
D18-1434:
  abstract: Generating natural questions from an image is a semantic task that requires
    using visual and language modality to learn multimodal representations. Images
    can have multiple visual and language contexts that are relevant for generating
    questions namely places, captions, and tags. In this paper, we propose the use
    of exemplars for obtaining the relevant context. We obtain this by using a Multimodal
    Differential Network to produce natural and engaging questions. The generated
    questions show a remarkable similarity to the natural questions as validated by
    a human study. Further, we observe that the proposed approach substantially improves
    over state-of-the-art benchmarks on the quantitative metrics (BLEU, METEOR, ROUGE,
    and CIDEr).
  address: Brussels, Belgium
  attachment:
  - filename: D18-1434.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1434.Attachment.zip
  author:
  - first: Badri Narayana
    full: Badri Narayana Patro
    id: badri-narayana-patro
    last: Patro
  - first: Sandeep
    full: Sandeep Kumar
    id: sandeep-kumar
    last: Kumar
  - first: Vinod Kumar
    full: Vinod Kumar Kurmi
    id: vinod-kumar-kurmi
    last: Kurmi
  - first: Vinay
    full: Vinay Namboodiri
    id: vinay-namboodiri
    last: Namboodiri
  author_string: Badri Narayana Patro, Sandeep Kumar, Vinod Kumar Kurmi, Vinay Namboodiri
  bibkey: patro-etal-2018-multimodal
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1434
  month: October-November
  page_first: '4002'
  page_last: '4012'
  pages: "4002\u20134012"
  paper_id: '434'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1434.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1434.jpg
  title: Multimodal Differential Network for Visual Question Generation
  title_html: Multimodal Differential Network for Visual Question Generation
  url: https://www.aclweb.org/anthology/D18-1434
  year: '2018'
D18-1435:
  abstract: Current image captioning approaches generate descriptions which lack specific
    information, such as named entities that are involved in the images. In this paper
    we propose a new task which aims to generate informative image captions, given
    images and hashtags as input. We propose a simple but effective approach to tackle
    this problem. We first train a convolutional neural networks - long short term
    memory networks (CNN-LSTM) model to generate a template caption based on the input
    image. Then we use a knowledge graph based collective inference algorithm to fill
    in the template with specific named entities retrieved via the hashtags. Experiments
    on a new benchmark dataset collected from Flickr show that our model generates
    news-style image descriptions with much richer information. Our model outperforms
    unimodal baselines significantly with various evaluation metrics.
  address: Brussels, Belgium
  author:
  - first: Di
    full: Di Lu
    id: di-lu
    last: Lu
  - first: Spencer
    full: Spencer Whitehead
    id: spencer-whitehead
    last: Whitehead
  - first: Lifu
    full: Lifu Huang
    id: lifu-huang
    last: Huang
  - first: Heng
    full: Heng Ji
    id: heng-ji
    last: Ji
  - first: Shih-Fu
    full: Shih-Fu Chang
    id: shih-fu-chang
    last: Chang
  author_string: Di Lu, Spencer Whitehead, Lifu Huang, Heng Ji, Shih-Fu Chang
  bibkey: lu-etal-2018-entity
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1435
  month: October-November
  page_first: '4013'
  page_last: '4023'
  pages: "4013\u20134023"
  paper_id: '435'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1435.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1435.jpg
  title: Entity-aware Image Caption Generation
  title_html: Entity-aware Image Caption Generation
  url: https://www.aclweb.org/anthology/D18-1435
  year: '2018'
D18-1436:
  abstract: In this paper, we introduce the task of automatically generating text
    to describe the differences between two similar images. We collect a new dataset
    by crowd-sourcing difference descriptions for pairs of image frames extracted
    from video-surveillance footage. Annotators were asked to succinctly describe
    all the differences in a short paragraph. As a result, our novel dataset provides
    an opportunity to explore models that align language and vision, and capture visual
    salience. The dataset may also be a useful benchmark for coherent multi-sentence
    generation. We perform a first-pass visual analysis that exposes clusters of differing
    pixels as a proxy for object-level differences. We propose a model that captures
    visual salience by using a latent variable to align clusters of differing pixels
    with output sentences. We find that, for both single-sentence generation and as
    well as multi-sentence generation, the proposed model outperforms the models that
    use attention alone.
  address: Brussels, Belgium
  author:
  - first: Harsh
    full: Harsh Jhamtani
    id: harsh-jhamtani
    last: Jhamtani
  - first: Taylor
    full: Taylor Berg-Kirkpatrick
    id: taylor-berg-kirkpatrick
    last: Berg-Kirkpatrick
  author_string: Harsh Jhamtani, Taylor Berg-Kirkpatrick
  bibkey: jhamtani-berg-kirkpatrick-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1436
  month: October-November
  page_first: '4024'
  page_last: '4034'
  pages: "4024\u20134034"
  paper_id: '436'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1436.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1436.jpg
  title: Learning to Describe Differences Between Pairs of Similar Images
  title_html: Learning to Describe Differences Between Pairs of Similar Images
  url: https://www.aclweb.org/anthology/D18-1436
  year: '2018'
D18-1437:
  abstract: "Despite continuously improving performance, contemporary image captioning\
    \ models are prone to \u201Challucinating\u201D objects that are not actually\
    \ in a scene. One problem is that standard metrics only measure similarity to\
    \ ground truth captions and may not fully capture image relevance. In this work,\
    \ we propose a new image relevance metric to evaluate current models with veridical\
    \ visual labels and assess their rate of object hallucination. We analyze how\
    \ captioning model architectures and learning objectives contribute to object\
    \ hallucination, explore when hallucination is likely due to image misclassification\
    \ or language priors, and assess how well current sentence metrics capture object\
    \ hallucination. We investigate these questions on the standard image captioning\
    \ benchmark, MSCOCO, using a diverse set of models. Our analysis yields several\
    \ interesting findings, including that models which score best on standard sentence\
    \ metrics do not always have lower hallucination and that models which hallucinate\
    \ more tend to make errors driven by language priors."
  address: Brussels, Belgium
  author:
  - first: Anna
    full: Anna Rohrbach
    id: anna-rohrbach
    last: Rohrbach
  - first: Lisa Anne
    full: Lisa Anne Hendricks
    id: lisa-anne-hendricks
    last: Hendricks
  - first: Kaylee
    full: Kaylee Burns
    id: kaylee-burns
    last: Burns
  - first: Trevor
    full: Trevor Darrell
    id: trevor-darrell
    last: Darrell
  - first: Kate
    full: Kate Saenko
    id: kate-saenko
    last: Saenko
  author_string: Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell,
    Kate Saenko
  bibkey: rohrbach-etal-2018-object
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1437
  month: October-November
  page_first: '4035'
  page_last: '4045'
  pages: "4035\u20134045"
  paper_id: '437'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1437.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1437.jpg
  title: Object Hallucination in Image Captioning
  title_html: Object Hallucination in Image Captioning
  url: https://www.aclweb.org/anthology/D18-1437
  year: '2018'
D18-1438:
  abstract: Rapid growth of multi-modal documents on the Internet makes multi-modal
    summarization research necessary. Most previous research summarizes texts or images
    separately. Recent neural summarization research shows the strength of the Encoder-Decoder
    model in text summarization. This paper proposes an abstractive text-image summarization
    model using the attentional hierarchical Encoder-Decoder model to summarize a
    text document and its accompanying images simultaneously, and then to align the
    sentences and images in summaries. A multi-modal attentional mechanism is proposed
    to attend original sentences, images, and captions when decoding. The DailyMail
    dataset is extended by collecting images and captions from the Web. Experiments
    show our model outperforms the neural abstractive and extractive text summarization
    methods that do not consider images. In addition, our model can generate informative
    summaries of images.
  address: Brussels, Belgium
  author:
  - first: Jingqiang
    full: Jingqiang Chen
    id: jingqiang-chen
    last: Chen
  - first: Hai
    full: Hai Zhuge
    id: hai-zhuge
    last: Zhuge
  author_string: Jingqiang Chen, Hai Zhuge
  bibkey: chen-zhuge-2018-abstractive
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1438
  month: October-November
  page_first: '4046'
  page_last: '4056'
  pages: "4046\u20134056"
  paper_id: '438'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1438.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1438.jpg
  title: Abstractive Text-Image Summarization Using Multi-Modal Attentional Hierarchical
    RNN
  title_html: Abstractive Text-Image Summarization Using Multi-Modal Attentional Hierarchical
    <span class="acl-fixed-case">RNN</span>
  url: https://www.aclweb.org/anthology/D18-1438
  year: '2018'
D18-1439:
  abstract: In this paper, we study automatic keyphrase generation. Although conventional
    approaches to this task show promising results, they neglect correlation among
    keyphrases, resulting in duplication and coverage issues. To solve these problems,
    we propose a new sequence-to-sequence architecture for keyphrase generation named
    CorrRNN, which captures correlation among multiple keyphrases in two ways. First,
    we employ a coverage vector to indicate whether the word in the source document
    has been summarized by previous phrases to improve the coverage for keyphrases.
    Second, preceding phrases are taken into account to eliminate duplicate phrases
    and improve result coherence. Experiment results show that our model significantly
    outperforms the state-of-the-art method on benchmark datasets in terms of both
    accuracy and diversity.
  address: Brussels, Belgium
  author:
  - first: Jun
    full: Jun Chen
    id: jun-chen
    last: Chen
  - first: Xiaoming
    full: Xiaoming Zhang
    id: xiaoming-zhang
    last: Zhang
  - first: Yu
    full: Yu Wu
    id: yu-wu
    last: Wu
  - first: Zhao
    full: Zhao Yan
    id: zhao-yan
    last: Yan
  - first: Zhoujun
    full: Zhoujun Li
    id: zhoujun-li
    last: Li
  author_string: Jun Chen, Xiaoming Zhang, Yu Wu, Zhao Yan, Zhoujun Li
  bibkey: chen-etal-2018-keyphrase
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1439
  month: October-November
  page_first: '4057'
  page_last: '4066'
  pages: "4057\u20134066"
  paper_id: '439'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1439.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1439.jpg
  title: Keyphrase Generation with Correlation Constraints
  title_html: Keyphrase Generation with Correlation Constraints
  url: https://www.aclweb.org/anthology/D18-1439
  year: '2018'
D18-1440:
  abstract: "A good neural sequence-to-sequence summarization model should have a\
    \ strong encoder that can distill and memorize the important information from\
    \ long input texts so that the decoder can generate salient summaries based on\
    \ the encoder\u2019s memory. In this paper, we aim to improve the memorization\
    \ capabilities of the encoder of a pointer-generator model by adding an additional\
    \ \u2018closed-book\u2019 decoder without attention and pointer mechanisms. Such\
    \ a decoder forces the encoder to be more selective in the information encoded\
    \ in its memory state because the decoder can\u2019t rely on the extra information\
    \ provided by the attention and possibly copy modules, and hence improves the\
    \ entire model. On the CNN/Daily Mail dataset, our 2-decoder model outperforms\
    \ the baseline significantly in terms of ROUGE and METEOR metrics, for both cross-entropy\
    \ and reinforced setups (and on human evaluation). Moreover, our model also achieves\
    \ higher scores in a test-only DUC-2002 generalizability setup. We further present\
    \ a memory ability test, two saliency metrics, as well as several sanity-check\
    \ ablations (based on fixed-encoder, gradient-flow cut, and model capacity) to\
    \ prove that the encoder of our 2-decoder model does in fact learn stronger memory\
    \ representations than the baseline encoder."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1440.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1440.Attachment.pdf
  author:
  - first: Yichen
    full: Yichen Jiang
    id: yichen-jiang
    last: Jiang
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  author_string: Yichen Jiang, Mohit Bansal
  bibkey: jiang-bansal-2018-closed
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1440
  month: October-November
  page_first: '4067'
  page_last: '4077'
  pages: "4067\u20134077"
  paper_id: '440'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1440.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1440.jpg
  title: Closed-Book Training to Improve Summarization Encoder Memory
  title_html: Closed-Book Training to Improve Summarization Encoder Memory
  url: https://www.aclweb.org/anthology/D18-1440
  year: '2018'
D18-1441:
  abstract: Recent neural sequence-to-sequence models have shown significant progress
    on short text summarization. However, for document summarization, they fail to
    capture the long-term structure of both documents and multi-sentence summaries,
    resulting in information loss and repetitions. In this paper, we propose to leverage
    the structural information of both documents and multi-sentence summaries to improve
    the document summarization performance. Specifically, we import both structural-compression
    and structural-coverage regularization into the summarization process in order
    to capture the information compression and information coverage properties, which
    are the two most important structural properties of document summarization. Experimental
    results demonstrate that the structural regularization improves the document summarization
    performance significantly, which enables our model to generate more informative
    and concise summaries, and thus significantly outperforms state-of-the-art neural
    abstractive methods.
  address: Brussels, Belgium
  author:
  - first: Wei
    full: Wei Li
    id: wei-li
    last: Li
  - first: Xinyan
    full: Xinyan Xiao
    id: xinyan-xiao
    last: Xiao
  - first: Yajuan
    full: Yajuan Lyu
    id: yajuan-lyu
    last: Lyu
  - first: Yuanzhuo
    full: Yuanzhuo Wang
    id: yuanzhuo-wang
    last: Wang
  author_string: Wei Li, Xinyan Xiao, Yajuan Lyu, Yuanzhuo Wang
  bibkey: li-etal-2018-improving-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1441
  month: October-November
  page_first: '4078'
  page_last: '4087'
  pages: "4078\u20134087"
  paper_id: '441'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1441.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1441.jpg
  title: Improving Neural Abstractive Document Summarization with Structural Regularization
  title_html: Improving Neural Abstractive Document Summarization with Structural
    Regularization
  url: https://www.aclweb.org/anthology/D18-1441
  year: '2018'
D18-1442:
  abstract: In this paper, we introduce Iterative Text Summarization (ITS), an iteration-based
    model for supervised extractive text summarization, inspired by the observation
    that it is often necessary for a human to read an article multiple times in order
    to fully understand and summarize its contents. Current summarization approaches
    read through a document only once to generate a document representation, resulting
    in a sub-optimal representation. To address this issue we introduce a model which
    iteratively polishes the document representation on many passes through the document.
    As part of our model, we also introduce a selective reading mechanism that decides
    more accurately the extent to which each sentence in the model should be updated.
    Experimental results on the CNN/DailyMail and DUC2002 datasets demonstrate that
    our model significantly outperforms state-of-the-art extractive systems when evaluated
    by machines and by humans.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1442.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1442.Attachment.pdf
  author:
  - first: Xiuying
    full: Xiuying Chen
    id: xiuying-chen
    last: Chen
  - first: Shen
    full: Shen Gao
    id: shen-gao
    last: Gao
  - first: Chongyang
    full: Chongyang Tao
    id: chongyang-tao
    last: Tao
  - first: Yan
    full: Yan Song
    id: yan-song
    last: Song
  - first: Dongyan
    full: Dongyan Zhao
    id: dongyan-zhao
    last: Zhao
  - first: Rui
    full: Rui Yan
    id: rui-yan
    last: Yan
  author_string: Xiuying Chen, Shen Gao, Chongyang Tao, Yan Song, Dongyan Zhao, Rui
    Yan
  bibkey: chen-etal-2018-iterative
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1442
  month: October-November
  page_first: '4088'
  page_last: '4097'
  pages: "4088\u20134097"
  paper_id: '442'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1442.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1442.jpg
  title: Iterative Document Representation Learning Towards Summarization with Polishing
  title_html: Iterative Document Representation Learning Towards Summarization with
    Polishing
  url: https://www.aclweb.org/anthology/D18-1442
  year: '2018'
D18-1443:
  abstract: Neural summarization produces outputs that are fluent and readable, but
    which can be poor at content selection, for instance often copying full sentences
    from the source document. This work explores the use of data-efficient content
    selectors to over-determine phrases in a source document that should be part of
    the summary. We use this selector as a bottom-up attention step to constrain the
    model to likely phrases. We show that this approach improves the ability to compress
    text, while still generating fluent summaries. This two-step process is both simpler
    and higher performing than other end-to-end content selection models, leading
    to significant improvements on ROUGE for both the CNN-DM and NYT corpus. Furthermore,
    the content selector can be trained with as little as 1,000 sentences making it
    easy to transfer a trained summarizer to a new domain.
  address: Brussels, Belgium
  author:
  - first: Sebastian
    full: Sebastian Gehrmann
    id: sebastian-gehrmann
    last: Gehrmann
  - first: Yuntian
    full: Yuntian Deng
    id: yuntian-deng
    last: Deng
  - first: Alexander
    full: Alexander Rush
    id: alexander-m-rush
    last: Rush
  author_string: Sebastian Gehrmann, Yuntian Deng, Alexander Rush
  bibkey: gehrmann-etal-2018-bottom
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1443
  month: October-November
  page_first: '4098'
  page_last: '4109'
  pages: "4098\u20134109"
  paper_id: '443'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1443.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1443.jpg
  title: Bottom-Up Abstractive Summarization
  title_html: Bottom-Up Abstractive Summarization
  url: https://www.aclweb.org/anthology/D18-1443
  year: '2018'
D18-1444:
  abstract: Convolutional neural networks (CNNs) have met great success in abstractive
    summarization, but they cannot effectively generate summaries of desired lengths.
    Because generated summaries are used in difference scenarios which may have space
    or length constraints, the ability to control the summary length in abstractive
    summarization is an important problem. In this paper, we propose an approach to
    constrain the summary length by extending a convolutional sequence to sequence
    model. The results show that this approach generates high-quality summaries with
    user defined length, and outperforms the baselines consistently in terms of ROUGE
    score, length variations and semantic similarity.
  address: Brussels, Belgium
  author:
  - first: Yizhu
    full: Yizhu Liu
    id: yizhu-liu
    last: Liu
  - first: Zhiyi
    full: Zhiyi Luo
    id: zhiyi-luo
    last: Luo
  - first: Kenny
    full: Kenny Zhu
    id: kenny-zhu
    last: Zhu
  author_string: Yizhu Liu, Zhiyi Luo, Kenny Zhu
  bibkey: liu-etal-2018-controlling
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1444
  month: October-November
  page_first: '4110'
  page_last: '4119'
  pages: "4110\u20134119"
  paper_id: '444'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1444.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1444.jpg
  title: Controlling Length in Abstractive Summarization Using a Convolutional Neural
    Network
  title_html: Controlling Length in Abstractive Summarization Using a Convolutional
    Neural Network
  url: https://www.aclweb.org/anthology/D18-1444
  year: '2018'
D18-1445:
  abstract: "We propose a method to perform automatic document summarisation without\
    \ using reference summaries. Instead, our method interactively learns from users\u2019\
    \ preferences. The merit of preference-based interactive summarisation is that\
    \ preferences are easier for users to provide than reference summaries. Existing\
    \ preference-based interactive learning methods suffer from high sample complexity,\
    \ i.e. they need to interact with the oracle for many rounds in order to converge.\
    \ In this work, we propose a new objective function, which enables us to leverage\
    \ active learning, preference learning and reinforcement learning techniques in\
    \ order to reduce the sample complexity. Both simulation and real-user experiments\
    \ suggest that our method significantly advances the state of the art. Our source\
    \ code is freely available at https://github.com/UKPLab/emnlp2018-april."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1445.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1445.Attachment.zip
  author:
  - first: Yang
    full: Yang Gao
    id: yang-gao
    last: Gao
  - first: Christian M.
    full: Christian M. Meyer
    id: christian-m-meyer
    last: Meyer
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Yang Gao, Christian M. Meyer, Iryna Gurevych
  bibkey: gao-etal-2018-april
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1445
  month: October-November
  page_first: '4120'
  page_last: '4130'
  pages: "4120\u20134130"
  paper_id: '445'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1445.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1445.jpg
  title: 'APRIL: Interactively Learning to Summarise by Combining Active Preference
    Learning and Reinforcement Learning'
  title_html: '<span class="acl-fixed-case">APRIL</span>: Interactively Learning to
    Summarise by Combining Active Preference Learning and Reinforcement Learning'
  url: https://www.aclweb.org/anthology/D18-1445
  year: '2018'
D18-1446:
  abstract: Generating a text abstract from a set of documents remains a challenging
    task. The neural encoder-decoder framework has recently been exploited to summarize
    single documents, but its success can in part be attributed to the availability
    of large parallel data automatically acquired from the Web. In contrast, parallel
    data for multi-document summarization are scarce and costly to obtain. There is
    a pressing need to adapt an encoder-decoder model trained on single-document summarization
    data to work with multiple-document input. In this paper, we present an initial
    investigation into a novel adaptation method. It exploits the maximal marginal
    relevance method to select representative sentences from multi-document input,
    and leverages an abstractive encoder-decoder model to fuse disparate sentences
    to an abstractive summary. The adaptation method is robust and itself requires
    no training data. Our system compares favorably to state-of-the-art extractive
    and abstractive approaches judged by automatic metrics and human assessors.
  address: Brussels, Belgium
  author:
  - first: Logan
    full: Logan Lebanoff
    id: logan-lebanoff
    last: Lebanoff
  - first: Kaiqiang
    full: Kaiqiang Song
    id: kaiqiang-song
    last: Song
  - first: Fei
    full: Fei Liu
    id: fei-liu-utdallas
    last: Liu
  author_string: Logan Lebanoff, Kaiqiang Song, Fei Liu
  bibkey: lebanoff-etal-2018-adapting
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1446
  month: October-November
  page_first: '4131'
  page_last: '4141'
  pages: "4131\u20134141"
  paper_id: '446'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1446.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1446.jpg
  title: Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document
    Summarization
  title_html: Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document
    Summarization
  url: https://www.aclweb.org/anthology/D18-1446
  year: '2018'
D18-1447:
  abstract: We study the problem of generating keyphrases that summarize the key points
    for a given document. While sequence-to-sequence (seq2seq) models have achieved
    remarkable performance on this task (Meng et al., 2017), model training often
    relies on large amounts of labeled data, which is only applicable to resource-rich
    domains. In this paper, we propose semi-supervised keyphrase generation methods
    by leveraging both labeled data and large-scale unlabeled samples for learning.
    Two strategies are proposed. First, unlabeled documents are first tagged with
    synthetic keyphrases obtained from unsupervised keyphrase extraction methods or
    a self-learning algorithm, and then combined with labeled samples for training.
    Furthermore, we investigate a multi-task learning framework to jointly learn to
    generate keyphrases as well as the titles of the articles. Experimental results
    show that our semi-supervised learning-based methods outperform a state-of-the-art
    model trained with labeled data only.
  address: Brussels, Belgium
  author:
  - first: Hai
    full: Hai Ye
    id: hai-ye
    last: Ye
  - first: Lu
    full: Lu Wang
    id: lu-wang
    last: Wang
  author_string: Hai Ye, Lu Wang
  bibkey: ye-wang-2018-semi
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1447
  month: October-November
  page_first: '4142'
  page_last: '4153'
  pages: "4142\u20134153"
  paper_id: '447'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1447.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1447.jpg
  title: Semi-Supervised Learning for Neural Keyphrase Generation
  title_html: Semi-Supervised Learning for Neural Keyphrase Generation
  url: https://www.aclweb.org/anthology/D18-1447
  year: '2018'
D18-1448:
  abstract: Multimodal summarization has drawn much attention due to the rapid growth
    of multimedia data. The output of the current multimodal summarization systems
    is usually represented in texts. However, we have found through experiments that
    multimodal output can significantly improve user satisfaction for informativeness
    of summaries. In this paper, we propose a novel task, multimodal summarization
    with multimodal output (MSMO). To handle this task, we first collect a large-scale
    dataset for MSMO research. We then propose a multimodal attention model to jointly
    generate text and select the most relevant image from the multimodal input. Finally,
    to evaluate multimodal outputs, we construct a novel multimodal automatic evaluation
    (MMAE) method which considers both intra-modality salience and inter-modality
    relevance. The experimental results show the effectiveness of MMAE.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1448.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1448.Attachment.zip
  author:
  - first: Junnan
    full: Junnan Zhu
    id: junnan-zhu
    last: Zhu
  - first: Haoran
    full: Haoran Li
    id: haoran-li
    last: Li
  - first: Tianshang
    full: Tianshang Liu
    id: tianshang-liu
    last: Liu
  - first: Yu
    full: Yu Zhou
    id: yu-zhou
    last: Zhou
  - first: Jiajun
    full: Jiajun Zhang
    id: jiajun-zhang
    last: Zhang
  - first: Chengqing
    full: Chengqing Zong
    id: chengqing-zong
    last: Zong
  author_string: Junnan Zhu, Haoran Li, Tianshang Liu, Yu Zhou, Jiajun Zhang, Chengqing
    Zong
  bibkey: zhu-etal-2018-msmo
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1448
  month: October-November
  page_first: '4154'
  page_last: '4164'
  pages: "4154\u20134164"
  paper_id: '448'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1448.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1448.jpg
  title: 'MSMO: Multimodal Summarization with Multimodal Output'
  title_html: '<span class="acl-fixed-case">MSMO</span>: Multimodal Summarization
    with Multimodal Output'
  url: https://www.aclweb.org/anthology/D18-1448
  year: '2018'
D18-1449:
  abstract: Ensemble methods, which combine multiple models at decoding time, are
    now widely known to be effective for text-generation tasks. However, they generally
    increase computational costs, and thus, there have been many studies on compressing
    or distilling ensemble models. In this paper, we propose an alternative, simple
    but effective unsupervised ensemble method, post-ensemble, that combines multiple
    models by selecting a majority-like output in post-processing. We theoretically
    prove that our method is closely related to kernel density estimation based on
    the von Mises-Fisher kernel. Experimental results on a news-headline-generation
    task show that the proposed method performs better than the current ensemble methods.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1449.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1449.Attachment.pdf
  - filename: D18-1449.Poster.pdf
    type: poster
    url: https://www.aclweb.org/anthology/attachments/D18-1449.Poster.pdf
  author:
  - first: Hayato
    full: Hayato Kobayashi
    id: hayato-kobayashi
    last: Kobayashi
  author_string: Hayato Kobayashi
  bibkey: kobayashi-2018-frustratingly
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1449
  month: October-November
  page_first: '4165'
  page_last: '4176'
  pages: "4165\u20134176"
  paper_id: '449'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1449.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1449.jpg
  title: Frustratingly Easy Model Ensemble for Abstractive Summarization
  title_html: Frustratingly Easy Model Ensemble for Abstractive Summarization
  url: https://www.aclweb.org/anthology/D18-1449
  year: '2018'
D18-1450:
  abstract: This paper tackles automation of the pyramid method, a reliable manual
    evaluation framework. To construct a pyramid, we transform human-made reference
    summaries into extractive reference summaries that consist of Elementary Discourse
    Units (EDUs) obtained from source documents and then weight every EDU by counting
    the number of extractive reference summaries that contain the EDU. A summary is
    scored by the correspondences between EDUs in the summary and those in the pyramid.
    Experiments on DUC and TAC data sets show that our methods strongly correlate
    with various manual evaluations.
  address: Brussels, Belgium
  author:
  - first: Tsutomu
    full: Tsutomu Hirao
    id: tsutomu-hirao
    last: Hirao
  - first: Hidetaka
    full: Hidetaka Kamigaito
    id: hidetaka-kamigaito
    last: Kamigaito
  - first: Masaaki
    full: Masaaki Nagata
    id: masaaki-nagata
    last: Nagata
  author_string: Tsutomu Hirao, Hidetaka Kamigaito, Masaaki Nagata
  bibkey: hirao-etal-2018-automatic
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1450
  month: October-November
  page_first: '4177'
  page_last: '4186'
  pages: "4177\u20134186"
  paper_id: '450'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1450.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1450.jpg
  title: Automatic Pyramid Evaluation Exploiting EDU-based Extractive Reference Summaries
  title_html: Automatic Pyramid Evaluation Exploiting <span class="acl-fixed-case">EDU</span>-based
    Extractive Reference Summaries
  url: https://www.aclweb.org/anthology/D18-1450
  year: '2018'
D18-1451:
  abstract: Auto-encoders compress input data into a latent-space representation and
    reconstruct the original data from the representation. This latent representation
    is not easily interpreted by humans. In this paper, we propose training an auto-encoder
    that encodes input text into human-readable sentences, and unpaired abstractive
    summarization is thereby achieved. The auto-encoder is composed of a generator
    and a reconstructor. The generator encodes the input text into a shorter word
    sequence, and the reconstructor recovers the generator input from the generator
    output. To make the generator output human-readable, a discriminator restricts
    the output of the generator to resemble human-written sentences. By taking the
    generator output as the summary of the input text, abstractive summarization is
    achieved without document-summary pairs as training data. Promising results are
    shown on both English and Chinese corpora.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1451.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1451.Attachment.pdf
  author:
  - first: Yaushian
    full: Yaushian Wang
    id: yaushian-wang
    last: Wang
  - first: Hung-Yi
    full: Hung-Yi Lee
    id: hung-yi-lee
    last: Lee
  author_string: Yaushian Wang, Hung-Yi Lee
  bibkey: wang-lee-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1451
  month: October-November
  page_first: '4187'
  page_last: '4195'
  pages: "4187\u20134195"
  paper_id: '451'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1451.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1451.jpg
  title: Learning to Encode Text as Human-Readable Summaries using Generative Adversarial
    Networks
  title_html: Learning to Encode Text as Human-Readable Summaries using Generative
    Adversarial Networks
  url: https://www.aclweb.org/anthology/D18-1451
  year: '2018'
D18-1452:
  abstract: 'We address jointly two important tasks for Question Answering in community
    forums: given a new question, (i) find related existing questions, and (ii) find
    relevant answers to this new question. We further use an auxiliary task to complement
    the previous two, i.e., (iii) find good answers with respect to the thread question
    in a question-comment thread. We use deep neural networks (DNNs) to learn meaningful
    task-specific embeddings, which we then incorporate into a conditional random
    field (CRF) model for the multitask setting, performing joint learning over a
    complex graph structure. While DNNs alone achieve competitive results when trained
    to produce the embeddings, the CRF, which makes use of the embeddings and the
    dependencies between the tasks, improves the results significantly and consistently
    across a variety of evaluation metrics, thus showing the complementarity of DNNs
    and structured learning.'
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306149753
    type: video
    url: https://vimeo.com/306149753
  author:
  - first: Shafiq
    full: Shafiq Joty
    id: shafiq-joty
    last: Joty
  - first: "Llu\xEDs"
    full: "Llu\xEDs M\xE0rquez"
    id: lluis-marquez
    last: "M\xE0rquez"
  - first: Preslav
    full: Preslav Nakov
    id: preslav-nakov
    last: Nakov
  author_string: "Shafiq Joty, Llu\xEDs M\xE0rquez, Preslav Nakov"
  bibkey: joty-etal-2018-joint
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1452
  month: October-November
  page_first: '4196'
  page_last: '4207'
  pages: "4196\u20134207"
  paper_id: '452'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1452.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1452.jpg
  title: Joint Multitask Learning for Community Question Answering Using Task-Specific
    Embeddings
  title_html: Joint Multitask Learning for Community Question Answering Using Task-Specific
    Embeddings
  url: https://www.aclweb.org/anthology/D18-1452
  year: '2018'
D18-1453:
  abstract: A challenge in creating a dataset for machine reading comprehension (MRC)
    is to collect questions that require a sophisticated understanding of language
    to answer beyond using superficial cues. In this work, we investigate what makes
    questions easier across recent 12 MRC datasets with three question styles (answer
    extraction, description, and multiple choice). We propose to employ simple heuristics
    to split each dataset into easy and hard subsets and examine the performance of
    two baseline models for each of the subsets. We then manually annotate questions
    sampled from each subset with both validity and requisite reasoning skills to
    investigate which skills explain the difference between easy and hard questions.
    From this study, we observed that (i) the baseline performances for the hard subsets
    remarkably degrade compared to those of entire datasets, (ii) hard questions require
    knowledge inference and multiple-sentence reasoning in comparison with easy questions,
    and (iii) multiple-choice questions tend to require a broader range of reasoning
    skills than answer extraction and description questions. These results suggest
    that one might overestimate recent advances in MRC.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306150555
    type: video
    url: https://vimeo.com/306150555
  author:
  - first: Saku
    full: Saku Sugawara
    id: saku-sugawara
    last: Sugawara
  - first: Kentaro
    full: Kentaro Inui
    id: kentaro-inui
    last: Inui
  - first: Satoshi
    full: Satoshi Sekine
    id: satoshi-sekine
    last: Sekine
  - first: Akiko
    full: Akiko Aizawa
    id: akiko-aizawa
    last: Aizawa
  author_string: Saku Sugawara, Kentaro Inui, Satoshi Sekine, Akiko Aizawa
  bibkey: sugawara-etal-2018-makes
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1453
  month: October-November
  page_first: '4208'
  page_last: '4219'
  pages: "4208\u20134219"
  paper_id: '453'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1453.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1453.jpg
  title: What Makes Reading Comprehension Questions Easier?
  title_html: What Makes Reading Comprehension Questions Easier?
  url: https://www.aclweb.org/anthology/D18-1453
  year: '2018'
D18-1454:
  abstract: "Reading comprehension QA tasks have seen a recent surge in popularity,\
    \ yet most works have focused on fact-finding extractive QA. We instead focus\
    \ on a more challenging multi-hop generative task (NarrativeQA), which requires\
    \ the model to reason, gather, and synthesize disjoint pieces of information within\
    \ the context to generate an answer. This type of multi-step reasoning also often\
    \ requires understanding implicit relations, which humans resolve via external,\
    \ background commonsense knowledge. We first present a strong generative baseline\
    \ that uses a multi-attention mechanism to perform multiple hops of reasoning\
    \ and a pointer-generator decoder to synthesize the answer. This model performs\
    \ substantially better than previous generative models, and is competitive with\
    \ current state-of-the-art span prediction models. We next introduce a novel system\
    \ for selecting grounded multi-hop relational commonsense information from ConceptNet\
    \ via a pointwise mutual information and term-frequency based scoring function.\
    \ Finally, we effectively use this extracted commonsense information to fill in\
    \ gaps of reasoning between context hops, using a selectively-gated attention\
    \ mechanism. This boosts the model\u2019s performance significantly (also verified\
    \ via human evaluation), establishing a new state-of-the-art for the task. We\
    \ also show that our background knowledge enhancements are generalizable and improve\
    \ performance on QAngaroo-WikiHop, another multi-hop reasoning dataset."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1454.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1454.Attachment.pdf
  - filename: https://vimeo.com/306151626
    type: video
    url: https://vimeo.com/306151626
  author:
  - first: Lisa
    full: Lisa Bauer
    id: lisa-bauer
    last: Bauer
  - first: Yicheng
    full: Yicheng Wang
    id: yicheng-wang
    last: Wang
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  author_string: Lisa Bauer, Yicheng Wang, Mohit Bansal
  bibkey: bauer-etal-2018-commonsense
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1454
  month: October-November
  page_first: '4220'
  page_last: '4230'
  pages: "4220\u20134230"
  paper_id: '454'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1454.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1454.jpg
  title: Commonsense for Generative Multi-Hop Question Answering Tasks
  title_html: Commonsense for Generative Multi-Hop Question Answering Tasks
  url: https://www.aclweb.org/anthology/D18-1454
  year: '2018'
D18-1455:
  abstract: Open Domain Question Answering (QA) is evolving from complex pipelined
    systems to end-to-end deep neural networks. Specialized neural models have been
    developed for extracting answers from either text alone or Knowledge Bases (KBs)
    alone. In this paper we look at a more practical setting, namely QA over the combination
    of a KB and entity-linked text, which is appropriate when an incomplete KB is
    available with a large text corpus. Building on recent advances in graph representation
    learning we propose a novel model, GRAFT-Net, for extracting answers from a question-specific
    subgraph containing text and KB entities and relations. We construct a suite of
    benchmark tasks for this problem, varying the difficulty of questions, the amount
    of training data, and KB completeness. We show that GRAFT-Net is competitive with
    the state-of-the-art when tested using either KBs or text alone, and vastly outperforms
    existing methods in the combined setting.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306152381
    type: video
    url: https://vimeo.com/306152381
  author:
  - first: Haitian
    full: Haitian Sun
    id: haitian-sun
    last: Sun
  - first: Bhuwan
    full: Bhuwan Dhingra
    id: bhuwan-dhingra
    last: Dhingra
  - first: Manzil
    full: Manzil Zaheer
    id: manzil-zaheer
    last: Zaheer
  - first: Kathryn
    full: Kathryn Mazaitis
    id: kathryn-mazaitis
    last: Mazaitis
  - first: Ruslan
    full: Ruslan Salakhutdinov
    id: ruslan-salakhutdinov
    last: Salakhutdinov
  - first: William
    full: William Cohen
    id: william-cohen
    last: Cohen
  author_string: Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan
    Salakhutdinov, William Cohen
  bibkey: sun-etal-2018-open
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1455
  month: October-November
  page_first: '4231'
  page_last: '4242'
  pages: "4231\u20134242"
  paper_id: '455'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1455.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1455.jpg
  title: Open Domain Question Answering Using Early Fusion of Knowledge Bases and
    Text
  title_html: Open Domain Question Answering Using Early Fusion of Knowledge Bases
    and Text
  url: https://www.aclweb.org/anthology/D18-1455
  year: '2018'
D18-1456:
  abstract: Recently, there has been a surge of interest in reading comprehension-based
    (RC) question answering (QA). However, current approaches suffer from an impractical
    assumption that every question has a valid answer in the associated passage. A
    practical QA system must possess the ability to determine whether a valid answer
    exists in a given text passage. In this paper, we focus on developing QA systems
    that can extract an answer for a question if and only if the associated passage
    contains an answer. If the associated passage does not contain any valid answer,
    the QA system will correctly return Nil. We propose a novel nil-aware answer span
    extraction framework that is capable of returning Nil or a text span from the
    associated passage as an answer in a single step. We show that our proposed framework
    can be easily integrated with several recently proposed QA models developed for
    reading comprehension and can be trained in an end-to-end fashion. Our proposed
    nil-aware answer extraction neural network decomposes pieces of evidence into
    relevant and irrelevant parts and then combines them to infer the existence of
    any answer. Experiments on the NewsQA dataset show that the integration of our
    proposed framework significantly outperforms several strong baseline systems that
    use pipeline or threshold-based approaches.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306152896
    type: video
    url: https://vimeo.com/306152896
  author:
  - first: Souvik
    full: Souvik Kundu
    id: souvik-kundu
    last: Kundu
  - first: Hwee Tou
    full: Hwee Tou Ng
    id: hwee-tou-ng
    last: Ng
  author_string: Souvik Kundu, Hwee Tou Ng
  bibkey: kundu-ng-2018-nil
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1456
  month: October-November
  page_first: '4243'
  page_last: '4252'
  pages: "4243\u20134252"
  paper_id: '456'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1456.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1456.jpg
  title: A Nil-Aware Answer Extraction Framework for Question Answering
  title_html: A Nil-Aware Answer Extraction Framework for Question Answering
  url: https://www.aclweb.org/anthology/D18-1456
  year: '2018'
D18-1457:
  abstract: Advanced neural machine translation (NMT) models generally implement encoder
    and decoder as multiple layers, which allows systems to model complex functions
    and capture complicated linguistic structures. However, only the top layers of
    encoder and decoder are leveraged in the subsequent process, which misses the
    opportunity to exploit the useful information embedded in other layers. In this
    work, we propose to simultaneously expose all of these signals with layer aggregation
    and multi-layer attention mechanisms. In addition, we introduce an auxiliary regularization
    term to encourage different layers to capture diverse information. Experimental
    results on widely-used WMT14 English-German and WMT17 Chinese-English translation
    data demonstrate the effectiveness and universality of the proposed approach.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306130778
    type: video
    url: https://vimeo.com/306130778
  author:
  - first: Zi-Yi
    full: Zi-Yi Dou
    id: zi-yi-dou
    last: Dou
  - first: Zhaopeng
    full: Zhaopeng Tu
    id: zhaopeng-tu
    last: Tu
  - first: Xing
    full: Xing Wang
    id: xing-wang
    last: Wang
  - first: Shuming
    full: Shuming Shi
    id: shuming-shi
    last: Shi
  - first: Tong
    full: Tong Zhang
    id: tong-zhang
    last: Zhang
  author_string: Zi-Yi Dou, Zhaopeng Tu, Xing Wang, Shuming Shi, Tong Zhang
  bibkey: dou-etal-2018-exploiting
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1457
  month: October-November
  page_first: '4253'
  page_last: '4262'
  pages: "4253\u20134262"
  paper_id: '457'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1457.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1457.jpg
  title: Exploiting Deep Representations for Neural Machine Translation
  title_html: Exploiting Deep Representations for Neural Machine Translation
  url: https://www.aclweb.org/anthology/D18-1457
  year: '2018'
D18-1458:
  abstract: 'Recently, non-recurrent architectures (convolutional, self-attentional)
    have outperformed RNNs in neural machine translation. CNNs and self-attentional
    networks can connect distant words via shorter network paths than RNNs, and it
    has been speculated that this improves their ability to model long-range dependencies.
    However, this theoretical argument has not been tested empirically, nor have alternative
    explanations for their strong performance been explored in-depth. We hypothesize
    that the strong performance of CNNs and self-attentional networks could also be
    due to their ability to extract semantic features from the source text, and we
    evaluate RNNs, CNNs and self-attention networks on two tasks: subject-verb agreement
    (where capturing long-range dependencies is required) and word sense disambiguation
    (where semantic feature extraction is required). Our experimental results show
    that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling
    subject-verb agreement over long distances; 2) self-attentional networks perform
    distinctly better than RNNs and CNNs on word sense disambiguation.'
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306131741
    type: video
    url: https://vimeo.com/306131741
  author:
  - first: Gongbo
    full: Gongbo Tang
    id: gongbo-tang
    last: Tang
  - first: Mathias
    full: "Mathias M\xFCller"
    id: mathias-muller
    last: "M\xFCller"
  - first: Annette
    full: Annette Rios
    id: annette-rios-gonzales
    last: Rios
  - first: Rico
    full: Rico Sennrich
    id: rico-sennrich
    last: Sennrich
  author_string: "Gongbo Tang, Mathias M\xFCller, Annette Rios, Rico Sennrich"
  bibkey: tang-etal-2018-self
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1458
  month: October-November
  page_first: '4263'
  page_last: '4272'
  pages: "4263\u20134272"
  paper_id: '458'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1458.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1458.jpg
  title: Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures
  title_html: Why Self-Attention? A Targeted Evaluation of Neural Machine Translation
    Architectures
  url: https://www.aclweb.org/anthology/D18-1458
  year: '2018'
D18-1459:
  abstract: In this paper, we propose an additionsubtraction twin-gated recurrent
    network (ATR) to simplify neural machine translation. The recurrent units of ATR
    are heavily simplified to have the smallest number of weight matrices among units
    of all existing gated RNNs. With the simple addition and subtraction operation,
    we introduce a twin-gated mechanism to build input and forget gates which are
    highly correlated. Despite this simplification, the essential non-linearities
    and capability of modeling long-distance dependencies are preserved. Additionally,
    the proposed ATR is more transparent than LSTM/GRU due to the simplification.
    Forward self-attention can be easily established in ATR, which makes the proposed
    network interpretable. Experiments on WMT14 translation tasks demonstrate that
    ATR-based neural machine translation can yield competitive performance on English-German
    and English-French language pairs in terms of both translation quality and speed.
    Further experiments on NIST Chinese-English translation, natural language inference
    and Chinese word segmentation verify the generality and applicability of ATR on
    different natural language processing tasks.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1459.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1459.Attachment.zip
  - filename: https://vimeo.com/306132998
    type: video
    url: https://vimeo.com/306132998
  author:
  - first: Biao
    full: Biao Zhang
    id: biao-zhang
    last: Zhang
  - first: Deyi
    full: Deyi Xiong
    id: deyi-xiong
    last: Xiong
  - first: Jinsong
    full: Jinsong Su
    id: jinsong-su
    last: Su
  - first: Qian
    full: Qian Lin
    id: qian-lin
    last: Lin
  - first: Huiji
    full: Huiji Zhang
    id: huiji-zhang
    last: Zhang
  author_string: Biao Zhang, Deyi Xiong, Jinsong Su, Qian Lin, Huiji Zhang
  bibkey: zhang-etal-2018-simplifying
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1459
  month: October-November
  page_first: '4273'
  page_last: '4283'
  pages: "4273\u20134283"
  paper_id: '459'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1459.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1459.jpg
  title: Simplifying Neural Machine Translation with Addition-Subtraction Twin-Gated
    Recurrent Networks
  title_html: Simplifying Neural Machine Translation with Addition-Subtraction Twin-Gated
    Recurrent Networks
  url: https://www.aclweb.org/anthology/D18-1459
  year: '2018'
D18-1460:
  abstract: Although neural machine translation has achieved promising results, it
    suffers from slow translation speed. The direct consequence is that a trade-off
    has to be made between translation quality and speed, thus its performance can
    not come into full play. We apply cube pruning, a popular technique to speed up
    dynamic programming, into neural machine translation to speed up the translation.
    To construct the equivalence class, similar target hidden states are combined,
    leading to less RNN expansion operations on the target side and less softmax operations
    over the large target vocabulary. The experiments show that, at the same or even
    better translation quality, our method can translate faster compared with naive
    beam search by 3.3x on GPUs and 3.5x on CPUs.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306134160
    type: video
    url: https://vimeo.com/306134160
  author:
  - first: Wen
    full: Wen Zhang
    id: wen-zhang
    last: Zhang
  - first: Liang
    full: Liang Huang
    id: liang-huang
    last: Huang
  - first: Yang
    full: Yang Feng
    id: yang-feng
    last: Feng
  - first: Lei
    full: Lei Shen
    id: lei-shen
    last: Shen
  - first: Qun
    full: Qun Liu
    id: qun-liu
    last: Liu
  author_string: Wen Zhang, Liang Huang, Yang Feng, Lei Shen, Qun Liu
  bibkey: zhang-etal-2018-speeding
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1460
  month: October-November
  page_first: '4284'
  page_last: '4294'
  pages: "4284\u20134294"
  paper_id: '460'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1460.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1460.jpg
  title: Speeding Up Neural Machine Translation Decoding by Cube Pruning
  title_html: Speeding Up Neural Machine Translation Decoding by Cube Pruning
  url: https://www.aclweb.org/anthology/D18-1460
  year: '2018'
D18-1461:
  abstract: Translating characters instead of words or word-fragments has the potential
    to simplify the processing pipeline for neural machine translation (NMT), and
    improve results by eliminating hyper-parameters and manual feature engineering.
    However, it results in longer sequences in which each symbol contains less information,
    creating both modeling and computational challenges. In this paper, we show that
    the modeling problem can be solved by standard sequence-to-sequence architectures
    of sufficient depth, and that deep models operating at the character level outperform
    identical models operating over word fragments. This result implies that alternative
    architectures for handling character input are better viewed as methods for reducing
    computation time than as improved ways of modeling longer sequences. From this
    perspective, we evaluate several techniques for character-level NMT, verify that
    they do not match the performance of our deep character baseline model, and evaluate
    the performance versus computation time tradeoffs they offer. Within this framework,
    we also perform the first evaluation for NMT of conditional computation over time,
    in which the model learns which timesteps can be skipped, rather than having them
    be dictated by a fixed schedule specified before training begins.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306134793
    type: video
    url: https://vimeo.com/306134793
  author:
  - first: Colin
    full: Colin Cherry
    id: colin-cherry
    last: Cherry
  - first: George
    full: George Foster
    id: george-foster
    last: Foster
  - first: Ankur
    full: Ankur Bapna
    id: ankur-bapna
    last: Bapna
  - first: Orhan
    full: Orhan Firat
    id: orhan-firat
    last: Firat
  - first: Wolfgang
    full: Wolfgang Macherey
    id: wolfgang-macherey
    last: Macherey
  author_string: Colin Cherry, George Foster, Ankur Bapna, Orhan Firat, Wolfgang Macherey
  bibkey: cherry-etal-2018-revisiting
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1461
  month: October-November
  page_first: '4295'
  page_last: '4305'
  pages: "4295\u20134305"
  paper_id: '461'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1461.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1461.jpg
  title: Revisiting Character-Based Neural Machine Translation with Capacity and Compression
  title_html: Revisiting Character-Based Neural Machine Translation with Capacity
    and Compression
  url: https://www.aclweb.org/anthology/D18-1461
  year: '2018'
D18-1462:
  abstract: Narrative story generation is a challenging problem because it demands
    the generated sentences with tight semantic connections, which has not been well
    studied by most existing generative models. To address this problem, we propose
    a skeleton-based model to promote the coherence of generated stories. Different
    from traditional models that generate a complete sentence at a stroke, the proposed
    model first generates the most critical phrases, called skeleton, and then expands
    the skeleton to a complete and fluent sentence. The skeleton is not manually defined,
    but learned by a reinforcement learning method. Compared to the state-of-the-art
    models, our skeleton-based model can generate significantly more coherent text
    according to human evaluation and automatic evaluation. The G-score is improved
    by 20.1% in human evaluation.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306161720
    type: video
    url: https://vimeo.com/306161720
  author:
  - first: Jingjing
    full: Jingjing Xu
    id: jingjing-xu
    last: Xu
  - first: Xuancheng
    full: Xuancheng Ren
    id: xuancheng-ren
    last: Ren
  - first: Yi
    full: Yi Zhang
    id: yi-zhang
    last: Zhang
  - first: Qi
    full: Qi Zeng
    id: qi-zeng
    last: Zeng
  - first: Xiaoyan
    full: Xiaoyan Cai
    id: xiaoyan-cai
    last: Cai
  - first: Xu
    full: Xu Sun
    id: xu-sun
    last: Sun
  author_string: Jingjing Xu, Xuancheng Ren, Yi Zhang, Qi Zeng, Xiaoyan Cai, Xu Sun
  bibkey: xu-etal-2018-skeleton
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1462
  month: October-November
  page_first: '4306'
  page_last: '4315'
  pages: "4306\u20134315"
  paper_id: '462'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1462.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1462.jpg
  title: A Skeleton-Based Model for Promoting Coherence Among Sentences in Narrative
    Story Generation
  title_html: A Skeleton-Based Model for Promoting Coherence Among Sentences in Narrative
    Story Generation
  url: https://www.aclweb.org/anthology/D18-1462
  year: '2018'
D18-1463:
  abstract: Sequence-to-Sequence (seq2seq) models have become overwhelmingly popular
    in building end-to-end trainable dialogue systems. Though highly efficient in
    learning the backbone of human-computer communications, they suffer from the problem
    of strongly favoring short generic responses. In this paper, we argue that a good
    response should smoothly connect both the preceding dialogue history and the following
    conversations. We strengthen this connection by mutual information maximization.
    To sidestep the non-differentiability of discrete natural language tokens, we
    introduce an auxiliary continuous code space and map such code space to a learnable
    prior distribution for generation purpose. Experiments on two dialogue datasets
    validate the effectiveness of our model, where the generated responses are closely
    related to the dialogue context and lead to more interactive conversations.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1463.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1463.Attachment.pdf
  - filename: https://vimeo.com/306163081
    type: video
    url: https://vimeo.com/306163081
  author:
  - first: Xiaoyu
    full: Xiaoyu Shen
    id: xiaoyu-shen
    last: Shen
  - first: Hui
    full: Hui Su
    id: hui-su
    last: Su
  - first: Wenjie
    full: Wenjie Li
    id: wenjie-li
    last: Li
  - first: Dietrich
    full: Dietrich Klakow
    id: dietrich-klakow
    last: Klakow
  author_string: Xiaoyu Shen, Hui Su, Wenjie Li, Dietrich Klakow
  bibkey: shen-etal-2018-nexus
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1463
  month: October-November
  page_first: '4316'
  page_last: '4327'
  pages: "4316\u20134327"
  paper_id: '463'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1463.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1463.jpg
  title: 'NEXUS Network: Connecting the Preceding and the Following in Dialogue Generation'
  title_html: '<span class="acl-fixed-case">NEXUS</span> Network: Connecting the Preceding
    and the Following in Dialogue Generation'
  url: https://www.aclweb.org/anthology/D18-1463
  year: '2018'
D18-1464:
  abstract: 'We propose a local coherence model that captures the flow of what semantically
    connects adjacent sentences in a text. We represent the semantics of a sentence
    by a vector and capture its state at each word of the sentence. We model what
    relates two adjacent sentences based on the two most similar semantic states,
    each of which is in one of the sentences. We encode the perceived coherence of
    a text by a vector, which represents patterns of changes in salient information
    that relates adjacent sentences. Our experiments demonstrate that our approach
    is beneficial for two downstream tasks: Readability assessment, in which our model
    achieves new state-of-the-art results; and essay scoring, in which the combination
    of our coherence vectors and other task-dependent features significantly improves
    the performance of a strong essay scorer.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1464.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1464.Attachment.pdf
  - filename: https://vimeo.com/306164201
    type: video
    url: https://vimeo.com/306164201
  author:
  - first: Mohsen
    full: Mohsen Mesgar
    id: mohsen-mesgar
    last: Mesgar
  - first: Michael
    full: Michael Strube
    id: michael-strube
    last: Strube
  author_string: Mohsen Mesgar, Michael Strube
  bibkey: mesgar-strube-2018-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1464
  month: October-November
  page_first: '4328'
  page_last: '4339'
  pages: "4328\u20134339"
  paper_id: '464'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1464.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1464.jpg
  title: A Neural Local Coherence Model for Text Quality Assessment
  title_html: A Neural Local Coherence Model for Text Quality Assessment
  url: https://www.aclweb.org/anthology/D18-1464
  year: '2018'
D18-1465:
  abstract: In this paper, we propose a novel deep attentive sentence ordering network
    (referred as ATTOrderNet) which integrates self-attention mechanism with LSTMs
    in the encoding of input sentences. It enables us to capture global dependencies
    among sentences regardless of their input order and obtains a reliable representation
    of the sentence set. With this representation, a pointer network is exploited
    to generate an ordered sequence. The proposed model is evaluated on Sentence Ordering
    and Order Discrimination tasks. The extensive experimental results demonstrate
    its effectiveness and superiority to the state-of-the-art methods.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306165142
    type: video
    url: https://vimeo.com/306165142
  author:
  - first: Baiyun
    full: Baiyun Cui
    id: baiyun-cui
    last: Cui
  - first: Yingming
    full: Yingming Li
    id: yingming-li
    last: Li
  - first: Ming
    full: Ming Chen
    id: ming-chen
    last: Chen
  - first: Zhongfei
    full: Zhongfei Zhang
    id: zhongfei-zhang
    last: Zhang
  author_string: Baiyun Cui, Yingming Li, Ming Chen, Zhongfei Zhang
  bibkey: cui-etal-2018-deep
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1465
  month: October-November
  page_first: '4340'
  page_last: '4349'
  pages: "4340\u20134349"
  paper_id: '465'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1465.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1465.jpg
  title: Deep Attentive Sentence Ordering Network
  title_html: Deep Attentive Sentence Ordering Network
  url: https://www.aclweb.org/anthology/D18-1465
  year: '2018'
D18-1466:
  abstract: When a reader is first introduced to an entity, its referring expression
    must describe the entity. For entities that are widely known, a single word or
    phrase often suffices. This paper presents the first study of how expressions
    that refer to the same entity develop over time. We track thousands of person
    and organization entities over 20 years of New York Times (NYT). As entities move
    from hearer-new (first introduction to the NYT audience) to hearer-old (common
    knowledge) status, we show empirically that the referring expressions along this
    trajectory depend on the type of the entity, and exhibit linguistic properties
    related to becoming common knowledge (e.g., shorter length, less use of appositives,
    more definiteness). These properties can also be used to build a model to predict
    how long it will take for an entity to reach hearer-old status. Our results reach
    10-30% absolute improvement over a majority-class baseline.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306166016
    type: video
    url: https://vimeo.com/306166016
  author:
  - first: Ieva
    full: "Ieva Stali\u016Bnait\u0117"
    id: ieva-staliunaite
    last: "Stali\u016Bnait\u0117"
  - first: Hannah
    full: Hannah Rohde
    id: hannah-rohde
    last: Rohde
  - first: Bonnie
    full: Bonnie Webber
    id: bonnie-webber
    last: Webber
  - first: Annie
    full: Annie Louis
    id: annie-louis
    last: Louis
  author_string: "Ieva Stali\u016Bnait\u0117, Hannah Rohde, Bonnie Webber, Annie Louis"
  bibkey: staliunaite-etal-2018-getting
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1466
  month: October-November
  page_first: '4350'
  page_last: '4359'
  pages: "4350\u20134359"
  paper_id: '466'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1466.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1466.jpg
  title: "Getting to \u201CHearer-old\u201D: Charting Referring Expressions Across\
    \ Time"
  title_html: "Getting to \u201CHearer-old\u201D: Charting Referring Expressions Across\
    \ Time"
  url: https://www.aclweb.org/anthology/D18-1466
  year: '2018'
D18-1467:
  abstract: "In an online community, new words come and go: today\u2019s \u201Chaha\u201D\
    \ may be replaced by tomorrow\u2019s \u201Clol.\u201D Changes in online writing\
    \ are usually studied as a social process, with innovations diffusing through\
    \ a network of individuals in a speech community. But unlike other types of innovation,\
    \ language change is shaped and constrained by the grammatical system in which\
    \ it takes part. To investigate the role of social and structural factors in language\
    \ change, we undertake a large-scale analysis of the frequencies of non-standard\
    \ words in Reddit. Dissemination across many linguistic contexts is a predictor\
    \ of success: words that appear in more linguistic contexts grow faster and survive\
    \ longer. Furthermore, social dissemination plays a less important role in explaining\
    \ word growth and decline than previously hypothesized."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306120421
    type: video
    url: https://vimeo.com/306120421
  author:
  - first: Ian
    full: Ian Stewart
    id: ian-stewart
    last: Stewart
  - first: Jacob
    full: Jacob Eisenstein
    id: jacob-eisenstein
    last: Eisenstein
  author_string: Ian Stewart, Jacob Eisenstein
  bibkey: stewart-eisenstein-2018-making
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1467
  month: October-November
  page_first: '4360'
  page_last: '4370'
  pages: "4360\u20134370"
  paper_id: '467'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1467.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1467.jpg
  title: "Making \u201Cfetch\u201D happen: The influence of social and linguistic\
    \ context on nonstandard word growth and decline"
  title_html: "Making \u201Cfetch\u201D happen: The influence of social and linguistic\
    \ context on nonstandard word growth and decline"
  url: https://www.aclweb.org/anthology/D18-1467
  year: '2018'
D18-1468:
  abstract: Statistical phylogenetic models have allowed the quantitative analysis
    of the evolution of a single categorical feature and a pair of binary features,
    but correlated evolution involving multiple discrete features is yet to be explored.
    Here we propose latent representation-based analysis in which (1) a sequence of
    discrete surface features is projected to a sequence of independent binary variables
    and (2) phylogenetic inference is performed on the latent space. In the experiments,
    we analyze the features of linguistic typology, with a special focus on the order
    of subject, object and verb. Our analysis suggests that languages sharing the
    same word order are not necessarily a coherent group but exhibit varying degrees
    of diachronic stability depending on other features.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1468.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1468.Attachment.pdf
  - filename: https://vimeo.com/306121200
    type: video
    url: https://vimeo.com/306121200
  author:
  - first: Yugo
    full: Yugo Murawaki
    id: yugo-murawaki
    last: Murawaki
  author_string: Yugo Murawaki
  bibkey: murawaki-2018-analyzing
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1468
  month: October-November
  page_first: '4371'
  page_last: '4382'
  pages: "4371\u20134382"
  paper_id: '468'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1468.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1468.jpg
  title: Analyzing Correlated Evolution of Multiple Features Using Latent Representations
  title_html: Analyzing Correlated Evolution of Multiple Features Using Latent Representations
  url: https://www.aclweb.org/anthology/D18-1468
  year: '2018'
D18-1469:
  abstract: Dialects are one of the main drivers of language variation, a major challenge
    for natural language processing tools. In most languages, dialects exist along
    a continuum, and are commonly discretized by combining the extent of several preselected
    linguistic variables. However, the selection of these variables is theory-driven
    and itself insensitive to change. We use Doc2Vec on a corpus of 16.8M anonymous
    online posts in the German-speaking area to learn continuous document representations
    of cities. These representations capture continuous regional linguistic distinctions,
    and can serve as input to downstream NLP tasks sensitive to regional variation.
    By incorporating geographic information via retrofitting and agglomerative clustering
    with structure, we recover dialect areas at various levels of granularity. Evaluating
    these clusters against an existing dialect map, we achieve a match of up to 0.77
    V-score (harmonic mean of cluster completeness and homogeneity). Our results show
    that representation learning with retrofitting offers a robust general method
    to automatically expose dialectal differences and regional variation at a finer
    granularity than was previously possible.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306121832
    type: video
    url: https://vimeo.com/306121832
  author:
  - first: Dirk
    full: Dirk Hovy
    id: dirk-hovy
    last: Hovy
  - first: Christoph
    full: Christoph Purschke
    id: christoph-purschke
    last: Purschke
  author_string: Dirk Hovy, Christoph Purschke
  bibkey: hovy-purschke-2018-capturing
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1469
  month: October-November
  page_first: '4383'
  page_last: '4394'
  pages: "4383\u20134394"
  paper_id: '469'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1469.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1469.jpg
  title: Capturing Regional Variation with Distributed Place Representations and Geographic
    Retrofitting
  title_html: Capturing Regional Variation with Distributed Place Representations
    and Geographic Retrofitting
  url: https://www.aclweb.org/anthology/D18-1469
  year: '2018'
D18-1470:
  abstract: This paper presents a set of dimensions to characterize the association
    between two people. We distinguish between interactions (when somebody refers
    to somebody in a conversation) and relationships (a sequence of interactions).
    We work with dialogue scripts from the TV show Friends, and do not impose any
    restrictions on the interactions and relationships. We introduce and analyze a
    new corpus, and present experimental results showing that the task can be automated.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1470.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1470.Attachment.pdf
  - filename: https://vimeo.com/306122681
    type: video
    url: https://vimeo.com/306122681
  author:
  - first: Farzana
    full: Farzana Rashid
    id: farzana-rashid
    last: Rashid
  - first: Eduardo
    full: Eduardo Blanco
    id: eduardo-blanco
    last: Blanco
  author_string: Farzana Rashid, Eduardo Blanco
  bibkey: rashid-blanco-2018-characterizing
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1470
  month: October-November
  page_first: '4395'
  page_last: '4404'
  pages: "4395\u20134404"
  paper_id: '470'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1470.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1470.jpg
  title: Characterizing Interactions and Relationships between People
  title_html: Characterizing Interactions and Relationships between People
  url: https://www.aclweb.org/anthology/D18-1470
  year: '2018'
D18-1471:
  abstract: Vulgar words are employed in language use for several different functions,
    ranging from expressing aggression to signaling group identity or the informality
    of the communication. This versatility of usage of a restricted set of words is
    challenging for downstream applications and has yet to be studied quantitatively
    or using natural language processing techniques. We introduce a novel data set
    of 7,800 tweets from users with known demographic traits where all instances of
    vulgar words are annotated with one of the six categories of vulgar word use.
    Using this data set, we present the first analysis of the pragmatic aspects of
    vulgarity and how they relate to social factors. We build a model able to predict
    the category of a vulgar word based on the immediate context it appears in with
    67.4 macro F1 across six classes. Finally, we demonstrate the utility of modeling
    the type of vulgar word use in context by using this information to achieve state-of-the-art
    performance in hate speech detection on a benchmark data set.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1471.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1471.Attachment.zip
  - filename: https://vimeo.com/306123618
    type: video
    url: https://vimeo.com/306123618
  author:
  - first: Eric
    full: Eric Holgate
    id: eric-holgate
    last: Holgate
  - first: Isabel
    full: Isabel Cachola
    id: isabel-cachola
    last: Cachola
  - first: Daniel
    full: "Daniel Preo\u0163iuc-Pietro"
    id: daniel-preotiuc-pietro
    last: "Preo\u0163iuc-Pietro"
  - first: Junyi Jessy
    full: Junyi Jessy Li
    id: junyi-jessy-li
    last: Li
  author_string: "Eric Holgate, Isabel Cachola, Daniel Preo\u0163iuc-Pietro, Junyi\
    \ Jessy Li"
  bibkey: holgate-etal-2018-swear
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1471
  month: October-November
  page_first: '4405'
  page_last: '4414'
  pages: "4405\u20134414"
  paper_id: '471'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1471.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1471.jpg
  title: Why Swear? Analyzing and Inferring the Intentions of Vulgar Expressions
  title_html: Why Swear? Analyzing and Inferring the Intentions of Vulgar Expressions
  url: https://www.aclweb.org/anthology/D18-1471
  year: '2018'
D18-1472:
  abstract: "Activation functions play a crucial role in neural networks because they\
    \ are the nonlinearities which have been attributed to the success story of deep\
    \ learning. One of the currently most popular activation functions is ReLU, but\
    \ several competitors have recently been proposed or \u2018discovered\u2019, including\
    \ LReLU functions and swish. While most works compare newly proposed activation\
    \ functions on few tasks (usually from image classification) and against few competitors\
    \ (usually ReLU), we perform the first largescale comparison of 21 activation\
    \ functions across eight different NLP tasks. We find that a largely unknown activation\
    \ function performs most stably across all tasks, the so-called penalized tanh\
    \ function. We also show that it can successfully replace the sigmoid and tanh\
    \ gates in LSTM cells, leading to a 2 percentage point (pp) improvement over the\
    \ standard choices on a challenging NLP task."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1472.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1472.Attachment.zip
  author:
  - first: Steffen
    full: Steffen Eger
    id: steffen-eger
    last: Eger
  - first: Paul
    full: Paul Youssef
    id: paul-youssef
    last: Youssef
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Steffen Eger, Paul Youssef, Iryna Gurevych
  bibkey: eger-etal-2018-time
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1472
  month: October-November
  page_first: '4415'
  page_last: '4424'
  pages: "4415\u20134424"
  paper_id: '472'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1472.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1472.jpg
  title: Is it Time to Swish? Comparing Deep Learning Activation Functions Across
    NLP tasks
  title_html: Is it Time to Swish? Comparing Deep Learning Activation Functions Across
    <span class="acl-fixed-case">NLP</span> tasks
  url: https://www.aclweb.org/anthology/D18-1472
  year: '2018'
D18-1473:
  abstract: Character-level string-to-string transduction is an important component
    of various NLP tasks. The goal is to map an input string to an output string,
    where the strings may be of different lengths and have characters taken from different
    alphabets. Recent approaches have used sequence-to-sequence models with an attention
    mechanism to learn which parts of the input string the model should focus on during
    the generation of the output string. Both soft attention and hard monotonic attention
    have been used, but hard non-monotonic attention has only been used in other sequence
    modeling tasks and has required a stochastic approximation to compute the gradient.
    In this work, we introduce an exact, polynomial-time algorithm for marginalizing
    over the exponential number of non-monotonic alignments between two strings, showing
    that hard attention models can be viewed as neural reparameterizations of the
    classical IBM Model 1. We compare soft and hard non-monotonic attention experimentally
    and find that the exact algorithm significantly improves performance over the
    stochastic approximation and outperforms soft attention.
  address: Brussels, Belgium
  author:
  - first: Shijie
    full: Shijie Wu
    id: shijie-wu
    last: Wu
  - first: Pamela
    full: Pamela Shapiro
    id: pamela-shapiro
    last: Shapiro
  - first: Ryan
    full: Ryan Cotterell
    id: ryan-cotterell
    last: Cotterell
  author_string: Shijie Wu, Pamela Shapiro, Ryan Cotterell
  bibkey: wu-etal-2018-hard
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1473
  month: October-November
  page_first: '4425'
  page_last: '4438'
  pages: "4425\u20134438"
  paper_id: '473'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1473.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1473.jpg
  title: Hard Non-Monotonic Attention for Character-Level Transduction
  title_html: Hard Non-Monotonic Attention for Character-Level Transduction
  url: https://www.aclweb.org/anthology/D18-1473
  year: '2018'
D18-1474:
  abstract: "We present LSTM-Shuttle, which applies human speed reading techniques\
    \ to natural language processing tasks for accurate and efficient comprehension.\
    \ In contrast to previous work, LSTM-Shuttle not only reads shuttling forward\
    \ but also goes back. Shuttling forward enables high efficiency, and going backward\
    \ gives the model a chance to recover lost information, ensuring better prediction.\
    \ We evaluate LSTM-Shuttle on sentiment analysis, news classification, and cloze\
    \ on IMDB, Rotten Tomatoes, AG, and Children\u2019s Book Test datasets. We show\
    \ that LSTM-Shuttle predicts both better and more quickly. To demonstrate how\
    \ LSTM-Shuttle actually behaves, we also analyze the shuttling operation and present\
    \ a case study."
  address: Brussels, Belgium
  author:
  - first: Tsu-Jui
    full: Tsu-Jui Fu
    id: tsu-jui-fu
    last: Fu
  - first: Wei-Yun
    full: Wei-Yun Ma
    id: wei-yun-ma
    last: Ma
  author_string: Tsu-Jui Fu, Wei-Yun Ma
  bibkey: fu-ma-2018-speed
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1474
  month: October-November
  page_first: '4439'
  page_last: '4448'
  pages: "4439\u20134448"
  paper_id: '474'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1474.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1474.jpg
  title: 'Speed Reading: Learning to Read ForBackward via Shuttle'
  title_html: 'Speed Reading: Learning to Read <span class="acl-fixed-case">F</span>or<span
    class="acl-fixed-case">B</span>ackward via Shuttle'
  url: https://www.aclweb.org/anthology/D18-1474
  year: '2018'
D18-1475:
  abstract: Self-attention networks have proven to be of profound value for its strength
    of capturing global dependencies. In this work, we propose to model localness
    for self-attention networks, which enhances the ability of capturing useful local
    context. We cast localness modeling as a learnable Gaussian bias, which indicates
    the central and scope of the local region to be paid more attention. The bias
    is then incorporated into the original attention distribution to form a revised
    distribution. To maintain the strength of capturing long distance dependencies
    while enhance the ability of capturing short-range dependencies, we only apply
    localness modeling to lower layers of self-attention networks. Quantitative and
    qualitative analyses on Chinese-English and English-German translation tasks demonstrate
    the effectiveness and universality of the proposed approach.
  address: Brussels, Belgium
  author:
  - first: Baosong
    full: Baosong Yang
    id: baosong-yang
    last: Yang
  - first: Zhaopeng
    full: Zhaopeng Tu
    id: zhaopeng-tu
    last: Tu
  - first: Derek F.
    full: Derek F. Wong
    id: derek-f-wong
    last: Wong
  - first: Fandong
    full: Fandong Meng
    id: fandong-meng
    last: Meng
  - first: Lidia S.
    full: Lidia S. Chao
    id: lidia-s-chao
    last: Chao
  - first: Tong
    full: Tong Zhang
    id: tong-zhang
    last: Zhang
  author_string: Baosong Yang, Zhaopeng Tu, Derek F. Wong, Fandong Meng, Lidia S.
    Chao, Tong Zhang
  bibkey: yang-etal-2018-modeling
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1475
  month: October-November
  page_first: '4449'
  page_last: '4458'
  pages: "4449\u20134458"
  paper_id: '475'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1475.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1475.jpg
  title: Modeling Localness for Self-Attention Networks
  title_html: Modeling Localness for Self-Attention Networks
  url: https://www.aclweb.org/anthology/D18-1475
  year: '2018'
D18-1476:
  abstract: We introduce a novel type of text representation that preserves the 2D
    layout of a document. This is achieved by encoding each document page as a two-dimensional
    grid of characters. Based on this representation, we present a generic document
    understanding pipeline for structured documents. This pipeline makes use of a
    fully convolutional encoder-decoder network that predicts a segmentation mask
    and bounding boxes. We demonstrate its capabilities on an information extraction
    task from invoices and show that it significantly outperforms approaches based
    on sequential text or document images.
  address: Brussels, Belgium
  author:
  - first: Anoop R
    full: Anoop R Katti
    id: anoop-r-katti
    last: Katti
  - first: Christian
    full: Christian Reisswig
    id: christian-reisswig
    last: Reisswig
  - first: Cordula
    full: Cordula Guder
    id: cordula-guder
    last: Guder
  - first: Sebastian
    full: Sebastian Brarda
    id: sebastian-brarda
    last: Brarda
  - first: Steffen
    full: Steffen Bickel
    id: steffen-bickel
    last: Bickel
  - first: Johannes
    full: "Johannes H\xF6hne"
    id: johannes-hohne
    last: "H\xF6hne"
  - first: Jean Baptiste
    full: Jean Baptiste Faddoul
    id: jean-baptiste-faddoul
    last: Faddoul
  author_string: "Anoop R Katti, Christian Reisswig, Cordula Guder, Sebastian Brarda,\
    \ Steffen Bickel, Johannes H\xF6hne, Jean Baptiste Faddoul"
  bibkey: katti-etal-2018-chargrid
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1476
  month: October-November
  page_first: '4459'
  page_last: '4469'
  pages: "4459\u20134469"
  paper_id: '476'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1476.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1476.jpg
  title: 'Chargrid: Towards Understanding 2D Documents'
  title_html: '<span class="acl-fixed-case">C</span>hargrid: Towards Understanding
    2<span class="acl-fixed-case">D</span> Documents'
  url: https://www.aclweb.org/anthology/D18-1476
  year: '2018'
D18-1477:
  abstract: "Common recurrent neural architectures scale poorly due to the intrinsic\
    \ difficulty in parallelizing their state computations. In this work, we propose\
    \ the Simple Recurrent Unit (SRU), a light recurrent unit that balances model\
    \ capacity and scalability. SRU is designed to provide expressive recurrence,\
    \ enable highly parallelized implementation, and comes with careful initialization\
    \ to facilitate training of deep models. We demonstrate the effectiveness of SRU\
    \ on multiple NLP tasks. SRU achieves 5\u20149x speed-up over cuDNN-optimized\
    \ LSTM on classification and question answering datasets, and delivers stronger\
    \ results than LSTM and convolutional models. We also obtain an average of 0.7\
    \ BLEU improvement over the Transformer model (Vaswani et al., 2017) on translation\
    \ by incorporating SRU into the architecture."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1477.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1477.Attachment.pdf
  author:
  - first: Tao
    full: Tao Lei
    id: tao-lei
    last: Lei
  - first: Yu
    full: Yu Zhang
    id: yu-zhang
    last: Zhang
  - first: Sida I.
    full: Sida I. Wang
    id: sida-i-wang
    last: Wang
  - first: Hui
    full: Hui Dai
    id: hui-dai
    last: Dai
  - first: Yoav
    full: Yoav Artzi
    id: yoav-artzi
    last: Artzi
  author_string: Tao Lei, Yu Zhang, Sida I. Wang, Hui Dai, Yoav Artzi
  bibkey: lei-etal-2018-simple
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1477
  month: October-November
  page_first: '4470'
  page_last: '4481'
  pages: "4470\u20134481"
  paper_id: '477'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1477.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1477.jpg
  title: Simple Recurrent Units for Highly Parallelizable Recurrence
  title_html: Simple Recurrent Units for Highly Parallelizable Recurrence
  url: https://www.aclweb.org/anthology/D18-1477
  year: '2018'
D18-1478:
  abstract: Pseudo relevance feedback (PRF) is commonly used to boost the performance
    of traditional information retrieval (IR) models by using top-ranked documents
    to identify and weight new query terms, thereby reducing the effect of query-document
    vocabulary mismatches. While neural retrieval models have recently demonstrated
    strong results for ad-hoc retrieval, combining them with PRF is not straightforward
    due to incompatibilities between existing PRF approaches and neural architectures.
    To bridge this gap, we propose an end-to-end neural PRF framework that can be
    used with existing neural IR models by embedding different neural models as building
    blocks. Extensive experiments on two standard test collections confirm the effectiveness
    of the proposed NPRF framework in improving the performance of two state-of-the-art
    neural IR models.
  address: Brussels, Belgium
  author:
  - first: Canjia
    full: Canjia Li
    id: canjia-li
    last: Li
  - first: Yingfei
    full: Yingfei Sun
    id: yingfei-sun
    last: Sun
  - first: Ben
    full: Ben He
    id: ben-he
    last: He
  - first: Le
    full: Le Wang
    id: le-wang
    last: Wang
  - first: Kai
    full: Kai Hui
    id: kai-hui
    last: Hui
  - first: Andrew
    full: Andrew Yates
    id: andrew-yates
    last: Yates
  - first: Le
    full: Le Sun
    id: le-sun
    last: Sun
  - first: Jungang
    full: Jungang Xu
    id: jungang-xu
    last: Xu
  author_string: Canjia Li, Yingfei Sun, Ben He, Le Wang, Kai Hui, Andrew Yates, Le
    Sun, Jungang Xu
  bibkey: li-etal-2018-nprf
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1478
  month: October-November
  page_first: '4482'
  page_last: '4491'
  pages: "4482\u20134491"
  paper_id: '478'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1478.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1478.jpg
  title: 'NPRF: A Neural Pseudo Relevance Feedback Framework for Ad-hoc Information
    Retrieval'
  title_html: '<span class="acl-fixed-case">NPRF</span>: A Neural Pseudo Relevance
    Feedback Framework for Ad-hoc Information Retrieval'
  url: https://www.aclweb.org/anthology/D18-1478
  year: '2018'
D18-1479:
  abstract: Learning a matching function between two text sequences is a long standing
    problem in NLP research. This task enables many potential applications such as
    question answering and paraphrase identification. This paper proposes Co-Stack
    Residual Affinity Networks (CSRAN), a new and universal neural architecture for
    this problem. CSRAN is a deep architecture, involving stacked (multi-layered)
    recurrent encoders. Stacked/Deep architectures are traditionally difficult to
    train, due to the inherent weaknesses such as difficulty with feature propagation
    and vanishing gradients. CSRAN incorporates two novel components to take advantage
    of the stacked architecture. Firstly, it introduces a new bidirectional alignment
    mechanism that learns affinity weights by fusing sequence pairs across stacked
    hierarchies. Secondly, it leverages a multi-level attention refinement component
    between stacked recurrent layers. The key intuition is that, by leveraging information
    across all network hierarchies, we can not only improve gradient flow but also
    improve overall performance. We conduct extensive experiments on six well-studied
    text sequence matching datasets, achieving state-of-the-art performance on all.
  address: Brussels, Belgium
  author:
  - first: Yi
    full: Yi Tay
    id: yi-tay
    last: Tay
  - first: Anh Tuan
    full: Anh Tuan Luu
    id: anh-tuan-luu
    last: Luu
  - first: Siu Cheung
    full: Siu Cheung Hui
    id: siu-cheung-hui
    last: Hui
  author_string: Yi Tay, Anh Tuan Luu, Siu Cheung Hui
  bibkey: tay-etal-2018-co
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1479
  month: October-November
  page_first: '4492'
  page_last: '4502'
  pages: "4492\u20134502"
  paper_id: '479'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1479.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1479.jpg
  title: Co-Stack Residual Affinity Networks with Multi-level Attention Refinement
    for Matching Text Sequences
  title_html: Co-Stack Residual Affinity Networks with Multi-level Attention Refinement
    for Matching Text Sequences
  url: https://www.aclweb.org/anthology/D18-1479
  year: '2018'
D18-1480:
  abstract: "A hallmark of variational autoencoders (VAEs) for text processing is\
    \ their combination of powerful encoder-decoder models, such as LSTMs, with simple\
    \ latent distributions, typically multivariate Gaussians. These models pose a\
    \ difficult optimization problem: there is an especially bad local optimum where\
    \ the variational posterior always equals the prior and the model does not use\
    \ the latent variable at all, a kind of \u201Ccollapse\u201D which is encouraged\
    \ by the KL divergence term of the objective. In this work, we experiment with\
    \ another choice of latent distribution, namely the von Mises-Fisher (vMF) distribution,\
    \ which places mass on the surface of the unit hypersphere. With this choice of\
    \ prior and posterior, the KL divergence term now only depends on the variance\
    \ of the vMF distribution, giving us the ability to treat it as a fixed hyperparameter.\
    \ We show that doing so not only averts the KL collapse, but consistently gives\
    \ better likelihoods than Gaussians across a range of modeling conditions, including\
    \ recurrent language modeling and bag-of-words document modeling. An analysis\
    \ of the properties of our vMF representations shows that they learn richer and\
    \ more nuanced structures in their latent representations than their Gaussian\
    \ counterparts."
  address: Brussels, Belgium
  author:
  - first: Jiacheng
    full: Jiacheng Xu
    id: jiacheng-xu
    last: Xu
  - first: Greg
    full: Greg Durrett
    id: greg-durrett
    last: Durrett
  author_string: Jiacheng Xu, Greg Durrett
  bibkey: xu-durrett-2018-spherical
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1480
  month: October-November
  page_first: '4503'
  page_last: '4513'
  pages: "4503\u20134513"
  paper_id: '480'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1480.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1480.jpg
  title: Spherical Latent Spaces for Stable Variational Autoencoders
  title_html: Spherical Latent Spaces for Stable Variational Autoencoders
  url: https://www.aclweb.org/anthology/D18-1480
  year: '2018'
D18-1481:
  abstract: In order to learn universal sentence representations, previous methods
    focus on complex recurrent neural networks or supervised learning. In this paper,
    we propose a mean-max attention autoencoder (mean-max AAE) within the encoder-decoder
    framework. Our autoencoder rely entirely on the MultiHead self-attention mechanism
    to reconstruct the input sequence. In the encoding we propose a mean-max strategy
    that applies both mean and max pooling operations over the hidden vectors to capture
    diverse information of the input. To enable the information to steer the reconstruction
    process dynamically, the decoder performs attention over the mean-max representation.
    By training our model on a large collection of unlabelled data, we obtain high-quality
    representations of sentences. Experimental results on a broad range of 10 transfer
    tasks demonstrate that our model outperforms the state-of-the-art unsupervised
    single methods, including the classical skip-thoughts and the advanced skip-thoughts+LN
    model. Furthermore, compared with the traditional recurrent neural network, our
    mean-max AAE greatly reduce the training time.
  address: Brussels, Belgium
  author:
  - first: Minghua
    full: Minghua Zhang
    id: minghua-zhang
    last: Zhang
  - first: Yunfang
    full: Yunfang Wu
    id: yunfang-wu
    last: Wu
  - first: Weikang
    full: Weikang Li
    id: weigang-li
    last: Li
  - first: Wei
    full: Wei Li
    id: wei-li
    last: Li
  author_string: Minghua Zhang, Yunfang Wu, Weikang Li, Wei Li
  bibkey: zhang-etal-2018-learning-universal
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1481
  month: October-November
  page_first: '4514'
  page_last: '4523'
  pages: "4514\u20134523"
  paper_id: '481'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1481.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1481.jpg
  title: Learning Universal Sentence Representations with Mean-Max Attention Autoencoder
  title_html: Learning Universal Sentence Representations with Mean-Max Attention
    Autoencoder
  url: https://www.aclweb.org/anthology/D18-1481
  year: '2018'
D18-1482:
  abstract: "While the celebrated Word2Vec technique yields semantically rich representations\
    \ for individual words, there has been relatively less success in extending to\
    \ generate unsupervised sentences or documents embeddings. Recent work has demonstrated\
    \ that a distance measure between documents called Word Mover\u2019s Distance\
    \ (WMD) that aligns semantically similar words, yields unprecedented KNN classification\
    \ accuracy. However, WMD is expensive to compute, and it is hard to extend its\
    \ use beyond a KNN classifier. In this paper, we propose the Word Mover\u2019\
    s Embedding (WME), a novel approach to building an unsupervised document (sentence)\
    \ embedding from pre-trained word embeddings. In our experiments on 9 benchmark\
    \ text classification datasets and 22 textual similarity tasks, the proposed technique\
    \ consistently matches or outperforms state-of-the-art techniques, with significantly\
    \ higher accuracy on problems of short length."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1482.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1482.Attachment.zip
  author:
  - first: Lingfei
    full: Lingfei Wu
    id: lingfei-wu
    last: Wu
  - first: Ian En-Hsu
    full: Ian En-Hsu Yen
    id: ian-en-hsu-yen
    last: Yen
  - first: Kun
    full: Kun Xu
    id: kun-xu
    last: Xu
  - first: Fangli
    full: Fangli Xu
    id: fangli-xu
    last: Xu
  - first: Avinash
    full: Avinash Balakrishnan
    id: avinash-balakrishnan
    last: Balakrishnan
  - first: Pin-Yu
    full: Pin-Yu Chen
    id: pin-yu-chen
    last: Chen
  - first: Pradeep
    full: Pradeep Ravikumar
    id: pradeep-ravikumar
    last: Ravikumar
  - first: Michael J.
    full: Michael J. Witbrock
    id: michael-j-witbrock
    last: Witbrock
  author_string: Lingfei Wu, Ian En-Hsu Yen, Kun Xu, Fangli Xu, Avinash Balakrishnan,
    Pin-Yu Chen, Pradeep Ravikumar, Michael J. Witbrock
  bibkey: wu-etal-2018-word
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1482
  month: October-November
  page_first: '4524'
  page_last: '4534'
  pages: "4524\u20134534"
  paper_id: '482'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1482.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1482.jpg
  title: "Word Mover\u2019s Embedding: From Word2Vec to Document Embedding"
  title_html: "Word Mover\u2019s Embedding: From <span class=\"acl-fixed-case\">W</span>ord2<span\
    \ class=\"acl-fixed-case\">V</span>ec to Document Embedding"
  url: https://www.aclweb.org/anthology/D18-1482
  year: '2018'
D18-1483:
  abstract: Clustering news across languages enables efficient media monitoring by
    aggregating articles from multilingual sources into coherent stories. Doing so
    in an online setting allows scalable processing of massive news streams. To this
    end, we describe a novel method for clustering an incoming stream of multilingual
    documents into monolingual and crosslingual clusters. Unlike typical clustering
    approaches that report results on datasets with a small and known number of labels,
    we tackle the problem of discovering an ever growing number of cluster labels
    in an online fashion, using real news datasets in multiple languages. In our formulation,
    the monolingual clusters group together documents while the crosslingual clusters
    group together monolingual clusters, one per language that appears in the stream.
    Our method is simple to implement, computationally efficient and produces state-of-the-art
    results on datasets in German, English and Spanish.
  address: Brussels, Belgium
  author:
  - first: "Sebasti\xE3o"
    full: "Sebasti\xE3o Miranda"
    id: sebastiao-miranda
    last: Miranda
  - first: "Art\u016Brs"
    full: "Art\u016Brs Znoti\u0146\u0161"
    id: arturs-znotins
    last: "Znoti\u0146\u0161"
  - first: Shay B.
    full: Shay B. Cohen
    id: shay-b-cohen
    last: Cohen
  - first: Guntis
    full: Guntis Barzdins
    id: guntis-barzdins
    last: Barzdins
  author_string: "Sebasti\xE3o Miranda, Art\u016Brs Znoti\u0146\u0161, Shay B. Cohen,\
    \ Guntis Barzdins"
  bibkey: miranda-etal-2018-multilingual
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1483
  month: October-November
  page_first: '4535'
  page_last: '4544'
  pages: "4535\u20134544"
  paper_id: '483'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1483.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1483.jpg
  title: Multilingual Clustering of Streaming News
  title_html: Multilingual Clustering of Streaming News
  url: https://www.aclweb.org/anthology/D18-1483
  year: '2018'
D18-1484:
  abstract: Multi-task learning in text classification leverages implicit correlations
    among related tasks to extract common features and yield performance gains. However,
    a large body of previous work treats labels of each task as independent and meaningless
    one-hot vectors, which cause a loss of potential label information. In this paper,
    we propose Multi-Task Label Embedding to convert labels in text classification
    into semantic vectors, thereby turning the original tasks into vector matching
    tasks. Our model utilizes semantic correlations among tasks and makes it convenient
    to scale or transfer when new tasks are involved. Extensive experiments on five
    benchmark datasets for text classification show that our model can effectively
    improve the performances of related tasks with semantic representations of labels
    and additional information from each other.
  address: Brussels, Belgium
  author:
  - first: Honglun
    full: Honglun Zhang
    id: honglun-zhang
    last: Zhang
  - first: Liqiang
    full: Liqiang Xiao
    id: liqiang-xiao
    last: Xiao
  - first: Wenqing
    full: Wenqing Chen
    id: wenqing-chen
    last: Chen
  - first: Yongkun
    full: Yongkun Wang
    id: yongkun-wang
    last: Wang
  - first: Yaohui
    full: Yaohui Jin
    id: yaohui-jin
    last: Jin
  author_string: Honglun Zhang, Liqiang Xiao, Wenqing Chen, Yongkun Wang, Yaohui Jin
  bibkey: zhang-etal-2018-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1484
  month: October-November
  page_first: '4545'
  page_last: '4553'
  pages: "4545\u20134553"
  paper_id: '484'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1484.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1484.jpg
  title: Multi-Task Label Embedding for Text Classification
  title_html: Multi-Task Label Embedding for Text Classification
  url: https://www.aclweb.org/anthology/D18-1484
  year: '2018'
D18-1485:
  abstract: We propose a novel model for multi-label text classification, which is
    based on sequence-to-sequence learning. The model generates higher-level semantic
    unit representations with multi-level dilated convolution as well as a corresponding
    hybrid attention mechanism that extracts both the information at the word-level
    and the level of the semantic unit. Our designed dilated convolution effectively
    reduces dimension and supports an exponential expansion of receptive fields without
    loss of local information, and the attention-over-attention mechanism is able
    to capture more summary relevant information from the source context. Results
    of our experiments show that the proposed model has significant advantages over
    the baseline models on the dataset RCV1-V2 and Ren-CECps, and our analysis demonstrates
    that our model is competitive to the deterministic hierarchical models and it
    is more robust to classifying low-frequency labels
  address: Brussels, Belgium
  author:
  - first: Junyang
    full: Junyang Lin
    id: junyang-lin
    last: Lin
  - first: Qi
    full: Qi Su
    id: qi-su
    last: Su
  - first: Pengcheng
    full: Pengcheng Yang
    id: pengcheng-yang
    last: Yang
  - first: Shuming
    full: Shuming Ma
    id: shuming-ma
    last: Ma
  - first: Xu
    full: Xu Sun
    id: xu-sun
    last: Sun
  author_string: Junyang Lin, Qi Su, Pengcheng Yang, Shuming Ma, Xu Sun
  bibkey: lin-etal-2018-semantic-unit
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1485
  month: October-November
  page_first: '4554'
  page_last: '4564'
  pages: "4554\u20134564"
  paper_id: '485'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1485.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1485.jpg
  title: Semantic-Unit-Based Dilated Convolution for Multi-Label Text Classification
  title_html: Semantic-Unit-Based Dilated Convolution for Multi-Label Text Classification
  url: https://www.aclweb.org/anthology/D18-1485
  year: '2018'
D18-1486:
  abstract: Multi-task learning has an ability to share the knowledge among related
    tasks and implicitly increase the training data. However, it has long been frustrated
    by the interference among tasks. This paper investigates the performance of capsule
    network for text, and proposes a capsule-based multi-task learning architecture,
    which is unified, simple and effective. With the advantages of capsules for feature
    clustering, proposed task routing algorithm can cluster the features for each
    task in the network, which helps reduce the interference among tasks. Experiments
    on six text classification datasets demonstrate the effectiveness of our models
    and their characteristics for feature clustering.
  address: Brussels, Belgium
  author:
  - first: Liqiang
    full: Liqiang Xiao
    id: liqiang-xiao
    last: Xiao
  - first: Honglun
    full: Honglun Zhang
    id: honglun-zhang
    last: Zhang
  - first: Wenqing
    full: Wenqing Chen
    id: wenqing-chen
    last: Chen
  - first: Yongkun
    full: Yongkun Wang
    id: yongkun-wang
    last: Wang
  - first: Yaohui
    full: Yaohui Jin
    id: yaohui-jin
    last: Jin
  author_string: Liqiang Xiao, Honglun Zhang, Wenqing Chen, Yongkun Wang, Yaohui Jin
  bibkey: xiao-etal-2018-mcapsnet
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1486
  month: October-November
  page_first: '4565'
  page_last: '4574'
  pages: "4565\u20134574"
  paper_id: '486'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1486.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1486.jpg
  title: 'MCapsNet: Capsule Network for Text with Multi-Task Learning'
  title_html: '<span class="acl-fixed-case">MC</span>aps<span class="acl-fixed-case">N</span>et:
    Capsule Network for Text with Multi-Task Learning'
  url: https://www.aclweb.org/anthology/D18-1486
  year: '2018'
D18-1487:
  abstract: "Prevalence estimation is the task of inferring the relative frequency\
    \ of classes of unlabeled examples in a group\u2014for example, the proportion\
    \ of a document collection with positive sentiment. Previous work has focused\
    \ on aggregating and adjusting discriminative individual classifiers to obtain\
    \ prevalence point estimates. But imperfect classifier accuracy ought to be reflected\
    \ in uncertainty over the predicted prevalence for scientifically valid inference.\
    \ In this work, we present (1) a generative probabilistic modeling approach to\
    \ prevalence estimation, and (2) the construction and evaluation of prevalence\
    \ confidence intervals; in particular, we demonstrate that an off-the-shelf discriminative\
    \ classifier can be given a generative re-interpretation, by backing out an implicit\
    \ individual-level likelihood function, which can be used to conduct fast and\
    \ simple group-level Bayesian inference. Empirically, we demonstrate our approach\
    \ provides better confidence interval coverage than an alternative, and is dramatically\
    \ more robust to shifts in the class prior between training and testing."
  address: Brussels, Belgium
  author:
  - first: Katherine
    full: Katherine Keith
    id: katherine-keith
    last: Keith
  - first: Brendan
    full: "Brendan O\u2019Connor"
    id: brendan-oconnor
    last: "O\u2019Connor"
  author_string: "Katherine Keith, Brendan O\u2019Connor"
  bibkey: keith-oconnor-2018-uncertainty
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1487
  month: October-November
  page_first: '4575'
  page_last: '4585'
  pages: "4575\u20134585"
  paper_id: '487'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1487.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1487.jpg
  title: Uncertainty-aware generative models for inferring document class prevalence
  title_html: Uncertainty-aware generative models for inferring document class prevalence
  url: https://www.aclweb.org/anthology/D18-1487
  year: '2018'
D18-1488:
  abstract: Causal understanding is essential for many kinds of decision-making, but
    causal inference from observational data has typically only been applied to structured,
    low-dimensional datasets. While text classifiers produce low-dimensional outputs,
    their use in causal inference has not previously been studied. To facilitate causal
    analyses based on language data, we consider the role that text classifiers can
    play in causal inference through established modeling mechanisms from the causality
    literature on missing data and measurement error. We demonstrate how to conduct
    causal analyses using text classifiers on simulated and Yelp data, and discuss
    the opportunities and challenges of future work that uses text data in causal
    inference.
  address: Brussels, Belgium
  author:
  - first: Zach
    full: Zach Wood-Doughty
    id: zach-wood-doughty
    last: Wood-Doughty
  - first: Ilya
    full: Ilya Shpitser
    id: ilya-shpitser
    last: Shpitser
  - first: Mark
    full: Mark Dredze
    id: mark-dredze
    last: Dredze
  author_string: Zach Wood-Doughty, Ilya Shpitser, Mark Dredze
  bibkey: wood-doughty-etal-2018-challenges
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1488
  month: October-November
  page_first: '4586'
  page_last: '4598'
  pages: "4586\u20134598"
  paper_id: '488'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1488.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1488.jpg
  title: Challenges of Using Text Classifiers for Causal Inference
  title_html: Challenges of Using Text Classifiers for Causal Inference
  url: https://www.aclweb.org/anthology/D18-1488
  year: '2018'
D18-1489:
  abstract: 'This paper proposes a state-of-the-art recurrent neural network (RNN)
    language model that combines probability distributions computed not only from
    a final RNN layer but also middle layers. This method raises the expressive power
    of a language model based on the matrix factorization interpretation of language
    modeling introduced by Yang et al. (2018). Our proposed method improves the current
    state-of-the-art language model and achieves the best score on the Penn Treebank
    and WikiText-2, which are the standard benchmark datasets. Moreover, we indicate
    our proposed method contributes to application tasks: machine translation and
    headline generation.'
  address: Brussels, Belgium
  author:
  - first: Sho
    full: Sho Takase
    id: sho-takase
    last: Takase
  - first: Jun
    full: Jun Suzuki
    id: jun-suzuki
    last: Suzuki
  - first: Masaaki
    full: Masaaki Nagata
    id: masaaki-nagata
    last: Nagata
  author_string: Sho Takase, Jun Suzuki, Masaaki Nagata
  bibkey: takase-etal-2018-direct
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1489
  month: October-November
  page_first: '4599'
  page_last: '4609'
  pages: "4599\u20134609"
  paper_id: '489'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1489.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1489.jpg
  title: Direct Output Connection for a High-Rank Language Model
  title_html: Direct Output Connection for a High-Rank Language Model
  url: https://www.aclweb.org/anthology/D18-1489
  year: '2018'
D18-1490:
  abstract: "In recent years, the natural language processing community has moved\
    \ away from task-specific feature engineering, i.e., researchers discovering ad-hoc\
    \ feature representations for various tasks, in favor of general-purpose methods\
    \ that learn the input representation by themselves. However, state-of-the-art\
    \ approaches to disfluency detection in spontaneous speech transcripts currently\
    \ still depend on an array of hand-crafted features, and other representations\
    \ derived from the output of pre-existing systems such as language models or dependency\
    \ parsers. As an alternative, this paper proposes a simple yet effective model\
    \ for automatic disfluency detection, called an auto-correlational neural network\
    \ (ACNN). The model uses a convolutional neural network (CNN) and augments it\
    \ with a new auto-correlation operator at the lowest layer that can capture the\
    \ kinds of \u201Crough copy\u201D dependencies that are characteristic of repair\
    \ disfluencies in speech. In experiments, the ACNN model outperforms the baseline\
    \ CNN on a disfluency detection task with a 5% increase in f-score, which is close\
    \ to the previous best result on this task."
  address: Brussels, Belgium
  author:
  - first: Paria
    full: Paria Jamshid Lou
    id: paria-jamshid-lou
    last: Jamshid Lou
  - first: Peter
    full: Peter Anderson
    id: peter-anderson
    last: Anderson
  - first: Mark
    full: Mark Johnson
    id: mark-johnson
    last: Johnson
  author_string: Paria Jamshid Lou, Peter Anderson, Mark Johnson
  bibkey: jamshid-lou-etal-2018-disfluency
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1490
  month: October-November
  page_first: '4610'
  page_last: '4619'
  pages: "4610\u20134619"
  paper_id: '490'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1490.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1490.jpg
  title: Disfluency Detection using Auto-Correlational Neural Networks
  title_html: Disfluency Detection using Auto-Correlational Neural Networks
  url: https://www.aclweb.org/anthology/D18-1490
  year: '2018'
D18-1491:
  abstract: LSTMs are powerful tools for modeling contextual information, as evidenced
    by their success at the task of language modeling. However, modeling contexts
    in very high dimensional space can lead to poor generalizability. We introduce
    the Pyramidal Recurrent Unit (PRU), which enables learning representations in
    high dimensional space with more generalization power and fewer parameters. PRUs
    replace the linear transformation in LSTMs with more sophisticated interactions
    such as pyramidal or grouped linear transformations. This architecture gives strong
    results on word-level language modeling while reducing parameters significantly.
    In particular, PRU improves the perplexity of a recent state-of-the-art language
    model by up to 1.3 points while learning 15-20% fewer parameters. For similar
    number of model parameters, PRU outperforms all previous RNN models that exploit
    different gating mechanisms and transformations. We provide a detailed examination
    of the PRU and its behavior on the language modeling tasks. Our code is open-source
    and available at https://sacmehta.github.io/PRU/.
  address: Brussels, Belgium
  author:
  - first: Sachin
    full: Sachin Mehta
    id: sachin-mehta
    last: Mehta
  - first: Rik
    full: Rik Koncel-Kedziorski
    id: rik-koncel-kedziorski
    last: Koncel-Kedziorski
  - first: Mohammad
    full: Mohammad Rastegari
    id: mohammad-rastegari
    last: Rastegari
  - first: Hannaneh
    full: Hannaneh Hajishirzi
    id: hannaneh-hajishirzi
    last: Hajishirzi
  author_string: Sachin Mehta, Rik Koncel-Kedziorski, Mohammad Rastegari, Hannaneh
    Hajishirzi
  bibkey: mehta-etal-2018-pyramidal
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1491
  month: October-November
  page_first: '4620'
  page_last: '4630'
  pages: "4620\u20134630"
  paper_id: '491'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1491.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1491.jpg
  title: Pyramidal Recurrent Unit for Language Modeling
  title_html: Pyramidal Recurrent Unit for Language Modeling
  url: https://www.aclweb.org/anthology/D18-1491
  year: '2018'
D18-1492:
  abstract: Neural networks with tree-based sentence encoders have shown better results
    on many downstream tasks. Most of existing tree-based encoders adopt syntactic
    parsing trees as the explicit structure prior. To study the effectiveness of different
    tree structures, we replace the parsing trees with trivial trees (i.e., binary
    balanced tree, left-branching tree and right-branching tree) in the encoders.
    Though trivial trees contain no syntactic information, those encoders get competitive
    or even better results on all of the ten downstream tasks we investigated. This
    surprising result indicates that explicit syntax guidance may not be the main
    contributor to the superior performances of tree-based neural sentence modeling.
    Further analysis show that tree modeling gives better results when crucial words
    are closer to the final representation. Additional experiments give more clues
    on how to design an effective tree-based encoder. Our code is open-source and
    available at https://github.com/ExplorerFreda/TreeEnc.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1492.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1492.Attachment.zip
  author:
  - first: Haoyue
    full: Haoyue Shi
    id: haoyue-shi
    last: Shi
  - first: Hao
    full: Hao Zhou
    id: hao-zhou
    last: Zhou
  - first: Jiaze
    full: Jiaze Chen
    id: jiaze-chen
    last: Chen
  - first: Lei
    full: Lei Li
    id: lei-li
    last: Li
  author_string: Haoyue Shi, Hao Zhou, Jiaze Chen, Lei Li
  bibkey: shi-etal-2018-tree
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1492
  month: October-November
  page_first: '4631'
  page_last: '4641'
  pages: "4631\u20134641"
  paper_id: '492'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1492.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1492.jpg
  title: On Tree-Based Neural Sentence Modeling
  title_html: On Tree-Based Neural Sentence Modeling
  url: https://www.aclweb.org/anthology/D18-1492
  year: '2018'
D18-1493:
  abstract: Most language modeling methods rely on large-scale data to statistically
    learn the sequential patterns of words. In this paper, we argue that words are
    atomic language units but not necessarily atomic semantic units. Inspired by HowNet,
    we use sememes, the minimum semantic units in human languages, to represent the
    implicit semantics behind words for language modeling, named Sememe-Driven Language
    Model (SDLM). More specifically, to predict the next word, SDLM first estimates
    the sememe distribution given textual context. Afterwards, it regards each sememe
    as a distinct semantic expert, and these experts jointly identify the most probable
    senses and the corresponding word. In this way, SDLM enables language models to
    work beyond word-level manipulation to fine-grained sememe-level semantics, and
    offers us more powerful tools to fine-tune language models and improve the interpretability
    as well as the robustness of language models. Experiments on language modeling
    and the downstream application of headline generation demonstrate the significant
    effectiveness of SDLM.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1493.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1493.Attachment.pdf
  author:
  - first: Yihong
    full: Yihong Gu
    id: yihong-gu
    last: Gu
  - first: Jun
    full: Jun Yan
    id: jun-yan
    last: Yan
  - first: Hao
    full: Hao Zhu
    id: hao-zhu
    last: Zhu
  - first: Zhiyuan
    full: Zhiyuan Liu
    id: zhiyuan-liu
    last: Liu
  - first: Ruobing
    full: Ruobing Xie
    id: ruobing-xie
    last: Xie
  - first: Maosong
    full: Maosong Sun
    id: maosong-sun
    last: Sun
  - first: Fen
    full: Fen Lin
    id: fen-lin
    last: Lin
  - first: Leyu
    full: Leyu Lin
    id: leyu-lin
    last: Lin
  author_string: Yihong Gu, Jun Yan, Hao Zhu, Zhiyuan Liu, Ruobing Xie, Maosong Sun,
    Fen Lin, Leyu Lin
  bibkey: gu-etal-2018-language
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1493
  month: October-November
  page_first: '4642'
  page_last: '4651'
  pages: "4642\u20134651"
  paper_id: '493'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1493.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1493.jpg
  title: Language Modeling with Sparse Product of Sememe Experts
  title_html: Language Modeling with Sparse Product of Sememe Experts
  url: https://www.aclweb.org/anthology/D18-1493
  year: '2018'
D18-1494:
  abstract: Label-specific topics can be widely used for supporting personality psychology,
    aspect-level sentiment analysis, and cross-domain sentiment classification. To
    generate label-specific topics, several supervised topic models which adopt likelihood-driven
    objective functions have been proposed. However, it is hard for them to get a
    precise estimation on both topic discovery and supervised learning. In this study,
    we propose a supervised topic model based on the Siamese network, which can trade
    off label-specific word distributions with document-specific label distributions
    in a uniform framework. Experiments on real-world datasets validate that our model
    performs competitive in topic discovery quantitatively and qualitatively. Furthermore,
    the proposed model can effectively predict categorical or real-valued labels for
    new documents by generating word embeddings from a label-specific topical space.
  address: Brussels, Belgium
  author:
  - first: Minghui
    full: Minghui Huang
    id: minghui-huang
    last: Huang
  - first: Yanghui
    full: Yanghui Rao
    id: yanghui-rao
    last: Rao
  - first: Yuwei
    full: Yuwei Liu
    id: yuwei-liu
    last: Liu
  - first: Haoran
    full: Haoran Xie
    id: haoran-xie
    last: Xie
  - first: Fu Lee
    full: Fu Lee Wang
    id: fu-lee-wang
    last: Wang
  author_string: Minghui Huang, Yanghui Rao, Yuwei Liu, Haoran Xie, Fu Lee Wang
  bibkey: huang-etal-2018-siamese
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1494
  month: October-November
  page_first: '4652'
  page_last: '4662'
  pages: "4652\u20134662"
  paper_id: '494'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1494.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1494.jpg
  title: Siamese Network-Based Supervised Topic Modeling
  title_html: <span class="acl-fixed-case">S</span>iamese Network-Based Supervised
    Topic Modeling
  url: https://www.aclweb.org/anthology/D18-1494
  year: '2018'
D18-1495:
  abstract: Discovering the latent topics within texts has been a fundamental task
    for many applications. However, conventional topic models suffer different problems
    in different settings. The Latent Dirichlet Allocation (LDA) may not work well
    for short texts due to the data sparsity (i.e. the sparse word co-occurrence patterns
    in short documents). The Biterm Topic Model (BTM) learns topics by modeling the
    word-pairs named biterms in the whole corpus. This assumption is very strong when
    documents are long with rich topic information and do not exhibit the transitivity
    of biterms. In this paper, we propose a novel way called GraphBTM to represent
    biterms as graphs and design a Graph Convolutional Networks (GCNs) with residual
    connections to extract transitive features from biterms. To overcome the data
    sparsity of LDA and the strong assumption of BTM, we sample a fixed number of
    documents to form a mini-corpus as a sample. We also propose a dataset called
    All News extracted from 15 news publishers, in which documents are much longer
    than 20 Newsgroups. We present an amortized variational inference method for GraphBTM.
    Our method generates more coherent topics compared with previous approaches. Experiments
    show that the sampling strategy improves performance by a large margin.
  address: Brussels, Belgium
  author:
  - first: Qile
    full: Qile Zhu
    id: qile-zhu
    last: Zhu
  - first: Zheng
    full: Zheng Feng
    id: zheng-feng
    last: Feng
  - first: Xiaolin
    full: Xiaolin Li
    id: xiaolin-li
    last: Li
  author_string: Qile Zhu, Zheng Feng, Xiaolin Li
  bibkey: zhu-etal-2018-graphbtm
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1495
  month: October-November
  page_first: '4663'
  page_last: '4672'
  pages: "4663\u20134672"
  paper_id: '495'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1495.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1495.jpg
  title: 'GraphBTM: Graph Enhanced Autoencoded Variational Inference for Biterm Topic
    Model'
  title_html: '<span class="acl-fixed-case">G</span>raph<span class="acl-fixed-case">BTM</span>:
    Graph Enhanced Autoencoded Variational Inference for Biterm Topic Model'
  url: https://www.aclweb.org/anthology/D18-1495
  year: '2018'
D18-1496:
  abstract: In this paper, we propose a deep, globally normalized topic model that
    incorporates structural relationships connecting documents in socially generated
    corpora, such as online forums. Our model (1) captures discursive interactions
    along observed reply links in addition to traditional topic information, and (2)
    incorporates latent distributed representations arranged in a deep architecture,
    which enables a GPU-based mean-field inference procedure that scales efficiently
    to large data. We apply our model to a new social media dataset consisting of
    13M comments mined from the popular internet forum Reddit, a domain that poses
    significant challenges to models that do not account for relationships connecting
    user comments. We evaluate against existing methods across multiple metrics including
    perplexity and metadata prediction, and qualitatively analyze the learned interaction
    patterns.
  address: Brussels, Belgium
  author:
  - first: Akshay
    full: Akshay Srivatsan
    id: akshay-srivatsan
    last: Srivatsan
  - first: Zachary
    full: Zachary Wojtowicz
    id: zachary-wojtowicz
    last: Wojtowicz
  - first: Taylor
    full: Taylor Berg-Kirkpatrick
    id: taylor-berg-kirkpatrick
    last: Berg-Kirkpatrick
  author_string: Akshay Srivatsan, Zachary Wojtowicz, Taylor Berg-Kirkpatrick
  bibkey: srivatsan-etal-2018-modeling
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1496
  month: October-November
  page_first: '4673'
  page_last: '4682'
  pages: "4673\u20134682"
  paper_id: '496'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1496.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1496.jpg
  title: Modeling Online Discourse with Coupled Distributed Topics
  title_html: Modeling Online Discourse with Coupled Distributed Topics
  url: https://www.aclweb.org/anthology/D18-1496
  year: '2018'
D18-1497:
  abstract: We propose a method for learning disentangled representations of texts
    that code for distinct and complementary aspects, with the aim of affording efficient
    model transfer and interpretability. To induce disentangled embeddings, we propose
    an adversarial objective based on the (dis)similarity between triplets of documents
    with respect to specific aspects. Our motivating application is embedding biomedical
    abstracts describing clinical trials in a manner that disentangles the populations,
    interventions, and outcomes in a given trial. We show that our method learns representations
    that encode these clinically salient aspects, and that these can be effectively
    used to perform aspect-specific retrieval. We demonstrate that the approach generalizes
    beyond our motivating application in experiments on two multi-aspect review corpora.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1497.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1497.Attachment.pdf
  author:
  - first: Sarthak
    full: Sarthak Jain
    id: sarthak-jain
    last: Jain
  - first: Edward
    full: Edward Banner
    id: edward-banner
    last: Banner
  - first: Jan-Willem
    full: Jan-Willem van de Meent
    id: jan-willem-van-de-meent
    last: van de Meent
  - first: Iain J.
    full: Iain J. Marshall
    id: iain-marshall
    last: Marshall
  - first: Byron C.
    full: Byron C. Wallace
    id: byron-c-wallace
    last: Wallace
  author_string: Sarthak Jain, Edward Banner, Jan-Willem van de Meent, Iain J. Marshall,
    Byron C. Wallace
  bibkey: jain-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1497
  month: October-November
  page_first: '4683'
  page_last: '4693'
  pages: "4683\u20134693"
  paper_id: '497'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1497.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1497.jpg
  title: Learning Disentangled Representations of Texts with Application to Biomedical
    Abstracts
  title_html: Learning Disentangled Representations of Texts with Application to Biomedical
    Abstracts
  url: https://www.aclweb.org/anthology/D18-1497
  year: '2018'
D18-1498:
  abstract: We propose a mixture-of-experts approach for unsupervised domain adaptation
    from multiple sources. The key idea is to explicitly capture the relationship
    between a target example and different source domains. This relationship, expressed
    by a point-to-set metric, determines how to combine predictors trained on various
    domains. The metric is learned in an unsupervised fashion using meta-training.
    Experimental results on sentiment analysis and part-of-speech tagging demonstrate
    that our approach consistently outperforms multiple baselines and can robustly
    handle negative transfer.
  address: Brussels, Belgium
  author:
  - first: Jiang
    full: Jiang Guo
    id: jiang-guo
    last: Guo
  - first: Darsh
    full: Darsh Shah
    id: darsh-shah
    last: Shah
  - first: Regina
    full: Regina Barzilay
    id: regina-barzilay
    last: Barzilay
  author_string: Jiang Guo, Darsh Shah, Regina Barzilay
  bibkey: guo-etal-2018-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1498
  month: October-November
  page_first: '4694'
  page_last: '4703'
  pages: "4694\u20134703"
  paper_id: '498'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1498.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1498.jpg
  title: Multi-Source Domain Adaptation with Mixture of Experts
  title_html: Multi-Source Domain Adaptation with Mixture of Experts
  url: https://www.aclweb.org/anthology/D18-1498
  year: '2018'
D18-1499:
  abstract: It has been argued that humans rapidly adapt their lexical and syntactic
    expectations to match the statistics of the current linguistic context. We provide
    further support to this claim by showing that the addition of a simple adaptation
    mechanism to a neural language model improves our predictions of human reading
    times compared to a non-adaptive model. We analyze the performance of the model
    on controlled materials from psycholinguistic experiments and show that it adapts
    not only to lexical items but also to abstract syntactic structures.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1499.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1499.Attachment.zip
  - filename: https://vimeo.com/306153668
    type: video
    url: https://vimeo.com/306153668
  author:
  - first: Marten
    full: Marten van Schijndel
    id: marten-van-schijndel
    last: van Schijndel
  - first: Tal
    full: Tal Linzen
    id: tal-linzen
    last: Linzen
  author_string: Marten van Schijndel, Tal Linzen
  bibkey: van-schijndel-linzen-2018-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1499
  month: October-November
  page_first: '4704'
  page_last: '4710'
  pages: "4704\u20134710"
  paper_id: '499'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1499.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1499.jpg
  title: A Neural Model of Adaptation in Reading
  title_html: A Neural Model of Adaptation in Reading
  url: https://www.aclweb.org/anthology/D18-1499
  year: '2018'
D18-1500:
  abstract: "Interpreting the performance of deep learning models beyond test set\
    \ accuracy is challenging. Characteristics of individual data points are often\
    \ not considered during evaluation, and each data point is treated equally. In\
    \ this work we examine the impact of a test set question\u2019s difficulty to\
    \ determine if there is a relationship between difficulty and performance. We\
    \ model difficulty using well-studied psychometric methods on human response patterns.\
    \ Experiments on Natural Language Inference (NLI) and Sentiment Analysis (SA)\
    \ show that the likelihood of answering a question correctly is impacted by the\
    \ question\u2019s difficulty. In addition, as DNNs are trained on larger datasets\
    \ easy questions start to have a higher probability of being answered correctly\
    \ than harder questions."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1500.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1500.Attachment.zip
  - filename: https://vimeo.com/306154181
    type: video
    url: https://vimeo.com/306154181
  author:
  - first: John P.
    full: John P. Lalor
    id: john-p-lalor
    last: Lalor
  - first: Hao
    full: Hao Wu
    id: hao-wu
    last: Wu
  - first: Tsendsuren
    full: Tsendsuren Munkhdalai
    id: tsendsuren-munkhdalai
    last: Munkhdalai
  - first: Hong
    full: Hong Yu
    id: hong-yu
    last: Yu
  author_string: John P. Lalor, Hao Wu, Tsendsuren Munkhdalai, Hong Yu
  bibkey: lalor-etal-2018-understanding
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1500
  month: October-November
  page_first: '4711'
  page_last: '4716'
  pages: "4711\u20134716"
  paper_id: '500'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1500.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1500.jpg
  title: 'Understanding Deep Learning Performance through an Examination of Test Set
    Difficulty: A Psychometric Case Study'
  title_html: 'Understanding Deep Learning Performance through an Examination of Test
    Set Difficulty: A Psychometric Case Study'
  url: https://www.aclweb.org/anthology/D18-1500
  year: '2018'
D18-1501:
  abstract: "We investigate neural models\u2019 ability to capture lexicosyntactic\
    \ inferences: inferences triggered by the interaction of lexical and syntactic\
    \ information. We take the task of event factuality prediction as a case study\
    \ and build a factuality judgment dataset for all English clause-embedding verbs\
    \ in various syntactic contexts. We use this dataset, which we make publicly available,\
    \ to probe the behavior of current state-of-the-art neural systems, showing that\
    \ these systems make certain systematic errors that are clearly visible through\
    \ the lens of factuality prediction."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1501.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1501.Attachment.pdf
  - filename: https://vimeo.com/306154470
    type: video
    url: https://vimeo.com/306154470
  author:
  - first: Aaron Steven
    full: Aaron Steven White
    id: aaron-steven-white
    last: White
  - first: Rachel
    full: Rachel Rudinger
    id: rachel-rudinger
    last: Rudinger
  - first: Kyle
    full: Kyle Rawlins
    id: kyle-rawlins
    last: Rawlins
  - first: Benjamin
    full: Benjamin Van Durme
    id: benjamin-van-durme
    last: Van Durme
  author_string: Aaron Steven White, Rachel Rudinger, Kyle Rawlins, Benjamin Van Durme
  bibkey: white-etal-2018-lexicosyntactic
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1501
  month: October-November
  page_first: '4717'
  page_last: '4724'
  pages: "4717\u20134724"
  paper_id: '501'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1501.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1501.jpg
  title: Lexicosyntactic Inference in Neural Models
  title_html: Lexicosyntactic Inference in Neural Models
  url: https://www.aclweb.org/anthology/D18-1501
  year: '2018'
D18-1502:
  abstract: In this paper, we propose a new approach to employ the fixed-size ordinally-forgetting
    encoding (FOFE) (Zhang et al., 2015b) in neural languages modelling, called dual-FOFE.
    The main idea of dual-FOFE is that it allows to use two different forgetting factors
    so that it can avoid the trade-off in choosing either a small or large values
    for the single forgetting factor. In our experiments, we have compared the dual-FOFE
    based neural network language models (NNLM) against the original FOFE counterparts
    and various traditional NNLMs. Our results on the challenging Google Billion word
    corpus show that both FOFE and dual FOFE yield very strong performance while significantly
    reducing the computational complexity over other NNLMs. Furthermore, the proposed
    dual-FOFE method further gives over 10% improvement in perplexity over the original
    FOFE model.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306154936
    type: video
    url: https://vimeo.com/306154936
  author:
  - first: Sedtawut
    full: Sedtawut Watcharawittayakul
    id: sedtawut-watcharawittayakul
    last: Watcharawittayakul
  - first: Mingbin
    full: Mingbin Xu
    id: mingbin-xu
    last: Xu
  - first: Hui
    full: Hui Jiang
    id: hui-jiang
    last: Jiang
  author_string: Sedtawut Watcharawittayakul, Mingbin Xu, Hui Jiang
  bibkey: watcharawittayakul-etal-2018-dual
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1502
  month: October-November
  page_first: '4725'
  page_last: '4730'
  pages: "4725\u20134730"
  paper_id: '502'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1502.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1502.jpg
  title: Dual Fixed-Size Ordinally Forgetting Encoding (FOFE) for Competitive Neural
    Language Models
  title_html: Dual Fixed-Size Ordinally Forgetting Encoding (<span class="acl-fixed-case">FOFE</span>)
    for Competitive Neural Language Models
  url: https://www.aclweb.org/anthology/D18-1502
  year: '2018'
D18-1503:
  abstract: "Recent work has shown that recurrent neural networks (RNNs) can implicitly\
    \ capture and exploit hierarchical information when trained to solve common natural\
    \ language processing tasks (Blevins et al., 2018) such as language modeling (Linzen\
    \ et al., 2016; Gulordava et al., 2018) and neural machine translation (Shi et\
    \ al., 2016). In contrast, the ability to model structured data with non-recurrent\
    \ neural networks has received little attention despite their success in many\
    \ NLP tasks (Gehring et al., 2017; Vaswani et al., 2017). In this work, we compare\
    \ the two architectures\u2014recurrent versus non-recurrent\u2014with respect\
    \ to their ability to model hierarchical structure and find that recurrency is\
    \ indeed important for this purpose. The code and data used in our experiments\
    \ is available at https://github.com/ ketranm/fan_vs_rnn"
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306155520
    type: video
    url: https://vimeo.com/306155520
  author:
  - first: Ke
    full: Ke Tran
    id: ke-m-tran
    last: Tran
  - first: Arianna
    full: Arianna Bisazza
    id: arianna-bisazza
    last: Bisazza
  - first: Christof
    full: Christof Monz
    id: christof-monz
    last: Monz
  author_string: Ke Tran, Arianna Bisazza, Christof Monz
  bibkey: tran-etal-2018-importance
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1503
  month: October-November
  page_first: '4731'
  page_last: '4736'
  pages: "4731\u20134736"
  paper_id: '503'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1503.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1503.jpg
  title: The Importance of Being Recurrent for Modeling Hierarchical Structure
  title_html: The Importance of Being Recurrent for Modeling Hierarchical Structure
  url: https://www.aclweb.org/anthology/D18-1503
  year: '2018'
D18-1504:
  abstract: Targeted sentiment analysis (TSA) aims at extracting targets and classifying
    their sentiment classes. Previous works only exploit word embeddings as features
    and do not explore more potentials of neural networks when jointly learning the
    two tasks. In this paper, we carefully design the hierarchical stack bidirectional
    gated recurrent units (HSBi-GRU) model to learn abstract features for both tasks,
    and we propose a HSBi-GRU based joint model which allows the target label to have
    influence on their sentiment label. Experimental results on two datasets show
    that our joint learning model can outperform other baselines and demonstrate the
    effectiveness of HSBi-GRU in learning abstract features.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306135717
    type: video
    url: https://vimeo.com/306135717
  author:
  - first: Dehong
    full: Dehong Ma
    id: dehong-ma
    last: Ma
  - first: Sujian
    full: Sujian Li
    id: sujian-li
    last: Li
  - first: Houfeng
    full: Houfeng Wang
    id: houfeng-wang
    last: Wang
  author_string: Dehong Ma, Sujian Li, Houfeng Wang
  bibkey: ma-etal-2018-joint
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1504
  month: October-November
  page_first: '4737'
  page_last: '4742'
  pages: "4737\u20134742"
  paper_id: '504'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1504.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1504.jpg
  title: Joint Learning for Targeted Sentiment Analysis
  title_html: Joint Learning for Targeted Sentiment Analysis
  url: https://www.aclweb.org/anthology/D18-1504
  year: '2018'
D18-1505:
  abstract: "We analyze the performance of different sentiment classification models\
    \ on syntactically complex inputs like A-but-B sentences. The first contribution\
    \ of this analysis addresses reproducible research: to meaningfully compare different\
    \ models, their accuracies must be averaged over far more random seeds than what\
    \ has traditionally been reported. With proper averaging in place, we notice that\
    \ the distillation model described in Hu et al. (2016), which incorporates explicit\
    \ logic rules for sentiment classification, is ineffective. In contrast, using\
    \ contextualized ELMo embeddings (Peters et al., 2018a) instead of logic rules\
    \ yields significantly better performance. Additionally, we provide analysis and\
    \ visualizations that demonstrate ELMo\u2019s ability to implicitly learn logic\
    \ rules. Finally, a crowdsourced analysis reveals how ELMo outperforms baseline\
    \ models even on sentences with ambiguous sentiment labels."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1505.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1505.Attachment.zip
  - filename: https://vimeo.com/306136412
    type: video
    url: https://vimeo.com/306136412
  author:
  - first: Kalpesh
    full: Kalpesh Krishna
    id: kalpesh-krishna
    last: Krishna
  - first: Preethi
    full: Preethi Jyothi
    id: preethi-jyothi
    last: Jyothi
  - first: Mohit
    full: Mohit Iyyer
    id: mohit-iyyer
    last: Iyyer
  author_string: Kalpesh Krishna, Preethi Jyothi, Mohit Iyyer
  bibkey: krishna-etal-2018-revisiting
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1505
  month: October-November
  page_first: '4743'
  page_last: '4751'
  pages: "4743\u20134751"
  paper_id: '505'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1505.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1505.jpg
  title: Revisiting the Importance of Encoding Logic Rules in Sentiment Classification
  title_html: Revisiting the Importance of Encoding Logic Rules in Sentiment Classification
  url: https://www.aclweb.org/anthology/D18-1505
  year: '2018'
D18-1506:
  abstract: Emotion cause analysis has been a key topic in natural language processing.
    Existing methods ignore the contexts around the emotion word which can provide
    an emotion cause clue. Meanwhile, the clauses in a document play different roles
    on stimulating a certain emotion, depending on their content relevance. Therefore,
    we propose a co-attention neural network model for emotion cause analysis with
    emotional context awareness. The method encodes the clauses with a co-attention
    based bi-directional long short-term memory into high-level input representations,
    which are further fed into a convolutional layer for emotion cause analysis. Experimental
    results show that our approach outperforms the state-of-the-art baseline methods.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306136988
    type: video
    url: https://vimeo.com/306136988
  author:
  - first: Xiangju
    full: Xiangju Li
    id: xiangju-li
    last: Li
  - first: Kaisong
    full: Kaisong Song
    id: kaisong-song
    last: Song
  - first: Shi
    full: Shi Feng
    id: shi-feng
    last: Feng
  - first: Daling
    full: Daling Wang
    id: daling-wang
    last: Wang
  - first: Yifei
    full: Yifei Zhang
    id: yifei-zhang
    last: Zhang
  author_string: Xiangju Li, Kaisong Song, Shi Feng, Daling Wang, Yifei Zhang
  bibkey: li-etal-2018-co
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1506
  month: October-November
  page_first: '4752'
  page_last: '4757'
  pages: "4752\u20134757"
  paper_id: '506'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1506.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1506.jpg
  title: A Co-Attention Neural Network Model for Emotion Cause Analysis with Emotional
    Context Awareness
  title_html: A Co-Attention Neural Network Model for Emotion Cause Analysis with
    Emotional Context Awareness
  url: https://www.aclweb.org/anthology/D18-1506
  year: '2018'
D18-1507:
  abstract: 'Computational detection and understanding of empathy is an important
    factor in advancing human-computer interaction. Yet to date, text-based empathy
    prediction has the following major limitations: It underestimates the psychological
    complexity of the phenomenon, adheres to a weak notion of ground truth where empathic
    states are ascribed by third parties, and lacks a shared corpus. In contrast,
    this contribution presents the first publicly available gold standard for empathy
    prediction. It is constructed using a novel annotation methodology which reliably
    captures empathy assessments by the writer of a statement using multi-item scales.
    This is also the first computational work distinguishing between multiple forms
    of empathy, empathic concern, and personal distress, as recognized throughout
    psychology. Finally, we present experimental results for three different predictive
    models, of which a CNN performs the best.'
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306137544
    type: video
    url: https://vimeo.com/306137544
  author:
  - first: Sven
    full: Sven Buechel
    id: sven-buechel
    last: Buechel
  - first: Anneke
    full: Anneke Buffone
    id: anneke-buffone
    last: Buffone
  - first: Barry
    full: Barry Slaff
    id: barry-slaff
    last: Slaff
  - first: Lyle
    full: Lyle Ungar
    id: lyle-ungar
    last: Ungar
  - first: "Jo\xE3o"
    full: "Jo\xE3o Sedoc"
    id: joao-sedoc
    last: Sedoc
  author_string: "Sven Buechel, Anneke Buffone, Barry Slaff, Lyle Ungar, Jo\xE3o Sedoc"
  bibkey: buechel-etal-2018-modeling
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1507
  month: October-November
  page_first: '4758'
  page_last: '4765'
  pages: "4758\u20134765"
  paper_id: '507'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1507.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1507.jpg
  title: Modeling Empathy and Distress in Reaction to News Stories
  title_html: Modeling Empathy and Distress in Reaction to News Stories
  url: https://www.aclweb.org/anthology/D18-1507
  year: '2018'
D18-1508:
  abstract: Human language has evolved towards newer forms of communication such as
    social media, where emojis (i.e., ideograms bearing a visual meaning) play a key
    role. While there is an increasing body of work aimed at the computational modeling
    of emoji semantics, there is currently little understanding about what makes a
    computational model represent or predict a given emoji in a certain way. In this
    paper we propose a label-wise attention mechanism with which we attempt to better
    understand the nuances underlying emoji prediction. In addition to advantages
    in terms of interpretability, we show that our proposed architecture improves
    over standard baselines in emoji prediction, and does particularly well when predicting
    infrequent emojis.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306138266
    type: video
    url: https://vimeo.com/306138266
  author:
  - first: Francesco
    full: Francesco Barbieri
    id: francesco-barbieri
    last: Barbieri
  - first: Luis
    full: Luis Espinosa-Anke
    id: luis-espinosa-anke
    last: Espinosa-Anke
  - first: Jose
    full: Jose Camacho-Collados
    id: jose-camacho-collados
    last: Camacho-Collados
  - first: Steven
    full: Steven Schockaert
    id: steven-schockaert
    last: Schockaert
  - first: Horacio
    full: Horacio Saggion
    id: horacio-saggion
    last: Saggion
  author_string: Francesco Barbieri, Luis Espinosa-Anke, Jose Camacho-Collados, Steven
    Schockaert, Horacio Saggion
  bibkey: barbieri-etal-2018-interpretable
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1508
  month: October-November
  page_first: '4766'
  page_last: '4771'
  pages: "4766\u20134771"
  paper_id: '508'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1508.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1508.jpg
  title: Interpretable Emoji Prediction via Label-Wise Attention LSTMs
  title_html: Interpretable Emoji Prediction via Label-Wise Attention <span class="acl-fixed-case">LSTM</span>s
  url: https://www.aclweb.org/anthology/D18-1508
  year: '2018'
D18-1509:
  abstract: Recent advances in Neural Machine Translation (NMT) show that adding syntactic
    information to NMT systems can improve the quality of their translations. Most
    existing work utilizes some specific types of linguistically-inspired tree structures,
    like constituency and dependency parse trees. This is often done via a standard
    RNN decoder that operates on a linearized target tree structure. However, it is
    an open question of what specific linguistic formalism, if any, is the best structural
    representation for NMT. In this paper, we (1) propose an NMT model that can naturally
    generate the topology of an arbitrary tree structure on the target side, and (2)
    experiment with various target tree structures. Our experiments show the surprising
    result that our model delivers the best improvements with balanced binary trees
    constructed without any linguistic knowledge; this model outperforms standard
    seq2seq models by up to 2.1 BLEU points, and other methods for incorporating target-side
    syntax by up to 0.7 BLEU.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1509.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1509.Attachment.pdf
  - filename: https://vimeo.com/306166768
    type: video
    url: https://vimeo.com/306166768
  author:
  - first: Xinyi
    full: Xinyi Wang
    id: xinyi-wang
    last: Wang
  - first: Hieu
    full: Hieu Pham
    id: hieu-pham
    last: Pham
  - first: Pengcheng
    full: Pengcheng Yin
    id: pengcheng-yin
    last: Yin
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  author_string: Xinyi Wang, Hieu Pham, Pengcheng Yin, Graham Neubig
  bibkey: wang-etal-2018-tree
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1509
  month: October-November
  page_first: '4772'
  page_last: '4777'
  pages: "4772\u20134777"
  paper_id: '509'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1509.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1509.jpg
  title: A Tree-based Decoder for Neural Machine Translation
  title_html: A Tree-based Decoder for Neural Machine Translation
  url: https://www.aclweb.org/anthology/D18-1509
  year: '2018'
D18-1510:
  abstract: Neural machine translation (NMT) models are usually trained with the word-level
    loss using the teacher forcing algorithm, which not only evaluates the translation
    improperly but also suffers from exposure bias. Sequence-level training under
    the reinforcement framework can mitigate the problems of the word-level loss,
    but its performance is unstable due to the high variance of the gradient estimation.
    On these grounds, we present a method with a differentiable sequence-level training
    objective based on probabilistic n-gram matching which can avoid the reinforcement
    framework. In addition, this method performs greedy search in the training which
    uses the predicted words as context just as at inference to alleviate the problem
    of exposure bias. Experiment results on the NIST Chinese-to-English translation
    tasks show that our method significantly outperforms the reinforcement-based algorithms
    and achieves an improvement of 1.5 BLEU points on average over a strong baseline
    system.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306167593
    type: video
    url: https://vimeo.com/306167593
  author:
  - first: Chenze
    full: Chenze Shao
    id: chenze-shao
    last: Shao
  - first: Xilin
    full: Xilin Chen
    id: xilin-chen
    last: Chen
  - first: Yang
    full: Yang Feng
    id: yang-feng
    last: Feng
  author_string: Chenze Shao, Xilin Chen, Yang Feng
  bibkey: shao-etal-2018-greedy
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1510
  month: October-November
  page_first: '4778'
  page_last: '4784'
  pages: "4778\u20134784"
  paper_id: '510'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1510.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1510.jpg
  title: Greedy Search with Probabilistic N-gram Matching for Neural Machine Translation
  title_html: Greedy Search with Probabilistic N-gram Matching for Neural Machine
    Translation
  url: https://www.aclweb.org/anthology/D18-1510
  year: '2018'
D18-1511:
  abstract: "In Neural Machine Translation (NMT), the decoder can capture the features\
    \ of the entire prediction history with neural connections and representations.\
    \ This means that partial hypotheses with different prefixes will be regarded\
    \ differently no matter how similar they are. However, this might be inefficient\
    \ since some partial hypotheses can contain only local differences that will not\
    \ influence future predictions. In this work, we introduce recombination in NMT\
    \ decoding based on the concept of the \u201Cequivalence\u201D of partial hypotheses.\
    \ Heuristically, we use a simple n-gram suffix based equivalence function and\
    \ adapt it into beam search decoding. Through experiments on large-scale Chinese-to-English\
    \ and English-to-Germen translation tasks, we show that the proposed method can\
    \ obtain similar translation quality with a smaller beam size, making NMT decoding\
    \ more efficient."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1511.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1511.Attachment.zip
  - filename: https://vimeo.com/306168250
    type: video
    url: https://vimeo.com/306168250
  author:
  - first: Zhisong
    full: Zhisong Zhang
    id: zhisong-zhang
    last: Zhang
  - first: Rui
    full: Rui Wang
    id: rui-wang
    last: Wang
  - first: Masao
    full: Masao Utiyama
    id: masao-utiyama
    last: Utiyama
  - first: Eiichiro
    full: Eiichiro Sumita
    id: eiichiro-sumita
    last: Sumita
  - first: Hai
    full: Hai Zhao
    id: hai-zhao
    last: Zhao
  author_string: Zhisong Zhang, Rui Wang, Masao Utiyama, Eiichiro Sumita, Hai Zhao
  bibkey: zhang-etal-2018-exploring
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1511
  month: October-November
  page_first: '4785'
  page_last: '4790'
  pages: "4785\u20134790"
  paper_id: '511'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1511.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1511.jpg
  title: Exploring Recombination for Efficient Decoding of Neural Machine Translation
  title_html: Exploring Recombination for Efficient Decoding of Neural Machine Translation
  url: https://www.aclweb.org/anthology/D18-1511
  year: '2018'
D18-1512:
  abstract: "Recent research suggests that neural machine translation achieves parity\
    \ with professional human translation on the WMT Chinese\u2013English news translation\
    \ task. We empirically test this claim with alternative evaluation protocols,\
    \ contrasting the evaluation of single sentences and entire documents. In a pairwise\
    \ ranking experiment, human raters assessing adequacy and fluency show a stronger\
    \ preference for human over machine translation when evaluating documents as compared\
    \ to isolated sentences. Our findings emphasise the need to shift towards document-level\
    \ evaluation as machine translation improves to the degree that errors which are\
    \ hard or impossible to spot at the sentence-level become decisive in discriminating\
    \ quality of different translation outputs."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1512.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1512.Attachment.pdf
  - filename: https://vimeo.com/306169001
    type: video
    url: https://vimeo.com/306169001
  author:
  - first: Samuel
    full: "Samuel L\xE4ubli"
    id: samuel-laubli
    last: "L\xE4ubli"
  - first: Rico
    full: Rico Sennrich
    id: rico-sennrich
    last: Sennrich
  - first: Martin
    full: Martin Volk
    id: martin-volk
    last: Volk
  author_string: "Samuel L\xE4ubli, Rico Sennrich, Martin Volk"
  bibkey: laubli-etal-2018-machine
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1512
  month: October-November
  page_first: '4791'
  page_last: '4796'
  pages: "4791\u20134796"
  paper_id: '512'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1512.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1512.jpg
  title: Has Machine Translation Achieved Human Parity? A Case for Document-level
    Evaluation
  title_html: Has Machine Translation Achieved Human Parity? A Case for Document-level
    Evaluation
  url: https://www.aclweb.org/anthology/D18-1512
  year: '2018'
D18-1513:
  abstract: We compare the performance of the APT and AutoPRF metrics for pronoun
    translation against a manually annotated dataset comprising human judgements as
    to the correctness of translations of the PROTEST test suite. Although there is
    some correlation with the human judgements, a range of issues limit the performance
    of the automated metrics. Instead, we recommend the use of semi-automatic metrics
    and test suites in place of fully automatic metrics.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306169819
    type: video
    url: https://vimeo.com/306169819
  author:
  - first: Liane
    full: Liane Guillou
    id: liane-guillou
    last: Guillou
  - first: Christian
    full: Christian Hardmeier
    id: christian-hardmeier
    last: Hardmeier
  author_string: Liane Guillou, Christian Hardmeier
  bibkey: guillou-hardmeier-2018-automatic
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1513
  month: October-November
  page_first: '4797'
  page_last: '4802'
  pages: "4797\u20134802"
  paper_id: '513'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1513.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1513.jpg
  title: Automatic Reference-Based Evaluation of Pronoun Translation Misses the Point
  title_html: Automatic Reference-Based Evaluation of Pronoun Translation Misses the
    Point
  url: https://www.aclweb.org/anthology/D18-1513
  year: '2018'
D18-1514:
  abstract: We present a Few-Shot Relation Classification Dataset (dataset), consisting
    of 70, 000 sentences on 100 relations derived from Wikipedia and annotated by
    crowdworkers. The relation of each sentence is first recognized by distant supervision
    methods, and then filtered by crowdworkers. We adapt the most recent state-of-the-art
    few-shot learning methods for relation classification and conduct thorough evaluation
    of these methods. Empirical results show that even the most competitive few-shot
    learning models struggle on this task, especially as compared with humans. We
    also show that a range of different reasoning skills are needed to solve our task.
    These results indicate that few-shot relation classification remains an open problem
    and still requires further research. Our detailed analysis points multiple directions
    for future research.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306124333
    type: video
    url: https://vimeo.com/306124333
  author:
  - first: Xu
    full: Xu Han
    id: xu-han
    last: Han
  - first: Hao
    full: Hao Zhu
    id: hao-zhu
    last: Zhu
  - first: Pengfei
    full: Pengfei Yu
    id: pengfei-yu
    last: Yu
  - first: Ziyun
    full: Ziyun Wang
    id: ziyun-wang
    last: Wang
  - first: Yuan
    full: Yuan Yao
    id: yuan-yao
    last: Yao
  - first: Zhiyuan
    full: Zhiyuan Liu
    id: zhiyuan-liu
    last: Liu
  - first: Maosong
    full: Maosong Sun
    id: maosong-sun
    last: Sun
  author_string: Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, Maosong
    Sun
  bibkey: han-etal-2018-fewrel
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1514
  month: October-November
  page_first: '4803'
  page_last: '4809'
  pages: "4803\u20134809"
  paper_id: '514'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1514.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1514.jpg
  title: 'FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset
    with State-of-the-Art Evaluation'
  title_html: '<span class="acl-fixed-case">F</span>ew<span class="acl-fixed-case">R</span>el:
    A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art
    Evaluation'
  url: https://www.aclweb.org/anthology/D18-1514
  year: '2018'
D18-1515:
  abstract: "The best systems at the SemEval-16 and SemEval-17 community question\
    \ answering shared tasks \u2013 a task that amounts to question relevancy ranking\
    \ \u2013 involve complex pipelines and manual feature engineering. Despite this,\
    \ many of these still fail at beating the IR baseline, i.e., the rankings provided\
    \ by Google\u2019s search engine. We present a strong baseline for question relevancy\
    \ ranking by training a simple multi-task feed forward network on a bag of 14\
    \ distance measures for the input question pair. This baseline model, which is\
    \ fast to train and uses only language-independent features, outperforms the best\
    \ shared task systems on the task of retrieving relevant previously asked questions."
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306167593
    type: video
    url: https://vimeo.com/306167593
  author:
  - first: Ana
    full: Ana Gonzalez
    id: ana-gonzalez-ledesma
    last: Gonzalez
  - first: Isabelle
    full: Isabelle Augenstein
    id: isabelle-augenstein
    last: Augenstein
  - first: Anders
    full: "Anders S\xF8gaard"
    id: anders-sogaard
    last: "S\xF8gaard"
  author_string: "Ana Gonzalez, Isabelle Augenstein, Anders S\xF8gaard"
  bibkey: gonzalez-etal-2018-strong
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1515
  month: October-November
  page_first: '4810'
  page_last: '4815'
  pages: "4810\u20134815"
  paper_id: '515'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1515.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1515.jpg
  title: A strong baseline for question relevancy ranking
  title_html: A strong baseline for question relevancy ranking
  url: https://www.aclweb.org/anthology/D18-1515
  year: '2018'
D18-1516:
  abstract: 'Research on link prediction in knowledge graphs has mainly focused on
    static multi-relational data. In this work we consider temporal knowledge graphs
    where relations between entities may only hold for a time interval or a specific
    point in time. In line with previous work on static knowledge graphs, we propose
    to address this problem by learning latent entity and relation type representations.
    To incorporate temporal information, we utilize recurrent neural networks to learn
    time-aware representations of relation types which can be used in conjunction
    with existing latent factorization methods. The proposed approach is shown to
    be robust to common challenges in real-world KGs: the sparsity and heterogeneity
    of temporal expressions. Experiments show the benefits of our approach on four
    temporal KGs. The data sets are available under a permissive BSD-3 license.'
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306125393
    type: video
    url: https://vimeo.com/306125393
  author:
  - first: Alberto
    full: "Alberto Garc\xEDa-Dur\xE1n"
    id: alberto-garcia-duran
    last: "Garc\xEDa-Dur\xE1n"
  - first: Sebastijan
    full: "Sebastijan Duman\u010Di\u0107"
    id: sebastijan-dumancic
    last: "Duman\u010Di\u0107"
  - first: Mathias
    full: Mathias Niepert
    id: mathias-niepert
    last: Niepert
  author_string: "Alberto Garc\xEDa-Dur\xE1n, Sebastijan Duman\u010Di\u0107, Mathias\
    \ Niepert"
  bibkey: garcia-duran-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1516
  month: October-November
  page_first: '4816'
  page_last: '4821'
  pages: "4816\u20134821"
  paper_id: '516'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1516.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1516.jpg
  title: Learning Sequence Encoders for Temporal Knowledge Graph Completion
  title_html: Learning Sequence Encoders for Temporal Knowledge Graph Completion
  url: https://www.aclweb.org/anthology/D18-1516
  year: '2018'
D18-1517:
  abstract: Event detection (ED) and word sense disambiguation (WSD) are two similar
    tasks in that they both involve identifying the classes (i.e. event types or word
    senses) of some word in a given sentence. It is thus possible to extract the knowledge
    hidden in the data for WSD, and utilize it to improve the performance on ED. In
    this work, we propose a method to transfer the knowledge learned on WSD to ED
    by matching the neural representations learned for the two tasks. Our experiments
    on two widely used datasets for ED demonstrate the effectiveness of the proposed
    method.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306125936
    type: video
    url: https://vimeo.com/306125936
  author:
  - first: Weiyi
    full: Weiyi Lu
    id: weiyi-liu
    last: Lu
  - first: Thien Huu
    full: Thien Huu Nguyen
    id: thien-huu-nguyen
    last: Nguyen
  author_string: Weiyi Lu, Thien Huu Nguyen
  bibkey: lu-nguyen-2018-similar
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1517
  month: October-November
  page_first: '4822'
  page_last: '4828'
  pages: "4822\u20134828"
  paper_id: '517'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1517.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1517.jpg
  title: 'Similar but not the Same: Word Sense Disambiguation Improves Event Detection
    via Neural Representation Matching'
  title_html: 'Similar but not the Same: Word Sense Disambiguation Improves Event
    Detection via Neural Representation Matching'
  url: https://www.aclweb.org/anthology/D18-1517
  year: '2018'
D18-1518:
  abstract: In this work, we present a word embedding model that learns cross-sentence
    dependency for improving end-to-end co-reference resolution (E2E-CR). While the
    traditional E2E-CR model generates word representations by running long short-term
    memory (LSTM) recurrent neural networks on each sentence of an input article or
    conversation separately, we propose linear sentence linking and attentional sentence
    linking models to learn cross-sentence dependency. Both sentence linking strategies
    enable the LSTMs to make use of valuable information from context sentences while
    calculating the representation of the current input word. With this approach,
    the LSTMs learn word embeddings considering knowledge not only from the current
    sentence but also from the entire input document. Experiments show that learning
    cross-sentence dependency enriches information contained by the word representations,
    and improves the performance of the co-reference resolution model compared with
    our baseline.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306126460
    type: video
    url: https://vimeo.com/306126460
  author:
  - first: Hongyin
    full: Hongyin Luo
    id: hongyin-luo
    last: Luo
  - first: Jim
    full: Jim Glass
    id: jim-glass
    last: Glass
  author_string: Hongyin Luo, Jim Glass
  bibkey: luo-glass-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1518
  month: October-November
  page_first: '4829'
  page_last: '4833'
  pages: "4829\u20134833"
  paper_id: '518'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1518.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1518.jpg
  title: Learning Word Representations with Cross-Sentence Dependency for End-to-End
    Co-reference Resolution
  title_html: Learning Word Representations with Cross-Sentence Dependency for End-to-End
    Co-reference Resolution
  url: https://www.aclweb.org/anthology/D18-1518
  year: '2018'
D18-1519:
  abstract: Lexicon relation extraction given distributional representation of words
    is an important topic in NLP. We observe that the state-of-the-art projection-based
    methods cannot be generalized to handle unseen hypernyms. We propose to analyze
    it in the perspective of pollution and construct the corresponding indicator to
    measure it. We propose a word relation autoencoder (WRAE) model to address the
    challenge. Experiments on several hypernym-like lexicon datasets show that our
    model outperforms the competitors significantly.
  address: Brussels, Belgium
  author:
  - first: Hong-You
    full: Hong-You Chen
    id: hong-you-chen
    last: Chen
  - first: Cheng-Syuan
    full: Cheng-Syuan Lee
    id: cheng-syuan-lee
    last: Lee
  - first: Keng-Te
    full: Keng-Te Liao
    id: keng-te-liao
    last: Liao
  - first: Shou-De
    full: Shou-De Lin
    id: shou-de-lin
    last: Lin
  author_string: Hong-You Chen, Cheng-Syuan Lee, Keng-Te Liao, Shou-De Lin
  bibkey: chen-etal-2018-word
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1519
  month: October-November
  page_first: '4834'
  page_last: '4839'
  pages: "4834\u20134839"
  paper_id: '519'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1519.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1519.jpg
  title: Word Relation Autoencoder for Unseen Hypernym Extraction Using Word Embeddings
  title_html: Word Relation Autoencoder for Unseen Hypernym Extraction Using Word
    Embeddings
  url: https://www.aclweb.org/anthology/D18-1519
  year: '2018'
D18-1520:
  abstract: In this paper, we propose a simple method for refining pretrained word
    embeddings using layer-wise relevance propagation. Given a target semantic representation
    one would like word vectors to reflect, our method first trains the mapping between
    the original word vectors and the target representation using a neural network.
    Estimated target values are then propagated backward toward word vectors, and
    a relevance score is computed for each dimension of word vectors. Finally, the
    relevance score vectors are used to refine the original word vectors so that they
    are projected into the subspace that reflects the information relevant to the
    target representation. The evaluation experiment using binary classification of
    word pairs demonstrates that the refined vectors by our method achieve the higher
    performance than the original vectors.
  address: Brussels, Belgium
  author:
  - first: Akira
    full: Akira Utsumi
    id: akira-utsumi
    last: Utsumi
  author_string: Akira Utsumi
  bibkey: utsumi-2018-refining
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1520
  month: October-November
  page_first: '4840'
  page_last: '4846'
  pages: "4840\u20134846"
  paper_id: '520'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1520.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1520.jpg
  title: Refining Pretrained Word Embeddings Using Layer-wise Relevance Propagation
  title_html: Refining Pretrained Word Embeddings Using Layer-wise Relevance Propagation
  url: https://www.aclweb.org/anthology/D18-1520
  year: '2018'
D18-1521:
  abstract: Word embedding models have become a fundamental component in a wide range
    of Natural Language Processing (NLP) applications. However, embeddings trained
    on human-generated corpora have been demonstrated to inherit strong gender stereotypes
    that reflect social constructs. To address this concern, in this paper, we propose
    a novel training procedure for learning gender-neutral word embeddings. Our approach
    aims to preserve gender information in certain dimensions of word vectors while
    compelling other dimensions to be free of gender influence. Based on the proposed
    method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative
    and qualitative experiments demonstrate that GN-GloVe successfully isolates gender
    information without sacrificing the functionality of the embedding model.
  address: Brussels, Belgium
  author:
  - first: Jieyu
    full: Jieyu Zhao
    id: jieyu-zhao
    last: Zhao
  - first: Yichao
    full: Yichao Zhou
    id: yichao-zhou
    last: Zhou
  - first: Zeyu
    full: Zeyu Li
    id: zeyu-li
    last: Li
  - first: Wei
    full: Wei Wang
    id: wei-wang
    last: Wang
  - first: Kai-Wei
    full: Kai-Wei Chang
    id: kai-wei-chang
    last: Chang
  author_string: Jieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, Kai-Wei Chang
  bibkey: zhao-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1521
  month: October-November
  page_first: '4847'
  page_last: '4853'
  pages: "4847\u20134853"
  paper_id: '521'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1521.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1521.jpg
  title: Learning Gender-Neutral Word Embeddings
  title_html: Learning Gender-Neutral Word Embeddings
  url: https://www.aclweb.org/anthology/D18-1521
  year: '2018'
D18-1522:
  abstract: We introduce a weakly supervised approach for inferring the property of
    abstractness of words and expressions in the complete absence of labeled data.
    Exploiting only minimal linguistic clues and the contextual usage of a concept
    as manifested in textual data, we train sufficiently powerful classifiers, obtaining
    high correlation with human labels. The results imply the applicability of this
    approach to additional properties of concepts, additional languages, and resource-scarce
    scenarios.
  address: Brussels, Belgium
  author:
  - first: Ella
    full: Ella Rabinovich
    id: ella-rabinovich
    last: Rabinovich
  - first: Benjamin
    full: Benjamin Sznajder
    id: benjamin-sznajder
    last: Sznajder
  - first: Artem
    full: Artem Spector
    id: artem-spector
    last: Spector
  - first: Ilya
    full: Ilya Shnayderman
    id: ilya-shnayderman
    last: Shnayderman
  - first: Ranit
    full: Ranit Aharonov
    id: ranit-aharonov
    last: Aharonov
  - first: David
    full: David Konopnicki
    id: david-konopnicki
    last: Konopnicki
  - first: Noam
    full: Noam Slonim
    id: noam-slonim
    last: Slonim
  author_string: Ella Rabinovich, Benjamin Sznajder, Artem Spector, Ilya Shnayderman,
    Ranit Aharonov, David Konopnicki, Noam Slonim
  bibkey: rabinovich-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1522
  month: October-November
  page_first: '4854'
  page_last: '4859'
  pages: "4854\u20134859"
  paper_id: '522'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1522.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1522.jpg
  title: Learning Concept Abstractness Using Weak Supervision
  title_html: Learning Concept Abstractness Using Weak Supervision
  url: https://www.aclweb.org/anthology/D18-1522
  year: '2018'
D18-1523:
  abstract: An established method for Word Sense Induction (WSI) uses a language model
    to predict probable substitutes for target words, and induces senses by clustering
    these resulting substitute vectors. We replace the ngram-based language model
    (LM) with a recurrent one. Beyond being more accurate, the use of the recurrent
    LM allows us to effectively query it in a creative way, using what we call dynamic
    symmetric patterns. The combination of the RNN-LM and the dynamic symmetric patterns
    results in strong substitute vectors for WSI, allowing to surpass the current
    state-of-the-art on the SemEval 2013 WSI shared task by a large margin.
  address: Brussels, Belgium
  author:
  - first: Asaf
    full: Asaf Amrami
    id: asaf-amrami
    last: Amrami
  - first: Yoav
    full: Yoav Goldberg
    id: yoav-goldberg
    last: Goldberg
  author_string: Asaf Amrami, Yoav Goldberg
  bibkey: amrami-goldberg-2018-word
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1523
  month: October-November
  page_first: '4860'
  page_last: '4867'
  pages: "4860\u20134867"
  paper_id: '523'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1523.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1523.jpg
  title: Word Sense Induction with Neural biLM and Symmetric Patterns
  title_html: Word Sense Induction with Neural bi<span class="acl-fixed-case">LM</span>
    and Symmetric Patterns
  url: https://www.aclweb.org/anthology/D18-1523
  year: '2018'
D18-1524:
  abstract: Natural language inference has been shown to be an effective supervised
    task for learning generic sentence embeddings. In order to better understand the
    components that lead to effective representations, we propose a lightweight version
    of InferSent, called InferLite, that does not use any recurrent layers and operates
    on a collection of pre-trained word embeddings. We show that a simple instance
    of our model that makes no use of context, word ordering or position can still
    obtain competitive performance on the majority of downstream prediction tasks,
    with most performance gaps being filled by adding local contextual information
    through temporal convolutions. Our models can be trained in under 1 hour on a
    single GPU and allows for fast inference of new representations. Finally we describe
    a semantic hashing layer that allows our model to learn generic binary codes for
    sentences.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1524.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1524.Attachment.zip
  author:
  - first: Jamie
    full: Jamie Kiros
    id: jamie-kiros
    last: Kiros
  - first: William
    full: William Chan
    id: william-chan
    last: Chan
  author_string: Jamie Kiros, William Chan
  bibkey: kiros-chan-2018-inferlite
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1524
  month: October-November
  page_first: '4868'
  page_last: '4874'
  pages: "4868\u20134874"
  paper_id: '524'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1524.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1524.jpg
  title: 'InferLite: Simple Universal Sentence Representations from Natural Language
    Inference Data'
  title_html: '<span class="acl-fixed-case">I</span>nfer<span class="acl-fixed-case">L</span>ite:
    Simple Universal Sentence Representations from Natural Language Inference Data'
  url: https://www.aclweb.org/anthology/D18-1524
  year: '2018'
D18-1525:
  abstract: This paper addresses the problem of representation learning. Using an
    autoencoder framework, we propose and evaluate several loss functions that can
    be used as an alternative to the commonly used cross-entropy reconstruction loss.
    The proposed loss functions use similarities between words in the embedding space,
    and can be used to train any neural model for text generation. We show that the
    introduced loss functions amplify semantic diversity of reconstructed sentences,
    while preserving the original meaning of the input. We test the derived autoencoder-generated
    representations on paraphrase detection and language inference tasks and demonstrate
    performance improvement compared to the traditional cross-entropy loss.
  address: Brussels, Belgium
  author:
  - first: Olga
    full: Olga Kovaleva
    id: olga-kovaleva
    last: Kovaleva
  - first: Anna
    full: Anna Rumshisky
    id: anna-rumshisky
    last: Rumshisky
  - first: Alexey
    full: Alexey Romanov
    id: alexey-romanov
    last: Romanov
  author_string: Olga Kovaleva, Anna Rumshisky, Alexey Romanov
  bibkey: kovaleva-etal-2018-similarity
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1525
  month: October-November
  page_first: '4875'
  page_last: '4880'
  pages: "4875\u20134880"
  paper_id: '525'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1525.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1525.jpg
  title: Similarity-Based Reconstruction Loss for Meaning Representation
  title_html: Similarity-Based Reconstruction Loss for Meaning Representation
  url: https://www.aclweb.org/anthology/D18-1525
  year: '2018'
D18-1526:
  abstract: 'We investigate the effects of multi-task learning using the recently
    introduced task of semantic tagging. We employ semantic tagging as an auxiliary
    task for three different NLP tasks: part-of-speech tagging, Universal Dependency
    parsing, and Natural Language Inference. We compare full neural network sharing,
    partial neural network sharing, and what we term the learning what to share setting
    where negative transfer between tasks is less likely. Our findings show considerable
    improvements for all tasks, particularly in the learning what to share setting
    which shows consistent gains across all tasks.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1526.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1526.Attachment.pdf
  author:
  - first: Mostafa
    full: Mostafa Abdou
    id: mostafa-abdou
    last: Abdou
  - first: Artur
    full: Artur Kulmizev
    id: artur-kulmizev
    last: Kulmizev
  - first: Vinit
    full: Vinit Ravishankar
    id: vinit-ravishankar
    last: Ravishankar
  - first: Lasha
    full: Lasha Abzianidze
    id: lasha-abzianidze
    last: Abzianidze
  - first: Johan
    full: Johan Bos
    id: johan-bos
    last: Bos
  author_string: Mostafa Abdou, Artur Kulmizev, Vinit Ravishankar, Lasha Abzianidze,
    Johan Bos
  bibkey: abdou-etal-2018-learn
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1526
  month: October-November
  page_first: '4881'
  page_last: '4889'
  pages: "4881\u20134889"
  paper_id: '526'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1526.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1526.jpg
  title: What can we learn from Semantic Tagging?
  title_html: What can we learn from Semantic Tagging?
  url: https://www.aclweb.org/anthology/D18-1526
  year: '2018'
D18-1527:
  abstract: Conventional word embedding models do not leverage information from document
    meta-data, and they do not model uncertainty. We address these concerns with a
    model that incorporates document covariates to estimate conditional word embedding
    distributions. Our model allows for (a) hypothesis tests about the meanings of
    terms, (b) assessments as to whether a word is near or far from another conditioned
    on different covariate values, and (c) assessments as to whether estimated differences
    are statistically significant.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1527.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1527.Attachment.pdf
  author:
  - first: Rujun
    full: Rujun Han
    id: rujun-han
    last: Han
  - first: Michael
    full: Michael Gill
    id: michael-gill
    last: Gill
  - first: Arthur
    full: Arthur Spirling
    id: arthur-spirling
    last: Spirling
  - first: Kyunghyun
    full: Kyunghyun Cho
    id: kyunghyun-cho
    last: Cho
  author_string: Rujun Han, Michael Gill, Arthur Spirling, Kyunghyun Cho
  bibkey: han-etal-2018-conditional
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1527
  month: October-November
  page_first: '4890'
  page_last: '4895'
  pages: "4890\u20134895"
  paper_id: '527'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1527.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1527.jpg
  title: Conditional Word Embedding and Hypothesis Testing via Bayes-by-Backprop
  title_html: Conditional Word Embedding and Hypothesis Testing via <span class="acl-fixed-case">B</span>ayes-by-Backprop
  url: https://www.aclweb.org/anthology/D18-1527
  year: '2018'
D18-1528:
  abstract: When processing a text, humans and machines must disambiguate between
    different uses of the pronoun it, including non-referential, nominal anaphoric
    or clause anaphoric ones. In this paper we use eye-tracking data to learn how
    humans perform this disambiguation and use this knowledge to improve the automatic
    classification of it. We show that by using gaze data and a POS-tagger we are
    able to significantly outperform a common baseline and classify between three
    categories of it with an accuracy comparable to that of linguistic-based approaches.
    In addition, the discriminatory power of specific gaze features informs the way
    humans process the pronoun, which, to the best of our knowledge, has not been
    explored using data from a natural reading task.
  address: Brussels, Belgium
  author:
  - first: Victoria
    full: Victoria Yaneva
    id: victoria-yaneva
    last: Yaneva
  - first: Le An
    full: Le An Ha
    id: le-an-ha
    last: Ha
  - first: Richard
    full: Richard Evans
    id: richard-evans
    last: Evans
  - first: Ruslan
    full: Ruslan Mitkov
    id: ruslan-mitkov
    last: Mitkov
  author_string: Victoria Yaneva, Le An Ha, Richard Evans, Ruslan Mitkov
  bibkey: yaneva-etal-2018-classifying
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1528
  month: October-November
  page_first: '4896'
  page_last: '4901'
  pages: "4896\u20134901"
  paper_id: '528'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1528.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1528.jpg
  title: Classifying Referential and Non-referential It Using Gaze
  title_html: Classifying Referential and Non-referential It Using Gaze
  url: https://www.aclweb.org/anthology/D18-1528
  year: '2018'
D18-1529:
  abstract: A wide variety of neural-network architectures have been proposed for
    the task of Chinese word segmentation. Surprisingly, we find that a bidirectional
    LSTM model, when combined with standard deep learning techniques and best practices,
    can achieve better accuracy on many of the popular datasets as compared to models
    based on more complex neuralnetwork architectures. Furthermore, our error analysis
    shows that out-of-vocabulary words remain challenging for neural-network models,
    and many of the remaining errors are unlikely to be fixed through architecture
    changes. Instead, more effort should be made on exploring resources for further
    improvement.
  address: Brussels, Belgium
  author:
  - first: Ji
    full: Ji Ma
    id: ji-ma
    last: Ma
  - first: Kuzman
    full: Kuzman Ganchev
    id: kuzman-ganchev
    last: Ganchev
  - first: David
    full: David Weiss
    id: david-weiss
    last: Weiss
  author_string: Ji Ma, Kuzman Ganchev, David Weiss
  bibkey: ma-etal-2018-state
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1529
  month: October-November
  page_first: '4902'
  page_last: '4908'
  pages: "4902\u20134908"
  paper_id: '529'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1529.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1529.jpg
  title: State-of-the-art Chinese Word Segmentation with Bi-LSTMs
  title_html: State-of-the-art <span class="acl-fixed-case">C</span>hinese Word Segmentation
    with Bi-<span class="acl-fixed-case">LSTM</span>s
  url: https://www.aclweb.org/anthology/D18-1529
  year: '2018'
D18-1530:
  abstract: In Sanskrit, small words (morphemes) are combined to form compound words
    through a process known as Sandhi. Sandhi splitting is the process of splitting
    a given compound word into its constituent morphemes. Although rules governing
    word splitting exists in the language, it is highly challenging to identify the
    location of the splits in a compound word. Though existing Sandhi splitting systems
    incorporate these pre-defined splitting rules, they have a low accuracy as the
    same compound word might be broken down in multiple ways to provide syntactically
    correct splits. In this research, we propose a novel deep learning architecture
    called Double Decoder RNN (DD-RNN), which (i) predicts the location of the split(s)
    with 95% accuracy, and (ii) predicts the constituent words (learning the Sandhi
    splitting rules) with 79.5% accuracy, outperforming the state-of-art by 20%. Additionally,
    we show the generalization capability of our deep learning model, by showing competitive
    results in the problem of Chinese word segmentation, as well.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1530.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1530.Attachment.zip
  author:
  - first: Rahul
    full: Rahul Aralikatte
    id: rahul-aralikatte
    last: Aralikatte
  - first: Neelamadhav
    full: Neelamadhav Gantayat
    id: neelamadhav-gantayat
    last: Gantayat
  - first: Naveen
    full: Naveen Panwar
    id: naveen-panwar
    last: Panwar
  - first: Anush
    full: Anush Sankaran
    id: anush-sankaran
    last: Sankaran
  - first: Senthil
    full: Senthil Mani
    id: senthil-mani
    last: Mani
  author_string: Rahul Aralikatte, Neelamadhav Gantayat, Naveen Panwar, Anush Sankaran,
    Senthil Mani
  bibkey: aralikatte-etal-2018-sanskrit
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1530
  month: October-November
  page_first: '4909'
  page_last: '4914'
  pages: "4909\u20134914"
  paper_id: '530'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1530.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1530.jpg
  title: Sanskrit Sandhi Splitting using seq2(seq)2
  title_html: <span class="acl-fixed-case">S</span>anskrit Sandhi Splitting using
    seq2(seq)2
  url: https://www.aclweb.org/anthology/D18-1530
  year: '2018'
D18-1531:
  abstract: Previous traditional approaches to unsupervised Chinese word segmentation
    (CWS) can be roughly classified into discriminative and generative models. The
    former uses the carefully designed goodness measures for candidate segmentation,
    while the latter focuses on finding the optimal segmentation of the highest generative
    probability. However, while there exists a trivial way to extend the discriminative
    models into neural version by using neural language models, those of generative
    ones are non-trivial. In this paper, we propose the segmental language models
    (SLMs) for CWS. Our approach explicitly focuses on the segmental nature of Chinese,
    as well as preserves several properties of language models. In SLMs, a context
    encoder encodes the previous context and a segment decoder generates each segment
    incrementally. As far as we know, we are the first to propose a neural model for
    unsupervised CWS and achieve competitive performance to the state-of-the-art statistical
    models on four different datasets from SIGHAN 2005 bakeoff.
  address: Brussels, Belgium
  author:
  - first: Zhiqing
    full: Zhiqing Sun
    id: zhiqing-sun
    last: Sun
  - first: Zhi-Hong
    full: Zhi-Hong Deng
    id: zhi-hong-deng
    last: Deng
  author_string: Zhiqing Sun, Zhi-Hong Deng
  bibkey: sun-deng-2018-unsupervised
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1531
  month: October-November
  page_first: '4915'
  page_last: '4920'
  pages: "4915\u20134920"
  paper_id: '531'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1531.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1531.jpg
  title: Unsupervised Neural Word Segmentation for Chinese via Segmental Language
    Modeling
  title_html: Unsupervised Neural Word Segmentation for <span class="acl-fixed-case">C</span>hinese
    via Segmental Language Modeling
  url: https://www.aclweb.org/anthology/D18-1531
  year: '2018'
D18-1532:
  abstract: We present LemmaTag, a featureless neural network architecture that jointly
    generates part-of-speech tags and lemmas for sentences by using bidirectional
    RNNs with character-level and word-level embeddings. We demonstrate that both
    tasks benefit from sharing the encoding part of the network, predicting tag subcategories,
    and using the tagger output as an input to the lemmatizer. We evaluate our model
    across several languages with complex morphology, which surpasses state-of-the-art
    accuracy in both part-of-speech tagging and lemmatization in Czech, German, and
    Arabic.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1532.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1532.Attachment.zip
  author:
  - first: Daniel
    full: Daniel Kondratyuk
    id: daniel-kondratyuk
    last: Kondratyuk
  - first: "Tom\xE1\u0161"
    full: "Tom\xE1\u0161 Gaven\u010Diak"
    id: tomas-gavenciak
    last: "Gaven\u010Diak"
  - first: Milan
    full: Milan Straka
    id: milan-straka
    last: Straka
  - first: Jan
    full: "Jan Haji\u010D"
    id: jan-hajic
    last: "Haji\u010D"
  author_string: "Daniel Kondratyuk, Tom\xE1\u0161 Gaven\u010Diak, Milan Straka, Jan\
    \ Haji\u010D"
  bibkey: kondratyuk-etal-2018-lemmatag
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1532
  month: October-November
  page_first: '4921'
  page_last: '4928'
  pages: "4921\u20134928"
  paper_id: '532'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1532.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1532.jpg
  title: 'LemmaTag: Jointly Tagging and Lemmatizing for Morphologically Rich Languages
    with BRNNs'
  title_html: '<span class="acl-fixed-case">L</span>emma<span class="acl-fixed-case">T</span>ag:
    Jointly Tagging and Lemmatizing for Morphologically Rich Languages with <span
    class="acl-fixed-case">BRNN</span>s'
  url: https://www.aclweb.org/anthology/D18-1532
  year: '2018'
D18-1533:
  abstract: In contrast to the older writing system of the 19th century, modern Hawaiian
    orthography employs characters for long vowels and glottal stops. These extra
    characters account for about one-third of the phonemes in Hawaiian, so including
    them makes a big difference to reading comprehension and pronunciation. However,
    transliterating between older and newer texts is a laborious task when performed
    manually. We introduce two related methods to help solve this transliteration
    problem automatically. One approach is implemented, end-to-end, using finite state
    transducers (FSTs). The other is a hybrid deep learning approach, which approximately
    composes an FST with a recurrent neural network language model.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1533.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1533.Attachment.pdf
  author:
  - first: Brendan
    full: Brendan Shillingford
    id: brendan-shillingford
    last: Shillingford
  - first: Oiwi
    full: Oiwi Parker Jones
    id: oiwi-parker-jones
    last: Parker Jones
  author_string: Brendan Shillingford, Oiwi Parker Jones
  bibkey: shillingford-parker-jones-2018-recovering
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1533
  month: October-November
  page_first: '4929'
  page_last: '4934'
  pages: "4929\u20134934"
  paper_id: '533'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1533.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1533.jpg
  title: Recovering Missing Characters in Old Hawaiian Writing
  title_html: Recovering Missing Characters in Old <span class="acl-fixed-case">H</span>awaiian
    Writing
  url: https://www.aclweb.org/anthology/D18-1533
  year: '2018'
D18-1534:
  abstract: 'Consider two competitive machine learning models, one of which was considered
    state-of-the art, and the other a competitive baseline. Suppose that by just permuting
    the examples of the training set, say by reversing the original order, by shuffling,
    or by mini-batching, you could report substantially better/worst performance for
    the system of your choice, by multiple percentage points. In this paper, we illustrate
    this scenario for a trending NLP task: Natural Language Inference (NLI). We show
    that for the two central NLI corpora today, the learning process of neural systems
    is far too sensitive to permutations of the data. In doing so we reopen the question
    of how to judge a good neural architecture for NLI, given the available dataset
    and perhaps, further, the soundness of the NLI task itself in its current state.'
  address: Brussels, Belgium
  author:
  - first: Natalie
    full: Natalie Schluter
    id: natalie-schluter
    last: Schluter
  - first: Daniel
    full: Daniel Varab
    id: daniel-varab
    last: Varab
  author_string: Natalie Schluter, Daniel Varab
  bibkey: schluter-varab-2018-data
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1534
  month: October-November
  page_first: '4935'
  page_last: '4939'
  pages: "4935\u20134939"
  paper_id: '534'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1534.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1534.jpg
  title: 'When data permutations are pathological: the case of neural natural language
    inference'
  title_html: 'When data permutations are pathological: the case of neural natural
    language inference'
  url: https://www.aclweb.org/anthology/D18-1534
  year: '2018'
D18-1535:
  abstract: Most textual entailment models focus on lexical gaps between the premise
    text and the hypothesis, but rarely on knowledge gaps. We focus on filling these
    knowledge gaps in the Science Entailment task, by leveraging an external structured
    knowledge base (KB) of science facts. Our new architecture combines standard neural
    entailment models with a knowledge lookup module. To facilitate this lookup, we
    propose a fact-level decomposition of the hypothesis, and verifying the resulting
    sub-facts against both the textual premise and the structured KB. Our model, NSNet,
    learns to aggregate predictions from these heterogeneous data formats. On the
    SciTail dataset, NSNet outperforms a simpler combination of the two predictions
    by 3% and the base entailment model by 5%.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1535.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1535.Attachment.pdf
  author:
  - first: Dongyeop
    full: Dongyeop Kang
    id: dongyeop-kang
    last: Kang
  - first: Tushar
    full: Tushar Khot
    id: tushar-khot
    last: Khot
  - first: Ashish
    full: Ashish Sabharwal
    id: ashish-sabharwal
    last: Sabharwal
  - first: Peter
    full: Peter Clark
    id: peter-clark
    last: Clark
  author_string: Dongyeop Kang, Tushar Khot, Ashish Sabharwal, Peter Clark
  bibkey: kang-etal-2018-bridging
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1535
  month: October-November
  page_first: '4940'
  page_last: '4945'
  pages: "4940\u20134945"
  paper_id: '535'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1535.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1535.jpg
  title: Bridging Knowledge Gaps in Neural Entailment via Symbolic Models
  title_html: Bridging Knowledge Gaps in Neural Entailment via Symbolic Models
  url: https://www.aclweb.org/anthology/D18-1535
  year: '2018'
D18-1536:
  abstract: "This paper introduces the Bank Question (BQ) corpus, a Chinese corpus\
    \ for sentence semantic equivalence identification (SSEI). The BQ corpus contains\
    \ 120,000 question pairs from 1-year online bank custom service logs. To efficiently\
    \ process and annotate questions from such a large scale of logs, this paper proposes\
    \ a clustering based annotation method to achieve questions with the same intent.\
    \ First, the deduplicated questions with the same answer are clustered into stacks\
    \ by the Word Mover\u2019s Distance (WMD) based Affinity Propagation (AP) algorithm.\
    \ Then, the annotators are asked to assign the clustered questions into different\
    \ intent categories. Finally, the positive and negative question pairs for SSEI\
    \ are selected in the same intent category and between different intent categories\
    \ respectively. We also present six SSEI benchmark performance on our corpus,\
    \ including state-of-the-art algorithms. As the largest manually annotated public\
    \ Chinese SSEI corpus in the bank domain, the BQ corpus is not only useful for\
    \ Chinese question semantic matching research, but also a significant resource\
    \ for cross-lingual and cross-domain SSEI research. The corpus is available in\
    \ public."
  address: Brussels, Belgium
  author:
  - first: Jing
    full: Jing Chen
    id: jing-chen
    last: Chen
  - first: Qingcai
    full: Qingcai Chen
    id: qingcai-chen
    last: Chen
  - first: Xin
    full: Xin Liu
    id: xin-liu
    last: Liu
  - first: Haijun
    full: Haijun Yang
    id: haijun-yang
    last: Yang
  - first: Daohe
    full: Daohe Lu
    id: daohe-lu
    last: Lu
  - first: Buzhou
    full: Buzhou Tang
    id: buzhou-tang
    last: Tang
  author_string: Jing Chen, Qingcai Chen, Xin Liu, Haijun Yang, Daohe Lu, Buzhou Tang
  bibkey: chen-etal-2018-bq
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1536
  month: October-November
  page_first: '4946'
  page_last: '4951'
  pages: "4946\u20134951"
  paper_id: '536'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1536.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1536.jpg
  title: 'The BQ Corpus: A Large-scale Domain-specific Chinese Corpus For Sentence
    Semantic Equivalence Identification'
  title_html: 'The <span class="acl-fixed-case">BQ</span> Corpus: A Large-scale Domain-specific
    <span class="acl-fixed-case">C</span>hinese Corpus For Sentence Semantic Equivalence
    Identification'
  url: https://www.aclweb.org/anthology/D18-1536
  year: '2018'
D18-1537:
  abstract: Deep learning models have achieved remarkable success in natural language
    inference (NLI) tasks. While these models are widely explored, they are hard to
    interpret and it is often unclear how and why they actually work. In this paper,
    we take a step toward explaining such deep learning based models through a case
    study on a popular neural model for NLI. In particular, we propose to interpret
    the intermediate layers of NLI models by visualizing the saliency of attention
    and LSTM gating signals. We present several examples for which our methods are
    able to reveal interesting insights and identify the critical information contributing
    to the model decisions.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1537.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1537.Attachment.zip
  author:
  - first: Reza
    full: Reza Ghaeini
    id: reza-ghaeini
    last: Ghaeini
  - first: Xiaoli
    full: Xiaoli Fern
    id: xiaoli-fern
    last: Fern
  - first: Prasad
    full: Prasad Tadepalli
    id: prasad-tadepalli
    last: Tadepalli
  author_string: Reza Ghaeini, Xiaoli Fern, Prasad Tadepalli
  bibkey: ghaeini-etal-2018-interpreting
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1537
  month: October-November
  page_first: '4952'
  page_last: '4957'
  pages: "4952\u20134957"
  paper_id: '537'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1537.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1537.jpg
  title: 'Interpreting Recurrent and Attention-Based Neural Models: a Case Study on
    Natural Language Inference'
  title_html: 'Interpreting Recurrent and Attention-Based Neural Models: a Case Study
    on Natural Language Inference'
  url: https://www.aclweb.org/anthology/D18-1537
  year: '2018'
D18-1538:
  abstract: Neural models have shown several state-of-the-art performances on Semantic
    Role Labeling (SRL). However, the neural models require an immense amount of semantic-role
    corpora and are thus not well suited for low-resource languages or domains. The
    paper proposes a semi-supervised semantic role labeling method that outperforms
    the state-of-the-art in limited SRL training corpora. The method is based on explicitly
    enforcing syntactic constraints by augmenting the training objective with a syntactic-inconsistency
    loss component and uses SRL-unlabeled instances to train a joint-objective LSTM.
    On CoNLL-2012 English section, the proposed semi-supervised training with 1%,
    10% SRL-labeled data and varying amounts of SRL-unlabeled data achieves +1.58,
    +0.78 F1, respectively, over the pre-trained models that were trained on SOTA
    architecture with ELMo on the same SRL-labeled data. Additionally, by using the
    syntactic-inconsistency loss on inference time, the proposed model achieves +3.67,
    +2.1 F1 over pre-trained model on 1%, 10% SRL-labeled data, respectively.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1538.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1538.Attachment.pdf
  author:
  - first: Sanket Vaibhav
    full: Sanket Vaibhav Mehta
    id: sanket-vaibhav-mehta
    last: Mehta
  - first: Jay Yoon
    full: Jay Yoon Lee
    id: jay-yoon-lee
    last: Lee
  - first: Jaime
    full: Jaime Carbonell
    id: jaime-g-carbonell
    last: Carbonell
  author_string: Sanket Vaibhav Mehta, Jay Yoon Lee, Jaime Carbonell
  bibkey: mehta-etal-2018-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1538
  month: October-November
  page_first: '4958'
  page_last: '4963'
  pages: "4958\u20134963"
  paper_id: '538'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1538.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1538.jpg
  title: Towards Semi-Supervised Learning for Deep Semantic Role Labeling
  title_html: Towards Semi-Supervised Learning for Deep Semantic Role Labeling
  url: https://www.aclweb.org/anthology/D18-1538
  year: '2018'
D18-1539:
  abstract: "When the semantics of a sentence are not representable in a semantic\
    \ parser\u2019s output schema, parsing will inevitably fail. Detection of these\
    \ instances is commonly treated as an out-of-domain classification problem. However,\
    \ there is also a more subtle scenario in which the test data is drawn from the\
    \ same domain. In addition to formalizing this problem of domain-adjacency, we\
    \ present a comparison of various baselines that could be used to solve it. We\
    \ also propose a new simple sentence representation that emphasizes words which\
    \ are unexpected. This approach improves the performance of a downstream semantic\
    \ parser run on in-domain and domain-adjacent instances."
  address: Brussels, Belgium
  author:
  - first: James
    full: James Ferguson
    id: james-ferguson
    last: Ferguson
  - first: Janara
    full: Janara Christensen
    id: janara-christensen
    last: Christensen
  - first: Edward
    full: Edward Li
    id: edward-li
    last: Li
  - first: Edgar
    full: "Edgar Gonz\xE0lez"
    id: edgar-gonzalez-pellicer
    last: "Gonz\xE0lez"
  author_string: "James Ferguson, Janara Christensen, Edward Li, Edgar Gonz\xE0lez"
  bibkey: ferguson-etal-2018-identifying
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1539
  month: October-November
  page_first: '4964'
  page_last: '4969'
  pages: "4964\u20134969"
  paper_id: '539'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1539.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1539.jpg
  title: Identifying Domain Adjacent Instances for Semantic Parsers
  title_html: Identifying Domain Adjacent Instances for Semantic Parsers
  url: https://www.aclweb.org/anthology/D18-1539
  year: '2018'
D18-1540:
  abstract: "The web provides a rich, open-domain environment with textual, structural,\
    \ and spatial properties. We propose a new task for grounding language in this\
    \ environment: given a natural language command (e.g., \u201Cclick on the second\
    \ article\u201D), choose the correct element on the web page (e.g., a hyperlink\
    \ or text box). We collected a dataset of over 50,000 commands that capture various\
    \ phenomena such as functional references (e.g. \u201Cfind who made this site\u201D\
    ), relational reasoning (e.g. \u201Carticle by john\u201D), and visual reasoning\
    \ (e.g. \u201Ctop-most article\u201D). We also implemented and analyzed three\
    \ baseline models that capture different phenomena present in the dataset."
  address: Brussels, Belgium
  author:
  - first: Panupong
    full: Panupong Pasupat
    id: panupong-pasupat
    last: Pasupat
  - first: Tian-Shun
    full: Tian-Shun Jiang
    id: tian-shun-jiang
    last: Jiang
  - first: Evan
    full: Evan Liu
    id: evan-liu
    last: Liu
  - first: Kelvin
    full: Kelvin Guu
    id: kelvin-guu
    last: Guu
  - first: Percy
    full: Percy Liang
    id: percy-liang
    last: Liang
  author_string: Panupong Pasupat, Tian-Shun Jiang, Evan Liu, Kelvin Guu, Percy Liang
  bibkey: pasupat-etal-2018-mapping
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1540
  month: October-November
  page_first: '4970'
  page_last: '4976'
  pages: "4970\u20134976"
  paper_id: '540'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1540.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1540.jpg
  title: Mapping natural language commands to web elements
  title_html: Mapping natural language commands to web elements
  url: https://www.aclweb.org/anthology/D18-1540
  year: '2018'
D18-1541:
  abstract: Grammatical error correction, like other machine learning tasks, greatly
    benefits from large quantities of high quality training data, which is typically
    expensive to produce. While writing a program to automatically generate realistic
    grammatical errors would be difficult, one could learn the distribution of naturally-occurring
    errors and attempt to introduce them into other datasets. Initial work on inducing
    errors in this way using statistical machine translation has shown promise; we
    investigate cheaply constructing synthetic samples, given a small corpus of human-annotated
    data, using an off-the-rack attentive sequence-to-sequence model and a straight-forward
    post-processing procedure. Our approach yields error-filled artificial data that
    helps a vanilla bi-directional LSTM to outperform the previous state of the art
    at grammatical error detection, and a previously introduced model to gain further
    improvements of over 5% F0.5 score. When attempting to determine if a given sentence
    is synthetic, a human annotator at best achieves 39.39 F1 score, indicating that
    our model generates mostly human-like instances.
  address: Brussels, Belgium
  author:
  - first: Sudhanshu
    full: Sudhanshu Kasewa
    id: sudhanshu-kasewa
    last: Kasewa
  - first: Pontus
    full: Pontus Stenetorp
    id: pontus-stenetorp
    last: Stenetorp
  - first: Sebastian
    full: Sebastian Riedel
    id: sebastian-riedel
    last: Riedel
  author_string: Sudhanshu Kasewa, Pontus Stenetorp, Sebastian Riedel
  bibkey: kasewa-etal-2018-wronging
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1541
  month: October-November
  page_first: '4977'
  page_last: '4983'
  pages: "4977\u20134983"
  paper_id: '541'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1541.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1541.jpg
  title: 'Wronging a Right: Generating Better Errors to Improve Grammatical Error
    Detection'
  title_html: 'Wronging a Right: Generating Better Errors to Improve Grammatical Error
    Detection'
  url: https://www.aclweb.org/anthology/D18-1541
  year: '2018'
D18-1542:
  abstract: Recently introduced neural network parsers allow for new approaches to
    circumvent data sparsity issues by modeling character level information and by
    exploiting raw data in a semi-supervised setting. Data sparsity is especially
    prevailing when transferring to non-standard domains. In this setting, lexical
    normalization has often been used in the past to circumvent data sparsity. In
    this paper, we investigate whether these new neural approaches provide similar
    functionality as lexical normalization, or whether they are complementary. We
    provide experimental results which show that a separate normalization component
    improves performance of a neural network parser even if it has access to character
    level information as well as external word embeddings. Further improvements are
    obtained by a straightforward but novel approach in which the top-N best candidates
    provided by the normalization component are available to the parser.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1542.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1542.Attachment.pdf
  author:
  - first: Rob
    full: Rob van der Goot
    id: rob-van-der-goot
    last: van der Goot
  - first: Gertjan
    full: Gertjan van Noord
    id: gertjan-van-noord
    last: van Noord
  author_string: Rob van der Goot, Gertjan van Noord
  bibkey: van-der-goot-van-noord-2018-modeling
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1542
  month: October-November
  page_first: '4984'
  page_last: '4991'
  pages: "4984\u20134991"
  paper_id: '542'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1542.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1542.jpg
  title: Modeling Input Uncertainty in Neural Network Dependency Parsing
  title_html: Modeling Input Uncertainty in Neural Network Dependency Parsing
  url: https://www.aclweb.org/anthology/D18-1542
  year: '2018'
D18-1543:
  abstract: Previous work has suggested that parameter sharing between transition-based
    neural dependency parsers for related languages can lead to better performance,
    but there is no consensus on what parameters to share. We present an evaluation
    of 27 different parameter sharing strategies across 10 languages, representing
    five pairs of related languages, each pair from a different language family. We
    find that sharing transition classifier parameters always helps, whereas the usefulness
    of sharing word and/or character LSTM parameters varies. Based on this result,
    we propose an architecture where the transition classifier is shared, and the
    sharing of word and character parameters is controlled by a parameter that can
    be tuned on validation data. This model is linguistically motivated and obtains
    significant improvements over a monolingually trained baseline. We also find that
    sharing transition classifier parameters helps when training a parser on unrelated
    language pairs, but we find that, in the case of unrelated languages, sharing
    too many parameters does not help.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1543.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1543.Attachment.pdf
  - filename: D18-1543.Poster.pdf
    type: poster
    url: https://www.aclweb.org/anthology/attachments/D18-1543.Poster.pdf
  author:
  - first: Miryam
    full: Miryam de Lhoneux
    id: miryam-de-lhoneux
    last: de Lhoneux
  - first: Johannes
    full: Johannes Bjerva
    id: johannes-bjerva
    last: Bjerva
  - first: Isabelle
    full: Isabelle Augenstein
    id: isabelle-augenstein
    last: Augenstein
  - first: Anders
    full: "Anders S\xF8gaard"
    id: anders-sogaard
    last: "S\xF8gaard"
  author_string: "Miryam de Lhoneux, Johannes Bjerva, Isabelle Augenstein, Anders\
    \ S\xF8gaard"
  bibkey: de-lhoneux-etal-2018-parameter
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1543
  month: October-November
  page_first: '4992'
  page_last: '4997'
  pages: "4992\u20134997"
  paper_id: '543'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1543.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1543.jpg
  title: Parameter sharing between dependency parsers for related languages
  title_html: Parameter sharing between dependency parsers for related languages
  url: https://www.aclweb.org/anthology/D18-1543
  year: '2018'
D18-1544:
  abstract: 'A substantial thread of recent work on latent tree learning has attempted
    to develop neural network models with parse-valued latent variables and train
    them on non-parsing tasks, in the hope of having them discover interpretable tree
    structure. In a recent paper, Shen et al. (2018) introduce such a model and report
    near-state-of-the-art results on the target task of language modeling, and the
    first strong latent tree learning result on constituency parsing. In an attempt
    to reproduce these results, we discover issues that make the original results
    hard to trust, including tuning and even training on what is effectively the test
    set. Here, we attempt to reproduce these results in a fair experiment and to extend
    them to two new datasets. We find that the results of this work are robust: All
    variants of the model under study outperform all latent tree learning baselines,
    and perform competitively with symbolic grammar induction systems. We find that
    this model represents the first empirical success for latent tree learning, and
    that neural network language modeling warrants further study as a setting for
    grammar induction.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1544.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1544.Attachment.zip
  author:
  - first: Phu Mon
    full: Phu Mon Htut
    id: phu-mon-htut
    last: Htut
  - first: Kyunghyun
    full: Kyunghyun Cho
    id: kyunghyun-cho
    last: Cho
  - first: Samuel
    full: Samuel Bowman
    id: samuel-bowman
    last: Bowman
  author_string: Phu Mon Htut, Kyunghyun Cho, Samuel Bowman
  bibkey: htut-etal-2018-grammar
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1544
  month: October-November
  page_first: '4998'
  page_last: '5003'
  pages: "4998\u20135003"
  paper_id: '544'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1544.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1544.jpg
  title: 'Grammar Induction with Neural Language Models: An Unusual Replication'
  title_html: 'Grammar Induction with Neural Language Models: An Unusual Replication'
  url: https://www.aclweb.org/anthology/D18-1544
  year: '2018'
D18-1545:
  abstract: "Neural NLP systems achieve high scores in the presence of sizable training\
    \ dataset. Lack of such datasets leads to poor system performances in the case\
    \ low-resource languages. We present two simple text augmentation techniques using\
    \ dependency trees, inspired from image processing. We \u201Ccrop\u201D sentences\
    \ by removing dependency links, and we \u201Crotate\u201D sentences by moving\
    \ the tree fragments around the root. We apply these techniques to augment the\
    \ training sets of low-resource languages in Universal Dependencies project. We\
    \ implement a character-level sequence tagging model and evaluate the augmented\
    \ datasets on part-of-speech tagging task. We show that crop and rotate provides\
    \ improvements over the models trained with non-augmented data for majority of\
    \ the languages, especially for languages with rich case marking systems."
  address: Brussels, Belgium
  author:
  - first: "G\xF6zde G\xFCl"
    full: "G\xF6zde G\xFCl \u015Eahin"
    id: gozde-gul-sahin
    last: "\u015Eahin"
  - first: Mark
    full: Mark Steedman
    id: mark-steedman
    last: Steedman
  author_string: "G\xF6zde G\xFCl \u015Eahin, Mark Steedman"
  bibkey: sahin-steedman-2018-data
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1545
  month: October-November
  page_first: '5004'
  page_last: '5009'
  pages: "5004\u20135009"
  paper_id: '545'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1545.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1545.jpg
  title: Data Augmentation via Dependency Tree Morphing for Low-Resource Languages
  title_html: Data Augmentation via Dependency Tree Morphing for Low-Resource Languages
  url: https://www.aclweb.org/anthology/D18-1545
  year: '2018'
D18-1546:
  abstract: Many recent papers address reading comprehension, where examples consist
    of (question, passage, answer) tuples. Presumably, a model must combine information
    from both questions and passages to predict corresponding answers. However, despite
    intense interest in the topic, with hundreds of published papers vying for leaderboard
    dominance, basic questions about the difficulty of many popular benchmarks remain
    unanswered. In this paper, we establish sensible baselines for the bAbI, SQuAD,
    CBT, CNN, and Who-did-What datasets, finding that question- and passage-only models
    often perform surprisingly well. On 14 out of 20 bAbI tasks, passage-only models
    achieve greater than 50% accuracy, sometimes matching the full model. Interestingly,
    while CBT provides 20-sentence passages, only the last is needed for accurate
    prediction. By comparison, SQuAD and CNN appear better-constructed.
  address: Brussels, Belgium
  attachment:
  - filename: https://vimeo.com/306140720
    type: video
    url: https://vimeo.com/306140720
  author:
  - first: Divyansh
    full: Divyansh Kaushik
    id: divyansh-kaushik
    last: Kaushik
  - first: Zachary C.
    full: Zachary C. Lipton
    id: zachary-c-lipton
    last: Lipton
  author_string: Divyansh Kaushik, Zachary C. Lipton
  bibkey: kaushik-lipton-2018-much
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1546
  month: October-November
  page_first: '5010'
  page_last: '5015'
  pages: "5010\u20135015"
  paper_id: '546'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1546.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1546.jpg
  title: How Much Reading Does Reading Comprehension Require? A Critical Investigation
    of Popular Benchmarks
  title_html: How Much Reading Does Reading Comprehension Require? A Critical Investigation
    of Popular Benchmarks
  url: https://www.aclweb.org/anthology/D18-1546
  year: '2018'
D18-1547:
  abstract: Even though machine learning has become the major scene in dialogue research
    community, the real breakthrough has been blocked by the scale of data available.To
    address this fundamental obstacle, we introduce the Multi-Domain Wizard-of-Oz
    dataset (MultiWOZ), a fully-labeled collection of human-human written conversations
    spanning over multiple domains and topics.At a size of 10k dialogues, it is at
    least one order of magnitude larger than all previous annotated task-oriented
    corpora.The contribution of this work apart from the open-sourced dataset is two-fold:firstly,
    a detailed description of the data collection procedure along with a summary of
    data structure and analysis is provided. The proposed data-collection pipeline
    is entirely based on crowd-sourcing without the need of hiring professional annotators;secondly,
    a set of benchmark results of belief tracking, dialogue act and response generation
    is reported, which shows the usability of the data and sets a baseline for future
    studies.
  address: Brussels, Belgium
  attachment:
  - filename: D18-1547.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1547.Attachment.pdf
  - filename: https://vimeo.com/306141298
    type: video
    url: https://vimeo.com/306141298
  author:
  - first: "Pawe\u0142"
    full: "Pawe\u0142 Budzianowski"
    id: pawel-budzianowski
    last: Budzianowski
  - first: Tsung-Hsien
    full: Tsung-Hsien Wen
    id: tsung-hsien-wen
    last: Wen
  - first: Bo-Hsiang
    full: Bo-Hsiang Tseng
    id: bo-hsiang-tseng
    last: Tseng
  - first: "I\xF1igo"
    full: "I\xF1igo Casanueva"
    id: inigo-casanueva
    last: Casanueva
  - first: Stefan
    full: Stefan Ultes
    id: stefan-ultes
    last: Ultes
  - first: Osman
    full: Osman Ramadan
    id: osman-ramadan
    last: Ramadan
  - first: Milica
    full: "Milica Ga\u0161i\u0107"
    id: milica-gasic
    last: "Ga\u0161i\u0107"
  author_string: "Pawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\xF1\
    igo Casanueva, Stefan Ultes, Osman Ramadan, Milica Ga\u0161i\u0107"
  bibkey: budzianowski-etal-2018-multiwoz
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1547
  month: October-November
  page_first: '5016'
  page_last: '5026'
  pages: "5016\u20135026"
  paper_id: '547'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1547.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1547.jpg
  title: MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented
    Dialogue Modelling
  title_html: <span class="acl-fixed-case">M</span>ulti<span class="acl-fixed-case">WOZ</span>
    - A Large-Scale Multi-Domain Wizard-of-<span class="acl-fixed-case">O</span>z
    Dataset for Task-Oriented Dialogue Modelling
  url: https://www.aclweb.org/anthology/D18-1547
  year: '2018'
D18-1548:
  abstract: 'Current state-of-the-art semantic role labeling (SRL) uses a deep neural
    network with no explicit linguistic features. However, prior work has shown that
    gold syntax trees can dramatically improve SRL decoding, suggesting the possibility
    of increased accuracy from explicit modeling of syntax. In this work, we present
    linguistically-informed self-attention (LISA): a neural network model that combines
    multi-head self-attention with multi-task learning across dependency parsing,
    part-of-speech tagging, predicate detection and SRL. Unlike previous models which
    require significant pre-processing to prepare linguistic features, LISA can incorporate
    syntax using merely raw tokens as input, encoding the sequence only once to simultaneously
    perform parsing, predicate detection and role labeling for all predicates. Syntax
    is incorporated by training one attention head to attend to syntactic parents
    for each token. Moreover, if a high-quality syntactic parse is already available,
    it can be beneficially injected at test time without re-training our SRL model.
    In experiments on CoNLL-2005 SRL, LISA achieves new state-of-the-art performance
    for a model using predicted predicates and standard word embeddings, attaining
    2.5 F1 absolute higher than the previous state-of-the-art on newswire and more
    than 3.5 F1 on out-of-domain data, nearly 10% reduction in error. On ConLL-2012
    English SRL we also show an improvement of more than 2.5 F1. LISA also out-performs
    the state-of-the-art with contextually-encoded (ELMo) word representations, by
    nearly 1.0 F1 on news and more than 2.0 F1 on out-of-domain text.'
  address: Brussels, Belgium
  attachment:
  - filename: D18-1548.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1548.Attachment.pdf
  - filename: https://vimeo.com/306141078
    type: video
    url: https://vimeo.com/306141078
  author:
  - first: Emma
    full: Emma Strubell
    id: emma-strubell
    last: Strubell
  - first: Patrick
    full: Patrick Verga
    id: patrick-verga
    last: Verga
  - first: Daniel
    full: Daniel Andor
    id: daniel-andor
    last: Andor
  - first: David
    full: David Weiss
    id: david-weiss
    last: Weiss
  - first: Andrew
    full: Andrew McCallum
    id: andrew-mccallum
    last: McCallum
  author_string: Emma Strubell, Patrick Verga, Daniel Andor, David Weiss, Andrew McCallum
  bibkey: strubell-etal-2018-linguistically
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1548
  month: October-November
  page_first: '5027'
  page_last: '5038'
  pages: "5027\u20135038"
  paper_id: '548'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1548.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1548.jpg
  title: Linguistically-Informed Self-Attention for Semantic Role Labeling
  title_html: Linguistically-Informed Self-Attention for Semantic Role Labeling
  url: https://www.aclweb.org/anthology/D18-1548
  year: '2018'
D18-1549:
  abstract: "Machine translation systems achieve near human-level performance on some\
    \ languages, yet their effectiveness strongly relies on the availability of large\
    \ amounts of parallel sentences, which hinders their applicability to the majority\
    \ of language pairs. This work investigates how to learn to translate when having\
    \ access to only large monolingual corpora in each language. We propose two model\
    \ variants, a neural and a phrase-based model. Both versions leverage a careful\
    \ initialization of the parameters, the denoising effect of language models and\
    \ automatic generation of parallel data by iterative back-translation. These models\
    \ are significantly better than methods from the literature, while being simpler\
    \ and having fewer hyper-parameters. On the widely used WMT\u201914 English-French\
    \ and WMT\u201916 German-English benchmarks, our models respectively obtain 28.1\
    \ and 25.2 BLEU points without using a single parallel sentence, outperforming\
    \ the state of the art by more than 11 BLEU points. On low-resource languages\
    \ like English-Urdu and English-Romanian, our methods achieve even better results\
    \ than semi-supervised and supervised approaches leveraging the paucity of available\
    \ bitexts. Our code for NMT and PBSMT is publicly available."
  address: Brussels, Belgium
  attachment:
  - filename: D18-1549.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/D18-1549.Attachment.pdf
  - filename: https://vimeo.com/306145842
    type: video
    url: https://vimeo.com/306145842
  author:
  - first: Guillaume
    full: Guillaume Lample
    id: guillaume-lample
    last: Lample
  - first: Myle
    full: Myle Ott
    id: myle-ott
    last: Ott
  - first: Alexis
    full: Alexis Conneau
    id: alexis-conneau
    last: Conneau
  - first: Ludovic
    full: Ludovic Denoyer
    id: ludovic-denoyer
    last: Denoyer
  - first: "Marc\u2019Aurelio"
    full: "Marc\u2019Aurelio Ranzato"
    id: marcaurelio-ranzato
    last: Ranzato
  author_string: "Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, Marc\u2019\
    Aurelio Ranzato"
  bibkey: lample-etal-2018-phrase
  bibtype: inproceedings
  booktitle: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing
  booktitle_html: Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing
  doi: 10.18653/v1/D18-1549
  month: October-November
  page_first: '5039'
  page_last: '5049'
  pages: "5039\u20135049"
  paper_id: '549'
  parent_volume_id: D18-1
  pdf: https://www.aclweb.org/anthology/D18-1549.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-1549.jpg
  title: Phrase-Based & Neural Unsupervised Machine Translation
  title_html: Phrase-Based &amp; Neural Unsupervised Machine Translation
  url: https://www.aclweb.org/anthology/D18-1549
  year: '2018'
D18-2000:
  address: Brussels, Belgium
  author:
  - first: Eduardo
    full: Eduardo Blanco
    id: eduardo-blanco
    last: Blanco
  - first: Wei
    full: Wei Lu
    id: wei-lu
    last: Lu
  author_string: Eduardo Blanco, Wei Lu
  bibkey: emnlp-2018-2018-empirical
  bibtype: proceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  month: November
  paper_id: '0'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2000.jpg
  title: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  title_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  url: https://www.aclweb.org/anthology/D18-2000
  year: '2018'
D18-2001:
  abstract: This paper describes SyntaViz, a visualization interface specifically
    designed for analyzing natural-language queries that were created by users of
    a voice-enabled product. SyntaViz provides a platform for browsing the ontology
    of user queries from a syntax-driven perspective, providing quick access to high-impact
    failure points of the existing intent understanding system and evidence for data-driven
    decisions in the development cycle. A case study on Xfinity X1 (a voice-enabled
    entertainment platform from Comcast) reveals that SyntaViz helps developers identify
    multiple action items in a short amount of time without any special training.
    SyntaViz has been open-sourced for the benefit of the community.
  address: Brussels, Belgium
  author:
  - first: Md Iftekhar
    full: Md Iftekhar Tanveer
    id: md-iftekhar-tanveer
    last: Tanveer
  - first: Ferhan
    full: Ferhan Ture
    id: ferhan-ture
    last: Ture
  author_string: Md Iftekhar Tanveer, Ferhan Ture
  bibkey: tanveer-ture-2018-syntaviz
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2001
  month: November
  page_first: '1'
  page_last: '6'
  pages: "1\u20136"
  paper_id: '1'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2001.jpg
  title: 'SyntaViz: Visualizing Voice Queries through a Syntax-Driven Hierarchical
    Ontology'
  title_html: '<span class="acl-fixed-case">S</span>ynta<span class="acl-fixed-case">V</span>iz:
    Visualizing Voice Queries through a Syntax-Driven Hierarchical Ontology'
  url: https://www.aclweb.org/anthology/D18-2001
  year: '2018'
D18-2002:
  abstract: 'We present TRANX, a transition-based neural semantic parser that maps
    natural language (NL) utterances into formal meaning representations (MRs). TRANX
    uses a transition system based on the abstract syntax description language for
    the target MR, which gives it two major advantages: (1) it is highly accurate,
    using information from the syntax of the target MR to constrain the output space
    and model the information flow, and (2) it is highly generalizable, and can easily
    be applied to new types of MR by just writing a new abstract syntax description
    corresponding to the allowable structures in the MR. Experiments on four different
    semantic parsing and code generation tasks show that our system is generalizable,
    extensible, and effective, registering strong results compared to existing neural
    semantic parsers.'
  address: Brussels, Belgium
  author:
  - first: Pengcheng
    full: Pengcheng Yin
    id: pengcheng-yin
    last: Yin
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  author_string: Pengcheng Yin, Graham Neubig
  bibkey: yin-neubig-2018-tranx
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2002
  month: November
  page_first: '7'
  page_last: '12'
  pages: "7\u201312"
  paper_id: '2'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2002.jpg
  title: 'TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing
    and Code Generation'
  title_html: '<span class="acl-fixed-case">TRANX</span>: A Transition-based Neural
    Abstract Syntax Parser for Semantic Parsing and Code Generation'
  url: https://www.aclweb.org/anthology/D18-2002
  year: '2018'
D18-2003:
  abstract: Data2Text Studio is a platform for automated text generation from structured
    data. It is equipped with a Semi-HMMs model to extract high-quality templates
    and corresponding trigger conditions from parallel data automatically, which improves
    the interactivity and interpretability of the generated text. In addition, several
    easy-to-use tools are provided for developers to edit templates of pre-trained
    models, and APIs are released for developers to call the pre-trained model to
    generate texts in third-party applications. We conduct experiments on RotoWire
    datasets for template extraction and text generation. The results show that our
    model achieves improvements on both tasks.
  address: Brussels, Belgium
  author:
  - first: Longxu
    full: Longxu Dou
    id: longxu-dou
    last: Dou
  - first: Guanghui
    full: Guanghui Qin
    id: guanghui-qin
    last: Qin
  - first: Jinpeng
    full: Jinpeng Wang
    id: jinpeng-wang
    last: Wang
  - first: Jin-Ge
    full: Jin-Ge Yao
    id: jin-ge-yao
    last: Yao
  - first: Chin-Yew
    full: Chin-Yew Lin
    id: chin-yew-lin
    last: Lin
  author_string: Longxu Dou, Guanghui Qin, Jinpeng Wang, Jin-Ge Yao, Chin-Yew Lin
  bibkey: dou-etal-2018-data2text
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2003
  month: November
  page_first: '13'
  page_last: '18'
  pages: "13\u201318"
  paper_id: '3'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2003.jpg
  title: 'Data2Text Studio: Automated Text Generation from Structured Data'
  title_html: '<span class="acl-fixed-case">D</span>ata2<span class="acl-fixed-case">T</span>ext
    Studio: Automated Text Generation from Structured Data'
  url: https://www.aclweb.org/anthology/D18-2003
  year: '2018'
D18-2004:
  abstract: We present SetExpander, a corpus-based system for expanding a seed set
    of terms into a more complete set of terms that belong to the same semantic class.
    SetExpander implements an iterative end-to-end workflow. It enables users to easily
    select a seed set of terms, expand it, view the expanded set, validate it, re-expand
    the validated set and store it, thus simplifying the extraction of domain-specific
    fine-grained semantic classes. SetExpander has been used successfully in real-life
    use cases including integration into an automated recruitment system and an issues
    and defects resolution system.
  address: Brussels, Belgium
  author:
  - first: Jonathan
    full: Jonathan Mamou
    id: jonathan-mamou
    last: Mamou
  - first: Oren
    full: Oren Pereg
    id: oren-pereg
    last: Pereg
  - first: Moshe
    full: Moshe Wasserblat
    id: moshe-wasserblat
    last: Wasserblat
  - first: Alon
    full: Alon Eirew
    id: alon-eirew
    last: Eirew
  - first: Yael
    full: Yael Green
    id: yael-green
    last: Green
  - first: Shira
    full: Shira Guskin
    id: shira-guskin
    last: Guskin
  - first: Peter
    full: Peter Izsak
    id: peter-izsak
    last: Izsak
  - first: Daniel
    full: Daniel Korat
    id: daniel-korat
    last: Korat
  author_string: Jonathan Mamou, Oren Pereg, Moshe Wasserblat, Alon Eirew, Yael Green,
    Shira Guskin, Peter Izsak, Daniel Korat
  bibkey: mamou-etal-2018-term
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2004
  month: November
  page_first: '19'
  page_last: '24'
  pages: "19\u201324"
  paper_id: '4'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2004.jpg
  title: Term Set Expansion based NLP Architect by Intel AI Lab
  title_html: Term Set Expansion based <span class="acl-fixed-case">NLP</span> Architect
    by <span class="acl-fixed-case">I</span>ntel <span class="acl-fixed-case">AI</span>
    Lab
  url: https://www.aclweb.org/anthology/D18-2004
  year: '2018'
D18-2005:
  abstract: MorAz is an open-source morphological analyzer for Azerbaijani Turkish.
    The analyzer is available through both as a website for interactive exploration
    and as a RESTful web service for integration into a natural language processing
    pipeline. MorAz implements the morphology of Azerbaijani Turkish in two-level
    using Helsinki finite-state transducer and wraps the analyzer with python scripts
    in a Django instance.
  address: Brussels, Belgium
  author:
  - first: Berke
    full: "Berke \xD6zen\xE7"
    id: berke-ozenc
    last: "\xD6zen\xE7"
  - first: Razieh
    full: Razieh Ehsani
    id: razieh-ehsani
    last: Ehsani
  - first: Ercan
    full: Ercan Solak
    id: ercan-solak
    last: Solak
  author_string: "Berke \xD6zen\xE7, Razieh Ehsani, Ercan Solak"
  bibkey: ozenc-etal-2018-moraz
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2005
  month: November
  page_first: '25'
  page_last: '29'
  pages: "25\u201329"
  paper_id: '5'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2005.pdf
  publisher: Association for Computational Linguistics
  revision:
  - id: '1'
    url: https://www.aclweb.org/anthology/D18-2005v1.pdf
    value: D18-2005v1
  - explanation: No description of the changes were recorded.
    id: '2'
    url: https://www.aclweb.org/anthology/D18-2005v2.pdf
    value: D18-2005v2
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2005.jpg
  title: 'MorAz: an Open-source Morphological Analyzer for Azerbaijani Turkish'
  title_html: '<span class="acl-fixed-case">M</span>or<span class="acl-fixed-case">A</span>z:
    an Open-source Morphological Analyzer for <span class="acl-fixed-case">A</span>zerbaijani
    <span class="acl-fixed-case">T</span>urkish'
  url: https://www.aclweb.org/anthology/D18-2005
  year: '2018'
D18-2006:
  abstract: "We present a visualisation tool which aims to illuminate the inner workings\
    \ of an LSTM model for question answering. It plots heatmaps of neurons\u2019\
    \ firings and allows a user to check the dependency between neurons and manual\
    \ features. The system possesses an interactive web-interface and can be adapted\
    \ to other models and domains."
  address: Brussels, Belgium
  author:
  - first: Ekaterina
    full: Ekaterina Loginova
    id: ekaterina-loginova
    last: Loginova
  - first: "G\xFCnter"
    full: "G\xFCnter Neumann"
    id: gunter-neumann
    last: Neumann
  author_string: "Ekaterina Loginova, G\xFCnter Neumann"
  bibkey: loginova-neumann-2018-interactive
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2006
  month: November
  page_first: '30'
  page_last: '35'
  pages: "30\u201335"
  paper_id: '6'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2006.jpg
  title: An Interactive Web-Interface for Visualizing the Inner Workings of the Question
    Answering LSTM
  title_html: An Interactive Web-Interface for Visualizing the Inner Workings of the
    Question Answering <span class="acl-fixed-case">LSTM</span>
  url: https://www.aclweb.org/anthology/D18-2006
  year: '2018'
D18-2007:
  abstract: Neural networks models have gained unprecedented popularity in natural
    language processing due to their state-of-the-art performance and the flexible
    end-to-end training scheme. Despite their advantages, the lack of interpretability
    hinders the deployment and refinement of the models. In this work, we present
    a flexible visualization library for creating customized visual analytic environments,
    in which the user can investigate and interrogate the relationships among the
    input, the model internals (i.e., attention), and the output predictions, which
    in turn shed light on the model decision-making process.
  address: Brussels, Belgium
  author:
  - first: Shusen
    full: Shusen Liu
    id: shusen-liu
    last: Liu
  - first: Tao
    full: Tao Li
    id: tao-li
    last: Li
  - first: Zhimin
    full: Zhimin Li
    id: zhimin-li
    last: Li
  - first: Vivek
    full: Vivek Srikumar
    id: vivek-srikumar
    last: Srikumar
  - first: Valerio
    full: Valerio Pascucci
    id: valerio-pascucci
    last: Pascucci
  - first: Peer-Timo
    full: Peer-Timo Bremer
    id: peer-timo-bremer
    last: Bremer
  author_string: Shusen Liu, Tao Li, Zhimin Li, Vivek Srikumar, Valerio Pascucci,
    Peer-Timo Bremer
  bibkey: liu-etal-2018-visual
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2007
  month: November
  page_first: '36'
  page_last: '41'
  pages: "36\u201341"
  paper_id: '7'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2007.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2007.jpg
  title: Visual Interrogation of Attention-Based Models for Natural Language Inference
    and Machine Comprehension
  title_html: Visual Interrogation of Attention-Based Models for Natural Language
    Inference and Machine Comprehension
  url: https://www.aclweb.org/anthology/D18-2007
  year: '2018'
D18-2008:
  abstract: Most machine learning systems for natural language processing are tailored
    to specific tasks. As a result, comparability of models across tasks is missing
    and their applicability to new tasks is limited. This affects end users without
    machine learning experience as well as model developers. To address these limitations,
    we present DERE, a novel framework for declarative specification and compilation
    of template-based information extraction. It uses a generic specification language
    for the task and for data annotations in terms of spans and frames. This formalism
    enables the representation of a large variety of natural language processing challenges.
    The backend can be instantiated by different models, following different paradigms.
    The clear separation of frame specification and model backend will ease the implementation
    of new models and the evaluation of different models across different tasks. Furthermore,
    it simplifies transfer learning, joint learning across tasks and/or domains as
    well as the assessment of model generalizability. DERE is available as open-source
    software.
  address: Brussels, Belgium
  author:
  - first: Heike
    full: Heike Adel
    id: heike-adel
    last: Adel
  - first: Laura Ana Maria
    full: Laura Ana Maria Bostan
    id: laura-ana-maria-bostan1
    last: Bostan
  - first: Sean
    full: Sean Papay
    id: sean-papay
    last: Papay
  - first: Sebastian
    full: "Sebastian Pad\xF3"
    id: sebastian-pado
    last: "Pad\xF3"
  - first: Roman
    full: Roman Klinger
    id: roman-klinger
    last: Klinger
  author_string: "Heike Adel, Laura Ana Maria Bostan, Sean Papay, Sebastian Pad\xF3\
    , Roman Klinger"
  bibkey: adel-etal-2018-dere
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2008
  month: November
  page_first: '42'
  page_last: '47'
  pages: "42\u201347"
  paper_id: '8'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2008.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2008.jpg
  title: 'DERE: A Task and Domain-Independent Slot Filling Framework for Declarative
    Relation Extraction'
  title_html: '<span class="acl-fixed-case">DERE</span>: A Task and Domain-Independent
    Slot Filling Framework for Declarative Relation Extraction'
  url: https://www.aclweb.org/anthology/D18-2008
  year: '2018'
D18-2009:
  abstract: In this paper, we present Par4Sem, a semantic writing aid tool based on
    adaptive paraphrasing. Unlike many annotation tools that are primarily used to
    collect training examples, Par4Sem is integrated into a real word application,
    in this case a writing aid tool, in order to collect training examples from usage
    data. Par4Sem is a tool, which supports an adaptive, iterative, and interactive
    process where the underlying machine learning models are updated for each iteration
    using new training examples from usage data. After motivating the use of ever-learning
    tools in NLP applications, we evaluate Par4Sem by adopting it to a text simplification
    task through mere usage.
  address: Brussels, Belgium
  author:
  - first: Seid Muhie
    full: Seid Muhie Yimam
    id: seid-muhie-yimam
    last: Yimam
  - first: Chris
    full: Chris Biemann
    id: chris-biemann
    last: Biemann
  author_string: Seid Muhie Yimam, Chris Biemann
  bibkey: yimam-biemann-2018-demonstrating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2009
  month: November
  page_first: '48'
  page_last: '53'
  pages: "48\u201353"
  paper_id: '9'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2009.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2009.jpg
  title: Demonstrating Par4Sem - A Semantic Writing Aid with Adaptive Paraphrasing
  title_html: Demonstrating <span class="acl-fixed-case">P</span>ar4<span class="acl-fixed-case">S</span>em
    - A Semantic Writing Aid with Adaptive Paraphrasing
  url: https://www.aclweb.org/anthology/D18-2009
  year: '2018'
D18-2010:
  abstract: We present a three-part toolkit for developing morphological analyzers
    for languages without natural word boundaries. The first part is a C++11/14 lattice-based
    morphological analysis library that uses a combination of linear and recurrent
    neural net language models for analysis. The other parts are a tool for exposing
    problems in the trained model and a partial annotation tool. Our morphological
    analyzer of Japanese achieves new SOTA on Jumandic-based corpora while being 250
    times faster than the previous one. We also perform a small experiment and quantitive
    analysis and experience of using development tools. All components of the toolkit
    is open source and available under a permissive Apache 2 License.
  address: Brussels, Belgium
  author:
  - first: Arseny
    full: Arseny Tolmachev
    id: arseny-tolmachev
    last: Tolmachev
  - first: Daisuke
    full: Daisuke Kawahara
    id: daisuke-kawahara
    last: Kawahara
  - first: Sadao
    full: Sadao Kurohashi
    id: sadao-kurohashi
    last: Kurohashi
  author_string: Arseny Tolmachev, Daisuke Kawahara, Sadao Kurohashi
  bibkey: tolmachev-etal-2018-juman
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2010
  month: November
  page_first: '54'
  page_last: '59'
  pages: "54\u201359"
  paper_id: '10'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2010.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2010.jpg
  title: 'Juman++: A Morphological Analysis Toolkit for Scriptio Continua'
  title_html: '<span class="acl-fixed-case">J</span>uman++: A Morphological Analysis
    Toolkit for Scriptio Continua'
  url: https://www.aclweb.org/anthology/D18-2010
  year: '2018'
D18-2011:
  abstract: In times of fake news and alternative facts, pro and con arguments on
    controversial topics are of increasing importance. Recently, we presented args.me
    as the first search engine for arguments on the web. In its initial version, args.me
    ranked arguments solely by their relevance to a topic queried for, making it hard
    to learn about the diverse topical aspects covered by the search results. To tackle
    this shortcoming, we integrated a visualization interface for result exploration
    in args.me that provides an instant overview of the main aspects in a barycentric
    coordinate system. This topic space is generated ad-hoc from controversial issues
    on Wikipedia and argument-specific LDA models. In two case studies, we demonstrate
    how individual arguments can be found easily through interactions with the visualization,
    such as highlighting and filtering.
  address: Brussels, Belgium
  author:
  - first: Yamen
    full: Yamen Ajjour
    id: yamen-ajjour
    last: Ajjour
  - first: Henning
    full: Henning Wachsmuth
    id: henning-wachsmuth
    last: Wachsmuth
  - first: Dora
    full: Dora Kiesel
    id: dora-kiesel
    last: Kiesel
  - first: Patrick
    full: Patrick Riehmann
    id: patrick-riehmann
    last: Riehmann
  - first: Fan
    full: Fan Fan
    id: fan-fan
    last: Fan
  - first: Giuliano
    full: Giuliano Castiglia
    id: giuliano-castiglia
    last: Castiglia
  - first: Rosemary
    full: Rosemary Adejoh
    id: rosemary-adejoh
    last: Adejoh
  - first: Bernd
    full: "Bernd Fr\xF6hlich"
    id: bernd-frohlich
    last: "Fr\xF6hlich"
  - first: Benno
    full: Benno Stein
    id: benno-stein
    last: Stein
  author_string: "Yamen Ajjour, Henning Wachsmuth, Dora Kiesel, Patrick Riehmann,\
    \ Fan Fan, Giuliano Castiglia, Rosemary Adejoh, Bernd Fr\xF6hlich, Benno Stein"
  bibkey: ajjour-etal-2018-visualization
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2011
  month: November
  page_first: '60'
  page_last: '65'
  pages: "60\u201365"
  paper_id: '11'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2011.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2011.jpg
  title: Visualization of the Topic Space of Argument Search Results in args.me
  title_html: Visualization of the Topic Space of Argument Search Results in args.me
  url: https://www.aclweb.org/anthology/D18-2011
  year: '2018'
D18-2012:
  abstract: This paper describes SentencePiece, a language-independent subword tokenizer
    and detokenizer designed for Neural-based text processing, including Neural Machine
    Translation. It provides open-source C++ and Python implementations for subword
    units. While existing subword segmentation tools assume that the input is pre-tokenized
    into word sequences, SentencePiece can train subword models directly from raw
    sentences, which allows us to make a purely end-to-end and language independent
    system. We perform a validation experiment of NMT on English-Japanese machine
    translation, and find that it is possible to achieve comparable accuracy to direct
    subword training from raw sentences. We also compare the performance of subword
    training and segmentation with various configurations. SentencePiece is available
    under the Apache 2 license at https://github.com/google/sentencepiece.
  address: Brussels, Belgium
  author:
  - first: Taku
    full: Taku Kudo
    id: taku-kudo
    last: Kudo
  - first: John
    full: John Richardson
    id: john-richardson
    last: Richardson
  author_string: Taku Kudo, John Richardson
  bibkey: kudo-richardson-2018-sentencepiece
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2012
  month: November
  page_first: '66'
  page_last: '71'
  pages: "66\u201371"
  paper_id: '12'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2012.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2012.jpg
  title: 'SentencePiece: A simple and language independent subword tokenizer and detokenizer
    for Neural Text Processing'
  title_html: '<span class="acl-fixed-case">S</span>entence<span class="acl-fixed-case">P</span>iece:
    A simple and language independent subword tokenizer and detokenizer for Neural
    Text Processing'
  url: https://www.aclweb.org/anthology/D18-2012
  year: '2018'
D18-2013:
  abstract: 'Automatic extraction of temporal information is important for natural
    language understanding. It involves two basic tasks: (1) Understanding time expressions
    that are mentioned explicitly in text (e.g., February 27, 1998 or tomorrow), and
    (2) Understanding temporal information that is conveyed implicitly via relations.
    This paper introduces CogCompTime, a system that has these two important functionalities.
    It incorporates the most recent progress, achieves state-of-the-art performance,
    and is publicly available at http://cogcomp.org/page/publication_view/844.'
  address: Brussels, Belgium
  author:
  - first: Qiang
    full: Qiang Ning
    id: qiang-ning
    last: Ning
  - first: Ben
    full: Ben Zhou
    id: ben-zhou
    last: Zhou
  - first: Zhili
    full: Zhili Feng
    id: zhili-feng
    last: Feng
  - first: Haoruo
    full: Haoruo Peng
    id: haoruo-peng
    last: Peng
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Qiang Ning, Ben Zhou, Zhili Feng, Haoruo Peng, Dan Roth
  bibkey: ning-etal-2018-cogcomptime
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2013
  month: November
  page_first: '72'
  page_last: '77'
  pages: "72\u201377"
  paper_id: '13'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2013.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2013.jpg
  title: 'CogCompTime: A Tool for Understanding Time in Natural Language'
  title_html: '<span class="acl-fixed-case">C</span>og<span class="acl-fixed-case">C</span>omp<span
    class="acl-fixed-case">T</span>ime: A Tool for Understanding Time in Natural Language'
  url: https://www.aclweb.org/anthology/D18-2013
  year: '2018'
D18-2014:
  abstract: We introduce an advanced information extraction pipeline to automatically
    process very large collections of unstructured textual data for the purpose of
    investigative journalism. The pipeline serves as a new input processor for the
    upcoming major release of our New/s/leak 2.0 software, which we develop in cooperation
    with a large German news organization. The use case is that journalists receive
    a large collection of files up to several Gigabytes containing unknown contents.
    Collections may originate either from official disclosures of documents, e.g.
    Freedom of Information Act requests, or unofficial data leaks.
  address: Brussels, Belgium
  author:
  - first: Gregor
    full: Gregor Wiedemann
    id: gregor-wiedemann
    last: Wiedemann
  - first: Seid Muhie
    full: Seid Muhie Yimam
    id: seid-muhie-yimam
    last: Yimam
  - first: Chris
    full: Chris Biemann
    id: chris-biemann
    last: Biemann
  author_string: Gregor Wiedemann, Seid Muhie Yimam, Chris Biemann
  bibkey: wiedemann-etal-2018-multilingual
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2014
  month: November
  page_first: '78'
  page_last: '83'
  pages: "78\u201383"
  paper_id: '14'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2014.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2014.jpg
  title: A Multilingual Information Extraction Pipeline for Investigative Journalism
  title_html: A Multilingual Information Extraction Pipeline for Investigative Journalism
  url: https://www.aclweb.org/anthology/D18-2014
  year: '2018'
D18-2015:
  abstract: Training and testing many possible parameters or model architectures of
    state-of-the-art machine translation or automatic speech recognition system is
    a cumbersome task. They usually require a long pipeline of commands reaching from
    pre-processing the training data to post-processing and evaluating the output.
  address: Brussels, Belgium
  author:
  - first: Jan-Thorsten
    full: Jan-Thorsten Peter
    id: jan-thorsten-peter
    last: Peter
  - first: Eugen
    full: Eugen Beck
    id: eugen-beck
    last: Beck
  - first: Hermann
    full: Hermann Ney
    id: hermann-ney
    last: Ney
  author_string: Jan-Thorsten Peter, Eugen Beck, Hermann Ney
  bibkey: peter-etal-2018-sisyphus
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2015
  month: November
  page_first: '84'
  page_last: '89'
  pages: "84\u201389"
  paper_id: '15'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2015.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2015.jpg
  title: Sisyphus, a Workflow Manager Designed for Machine Translation and Automatic
    Speech Recognition
  title_html: Sisyphus, a Workflow Manager Designed for Machine Translation and Automatic
    Speech Recognition
  url: https://www.aclweb.org/anthology/D18-2015
  year: '2018'
D18-2016:
  abstract: 'We describe KT-Speech-Crawler: an approach for automatic dataset construction
    for speech recognition by crawling YouTube videos. We outline several filtering
    and post-processing steps, which extract samples that can be used for training
    end-to-end neural speech recognition systems. In our experiments, we demonstrate
    that a single-core version of the crawler can obtain around 150 hours of transcribed
    speech within a day, containing an estimated 3.5% word error rate in the transcriptions.
    Automatically collected samples contain reading and spontaneous speech recorded
    in various conditions including background noise and music, distant microphone
    recordings, and a variety of accents and reverberation. When training a deep neural
    network on speech recognition, we observed around 40% word error rate reduction
    on the Wall Street Journal dataset by integrating 200 hours of the collected samples
    into the training set.'
  address: Brussels, Belgium
  author:
  - first: Egor
    full: Egor Lakomkin
    id: egor-lakomkin
    last: Lakomkin
  - first: Sven
    full: Sven Magg
    id: sven-magg
    last: Magg
  - first: Cornelius
    full: Cornelius Weber
    id: cornelius-weber
    last: Weber
  - first: Stefan
    full: Stefan Wermter
    id: stefan-wermter
    last: Wermter
  author_string: Egor Lakomkin, Sven Magg, Cornelius Weber, Stefan Wermter
  bibkey: lakomkin-etal-2018-kt
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2016
  month: November
  page_first: '90'
  page_last: '95'
  pages: "90\u201395"
  paper_id: '16'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2016.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2016.jpg
  title: 'KT-Speech-Crawler: Automatic Dataset Construction for Speech Recognition
    from YouTube Videos'
  title_html: '<span class="acl-fixed-case">KT</span>-Speech-Crawler: Automatic Dataset
    Construction for Speech Recognition from <span class="acl-fixed-case">Y</span>ou<span
    class="acl-fixed-case">T</span>ube Videos'
  url: https://www.aclweb.org/anthology/D18-2016
  year: '2018'
D18-2017:
  abstract: "Group discussions are usually aimed at sharing opinions, reaching consensus\
    \ and making good decisions based on group knowledge. During a discussion, participants\
    \ might adjust their own opinions as well as tune their attitudes towards others\u2019\
    \ opinions, based on the unfolding interactions. In this paper, we demonstrate\
    \ a framework to visualize such dynamics; at each instant of a conversation, the\
    \ participants\u2019 opinions and potential influence on their counterparts is\
    \ easily visualized. We use multi-party meeting opinion mining based on bipartite\
    \ graphs to extract opinions and calculate mutual influential factors, using the\
    \ Lunar Survival Task as a study case."
  address: Brussels, Belgium
  author:
  - first: Ni
    full: Ni Zhang
    id: ni-zhang
    last: Zhang
  - first: Tongtao
    full: Tongtao Zhang
    id: tongtao-zhang
    last: Zhang
  - first: Indrani
    full: Indrani Bhattacharya
    id: indrani-bhattacharya
    last: Bhattacharya
  - first: Heng
    full: Heng Ji
    id: heng-ji
    last: Ji
  - first: Rich
    full: Rich Radke
    id: rich-radke
    last: Radke
  author_string: Ni Zhang, Tongtao Zhang, Indrani Bhattacharya, Heng Ji, Rich Radke
  bibkey: zhang-etal-2018-visualizing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2017
  month: November
  page_first: '96'
  page_last: '101'
  pages: "96\u2013101"
  paper_id: '17'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2017.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2017.jpg
  title: Visualizing Group Dynamics based on Multiparty Meeting Understanding
  title_html: Visualizing Group Dynamics based on Multiparty Meeting Understanding
  url: https://www.aclweb.org/anthology/D18-2017
  year: '2018'
D18-2018:
  abstract: Recent work introduces the AI2 Reasoning Challenge (ARC) and the associated
    ARC dataset that partitions open domain, complex science questions into an Easy
    Set and a Challenge Set. That work includes an analysis of 100 questions with
    respect to the types of knowledge and reasoning required to answer them. However,
    it does not include clear definitions of these types, nor does it offer information
    about the quality of the labels or the annotation process used. In this paper,
    we introduce a novel interface for human annotation of science question-answer
    pairs with their respective knowledge and reasoning types, in order that the classification
    of new questions may be improved. We build on the classification schema proposed
    by prior work on the ARC dataset, and evaluate the effectiveness of our interface
    with a preliminary study involving 10 participants.
  address: Brussels, Belgium
  author:
  - first: Michael
    full: Michael Boratko
    id: michael-boratko
    last: Boratko
  - first: Harshit
    full: Harshit Padigela
    id: harshit-padigela
    last: Padigela
  - first: Divyendra
    full: Divyendra Mikkilineni
    id: divyendra-mikkilineni
    last: Mikkilineni
  - first: Pritish
    full: Pritish Yuvraj
    id: pritish-yuvraj
    last: Yuvraj
  - first: Rajarshi
    full: Rajarshi Das
    id: rajarshi-das
    last: Das
  - first: Andrew
    full: Andrew McCallum
    id: andrew-mccallum
    last: McCallum
  - first: Maria
    full: Maria Chang
    id: maria-chang
    last: Chang
  - first: Achille
    full: Achille Fokoue
    id: achille-fokoue-nkoutche
    last: Fokoue
  - first: Pavan
    full: Pavan Kapanipathi
    id: pavan-kapanipathi
    last: Kapanipathi
  - first: Nicholas
    full: Nicholas Mattei
    id: nicholas-mattei
    last: Mattei
  - first: Ryan
    full: Ryan Musa
    id: ryan-musa
    last: Musa
  - first: Kartik
    full: Kartik Talamadupula
    id: kartik-talamadupula
    last: Talamadupula
  - first: Michael
    full: Michael Witbrock
    id: michael-j-witbrock
    last: Witbrock
  author_string: Michael Boratko, Harshit Padigela, Divyendra Mikkilineni, Pritish
    Yuvraj, Rajarshi Das, Andrew McCallum, Maria Chang, Achille Fokoue, Pavan Kapanipathi,
    Nicholas Mattei, Ryan Musa, Kartik Talamadupula, Michael Witbrock
  bibkey: boratko-etal-2018-interface
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2018
  month: November
  page_first: '102'
  page_last: '107'
  pages: "102\u2013107"
  paper_id: '18'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2018.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2018.jpg
  title: An Interface for Annotating Science Questions
  title_html: An Interface for Annotating Science Questions
  url: https://www.aclweb.org/anthology/D18-2018
  year: '2018'
D18-2019:
  abstract: In this paper, we present APLenty, an annotation tool for creating high-quality
    sequence labeling datasets using active and proactive learning. A major innovation
    of our tool is the integration of automatic annotation with active learning and
    proactive learning. This makes the task of creating labeled datasets easier, less
    time-consuming and requiring less human effort. APLenty is highly flexible and
    can be adapted to various other tasks.
  address: Brussels, Belgium
  author:
  - first: Minh-Quoc
    full: Minh-Quoc Nghiem
    id: minh-quoc-nghiem
    last: Nghiem
  - first: Sophia
    full: Sophia Ananiadou
    id: sophia-ananiadou
    last: Ananiadou
  author_string: Minh-Quoc Nghiem, Sophia Ananiadou
  bibkey: nghiem-ananiadou-2018-aplenty
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2019
  month: November
  page_first: '108'
  page_last: '113'
  pages: "108\u2013113"
  paper_id: '19'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2019.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2019.jpg
  title: 'APLenty: annotation tool for creating high-quality datasets using active
    and proactive learning'
  title_html: '<span class="acl-fixed-case">APL</span>enty: annotation tool for creating
    high-quality datasets using active and proactive learning'
  url: https://www.aclweb.org/anthology/D18-2019
  year: '2018'
D18-2020:
  abstract: Most approaches to Knowledge Base Question Answering are based on semantic
    parsing. In this paper, we present a tool that aids in debugging of question answering
    systems that construct a structured semantic representation for the input question.
    Previous work has largely focused on building question answering interfaces or
    evaluation frameworks that unify multiple data sets. The primary objective of
    our system is to enable interactive debugging of model predictions on individual
    instances (questions) and to simplify manual error analysis. Our interactive interface
    helps researchers to understand the shortcomings of a particular model, qualitatively
    analyze the complete pipeline and compare different models. A set of sit-by sessions
    was used to validate our interface design.
  address: Brussels, Belgium
  author:
  - first: Daniil
    full: Daniil Sorokin
    id: daniil-sorokin
    last: Sorokin
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Daniil Sorokin, Iryna Gurevych
  bibkey: sorokin-gurevych-2018-interactive
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2020
  month: November
  page_first: '114'
  page_last: '119'
  pages: "114\u2013119"
  paper_id: '20'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2020.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2020.jpg
  title: Interactive Instance-based Evaluation of Knowledge Base Question Answering
  title_html: Interactive Instance-based Evaluation of Knowledge Base Question Answering
  url: https://www.aclweb.org/anthology/D18-2020
  year: '2018'
D18-2021:
  abstract: Vector space embedding models like word2vec, GloVe, and fastText are extremely
    popular representations in natural language processing (NLP) applications. We
    present Magnitude, a fast, lightweight tool for utilizing and processing embeddings.
    Magnitude is an open source Python package with a compact vector storage file
    format that allows for efficient manipulation of huge numbers of embeddings. Magnitude
    performs common operations up to 60 to 6,000 times faster than Gensim. Magnitude
    introduces several novel features for improved robustness like out-of-vocabulary
    lookups.
  address: Brussels, Belgium
  author:
  - first: Ajay
    full: Ajay Patel
    id: ajay-patel
    last: Patel
  - first: Alexander
    full: Alexander Sands
    id: alexander-sands
    last: Sands
  - first: Chris
    full: Chris Callison-Burch
    id: chris-callison-burch
    last: Callison-Burch
  - first: Marianna
    full: Marianna Apidianaki
    id: marianna-apidianaki
    last: Apidianaki
  author_string: Ajay Patel, Alexander Sands, Chris Callison-Burch, Marianna Apidianaki
  bibkey: patel-etal-2018-magnitude
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2021
  month: November
  page_first: '120'
  page_last: '126'
  pages: "120\u2013126"
  paper_id: '21'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2021.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2021.jpg
  title: 'Magnitude: A Fast, Efficient Universal Vector Embedding Utility Package'
  title_html: '<span class="acl-fixed-case">M</span>agnitude: A Fast, Efficient Universal
    Vector Embedding Utility Package'
  url: https://www.aclweb.org/anthology/D18-2021
  year: '2018'
D18-2022:
  abstract: Annotating entity mentions and linking them to a knowledge resource are
    essential tasks in many domains. It disambiguates mentions, introduces cross-document
    coreferences, and the resources contribute extra information, e.g. taxonomic relations.
    Such tasks benefit from text annotation tools that integrate a search which covers
    the text, the annotations, as well as the knowledge resource. However, to the
    best of our knowledge, no current tools integrate knowledge-supported search as
    well as entity linking support. We address this gap by introducing knowledge-supported
    search functionality into the INCEpTION text annotation platform. In our approach,
    cross-document references are created by linking entity mentions to a knowledge
    base in the form of a structured hierarchical vocabulary. The resulting annotations
    are then indexed to enable fast and yet complex queries taking into account the
    text, the annotations, and the vocabulary structure.
  address: Brussels, Belgium
  author:
  - first: Beto
    full: Beto Boullosa
    id: beto-boullosa
    last: Boullosa
  - first: Richard
    full: Richard Eckart de Castilho
    id: richard-eckart-de-castilho
    last: Eckart de Castilho
  - first: Naveen
    full: Naveen Kumar
    id: naveen-kumar-laskari
    last: Kumar
  - first: Jan-Christoph
    full: Jan-Christoph Klie
    id: jan-christoph-klie
    last: Klie
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Beto Boullosa, Richard Eckart de Castilho, Naveen Kumar, Jan-Christoph
    Klie, Iryna Gurevych
  bibkey: boullosa-etal-2018-integrating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2022
  month: November
  page_first: '127'
  page_last: '132'
  pages: "127\u2013132"
  paper_id: '22'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2022.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2022.jpg
  title: Integrating Knowledge-Supported Search into the INCEpTION Annotation Platform
  title_html: Integrating Knowledge-Supported Search into the <span class="acl-fixed-case">INCE</span>p<span
    class="acl-fixed-case">TION</span> Annotation Platform
  url: https://www.aclweb.org/anthology/D18-2022
  year: '2018'
D18-2023:
  abstract: "This paper presents an open-source neural machine translation toolkit\
    \ named CytonMT. The toolkit is built from scratch only using C++ and NVIDIA\u2019\
    s GPU-accelerated libraries. The toolkit features training efficiency, code simplicity\
    \ and translation quality. Benchmarks show that cytonMT accelerates the training\
    \ speed by 64.5% to 110.8% on neural networks of various sizes, and achieves competitive\
    \ translation quality."
  address: Brussels, Belgium
  author:
  - first: Xiaolin
    full: Xiaolin Wang
    id: xiaolin-wang
    last: Wang
  - first: Masao
    full: Masao Utiyama
    id: masao-utiyama
    last: Utiyama
  - first: Eiichiro
    full: Eiichiro Sumita
    id: eiichiro-sumita
    last: Sumita
  author_string: Xiaolin Wang, Masao Utiyama, Eiichiro Sumita
  bibkey: wang-etal-2018-cytonmt
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2023
  month: November
  page_first: '133'
  page_last: '138'
  pages: "133\u2013138"
  paper_id: '23'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2023.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2023.jpg
  title: 'CytonMT: an Efficient Neural Machine Translation Open-source Toolkit Implemented
    in C++'
  title_html: '<span class="acl-fixed-case">C</span>yton<span class="acl-fixed-case">MT</span>:
    an Efficient Neural Machine Translation Open-source Toolkit Implemented in <span
    class="acl-fixed-case">C</span>++'
  url: https://www.aclweb.org/anthology/D18-2023
  year: '2018'
D18-2024:
  abstract: We release an open toolkit for knowledge embedding (OpenKE), which provides
    a unified framework and various fundamental models to embed knowledge graphs into
    a continuous low-dimensional space. OpenKE prioritizes operational efficiency
    to support quick model validation and large-scale knowledge representation learning.
    Meanwhile, OpenKE maintains sufficient modularity and extensibility to easily
    incorporate new models into the framework. Besides the toolkit, the embeddings
    of some existing large-scale knowledge graphs pre-trained by OpenKE are also available,
    which can be directly applied for many applications including information retrieval,
    personalized recommendation and question answering. The toolkit, documentation,
    and pre-trained embeddings are all released on http://openke.thunlp.org/.
  address: Brussels, Belgium
  author:
  - first: Xu
    full: Xu Han
    id: xu-han
    last: Han
  - first: Shulin
    full: Shulin Cao
    id: shulin-cao
    last: Cao
  - first: Xin
    full: Xin Lv
    id: xin-lv
    last: Lv
  - first: Yankai
    full: Yankai Lin
    id: yankai-lin
    last: Lin
  - first: Zhiyuan
    full: Zhiyuan Liu
    id: zhiyuan-liu
    last: Liu
  - first: Maosong
    full: Maosong Sun
    id: maosong-sun
    last: Sun
  - first: Juanzi
    full: Juanzi Li
    id: juanzi-li
    last: Li
  author_string: Xu Han, Shulin Cao, Xin Lv, Yankai Lin, Zhiyuan Liu, Maosong Sun,
    Juanzi Li
  bibkey: han-etal-2018-openke
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2024
  month: November
  page_first: '139'
  page_last: '144'
  pages: "139\u2013144"
  paper_id: '24'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2024.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2024.jpg
  title: 'OpenKE: An Open Toolkit for Knowledge Embedding'
  title_html: '<span class="acl-fixed-case">O</span>pen<span class="acl-fixed-case">KE</span>:
    An Open Toolkit for Knowledge Embedding'
  url: https://www.aclweb.org/anthology/D18-2024
  year: '2018'
D18-2025:
  abstract: We present LIA, an intelligent personal assistant that can be programmed
    using natural language. Our system demonstrates multiple competencies towards
    learning from human-like interactions. These include the ability to be taught
    reusable conditional procedures, the ability to be taught new knowledge about
    the world (concepts in an ontology) and the ability to be taught how to ground
    that knowledge in a set of sensors and effectors. Building such a system highlights
    design questions regarding the overall architecture that such an agent should
    have, as well as questions about parsing and grounding language in situational
    contexts. We outline key properties of this architecture, and demonstrate a prototype
    that embodies them in the form of a personal assistant on an Android device.
  address: Brussels, Belgium
  author:
  - first: Igor
    full: Igor Labutov
    id: igor-labutov
    last: Labutov
  - first: Shashank
    full: Shashank Srivastava
    id: shashank-srivastava
    last: Srivastava
  - first: Tom
    full: Tom Mitchell
    id: tom-mitchell
    last: Mitchell
  author_string: Igor Labutov, Shashank Srivastava, Tom Mitchell
  bibkey: labutov-etal-2018-lia
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2025
  month: November
  page_first: '145'
  page_last: '150'
  pages: "145\u2013150"
  paper_id: '25'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2025.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2025.jpg
  title: 'LIA: A Natural Language Programmable Personal Assistant'
  title_html: '<span class="acl-fixed-case">LIA</span>: A Natural Language Programmable
    Personal Assistant'
  url: https://www.aclweb.org/anthology/D18-2025
  year: '2018'
D18-2026:
  abstract: This paper describes PizzaPal, a voice-only agent for ordering pizza,
    as well as the Conversational AI architecture built at b4.ai. Based on the principles
    of high-density conversational AI, it supports natural and flexible interactions
    through neural conversational language understanding, robust dialog state tracking,
    and hierarchical task decomposition.
  address: Brussels, Belgium
  author:
  - first: Antoine
    full: Antoine Raux
    id: antoine-raux
    last: Raux
  - first: Yi
    full: Yi Ma
    id: yi-ma
    last: Ma
  - first: Paul
    full: Paul Yang
    id: paul-yang
    last: Yang
  - first: Felicia
    full: Felicia Wong
    id: felicia-wong
    last: Wong
  author_string: Antoine Raux, Yi Ma, Paul Yang, Felicia Wong
  bibkey: raux-etal-2018-pizzapal
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2026
  month: November
  page_first: '151'
  page_last: '156'
  pages: "151\u2013156"
  paper_id: '26'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2026.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2026.jpg
  title: 'PizzaPal: Conversational Pizza Ordering using a High-Density Conversational
    AI Platform'
  title_html: '<span class="acl-fixed-case">P</span>izza<span class="acl-fixed-case">P</span>al:
    Conversational Pizza Ordering using a High-Density Conversational <span class="acl-fixed-case">AI</span>
    Platform'
  url: https://www.aclweb.org/anthology/D18-2026
  year: '2018'
D18-2027:
  abstract: 'We demonstrate an end-to-end approach for building conversational interfaces
    from prototype to production that has proven to work well for a number of applications
    across diverse verticals. Our architecture improves on the standard domain-intent-entity
    classification hierarchy and dialogue management architecture by leveraging shallow
    semantic parsing. We observe that NLU systems for industry applications often
    require more structured representations of entity relations than provided by the
    standard hierarchy, yet without requiring full semantic parses which are often
    inaccurate on real-world conversational data. We distinguish two kinds of semantic
    properties that can be provided through shallow semantic parsing: entity groups
    and entity roles. We also provide live demos of conversational apps built for
    two different use cases: food ordering and meeting control.'
  address: Brussels, Belgium
  author:
  - first: Arushi
    full: Arushi Raghuvanshi
    id: arushi-raghuvanshi
    last: Raghuvanshi
  - first: Lucien
    full: Lucien Carroll
    id: lucien-carroll
    last: Carroll
  - first: Karthik
    full: Karthik Raghunathan
    id: karthik-raghunathan
    last: Raghunathan
  author_string: Arushi Raghuvanshi, Lucien Carroll, Karthik Raghunathan
  bibkey: raghuvanshi-etal-2018-developing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2027
  month: November
  page_first: '157'
  page_last: '162'
  pages: "157\u2013162"
  paper_id: '27'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2027.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2027.jpg
  title: Developing Production-Level Conversational Interfaces with Shallow Semantic
    Parsing
  title_html: Developing Production-Level Conversational Interfaces with Shallow Semantic
    Parsing
  url: https://www.aclweb.org/anthology/D18-2027
  year: '2018'
D18-2028:
  abstract: We present an online interactive tool that generates titles of blog titles
    and thus take the first step toward automating science journalism. Science journalism
    aims to transform jargon-laden scientific articles into a form that the common
    reader can comprehend while ensuring that the underlying meaning of the article
    is retained. In this work, we present a tool, which, given the title and abstract
    of a research paper will generate a blog title by mimicking a human science journalist.
    The tool makes use of a model trained on a corpus of 87,328 pairs of research
    papers and their corresponding blogs, built from two science news aggregators.
    The architecture of the model is a two-stage mechanism which generates blog titles.
    Evaluation using standard metrics indicate the viability of the proposed system.
  address: Brussels, Belgium
  author:
  - first: Raghuram
    full: Raghuram Vadapalli
    id: raghuram-vadapalli
    last: Vadapalli
  - first: Bakhtiyar
    full: Bakhtiyar Syed
    id: bakhtiyar-syed
    last: Syed
  - first: Nishant
    full: Nishant Prabhu
    id: nishant-prabhu
    last: Prabhu
  - first: Balaji Vasan
    full: Balaji Vasan Srinivasan
    id: balaji-vasan-srinivasan
    last: Srinivasan
  - first: Vasudeva
    full: Vasudeva Varma
    id: vasudeva-varma
    last: Varma
  author_string: Raghuram Vadapalli, Bakhtiyar Syed, Nishant Prabhu, Balaji Vasan
    Srinivasan, Vasudeva Varma
  bibkey: vadapalli-etal-2018-science
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2028
  month: November
  page_first: '163'
  page_last: '168'
  pages: "163\u2013168"
  paper_id: '28'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2028.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2028.jpg
  title: 'When science journalism meets artificial intelligence : An interactive demonstration'
  title_html: 'When science journalism meets artificial intelligence : An interactive
    demonstration'
  url: https://www.aclweb.org/anthology/D18-2028
  year: '2018'
D18-2029:
  abstract: We present easy-to-use TensorFlow Hub sentence embedding models having
    good task transfer performance. Model variants allow for trade-offs between accuracy
    and compute resources. We report the relationship between model complexity, resources,
    and transfer performance. Comparisons are made with baselines without transfer
    learning and to baselines that incorporate word-level transfer. Transfer learning
    using sentence-level embeddings is shown to outperform models without transfer
    learning and often those that use only word-level transfer. We show good transfer
    task performance with minimal training data and obtain encouraging results on
    word embedding association tests (WEAT) of model bias.
  address: Brussels, Belgium
  author:
  - first: Daniel
    full: Daniel Cer
    id: daniel-cer
    last: Cer
  - first: Yinfei
    full: Yinfei Yang
    id: yinfei-yang
    last: Yang
  - first: Sheng-yi
    full: Sheng-yi Kong
    id: sheng-yi-kong
    last: Kong
  - first: Nan
    full: Nan Hua
    id: nan-hua
    last: Hua
  - first: Nicole
    full: Nicole Limtiaco
    id: nicole-limtiaco
    last: Limtiaco
  - first: Rhomni
    full: Rhomni St. John
    id: rhomni-st-john
    last: St. John
  - first: Noah
    full: Noah Constant
    id: noah-constant
    last: Constant
  - first: Mario
    full: Mario Guajardo-Cespedes
    id: mario-guajardo-cespedes
    last: Guajardo-Cespedes
  - first: Steve
    full: Steve Yuan
    id: steve-yuan
    last: Yuan
  - first: Chris
    full: Chris Tar
    id: chris-tar
    last: Tar
  - first: Brian
    full: Brian Strope
    id: brian-strope
    last: Strope
  - first: Ray
    full: Ray Kurzweil
    id: ray-kurzweil
    last: Kurzweil
  author_string: Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco,
    Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,
    Brian Strope, Ray Kurzweil
  bibkey: cer-etal-2018-universal
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations'
  doi: 10.18653/v1/D18-2029
  month: November
  page_first: '169'
  page_last: '174'
  pages: "169\u2013174"
  paper_id: '29'
  parent_volume_id: D18-2
  pdf: https://www.aclweb.org/anthology/D18-2029.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-2029.jpg
  title: Universal Sentence Encoder for English
  title_html: Universal Sentence Encoder for <span class="acl-fixed-case">E</span>nglish
  url: https://www.aclweb.org/anthology/D18-2029
  year: '2018'
D18-3001:
  abstract: Joint models have received much research attention in NLP, allowing relevant
    tasks to share common information while avoiding error propagation in multi-stage
    pepelines. Several main approaches have been taken by statistical joint modeling,
    while neural models allow parameter sharing and adversarial training. This tutorial
    reviews main approaches to joint modeling for both statistical and neural methods.
  address: Melbourne, Australia
  author:
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  author_string: Yue Zhang
  bibkey: zhang-2018-joint
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: Tutorial Abstracts'
  month: October-November
  paper_id: '1'
  parent_volume_id: D18-3
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-3001.jpg
  title: Joint models for NLP
  title_html: Joint models for <span class="acl-fixed-case">NLP</span>
  year: '2018'
D18-3002:
  abstract: In this tutorial we will focus on Hyperedge Replacement Languages (HRL;
    Drewes et al. 1997), a context-free graph rewriting system. HRL are one of the
    most popular graph formalisms to be studied in NLP (Chiang et al., 2013; Peng
    et al., 2015; Bauer and Rambow, 2016). We will discuss HRL by formally defining
    them, studying several examples, discussing their properties, and providing exercises
    for the tutorial. While HRL have been used in NLP in the past, there is some speculation
    that they are more expressive than is necessary for graphs representing natural
    language (Drewes, 2017). Part of our own research has been exploring what restrictions
    of HRL could yield languages that are more useful for NLP and also those that
    have desirable properties for NLP models, such as being closed under intersection.
    With that in mind, we also plan to discuss Regular Graph Languages (RGL; Courcelle
    1991), a subfamily of HRL which are closed under intersection. The definition
    of RGL is relatively simple after being introduced to HRL. We do not plan on discussing
    any proofs of why RGL are also a subfamily of MSOL, as described in Gilroy et
    al. (2017b). We will briefly mention the other formalisms shown in Figure 1 such
    as MSOL and DAGAL but this will focus on their properties rather than any formal
    definitions.
  address: Melbourne, Australia
  author:
  - first: Adam
    full: Adam Lopez
    id: adam-lopez
    last: Lopez
  - first: Sorcha
    full: Sorcha Gilroy
    id: sorcha-gilroy
    last: Gilroy
  author_string: Adam Lopez, Sorcha Gilroy
  bibkey: lopez-gilroy-2018-graph
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: Tutorial Abstracts'
  month: October-November
  paper_id: '2'
  parent_volume_id: D18-3
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-3002.jpg
  title: Graph Formalisms for Meaning Representations
  title_html: Graph Formalisms for Meaning Representations
  year: '2018'
D18-3003:
  abstract: Doing modern NLP research requires writing code. Good code enables fast
    prototyping, easy debugging, controlled experiments, and accessible visualizations
    that help researchers understand what a model is doing. Bad code leads to research
    that is at best hard to reproduce and extend, and at worst simply incorrect. Indeed,
    there is a growing recognition of the importance of having good tools to assist
    good research in our field, as the upcoming workshop on open source software for
    NLP demonstrates. This tutorial aims to share best practices for writing code
    for NLP research, drawing on the instructors' experience designing the recently-released
    AllenNLP toolkit, a PyTorch-based library for deep learning NLP research. We will
    explain how a library with the right abstractions and components enables better
    code and better science, using models implemented in AllenNLP as examples. Participants
    will learn how to write research code in a way that facilitates good science and
    easy experimentation, regardless of what framework they use.
  address: Melbourne, Australia
  author:
  - first: Matt
    full: Matt Gardner
    id: matt-gardner
    last: Gardner
  - first: Mark
    full: Mark Neumann
    id: mark-neumann
    last: Neumann
  - first: Joel
    full: Joel Grus
    id: joel-grus
    last: Grus
  - first: Nicholas
    full: Nicholas Lourie
    id: nicholas-lourie
    last: Lourie
  author_string: Matt Gardner, Mark Neumann, Joel Grus, Nicholas Lourie
  bibkey: gardner-etal-2018-writing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: Tutorial Abstracts'
  month: October-November
  paper_id: '3'
  parent_volume_id: D18-3
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-3003.jpg
  title: Writing Code for NLP Research
  title_html: Writing Code for <span class="acl-fixed-case">NLP</span> Research
  year: '2018'
D18-3004:
  abstract: The proposed tutorial will cover deep latent variable models both in the
    case where exact inference over the latent variables is tractable and when it
    is not. The former case includes neural extensions of unsupervised tagging and
    parsing models. Our discussion of the latter case, where inference cannot be performed
    tractably, will restrict itself to continuous latent variables. In particular,
    we will discuss recent developments both in neural variational inference (e.g.,
    relating to Variational Auto-encoders) and in implicit density modeling (e.g.,
    relating to Generative Adversarial Networks). We will highlight the challenges
    of applying these families of methods to NLP problems, and discuss recent successes
    and best practices.
  address: Melbourne, Australia
  author:
  - first: Alexander
    full: Alexander Rush
    id: alexander-m-rush
    last: Rush
  - first: Yoon
    full: Yoon Kim
    id: yoon-kim
    last: Kim
  - first: Sam
    full: Sam Wiseman
    id: sam-wiseman
    last: Wiseman
  author_string: Alexander Rush, Yoon Kim, Sam Wiseman
  bibkey: rush-etal-2018-deep
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: Tutorial Abstracts'
  month: October-November
  paper_id: '4'
  parent_volume_id: D18-3
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-3004.jpg
  title: Deep Latent Variable Models of Natural Language
  title_html: Deep Latent Variable Models of Natural Language
  year: '2018'
D18-3005:
  abstract: 'Standardized tests have recently been proposed as replacements to the
    Turing test as a driver for progress in AI (Clark, 2015). These include tests
    on understanding passages and stories and answering questions about them (Richardson
    et al., 2013; Rajpurkar et al., 2016a, inter alia), science question answering
    (Schoenick et al., 2016, inter alia), algebra word problems (Kushman et al., 2014,
    inter alia), geometry problems (Seo et al., 2015; Sachan et al., 2016), visual
    question answering (Antol et al., 2015), etc. Many of these tests require sophisticated
    understanding of the world, aiming to push the boundaries of AI. For this tutorial,
    we broadly categorize these tests into two categories: open domain tests such
    as reading comprehensions and elementary school tests where the goal is to find
    the support for an answer from the student curriculum, and closed domain tests
    such as intermediate level math and science tests (algebra, geometry, Newtonian
    physics problems, etc.). Unlike open domain tests, closed domain tests require
    the system to have significant domain knowledge and reasoning capabilities. For
    example, geometry questions typically involve a number of geometry primitives
    (lines, quadrilaterals, circles, etc) and require students to use axioms and theorems
    of geometry (Pythagoras theorem, alternating angles, etc) to solve them. These
    closed domains often have a formal logical basis and the question can be mapped
    to a formal language by semantic parsing. The formal question representation can
    then provided as an input to an expert system to solve the question.'
  address: Melbourne, Australia
  author:
  - first: Mrinmaya
    full: Mrinmaya Sachan
    id: mrinmaya-sachan
    last: Sachan
  - first: Minjoon
    full: Minjoon Seo
    id: minjoon-seo
    last: Seo
  - first: Hannaneh
    full: Hannaneh Hajishirzi
    id: hannaneh-hajishirzi
    last: Hajishirzi
  - first: Eric
    full: Eric Xing
    id: eric-xing
    last: Xing
  author_string: Mrinmaya Sachan, Minjoon Seo, Hannaneh Hajishirzi, Eric Xing
  bibkey: sachan-etal-2018-standardized
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: Tutorial Abstracts'
  month: October-November
  paper_id: '5'
  parent_volume_id: D18-3
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-3005.jpg
  title: Standardized Tests as benchmarks for Artificial Intelligence
  title_html: Standardized Tests as benchmarks for Artificial Intelligence
  year: '2018'
D18-3006:
  abstract: The tutorial is based on the long-term efforts on building conversational
    models with deep learning approaches for chatbots. We will summarize the fundamental
    challenges in modeling open domain dialogues, clarify the difference from modeling
    goal-oriented dialogues, and give an overview of state-of-the-art methods for
    open domain conversation including both retrieval-based methods and generation-based
    methods. In addition to these, our tutorial will also cover some new trends of
    research of chatbots, such as how to design a reasonable evaluation system and
    how to "control" conversations from a chatbot with some specific information such
    as personas, styles, and emotions, etc.
  address: Melbourne, Australia
  author:
  - first: Wei
    full: Wei Wu
    id: wei-wu
    last: Wu
  - first: Rui
    full: Rui Yan
    id: rui-yan
    last: Yan
  author_string: Wei Wu, Rui Yan
  bibkey: wu-yan-2018-deep
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing: Tutorial Abstracts'
  month: October-November
  paper_id: '6'
  parent_volume_id: D18-3
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/D18-3006.jpg
  title: 'Deep Chit-Chat: Deep Learning for ChatBots'
  title_html: 'Deep Chit-Chat: Deep Learning for ChatBots'
  year: '2018'
