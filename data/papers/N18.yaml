N18-1000:
  address: New Orleans, Louisiana
  author:
  - first: Marilyn
    full: Marilyn Walker
    id: marilyn-walker
    last: Walker
  - first: Heng
    full: Heng Ji
    id: heng-ji
    last: Ji
  - first: Amanda
    full: Amanda Stent
    id: amanda-stent
    last: Stent
  author_string: Marilyn Walker, Heng Ji, Amanda Stent
  bibkey: naacl-2018-2018
  bibtype: proceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  doi: 10.18653/v1/N18-1
  month: June
  paper_id: '0'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1000.jpg
  title: 'Proceedings of the 2018 Conference of the North American Chapter of the
    Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  title_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  url: https://www.aclweb.org/anthology/N18-1000
  year: '2018'
N18-1001:
  abstract: 'We study the problem of named entity recognition (NER) from electronic
    medical records, which is one of the most fundamental and critical problems for
    medical text mining. Medical records which are written by clinicians from different
    specialties usually contain quite different terminologies and writing styles.
    The difference of specialties and the cost of human annotation makes it particularly
    difficult to train a universal medical NER system. In this paper, we propose a
    label-aware double transfer learning framework (La-DTL) for cross-specialty NER,
    so that a medical NER system designed for one specialty could be conveniently
    applied to another one with minimal annotation efforts. The transferability is
    guaranteed by two components: (i) we propose label-aware MMD for feature representation
    transfer, and (ii) we perform parameter transfer with a theoretical upper bound
    which is also label aware. We conduct extensive experiments on 12 cross-specialty
    NER tasks. The experimental results demonstrate that La-DTL provides consistent
    accuracy improvement over strong baselines. Besides, the promising experimental
    results on non-medical NER scenarios indicate that La-DTL is potential to be seamlessly
    adapted to a wide range of NER tasks.'
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276388781
    type: video
    url: http://vimeo.com/276388781
  author:
  - first: Zhenghui
    full: Zhenghui Wang
    id: zhenghui-wang
    last: Wang
  - first: Yanru
    full: Yanru Qu
    id: yanru-qu
    last: Qu
  - first: Liheng
    full: Liheng Chen
    id: liheng-chen
    last: Chen
  - first: Jian
    full: Jian Shen
    id: jian-shen
    last: Shen
  - first: Weinan
    full: Weinan Zhang
    id: weinan-zhang
    last: Zhang
  - first: Shaodian
    full: Shaodian Zhang
    id: shaodian-zhang
    last: Zhang
  - first: Yimei
    full: Yimei Gao
    id: yimei-gao
    last: Gao
  - first: Gen
    full: Gen Gu
    id: gen-gu
    last: Gu
  - first: Ken
    full: Ken Chen
    id: ken-chen
    last: Chen
  - first: Yong
    full: Yong Yu
    id: yong-yu
    last: Yu
  author_string: Zhenghui Wang, Yanru Qu, Liheng Chen, Jian Shen, Weinan Zhang, Shaodian
    Zhang, Yimei Gao, Gen Gu, Ken Chen, Yong Yu
  bibkey: wang-etal-2018-label
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1001
  month: June
  page_first: '1'
  page_last: '15'
  pages: "1\u201315"
  paper_id: '1'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1001.jpg
  title: Label-Aware Double Transfer Learning for Cross-Specialty Medical Named Entity
    Recognition
  title_html: Label-Aware Double Transfer Learning for Cross-Specialty Medical Named
    Entity Recognition
  url: https://www.aclweb.org/anthology/N18-1001
  year: '2018'
N18-1002:
  abstract: 'The task of Fine-grained Entity Type Classification (FETC) consists of
    assigning types from a hierarchy to entity mentions in text. Existing methods
    rely on distant supervision and are thus susceptible to noisy labels that can
    be out-of-context or overly-specific for the training sentence. Previous methods
    that attempt to address these issues do so with heuristics or with the help of
    hand-crafted features. Instead, we propose an end-to-end solution with a neural
    network model that uses a variant of cross-entropy loss function to handle out-of-context
    labels, and hierarchical loss normalization to cope with overly-specific ones.
    Also, previous work solve FETC a multi-label classification followed by ad-hoc
    post-processing. In contrast, our solution is more elegant: we use public word
    embeddings to train a single-label that jointly learns representations for entity
    mentions and their context. We show experimentally that our approach is robust
    against noise and consistently outperforms the state-of-the-art on established
    benchmarks for the task.'
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276389935
    type: video
    url: http://vimeo.com/276389935
  author:
  - first: Peng
    full: Peng Xu
    id: peng-xu
    last: Xu
  - first: Denilson
    full: Denilson Barbosa
    id: denilson-barbosa
    last: Barbosa
  author_string: Peng Xu, Denilson Barbosa
  bibkey: xu-barbosa-2018-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1002
  month: June
  page_first: '16'
  page_last: '25'
  pages: "16\u201325"
  paper_id: '2'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1002.jpg
  title: Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss
  title_html: Neural Fine-Grained Entity Type Classification with Hierarchy-Aware
    Loss
  url: https://www.aclweb.org/anthology/N18-1002
  year: '2018'
N18-1003:
  abstract: 'Semi-supervised bootstrapping techniques for relationship extraction
    from text iteratively expand a set of initial seed instances. Due to the lack
    of labeled data, a key challenge in bootstrapping is semantic drift: if a false
    positive instance is added during an iteration, then all following iterations
    are contaminated. We introduce BREX, a new bootstrapping method that protects
    against such contamination by highly effective confidence assessment. This is
    achieved by using entity and template seeds jointly (as opposed to just one as
    in previous work), by expanding entities and templates in parallel and in a mutually
    constraining fashion in each iteration and by introducing higherquality similarity
    measures for templates. Experimental results show that BREX achieves an F1 that
    is 0.13 (0.87 vs. 0.74) better than the state of the art for four relationships.'
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276391396
    type: video
    url: http://vimeo.com/276391396
  author:
  - first: Pankaj
    full: Pankaj Gupta
    id: pankaj-gupta
    last: Gupta
  - first: Benjamin
    full: Benjamin Roth
    id: benjamin-roth
    last: Roth
  - first: Hinrich
    full: "Hinrich Sch\xFCtze"
    id: hinrich-schutze
    last: "Sch\xFCtze"
  author_string: "Pankaj Gupta, Benjamin Roth, Hinrich Sch\xFCtze"
  bibkey: gupta-etal-2018-joint
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1003
  month: June
  page_first: '26'
  page_last: '36'
  pages: "26\u201336"
  paper_id: '3'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1003.jpg
  title: Joint Bootstrapping Machines for High Confidence Relation Extraction
  title_html: Joint Bootstrapping Machines for High Confidence Relation Extraction
  url: https://www.aclweb.org/anthology/N18-1003
  year: '2018'
N18-1004:
  abstract: "What makes some types of languages more probable than others? For instance,\
    \ we know that almost all spoken languages contain the vowel phoneme /i/; why\
    \ should that be? The field of linguistic typology seeks to answer these questions\
    \ and, thereby, divine the mechanisms that underlie human language. In our work,\
    \ we tackle the problem of vowel system typology, i.e., we propose a generative\
    \ probability model of which vowels a language contains. In contrast to previous\
    \ work, we work directly with the acoustic information\u2014the first two formant\
    \ values\u2014rather than modeling discrete sets of symbols from the international\
    \ phonetic alphabet. We develop a novel generative probability model and report\
    \ results on over 200 languages."
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276386708
    type: video
    url: http://vimeo.com/276386708
  author:
  - first: Ryan
    full: Ryan Cotterell
    id: ryan-cotterell
    last: Cotterell
  - first: Jason
    full: Jason Eisner
    id: jason-eisner
    last: Eisner
  author_string: Ryan Cotterell, Jason Eisner
  bibkey: cotterell-eisner-2018-deep
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1004
  month: June
  page_first: '37'
  page_last: '46'
  pages: "37\u201346"
  paper_id: '4'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1004.jpg
  title: A Deep Generative Model of Vowel Formant Typology
  title_html: A Deep Generative Model of Vowel Formant Typology
  url: https://www.aclweb.org/anthology/N18-1004
  year: '2018'
N18-1005:
  abstract: "Morphological segmentation for polysynthetic languages is challenging,\
    \ because a word may consist of many individual morphemes and training data can\
    \ be extremely scarce. Since neural sequence-to-sequence (seq2seq) models define\
    \ the state of the art for morphological segmentation in high-resource settings\
    \ and for (mostly) European languages, we first show that they also obtain competitive\
    \ performance for Mexican polysynthetic languages in minimal-resource settings.\
    \ We then propose two novel multi-task training approaches\u2014one with, one\
    \ without need for external unlabeled resources\u2014, and two corresponding data\
    \ augmentation methods, improving over the neural baseline for all languages.\
    \ Finally, we explore cross-lingual transfer as a third way to fortify our neural\
    \ model and show that we can train one single multi-lingual model for related\
    \ languages while maintaining comparable or even improved performance, thus reducing\
    \ the amount of parameters by close to 75%. We provide our morphological segmentation\
    \ datasets for Mexicanero, Nahuatl, Wixarika and Yorem Nokki for future research."
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276387788
    type: video
    url: http://vimeo.com/276387788
  author:
  - first: Katharina
    full: Katharina Kann
    id: katharina-kann
    last: Kann
  - first: Jesus Manuel
    full: Jesus Manuel Mager Hois
    id: jesus-manuel-mager-hois
    last: Mager Hois
  - first: Ivan Vladimir
    full: Ivan Vladimir Meza-Ruiz
    id: ivan-meza-ruiz
    last: Meza-Ruiz
  - first: Hinrich
    full: "Hinrich Sch\xFCtze"
    id: hinrich-schutze
    last: "Sch\xFCtze"
  author_string: "Katharina Kann, Jesus Manuel Mager Hois, Ivan Vladimir Meza-Ruiz,\
    \ Hinrich Sch\xFCtze"
  bibkey: kann-etal-2018-fortification
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1005
  month: June
  page_first: '47'
  page_last: '57'
  pages: "47\u201357"
  paper_id: '5'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1005.jpg
  title: Fortification of Neural Morphological Segmentation Models for Polysynthetic
    Minimal-Resource Languages
  title_html: Fortification of Neural Morphological Segmentation Models for Polysynthetic
    Minimal-Resource Languages
  url: https://www.aclweb.org/anthology/N18-1005
  year: '2018'
N18-1006:
  abstract: "Recently, neural machine translation (NMT) has emerged as a powerful\
    \ alternative to conventional statistical approaches. However, its performance\
    \ drops considerably in the presence of morphologically rich languages (MRLs).\
    \ Neural engines usually fail to tackle the large vocabulary and high out-of-vocabulary\
    \ (OOV) word rate of MRLs. Therefore, it is not suitable to exploit existing word-based\
    \ models to translate this set of languages. In this paper, we propose an extension\
    \ to the state-of-the-art model of Chung et al. (2016), which works at the character\
    \ level and boosts the decoder with target-side morphological information. In\
    \ our architecture, an additional morphology table is plugged into the model.\
    \ Each time the decoder samples from a target vocabulary, the table sends auxiliary\
    \ signals from the most relevant affixes in order to enrich the decoder\u2019\
    s current state and constrain it to provide better predictions. We evaluated our\
    \ model to translate English into German, Russian, and Turkish as three MRLs and\
    \ observed significant improvements."
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276415442
    type: video
    url: http://vimeo.com/276415442
  author:
  - first: Peyman
    full: Peyman Passban
    id: peyman-passban
    last: Passban
  - first: Qun
    full: Qun Liu
    id: qun-liu
    last: Liu
  - first: Andy
    full: Andy Way
    id: andy-way
    last: Way
  author_string: Peyman Passban, Qun Liu, Andy Way
  bibkey: passban-etal-2018-improving
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1006
  month: June
  page_first: '58'
  page_last: '68'
  pages: "58\u201368"
  paper_id: '6'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1006.jpg
  title: Improving Character-Based Decoding Using Target-Side Morphological Information
    for Neural Machine Translation
  title_html: Improving Character-Based Decoding Using Target-Side Morphological Information
    for Neural Machine Translation
  url: https://www.aclweb.org/anthology/N18-1006
  year: '2018'
N18-1007:
  abstract: In conversational speech, the acoustic signal provides cues that help
    listeners disambiguate difficult parses. For automatically parsing spoken utterances,
    we introduce a model that integrates transcribed text and acoustic-prosodic features
    using a convolutional neural network over energy and pitch trajectories coupled
    with an attention-based recurrent neural network that accepts text and prosodic
    features. We find that different types of acoustic-prosodic features are individually
    helpful, and together give statistically significant improvements in parse and
    disfluency detection F1 scores over a strong text-only baseline. For this study
    with known sentence boundaries, error analyses show that the main benefit of acoustic-prosodic
    features is in sentences with disfluencies, attachment decisions are most improved,
    and transcription errors obscure gains from prosody.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276439724
    type: video
    url: http://vimeo.com/276439724
  author:
  - first: Trang
    full: Trang Tran
    id: trang-tran
    last: Tran
  - first: Shubham
    full: Shubham Toshniwal
    id: shubham-toshniwal
    last: Toshniwal
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  - first: Kevin
    full: Kevin Gimpel
    id: kevin-gimpel
    last: Gimpel
  - first: Karen
    full: Karen Livescu
    id: karen-livescu
    last: Livescu
  - first: Mari
    full: Mari Ostendorf
    id: mari-ostendorf
    last: Ostendorf
  author_string: Trang Tran, Shubham Toshniwal, Mohit Bansal, Kevin Gimpel, Karen
    Livescu, Mari Ostendorf
  bibkey: tran-etal-2018-parsing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1007
  month: June
  page_first: '69'
  page_last: '81'
  pages: "69\u201381"
  paper_id: '7'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1007.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1007.jpg
  title: 'Parsing Speech: a Neural Approach to Integrating Lexical and Acoustic-Prosodic
    Information'
  title_html: 'Parsing Speech: a Neural Approach to Integrating Lexical and Acoustic-Prosodic
    Information'
  url: https://www.aclweb.org/anthology/N18-1007
  year: '2018'
N18-1008:
  abstract: We explore multitask models for neural translation of speech, augmenting
    them in order to reflect two intuitive notions. First, we introduce a model where
    the second task decoder receives information from the decoder of the first task,
    since higher-level intermediate representations should provide useful information.
    Second, we apply regularization that encourages transitivity and invertibility.
    We show that the application of these notions on jointly trained models improves
    performance on the tasks of low-resource speech transcription and translation.
    It also leads to better performance when using attention information for word
    discovery over unsegmented input.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276441141
    type: video
    url: http://vimeo.com/276441141
  author:
  - first: Antonios
    full: Antonios Anastasopoulos
    id: antonios-anastasopoulos
    last: Anastasopoulos
  - first: David
    full: David Chiang
    id: david-chiang
    last: Chiang
  author_string: Antonios Anastasopoulos, David Chiang
  bibkey: anastasopoulos-chiang-2018-tied
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1008
  month: June
  page_first: '82'
  page_last: '91'
  pages: "82\u201391"
  paper_id: '8'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1008.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1008.jpg
  title: Tied Multitask Learning for Neural Speech Translation
  title_html: Tied Multitask Learning for Neural Speech Translation
  url: https://www.aclweb.org/anthology/N18-1008
  year: '2018'
N18-1009:
  abstract: This work examines the rhetorical techniques that speakers employ during
    political campaigns. We introduce a new corpus of speeches from campaign events
    in the months leading up to the 2016 U.S. presidential election and develop new
    models for predicting moments of audience applause. In contrast to existing datasets,
    we tackle the challenge of working with transcripts that derive from uncorrected
    closed captioning, using associated audio recordings to automatically extract
    and align labels for instances of audience applause. In prediction experiments,
    we find that lexical features carry the most information, but that a variety of
    features are predictive, including prosody, long-term contextual dependencies,
    and theoretically motivated features designed to capture rhetorical techniques.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276443580
    type: video
    url: http://vimeo.com/276443580
  author:
  - first: Jon
    full: Jon Gillick
    id: jon-gillick
    last: Gillick
  - first: David
    full: David Bamman
    id: david-bamman
    last: Bamman
  author_string: Jon Gillick, David Bamman
  bibkey: gillick-bamman-2018-please
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1009
  month: June
  page_first: '92'
  page_last: '102'
  pages: "92\u2013102"
  paper_id: '9'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1009.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1009.jpg
  title: 'Please Clap: Modeling Applause in Campaign Speeches'
  title_html: 'Please Clap: Modeling Applause in Campaign Speeches'
  url: https://www.aclweb.org/anthology/N18-1009
  year: '2018'
N18-1010:
  abstract: "We present a neural architecture for modeling argumentative dialogue\
    \ that explicitly models the interplay between an Opinion Holder\u2019s (OH\u2019\
    s) reasoning and a challenger\u2019s argument, with the goal of predicting if\
    \ the argument successfully changes the OH\u2019s view. The model has two components:\
    \ (1) vulnerable region detection, an attention model that identifies parts of\
    \ the OH\u2019s reasoning that are amenable to change, and (2) interaction encoding,\
    \ which identifies the relationship between the content of the OH\u2019s reasoning\
    \ and that of the challenger\u2019s argument. Based on evaluation on discussions\
    \ from the Change My View forum on Reddit, the two components work together to\
    \ predict an OH\u2019s change in view, outperforming several baselines. A posthoc\
    \ analysis suggests that sentences picked out by the attention model are addressed\
    \ more frequently by successful arguments than by unsuccessful ones."
  address: New Orleans, Louisiana
  author:
  - first: Yohan
    full: Yohan Jo
    id: yohan-jo
    last: Jo
  - first: Shivani
    full: Shivani Poddar
    id: shivani-poddar
    last: Poddar
  - first: Byungsoo
    full: Byungsoo Jeon
    id: byungsoo-jeon
    last: Jeon
  - first: Qinlan
    full: Qinlan Shen
    id: qinlan-shen
    last: Shen
  - first: Carolyn
    full: "Carolyn Ros\xE9"
    id: carolyn-rose
    last: "Ros\xE9"
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  author_string: "Yohan Jo, Shivani Poddar, Byungsoo Jeon, Qinlan Shen, Carolyn Ros\xE9\
    , Graham Neubig"
  bibkey: jo-etal-2018-attentive
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1010
  month: June
  page_first: '103'
  page_last: '116'
  pages: "103\u2013116"
  paper_id: '10'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1010.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1010.jpg
  title: 'Attentive Interaction Model: Modeling Changes in View in Argumentation'
  title_html: 'Attentive Interaction Model: Modeling Changes in View in Argumentation'
  url: https://www.aclweb.org/anthology/N18-1010
  year: '2018'
N18-1011:
  abstract: Analyzing language in context, both from a theoretical and from a computational
    perspective, is receiving increased interest. Complementing the research in linguistics
    on discourse and information structure, in computational linguistics identifying
    discourse concepts was also shown to improve the performance of certain applications,
    for example, Short Answer Assessment systems (Ziai and Meurers, 2014). Building
    on the research that established detailed annotation guidelines for manual annotation
    of information structural concepts for written (Dipper et al., 2007; Ziai and
    Meurers, 2014) and spoken language data (Calhoun et al., 2010), this paper presents
    the first approach automating the analysis of focus in authentic written data.
    Our classification approach combines a range of lexical, syntactic, and semantic
    features to achieve an accuracy of 78.1% for identifying focus.
  address: New Orleans, Louisiana
  author:
  - first: Ramon
    full: Ramon Ziai
    id: ramon-ziai
    last: Ziai
  - first: Detmar
    full: Detmar Meurers
    id: detmar-meurers
    last: Meurers
  author_string: Ramon Ziai, Detmar Meurers
  bibkey: ziai-meurers-2018-automatic
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1011
  month: June
  page_first: '117'
  page_last: '128'
  pages: "117\u2013128"
  paper_id: '11'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1011.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1011.jpg
  title: 'Automatic Focus Annotation: Bringing Formal Pragmatics Alive in Analyzing
    the Information Structure of Authentic Data'
  title_html: 'Automatic Focus Annotation: Bringing Formal Pragmatics Alive in Analyzing
    the Information Structure of Authentic Data'
  url: https://www.aclweb.org/anthology/N18-1011
  year: '2018'
N18-1012:
  abstract: Style transfer is the task of automatically transforming a piece of text
    in one particular style into another. A major barrier to progress in this field
    has been a lack of training and evaluation datasets, as well as benchmarks and
    automatic metrics. In this work, we create the largest corpus for a particular
    stylistic transfer (formality) and show that techniques from the machine translation
    community can serve as strong baselines for future work. We also discuss challenges
    of using automatic metrics.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1012.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1012.Notes.pdf
  author:
  - first: Sudha
    full: Sudha Rao
    id: sudha-rao
    last: Rao
  - first: Joel
    full: Joel Tetreault
    id: joel-tetreault
    last: Tetreault
  author_string: Sudha Rao, Joel Tetreault
  bibkey: rao-tetreault-2018-dear
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1012
  month: June
  page_first: '129'
  page_last: '140'
  pages: "129\u2013140"
  paper_id: '12'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1012.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1012.jpg
  title: 'Dear Sir or Madam, May I Introduce the GYAFC Dataset: Corpus, Benchmarks
    and Metrics for Formality Style Transfer'
  title_html: 'Dear Sir or Madam, May <span class="acl-fixed-case">I</span> Introduce
    the <span class="acl-fixed-case">GYAFC</span> Dataset: Corpus, Benchmarks and
    Metrics for Formality Style Transfer'
  url: https://www.aclweb.org/anthology/N18-1012
  year: '2018'
N18-1013:
  abstract: We argue that semantic meanings of a sentence or clause can not be interpreted
    independently from the rest of a paragraph, or independently from all discourse
    relations and the overall paragraph-level discourse structure. With the goal of
    improving implicit discourse relation classification, we introduce a paragraph-level
    neural networks that model inter-dependencies between discourse units as well
    as discourse relation continuity and patterns, and predict a sequence of discourse
    relations in a paragraph. Experimental results show that our model outperforms
    the previous state-of-the-art systems on the benchmark corpus of PDTB.
  address: New Orleans, Louisiana
  author:
  - first: Zeyu
    full: Zeyu Dai
    id: zeyu-dai
    last: Dai
  - first: Ruihong
    full: Ruihong Huang
    id: ruihong-huang
    last: Huang
  author_string: Zeyu Dai, Ruihong Huang
  bibkey: dai-huang-2018-improving
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1013
  month: June
  page_first: '141'
  page_last: '151'
  pages: "141\u2013151"
  paper_id: '13'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1013.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1013.jpg
  title: Improving Implicit Discourse Relation Classification by Modeling Inter-dependencies
    of Discourse Units in a Paragraph
  title_html: Improving Implicit Discourse Relation Classification by Modeling Inter-dependencies
    of Discourse Units in a Paragraph
  url: https://www.aclweb.org/anthology/N18-1013
  year: '2018'
N18-1014:
  abstract: Natural language generation lies at the core of generative dialogue systems
    and conversational agents. We describe an ensemble neural language generator,
    and present several novel methods for data representation and augmentation that
    yield improved results in our model. We test the model on three datasets in the
    restaurant, TV and laptop domains, and report both objective and subjective evaluations
    of our best model. Using a range of automatic metrics, as well as human evaluators,
    we show that our approach achieves better results than state-of-the-art models
    on the same datasets.
  address: New Orleans, Louisiana
  author:
  - first: Juraj
    full: Juraj Juraska
    id: juraj-juraska
    last: Juraska
  - first: Panagiotis
    full: Panagiotis Karagiannis
    id: panagiotis-karagiannis
    last: Karagiannis
  - first: Kevin
    full: Kevin Bowden
    id: kevin-bowden
    last: Bowden
  - first: Marilyn
    full: Marilyn Walker
    id: marilyn-walker
    last: Walker
  author_string: Juraj Juraska, Panagiotis Karagiannis, Kevin Bowden, Marilyn Walker
  bibkey: juraska-etal-2018-deep
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1014
  month: June
  page_first: '152'
  page_last: '162'
  pages: "152\u2013162"
  paper_id: '14'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1014.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1014.jpg
  title: A Deep Ensemble Model with Slot Alignment for Sequence-to-Sequence Natural
    Language Generation
  title_html: A Deep Ensemble Model with Slot Alignment for Sequence-to-Sequence Natural
    Language Generation
  url: https://www.aclweb.org/anthology/N18-1014
  year: '2018'
N18-1015:
  abstract: This paper presents a novel, data-driven language model that produces
    entire lyrics for a given input melody. Previously proposed models for lyrics
    generation suffer from the inability of capturing the relationship between lyrics
    and melody partly due to the unavailability of lyrics-melody aligned data. In
    this study, we first propose a new practical method for creating a large collection
    of lyrics-melody aligned data and then create a collection of 1,000 lyrics-melody
    pairs augmented with precise syllable-note alignments and word/sentence/paragraph
    boundaries. We then provide a quantitative analysis of the correlation between
    word/sentence/paragraph boundaries in lyrics and melodies. We then propose an
    RNN-based lyrics language model conditioned on a featurized melody. Experimental
    results show that the proposed model generates fluent lyrics while maintaining
    the compatibility between boundaries of lyrics and melody structures.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1015.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1015.Notes.pdf
  author:
  - first: Kento
    full: Kento Watanabe
    id: kento-watanabe
    last: Watanabe
  - first: Yuichiroh
    full: Yuichiroh Matsubayashi
    id: yuichiroh-matsubayashi
    last: Matsubayashi
  - first: Satoru
    full: Satoru Fukayama
    id: satoru-fukayama
    last: Fukayama
  - first: Masataka
    full: Masataka Goto
    id: masataka-goto
    last: Goto
  - first: Kentaro
    full: Kentaro Inui
    id: kentaro-inui
    last: Inui
  - first: Tomoyasu
    full: Tomoyasu Nakano
    id: tomoyasu-nakano
    last: Nakano
  author_string: Kento Watanabe, Yuichiroh Matsubayashi, Satoru Fukayama, Masataka
    Goto, Kentaro Inui, Tomoyasu Nakano
  bibkey: watanabe-etal-2018-melody
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1015
  month: June
  page_first: '163'
  page_last: '172'
  pages: "163\u2013172"
  paper_id: '15'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1015.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1015.jpg
  title: A Melody-Conditioned Lyrics Language Model
  title_html: A Melody-Conditioned Lyrics Language Model
  url: https://www.aclweb.org/anthology/N18-1015
  year: '2018'
N18-1016:
  abstract: In this paper, we investigate the use of discourse-aware rewards with
    reinforcement learning to guide a model to generate long, coherent text. In particular,
    we propose to learn neural rewards to model cross-sentence ordering as a means
    to approximate desired discourse structure. Empirical results demonstrate that
    a generator trained with the learned reward produces more coherent and less repetitive
    text than models trained with cross-entropy or with reinforcement learning with
    commonly used scores as rewards.
  address: New Orleans, Louisiana
  author:
  - first: Antoine
    full: Antoine Bosselut
    id: antoine-bosselut
    last: Bosselut
  - first: Asli
    full: Asli Celikyilmaz
    id: asli-celikyilmaz
    last: Celikyilmaz
  - first: Xiaodong
    full: Xiaodong He
    id: xiaodong-he
    last: He
  - first: Jianfeng
    full: Jianfeng Gao
    id: jianfeng-gao
    last: Gao
  - first: Po-Sen
    full: Po-Sen Huang
    id: po-sen-huang
    last: Huang
  - first: Yejin
    full: Yejin Choi
    id: yejin-choi
    last: Choi
  author_string: Antoine Bosselut, Asli Celikyilmaz, Xiaodong He, Jianfeng Gao, Po-Sen
    Huang, Yejin Choi
  bibkey: bosselut-etal-2018-discourse
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1016
  month: June
  page_first: '173'
  page_last: '184'
  pages: "173\u2013184"
  paper_id: '16'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1016.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1016.jpg
  title: Discourse-Aware Neural Rewards for Coherent Text Generation
  title_html: Discourse-Aware Neural Rewards for Coherent Text Generation
  url: https://www.aclweb.org/anthology/N18-1016
  year: '2018'
N18-1017:
  abstract: Memory augmented encoder-decoder framework has achieved promising progress
    for natural language generation tasks. Such frameworks enable a decoder to retrieve
    from a memory during generation. However, less research has been done to take
    care of the memory contents from different sources, which are often of heterogeneous
    formats. In this work, we propose a novel attention mechanism to encourage the
    decoder to actively interact with the memory by taking its heterogeneity into
    account. Our solution attends across the generated history and memory to explicitly
    avoid repetition, and introduce related knowledge to enrich our generated sentences.
    Experiments on the answer sentence generation task show that our method can effectively
    explore heterogeneous memory to produce readable and meaningful answer sentences
    while maintaining high coverage for given answer information.
  address: New Orleans, Louisiana
  author:
  - first: Yao
    full: Yao Fu
    id: yao-fu
    last: Fu
  - first: Yansong
    full: Yansong Feng
    id: yansong-feng
    last: Feng
  author_string: Yao Fu, Yansong Feng
  bibkey: fu-feng-2018-natural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1017
  month: June
  page_first: '185'
  page_last: '195'
  pages: "185\u2013195"
  paper_id: '17'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1017.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1017.jpg
  title: Natural Answer Generation with Heterogeneous Memory
  title_html: Natural Answer Generation with Heterogeneous Memory
  url: https://www.aclweb.org/anthology/N18-1017
  year: '2018'
N18-1018:
  abstract: Most recent approaches use the sequence-to-sequence model for paraphrase
    generation. The existing sequence-to-sequence model tends to memorize the words
    and the patterns in the training dataset instead of learning the meaning of the
    words. Therefore, the generated sentences are often grammatically correct but
    semantically improper. In this work, we introduce a novel model based on the encoder-decoder
    framework, called Word Embedding Attention Network (WEAN). Our proposed model
    generates the words by querying distributed word representations (i.e. neural
    word embeddings), hoping to capturing the meaning of the according words. Following
    previous work, we evaluate our model on two paraphrase-oriented tasks, namely
    text simplification and short text abstractive summarization. Experimental results
    show that our model outperforms the sequence-to-sequence baseline by the BLEU
    score of 6.3 and 5.5 on two English text simplification datasets, and the ROUGE-2
    F1 score of 5.7 on a Chinese summarization dataset. Moreover, our model achieves
    state-of-the-art performances on these three benchmark datasets.
  address: New Orleans, Louisiana
  author:
  - first: Shuming
    full: Shuming Ma
    id: shuming-ma
    last: Ma
  - first: Xu
    full: Xu Sun
    id: xu-sun
    last: Sun
  - first: Wei
    full: Wei Li
    id: wei-li
    last: Li
  - first: Sujian
    full: Sujian Li
    id: sujian-li
    last: Li
  - first: Wenjie
    full: Wenjie Li
    id: wenjie-li
    last: Li
  - first: Xuancheng
    full: Xuancheng Ren
    id: xuancheng-ren
    last: Ren
  author_string: Shuming Ma, Xu Sun, Wei Li, Sujian Li, Wenjie Li, Xuancheng Ren
  bibkey: ma-etal-2018-query
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1018
  month: June
  page_first: '196'
  page_last: '206'
  pages: "196\u2013206"
  paper_id: '18'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1018.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1018.jpg
  title: 'Query and Output: Generating Words by Querying Distributed Word Representations
    for Paraphrase Generation'
  title_html: 'Query and Output: Generating Words by Querying Distributed Word Representations
    for Paraphrase Generation'
  url: https://www.aclweb.org/anthology/N18-1018
  year: '2018'
N18-1019:
  abstract: Lexical simplification involves identifying complex words or phrases that
    need to be simplified, and recommending simpler meaning-preserving substitutes
    that can be more easily understood. We propose a complex word identification (CWI)
    model that exploits both lexical and contextual features, and a simplification
    mechanism which relies on a word-embedding lexical substitution model to replace
    the detected complex words with simpler paraphrases. We compare our CWI and lexical
    simplification models to several baselines, and evaluate the performance of our
    simplification system against human judgments. The results show that our models
    are able to detect complex words with higher accuracy than other commonly used
    methods, and propose good simplification substitutes in context. They also highlight
    the limited contribution of context features for CWI, which nonetheless improve
    simplification compared to context-unaware models.
  address: New Orleans, Louisiana
  author:
  - first: Reno
    full: Reno Kriz
    id: reno-kriz
    last: Kriz
  - first: Eleni
    full: Eleni Miltsakaki
    id: eleni-miltsakaki
    last: Miltsakaki
  - first: Marianna
    full: Marianna Apidianaki
    id: marianna-apidianaki
    last: Apidianaki
  - first: Chris
    full: Chris Callison-Burch
    id: chris-callison-burch
    last: Callison-Burch
  author_string: Reno Kriz, Eleni Miltsakaki, Marianna Apidianaki, Chris Callison-Burch
  bibkey: kriz-etal-2018-simplification
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1019
  month: June
  page_first: '207'
  page_last: '217'
  pages: "207\u2013217"
  paper_id: '19'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1019.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1019.jpg
  title: Simplification Using Paraphrases and Context-Based Lexical Substitution
  title_html: Simplification Using Paraphrases and Context-Based Lexical Substitution
  url: https://www.aclweb.org/anthology/N18-1019
  year: '2018'
N18-1020:
  abstract: "We present a neural model for question generation from knowledge graphs\
    \ triples in a \u201CZero-shot\u201D setup, that is generating questions for predicate,\
    \ subject types or object types that were not seen at training time. Our model\
    \ leverages triples occurrences in the natural language corpus in a encoder-decoder\
    \ architecture, paired with an original part-of-speech copy action mechanism to\
    \ generate questions. Benchmark and human evaluation show that our model outperforms\
    \ state-of-the-art on this task."
  address: New Orleans, Louisiana
  author:
  - first: Hady
    full: Hady Elsahar
    id: hady-elsahar
    last: Elsahar
  - first: Christophe
    full: Christophe Gravier
    id: christophe-gravier
    last: Gravier
  - first: Frederique
    full: Frederique Laforest
    id: frederique-laforest
    last: Laforest
  author_string: Hady Elsahar, Christophe Gravier, Frederique Laforest
  bibkey: elsahar-etal-2018-zero
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1020
  month: June
  page_first: '218'
  page_last: '228'
  pages: "218\u2013228"
  paper_id: '20'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1020.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1020.jpg
  title: Zero-Shot Question Generation from Knowledge Graphs for Unseen Predicates
    and Entity Types
  title_html: Zero-Shot Question Generation from Knowledge Graphs for Unseen Predicates
    and Entity Types
  url: https://www.aclweb.org/anthology/N18-1020
  year: '2018'
N18-1021:
  abstract: Studies in Social Sciences have revealed that when people evaluate someone
    else, their evaluations often reflect their biases. As a result, rater bias may
    introduce highly subjective factors that make their evaluations inaccurate. This
    may affect automated essay scoring models in many ways, as these models are typically
    designed to model (potentially biased) essay raters. While there is sizeable literature
    on rater effects in general settings, it remains unknown how rater bias affects
    automated essay scoring. To this end, we present a new annotated corpus containing
    essays and their respective scores. Different from existing corpora, our corpus
    also contains comments provided by the raters in order to ground their scores.
    We present features to quantify rater bias based on their comments, and we found
    that rater bias plays an important role in automated essay scoring. We investigated
    the extent to which rater bias affects models based on hand-crafted features.
    Finally, we propose to rectify the training set by removing essays associated
    with potentially biased scores while learning the scoring model.
  address: New Orleans, Louisiana
  author:
  - first: Evelin
    full: Evelin Amorim
    id: evelin-amorim
    last: Amorim
  - first: Marcia
    full: "Marcia Can\xE7ado"
    id: marcia-cancado
    last: "Can\xE7ado"
  - first: Adriano
    full: Adriano Veloso
    id: adriano-veloso
    last: Veloso
  author_string: "Evelin Amorim, Marcia Can\xE7ado, Adriano Veloso"
  bibkey: amorim-etal-2018-automated
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1021
  month: June
  page_first: '229'
  page_last: '237'
  pages: "229\u2013237"
  paper_id: '21'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1021.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1021.jpg
  title: Automated Essay Scoring in the Presence of Biased Ratings
  title_html: Automated Essay Scoring in the Presence of Biased Ratings
  url: https://www.aclweb.org/anthology/N18-1021
  year: '2018'
N18-1022:
  abstract: 'We present a content-based method for recommending citations in an academic
    paper draft. We embed a given query document into a vector space, then use its
    nearest neighbors as candidates, and rerank the candidates using a discriminative
    model trained to distinguish between observed and unobserved citations. Unlike
    previous work, our method does not require metadata such as author names which
    can be missing, e.g., during the peer review process. Without using metadata,
    our method outperforms the best reported results on PubMed and DBLP datasets with
    relative improvements of over 18% in F1@20 and over 22% in MRR. We show empirically
    that, although adding metadata improves the performance on standard metrics, it
    favors self-citations which are less useful in a citation recommendation setup.
    We release an online portal for citation recommendation based on our method, (URL:
    http://bit.ly/citeDemo) and a new dataset OpenCorpus of 7 million research articles
    to facilitate future research on this task.'
  address: New Orleans, Louisiana
  author:
  - first: Chandra
    full: Chandra Bhagavatula
    id: chandra-bhagavatula
    last: Bhagavatula
  - first: Sergey
    full: Sergey Feldman
    id: sergey-feldman
    last: Feldman
  - first: Russell
    full: Russell Power
    id: russell-power
    last: Power
  - first: Waleed
    full: Waleed Ammar
    id: waleed-ammar
    last: Ammar
  author_string: Chandra Bhagavatula, Sergey Feldman, Russell Power, Waleed Ammar
  bibkey: bhagavatula-etal-2018-content
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1022
  month: June
  page_first: '238'
  page_last: '251'
  pages: "238\u2013251"
  paper_id: '22'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1022.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1022.jpg
  title: Content-Based Citation Recommendation
  title_html: Content-Based Citation Recommendation
  url: https://www.aclweb.org/anthology/N18-1022
  year: '2018'
N18-1023:
  abstract: We present a reading comprehension challenge in which questions can only
    be answered by taking into account information from multiple sentences. We solicit
    and verify questions and answers for this challenge through a 4-step crowdsourcing
    experiment. Our challenge dataset contains 6,500+ questions for 1000+ paragraphs
    across 7 different domains (elementary school science, news, travel guides, fiction
    stories, etc) bringing in linguistic diversity to the texts and to the questions
    wordings. On a subset of our dataset, we found human solvers to achieve an F1-score
    of 88.1%. We analyze a range of baselines, including a recent state-of-art reading
    comprehension system, and demonstrate the difficulty of this challenge, despite
    a high human performance. The dataset is the first to study multi-sentence inference
    at scale, with an open-ended set of question types that requires reasoning skills.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1023.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1023.Notes.pdf
  author:
  - first: Daniel
    full: Daniel Khashabi
    id: daniel-khashabi
    last: Khashabi
  - first: Snigdha
    full: Snigdha Chaturvedi
    id: snigdha-chaturvedi
    last: Chaturvedi
  - first: Michael
    full: Michael Roth
    id: michael-roth
    last: Roth
  - first: Shyam
    full: Shyam Upadhyay
    id: shyam-upadhyay
    last: Upadhyay
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay,
    Dan Roth
  bibkey: khashabi-etal-2018-looking
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1023
  month: June
  page_first: '252'
  page_last: '262'
  pages: "252\u2013262"
  paper_id: '23'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1023.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1023.jpg
  title: 'Looking Beyond the Surface: A Challenge Set for Reading Comprehension over
    Multiple Sentences'
  title_html: 'Looking Beyond the Surface: A Challenge Set for Reading Comprehension
    over Multiple Sentences'
  url: https://www.aclweb.org/anthology/N18-1023
  year: '2018'
N18-1024:
  abstract: We demonstrate that current state-of-the-art approaches to Automated Essay
    Scoring (AES) are not well-suited to capturing adversarially crafted input of
    grammatical but incoherent sequences of sentences. We develop a neural model of
    local coherence that can effectively learn connectedness features between sentences,
    and propose a framework for integrating and jointly training the local coherence
    model with a state-of-the-art AES model. We evaluate our approach against a number
    of baselines and experimentally demonstrate its effectiveness on both the AES
    task and the task of flagging adversarial input, further contributing to the development
    of an approach that strengthens the validity of neural essay scoring models.
  address: New Orleans, Louisiana
  author:
  - first: Youmna
    full: Youmna Farag
    id: youmna-farag
    last: Farag
  - first: Helen
    full: Helen Yannakoudakis
    id: helen-yannakoudakis
    last: Yannakoudakis
  - first: Ted
    full: Ted Briscoe
    id: ted-briscoe
    last: Briscoe
  author_string: Youmna Farag, Helen Yannakoudakis, Ted Briscoe
  bibkey: farag-etal-2018-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1024
  month: June
  page_first: '263'
  page_last: '271'
  pages: "263\u2013271"
  paper_id: '24'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1024.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1024.jpg
  title: Neural Automated Essay Scoring and Coherence Modeling for Adversarially Crafted
    Input
  title_html: Neural Automated Essay Scoring and Coherence Modeling for Adversarially
    Crafted Input
  url: https://www.aclweb.org/anthology/N18-1024
  year: '2018'
N18-1025:
  abstract: 'We propose a framework for computer-assisted text editing. It applies
    to translation post-editing and to paraphrasing. Our proposal relies on very simple
    interactions: a human editor modifies a sentence by marking tokens they would
    like the system to change. Our model then generates a new sentence which reformulates
    the initial sentence by avoiding marked words. The approach builds upon neural
    sequence-to-sequence modeling and introduces a neural network which takes as input
    a sentence along with change markers. Our model is trained on translation bitext
    by simulating post-edits. We demonstrate the advantage of our approach for translation
    post-editing through simulated post-edits. We also evaluate our model for paraphrasing
    through a user study.'
  address: New Orleans, Louisiana
  author:
  - first: David
    full: David Grangier
    id: david-grangier
    last: Grangier
  - first: Michael
    full: Michael Auli
    id: michael-auli
    last: Auli
  author_string: David Grangier, Michael Auli
  bibkey: grangier-auli-2018-quickedit
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1025
  month: June
  page_first: '272'
  page_last: '282'
  pages: "272\u2013282"
  paper_id: '25'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1025.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1025.jpg
  title: 'QuickEdit: Editing Text & Translations by Crossing Words Out'
  title_html: '<span class="acl-fixed-case">Q</span>uick<span class="acl-fixed-case">E</span>dit:
    Editing Text &amp; Translations by Crossing Words Out'
  url: https://www.aclweb.org/anthology/N18-1025
  year: '2018'
N18-1026:
  abstract: Task extraction is the process of identifying search intents over a set
    of queries potentially spanning multiple search sessions. Most existing research
    on task extraction has focused on identifying tasks within a single session, where
    the notion of a session is defined by a fixed length time window. By contrast,
    in this work we seek to identify tasks that span across multiple sessions. To
    identify tasks, we conduct a global analysis of a query log in its entirety without
    restricting analysis to individual temporal windows. To capture inherent task
    semantics, we represent queries as vectors in an abstract space. We learn the
    embedding of query words in this space by leveraging the temporal and lexical
    contexts of queries. Embedded query vectors are then clustered into tasks. Experiments
    demonstrate that task extraction effectiveness is improved significantly with
    our proposed method of query vector embedding in comparison to existing approaches
    that make use of documents retrieved from a collection to estimate semantic similarities
    between queries.
  address: New Orleans, Louisiana
  author:
  - first: Procheta
    full: Procheta Sen
    id: procheta-sen
    last: Sen
  - first: Debasis
    full: Debasis Ganguly
    id: debasis-ganguly
    last: Ganguly
  - first: Gareth
    full: Gareth Jones
    id: gareth-jones
    last: Jones
  author_string: Procheta Sen, Debasis Ganguly, Gareth Jones
  bibkey: sen-etal-2018-tempo
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1026
  month: June
  page_first: '283'
  page_last: '292'
  pages: "283\u2013292"
  paper_id: '26'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1026.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1026.jpg
  title: Tempo-Lexical Context Driven Word Embedding for Cross-Session Search Task
    Extraction
  title_html: Tempo-Lexical Context Driven Word Embedding for Cross-Session Search
    Task Extraction
  url: https://www.aclweb.org/anthology/N18-1026
  year: '2018'
N18-1027:
  abstract: Can attention- or gradient-based visualization techniques be used to infer
    token-level labels for binary sequence tagging problems, using networks trained
    only on sentence-level labels? We construct a neural network architecture based
    on soft attention, train it as a binary sentence classifier and evaluate against
    token-level annotation on four different datasets. Inferring token labels from
    a network provides a method for quantitatively evaluating what the model is learning,
    along with generating useful feedback in assistance systems. Our results indicate
    that attention-based methods are able to predict token-level labels more accurately,
    compared to gradient-based methods, sometimes even rivaling the supervised oracle
    network.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276393910
    type: video
    url: http://vimeo.com/276393910
  author:
  - first: Marek
    full: Marek Rei
    id: marek-rei
    last: Rei
  - first: Anders
    full: "Anders S\xF8gaard"
    id: anders-sogaard
    last: "S\xF8gaard"
  author_string: "Marek Rei, Anders S\xF8gaard"
  bibkey: rei-sogaard-2018-zero
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1027
  month: June
  page_first: '293'
  page_last: '302'
  pages: "293\u2013302"
  paper_id: '27'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1027.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1027.jpg
  title: 'Zero-Shot Sequence Labeling: Transferring Knowledge from Sentences to Tokens'
  title_html: 'Zero-Shot Sequence Labeling: Transferring Knowledge from Sentences
    to Tokens'
  url: https://www.aclweb.org/anthology/N18-1027
  year: '2018'
N18-1028:
  abstract: 'Information about the meaning of mathematical variables in text is useful
    in NLP/IR tasks such as symbol disambiguation, topic modeling and mathematical
    information retrieval (MIR). We introduce variable typing, the task of assigning
    one mathematical type (multi-word technical terms referring to mathematical concepts)
    to each variable in a sentence of mathematical text. As part of this work, we
    also introduce a new annotated data set composed of 33,524 data points extracted
    from scientific documents published on arXiv. Our intrinsic evaluation demonstrates
    that our data set is sufficient to successfully train and evaluate current classifiers
    from three different model architectures. The best performing model is evaluated
    on an extrinsic task: MIR, by producing a typed formula index. Our results show
    that the best performing MIR models make use of our typed index, compared to a
    formula index only containing raw symbols, thereby demonstrating the usefulness
    of variable typing.'
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1028.Datasets.tgz
    type: dataset
    url: https://www.aclweb.org/anthology/attachments/N18-1028.Datasets.tgz
  - filename: http://vimeo.com/276395060
    type: video
    url: http://vimeo.com/276395060
  author:
  - first: Yiannos
    full: Yiannos Stathopoulos
    id: yiannos-stathopoulos
    last: Stathopoulos
  - first: Simon
    full: Simon Baker
    id: simon-baker
    last: Baker
  - first: Marek
    full: Marek Rei
    id: marek-rei
    last: Rei
  - first: Simone
    full: Simone Teufel
    id: simone-teufel
    last: Teufel
  author_string: Yiannos Stathopoulos, Simon Baker, Marek Rei, Simone Teufel
  bibkey: stathopoulos-etal-2018-variable
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1028
  month: June
  page_first: '303'
  page_last: '312'
  pages: "303\u2013312"
  paper_id: '28'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1028.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1028.jpg
  title: 'Variable Typing: Assigning Meaning to Variables in Mathematical Text'
  title_html: 'Variable Typing: Assigning Meaning to Variables in Mathematical Text'
  url: https://www.aclweb.org/anthology/N18-1028
  year: '2018'
N18-1029:
  abstract: Machine Learning has been the quintessential solution for many AI problems,
    but learning models are heavily dependent on specific training data. Some learning
    models can be incorporated with prior knowledge using a Bayesian setup, but these
    learning models do not have the ability to access any organized world knowledge
    on demand. In this work, we propose to enhance learning models with world knowledge
    in the form of Knowledge Graph (KG) fact triples for Natural Language Processing
    (NLP) tasks. Our aim is to develop a deep learning model that can extract relevant
    prior support facts from knowledge graphs depending on the task using attention
    mechanism. We introduce a convolution-based model for learning representations
    of knowledge graph entity and relation clusters in order to reduce the attention
    space. We show that the proposed method is highly scalable to the amount of prior
    information that has to be processed and can be applied to any generic NLP task.
    Using this method we show significant improvement in performance for text classification
    with 20Newsgroups (News20) & DBPedia datasets, and natural language inference
    with Stanford Natural Language Inference (SNLI) dataset. We also demonstrate that
    a deep learning model can be trained with substantially less amount of labeled
    training data, when it has access to organized world knowledge in the form of
    a knowledge base.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276416920
    type: video
    url: http://vimeo.com/276416920
  author:
  - first: Annervaz
    full: Annervaz K M
    id: annervaz-k-m
    last: K M
  - first: Somnath
    full: Somnath Basu Roy Chowdhury
    id: somnath-basu-roy-chowdhury
    last: Basu Roy Chowdhury
  - first: Ambedkar
    full: Ambedkar Dukkipati
    id: ambedkar-dukkipati
    last: Dukkipati
  author_string: Annervaz K M, Somnath Basu Roy Chowdhury, Ambedkar Dukkipati
  bibkey: k-m-etal-2018-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1029
  month: June
  page_first: '313'
  page_last: '322'
  pages: "313\u2013322"
  paper_id: '29'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1029.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1029.jpg
  title: 'Learning beyond Datasets: Knowledge Graph Augmented Neural Networks for
    Natural Language Processing'
  title_html: 'Learning beyond Datasets: Knowledge Graph Augmented Neural Networks
    for Natural Language Processing'
  url: https://www.aclweb.org/anthology/N18-1029
  year: '2018'
N18-1030:
  abstract: "Building a taxonomy from the ground up involves several sub-tasks: selecting\
    \ terms to include, predicting semantic relations between terms, and selecting\
    \ a subset of relational instances to keep, given constraints on the taxonomy\
    \ graph. Methods for this final step \u2013 taxonomic organization \u2013 vary\
    \ both in terms of the constraints they impose, and whether they enable discovery\
    \ of synonymous terms. It is hard to isolate the impact of these factors on the\
    \ quality of the resulting taxonomy because organization methods are rarely compared\
    \ directly. In this paper, we present a head-to-head comparison of six taxonomic\
    \ organization algorithms that vary with respect to their structural and transitivity\
    \ constraints, and treatment of synonymy. We find that while transitive algorithms\
    \ out-perform their non-transitive counterparts, the top-performing transitive\
    \ algorithm is prohibitively slow for taxonomies with as few as 50 entities. We\
    \ propose a simple modification to a non-transitive optimum branching algorithm\
    \ to explicitly incorporate synonymy, resulting in a method that is substantially\
    \ faster than the best transitive algorithm while giving complementary performance."
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1030.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1030.Notes.pdf
  - filename: http://vimeo.com/276418325
    type: video
    url: http://vimeo.com/276418325
  author:
  - first: Anne
    full: Anne Cocos
    id: anne-cocos
    last: Cocos
  - first: Marianna
    full: Marianna Apidianaki
    id: marianna-apidianaki
    last: Apidianaki
  - first: Chris
    full: Chris Callison-Burch
    id: chris-callison-burch
    last: Callison-Burch
  author_string: Anne Cocos, Marianna Apidianaki, Chris Callison-Burch
  bibkey: cocos-etal-2018-comparing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1030
  month: June
  page_first: '323'
  page_last: '333'
  pages: "323\u2013333"
  paper_id: '30'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1030.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1030.jpg
  title: Comparing Constraints for Taxonomic Organization
  title_html: Comparing Constraints for Taxonomic Organization
  url: https://www.aclweb.org/anthology/N18-1030
  year: '2018'
N18-1031:
  abstract: We explore two solutions to the problem of mistranslating rare words in
    neural machine translation. First, we argue that the standard output layer, which
    computes the inner product of a vector representing the context with all possible
    output word embeddings, rewards frequent words disproportionately, and we propose
    to fix the norms of both vectors to a constant value. Second, we integrate a simple
    lexical module which is jointly trained with the rest of the model. We evaluate
    our approaches on eight language pairs with data sizes ranging from 100k to 8M
    words, and achieve improvements of up to +4.3 BLEU, surpassing phrase-based translation
    in nearly all settings.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276445914
    type: video
    url: http://vimeo.com/276445914
  author:
  - first: Toan
    full: Toan Nguyen
    id: toan-q-nguyen
    last: Nguyen
  - first: David
    full: David Chiang
    id: david-chiang
    last: Chiang
  author_string: Toan Nguyen, David Chiang
  bibkey: nguyen-chiang-2018-improving
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1031
  month: June
  page_first: '334'
  page_last: '343'
  pages: "334\u2013343"
  paper_id: '31'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1031.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1031.jpg
  title: Improving Lexical Choice in Neural Machine Translation
  title_html: Improving Lexical Choice in Neural Machine Translation
  url: https://www.aclweb.org/anthology/N18-1031
  year: '2018'
N18-1032:
  abstract: In this paper, we propose a new universal machine translation approach
    focusing on languages with a limited amount of parallel data. Our proposed approach
    utilizes a transfer-learning approach to share lexical and sentence level representations
    across multiple source languages into one target language. The lexical part is
    shared through a Universal Lexical Representation to support multi-lingual word-level
    sharing. The sentence-level sharing is represented by a model of experts from
    all source languages that share the source encoders with all other languages.
    This enables the low-resource language to utilize the lexical and sentence representations
    of the higher resource languages. Our approach is able to achieve 23 BLEU on Romanian-English
    WMT2016 using a tiny parallel corpus of 6k sentences, compared to the 18 BLEU
    of strong baseline system which uses multi-lingual training and back-translation.
    Furthermore, we show that the proposed approach can achieve almost 20 BLEU on
    the same dataset through fine-tuning a pre-trained multi-lingual system in a zero-shot
    setting.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276448004
    type: video
    url: http://vimeo.com/276448004
  author:
  - first: Jiatao
    full: Jiatao Gu
    id: jiatao-gu
    last: Gu
  - first: Hany
    full: Hany Hassan
    id: hany-hassan
    last: Hassan
  - first: Jacob
    full: Jacob Devlin
    id: jacob-devlin
    last: Devlin
  - first: Victor O.K.
    full: Victor O.K. Li
    id: victor-o-k-li
    last: Li
  author_string: Jiatao Gu, Hany Hassan, Jacob Devlin, Victor O.K. Li
  bibkey: gu-etal-2018-universal
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1032
  month: June
  page_first: '344'
  page_last: '354'
  pages: "344\u2013354"
  paper_id: '32'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1032.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1032.jpg
  title: Universal Neural Machine Translation for Extremely Low Resource Languages
  title_html: Universal Neural Machine Translation for Extremely Low Resource Languages
  url: https://www.aclweb.org/anthology/N18-1032
  year: '2018'
N18-1033:
  abstract: "There has been much recent work on training neural attention models at\
    \ the sequence-level using either reinforcement learning-style methods or by optimizing\
    \ the beam. In this paper, we survey a range of classical objective functions\
    \ that have been widely used to train linear models for structured prediction\
    \ and apply them to neural sequence to sequence models. Our experiments show that\
    \ these losses can perform surprisingly well by slightly outperforming beam search\
    \ optimization in a like for like setup. We also report new state of the art results\
    \ on both IWSLT\u201914 German-English translation as well as Gigaword abstractive\
    \ summarization. On the large WMT\u201914 English-French task, sequence-level\
    \ training achieves 41.5 BLEU which is on par with the state of the art."
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276450922
    type: video
    url: http://vimeo.com/276450922
  author:
  - first: Sergey
    full: Sergey Edunov
    id: sergey-edunov
    last: Edunov
  - first: Myle
    full: Myle Ott
    id: myle-ott
    last: Ott
  - first: Michael
    full: Michael Auli
    id: michael-auli
    last: Auli
  - first: David
    full: David Grangier
    id: david-grangier
    last: Grangier
  - first: "Marc\u2019Aurelio"
    full: "Marc\u2019Aurelio Ranzato"
    id: marcaurelio-ranzato
    last: Ranzato
  author_string: "Sergey Edunov, Myle Ott, Michael Auli, David Grangier, Marc\u2019\
    Aurelio Ranzato"
  bibkey: edunov-etal-2018-classical
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1033
  month: June
  page_first: '355'
  page_last: '364'
  pages: "355\u2013364"
  paper_id: '33'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1033.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1033.jpg
  title: Classical Structured Prediction Losses for Sequence to Sequence Learning
  title_html: Classical Structured Prediction Losses for Sequence to Sequence Learning
  url: https://www.aclweb.org/anthology/N18-1033
  year: '2018'
N18-1034:
  abstract: "Dirichlet Multinomial Regression (DMR) and other supervised topic models\
    \ can incorporate arbitrary document-level features to inform topic priors. However,\
    \ their ability to model corpora are limited by the representation and selection\
    \ of these features \u2013 a choice the topic modeler must make. Instead, we seek\
    \ models that can learn the feature representations upon which to condition topic\
    \ selection. We present deep Dirichlet Multinomial Regression (dDMR), a generative\
    \ topic model that simultaneously learns document feature representations and\
    \ topics. We evaluate dDMR on three datasets: New York Times articles with fine-grained\
    \ tags, Amazon product reviews with product images, and Reddit posts with subreddit\
    \ identity. dDMR learns representations that outperform DMR and LDA according\
    \ to heldout perplexity and are more effective at downstream predictive tasks\
    \ as the number of topics grows. Additionally, human subjects judge dDMR topics\
    \ as being more representative of associated document features. Finally, we find\
    \ that supervision leads to faster convergence as compared to an LDA baseline\
    \ and that dDMR\u2019s model fit is less sensitive to training parameters than\
    \ DMR."
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276397933
    type: video
    url: http://vimeo.com/276397933
  author:
  - first: Adrian
    full: Adrian Benton
    id: adrian-benton
    last: Benton
  - first: Mark
    full: Mark Dredze
    id: mark-dredze
    last: Dredze
  author_string: Adrian Benton, Mark Dredze
  bibkey: benton-dredze-2018-deep
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1034
  month: June
  page_first: '365'
  page_last: '374'
  pages: "365\u2013374"
  paper_id: '34'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1034.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1034.jpg
  title: Deep Dirichlet Multinomial Regression
  title_html: Deep <span class="acl-fixed-case">D</span>irichlet Multinomial Regression
  url: https://www.aclweb.org/anthology/N18-1034
  year: '2018'
N18-1035:
  abstract: 'Millions of conversations are generated every day on social media platforms.
    With limited attention, it is challenging for users to select which discussions
    they would like to participate in. Here we propose a new method for microblog
    conversation recommendation. While much prior work has focused on post-level recommendation,
    we exploit both the conversational context, and user content and behavior preferences.
    We propose a statistical model that jointly captures: (1) topics for representing
    user interests and conversation content, and (2) discourse modes for describing
    user replying behavior and conversation dynamics. Experimental results on two
    Twitter datasets demonstrate that our system outperforms methods that only model
    content without considering discourse.'
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276422518
    type: video
    url: http://vimeo.com/276422518
  author:
  - first: Xingshan
    full: Xingshan Zeng
    id: xingshan-zeng
    last: Zeng
  - first: Jing
    full: Jing Li
    id: jing-li
    last: Li
  - first: Lu
    full: Lu Wang
    id: lu-wang
    last: Wang
  - first: Nicholas
    full: Nicholas Beauchamp
    id: nicholas-beauchamp
    last: Beauchamp
  - first: Sarah
    full: Sarah Shugars
    id: sarah-shugars
    last: Shugars
  - first: Kam-Fai
    full: Kam-Fai Wong
    id: kam-fai-wong
    last: Wong
  author_string: Xingshan Zeng, Jing Li, Lu Wang, Nicholas Beauchamp, Sarah Shugars,
    Kam-Fai Wong
  bibkey: zeng-etal-2018-microblog
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1035
  month: June
  page_first: '375'
  page_last: '385'
  pages: "375\u2013385"
  paper_id: '35'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1035.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1035.jpg
  title: Microblog Conversation Recommendation via Joint Modeling of Topics and Discourse
  title_html: Microblog Conversation Recommendation via Joint Modeling of Topics and
    Discourse
  url: https://www.aclweb.org/anthology/N18-1035
  year: '2018'
N18-1036:
  abstract: Arguing without committing a fallacy is one of the main requirements of
    an ideal debate. But even when debating rules are strictly enforced and fallacious
    arguments punished, arguers often lapse into attacking the opponent by an ad hominem
    argument. As existing research lacks solid empirical investigation of the typology
    of ad hominem arguments as well as their potential causes, this paper fills this
    gap by (1) performing several large-scale annotation studies, (2) experimenting
    with various neural architectures and validating our working hypotheses, such
    as controversy or reasonableness, and (3) providing linguistic insights into triggers
    of ad hominem using explainable neural network architectures.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1036.Datasets.zip
    type: dataset
    url: https://www.aclweb.org/anthology/attachments/N18-1036.Datasets.zip
  - filename: http://vimeo.com/276425357
    type: video
    url: http://vimeo.com/276425357
  author:
  - first: Ivan
    full: Ivan Habernal
    id: ivan-habernal
    last: Habernal
  - first: Henning
    full: Henning Wachsmuth
    id: henning-wachsmuth
    last: Wachsmuth
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  - first: Benno
    full: Benno Stein
    id: benno-stein
    last: Stein
  author_string: Ivan Habernal, Henning Wachsmuth, Iryna Gurevych, Benno Stein
  bibkey: habernal-etal-2018-name
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1036
  month: June
  page_first: '386'
  page_last: '396'
  pages: "386\u2013396"
  paper_id: '36'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1036.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1036.jpg
  title: 'Before Name-Calling: Dynamics and Triggers of Ad Hominem Fallacies in Web
    Argumentation'
  title_html: 'Before Name-Calling: Dynamics and Triggers of Ad Hominem Fallacies
    in Web Argumentation'
  url: https://www.aclweb.org/anthology/N18-1036
  year: '2018'
N18-1037:
  abstract: 'In this paper, we study the problem of parsing structured knowledge graphs
    from textual descriptions. In particular, we consider the scene graph representation
    that considers objects together with their attributes and relations: this representation
    has been proved useful across a variety of vision and language applications. We
    begin by introducing an alternative but equivalent edge-centric view of scene
    graphs that connect to dependency parses. Together with a careful redesign of
    label and action space, we combine the two-stage pipeline used in prior work (generic
    dependency parsing followed by simple post-processing) into one, enabling end-to-end
    training. The scene graphs generated by our learned neural dependency parser achieve
    an F-score similarity of 49.67% to ground truth graphs on our evaluation set,
    surpassing best previous approaches by 5%. We further demonstrate the effectiveness
    of our learned parser on image retrieval applications.'
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276453229
    type: video
    url: http://vimeo.com/276453229
  author:
  - first: Yu-Siang
    full: Yu-Siang Wang
    id: yu-siang-wang
    last: Wang
  - first: Chenxi
    full: Chenxi Liu
    id: chenxi-liu
    last: Liu
  - first: Xiaohui
    full: Xiaohui Zeng
    id: xiaohui-zeng
    last: Zeng
  - first: Alan
    full: Alan Yuille
    id: alan-yuille
    last: Yuille
  author_string: Yu-Siang Wang, Chenxi Liu, Xiaohui Zeng, Alan Yuille
  bibkey: wang-etal-2018-scene
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1037
  month: June
  page_first: '397'
  page_last: '407'
  pages: "397\u2013407"
  paper_id: '37'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1037.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1037.jpg
  title: Scene Graph Parsing as Dependency Parsing
  title_html: Scene Graph Parsing as Dependency Parsing
  url: https://www.aclweb.org/anthology/N18-1037
  year: '2018'
N18-1038:
  abstract: "We investigate grounded sentence representations, where we train a sentence\
    \ encoder to predict the image features of a given caption\u2014i.e., we try to\
    \ \u201Cimagine\u201D how a sentence would be depicted visually\u2014and use the\
    \ resultant features as sentence representations. We examine the quality of the\
    \ learned representations on a variety of standard sentence representation quality\
    \ benchmarks, showing improved performance for grounded models over non-grounded\
    \ ones. In addition, we thoroughly analyze the extent to which grounding contributes\
    \ to improved performance, and show that the system also learns improved word\
    \ embeddings."
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/277631178
    type: video
    url: http://vimeo.com/277631178
  author:
  - first: Douwe
    full: Douwe Kiela
    id: douwe-kiela
    last: Kiela
  - first: Alexis
    full: Alexis Conneau
    id: alexis-conneau
    last: Conneau
  - first: Allan
    full: Allan Jabri
    id: allan-jabri
    last: Jabri
  - first: Maximilian
    full: Maximilian Nickel
    id: maximilian-nickel
    last: Nickel
  author_string: Douwe Kiela, Alexis Conneau, Allan Jabri, Maximilian Nickel
  bibkey: kiela-etal-2018-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1038
  month: June
  page_first: '408'
  page_last: '418'
  pages: "408\u2013418"
  paper_id: '38'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1038.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1038.jpg
  title: Learning Visually Grounded Sentence Representations
  title_html: Learning Visually Grounded Sentence Representations
  url: https://www.aclweb.org/anthology/N18-1038
  year: '2018'
N18-1039:
  abstract: The present work investigates whether different quantification mechanisms
    (set comparison, vague quantification, and proportional estimation) can be jointly
    learned from visual scenes by a multi-task computational model. The motivation
    is that, in humans, these processes underlie the same cognitive, non-symbolic
    ability, which allows an automatic estimation and comparison of set magnitudes.
    We show that when information about lower-complexity tasks is available, the higher-level
    proportional task becomes more accurate than when performed in isolation. Moreover,
    the multi-task model is able to generalize to unseen combinations of target/non-target
    objects. Consistently with behavioral evidence showing the interference of absolute
    number in the proportional task, the multi-task model no longer works when asked
    to provide the number of target objects in the scene.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/277631187
    type: video
    url: http://vimeo.com/277631187
  author:
  - first: Sandro
    full: Sandro Pezzelle
    id: sandro-pezzelle
    last: Pezzelle
  - first: Ionut-Teodor
    full: Ionut-Teodor Sorodoc
    id: ionut-sorodoc
    last: Sorodoc
  - first: Raffaella
    full: Raffaella Bernardi
    id: raffaella-bernardi
    last: Bernardi
  author_string: Sandro Pezzelle, Ionut-Teodor Sorodoc, Raffaella Bernardi
  bibkey: pezzelle-etal-2018-comparatives
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1039
  month: June
  page_first: '419'
  page_last: '430'
  pages: "419\u2013430"
  paper_id: '39'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1039.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1039.jpg
  title: 'Comparatives, Quantifiers, Proportions: a Multi-Task Model for the Learning
    of Quantities from Vision'
  title_html: 'Comparatives, Quantifiers, Proportions: a Multi-Task Model for the
    Learning of Quantities from Vision'
  url: https://www.aclweb.org/anthology/N18-1039
  year: '2018'
N18-1040:
  abstract: 'Visual question answering (Visual QA) has attracted a lot of attention
    lately, seen essentially as a form of (visual) Turing test that artificial intelligence
    should strive to achieve. In this paper, we study a crucial component of this
    task: how can we design good datasets for the task? We focus on the design of
    multiple-choice based datasets where the learner has to select the right answer
    from a set of candidate ones including the target (i.e., the correct one) and
    the decoys (i.e., the incorrect ones). Through careful analysis of the results
    attained by state-of-the-art learning models and human annotators on existing
    datasets, we show that the design of the decoy answers has a significant impact
    on how and what the learning models learn from the datasets. In particular, the
    resulting learner can ignore the visual information, the question, or both while
    still doing well on the task. Inspired by this, we propose automatic procedures
    to remedy such design deficiencies. We apply the procedures to re-construct decoy
    answers for two popular Visual QA datasets as well as to create a new Visual QA
    dataset from the Visual Genome project, resulting in the largest dataset for this
    task. Extensive empirical studies show that the design deficiencies have been
    alleviated in the remedied datasets and the performance on them is likely a more
    faithful indicator of the difference among learning models. The datasets are released
    and publicly available via http://www.teds.usc.edu/website_vqa/.'
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1040.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1040.Notes.pdf
  - filename: http://vimeo.com/276455887
    type: video
    url: http://vimeo.com/276455887
  author:
  - first: Wei-Lun
    full: Wei-Lun Chao
    id: wei-lun-chao
    last: Chao
  - first: Hexiang
    full: Hexiang Hu
    id: hexiang-hu
    last: Hu
  - first: Fei
    full: Fei Sha
    id: fei-sha
    last: Sha
  author_string: Wei-Lun Chao, Hexiang Hu, Fei Sha
  bibkey: chao-etal-2018-negative
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1040
  month: June
  page_first: '431'
  page_last: '441'
  pages: "431\u2013441"
  paper_id: '40'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1040.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1040.jpg
  title: 'Being Negative but Constructively: Lessons Learnt from Creating Better Visual
    Question Answering Datasets'
  title_html: 'Being Negative but Constructively: Lessons Learnt from Creating Better
    Visual Question Answering Datasets'
  url: https://www.aclweb.org/anthology/N18-1040
  year: '2018'
N18-1041:
  abstract: "Abstract Meaning Representation (AMR) parsing aims at abstracting away\
    \ from the syntactic realization of a sentence, and denote only its meaning in\
    \ a canonical form. As such, it is ideal for paraphrase detection, a problem in\
    \ which one is required to specify whether two sentences have the same meaning.\
    \ We show that na\xEFve use of AMR in paraphrase detection is not necessarily\
    \ useful, and turn to describe a technique based on latent semantic analysis in\
    \ combination with AMR parsing that significantly advances state-of-the-art results\
    \ in paraphrase detection for the Microsoft Research Paraphrase Corpus. Our best\
    \ results in the transductive setting are 86.6% for accuracy and 90.0% for F1\
    \ measure. measure."
  address: New Orleans, Louisiana
  author:
  - first: Fuad
    full: Fuad Issa
    id: fuad-issa
    last: Issa
  - first: Marco
    full: Marco Damonte
    id: marco-damonte
    last: Damonte
  - first: Shay B.
    full: Shay B. Cohen
    id: shay-b-cohen
    last: Cohen
  - first: Xiaohui
    full: Xiaohui Yan
    id: xiaohui-yan
    last: Yan
  - first: Yi
    full: Yi Chang
    id: yi-chang
    last: Chang
  author_string: Fuad Issa, Marco Damonte, Shay B. Cohen, Xiaohui Yan, Yi Chang
  bibkey: issa-etal-2018-abstract
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1041
  month: June
  page_first: '442'
  page_last: '452'
  pages: "442\u2013452"
  paper_id: '41'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1041.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1041.jpg
  title: Abstract Meaning Representation for Paraphrase Detection
  title_html: Abstract Meaning Representation for Paraphrase Detection
  url: https://www.aclweb.org/anthology/N18-1041
  year: '2018'
N18-1042:
  abstract: The widespread use of word embeddings is associated with the recent successes
    of many natural language processing (NLP) systems. The key approach of popular
    models such as word2vec and GloVe is to learn dense vector representations from
    the context of words. More recently, other approaches have been proposed that
    incorporate different types of contextual information, including topics, dependency
    relations, n-grams, and sentiment. However, these models typically integrate only
    limited additional contextual information, and often in ad hoc ways. In this work,
    we introduce attr2vec, a novel framework for jointly learning embeddings for words
    and contextual attributes based on factorization machines. We perform experiments
    with different types of contextual information. Our experimental results on a
    text classification task demonstrate that using attr2vec to jointly learn embeddings
    for words and Part-of-Speech (POS) tags improves results compared to learning
    the embeddings independently. Moreover, we use attr2vec to train dependency-based
    embeddings and we show that they exhibit higher similarity between functionally
    related words compared to traditional approaches.
  address: New Orleans, Louisiana
  author:
  - first: Fabio
    full: Fabio Petroni
    id: fabio-petroni
    last: Petroni
  - first: Vassilis
    full: Vassilis Plachouras
    id: vassilis-plachouras
    last: Plachouras
  - first: Timothy
    full: Timothy Nugent
    id: timothy-nugent
    last: Nugent
  - first: Jochen L.
    full: Jochen L. Leidner
    id: jochen-l-leidner
    last: Leidner
  author_string: Fabio Petroni, Vassilis Plachouras, Timothy Nugent, Jochen L. Leidner
  bibkey: petroni-etal-2018-attr2vec
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1042
  month: June
  page_first: '453'
  page_last: '462'
  pages: "453\u2013462"
  paper_id: '42'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1042.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1042.jpg
  title: 'attr2vec: Jointly Learning Word and Contextual Attribute Embeddings with
    Factorization Machines'
  title_html: 'attr2vec: Jointly Learning Word and Contextual Attribute Embeddings
    with Factorization Machines'
  url: https://www.aclweb.org/anthology/N18-1042
  year: '2018'
N18-1043:
  abstract: Distributed representations of words learned from text have proved to
    be successful in various natural language processing tasks in recent times. While
    some methods represent words as vectors computed from text using predictive model
    (Word2vec) or dense count based model (GloVe), others attempt to represent these
    in a distributional thesaurus network structure where the neighborhood of a word
    is a set of words having adequate context overlap. Being motivated by recent surge
    of research in network embedding techniques (DeepWalk, LINE, node2vec etc.), we
    turn a distributional thesaurus network into dense word vectors and investigate
    the usefulness of distributional thesaurus embedding in improving overall word
    representation. This is the first attempt where we show that combining the proposed
    word representation obtained by distributional thesaurus embedding with the state-of-the-art
    word representations helps in improving the performance by a significant margin
    when evaluated against NLP tasks like word similarity and relatedness, synonym
    detection, analogy detection. Additionally, we show that even without using any
    handcrafted lexical resources we can come up with representations having comparable
    performance in the word similarity and relatedness tasks compared to the representations
    where a lexical resource has been used.
  address: New Orleans, Louisiana
  author:
  - first: Abhik
    full: Abhik Jana
    id: abhik-jana
    last: Jana
  - first: Pawan
    full: Pawan Goyal
    id: pawan-goyal
    last: Goyal
  author_string: Abhik Jana, Pawan Goyal
  bibkey: jana-goyal-2018-network
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1043
  month: June
  page_first: '463'
  page_last: '473'
  pages: "463\u2013473"
  paper_id: '43'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1043.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1043.jpg
  title: Can Network Embedding of Distributional Thesaurus Be Combined with Word Vectors
    for Better Representation?
  title_html: Can Network Embedding of Distributional Thesaurus Be Combined with Word
    Vectors for Better Representation?
  url: https://www.aclweb.org/anthology/N18-1043
  year: '2018'
N18-1044:
  abstract: "Diachronic distributional models track changes in word use over time.\
    \ In this paper, we propose a deep neural network diachronic distributional model.\
    \ Instead of modeling lexical change via a time series as is done in previous\
    \ work, we represent time as a continuous variable and model a word\u2019s usage\
    \ as a function of time. Additionally, we have also created a novel synthetic\
    \ task which measures a model\u2019s ability to capture the semantic trajectory.\
    \ This evaluation quantitatively measures how well a model captures the semantic\
    \ trajectory of a word over time. Finally, we explore how well the derivatives\
    \ of our model can be used to measure the speed of lexical change."
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1044.Datasets.zip
    type: dataset
    url: https://www.aclweb.org/anthology/attachments/N18-1044.Datasets.zip
  author:
  - first: Alex
    full: Alex Rosenfeld
    id: alex-rosenfeld
    last: Rosenfeld
  - first: Katrin
    full: Katrin Erk
    id: katrin-erk
    last: Erk
  author_string: Alex Rosenfeld, Katrin Erk
  bibkey: rosenfeld-erk-2018-deep
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1044
  month: June
  page_first: '474'
  page_last: '484'
  pages: "474\u2013484"
  paper_id: '44'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1044.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1044.jpg
  title: Deep Neural Models of Semantic Shift
  title_html: Deep Neural Models of Semantic Shift
  url: https://www.aclweb.org/anthology/N18-1044
  year: '2018'
N18-1045:
  abstract: "Modeling hypernymy, such as poodle is-a dog, is an important generalization\
    \ aid to many NLP tasks, such as entailment, relation extraction, and question\
    \ answering. Supervised learning from labeled hypernym sources, such as WordNet,\
    \ limits the coverage of these models, which can be addressed by learning hypernyms\
    \ from unlabeled text. Existing unsupervised methods either do not scale to large\
    \ vocabularies or yield unacceptably poor accuracy. This paper introduces distributional\
    \ inclusion vector embedding (DIVE), a simple-to-implement unsupervised method\
    \ of hypernym discovery via per-word non-negative vector embeddings which preserve\
    \ the inclusion property of word contexts. In experimental evaluations more comprehensive\
    \ than any previous literature of which we are aware\u2014evaluating on 11 datasets\
    \ using multiple existing as well as newly proposed scoring functions\u2014we\
    \ find that our method provides up to double the precision of previous unsupervised\
    \ methods, and the highest average performance, using a much more compact word\
    \ representation, and yielding many new state-of-the-art results."
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1045.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1045.Notes.pdf
  author:
  - first: Haw-Shiuan
    full: Haw-Shiuan Chang
    id: haw-shiuan-chang
    last: Chang
  - first: Ziyun
    full: Ziyun Wang
    id: ziyun-wang
    last: Wang
  - first: Luke
    full: Luke Vilnis
    id: luke-vilnis
    last: Vilnis
  - first: Andrew
    full: Andrew McCallum
    id: andrew-mccallum
    last: McCallum
  author_string: Haw-Shiuan Chang, Ziyun Wang, Luke Vilnis, Andrew McCallum
  bibkey: chang-etal-2018-distributional
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1045
  month: June
  page_first: '485'
  page_last: '495'
  pages: "485\u2013495"
  paper_id: '45'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1045.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1045.jpg
  title: Distributional Inclusion Vector Embedding for Unsupervised Hypernymy Detection
  title_html: Distributional Inclusion Vector Embedding for Unsupervised Hypernymy
    Detection
  url: https://www.aclweb.org/anthology/N18-1045
  year: '2018'
N18-1046:
  abstract: This paper presents a corpus and experiments to mine possession relations
    from text. Specifically, we target alienable and control possessions, and assign
    temporal anchors indicating when the possession holds between possessor and possessee.
    We present new annotations for this task, and experimental results using both
    traditional classifiers and neural networks. Results show that the three subtasks
    (predicting possession existence, possession type and temporal anchors) can be
    automated.
  address: New Orleans, Louisiana
  author:
  - first: Dhivya
    full: Dhivya Chinnappa
    id: dhivya-chinnappa
    last: Chinnappa
  - first: Eduardo
    full: Eduardo Blanco
    id: eduardo-blanco
    last: Blanco
  author_string: Dhivya Chinnappa, Eduardo Blanco
  bibkey: chinnappa-blanco-2018-mining
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1046
  month: June
  page_first: '496'
  page_last: '505'
  pages: "496\u2013505"
  paper_id: '46'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1046.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1046.jpg
  title: 'Mining Possessions: Existence, Type and Temporal Anchors'
  title_html: 'Mining Possessions: Existence, Type and Temporal Anchors'
  url: https://www.aclweb.org/anthology/N18-1046
  year: '2018'
N18-1047:
  abstract: Although neural tensor networks (NTNs) have been successful in many NLP
    tasks, they require a large number of parameters to be estimated, which often
    leads to overfitting and a long training time. We address these issues by applying
    eigendecomposition to each slice matrix of a tensor to reduce its number of paramters.
    First, we evaluate our proposed NTN models on knowledge graph completion. Second,
    we extend the models to recursive NTNs (RNTNs) and evaluate them on logical reasoning
    tasks. These experiments show that our proposed models learn better and faster
    than the original (R)NTNs.
  address: New Orleans, Louisiana
  author:
  - first: Takahiro
    full: Takahiro Ishihara
    id: takahiro-ishihara
    last: Ishihara
  - first: Katsuhiko
    full: Katsuhiko Hayashi
    id: katsuhiko-hayashi
    last: Hayashi
  - first: Hitoshi
    full: Hitoshi Manabe
    id: hitoshi-manabe
    last: Manabe
  - first: Masashi
    full: Masashi Shimbo
    id: masashi-shimbo
    last: Shimbo
  - first: Masaaki
    full: Masaaki Nagata
    id: masaaki-nagata
    last: Nagata
  author_string: Takahiro Ishihara, Katsuhiko Hayashi, Hitoshi Manabe, Masashi Shimbo,
    Masaaki Nagata
  bibkey: ishihara-etal-2018-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1047
  month: June
  page_first: '506'
  page_last: '515'
  pages: "506\u2013515"
  paper_id: '47'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1047.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1047.jpg
  title: Neural Tensor Networks with Diagonal Slice Matrices
  title_html: Neural Tensor Networks with Diagonal Slice Matrices
  url: https://www.aclweb.org/anthology/N18-1047
  year: '2018'
N18-1048:
  abstract: 'Word vector specialisation (also known as retrofitting) is a portable,
    light-weight approach to fine-tuning arbitrary distributional word vector spaces
    by injecting external knowledge from rich lexical resources such as WordNet. By
    design, these post-processing methods only update the vectors of words occurring
    in external lexicons, leaving the representations of all unseen words intact.
    In this paper, we show that constraint-driven vector space specialisation can
    be extended to unseen words. We propose a novel post-specialisation method that:
    a) preserves the useful linguistic knowledge for seen words; while b) propagating
    this external signal to unseen words in order to improve their vector representations
    as well. Our post-specialisation approach explicits a non-linear specialisation
    function in the form of a deep neural network by learning to predict specialised
    vectors from their original distributional counterparts. The learned function
    is then used to specialise vectors of unseen words. This approach, applicable
    to any post-processing model, yields considerable gains over the initial specialisation
    models both in intrinsic word similarity tasks, and in two downstream tasks: dialogue
    state tracking and lexical text simplification. The positive effects persist across
    three languages, demonstrating the importance of specialising the full vocabulary
    of distributional word vector spaces.'
  address: New Orleans, Louisiana
  author:
  - first: Ivan
    full: "Ivan Vuli\u0107"
    id: ivan-vulic
    last: "Vuli\u0107"
  - first: Goran
    full: "Goran Glava\u0161"
    id: goran-glavas
    last: "Glava\u0161"
  - first: Nikola
    full: "Nikola Mrk\u0161i\u0107"
    id: nikola-mrksic
    last: "Mrk\u0161i\u0107"
  - first: Anna
    full: Anna Korhonen
    id: anna-korhonen
    last: Korhonen
  author_string: "Ivan Vuli\u0107, Goran Glava\u0161, Nikola Mrk\u0161i\u0107, Anna\
    \ Korhonen"
  bibkey: vulic-etal-2018-post
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1048
  month: June
  page_first: '516'
  page_last: '527'
  pages: "516\u2013527"
  paper_id: '48'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1048.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1048.jpg
  title: 'Post-Specialisation: Retrofitting Vectors of Words Unseen in Lexical Resources'
  title_html: 'Post-Specialisation: Retrofitting Vectors of Words Unseen in Lexical
    Resources'
  url: https://www.aclweb.org/anthology/N18-1048
  year: '2018'
N18-1049:
  abstract: The recent tremendous success of unsupervised word embeddings in a multitude
    of applications raises the obvious question if similar methods could be derived
    to improve embeddings (i.e. semantic representations) of word sequences as well.
    We present a simple but efficient unsupervised objective to train distributed
    representations of sentences. Our method outperforms the state-of-the-art unsupervised
    models on most benchmark tasks, highlighting the robustness of the produced general-purpose
    sentence embeddings.
  address: New Orleans, Louisiana
  author:
  - first: Matteo
    full: Matteo Pagliardini
    id: matteo-pagliardini
    last: Pagliardini
  - first: Prakhar
    full: Prakhar Gupta
    id: prakhar-gupta
    last: Gupta
  - first: Martin
    full: Martin Jaggi
    id: martin-jaggi
    last: Jaggi
  author_string: Matteo Pagliardini, Prakhar Gupta, Martin Jaggi
  bibkey: pagliardini-etal-2018-unsupervised
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1049
  month: June
  page_first: '528'
  page_last: '540'
  pages: "528\u2013540"
  paper_id: '49'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1049.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1049.jpg
  title: Unsupervised Learning of Sentence Embeddings Using Compositional n-Gram Features
  title_html: Unsupervised Learning of Sentence Embeddings Using Compositional n-Gram
    Features
  url: https://www.aclweb.org/anthology/N18-1049
  year: '2018'
N18-1050:
  abstract: Training data for sentiment analysis are abundant in multiple domains,
    yet scarce for other domains. It is useful to leveraging data available for all
    existing domains to enhance performance on different domains. We investigate this
    problem by learning domain-specific representations of input sentences using neural
    network. In particular, a descriptor vector is learned for representing each domain,
    which is used to map adversarially trained domain-general Bi-LSTM input representations
    into domain-specific representations. Based on this model, we further expand the
    input representation with exemplary domain knowledge, collected by attending over
    a memory network of domain training data. Results show that our model outperforms
    existing methods on multi-domain sentiment analysis significantly, giving the
    best accuracies on two different benchmarks.
  address: New Orleans, Louisiana
  author:
  - first: Qi
    full: Qi Liu
    id: qi-liu
    last: Liu
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  - first: Jiangming
    full: Jiangming Liu
    id: jiangming-liu
    last: Liu
  author_string: Qi Liu, Yue Zhang, Jiangming Liu
  bibkey: liu-etal-2018-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1050
  month: June
  page_first: '541'
  page_last: '550'
  pages: "541\u2013550"
  paper_id: '50'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1050.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1050.jpg
  title: Learning Domain Representation for Multi-Domain Sentiment Classification
  title_html: Learning Domain Representation for Multi-Domain Sentiment Classification
  url: https://www.aclweb.org/anthology/N18-1050
  year: '2018'
N18-1051:
  abstract: 'Target-dependent classification tasks, such as aspect-level sentiment
    analysis, perform fine-grained classifications towards specific targets. Semantic
    compositions over tree structures are promising for such tasks, as they can potentially
    capture long-distance interactions between targets and their contexts. However,
    previous work that operates on tree structures resorts to syntactic parsers or
    Treebank annotations, which are either subject to noise in informal texts or highly
    expensive to obtain. To address above issues, we propose a reinforcement learning
    based approach, which automatically induces target-specific sentence representations
    over tree structures. The underlying model is a RNN encoder-decoder that explores
    possible binary tree structures and a reward mechanism that encourages structures
    that improve performances on downstream tasks. We evaluate our approach on two
    benchmark tasks: firm-specific cumulative abnormal return prediction (based on
    formal news texts) and aspect-level sentiment analysis (based on informal social
    media texts). Experimental results show that our model gives superior performances
    compared to previous work that operates on parsed trees. Moreover, our approach
    gives some intuitions on how target-specific sentence representations can be achieved
    from its word constituents.'
  address: New Orleans, Louisiana
  author:
  - first: Junwen
    full: Junwen Duan
    id: junwen-duan
    last: Duan
  - first: Xiao
    full: Xiao Ding
    id: xiao-ding
    last: Ding
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  author_string: Junwen Duan, Xiao Ding, Ting Liu
  bibkey: duan-etal-2018-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1051
  month: June
  page_first: '551'
  page_last: '560'
  pages: "551\u2013560"
  paper_id: '51'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1051.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1051.jpg
  title: Learning Sentence Representations over Tree Structures for Target-Dependent
    Classification
  title_html: Learning Sentence Representations over Tree Structures for Target-Dependent
    Classification
  url: https://www.aclweb.org/anthology/N18-1051
  year: '2018'
N18-1052:
  abstract: Text might contain or invoke multiple emotions with varying intensities.
    As such, emotion detection, to predict multiple emotions associated with a given
    text, can be cast into a multi-label classification problem. We would like to
    go one step further so that a ranked list of relevant emotions are generated where
    top ranked emotions are more intensely associated with text compared to lower
    ranked emotions, whereas the rankings of irrelevant emotions are not important.
    A novel framework of relevant emotion ranking is proposed to tackle the problem.
    In the framework, the objective loss function is designed elaborately so that
    both emotion prediction and rankings of only relevant emotions can be achieved.
    Moreover, we observe that some emotions co-occur more often while other emotions
    rarely co-exist. Such information is incorporated into the framework as constraints
    to improve the accuracy of emotion detection. Experimental results on two real-world
    corpora show that the proposed framework can effectively deal with emotion detection
    and performs remarkably better than the state-of-the-art emotion detection approaches
    and multi-label learning methods.
  address: New Orleans, Louisiana
  author:
  - first: Deyu
    full: Deyu Zhou
    id: deyu-zhou
    last: Zhou
  - first: Yang
    full: Yang Yang
    id: yang-yang
    last: Yang
  - first: Yulan
    full: Yulan He
    id: yulan-he
    last: He
  author_string: Deyu Zhou, Yang Yang, Yulan He
  bibkey: zhou-etal-2018-relevant
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1052
  month: June
  page_first: '561'
  page_last: '571'
  pages: "561\u2013571"
  paper_id: '52'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1052.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1052.jpg
  title: Relevant Emotion Ranking from Text Constrained with Emotion Relationships
  title_html: Relevant Emotion Ranking from Text Constrained with Emotion Relationships
  url: https://www.aclweb.org/anthology/N18-1052
  year: '2018'
N18-1053:
  abstract: Efficient word representations play an important role in solving various
    problems related to Natural Language Processing (NLP), data mining, text mining
    etc. The issue of data sparsity poses a great challenge in creating efficient
    word representation model for solving the underlying problem. The problem is more
    intensified in resource-poor scenario due to the absence of sufficient amount
    of corpus. In this work we propose to minimize the effect of data sparsity by
    leveraging bilingual word embeddings learned through a parallel corpus. We train
    and evaluate Long Short Term Memory (LSTM) based architecture for aspect level
    sentiment classification. The neural network architecture is further assisted
    by the hand-crafted features for the prediction. We show the efficacy of the proposed
    model against state-of-the-art methods in two experimental setups i.e. multi-lingual
    and cross-lingual.
  address: New Orleans, Louisiana
  author:
  - first: Md Shad
    full: Md Shad Akhtar
    id: md-shad-akhtar
    last: Akhtar
  - first: Palaash
    full: Palaash Sawant
    id: palaash-sawant
    last: Sawant
  - first: Sukanta
    full: Sukanta Sen
    id: sukanta-sen
    last: Sen
  - first: Asif
    full: Asif Ekbal
    id: asif-ekbal
    last: Ekbal
  - first: Pushpak
    full: Pushpak Bhattacharyya
    id: pushpak-bhattacharyya
    last: Bhattacharyya
  author_string: Md Shad Akhtar, Palaash Sawant, Sukanta Sen, Asif Ekbal, Pushpak
    Bhattacharyya
  bibkey: akhtar-etal-2018-solving
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1053
  month: June
  page_first: '572'
  page_last: '582'
  pages: "572\u2013582"
  paper_id: '53'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1053.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1053.jpg
  title: Solving Data Sparsity for Aspect Based Sentiment Analysis Using Cross-Linguality
    and Multi-Linguality
  title_html: Solving Data Sparsity for Aspect Based Sentiment Analysis Using Cross-Linguality
    and Multi-Linguality
  url: https://www.aclweb.org/anthology/N18-1053
  year: '2018'
N18-1054:
  abstract: "For over a decade, machine learning has been used to extract opinion-holder-target\
    \ structures from text to answer the question \u201CWho expressed what kind of\
    \ sentiment towards what?\u201D. Recent neural approaches do not outperform the\
    \ state-of-the-art feature-based models for Opinion Role Labeling (ORL). We suspect\
    \ this is due to the scarcity of labeled training data and address this issue\
    \ using different multi-task learning (MTL) techniques with a related task which\
    \ has substantially more data, i.e. Semantic Role Labeling (SRL). We show that\
    \ two MTL models improve significantly over the single-task model for labeling\
    \ of both holders and targets, on the development and the test sets. We found\
    \ that the vanilla MTL model, which makes predictions using only shared ORL and\
    \ SRL features, performs the best. With deeper analysis we determine what works\
    \ and what might be done to make further improvements for ORL."
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1054.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1054.Notes.pdf
  author:
  - first: Ana
    full: "Ana Marasovi\u0107"
    id: ana-marasovic
    last: "Marasovi\u0107"
  - first: Anette
    full: Anette Frank
    id: anette-frank
    last: Frank
  author_string: "Ana Marasovi\u0107, Anette Frank"
  bibkey: marasovic-frank-2018-srl4orl
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1054
  month: June
  page_first: '583'
  page_last: '594'
  pages: "583\u2013594"
  paper_id: '54'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1054.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1054.jpg
  title: 'SRL4ORL: Improving Opinion Role Labeling Using Multi-Task Learning with
    Semantic Role Labeling'
  title_html: '<span class="acl-fixed-case">SRL</span>4<span class="acl-fixed-case">ORL</span>:
    Improving Opinion Role Labeling Using Multi-Task Learning with Semantic Role Labeling'
  url: https://www.aclweb.org/anthology/N18-1054
  year: '2018'
N18-1055:
  abstract: "Previously, neural methods in grammatical error correction (GEC) did\
    \ not reach state-of-the-art results compared to phrase-based statistical machine\
    \ translation (SMT) baselines. We demonstrate parallels between neural GEC and\
    \ low-resource neural MT and successfully adapt several methods from low-resource\
    \ MT to neural GEC. We further establish guidelines for trustable results in neural\
    \ GEC and propose a set of model-independent methods for neural GEC that can be\
    \ easily applied in most GEC settings. Proposed methods include adding source-side\
    \ noise, domain-adaptation techniques, a GEC-specific training-objective, transfer\
    \ learning with monolingual data, and ensembling of independently trained GEC\
    \ models and language models. The combined effects of these methods result in\
    \ better than state-of-the-art neural GEC models that outperform previously best\
    \ neural GEC systems by more than 10% M\xB2 on the CoNLL-2014 benchmark and 5.9%\
    \ on the JFLEG test set. Non-neural state-of-the-art systems are outperformed\
    \ by more than 2% on the CoNLL-2014 benchmark and by 4% on JFLEG."
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276409294
    type: video
    url: http://vimeo.com/276409294
  author:
  - first: Marcin
    full: Marcin Junczys-Dowmunt
    id: marcin-junczys-dowmunt
    last: Junczys-Dowmunt
  - first: Roman
    full: Roman Grundkiewicz
    id: roman-grundkiewicz
    last: Grundkiewicz
  - first: Shubha
    full: Shubha Guha
    id: shubha-guha
    last: Guha
  - first: Kenneth
    full: Kenneth Heafield
    id: kenneth-heafield
    last: Heafield
  author_string: Marcin Junczys-Dowmunt, Roman Grundkiewicz, Shubha Guha, Kenneth
    Heafield
  bibkey: junczys-dowmunt-etal-2018-approaching
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1055
  month: June
  page_first: '595'
  page_last: '606'
  pages: "595\u2013606"
  paper_id: '55'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1055.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1055.jpg
  title: Approaching Neural Grammatical Error Correction as a Low-Resource Machine
    Translation Task
  title_html: Approaching Neural Grammatical Error Correction as a Low-Resource Machine
    Translation Task
  url: https://www.aclweb.org/anthology/N18-1055
  year: '2018'
N18-1056:
  abstract: "Cross-lingual Hypernymy Detection involves determining if a word in one\
    \ language (\u201Cfruit\u201D) is a hypernym of a word in another language (\u201C\
    pomme\u201D i.e. apple in French). The ability to detect hypernymy cross-lingually\
    \ can aid in solving cross-lingual versions of tasks such as textual entailment\
    \ and event coreference. We propose BiSparse-Dep, a family of unsupervised approaches\
    \ for cross-lingual hypernymy detection, which learns sparse, bilingual word embeddings\
    \ based on dependency contexts. We show that BiSparse-Dep can significantly improve\
    \ performance on this task, compared to approaches based only on lexical context.\
    \ Our approach is also robust, showing promise for low-resource settings: our\
    \ dependency-based embeddings can be learned using a parser trained on related\
    \ languages, with negligible loss in performance. We also crowd-source a challenging\
    \ dataset for this task on four languages \u2013 Russian, French, Arabic, and\
    \ Chinese. Our embeddings and datasets are publicly available."
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276412103
    type: video
    url: http://vimeo.com/276412103
  author:
  - first: Shyam
    full: Shyam Upadhyay
    id: shyam-upadhyay
    last: Upadhyay
  - first: Yogarshi
    full: Yogarshi Vyas
    id: yogarshi-vyas
    last: Vyas
  - first: Marine
    full: Marine Carpuat
    id: marine-carpuat
    last: Carpuat
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Shyam Upadhyay, Yogarshi Vyas, Marine Carpuat, Dan Roth
  bibkey: upadhyay-etal-2018-robust
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1056
  month: June
  page_first: '607'
  page_last: '618'
  pages: "607\u2013618"
  paper_id: '56'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1056.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1056.jpg
  title: Robust Cross-Lingual Hypernymy Detection Using Dependency Context
  title_html: Robust Cross-Lingual Hypernymy Detection Using Dependency Context
  url: https://www.aclweb.org/anthology/N18-1056
  year: '2018'
N18-1057:
  abstract: Translation-based methods for grammar correction that directly map noisy,
    ungrammatical text to their clean counterparts are able to correct a broad range
    of errors; however, such techniques are bottlenecked by the need for a large parallel
    corpus of noisy and clean sentence pairs. In this paper, we consider synthesizing
    parallel data by noising a clean monolingual corpus. While most previous approaches
    introduce perturbations using features computed from local context windows, we
    instead develop error generation processes using a neural sequence transduction
    model trained to translate clean examples to their noisy counterparts. Given a
    corpus of clean examples, we propose beam search noising procedures to synthesize
    additional noisy examples that human evaluators were nearly unable to discriminate
    from nonsynthesized examples. Surprisingly, when trained on additional data synthesized
    using our best-performing noising scheme, our model approaches the same performance
    as when trained on additional nonsynthesized data.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276371501
    type: video
    url: http://vimeo.com/276371501
  author:
  - first: Ziang
    full: Ziang Xie
    id: ziang-xie
    last: Xie
  - first: Guillaume
    full: Guillaume Genthial
    id: guillaume-genthial
    last: Genthial
  - first: Stanley
    full: Stanley Xie
    id: stanley-xie
    last: Xie
  - first: Andrew
    full: Andrew Ng
    id: andrew-y-ng
    last: Ng
  - first: Dan
    full: Dan Jurafsky
    id: dan-jurafsky
    last: Jurafsky
  author_string: Ziang Xie, Guillaume Genthial, Stanley Xie, Andrew Ng, Dan Jurafsky
  bibkey: xie-etal-2018-noising
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1057
  month: June
  page_first: '619'
  page_last: '628'
  pages: "619\u2013628"
  paper_id: '57'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1057.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1057.jpg
  title: 'Noising and Denoising Natural Language: Diverse Backtranslation for Grammar
    Correction'
  title_html: 'Noising and Denoising Natural Language: Diverse Backtranslation for
    Grammar Correction'
  url: https://www.aclweb.org/anthology/N18-1057
  year: '2018'
N18-1058:
  abstract: 'Building curious machines that can answer as well as ask questions is
    an important challenge for AI. The two tasks of question answering and question
    generation are usually tackled separately in the NLP literature. At the same time,
    both require significant amounts of supervised data which is hard to obtain in
    many domains. To alleviate these issues, we propose a self-training method for
    jointly learning to ask as well as answer questions, leveraging unlabeled text
    along with labeled question answer pairs for learning. We evaluate our approach
    on four benchmark datasets: SQUAD, MS MARCO, WikiQA and TrecQA, and show significant
    improvements over a number of established baselines on both question answering
    and question generation tasks. We also achieved new state-of-the-art results on
    two competitive answer sentence selection tasks: WikiQA and TrecQA.'
  address: New Orleans, Louisiana
  author:
  - first: Mrinmaya
    full: Mrinmaya Sachan
    id: mrinmaya-sachan
    last: Sachan
  - first: Eric
    full: Eric Xing
    id: eric-xing
    last: Xing
  author_string: Mrinmaya Sachan, Eric Xing
  bibkey: sachan-xing-2018-self
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1058
  month: June
  page_first: '629'
  page_last: '640'
  pages: "629\u2013640"
  paper_id: '58'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1058.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1058.jpg
  title: Self-Training for Jointly Learning to Ask and Answer Questions
  title_html: Self-Training for Jointly Learning to Ask and Answer Questions
  url: https://www.aclweb.org/anthology/N18-1058
  year: '2018'
N18-1059:
  abstract: Answering complex questions is a time-consuming activity for humans that
    requires reasoning and integration of information. Recent work on reading comprehension
    made headway in answering simple questions, but tackling complex questions is
    still an ongoing research challenge. Conversely, semantic parsers have been successful
    at handling compositionality, but only when the information resides in a target
    knowledge-base. In this paper, we present a novel framework for answering broad
    and complex questions, assuming answering simple questions is possible using a
    search engine and a reading comprehension model. We propose to decompose complex
    questions into a sequence of simple questions, and compute the final answer from
    the sequence of answers. To illustrate the viability of our approach, we create
    a new dataset of complex questions, ComplexWebQuestions, and present a model that
    decomposes questions and interacts with the web to compute an answer. We empirically
    demonstrate that question decomposition improves performance from 20.8 precision@1
    to 27.5 precision@1 on this new dataset.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1059.Datasets.zip
    type: dataset
    url: https://www.aclweb.org/anthology/attachments/N18-1059.Datasets.zip
  - filename: http://vimeo.com/276429529
    type: video
    url: http://vimeo.com/276429529
  author:
  - first: Alon
    full: Alon Talmor
    id: alon-talmor
    last: Talmor
  - first: Jonathan
    full: Jonathan Berant
    id: jonathan-berant
    last: Berant
  author_string: Alon Talmor, Jonathan Berant
  bibkey: talmor-berant-2018-web
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1059
  month: June
  page_first: '641'
  page_last: '651'
  pages: "641\u2013651"
  paper_id: '59'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1059.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1059.jpg
  title: The Web as a Knowledge-Base for Answering Complex Questions
  title_html: The Web as a Knowledge-Base for Answering Complex Questions
  url: https://www.aclweb.org/anthology/N18-1059
  year: '2018'
N18-1060:
  abstract: We introduce MeSys, a meaning-based approach, for solving English math
    word problems (MWPs) via understanding and reasoning in this paper. It first analyzes
    the text, transforms both body and question parts into their corresponding logic
    forms, and then performs inference on them. The associated context of each quantity
    is represented with proposed role-tags (e.g., nsubj, verb, etc.), which provides
    the flexibility for annotating an extracted math quantity with its associated
    context information (i.e., the physical meaning of this quantity). Statistical
    models are proposed to select the operator and operands. A noisy dataset is designed
    to assess if a solver solves MWPs mainly via understanding or mechanical pattern
    matching. Experimental results show that our approach outperforms existing systems
    on both benchmark datasets and the noisy dataset, which demonstrates that the
    proposed approach understands the meaning of each quantity in the text more.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276431486
    type: video
    url: http://vimeo.com/276431486
  author:
  - first: Chao-Chun
    full: Chao-Chun Liang
    id: chao-chun-liang
    last: Liang
  - first: Yu-Shiang
    full: Yu-Shiang Wong
    id: yu-shiang-wong
    last: Wong
  - first: Yi-Chung
    full: Yi-Chung Lin
    id: yi-chung-lin
    last: Lin
  - first: Keh-Yih
    full: Keh-Yih Su
    id: keh-yih-su
    last: Su
  author_string: Chao-Chun Liang, Yu-Shiang Wong, Yi-Chung Lin, Keh-Yih Su
  bibkey: liang-etal-2018-meaning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1060
  month: June
  page_first: '652'
  page_last: '662'
  pages: "652\u2013662"
  paper_id: '60'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1060.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1060.jpg
  title: A Meaning-Based Statistical English Math Word Problem Solver
  title_html: A Meaning-Based Statistical <span class="acl-fixed-case">E</span>nglish
    Math Word Problem Solver
  url: https://www.aclweb.org/anthology/N18-1060
  year: '2018'
N18-1061:
  abstract: "Temporal orientation refers to an individual\u2019s tendency to connect\
    \ to the psychological concepts of past, present or future, and it affects personality,\
    \ motivation, emotion, decision making and stress coping processes. The study\
    \ of the social media users\u2019 psycho-demographic attributes from the perspective\
    \ of human temporal orientation can be of utmost interest and importance to the\
    \ business and administrative decision makers as it can provide an extra precious\
    \ information for them to make informed decisions. In this paper, we propose a\
    \ very first study to demonstrate the association between the sentiment view of\
    \ the temporal orientation of the users and their different psycho-demographic\
    \ attributes by analyzing their tweets. We first create a temporal orientation\
    \ classifier in a minimally supervised way which classifies each tweet of the\
    \ users in one of the three temporal categories, namely past, present, and future.\
    \ A deep Bi-directional Long Short Term Memory (BLSTM) is used for the tweet classification\
    \ task. Our tweet classifier achieves an accuracy of 78.27% when tested on a manually\
    \ created test set. We then determine the users\u2019 overall temporal orientation\
    \ based on their tweets on the social media. The sentiment is added to the tweets\
    \ at the fine-grained level where each temporal tweet is given a sentiment with\
    \ either of the positive, negative or neutral. Our experiment reveals that depending\
    \ upon the sentiment view of temporal orientation, a user\u2019s attributes vary.\
    \ We finally measure the correlation between the users\u2019 sentiment view of\
    \ temporal orientation and their different psycho-demographic factors using regression."
  address: New Orleans, Louisiana
  author:
  - first: Sabyasachi
    full: Sabyasachi Kamila
    id: sabyasachi-kamila
    last: Kamila
  - first: Mohammed
    full: Mohammed Hasanuzzaman
    id: mohammed-hasanuzzaman
    last: Hasanuzzaman
  - first: Asif
    full: Asif Ekbal
    id: asif-ekbal
    last: Ekbal
  - first: Pushpak
    full: Pushpak Bhattacharyya
    id: pushpak-bhattacharyya
    last: Bhattacharyya
  - first: Andy
    full: Andy Way
    id: andy-way
    last: Way
  author_string: Sabyasachi Kamila, Mohammed Hasanuzzaman, Asif Ekbal, Pushpak Bhattacharyya,
    Andy Way
  bibkey: kamila-etal-2018-fine
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1061
  month: June
  page_first: '663'
  page_last: '674'
  pages: "663\u2013674"
  paper_id: '61'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1061.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1061.jpg
  title: Fine-Grained Temporal Orientation and its Relationship with Psycho-Demographic
    Correlates
  title_html: Fine-Grained Temporal Orientation and its Relationship with Psycho-Demographic
    Correlates
  url: https://www.aclweb.org/anthology/N18-1061
  year: '2018'
N18-1062:
  abstract: Word embeddings obtained from neural network models such as Word2Vec Skipgram
    have become popular representations of word meaning and have been evaluated on
    a variety of word similarity and relatedness norming data. Skipgram generates
    a set of word and context embeddings, the latter typically discarded after training.
    We demonstrate the usefulness of context embeddings in predicting asymmetric association
    between words from a recently published dataset of production norms (Jouravlev
    & McRae, 2016). Our findings suggest that humans respond with words closer to
    the cue within the context embedding space (rather than the word embedding space),
    when asked to generate thematically related words.
  address: New Orleans, Louisiana
  author:
  - first: Fatemeh
    full: Fatemeh Torabi Asr
    id: fatemeh-torabi-asr
    last: Torabi Asr
  - first: Robert
    full: Robert Zinkov
    id: robert-zinkov
    last: Zinkov
  - first: Michael
    full: Michael Jones
    id: michael-jones
    last: Jones
  author_string: Fatemeh Torabi Asr, Robert Zinkov, Michael Jones
  bibkey: torabi-asr-etal-2018-querying
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1062
  month: June
  page_first: '675'
  page_last: '684'
  pages: "675\u2013684"
  paper_id: '62'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1062.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1062.jpg
  title: Querying Word Embeddings for Similarity and Relatedness
  title_html: Querying Word Embeddings for Similarity and Relatedness
  url: https://www.aclweb.org/anthology/N18-1062
  year: '2018'
N18-1063:
  abstract: "Current measures for evaluating text simplification systems focus on\
    \ evaluating lexical text aspects, neglecting its structural aspects. In this\
    \ paper we propose the first measure to address structural aspects of text simplification,\
    \ called SAMSA. It leverages recent advances in semantic parsing to assess simplification\
    \ quality by decomposing the input based on its semantic structure and comparing\
    \ it to the output. SAMSA provides a reference-less automatic evaluation procedure,\
    \ avoiding the problems that reference-based methods face due to the vast space\
    \ of valid simplifications for a given sentence. Our human evaluation experiments\
    \ show both SAMSA\u2019s substantial correlation with human judgments, as well\
    \ as the deficiency of existing reference-based measures in evaluating structural\
    \ simplification."
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/282318359
    type: video
    url: http://vimeo.com/282318359
  author:
  - first: Elior
    full: Elior Sulem
    id: elior-sulem
    last: Sulem
  - first: Omri
    full: Omri Abend
    id: omri-abend
    last: Abend
  - first: Ari
    full: Ari Rappoport
    id: ari-rappoport
    last: Rappoport
  author_string: Elior Sulem, Omri Abend, Ari Rappoport
  bibkey: sulem-etal-2018-semantic
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1063
  month: June
  page_first: '685'
  page_last: '696'
  pages: "685\u2013696"
  paper_id: '63'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1063.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1063.jpg
  title: Semantic Structural Evaluation for Text Simplification
  title_html: Semantic Structural Evaluation for Text Simplification
  url: https://www.aclweb.org/anthology/N18-1063
  year: '2018'
N18-1064:
  abstract: "A major proportion of a text summary includes important entities found\
    \ in the original text. These entities build up the topic of the summary. Moreover,\
    \ they hold commonsense information once they are linked to a knowledge base.\
    \ Based on these observations, this paper investigates the usage of linked entities\
    \ to guide the decoder of a neural text summarizer to generate concise and better\
    \ summaries. To this end, we leverage on an off-the-shelf entity linking system\
    \ (ELS) to extract linked entities and propose Entity2Topic (E2T), a module easily\
    \ attachable to a sequence-to-sequence model that transforms a list of entities\
    \ into a vector representation of the topic of the summary. Current available\
    \ ELS\u2019s are still not sufficiently effective, possibly introducing unresolved\
    \ ambiguities and irrelevant entities. We resolve the imperfections of the ELS\
    \ by (a) encoding entities with selective disambiguation, and (b) pooling entity\
    \ vectors using firm attention. By applying E2T to a simple sequenceto-sequence\
    \ model with attention mechanism as base model, we see significant improvements\
    \ of the performance in the Gigaword (sentence to title) and CNN (long document\
    \ to multi-sentence highlights) summarization datasets by at least 2 ROUGE points."
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/282319318
    type: video
    url: http://vimeo.com/282319318
  author:
  - first: Reinald Kim
    full: Reinald Kim Amplayo
    id: reinald-kim-amplayo
    last: Amplayo
  - first: Seonjae
    full: Seonjae Lim
    id: seonjae-lim
    last: Lim
  - first: Seung-won
    full: Seung-won Hwang
    id: seung-won-hwang
    last: Hwang
  author_string: Reinald Kim Amplayo, Seonjae Lim, Seung-won Hwang
  bibkey: amplayo-etal-2018-entity
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1064
  month: June
  page_first: '697'
  page_last: '707'
  pages: "697\u2013707"
  paper_id: '64'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1064.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1064.jpg
  title: Entity Commonsense Representation for Neural Abstractive Summarization
  title_html: Entity Commonsense Representation for Neural Abstractive Summarization
  url: https://www.aclweb.org/anthology/N18-1064
  year: '2018'
N18-1065:
  abstract: We present NEWSROOM, a summarization dataset of 1.3 million articles and
    summaries written by authors and editors in newsrooms of 38 major news publications.
    Extracted from search and social media metadata between 1998 and 2017, these high-quality
    summaries demonstrate high diversity of summarization styles. In particular, the
    summaries combine abstractive and extractive strategies, borrowing words and phrases
    from articles at varying rates. We analyze the extraction strategies used in NEWSROOM
    summaries against other datasets to quantify the diversity and difficulty of our
    new data, and train existing methods on the data to evaluate its utility and challenges.
    The dataset is available online at summari.es.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/282321393
    type: video
    url: http://vimeo.com/282321393
  author:
  - first: Max
    full: Max Grusky
    id: max-grusky
    last: Grusky
  - first: Mor
    full: Mor Naaman
    id: mor-naaman
    last: Naaman
  - first: Yoav
    full: Yoav Artzi
    id: yoav-artzi
    last: Artzi
  author_string: Max Grusky, Mor Naaman, Yoav Artzi
  bibkey: grusky-etal-2018-newsroom
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1065
  month: June
  page_first: '708'
  page_last: '719'
  pages: "708\u2013719"
  paper_id: '65'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1065.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1065.jpg
  title: 'Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies'
  title_html: '<span class="acl-fixed-case">N</span>ewsroom: A Dataset of 1.3 Million
    Summaries with Diverse Extractive Strategies'
  url: https://www.aclweb.org/anthology/N18-1065
  year: '2018'
N18-1066:
  abstract: Traditional approaches to semantic parsing (SP) work by training individual
    models for each available parallel dataset of text-meaning pairs. In this paper,
    we explore the idea of polyglot semantic translation, or learning semantic parsing
    models that are trained on multiple datasets and natural languages. In particular,
    we focus on translating text to code signature representations using the software
    component datasets of Richardson and Kuhn (2017b,a). The advantage of such models
    is that they can be used for parsing a wide variety of input natural languages
    and output programming languages, or mixed input languages, using a single unified
    model. To facilitate modeling of this type, we develop a novel graph-based decoding
    framework that achieves state-of-the-art performance on the above datasets, and
    apply this method to two other benchmark SP tasks.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1066.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1066.Notes.pdf
  - filename: http://vimeo.com/276898099
    type: video
    url: http://vimeo.com/276898099
  author:
  - first: Kyle
    full: Kyle Richardson
    id: kyle-richardson
    last: Richardson
  - first: Jonathan
    full: Jonathan Berant
    id: jonathan-berant
    last: Berant
  - first: Jonas
    full: Jonas Kuhn
    id: jonas-kuhn
    last: Kuhn
  author_string: Kyle Richardson, Jonathan Berant, Jonas Kuhn
  bibkey: richardson-etal-2018-polyglot
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1066
  month: June
  page_first: '720'
  page_last: '730'
  pages: "720\u2013730"
  paper_id: '66'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1066.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1066.jpg
  title: Polyglot Semantic Parsing in APIs
  title_html: Polyglot Semantic Parsing in <span class="acl-fixed-case">API</span>s
  url: https://www.aclweb.org/anthology/N18-1066
  year: '2018'
N18-1067:
  abstract: 'We present two neural models for event factuality prediction, which yield
    significant performance gains over previous models on three event factuality datasets:
    FactBank, UW, and MEANTIME. We also present a substantial expansion of the It
    Happened portion of the Universal Decompositional Semantics dataset, yielding
    the largest event factuality dataset to date. We report model results on this
    extended factuality dataset as well.'
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276898126
    type: video
    url: http://vimeo.com/276898126
  author:
  - first: Rachel
    full: Rachel Rudinger
    id: rachel-rudinger
    last: Rudinger
  - first: Aaron Steven
    full: Aaron Steven White
    id: aaron-steven-white
    last: White
  - first: Benjamin
    full: Benjamin Van Durme
    id: benjamin-van-durme
    last: Van Durme
  author_string: Rachel Rudinger, Aaron Steven White, Benjamin Van Durme
  bibkey: rudinger-etal-2018-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1067
  month: June
  page_first: '731'
  page_last: '744'
  pages: "731\u2013744"
  paper_id: '67'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1067.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1067.jpg
  title: Neural Models of Factuality
  title_html: Neural Models of Factuality
  url: https://www.aclweb.org/anthology/N18-1067
  year: '2018'
N18-1068:
  abstract: Previous representation learning techniques for knowledge graph representation
    usually represent the same entity or relation in different triples with the same
    representation, without considering the ambiguity of relations and entities. To
    appropriately handle the semantic variety of entities/relations in distinct triples,
    we propose an accurate text-enhanced knowledge graph representation learning method,
    which can represent a relation/entity with different representations in different
    triples by exploiting additional textual information. Specifically, our method
    enhances representations by exploiting the entity descriptions and triple-specific
    relation mention. And a mutual attention mechanism between relation mention and
    entity description is proposed to learn more accurate textual representations
    for further improving knowledge graph representation. Experimental results show
    that our method achieves the state-of-the-art performance on both link prediction
    and triple classification tasks, and significantly outperforms previous text-enhanced
    knowledge representation models.
  address: New Orleans, Louisiana
  author:
  - first: Bo
    full: Bo An
    id: bo-an
    last: An
  - first: Bo
    full: Bo Chen
    id: bo-chen
    last: Chen
  - first: Xianpei
    full: Xianpei Han
    id: xianpei-han
    last: Han
  - first: Le
    full: Le Sun
    id: le-sun
    last: Sun
  author_string: Bo An, Bo Chen, Xianpei Han, Le Sun
  bibkey: an-etal-2018-accurate
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1068
  month: June
  page_first: '745'
  page_last: '755'
  pages: "745\u2013755"
  paper_id: '68'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1068.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1068.jpg
  title: Accurate Text-Enhanced Knowledge Graph Representation Learning
  title_html: Accurate Text-Enhanced Knowledge Graph Representation Learning
  url: https://www.aclweb.org/anthology/N18-1068
  year: '2018'
N18-1069:
  abstract: How to identify, extract, and use phrasal knowledge is a crucial problem
    for the task of Recognizing Textual Entailment (RTE). To solve this problem, we
    propose a method for detecting paraphrases via natural deduction proofs of semantic
    relations between sentence pairs. Our solution relies on a graph reformulation
    of partial variable unifications and an algorithm that induces subgraph alignments
    between meaning representations. Experiments show that our method can automatically
    detect various paraphrases that are absent from existing paraphrase databases.
    In addition, the detection of paraphrases using proof information improves the
    accuracy of RTE tasks.
  address: New Orleans, Louisiana
  author:
  - first: Hitomi
    full: Hitomi Yanaka
    id: hitomi-yanaka
    last: Yanaka
  - first: Koji
    full: Koji Mineshima
    id: koji-mineshima
    last: Mineshima
  - first: Pascual
    full: "Pascual Mart\xEDnez-G\xF3mez"
    id: pascual-martinez-gomez
    last: "Mart\xEDnez-G\xF3mez"
  - first: Daisuke
    full: Daisuke Bekki
    id: daisuke-bekki
    last: Bekki
  author_string: "Hitomi Yanaka, Koji Mineshima, Pascual Mart\xEDnez-G\xF3mez, Daisuke\
    \ Bekki"
  bibkey: yanaka-etal-2018-acquisition
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1069
  month: June
  page_first: '756'
  page_last: '766'
  pages: "756\u2013766"
  paper_id: '69'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1069.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1069.jpg
  title: Acquisition of Phrase Correspondences Using Natural Deduction Proofs
  title_html: Acquisition of Phrase Correspondences Using Natural Deduction Proofs
  url: https://www.aclweb.org/anthology/N18-1069
  year: '2018'
N18-1070:
  abstract: We present an effective end-to-end memory network model that jointly (i)
    predicts whether a given document can be considered as relevant evidence for a
    given claim, and (ii) extracts snippets of evidence that can be used to reason
    about the factuality of the target claim. Our model combines the advantages of
    convolutional and recurrent neural networks as part of a memory network. We further
    introduce a similarity matrix at the inference level of the memory network in
    order to extract snippets of evidence for input claims more accurately. Our experiments
    on a public benchmark dataset, FakeNewsChallenge, demonstrate the effectiveness
    of our approach.
  address: New Orleans, Louisiana
  author:
  - first: Mitra
    full: Mitra Mohtarami
    id: mitra-mohtarami
    last: Mohtarami
  - first: Ramy
    full: Ramy Baly
    id: ramy-baly
    last: Baly
  - first: James
    full: James Glass
    id: james-glass
    last: Glass
  - first: Preslav
    full: Preslav Nakov
    id: preslav-nakov
    last: Nakov
  - first: "Llu\xEDs"
    full: "Llu\xEDs M\xE0rquez"
    id: lluis-marquez
    last: "M\xE0rquez"
  - first: Alessandro
    full: Alessandro Moschitti
    id: alessandro-moschitti
    last: Moschitti
  author_string: "Mitra Mohtarami, Ramy Baly, James Glass, Preslav Nakov, Llu\xED\
    s M\xE0rquez, Alessandro Moschitti"
  bibkey: mohtarami-etal-2018-automatic
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1070
  month: June
  page_first: '767'
  page_last: '776'
  pages: "767\u2013776"
  paper_id: '70'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1070.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1070.jpg
  title: Automatic Stance Detection Using End-to-End Memory Networks
  title_html: Automatic Stance Detection Using End-to-End Memory Networks
  url: https://www.aclweb.org/anthology/N18-1070
  year: '2018'
N18-1071:
  abstract: We present a gradient-tree-boosting-based structured learning model for
    jointly disambiguating named entities in a document. Gradient tree boosting is
    a widely used machine learning algorithm that underlies many top-performing natural
    language processing systems. Surprisingly, most works limit the use of gradient
    tree boosting as a tool for regular classification or regression problems, despite
    the structured nature of language. To the best of our knowledge, our work is the
    first one that employs the structured gradient tree boosting (SGTB) algorithm
    for collective entity disambiguation. By defining global features over previous
    disambiguation decisions and jointly modeling them with local features, our system
    is able to produce globally optimized entity assignments for mentions in a document.
    Exact inference is prohibitively expensive for our globally normalized model.
    To solve this problem, we propose Bidirectional Beam Search with Gold path (BiBSG),
    an approximate inference algorithm that is a variant of the standard beam search
    algorithm. BiBSG makes use of global information from both past and future to
    perform better local search. Experiments on standard benchmark datasets show that
    SGTB significantly improves upon published results. Specifically, SGTB outperforms
    the previous state-of-the-art neural system by near 1% absolute accuracy on the
    popular AIDA-CoNLL dataset.
  address: New Orleans, Louisiana
  author:
  - first: Yi
    full: Yi Yang
    id: yi-yang
    last: Yang
  - first: Ozan
    full: Ozan Irsoy
    id: ozan-irsoy
    last: Irsoy
  - first: Kazi Shefaet
    full: Kazi Shefaet Rahman
    id: kazi-shefaet-rahman
    last: Rahman
  author_string: Yi Yang, Ozan Irsoy, Kazi Shefaet Rahman
  bibkey: yang-etal-2018-collective
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1071
  month: June
  page_first: '777'
  page_last: '786'
  pages: "777\u2013786"
  paper_id: '71'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1071.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1071.jpg
  title: Collective Entity Disambiguation with Structured Gradient Tree Boosting
  title_html: Collective Entity Disambiguation with Structured Gradient Tree Boosting
  url: https://www.aclweb.org/anthology/N18-1071
  year: '2018'
N18-1072:
  abstract: "Ontologies compartmentalize types and relations in a target domain and\
    \ provide the semantic backbone needed for a plethora of practical applications.\
    \ Very often different ontologies are developed independently for the same domain.\
    \ Such \u201Cparallel\u201D ontologies raise the need for a process that will\
    \ establish alignments between their entities in order to unify and extend the\
    \ existing knowledge. In this work, we present a novel entity alignment method\
    \ which we dub DeepAlignment. DeepAlignment refines pre-trained word vectors aiming\
    \ at deriving ontological entity descriptions which are tailored to the ontology\
    \ matching task. The absence of explicit information relevant to the ontology\
    \ matching task during the refinement process makes DeepAlignment completely unsupervised.\
    \ We empirically evaluate our method using standard ontology matching benchmarks.\
    \ We present significant performance improvements over the current state-of-the-art,\
    \ demonstrating the advantages that representation learning techniques bring to\
    \ ontology matching."
  address: New Orleans, Louisiana
  author:
  - first: Prodromos
    full: Prodromos Kolyvakis
    id: prodromos-kolyvakis
    last: Kolyvakis
  - first: Alexandros
    full: Alexandros Kalousis
    id: alexandros-kalousis
    last: Kalousis
  - first: Dimitris
    full: Dimitris Kiritsis
    id: dimitris-kiritsis
    last: Kiritsis
  author_string: Prodromos Kolyvakis, Alexandros Kalousis, Dimitris Kiritsis
  bibkey: kolyvakis-etal-2018-deepalignment
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1072
  month: June
  page_first: '787'
  page_last: '798'
  pages: "787\u2013798"
  paper_id: '72'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1072.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1072.jpg
  title: 'DeepAlignment: Unsupervised Ontology Matching with Refined Word Vectors'
  title_html: '<span class="acl-fixed-case">D</span>eep<span class="acl-fixed-case">A</span>lignment:
    Unsupervised Ontology Matching with Refined Word Vectors'
  url: https://www.aclweb.org/anthology/N18-1072
  year: '2018'
N18-1073:
  abstract: Recurrent neural networks have achieved state-of-the-art results in many
    artificial intelligence tasks, such as language modeling, neural machine translation,
    speech recognition and so on. One of the key factors to these successes is big
    models. However, training such big models usually takes days or even weeks of
    time even if using tens of GPU cards. In this paper, we propose an efficient architecture
    to improve the efficiency of such RNN model training, which adopts the group strategy
    for recurrent layers, while exploiting the representation rearrangement strategy
    between layers as well as time steps. To demonstrate the advantages of our models,
    we conduct experiments on several datasets and tasks. The results show that our
    architecture achieves comparable or better accuracy comparing with baselines,
    with a much smaller number of parameters and at a much lower computational cost.
  address: New Orleans, Louisiana
  author:
  - first: Fei
    full: Fei Gao
    id: fei-gao
    last: Gao
  - first: Lijun
    full: Lijun Wu
    id: lijun-wu
    last: Wu
  - first: Li
    full: Li Zhao
    id: li-zhao
    last: Zhao
  - first: Tao
    full: Tao Qin
    id: tao-qin
    last: Qin
  - first: Xueqi
    full: Xueqi Cheng
    id: xueqi-cheng
    last: Cheng
  - first: Tie-Yan
    full: Tie-Yan Liu
    id: tie-yan-liu
    last: Liu
  author_string: Fei Gao, Lijun Wu, Li Zhao, Tao Qin, Xueqi Cheng, Tie-Yan Liu
  bibkey: gao-etal-2018-efficient
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1073
  month: June
  page_first: '799'
  page_last: '808'
  pages: "799\u2013808"
  paper_id: '73'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1073.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1073.jpg
  title: Efficient Sequence Learning with Group Recurrent Networks
  title_html: Efficient Sequence Learning with Group Recurrent Networks
  url: https://www.aclweb.org/anthology/N18-1073
  year: '2018'
N18-1074:
  abstract: 'In this paper we introduce a new publicly available dataset for verification
    against textual sources, FEVER: Fact Extraction and VERification. It consists
    of 185,445 claims generated by altering sentences extracted from Wikipedia and
    subsequently verified without knowledge of the sentence they were derived from.
    The claims are classified as Supported, Refuted or NotEnoughInfo by annotators
    achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also
    recorded the sentence(s) forming the necessary evidence for their judgment. To
    characterize the challenge of the dataset presented, we develop a pipeline approach
    and compare it to suitably designed oracles. The best accuracy we achieve on labeling
    a claim accompanied by the correct evidence is 31.87%, while if we ignore the
    evidence we achieve 50.91%. Thus we believe that FEVER is a challenging testbed
    that will help stimulate progress on claim verification against textual sources.'
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1074.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1074.Notes.pdf
  author:
  - first: James
    full: James Thorne
    id: james-thorne
    last: Thorne
  - first: Andreas
    full: Andreas Vlachos
    id: andreas-vlachos
    last: Vlachos
  - first: Christos
    full: Christos Christodoulopoulos
    id: christos-christodoulopoulos
    last: Christodoulopoulos
  - first: Arpit
    full: Arpit Mittal
    id: arpit-mittal
    last: Mittal
  author_string: James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit
    Mittal
  bibkey: thorne-etal-2018-fever
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1074
  month: June
  page_first: '809'
  page_last: '819'
  pages: "809\u2013819"
  paper_id: '74'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1074.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1074.jpg
  title: 'FEVER: a Large-scale Dataset for Fact Extraction and VERification'
  title_html: '<span class="acl-fixed-case">FEVER</span>: a Large-scale Dataset for
    Fact Extraction and <span class="acl-fixed-case">VER</span>ification'
  url: https://www.aclweb.org/anthology/N18-1074
  year: '2018'
N18-1075:
  abstract: We study the problem of textual relation embedding with distant supervision.
    To combat the wrong labeling problem of distant supervision, we propose to embed
    textual relations with global statistics of relations, i.e., the co-occurrence
    statistics of textual and knowledge base relations collected from the entire corpus.
    This approach turns out to be more robust to the training noise introduced by
    distant supervision. On a popular relation extraction dataset, we show that the
    learned textual relation embedding can be used to augment existing relation extraction
    models and significantly improve their performance. Most remarkably, for the top
    1,000 relational facts discovered by the best existing model, the precision can
    be improved from 83.9% to 89.3%.
  address: New Orleans, Louisiana
  author:
  - first: Yu
    full: Yu Su
    id: yu-su
    last: Su
  - first: Honglei
    full: Honglei Liu
    id: honglei-liu
    last: Liu
  - first: Semih
    full: Semih Yavuz
    id: semih-yavuz
    last: Yavuz
  - first: Izzeddin
    full: "Izzeddin G\xFCr"
    id: izzeddin-gur
    last: "G\xFCr"
  - first: Huan
    full: Huan Sun
    id: huan-sun
    last: Sun
  - first: Xifeng
    full: Xifeng Yan
    id: xifeng-yan
    last: Yan
  author_string: "Yu Su, Honglei Liu, Semih Yavuz, Izzeddin G\xFCr, Huan Sun, Xifeng\
    \ Yan"
  bibkey: su-etal-2018-global
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1075
  month: June
  page_first: '820'
  page_last: '830'
  pages: "820\u2013830"
  paper_id: '75'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1075.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1075.jpg
  title: Global Relation Embedding for Relation Extraction
  title_html: Global Relation Embedding for Relation Extraction
  url: https://www.aclweb.org/anthology/N18-1075
  year: '2018'
N18-1076:
  abstract: Implicit arguments are not syntactically connected to their predicates,
    and are therefore hard to extract. Previous work has used models with large numbers
    of features, evaluated on very small datasets. We propose to train models for
    implicit argument prediction on a simple cloze task, for which data can be generated
    automatically at scale. This allows us to use a neural model, which draws on narrative
    coherence and entity salience for predictions. We show that our model has superior
    performance on both synthetic and natural data.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1076.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1076.Notes.pdf
  author:
  - first: Pengxiang
    full: Pengxiang Cheng
    id: pengxiang-cheng
    last: Cheng
  - first: Katrin
    full: Katrin Erk
    id: katrin-erk
    last: Erk
  author_string: Pengxiang Cheng, Katrin Erk
  bibkey: cheng-erk-2018-implicit
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1076
  month: June
  page_first: '831'
  page_last: '840'
  pages: "831\u2013840"
  paper_id: '76'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1076.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1076.jpg
  title: Implicit Argument Prediction with Event Knowledge
  title_html: Implicit Argument Prediction with Event Knowledge
  url: https://www.aclweb.org/anthology/N18-1076
  year: '2018'
N18-1077:
  abstract: "Extracting temporal relations (before, after, overlapping, etc.) is a\
    \ key aspect of understanding events described in natural language. We argue that\
    \ this task would gain from the availability of a resource that provides prior\
    \ knowledge in the form of the temporal order that events usually follow. This\
    \ paper develops such a resource \u2013 a probabilistic knowledge base acquired\
    \ in the news domain \u2013 by extracting temporal relations between events from\
    \ the New York Times (NYT) articles over a 20-year span (1987\u20132007). We show\
    \ that existing temporal extraction systems can be improved via this resource.\
    \ As a byproduct, we also show that interesting statistics can be retrieved from\
    \ this resource, which can potentially benefit other time-aware tasks. The proposed\
    \ system and resource are both publicly available."
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1077.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1077.Notes.pdf
  author:
  - first: Qiang
    full: Qiang Ning
    id: qiang-ning
    last: Ning
  - first: Hao
    full: Hao Wu
    id: hao-wu
    last: Wu
  - first: Haoruo
    full: Haoruo Peng
    id: haoruo-peng
    last: Peng
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Qiang Ning, Hao Wu, Haoruo Peng, Dan Roth
  bibkey: ning-etal-2018-improving
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1077
  month: June
  page_first: '841'
  page_last: '851'
  pages: "841\u2013851"
  paper_id: '77'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1077.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1077.jpg
  title: Improving Temporal Relation Extraction with a Globally Acquired Statistical
    Resource
  title_html: Improving Temporal Relation Extraction with a Globally Acquired Statistical
    Resource
  url: https://www.aclweb.org/anthology/N18-1077
  year: '2018'
N18-1078:
  abstract: We introduce a new task called Multimodal Named Entity Recognition (MNER)
    for noisy user-generated data such as tweets or Snapchat captions, which comprise
    short text with accompanying images. These social media posts often come in inconsistent
    or incomplete syntax and lexical notations with very limited surrounding textual
    contexts, bringing significant challenges for NER. To this end, we create a new
    dataset for MNER called SnapCaptions (Snapchat image-caption pairs submitted to
    public and crowd-sourced stories with fully annotated named entities). We then
    build upon the state-of-the-art Bi-LSTM word/character based NER models with 1)
    a deep image network which incorporates relevant visual context to augment textual
    information, and 2) a generic modality-attention module which learns to attenuate
    irrelevant modalities while amplifying the most informative ones to extract contexts
    from, adaptive to each sample and token. The proposed MNER model with modality
    attention significantly outperforms the state-of-the-art text-only NER models
    by successfully leveraging provided visual contexts, opening up potential applications
    of MNER on myriads of social media platforms.
  address: New Orleans, Louisiana
  author:
  - first: Seungwhan
    full: Seungwhan Moon
    id: seungwhan-moon
    last: Moon
  - first: Leonardo
    full: Leonardo Neves
    id: leonardo-neves
    last: Neves
  - first: Vitor
    full: Vitor Carvalho
    id: vitor-carvalho
    last: Carvalho
  author_string: Seungwhan Moon, Leonardo Neves, Vitor Carvalho
  bibkey: moon-etal-2018-multimodal
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1078
  month: June
  page_first: '852'
  page_last: '860'
  pages: "852\u2013860"
  paper_id: '78'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1078.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1078.jpg
  title: Multimodal Named Entity Recognition for Short Social Media Posts
  title_html: Multimodal Named Entity Recognition for Short Social Media Posts
  url: https://www.aclweb.org/anthology/N18-1078
  year: '2018'
N18-1079:
  abstract: 'We propose a novel recurrent neural network-based approach to simultaneously
    handle nested named entity recognition and nested entity mention detection. The
    model learns a hypergraph representation for nested entities using features extracted
    from a recurrent neural network. In evaluations on three standard data sets, we
    show that our approach significantly outperforms existing state-of-the-art methods,
    which are feature-based. The approach is also efficient: it operates linearly
    in the number of tokens and the number of possible output labels at any token.
    Finally, we present an extension of our model that jointly learns the head of
    each entity mention.'
  address: New Orleans, Louisiana
  author:
  - first: Arzoo
    full: Arzoo Katiyar
    id: arzoo-katiyar
    last: Katiyar
  - first: Claire
    full: Claire Cardie
    id: claire-cardie
    last: Cardie
  author_string: Arzoo Katiyar, Claire Cardie
  bibkey: katiyar-cardie-2018-nested
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1079
  month: June
  page_first: '861'
  page_last: '871'
  pages: "861\u2013871"
  paper_id: '79'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1079.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1079.jpg
  title: Nested Named Entity Recognition Revisited
  title_html: Nested Named Entity Recognition Revisited
  url: https://www.aclweb.org/anthology/N18-1079
  year: '2018'
N18-1080:
  abstract: Most work in relation extraction forms a prediction by looking at a short
    span of text within a single sentence containing a single entity pair mention.
    This approach often does not consider interactions across mentions, requires redundant
    computation for each mention pair, and ignores relationships expressed across
    sentence boundaries. These problems are exacerbated by the document- (rather than
    sentence-) level annotation common in biological text. In response, we propose
    a model which simultaneously predicts relationships between all mention pairs
    in a document. We form pairwise predictions over entire paper abstracts using
    an efficient self-attention encoder. All-pairs mention scores allow us to perform
    multi-instance learning by aggregating over mentions to form entity pair representations.
    We further adapt to settings without mention-level annotation by jointly training
    to predict named entities and adding a corpus of weakly labeled data. In experiments
    on two Biocreative benchmark datasets, we achieve state of the art performance
    on the Biocreative V Chemical Disease Relation dataset for models without external
    KB resources. We also introduce a new dataset an order of magnitude larger than
    existing human-annotated biological information extraction datasets and more accurate
    than distantly supervised alternatives.
  address: New Orleans, Louisiana
  author:
  - first: Patrick
    full: Patrick Verga
    id: patrick-verga
    last: Verga
  - first: Emma
    full: Emma Strubell
    id: emma-strubell
    last: Strubell
  - first: Andrew
    full: Andrew McCallum
    id: andrew-mccallum
    last: McCallum
  author_string: Patrick Verga, Emma Strubell, Andrew McCallum
  bibkey: verga-etal-2018-simultaneously
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1080
  month: June
  page_first: '872'
  page_last: '884'
  pages: "872\u2013884"
  paper_id: '80'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1080.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1080.jpg
  title: Simultaneously Self-Attending to All Mentions for Full-Abstract Biological
    Relation Extraction
  title_html: Simultaneously Self-Attending to All Mentions for Full-Abstract Biological
    Relation Extraction
  url: https://www.aclweb.org/anthology/N18-1080
  year: '2018'
N18-1081:
  abstract: We present data and methods that enable a supervised learning approach
    to Open Information Extraction (Open IE). Central to the approach is a novel formulation
    of Open IE as a sequence tagging problem, addressing challenges such as encoding
    multiple extractions for a predicate. We also develop a bi-LSTM transducer, extending
    recent deep Semantic Role Labeling models to extract Open IE tuples and provide
    confidence scores for tuning their precision-recall tradeoff. Furthermore, we
    show that the recently released Question-Answer Meaning Representation dataset
    can be automatically converted into an Open IE corpus which significantly increases
    the amount of available training data. Our supervised model outperforms the existing
    state-of-the-art Open IE systems on benchmark datasets.
  address: New Orleans, Louisiana
  author:
  - first: Gabriel
    full: Gabriel Stanovsky
    id: gabriel-stanovsky
    last: Stanovsky
  - first: Julian
    full: Julian Michael
    id: julian-michael
    last: Michael
  - first: Luke
    full: Luke Zettlemoyer
    id: luke-zettlemoyer
    last: Zettlemoyer
  - first: Ido
    full: Ido Dagan
    id: ido-dagan
    last: Dagan
  author_string: Gabriel Stanovsky, Julian Michael, Luke Zettlemoyer, Ido Dagan
  bibkey: stanovsky-etal-2018-supervised
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1081
  month: June
  page_first: '885'
  page_last: '895'
  pages: "885\u2013895"
  paper_id: '81'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1081.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1081.jpg
  title: Supervised Open Information Extraction
  title_html: Supervised Open Information Extraction
  url: https://www.aclweb.org/anthology/N18-1081
  year: '2018'
N18-1082:
  abstract: "Prepositions are among the most frequent words in English and play complex\
    \ roles in the syntax and semantics of sentences. Not surprisingly, they pose\
    \ well-known difficulties in automatic processing of sentences (prepositional\
    \ attachment ambiguities and idiosyncratic uses in phrases). Existing methods\
    \ on preposition representation treat prepositions no different from content words\
    \ (e.g., word2vec and GloVe). In addition, recent studies aiming at solving prepositional\
    \ attachment and preposition selection problems depend heavily on external linguistic\
    \ resources and use dataset-specific word representations. In this paper we use\
    \ word-triple counts (one of the triples being a preposition) to capture a preposition\u2019\
    s interaction with its attachment and complement. We then derive preposition embeddings\
    \ via tensor decomposition on a large unlabeled corpus. We reveal a new geometry\
    \ involving Hadamard products and empirically demonstrate its utility in paraphrasing\
    \ phrasal verbs. Furthermore, our preposition embeddings are used as simple features\
    \ in two challenging downstream tasks: preposition selection and prepositional\
    \ attachment disambiguation. We achieve results comparable to or better than the\
    \ state-of-the-art on multiple standardized datasets."
  address: New Orleans, Louisiana
  author:
  - first: Hongyu
    full: Hongyu Gong
    id: hongyu-gong
    last: Gong
  - first: Suma
    full: Suma Bhat
    id: suma-bhat
    last: Bhat
  - first: Pramod
    full: Pramod Viswanath
    id: pramod-viswanath
    last: Viswanath
  author_string: Hongyu Gong, Suma Bhat, Pramod Viswanath
  bibkey: gong-etal-2018-embedding
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1082
  month: June
  page_first: '896'
  page_last: '906'
  pages: "896\u2013906"
  paper_id: '82'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1082.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1082.jpg
  title: Embedding Syntax and Semantics of Prepositions via Tensor Decomposition
  title_html: Embedding Syntax and Semantics of Prepositions via Tensor Decomposition
  url: https://www.aclweb.org/anthology/N18-1082
  year: '2018'
N18-1083:
  abstract: "A core part of linguistic typology is the classification of languages\
    \ according to linguistic properties, such as those detailed in the World Atlas\
    \ of Language Structure (WALS). Doing this manually is prohibitively time-consuming,\
    \ which is in part evidenced by the fact that only 100 out of over 7,000 languages\
    \ spoken in the world are fully covered in WALS. We learn distributed language\
    \ representations, which can be used to predict typological properties on a massively\
    \ multilingual scale. Additionally, quantitative and qualitative analyses of these\
    \ language embeddings can tell us how language similarities are encoded in NLP\
    \ models for tasks at different typological levels. The representations are learned\
    \ in an unsupervised manner alongside tasks at three typological levels: phonology\
    \ (grapheme-to-phoneme prediction, and phoneme reconstruction), morphology (morphological\
    \ inflection), and syntax (part-of-speech tagging). We consider more than 800\
    \ languages and find significant differences in the language representations encoded,\
    \ depending on the target task. For instance, although Norwegian Bokm\xE5l and\
    \ Danish are typologically close to one another, they are phonologically distant,\
    \ which is reflected in their language embeddings growing relatively distant in\
    \ a phonological task. We are also able to predict typological features in WALS\
    \ with high accuracies, even for unseen language families."
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1083.Software.zip
    type: software
    url: https://www.aclweb.org/anthology/attachments/N18-1083.Software.zip
  author:
  - first: Johannes
    full: Johannes Bjerva
    id: johannes-bjerva
    last: Bjerva
  - first: Isabelle
    full: Isabelle Augenstein
    id: isabelle-augenstein
    last: Augenstein
  author_string: Johannes Bjerva, Isabelle Augenstein
  bibkey: bjerva-augenstein-2018-phonology
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1083
  month: June
  page_first: '907'
  page_last: '916'
  pages: "907\u2013916"
  paper_id: '83'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1083.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1083.jpg
  title: 'From Phonology to Syntax: Unsupervised Linguistic Typology at Different
    Levels with Language Embeddings'
  title_html: 'From Phonology to Syntax: Unsupervised Linguistic Typology at Different
    Levels with Language Embeddings'
  url: https://www.aclweb.org/anthology/N18-1083
  year: '2018'
N18-1084:
  abstract: 'Dependency parsing research, which has made significant gains in recent
    years, typically focuses on improving the accuracy of single-tree predictions.
    However, ambiguity is inherent to natural language syntax, and communicating such
    ambiguity is important for error analysis and better-informed downstream applications.
    In this work, we propose a transition sampling algorithm to sample from the full
    joint distribution of parse trees defined by a transition-based parsing model,
    and demonstrate the use of the samples in probabilistic dependency analysis. First,
    we define the new task of dependency path prediction, inferring syntactic substructures
    over part of a sentence, and provide the first analysis of performance on this
    task. Second, we demonstrate the usefulness of our Monte Carlo syntax marginal
    method for parser error analysis and calibration. Finally, we use this method
    to propagate parse uncertainty to two downstream information extraction applications:
    identifying persons killed by police and semantic role assignment.'
  address: New Orleans, Louisiana
  author:
  - first: Katherine
    full: Katherine Keith
    id: katherine-keith
    last: Keith
  - first: Su Lin
    full: Su Lin Blodgett
    id: su-lin-blodgett
    last: Blodgett
  - first: Brendan
    full: "Brendan O\u2019Connor"
    id: brendan-oconnor
    last: "O\u2019Connor"
  author_string: "Katherine Keith, Su Lin Blodgett, Brendan O\u2019Connor"
  bibkey: keith-etal-2018-monte
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1084
  month: June
  page_first: '917'
  page_last: '928'
  pages: "917\u2013928"
  paper_id: '84'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1084.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1084.jpg
  title: Monte Carlo Syntax Marginals for Exploring and Using Dependency Parses
  title_html: Monte <span class="acl-fixed-case">C</span>arlo Syntax Marginals for
    Exploring and Using Dependency Parses
  url: https://www.aclweb.org/anthology/N18-1084
  year: '2018'
N18-1085:
  abstract: We introduce neural particle smoothing, a sequential Monte Carlo method
    for sampling annotations of an input string from a given probability model. In
    contrast to conventional particle filtering algorithms, we train a proposal distribution
    that looks ahead to the end of the input string by means of a right-to-left LSTM.
    We demonstrate that this innovation can improve the quality of the sample. To
    motivate our formal choices, we explain how neural transduction models and our
    sampler can be viewed as low-dimensional but nonlinear approximations to working
    with HMMs over very large state spaces.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1085.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1085.Notes.pdf
  author:
  - first: Chu-Cheng
    full: Chu-Cheng Lin
    id: chu-cheng-lin
    last: Lin
  - first: Jason
    full: Jason Eisner
    id: jason-eisner
    last: Eisner
  author_string: Chu-Cheng Lin, Jason Eisner
  bibkey: lin-eisner-2018-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1085
  month: June
  page_first: '929'
  page_last: '941'
  pages: "929\u2013941"
  paper_id: '85'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1085.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1085.jpg
  title: Neural Particle Smoothing for Sampling from Conditional Sequence Models
  title_html: Neural Particle Smoothing for Sampling from Conditional Sequence Models
  url: https://www.aclweb.org/anthology/N18-1085
  year: '2018'
N18-1086:
  abstract: We present neural syntactic generative models with exact marginalization
    that support both dependency parsing and language modeling. Exact marginalization
    is made tractable through dynamic programming over shift-reduce parsing and minimal
    RNN-based feature sets. Our algorithms complement previous approaches by supporting
    batched training and enabling online computation of next word probabilities. For
    supervised dependency parsing, our model achieves a state-of-the-art result among
    generative approaches. We also report empirical results on unsupervised syntactic
    models and their role in language modeling. We find that our model formulation
    of latent dependencies with exact marginalization do not lead to better intrinsic
    language modeling performance than vanilla RNNs, and that parsing accuracy is
    not correlated with language modeling perplexity in stack-based models.
  address: New Orleans, Louisiana
  author:
  - first: Jan
    full: Jan Buys
    id: jan-buys
    last: Buys
  - first: Phil
    full: Phil Blunsom
    id: phil-blunsom
    last: Blunsom
  author_string: Jan Buys, Phil Blunsom
  bibkey: buys-blunsom-2018-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1086
  month: June
  page_first: '942'
  page_last: '952'
  pages: "942\u2013952"
  paper_id: '86'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1086.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1086.jpg
  title: Neural Syntactic Generative Models with Exact Marginalization
  title_html: Neural Syntactic Generative Models with Exact Marginalization
  url: https://www.aclweb.org/anthology/N18-1086
  year: '2018'
N18-1087:
  abstract: User-generated text tends to be noisy with many lexical and orthographic
    inconsistencies, making natural language processing (NLP) tasks more challenging.
    The challenging nature of noisy text processing is exacerbated for dialectal content,
    where in addition to spelling and lexical differences, dialectal text is characterized
    with morpho-syntactic and phonetic variations. These issues increase sparsity
    in NLP models and reduce accuracy. We present a neural morphological tagging and
    disambiguation model for Egyptian Arabic, with various extensions to handle noisy
    and inconsistent content. Our models achieve about 5% relative error reduction
    (1.1% absolute improvement) for full morphological analysis, and around 22% relative
    error reduction (1.8% absolute improvement) for part-of-speech tagging, over a
    state-of-the-art baseline.
  address: New Orleans, Louisiana
  author:
  - first: Nasser
    full: Nasser Zalmout
    id: nasser-zalmout
    last: Zalmout
  - first: Alexander
    full: Alexander Erdmann
    id: alexander-erdmann
    last: Erdmann
  - first: Nizar
    full: Nizar Habash
    id: nizar-habash
    last: Habash
  author_string: Nasser Zalmout, Alexander Erdmann, Nizar Habash
  bibkey: zalmout-etal-2018-noise
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1087
  month: June
  page_first: '953'
  page_last: '964'
  pages: "953\u2013964"
  paper_id: '87'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1087.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1087.jpg
  title: Noise-Robust Morphological Disambiguation for Dialectal Arabic
  title_html: Noise-Robust Morphological Disambiguation for Dialectal <span class="acl-fixed-case">A</span>rabic
  url: https://www.aclweb.org/anthology/N18-1087
  year: '2018'
N18-1088:
  abstract: We study the problem of analyzing tweets with universal dependencies (UD).
    We extend the UD guidelines to cover special constructions in tweets that affect
    tokenization, part-of-speech tagging, and labeled dependencies. Using the extended
    guidelines, we create a new tweet treebank for English (Tweebank v2) that is four
    times larger than the (unlabeled) Tweebank v1 introduced by Kong et al. (2014).
    We characterize the disagreements between our annotators and show that it is challenging
    to deliver consistent annotation due to ambiguity in understanding and explaining
    tweets. Nonetheless, using the new treebank, we build a pipeline system to parse
    raw tweets into UD. To overcome the annotation noise without sacrificing computational
    efficiency, we propose a new method to distill an ensemble of 20 transition-based
    parsers into a single one. Our parser achieves an improvement of 2.2 in LAS over
    the un-ensembled baseline and outperforms parsers that are state-of-the-art on
    other treebanks in both accuracy and speed.
  address: New Orleans, Louisiana
  author:
  - first: Yijia
    full: Yijia Liu
    id: yijia-liu
    last: Liu
  - first: Yi
    full: Yi Zhu
    id: yi-zhu
    last: Zhu
  - first: Wanxiang
    full: Wanxiang Che
    id: wanxiang-che
    last: Che
  - first: Bing
    full: Bing Qin
    id: bing-qin
    last: Qin
  - first: Nathan
    full: Nathan Schneider
    id: nathan-schneider
    last: Schneider
  - first: Noah A.
    full: Noah A. Smith
    id: noah-a-smith
    last: Smith
  author_string: Yijia Liu, Yi Zhu, Wanxiang Che, Bing Qin, Nathan Schneider, Noah
    A. Smith
  bibkey: liu-etal-2018-parsing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1088
  month: June
  page_first: '965'
  page_last: '975'
  pages: "965\u2013975"
  paper_id: '88'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1088.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1088.jpg
  title: Parsing Tweets into Universal Dependencies
  title_html: Parsing Tweets into Universal Dependencies
  url: https://www.aclweb.org/anthology/N18-1088
  year: '2018'
N18-1089:
  abstract: Adversarial training (AT) is a powerful regularization method for neural
    networks, aiming to achieve robustness to input perturbations. Yet, the specific
    effects of the robustness obtained from AT are still unclear in the context of
    natural language processing. In this paper, we propose and analyze a neural POS
    tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus
    and the Universal Dependencies (UD) dataset (27 languages), we find that AT not
    only improves the overall tagging accuracy, but also 1) prevents over-fitting
    well in low resource languages and 2) boosts tagging accuracy for rare / unseen
    words. We also demonstrate that 3) the improved tagging performance by AT contributes
    to the downstream task of dependency parsing, and that 4) AT helps the model to
    learn cleaner word representations. 5) The proposed AT model is generally effective
    in different sequence labeling tasks. These positive results motivate further
    use of AT for natural language tasks.
  address: New Orleans, Louisiana
  author:
  - first: Michihiro
    full: Michihiro Yasunaga
    id: michihiro-yasunaga
    last: Yasunaga
  - first: Jungo
    full: Jungo Kasai
    id: jungo-kasai
    last: Kasai
  - first: Dragomir
    full: Dragomir Radev
    id: dragomir-radev
    last: Radev
  author_string: Michihiro Yasunaga, Jungo Kasai, Dragomir Radev
  bibkey: yasunaga-etal-2018-robust
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1089
  month: June
  page_first: '976'
  page_last: '986'
  pages: "976\u2013986"
  paper_id: '89'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1089.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1089.jpg
  title: Robust Multilingual Part-of-Speech Tagging via Adversarial Training
  title_html: Robust Multilingual Part-of-Speech Tagging via Adversarial Training
  url: https://www.aclweb.org/anthology/N18-1089
  year: '2018'
N18-1090:
  abstract: Code-switching is a phenomenon of mixing grammatical structures of two
    or more languages under varied social constraints. The code-switching data differ
    so radically from the benchmark corpora used in NLP community that the application
    of standard technologies to these data degrades their performance sharply. Unlike
    standard corpora, these data often need to go through additional processes such
    as language identification, normalization and/or back-transliteration for their
    efficient processing. In this paper, we investigate these indispensable processes
    and other problems associated with syntactic parsing of code-switching data and
    propose methods to mitigate their effects. In particular, we study dependency
    parsing of code-switching data of Hindi and English multilingual speakers from
    Twitter. We present a treebank of Hindi-English code-switching tweets under Universal
    Dependencies scheme and propose a neural stacking model for parsing that efficiently
    leverages the part-of-speech tag and syntactic tree annotations in the code-switching
    treebank and the preexisting Hindi and English treebanks. We also present normalization
    and back-transliteration models with a decoding process tailored for code-switching
    data. Results show that our neural stacking parser is 1.5% LAS points better than
    the augmented parsing model and 3.8% LAS points better than the one which uses
    first-best normalization and/or back-transliteration.
  address: New Orleans, Louisiana
  author:
  - first: Irshad
    full: Irshad Bhat
    id: irshad-bhat
    last: Bhat
  - first: Riyaz A.
    full: Riyaz A. Bhat
    id: riyaz-ahmad-bhat
    last: Bhat
  - first: Manish
    full: Manish Shrivastava
    id: manish-shrivastava
    last: Shrivastava
  - first: Dipti
    full: Dipti Sharma
    id: dipti-misra-sharma
    last: Sharma
  author_string: Irshad Bhat, Riyaz A. Bhat, Manish Shrivastava, Dipti Sharma
  bibkey: bhat-etal-2018-universal
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1090
  month: June
  page_first: '987'
  page_last: '998'
  pages: "987\u2013998"
  paper_id: '90'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1090.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1090.jpg
  title: Universal Dependency Parsing for Hindi-English Code-Switching
  title_html: Universal Dependency Parsing for <span class="acl-fixed-case">H</span>indi-<span
    class="acl-fixed-case">E</span>nglish Code-Switching
  url: https://www.aclweb.org/anthology/N18-1090
  year: '2018'
N18-1091:
  abstract: A number of differences have emerged between modern and classic approaches
    to constituency parsing in recent years, with structural components like grammars
    and feature-rich lexicons becoming less central while recurrent neural network
    representations rise in popularity. The goal of this work is to analyze the extent
    to which information provided directly by the model structure in classical systems
    is still being captured by neural methods. To this end, we propose a high-performance
    neural model (92.08 F1 on PTB) that is representative of recent work and perform
    a series of investigative experiments. We find that our model implicitly learns
    to encode much of the same information that was explicitly provided by grammars
    and lexicons in the past, indicating that this scaffolding can largely be subsumed
    by powerful general-purpose neural machinery.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1091.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1091.Notes.pdf
  author:
  - first: David
    full: David Gaddy
    id: david-gaddy
    last: Gaddy
  - first: Mitchell
    full: Mitchell Stern
    id: mitchell-stern
    last: Stern
  - first: Dan
    full: Dan Klein
    id: dan-klein
    last: Klein
  author_string: David Gaddy, Mitchell Stern, Dan Klein
  bibkey: gaddy-etal-2018-whats
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1091
  month: June
  page_first: '999'
  page_last: '1010'
  pages: "999\u20131010"
  paper_id: '91'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1091.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1091.jpg
  title: "What\u2019s Going On in Neural Constituency Parsers? An Analysis"
  title_html: "What\u2019s Going On in Neural Constituency Parsers? An Analysis"
  url: https://www.aclweb.org/anthology/N18-1091
  year: '2018'
N18-1092:
  abstract: "This work exploits translation data as a source of semantically relevant\
    \ learning signal for models of word representation. In particular, we exploit\
    \ equivalence through translation as a form of distributional context and jointly\
    \ learn how to embed and align with a deep generative model. Our EmbedAlign model\
    \ embeds words in their complete observed context and learns by marginalisation\
    \ of latent lexical alignments. Besides, it embeds words as posterior probability\
    \ densities, rather than point estimates, which allows us to compare words in\
    \ context using a measure of overlap between distributions (e.g. KL divergence).\
    \ We investigate our model\u2019s performance on a range of lexical semantics\
    \ tasks achieving competitive results on several standard benchmarks including\
    \ natural language inference, paraphrasing, and text similarity."
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/277669962
    type: video
    url: http://vimeo.com/277669962
  author:
  - first: Miguel
    full: Miguel Rios
    id: miguel-rios
    last: Rios
  - first: Wilker
    full: Wilker Aziz
    id: wilker-aziz
    last: Aziz
  - first: Khalil
    full: "Khalil Sima\u2019an"
    id: khalil-simaan
    last: "Sima\u2019an"
  author_string: "Miguel Rios, Wilker Aziz, Khalil Sima\u2019an"
  bibkey: rios-etal-2018-deep
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1092
  month: June
  page_first: '1011'
  page_last: '1023'
  pages: "1011\u20131023"
  paper_id: '92'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1092.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1092.jpg
  title: Deep Generative Model for Joint Alignment and Word Representation
  title_html: Deep Generative Model for Joint Alignment and Word Representation
  url: https://www.aclweb.org/anthology/N18-1092
  year: '2018'
N18-1093:
  abstract: Word embedding is a key component in many downstream applications in processing
    natural languages. Existing approaches often assume the existence of a large collection
    of text for learning effective word embedding. However, such a corpus may not
    be available for some low-resource languages. In this paper, we study how to effectively
    learn a word embedding model on a corpus with only a few million tokens. In such
    a situation, the co-occurrence matrix is sparse as the co-occurrences of many
    word pairs are unobserved. In contrast to existing approaches often only sample
    a few unobserved word pairs as negative samples, we argue that the zero entries
    in the co-occurrence matrix also provide valuable information. We then design
    a Positive-Unlabeled Learning (PU-Learning) approach to factorize the co-occurrence
    matrix and validate the proposed approaches in four different languages.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1093.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1093.Notes.pdf
  - filename: http://vimeo.com/277670013
    type: video
    url: http://vimeo.com/277670013
  author:
  - first: Chao
    full: Chao Jiang
    id: chao-jiang
    last: Jiang
  - first: Hsiang-Fu
    full: Hsiang-Fu Yu
    id: hsiang-fu-yu
    last: Yu
  - first: Cho-Jui
    full: Cho-Jui Hsieh
    id: cho-jui-hsieh
    last: Hsieh
  - first: Kai-Wei
    full: Kai-Wei Chang
    id: kai-wei-chang
    last: Chang
  author_string: Chao Jiang, Hsiang-Fu Yu, Cho-Jui Hsieh, Kai-Wei Chang
  bibkey: jiang-etal-2018-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1093
  month: June
  page_first: '1024'
  page_last: '1034'
  pages: "1024\u20131034"
  paper_id: '93'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1093.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1093.jpg
  title: Learning Word Embeddings for Low-Resource Languages by PU Learning
  title_html: Learning Word Embeddings for Low-Resource Languages by <span class="acl-fixed-case">PU</span>
    Learning
  url: https://www.aclweb.org/anthology/N18-1093
  year: '2018'
N18-1094:
  abstract: "Public debate forums provide a common platform for exchanging opinions\
    \ on a topic of interest. While recent studies in natural language processing\
    \ (NLP) have provided empirical evidence that the language of the debaters and\
    \ their patterns of interaction play a key role in changing the mind of a reader,\
    \ research in psychology has shown that prior beliefs can affect our interpretation\
    \ of an argument and could therefore constitute a competing alternative explanation\
    \ for resistance to changing one\u2019s stance. To study the actual effect of\
    \ language use vs. prior beliefs on persuasion, we provide a new dataset and propose\
    \ a controlled setting that takes into consideration two reader-level factors:\
    \ political and religious ideology. We find that prior beliefs affected by these\
    \ reader-level factors play a more important role than language use effects and\
    \ argue that it is important to account for them in NLP studies of persuasion."
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/282323369
    type: video
    url: http://vimeo.com/282323369
  author:
  - first: Esin
    full: Esin Durmus
    id: esin-durmus
    last: Durmus
  - first: Claire
    full: Claire Cardie
    id: claire-cardie
    last: Cardie
  author_string: Esin Durmus, Claire Cardie
  bibkey: durmus-cardie-2018-exploring
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1094
  month: June
  page_first: '1035'
  page_last: '1045'
  pages: "1035\u20131045"
  paper_id: '94'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1094.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1094.jpg
  title: Exploring the Role of Prior Beliefs for Argument Persuasion
  title_html: Exploring the Role of Prior Beliefs for Argument Persuasion
  url: https://www.aclweb.org/anthology/N18-1094
  year: '2018'
N18-1095:
  abstract: We address the detection of abusive words. The task is to identify such
    words among a set of negative polar expressions. We propose novel features employing
    information from both corpora and lexical resources. These features are calibrated
    on a small manually annotated base lexicon which we use to produce a large lexicon.
    We show that the word-level information we learn cannot be equally derived from
    a large dataset of annotated microposts. We demonstrate the effectiveness of our
    (domain-independent) lexicon in the cross-domain detection of abusive microposts.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/282325809
    type: video
    url: http://vimeo.com/282325809
  author:
  - first: Michael
    full: Michael Wiegand
    id: michael-wiegand
    last: Wiegand
  - first: Josef
    full: Josef Ruppenhofer
    id: josef-ruppenhofer
    last: Ruppenhofer
  - first: Anna
    full: Anna Schmidt
    id: anna-schmidt
    last: Schmidt
  - first: Clayton
    full: Clayton Greenberg
    id: clayton-greenberg
    last: Greenberg
  author_string: Michael Wiegand, Josef Ruppenhofer, Anna Schmidt, Clayton Greenberg
  bibkey: wiegand-etal-2018-inducing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1095
  month: June
  page_first: '1046'
  page_last: '1056'
  pages: "1046\u20131056"
  paper_id: '95'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1095.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1095.jpg
  title: "Inducing a Lexicon of Abusive Words \u2013 a Feature-Based Approach"
  title_html: "Inducing a Lexicon of Abusive Words \u2013 a Feature-Based Approach"
  url: https://www.aclweb.org/anthology/N18-1095
  year: '2018'
N18-1096:
  abstract: "Understanding how social power structures affect the way we interact\
    \ with one another is of great interest to social scientists who want to answer\
    \ fundamental questions about human behavior, as well as to computer scientists\
    \ who want to build automatic methods to infer the social contexts of interactions.\
    \ In this paper, we employ advancements in extra-propositional semantics extraction\
    \ within NLP to study how author commitment reflects the social context of an\
    \ interactions. Specifically, we investigate whether the level of commitment expressed\
    \ by individuals in an organizational interaction reflects the hierarchical power\
    \ structures they are part of. We find that subordinates use significantly more\
    \ instances of non-commitment than superiors. More importantly, we also find that\
    \ subordinates attribute propositions to other agents more often than superiors\
    \ do \u2014 an aspect that has not been studied before. Finally, we show that\
    \ enriching lexical features with commitment labels captures important distinctions\
    \ in social meanings."
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/282327794
    type: video
    url: http://vimeo.com/282327794
  author:
  - first: Vinodkumar
    full: Vinodkumar Prabhakaran
    id: vinodkumar-prabhakaran
    last: Prabhakaran
  - first: Premkumar
    full: Premkumar Ganeshkumar
    id: premkumar-ganeshkumar
    last: Ganeshkumar
  - first: Owen
    full: Owen Rambow
    id: owen-rambow
    last: Rambow
  author_string: Vinodkumar Prabhakaran, Premkumar Ganeshkumar, Owen Rambow
  bibkey: prabhakaran-etal-2018-author
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1096
  month: June
  page_first: '1057'
  page_last: '1068'
  pages: "1057\u20131068"
  paper_id: '96'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1096.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1096.jpg
  title: 'Author Commitment and Social Power: Automatic Belief Tagging to Infer the
    Social Context of Interactions'
  title_html: 'Author Commitment and Social Power: Automatic Belief Tagging to Infer
    the Social Context of Interactions'
  url: https://www.aclweb.org/anthology/N18-1096
  year: '2018'
N18-1097:
  abstract: Text classification models are becoming increasingly complex and opaque,
    however for many applications it is essential that the models are interpretable.
    Recently, a variety of approaches have been proposed for generating local explanations.
    While robust evaluations are needed to drive further progress, so far it is unclear
    which evaluation approaches are suitable. This paper is a first step towards more
    robust evaluations of local explanations. We evaluate a variety of local explanation
    approaches using automatic measures based on word deletion. Furthermore, we show
    that an evaluation using a crowdsourcing experiment correlates moderately with
    these automatic measures and that a variety of other factors also impact the human
    judgements.
  address: New Orleans, Louisiana
  author:
  - first: Dong
    full: Dong Nguyen
    id: dong-nguyen
    last: Nguyen
  author_string: Dong Nguyen
  bibkey: nguyen-2018-comparing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1097
  month: June
  page_first: '1069'
  page_last: '1078'
  pages: "1069\u20131078"
  paper_id: '97'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1097.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1097.jpg
  title: Comparing Automatic and Human Evaluation of Local Explanations for Text Classification
  title_html: Comparing Automatic and Human Evaluation of Local Explanations for Text
    Classification
  url: https://www.aclweb.org/anthology/N18-1097
  year: '2018'
N18-1098:
  abstract: Dynamic topic modeling facilitates the identification of topical trends
    over time in temporal collections of unstructured documents. We introduce a novel
    unsupervised neural dynamic topic model named as Recurrent Neural Network-Replicated
    Softmax Model (RNNRSM), where the discovered topics at each time influence the
    topic discovery in the subsequent time steps. We account for the temporal ordering
    of documents by explicitly modeling a joint distribution of latent topical dependencies
    over time, using distributional estimators with temporal recurrent connections.
    Applying RNN-RSM to 19 years of articles on NLP research, we demonstrate that
    compared to state-of-the art topic models, RNNRSM shows better generalization,
    topic interpretation, evolution and trends. We also introduce a metric (named
    as SPAN) to quantify the capability of dynamic topic model to capture word evolution
    in topics over time.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/277669869
    type: video
    url: http://vimeo.com/277669869
  author:
  - first: Pankaj
    full: Pankaj Gupta
    id: pankaj-gupta
    last: Gupta
  - first: Subburam
    full: Subburam Rajaram
    id: subburam-rajaram
    last: Rajaram
  - first: Hinrich
    full: "Hinrich Sch\xFCtze"
    id: hinrich-schutze
    last: "Sch\xFCtze"
  - first: Bernt
    full: Bernt Andrassy
    id: bernt-andrassy
    last: Andrassy
  author_string: "Pankaj Gupta, Subburam Rajaram, Hinrich Sch\xFCtze, Bernt Andrassy"
  bibkey: gupta-etal-2018-deep
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1098
  month: June
  page_first: '1079'
  page_last: '1089'
  pages: "1079\u20131089"
  paper_id: '98'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1098.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1098.jpg
  title: Deep Temporal-Recurrent-Replicated-Softmax for Topical Trends over Time
  title_html: Deep Temporal-Recurrent-Replicated-Softmax for Topical Trends over Time
  url: https://www.aclweb.org/anthology/N18-1098
  year: '2018'
N18-1099:
  abstract: Multilingual topic models enable document analysis across languages through
    coherent multilingual summaries of the data. However, there is no standard and
    effective metric to evaluate the quality of multilingual topics. We introduce
    a new intrinsic evaluation of multilingual topic models that correlates well with
    human judgments of multilingual topic coherence as well as performance in downstream
    applications. Importantly, we also study evaluation for low-resource languages.
    Because standard metrics fail to accurately measure topic quality when robust
    external resources are unavailable, we propose an adaptation model that improves
    the accuracy and reliability of these metrics in low-resource settings.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/277669906
    type: video
    url: http://vimeo.com/277669906
  author:
  - first: Shudong
    full: Shudong Hao
    id: shudong-hao
    last: Hao
  - first: Jordan
    full: Jordan Boyd-Graber
    id: jordan-boyd-graber
    last: Boyd-Graber
  - first: Michael J.
    full: Michael J. Paul
    id: michael-paul
    last: Paul
  author_string: Shudong Hao, Jordan Boyd-Graber, Michael J. Paul
  bibkey: hao-etal-2018-lessons
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1099
  month: June
  page_first: '1090'
  page_last: '1100'
  pages: "1090\u20131100"
  paper_id: '99'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1099.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1099.jpg
  title: 'Lessons from the Bible on Modern Topics: Low-Resource Multilingual Topic
    Model Evaluation'
  title_html: 'Lessons from the <span class="acl-fixed-case">B</span>ible on Modern
    Topics: Low-Resource Multilingual Topic Model Evaluation'
  url: https://www.aclweb.org/anthology/N18-1099
  year: '2018'
N18-1100:
  abstract: Clinical notes are text documents that are created by clinicians for each
    patient encounter. They are typically accompanied by medical codes, which describe
    the diagnosis and treatment. Annotating these codes is labor intensive and error
    prone; furthermore, the connection between the codes and the text is not annotated,
    obscuring the reasons and details behind specific diagnoses and treatments. We
    present an attentional convolutional network that predicts medical codes from
    clinical text. Our method aggregates information across the document using a convolutional
    neural network, and uses an attention mechanism to select the most relevant segments
    for each of the thousands of possible codes. The method is accurate, achieving
    precision@8 of 0.71 and a Micro-F1 of 0.54, which are both better than the prior
    state of the art. Furthermore, through an interpretability evaluation by a physician,
    we show that the attention mechanism identifies meaningful explanations for each
    code assignment.
  address: New Orleans, Louisiana
  author:
  - first: James
    full: James Mullenbach
    id: james-mullenbach
    last: Mullenbach
  - first: Sarah
    full: Sarah Wiegreffe
    id: sarah-wiegreffe
    last: Wiegreffe
  - first: Jon
    full: Jon Duke
    id: jon-duke
    last: Duke
  - first: Jimeng
    full: Jimeng Sun
    id: jimeng-sun
    last: Sun
  - first: Jacob
    full: Jacob Eisenstein
    id: jacob-eisenstein
    last: Eisenstein
  author_string: James Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng Sun, Jacob Eisenstein
  bibkey: mullenbach-etal-2018-explainable
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1100
  month: June
  page_first: '1101'
  page_last: '1111'
  pages: "1101\u20131111"
  paper_id: '100'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1100.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1100.jpg
  title: Explainable Prediction of Medical Codes from Clinical Text
  title_html: Explainable Prediction of Medical Codes from Clinical Text
  url: https://www.aclweb.org/anthology/N18-1100
  year: '2018'
N18-1101:
  abstract: This paper introduces the Multi-Genre Natural Language Inference (MultiNLI)
    corpus, a dataset designed for use in the development and evaluation of machine
    learning models for sentence understanding. At 433k examples, this resource is
    one of the largest corpora available for natural language inference (a.k.a. recognizing
    textual entailment), improving upon available resources in both its coverage and
    difficulty. MultiNLI accomplishes this by offering data from ten distinct genres
    of written and spoken English, making it possible to evaluate systems on nearly
    the full complexity of the language, while supplying an explicit setting for evaluating
    cross-genre domain adaptation. In addition, an evaluation using existing machine
    learning models designed for the Stanford NLI corpus shows that it represents
    a substantially more difficult task than does that corpus, despite the two showing
    similar levels of inter-annotator agreement.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/282330248
    type: video
    url: http://vimeo.com/282330248
  author:
  - first: Adina
    full: Adina Williams
    id: adina-williams
    last: Williams
  - first: Nikita
    full: Nikita Nangia
    id: nikita-nangia
    last: Nangia
  - first: Samuel
    full: Samuel Bowman
    id: samuel-bowman
    last: Bowman
  author_string: Adina Williams, Nikita Nangia, Samuel Bowman
  bibkey: williams-etal-2018-broad
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1101
  month: June
  page_first: '1112'
  page_last: '1122'
  pages: "1112\u20131122"
  paper_id: '101'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1101.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1101.jpg
  title: A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference
  title_html: A Broad-Coverage Challenge Corpus for Sentence Understanding through
    Inference
  url: https://www.aclweb.org/anthology/N18-1101
  year: '2018'
N18-1102:
  abstract: "Recognizing lexical semantic relations between word pairs is an important\
    \ task for many applications of natural language processing. One of the mainstream\
    \ approaches to this task is to exploit the lexico-syntactic paths connecting\
    \ two target words, which reflect the semantic relations of word pairs. However,\
    \ this method requires that the considered words co-occur in a sentence. This\
    \ requirement is hardly satisfied because of Zipf\u2019s law, which states that\
    \ most content words occur very rarely. In this paper, we propose novel methods\
    \ with a neural model of P(path|w1,w2) to solve this problem. Our proposed model\
    \ of P (path|w1, w2 ) can be learned in an unsupervised manner and can generalize\
    \ the co-occurrences of word pairs and dependency paths. This model can be used\
    \ to augment the path data of word pairs that do not co-occur in the corpus, and\
    \ extract features capturing relational information from word pairs. Our experimental\
    \ results demonstrate that our methods improve on previous neural approaches based\
    \ on dependency paths and successfully solve the focused problem."
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1102.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1102.Notes.pdf
  - filename: http://vimeo.com/282332520
    type: video
    url: http://vimeo.com/282332520
  author:
  - first: Koki
    full: Koki Washio
    id: koki-washio
    last: Washio
  - first: Tsuneaki
    full: Tsuneaki Kato
    id: tsuneaki-kato
    last: Kato
  author_string: Koki Washio, Tsuneaki Kato
  bibkey: washio-kato-2018-filling
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1102
  month: June
  page_first: '1123'
  page_last: '1133'
  pages: "1123\u20131133"
  paper_id: '102'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1102.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1102.jpg
  title: 'Filling Missing Paths: Modeling Co-occurrences of Word Pairs and Dependency
    Paths for Recognizing Lexical Semantic Relations'
  title_html: 'Filling Missing Paths: Modeling Co-occurrences of Word Pairs and Dependency
    Paths for Recognizing Lexical Semantic Relations'
  url: https://www.aclweb.org/anthology/N18-1102
  year: '2018'
N18-1103:
  abstract: We present LEAR (Lexical Entailment Attract-Repel), a novel post-processing
    method that transforms any input word vector space to emphasise the asymmetric
    relation of lexical entailment (LE), also known as the IS-A or hyponymy-hypernymy
    relation. By injecting external linguistic constraints (e.g., WordNet links) into
    the initial vector space, the LE specialisation procedure brings true hyponymy-hypernymy
    pairs closer together in the transformed Euclidean space. The proposed asymmetric
    distance measure adjusts the norms of word vectors to reflect the actual WordNet-style
    hierarchy of concepts. Simultaneously, a joint objective enforces semantic similarity
    using the symmetric cosine distance, yielding a vector space specialised for both
    lexical relations at once. LEAR specialisation achieves state-of-the-art performance
    in the tasks of hypernymy directionality, hypernymy detection, and graded lexical
    entailment, demonstrating the effectiveness and robustness of the proposed asymmetric
    specialisation model.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/282333968
    type: video
    url: http://vimeo.com/282333968
  author:
  - first: Ivan
    full: "Ivan Vuli\u0107"
    id: ivan-vulic
    last: "Vuli\u0107"
  - first: Nikola
    full: "Nikola Mrk\u0161i\u0107"
    id: nikola-mrksic
    last: "Mrk\u0161i\u0107"
  author_string: "Ivan Vuli\u0107, Nikola Mrk\u0161i\u0107"
  bibkey: vulic-mrksic-2018-specialising
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1103
  month: June
  page_first: '1134'
  page_last: '1145'
  pages: "1134\u20131145"
  paper_id: '103'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1103.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1103.jpg
  title: Specialising Word Vectors for Lexical Entailment
  title_html: Specialising Word Vectors for Lexical Entailment
  url: https://www.aclweb.org/anthology/N18-1103
  year: '2018'
N18-1104:
  abstract: Abstract Meaning Representation (AMR) research has mostly focused on English.
    We show that it is possible to use AMR annotations for English as a semantic representation
    for sentences written in other languages. We exploit an AMR parser for English
    and parallel corpora to learn AMR parsers for Italian, Spanish, German and Chinese.
    Qualitative analysis show that the new parsers overcome structural differences
    between the languages. We further propose a method to evaluate the parsers that
    does not require gold standard data in the target languages. This method highly
    correlates with the gold standard evaluation, obtaining a Pearson correlation
    coefficient of 0.95.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/282336638
    type: video
    url: http://vimeo.com/282336638
  author:
  - first: Marco
    full: Marco Damonte
    id: marco-damonte
    last: Damonte
  - first: Shay B.
    full: Shay B. Cohen
    id: shay-b-cohen
    last: Cohen
  author_string: Marco Damonte, Shay B. Cohen
  bibkey: damonte-cohen-2018-cross
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1104
  month: June
  page_first: '1146'
  page_last: '1155'
  pages: "1146\u20131155"
  paper_id: '104'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1104.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1104.jpg
  title: Cross-Lingual Abstract Meaning Representation Parsing
  title_html: Cross-Lingual Abstract Meaning Representation Parsing
  url: https://www.aclweb.org/anthology/N18-1104
  year: '2018'
N18-1105:
  abstract: Sentences with gapping, such as Paul likes coffee and Mary tea, lack an
    overt predicate to indicate the relation between two or more arguments. Surface
    syntax representations of such sentences are often produced poorly by parsers,
    and even if correct, not well suited to downstream natural language understanding
    tasks such as relation extraction that are typically designed to extract information
    from sentences with canonical clause structure. In this paper, we present two
    methods for parsing to a Universal Dependencies graph representation that explicitly
    encodes the elided material with additional nodes and edges. We find that both
    methods can reconstruct elided material from dependency trees with high accuracy
    when the parser correctly predicts the existence of a gap. We further demonstrate
    that one of our methods can be applied to other languages based on a case study
    on Swedish.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276898174
    type: video
    url: http://vimeo.com/276898174
  author:
  - first: Sebastian
    full: Sebastian Schuster
    id: sebastian-schuster
    last: Schuster
  - first: Joakim
    full: Joakim Nivre
    id: joakim-nivre
    last: Nivre
  - first: Christopher D.
    full: Christopher D. Manning
    id: christopher-d-manning
    last: Manning
  author_string: Sebastian Schuster, Joakim Nivre, Christopher D. Manning
  bibkey: schuster-etal-2018-sentences
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1105
  month: June
  page_first: '1156'
  page_last: '1168'
  pages: "1156\u20131168"
  paper_id: '105'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1105.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1105.jpg
  title: 'Sentences with Gapping: Parsing and Reconstructing Elided Predicates'
  title_html: 'Sentences with Gapping: Parsing and Reconstructing Elided Predicates'
  url: https://www.aclweb.org/anthology/N18-1105
  year: '2018'
N18-1106:
  abstract: Abstract Meaning Representation (AMR) annotations are often assumed to
    closely mirror dependency syntax, but AMR explicitly does not require this, and
    the assumption has never been tested. To test it, we devise an expressive framework
    to align AMR graphs to dependency graphs, which we use to annotate 200 AMRs. Our
    annotation explains how 97% of AMR edges are evoked by words or syntax. Previously
    existing AMR alignment frameworks did not allow for mapping AMR onto syntax, and
    as a consequence they explained at most 23%. While we find that there are indeed
    many cases where AMR annotations closely mirror syntax, there are also pervasive
    differences. We use our annotations to test a baseline AMR-to-syntax aligner,
    finding that this task is more difficult than AMR-to-string alignment; and to
    pinpoint errors in an AMR parser. We make our data and code freely available for
    further research on AMR parsing and generation, and the relationship of AMR to
    syntax.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276898193
    type: video
    url: http://vimeo.com/276898193
  author:
  - first: Ida
    full: Ida Szubert
    id: ida-szubert
    last: Szubert
  - first: Adam
    full: Adam Lopez
    id: adam-lopez
    last: Lopez
  - first: Nathan
    full: Nathan Schneider
    id: nathan-schneider
    last: Schneider
  author_string: Ida Szubert, Adam Lopez, Nathan Schneider
  bibkey: szubert-etal-2018-structured
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1106
  month: June
  page_first: '1169'
  page_last: '1180'
  pages: "1169\u20131180"
  paper_id: '106'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1106.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1106.jpg
  title: A Structured Syntax-Semantics Interface for English-AMR Alignment
  title_html: A Structured Syntax-Semantics Interface for <span class="acl-fixed-case">E</span>nglish-<span
    class="acl-fixed-case">AMR</span> Alignment
  url: https://www.aclweb.org/anthology/N18-1106
  year: '2018'
N18-1107:
  abstract: We present a graph-based Tree Adjoining Grammar (TAG) parser that uses
    BiLSTMs, highway connections, and character-level CNNs. Our best end-to-end parser,
    which jointly performs supertagging, POS tagging, and parsing, outperforms the
    previously reported best results by more than 2.2 LAS and UAS points. The graph-based
    parsing architecture allows for global inference and rich feature representations
    for TAG parsing, alleviating the fundamental trade-off between transition-based
    and graph-based parsing systems. We also demonstrate that the proposed parser
    achieves state-of-the-art performance in the downstream tasks of Parsing Evaluation
    using Textual Entailments (PETE) and Unbounded Dependency Recovery. This provides
    further support for the claim that TAG is a viable formalism for problems that
    require rich structural analysis of sentences.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276898201
    type: video
    url: http://vimeo.com/276898201
  author:
  - first: Jungo
    full: Jungo Kasai
    id: jungo-kasai
    last: Kasai
  - first: Robert
    full: Robert Frank
    id: robert-frank
    last: Frank
  - first: Pauli
    full: Pauli Xu
    id: pauli-xu
    last: Xu
  - first: William
    full: William Merrill
    id: william-merrill
    last: Merrill
  - first: Owen
    full: Owen Rambow
    id: owen-rambow
    last: Rambow
  author_string: Jungo Kasai, Robert Frank, Pauli Xu, William Merrill, Owen Rambow
  bibkey: kasai-etal-2018-end
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1107
  month: June
  page_first: '1181'
  page_last: '1194'
  pages: "1181\u20131194"
  paper_id: '107'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1107.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1107.jpg
  title: End-to-End Graph-Based TAG Parsing with Neural Networks
  title_html: End-to-End Graph-Based <span class="acl-fixed-case">TAG</span> Parsing
    with Neural Networks
  url: https://www.aclweb.org/anthology/N18-1107
  year: '2018'
N18-1108:
  abstract: "Recurrent neural networks (RNNs) achieved impressive results in a variety\
    \ of linguistic processing tasks, suggesting that they can induce non-trivial\
    \ properties of language. We investigate to what extent RNNs learn to track abstract\
    \ hierarchical syntactic structure. We test whether RNNs trained with a generic\
    \ language modeling objective in four languages (Italian, English, Hebrew, Russian)\
    \ can predict long-distance number agreement in various constructions. We include\
    \ in our evaluation nonsensical sentences where RNNs cannot rely on semantic or\
    \ lexical cues (\u201CThe colorless green ideas I ate with the chair sleep furiously\u201D\
    ), and, for Italian, we compare model performance to human intuitions. Our language-model-trained\
    \ RNNs make reliable predictions about long-distance agreement, and do not lag\
    \ much behind human performance. We thus bring support to the hypothesis that\
    \ RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical\
    \ competence."
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276898233
    type: video
    url: http://vimeo.com/276898233
  author:
  - first: Kristina
    full: Kristina Gulordava
    id: kristina-gulordava
    last: Gulordava
  - first: Piotr
    full: Piotr Bojanowski
    id: piotr-bojanowski
    last: Bojanowski
  - first: Edouard
    full: Edouard Grave
    id: edouard-grave
    last: Grave
  - first: Tal
    full: Tal Linzen
    id: tal-linzen
    last: Linzen
  - first: Marco
    full: Marco Baroni
    id: marco-baroni
    last: Baroni
  author_string: Kristina Gulordava, Piotr Bojanowski, Edouard Grave, Tal Linzen,
    Marco Baroni
  bibkey: gulordava-etal-2018-colorless
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1108
  month: June
  page_first: '1195'
  page_last: '1205'
  pages: "1195\u20131205"
  paper_id: '108'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1108.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1108.jpg
  title: Colorless Green Recurrent Networks Dream Hierarchically
  title_html: Colorless Green Recurrent Networks Dream Hierarchically
  url: https://www.aclweb.org/anthology/N18-1108
  year: '2018'
N18-1109:
  abstract: We study few-shot learning in natural language domains. Compared to many
    existing works that apply either metric-based or optimization-based meta-learning
    to image domain with low inter-task variance, we consider a more realistic setting,
    where tasks are diverse. However, it imposes tremendous difficulties to existing
    state-of-the-art metric-based algorithms since a single metric is insufficient
    to capture complex task variations in natural language domain. To alleviate the
    problem, we propose an adaptive metric learning approach that automatically determines
    the best weighted combination from a set of metrics obtained from meta-training
    tasks for a newly seen few-shot task. Extensive quantitative evaluations on real-world
    sentiment analysis and dialog intent classification datasets demonstrate that
    the proposed method performs favorably against state-of-the-art few shot learning
    algorithms in terms of predictive accuracy. We make our code and data available
    for further study.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1109.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1109.Notes.pdf
  author:
  - first: Mo
    full: Mo Yu
    id: mo-yu
    last: Yu
  - first: Xiaoxiao
    full: Xiaoxiao Guo
    id: xiaoxiao-guo
    last: Guo
  - first: Jinfeng
    full: Jinfeng Yi
    id: jinfeng-yi
    last: Yi
  - first: Shiyu
    full: Shiyu Chang
    id: shiyu-chang
    last: Chang
  - first: Saloni
    full: Saloni Potdar
    id: saloni-potdar
    last: Potdar
  - first: Yu
    full: Yu Cheng
    id: yu-cheng
    last: Cheng
  - first: Gerald
    full: Gerald Tesauro
    id: gerald-tesauro
    last: Tesauro
  - first: Haoyu
    full: Haoyu Wang
    id: haoyu-wang
    last: Wang
  - first: Bowen
    full: Bowen Zhou
    id: bowen-zhou
    last: Zhou
  author_string: Mo Yu, Xiaoxiao Guo, Jinfeng Yi, Shiyu Chang, Saloni Potdar, Yu Cheng,
    Gerald Tesauro, Haoyu Wang, Bowen Zhou
  bibkey: yu-etal-2018-diverse
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1109
  month: June
  page_first: '1206'
  page_last: '1215'
  pages: "1206\u20131215"
  paper_id: '109'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1109.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1109.jpg
  title: Diverse Few-Shot Text Classification with Multiple Metrics
  title_html: Diverse Few-Shot Text Classification with Multiple Metrics
  url: https://www.aclweb.org/anthology/N18-1109
  year: '2018'
N18-1110:
  abstract: "The intensive use of e-communications in everyday life has given rise\
    \ to new threats and risks. When the vulnerable asset is the user, detecting these\
    \ potential attacks before they cause serious damages is extremely important.\
    \ This paper proposes a novel document representation to improve the early detection\
    \ of risks in social media sources. The goal is to effectively identify the potential\
    \ risk using as few text as possible and with as much anticipation as possible.\
    \ Accordingly, we devise a Multi-Resolution Representation (MulR), which allows\
    \ us to generate multiple \u201Cviews\u201D of the analyzed text. These views\
    \ capture different semantic meanings for words and documents at different levels\
    \ of detail, which is very useful in early scenarios to model the variable amounts\
    \ of evidence. Intuitively, the representation captures better the content of\
    \ short documents (very early stages) in low resolutions, whereas large documents\
    \ (medium/large stages) are better modeled with higher resolutions. We evaluate\
    \ the proposed ideas in two different tasks where anticipation is critical: sexual\
    \ predator detection and depression detection. The experimental evaluation for\
    \ these early tasks revealed that the proposed approach outperforms previous methodologies\
    \ by a considerable margin."
  address: New Orleans, Louisiana
  author:
  - first: Adrian Pastor
    full: "Adrian Pastor L\xF3pez-Monroy"
    id: adrian-pastor-lopez-monroy
    last: "L\xF3pez-Monroy"
  - first: Fabio A.
    full: "Fabio A. Gonz\xE1lez"
    id: fabio-a-gonzalez
    last: "Gonz\xE1lez"
  - first: Manuel
    full: Manuel Montes
    id: manuel-montes
    last: Montes
  - first: Hugo Jair
    full: Hugo Jair Escalante
    id: hugo-jair-escalante
    last: Escalante
  - first: Thamar
    full: Thamar Solorio
    id: thamar-solorio
    last: Solorio
  author_string: "Adrian Pastor L\xF3pez-Monroy, Fabio A. Gonz\xE1lez, Manuel Montes,\
    \ Hugo Jair Escalante, Thamar Solorio"
  bibkey: lopez-monroy-etal-2018-early
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1110
  month: June
  page_first: '1216'
  page_last: '1225'
  pages: "1216\u20131225"
  paper_id: '110'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1110.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1110.jpg
  title: Early Text Classification Using Multi-Resolution Concept Representations
  title_html: Early Text Classification Using Multi-Resolution Concept Representations
  url: https://www.aclweb.org/anthology/N18-1110
  year: '2018'
N18-1111:
  abstract: Many text classification tasks are known to be highly domain-dependent.
    Unfortunately, the availability of training data can vary drastically across domains.
    Worse still, for some domains there may not be any annotated data at all. In this
    work, we propose a multinomial adversarial network (MAN) to tackle this real-world
    problem of multi-domain text classification (MDTC) in which labeled data may exist
    for multiple domains, but in insufficient amounts to train effective classifiers
    for one or more of the domains. We provide theoretical justifications for the
    MAN framework, proving that different instances of MANs are essentially minimizers
    of various f-divergence metrics (Ali and Silvey, 1966) among multiple probability
    distributions. MANs are thus a theoretically sound generalization of traditional
    adversarial networks that discriminate over two distributions. More specifically,
    for the MDTC task, MAN learns features that are invariant across multiple domains
    by resorting to its ability to reduce the divergence among the feature distributions
    of each domain. We present experimental results showing that MANs significantly
    outperform the prior art on the MDTC task. We also show that MANs achieve state-of-the-art
    performance for domains with no labeled data.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1111.Software.tgz
    type: software
    url: https://www.aclweb.org/anthology/attachments/N18-1111.Software.tgz
  author:
  - first: Xilun
    full: Xilun Chen
    id: xilun-chen
    last: Chen
  - first: Claire
    full: Claire Cardie
    id: claire-cardie
    last: Cardie
  author_string: Xilun Chen, Claire Cardie
  bibkey: chen-cardie-2018-multinomial
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1111
  month: June
  page_first: '1226'
  page_last: '1240'
  pages: "1226\u20131240"
  paper_id: '111'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1111.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1111.jpg
  title: Multinomial Adversarial Networks for Multi-Domain Text Classification
  title_html: Multinomial Adversarial Networks for Multi-Domain Text Classification
  url: https://www.aclweb.org/anthology/N18-1111
  year: '2018'
N18-1112:
  abstract: Representation learning with pivot-based methods and with Neural Networks
    (NNs) have lead to significant progress in domain adaptation for Natural Language
    Processing. However, most previous work that follows these approaches does not
    explicitly exploit the structure of the input text, and its output is most often
    a single representation vector for the entire text. In this paper we present the
    Pivot Based Language Model (PBLM), a representation learning model that marries
    together pivot-based and NN modeling in a structure aware manner. Particularly,
    our model processes the information in the text with a sequential NN (LSTM) and
    its output consists of a representation vector for every input word. Unlike most
    previous representation learning models in domain adaptation, PBLM can naturally
    feed structure aware text classifiers such as LSTM and CNN. We experiment with
    the task of cross-domain sentiment classification on 20 domain pairs and show
    substantial improvements over strong baselines.
  address: New Orleans, Louisiana
  author:
  - first: Yftah
    full: Yftah Ziser
    id: yftah-ziser
    last: Ziser
  - first: Roi
    full: Roi Reichart
    id: roi-reichart
    last: Reichart
  author_string: Yftah Ziser, Roi Reichart
  bibkey: ziser-reichart-2018-pivot
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1112
  month: June
  page_first: '1241'
  page_last: '1251'
  pages: "1241\u20131251"
  paper_id: '112'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1112.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1112.jpg
  title: Pivot Based Language Modeling for Improved Neural Domain Adaptation
  title_html: Pivot Based Language Modeling for Improved Neural Domain Adaptation
  url: https://www.aclweb.org/anthology/N18-1112
  year: '2018'
N18-1113:
  abstract: Co-training is a popular semi-supervised learning framework to utilize
    a large amount of unlabeled data in addition to a small labeled set. Co-training
    methods exploit predicted labels on the unlabeled data and select samples based
    on prediction confidence to augment the training. However, the selection of samples
    in existing co-training methods is based on a predetermined policy, which ignores
    the sampling bias between the unlabeled and the labeled subsets, and fails to
    explore the data space. In this paper, we propose a novel method, Reinforced Co-Training,
    to select high-quality unlabeled samples to better co-train on. More specifically,
    our approach uses Q-learning to learn a data selection policy with a small labeled
    dataset, and then exploits this policy to train the co-training classifiers automatically.
    Experimental results on clickbait detection and generic text classification tasks
    demonstrate that our proposed method can obtain more accurate text classification
    results.
  address: New Orleans, Louisiana
  author:
  - first: Jiawei
    full: Jiawei Wu
    id: jiawei-wu
    last: Wu
  - first: Lei
    full: Lei Li
    id: lei-li
    last: Li
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  author_string: Jiawei Wu, Lei Li, William Yang Wang
  bibkey: wu-etal-2018-reinforced
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1113
  month: June
  page_first: '1252'
  page_last: '1262'
  pages: "1252\u20131262"
  paper_id: '113'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1113.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1113.jpg
  title: Reinforced Co-Training
  title_html: Reinforced Co-Training
  url: https://www.aclweb.org/anthology/N18-1113
  year: '2018'
N18-1114:
  abstract: "We present a new approach to the design of deep networks for natural\
    \ language processing (NLP), based on the general technique of Tensor Product\
    \ Representations (TPRs) for encoding and processing symbol structures in distributed\
    \ neural networks. A network architecture \u2014 the Tensor Product Generation\
    \ Network (TPGN) \u2014 is proposed which is capable in principle of carrying\
    \ out TPR computation, but which uses unconstrained deep learning to design its\
    \ internal representations. Instantiated in a model for image-caption generation,\
    \ TPGN outperforms LSTM baselines when evaluated on the COCO dataset. The TPR-capable\
    \ structure enables interpretation of internal representations and operations,\
    \ which prove to contain considerable grammatical content. Our caption-generation\
    \ model can be interpreted as generating sequences of grammatical categories and\
    \ retrieving words by their categories from a plan encoded as a distributed representation."
  address: New Orleans, Louisiana
  author:
  - first: Qiuyuan
    full: Qiuyuan Huang
    id: qiuyuan-huang
    last: Huang
  - first: Paul
    full: Paul Smolensky
    id: paul-smolensky
    last: Smolensky
  - first: Xiaodong
    full: Xiaodong He
    id: xiaodong-he
    last: He
  - first: Li
    full: Li Deng
    id: li-deng
    last: Deng
  - first: Dapeng
    full: Dapeng Wu
    id: dapeng-wu
    last: Wu
  author_string: Qiuyuan Huang, Paul Smolensky, Xiaodong He, Li Deng, Dapeng Wu
  bibkey: huang-etal-2018-tensor
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1114
  month: June
  page_first: '1263'
  page_last: '1273'
  pages: "1263\u20131273"
  paper_id: '114'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1114.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1114.jpg
  title: Tensor Product Generation Networks for Deep NLP Modeling
  title_html: Tensor Product Generation Networks for Deep <span class="acl-fixed-case">NLP</span>
    Modeling
  url: https://www.aclweb.org/anthology/N18-1114
  year: '2018'
N18-1115:
  abstract: 'Contextual sequence mapping is one of the fundamental problems in Natural
    Language Processing (NLP). Here, instead of relying solely on the information
    presented in the text, the learning agents have access to a strong external signal
    given to assist the learning process. In this paper, we propose a novel family
    of Recurrent Neural Network unit: the Context-dependent Additive Recurrent Neural
    Network (CARNN) that is designed specifically to address this type of problem.
    The experimental results on public datasets in the dialog problem (Babi dialog
    Task 6 and Frame), contextual language model (Switchboard and Penn Tree Bank)
    and question answering (Trec QA) show that our novel CARNN-based architectures
    outperform previous methods.'
  address: New Orleans, Louisiana
  author:
  - first: Quan Hung
    full: Quan Hung Tran
    id: quan-hung-tran
    last: Tran
  - first: Tuan
    full: Tuan Lai
    id: tuan-lai
    last: Lai
  - first: Gholamreza
    full: Gholamreza Haffari
    id: gholamreza-haffari
    last: Haffari
  - first: Ingrid
    full: Ingrid Zukerman
    id: ingrid-zukerman
    last: Zukerman
  - first: Trung
    full: Trung Bui
    id: trung-bui
    last: Bui
  - first: Hung
    full: Hung Bui
    id: hung-bui
    last: Bui
  author_string: Quan Hung Tran, Tuan Lai, Gholamreza Haffari, Ingrid Zukerman, Trung
    Bui, Hung Bui
  bibkey: tran-etal-2018-context
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1115
  month: June
  page_first: '1274'
  page_last: '1283'
  pages: "1274\u20131283"
  paper_id: '115'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1115.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1115.jpg
  title: The Context-Dependent Additive Recurrent Neural Net
  title_html: The Context-Dependent Additive Recurrent Neural Net
  url: https://www.aclweb.org/anthology/N18-1115
  year: '2018'
N18-1116:
  abstract: Natural language sentences, being hierarchical, can be represented at
    different levels of granularity, like words, subwords, or characters. But most
    neural machine translation systems require the sentence to be represented as a
    sequence at a single level of granularity. It can be difficult to determine which
    granularity is better for a particular translation task. In this paper, we improve
    the model by incorporating multiple levels of granularity. Specifically, we propose
    (1) an encoder with character attention which augments the (sub)word-level representation
    with character-level information; (2) a decoder with multiple attentions that
    enable the representations from different levels of granularity to control the
    translation cooperatively. Experiments on three translation tasks demonstrate
    that our proposed models outperform the standard word-based model, the subword-based
    model, and a strong character-based model.
  address: New Orleans, Louisiana
  author:
  - first: Huadong
    full: Huadong Chen
    id: huadong-chen
    last: Chen
  - first: Shujian
    full: Shujian Huang
    id: shujian-huang
    last: Huang
  - first: David
    full: David Chiang
    id: david-chiang
    last: Chiang
  - first: Xinyu
    full: Xinyu Dai
    id: xinyu-dai
    last: Dai
  - first: Jiajun
    full: Jiajun Chen
    id: jiajun-chen
    last: Chen
  author_string: Huadong Chen, Shujian Huang, David Chiang, Xinyu Dai, Jiajun Chen
  bibkey: chen-etal-2018-combining
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1116
  month: June
  page_first: '1284'
  page_last: '1293'
  pages: "1284\u20131293"
  paper_id: '116'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1116.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1116.jpg
  title: Combining Character and Word Information in Neural Machine Translation Using
    a Multi-Level Attention
  title_html: Combining Character and Word Information in Neural Machine Translation
    Using a Multi-Level Attention
  url: https://www.aclweb.org/anthology/N18-1116
  year: '2018'
N18-1117:
  abstract: Recently, neural machine translation has achieved remarkable progress
    by introducing well-designed deep neural networks into its encoder-decoder framework.
    From the optimization perspective, residual connections are adopted to improve
    learning performance for both encoder and decoder in most of these deep architectures,
    and advanced attention connections are applied as well. Inspired by the success
    of the DenseNet model in computer vision problems, in this paper, we propose a
    densely connected NMT architecture (DenseNMT) that is able to train more efficiently
    for NMT. The proposed DenseNMT not only allows dense connection in creating new
    features for both encoder and decoder, but also uses the dense attention structure
    to improve attention quality. Our experiments on multiple datasets show that DenseNMT
    structure is more competitive and efficient.
  address: New Orleans, Louisiana
  author:
  - first: Yanyao
    full: Yanyao Shen
    id: yanyao-shen
    last: Shen
  - first: Xu
    full: Xu Tan
    id: xu-tan
    last: Tan
  - first: Di
    full: Di He
    id: di-he
    last: He
  - first: Tao
    full: Tao Qin
    id: tao-qin
    last: Qin
  - first: Tie-Yan
    full: Tie-Yan Liu
    id: tie-yan-liu
    last: Liu
  author_string: Yanyao Shen, Xu Tan, Di He, Tao Qin, Tie-Yan Liu
  bibkey: shen-etal-2018-dense
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1117
  month: June
  page_first: '1294'
  page_last: '1303'
  pages: "1294\u20131303"
  paper_id: '117'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1117.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1117.jpg
  title: Dense Information Flow for Neural Machine Translation
  title_html: Dense Information Flow for Neural Machine Translation
  url: https://www.aclweb.org/anthology/N18-1117
  year: '2018'
N18-1118:
  abstract: "For machine translation to tackle discourse phenomena, models must have\
    \ access to extra-sentential linguistic context. There has been recent interest\
    \ in modelling context in neural machine translation (NMT), but models have been\
    \ principally evaluated with standard automatic metrics, poorly adapted to evaluating\
    \ discourse phenomena. In this article, we present hand-crafted, discourse test\
    \ sets, designed to test the models\u2019 ability to exploit previous source and\
    \ target sentences. We investigate the performance of recently proposed multi-encoder\
    \ NMT models trained on subtitles for English to French. We also explore a novel\
    \ way of exploiting context from the previous sentence. Despite gains using BLEU,\
    \ multi-encoder models give limited improvement in the handling of discourse phenomena:\
    \ 50% accuracy on our coreference test set and 53.5% for coherence/cohesion (compared\
    \ to a non-contextual baseline of 50%). A simple strategy of decoding the concatenation\
    \ of the previous and current sentence leads to good performance, and our novel\
    \ strategy of multi-encoding and decoding of two sentences leads to the best performance\
    \ (72.5% for coreference and 57% for coherence/cohesion), highlighting the importance\
    \ of target-side context."
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1118.Datasets.zip
    type: dataset
    url: https://www.aclweb.org/anthology/attachments/N18-1118.Datasets.zip
  author:
  - first: Rachel
    full: Rachel Bawden
    id: rachel-bawden
    last: Bawden
  - first: Rico
    full: Rico Sennrich
    id: rico-sennrich
    last: Sennrich
  - first: Alexandra
    full: Alexandra Birch
    id: alexandra-birch
    last: Birch
  - first: Barry
    full: Barry Haddow
    id: barry-haddow
    last: Haddow
  author_string: Rachel Bawden, Rico Sennrich, Alexandra Birch, Barry Haddow
  bibkey: bawden-etal-2018-evaluating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1118
  month: June
  page_first: '1304'
  page_last: '1313'
  pages: "1304\u20131313"
  paper_id: '118'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1118.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1118.jpg
  title: Evaluating Discourse Phenomena in Neural Machine Translation
  title_html: Evaluating Discourse Phenomena in Neural Machine Translation
  url: https://www.aclweb.org/anthology/N18-1118
  year: '2018'
N18-1119:
  abstract: "The end-to-end nature of neural machine translation (NMT) removes many\
    \ ways of manually guiding the translation process that were available in older\
    \ paradigms. Recent work, however, has introduced a new capability: lexically\
    \ constrained or guided decoding, a modification to beam search that forces the\
    \ inclusion of pre-specified words and phrases in the output. However, while theoretically\
    \ sound, existing approaches have computational complexities that are either linear\
    \ (Hokamp and Liu, 2017) or exponential (Anderson et al., 2017) in the number\
    \ of constraints. We present a algorithm for lexically constrained decoding with\
    \ a complexity of O(1) in the number of constraints. We demonstrate the algorithm\u2019\
    s remarkable ability to properly place these constraints, and use it to explore\
    \ the shaky relationship between model and BLEU scores. Our implementation is\
    \ available as part of Sockeye."
  address: New Orleans, Louisiana
  author:
  - first: Matt
    full: Matt Post
    id: matt-post
    last: Post
  - first: David
    full: David Vilar
    id: david-vilar
    last: Vilar
  author_string: Matt Post, David Vilar
  bibkey: post-vilar-2018-fast
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1119
  month: June
  page_first: '1314'
  page_last: '1324'
  pages: "1314\u20131324"
  paper_id: '119'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1119.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1119.jpg
  title: Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural
    Machine Translation
  title_html: Fast Lexically Constrained Decoding with Dynamic Beam Allocation for
    Neural Machine Translation
  url: https://www.aclweb.org/anthology/N18-1119
  year: '2018'
N18-1120:
  abstract: "One of the difficulties of neural machine translation (NMT) is the recall\
    \ and appropriate translation of low-frequency words or phrases. In this paper,\
    \ we propose a simple, fast, and effective method for recalling previously seen\
    \ translation examples and incorporating them into the NMT decoding process. Specifically,\
    \ for an input sentence, we use a search engine to retrieve sentence pairs whose\
    \ source sides are similar with the input sentence, and then collect n-grams that\
    \ are both in the retrieved target sentences and aligned with words that match\
    \ in the source sentences, which we call \u201Ctranslation pieces\u201D. We compute\
    \ pseudo-probabilities for each retrieved sentence based on similarities between\
    \ the input sentence and the retrieved source sentences, and use these to weight\
    \ the retrieved translation pieces. Finally, an existing NMT model is used to\
    \ translate the input sentence, with an additional bonus given to outputs that\
    \ contain the collected translation pieces. We show our method improves NMT translation\
    \ results up to 6 BLEU points on three narrow domain translation tasks where repetitiveness\
    \ of the target sentences is particularly salient. It also causes little increase\
    \ in the translation time, and compares favorably to another alternative retrieval-based\
    \ method with respect to accuracy, speed, and simplicity of implementation."
  address: New Orleans, Louisiana
  author:
  - first: Jingyi
    full: Jingyi Zhang
    id: jingyi-zhang
    last: Zhang
  - first: Masao
    full: Masao Utiyama
    id: masao-utiyama
    last: Utiyama
  - first: Eiichro
    full: Eiichro Sumita
    id: eiichiro-sumita
    last: Sumita
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  - first: Satoshi
    full: Satoshi Nakamura
    id: satoshi-nakamura
    last: Nakamura
  author_string: Jingyi Zhang, Masao Utiyama, Eiichro Sumita, Graham Neubig, Satoshi
    Nakamura
  bibkey: zhang-etal-2018-guiding
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1120
  month: June
  page_first: '1325'
  page_last: '1335'
  pages: "1325\u20131335"
  paper_id: '120'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1120.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1120.jpg
  title: Guiding Neural Machine Translation with Retrieved Translation Pieces
  title_html: Guiding Neural Machine Translation with Retrieved Translation Pieces
  url: https://www.aclweb.org/anthology/N18-1120
  year: '2018'
N18-1121:
  abstract: Homographs, words with different meanings but the same surface form, have
    long caused difficulty for machine translation systems, as it is difficult to
    select the correct translation based on the context. However, with the advent
    of neural machine translation (NMT) systems, which can theoretically take into
    account global sentential context, one may hypothesize that this problem has been
    alleviated. In this paper, we first provide empirical evidence that existing NMT
    systems in fact still have significant problems in properly translating ambiguous
    words. We then proceed to describe methods, inspired by the word sense disambiguation
    literature, that model the context of the input word with context-aware word embeddings
    that help to differentiate the word sense before feeding it into the encoder.
    Experiments on three language pairs demonstrate that such models improve the performance
    of NMT systems both in terms of BLEU score and in the accuracy of translating
    homographs.
  address: New Orleans, Louisiana
  author:
  - first: Frederick
    full: Frederick Liu
    id: frederick-liu
    last: Liu
  - first: Han
    full: Han Lu
    id: han-lu
    last: Lu
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  author_string: Frederick Liu, Han Lu, Graham Neubig
  bibkey: liu-etal-2018-handling
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1121
  month: June
  page_first: '1336'
  page_last: '1345'
  pages: "1336\u20131345"
  paper_id: '121'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1121.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1121.jpg
  title: Handling Homographs in Neural Machine Translation
  title_html: Handling Homographs in Neural Machine Translation
  url: https://www.aclweb.org/anthology/N18-1121
  year: '2018'
N18-1122:
  abstract: This paper proposes an approach for applying GANs to NMT. We build a conditional
    sequence generative adversarial net which comprises of two adversarial sub models,
    a generator and a discriminator. The generator aims to generate sentences which
    are hard to be discriminated from human-translated sentences ( i.e., the golden
    target sentences); And the discriminator makes efforts to discriminate the machine-generated
    sentences from human-translated ones. The two sub models play a mini-max game
    and achieve the win-win situation when they reach a Nash Equilibrium. Additionally,
    the static sentence-level BLEU is utilized as the reinforced objective for the
    generator, which biases the generation towards high BLEU points. During training,
    both the dynamic discriminator and the static BLEU objective are employed to evaluate
    the generated sentences and feedback the evaluations to guide the learning of
    the generator. Experimental results show that the proposed model consistently
    outperforms the traditional RNNSearch and the newly emerged state-of-the-art Transformer
    on English-German and Chinese-English translation tasks.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1122.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1122.Notes.pdf
  author:
  - first: Zhen
    full: Zhen Yang
    id: zhen-yang
    last: Yang
  - first: Wei
    full: Wei Chen
    id: wei-chen
    last: Chen
  - first: Feng
    full: Feng Wang
    id: feng-wang
    last: Wang
  - first: Bo
    full: Bo Xu
    id: bo-xu
    last: Xu
  author_string: Zhen Yang, Wei Chen, Feng Wang, Bo Xu
  bibkey: yang-etal-2018-improving
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1122
  month: June
  page_first: '1346'
  page_last: '1355'
  pages: "1346\u20131355"
  paper_id: '122'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1122.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1122.jpg
  title: Improving Neural Machine Translation with Conditional Sequence Generative
    Adversarial Nets
  title_html: Improving Neural Machine Translation with Conditional Sequence Generative
    Adversarial Nets
  url: https://www.aclweb.org/anthology/N18-1122
  year: '2018'
N18-1123:
  abstract: 'Neural machine translation requires large amount of parallel training
    text to learn a reasonable quality translation model. This is particularly inconvenient
    for language pairs for which enough parallel text is not available. In this paper,
    we use monolingual linguistic resources in the source side to address this challenging
    problem based on a multi-task learning approach. More specifically, we scaffold
    the machine translation task on auxiliary tasks including semantic parsing, syntactic
    parsing, and named-entity recognition. This effectively injects semantic and/or
    syntactic knowledge into the translation model, which would otherwise require
    a large amount of training bitext to learn from. We empirically analyze and show
    the effectiveness of our multitask learning approach on three translation tasks:
    English-to-French, English-to-Farsi, and English-to-Vietnamese.'
  address: New Orleans, Louisiana
  author:
  - first: Poorya
    full: Poorya Zaremoodi
    id: poorya-zaremoodi
    last: Zaremoodi
  - first: Gholamreza
    full: Gholamreza Haffari
    id: gholamreza-haffari
    last: Haffari
  author_string: Poorya Zaremoodi, Gholamreza Haffari
  bibkey: zaremoodi-haffari-2018-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1123
  month: June
  page_first: '1356'
  page_last: '1365'
  pages: "1356\u20131365"
  paper_id: '123'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1123.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1123.jpg
  title: 'Neural Machine Translation for Bilingually Scarce Scenarios: a Deep Multi-Task
    Learning Approach'
  title_html: 'Neural Machine Translation for Bilingually Scarce Scenarios: a Deep
    Multi-Task Learning Approach'
  url: https://www.aclweb.org/anthology/N18-1123
  year: '2018'
N18-1124:
  abstract: Neural sequence-to-sequence networks with attention have achieved remarkable
    performance for machine translation. One of the reasons for their effectiveness
    is their ability to capture relevant source-side contextual information at each
    time-step prediction through an attention mechanism. However, the target-side
    context is solely based on the sequence model which, in practice, is prone to
    a recency bias and lacks the ability to capture effectively non-sequential dependencies
    among words. To address this limitation, we propose a target-side-attentive residual
    recurrent network for decoding, where attention over previous words contributes
    directly to the prediction of the next word. The residual learning facilitates
    the flow of information from the distant past and is able to emphasize any of
    the previously translated words, hence it gains access to a wider context. The
    proposed model outperforms a neural MT baseline as well as a memory and self-attention
    network on three language pairs. The analysis of the attention learned by the
    decoder confirms that it emphasizes a wider context, and that it captures syntactic-like
    structures.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1124.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1124.Notes.pdf
  author:
  - first: Lesly
    full: Lesly Miculicich Werlen
    id: lesly-miculicich-werlen
    last: Miculicich Werlen
  - first: Nikolaos
    full: Nikolaos Pappas
    id: nikolaos-pappas
    last: Pappas
  - first: Dhananjay
    full: Dhananjay Ram
    id: dhananjay-ram
    last: Ram
  - first: Andrei
    full: Andrei Popescu-Belis
    id: andrei-popescu-belis
    last: Popescu-Belis
  author_string: Lesly Miculicich Werlen, Nikolaos Pappas, Dhananjay Ram, Andrei Popescu-Belis
  bibkey: miculicich-werlen-etal-2018-self
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1124
  month: June
  page_first: '1366'
  page_last: '1379'
  pages: "1366\u20131379"
  paper_id: '124'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1124.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1124.jpg
  title: Self-Attentive Residual Decoder for Neural Machine Translation
  title_html: Self-Attentive Residual Decoder for Neural Machine Translation
  url: https://www.aclweb.org/anthology/N18-1124
  year: '2018'
N18-1125:
  abstract: In neural machine translation, an attention model is used to identify
    the aligned source words for a target word (target foresight word) in order to
    select translation context, but it does not make use of any information of this
    target foresight word at all. Previous work proposed an approach to improve the
    attention model by explicitly accessing this target foresight word and demonstrated
    the substantial gains in alignment task. However, this approach is useless in
    machine translation task on which the target foresight word is unavailable. In
    this paper, we propose a new attention model enhanced by the implicit information
    of target foresight word oriented to both alignment and translation tasks. Empirical
    experiments on Chinese-to-English and Japanese-to-English datasets show that the
    proposed attention model delivers significant improvements in terms of both alignment
    error rate and BLEU.
  address: New Orleans, Louisiana
  author:
  - first: Xintong
    full: Xintong Li
    id: xintong-li
    last: Li
  - first: Lemao
    full: Lemao Liu
    id: lemao-liu
    last: Liu
  - first: Zhaopeng
    full: Zhaopeng Tu
    id: zhaopeng-tu
    last: Tu
  - first: Shuming
    full: Shuming Shi
    id: shuming-shi
    last: Shi
  - first: Max
    full: Max Meng
    id: max-meng
    last: Meng
  author_string: Xintong Li, Lemao Liu, Zhaopeng Tu, Shuming Shi, Max Meng
  bibkey: li-etal-2018-target
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1125
  month: June
  page_first: '1380'
  page_last: '1390'
  pages: "1380\u20131390"
  paper_id: '125'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1125.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1125.jpg
  title: Target Foresight Based Attention for Neural Machine Translation
  title_html: Target Foresight Based Attention for Neural Machine Translation
  url: https://www.aclweb.org/anthology/N18-1125
  year: '2018'
N18-1126:
  abstract: The main motivation for developing contextsensitive lemmatizers is to
    improve performance on unseen and ambiguous words. Yet previous systems have not
    carefully evaluated whether the use of context actually helps in these cases.
    We introduce Lematus, a lemmatizer based on a standard encoder-decoder architecture,
    which incorporates character-level sentence context. We evaluate its lemmatization
    accuracy across 20 languages in both a full data setting and a lower-resource
    setting with 10k training examples in each language. In both settings, we show
    that including context significantly improves results against a context-free version
    of the model. Context helps more for ambiguous words than for unseen words, though
    the latter has a greater effect on overall performance differences between languages.
    We also compare to three previous context-sensitive lemmatization systems, which
    all use pre-extracted edit trees as well as hand-selected features and/or additional
    sources of information such as tagged training data. Without using any of these,
    our context-sensitive model outperforms the best competitor system (Lemming) in
    the fulldata setting, and performs on par in the lowerresource setting.
  address: New Orleans, Louisiana
  author:
  - first: Toms
    full: Toms Bergmanis
    id: toms-bergmanis
    last: Bergmanis
  - first: Sharon
    full: Sharon Goldwater
    id: sharon-goldwater
    last: Goldwater
  author_string: Toms Bergmanis, Sharon Goldwater
  bibkey: bergmanis-goldwater-2018-context
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1126
  month: June
  page_first: '1391'
  page_last: '1400'
  pages: "1391\u20131400"
  paper_id: '126'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1126.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1126.jpg
  title: Context Sensitive Neural Lemmatization with Lematus
  title_html: Context Sensitive Neural Lemmatization with <span class="acl-fixed-case">L</span>ematus
  url: https://www.aclweb.org/anthology/N18-1126
  year: '2018'
N18-1127:
  abstract: Recognizing named entities in a document is a key task in many NLP applications.
    Although current state-of-the-art approaches to this task reach a high performance
    on clean text (e.g. newswire genres), those algorithms dramatically degrade when
    they are moved to noisy environments such as social media domains. We present
    two systems that address the challenges of processing social media data using
    character-level phonetics and phonology, word embeddings, and Part-of-Speech tags
    as features. The first model is a multitask end-to-end Bidirectional Long Short-Term
    Memory (BLSTM)-Conditional Random Field (CRF) network whose output layer contains
    two CRF classifiers. The second model uses a multitask BLSTM network as feature
    extractor that transfers the learning to a CRF classifier for the final prediction.
    Our systems outperform the current F1 scores of the state of the art on the Workshop
    on Noisy User-generated Text 2017 dataset by 2.45% and 3.69%, establishing a more
    suitable approach for social media environments.
  address: New Orleans, Louisiana
  author:
  - first: Gustavo
    full: Gustavo Aguilar
    id: gustavo-aguilar
    last: Aguilar
  - first: Adrian Pastor
    full: "Adrian Pastor L\xF3pez-Monroy"
    id: adrian-pastor-lopez-monroy
    last: "L\xF3pez-Monroy"
  - first: Fabio
    full: "Fabio Gonz\xE1lez"
    id: fabio-a-gonzalez
    last: "Gonz\xE1lez"
  - first: Thamar
    full: Thamar Solorio
    id: thamar-solorio
    last: Solorio
  author_string: "Gustavo Aguilar, Adrian Pastor L\xF3pez-Monroy, Fabio Gonz\xE1lez,\
    \ Thamar Solorio"
  bibkey: aguilar-etal-2018-modeling
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1127
  month: June
  page_first: '1401'
  page_last: '1412'
  pages: "1401\u20131412"
  paper_id: '127'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1127.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1127.jpg
  title: Modeling Noisiness to Recognize Named Entities using Multitask Neural Networks
    on Social Media
  title_html: Modeling Noisiness to Recognize Named Entities using Multitask Neural
    Networks on Social Media
  url: https://www.aclweb.org/anthology/N18-1127
  year: '2018'
N18-1128:
  abstract: 'We propose several ways of reusing subword embeddings and other weights
    in subword-aware neural language models. The proposed techniques do not benefit
    a competitive character-aware model, but some of them improve the performance
    of syllable- and morpheme-aware models while showing significant reductions in
    model sizes. We discover a simple hands-on principle: in a multi-layer input embedding
    model, layers should be tied consecutively bottom-up if reused at output. Our
    best morpheme-aware model with properly reused weights beats the competitive word-level
    model by a large margin across multiple languages and has 20%-87% fewer parameters.'
  address: New Orleans, Louisiana
  author:
  - first: Zhenisbek
    full: Zhenisbek Assylbekov
    id: zhenisbek-assylbekov
    last: Assylbekov
  - first: Rustem
    full: Rustem Takhanov
    id: rustem-takhanov
    last: Takhanov
  author_string: Zhenisbek Assylbekov, Rustem Takhanov
  bibkey: assylbekov-takhanov-2018-reusing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1128
  month: June
  page_first: '1413'
  page_last: '1423'
  pages: "1413\u20131423"
  paper_id: '128'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1128.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1128.jpg
  title: Reusing Weights in Subword-Aware Neural Language Models
  title_html: Reusing Weights in Subword-Aware Neural Language Models
  url: https://www.aclweb.org/anthology/N18-1128
  year: '2018'
N18-1129:
  abstract: "We propose the first generative models for three types of extra-grammatical\
    \ word formation phenomena abounding in slang: Blends, Clippings, and Reduplicatives.\
    \ Adopting a data-driven approach coupled with linguistic knowledge, we propose\
    \ simple models with state of the art performance on human annotated gold standard\
    \ datasets. Overall, our models reveal insights into the generative processes\
    \ of word formation in slang \u2013 insights which are increasingly relevant in\
    \ the context of the rising prevalence of slang and non-standard varieties on\
    \ the Internet"
  address: New Orleans, Louisiana
  author:
  - first: Vivek
    full: Vivek Kulkarni
    id: vivek-kulkarni
    last: Kulkarni
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  author_string: Vivek Kulkarni, William Yang Wang
  bibkey: kulkarni-wang-2018-simple
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1129
  month: June
  page_first: '1424'
  page_last: '1434'
  pages: "1424\u20131434"
  paper_id: '129'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1129.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1129.jpg
  title: Simple Models for Word Formation in Slang
  title_html: Simple Models for Word Formation in Slang
  url: https://www.aclweb.org/anthology/N18-1129
  year: '2018'
N18-1130:
  abstract: "Languages with productive morphology pose problems for language models\
    \ that generate words from a fixed vocabulary. Although character-based models\
    \ allow any possible word type to be generated, they are linguistically na\xEF\
    ve: they must discover that words exist and are delimited by spaces\u2014basic\
    \ linguistic facts that are built in to the structure of word-based models. We\
    \ introduce an open-vocabulary language model that incorporates more sophisticated\
    \ linguistic knowledge by predicting words using a mixture of three generative\
    \ processes: (1) by generating words as a sequence of characters, (2) by directly\
    \ generating full word forms, and (3) by generating words as a sequence of morphemes\
    \ that are combined using a hand-written morphological analyzer. Experiments on\
    \ Finnish, Turkish, and Russian show that our model outperforms character sequence\
    \ models and other strong baselines on intrinsic and extrinsic measures. Furthermore,\
    \ we show that our model learns to exploit morphological knowledge encoded in\
    \ the analyzer, and, as a byproduct, it can perform effective unsupervised morphological\
    \ disambiguation."
  address: New Orleans, Louisiana
  author:
  - first: Austin
    full: Austin Matthews
    id: austin-matthews
    last: Matthews
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  - first: Chris
    full: Chris Dyer
    id: chris-dyer
    last: Dyer
  author_string: Austin Matthews, Graham Neubig, Chris Dyer
  bibkey: matthews-etal-2018-using
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1130
  month: June
  page_first: '1435'
  page_last: '1445'
  pages: "1435\u20131445"
  paper_id: '130'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1130.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1130.jpg
  title: Using Morphological Knowledge in Open-Vocabulary Neural Language Models
  title_html: Using Morphological Knowledge in Open-Vocabulary Neural Language Models
  url: https://www.aclweb.org/anthology/N18-1130
  year: '2018'
N18-1131:
  abstract: Entity mentions embedded in longer entity mentions are referred to as
    nested entities. Most named entity recognition (NER) systems deal only with the
    flat entities and ignore the inner nested ones, which fails to capture finer-grained
    semantic information in underlying texts. To address this issue, we propose a
    novel neural model to identify nested entities by dynamically stacking flat NER
    layers. Each flat NER layer is based on the state-of-the-art flat NER model that
    captures sequential context representation with bidirectional Long Short-Term
    Memory (LSTM) layer and feeds it to the cascaded CRF layer. Our model merges the
    output of the LSTM layer in the current flat NER layer to build new representation
    for detected entities and subsequently feeds them into the next flat NER layer.
    This allows our model to extract outer entities by taking full advantage of information
    encoded in their corresponding inner entities, in an inside-to-outside way. Our
    model dynamically stacks the flat NER layers until no outer entities are extracted.
    Extensive evaluation shows that our dynamic model outperforms state-of-the-art
    feature-based systems on nested NER, achieving 74.7% and 72.2% on GENIA and ACE2005
    datasets, respectively, in terms of F-score.
  address: New Orleans, Louisiana
  attachment:
  - filename: https://vimeo.com/277349441
    type: video
    url: https://vimeo.com/277349441
  author:
  - first: Meizhi
    full: Meizhi Ju
    id: meizhi-ju
    last: Ju
  - first: Makoto
    full: Makoto Miwa
    id: makoto-miwa
    last: Miwa
  - first: Sophia
    full: Sophia Ananiadou
    id: sophia-ananiadou
    last: Ananiadou
  author_string: Meizhi Ju, Makoto Miwa, Sophia Ananiadou
  bibkey: ju-etal-2018-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1131
  month: June
  page_first: '1446'
  page_last: '1459'
  pages: "1446\u20131459"
  paper_id: '131'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1131.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1131.jpg
  title: A Neural Layered Model for Nested Named Entity Recognition
  title_html: A Neural Layered Model for Nested Named Entity Recognition
  url: https://www.aclweb.org/anthology/N18-1131
  year: '2018'
N18-1132:
  abstract: We present a novel deep learning architecture to address the natural language
    inference (NLI) task. Existing approaches mostly rely on simple reading mechanisms
    for independent encoding of the premise and hypothesis. Instead, we propose a
    novel dependent reading bidirectional LSTM network (DR-BiLSTM) to efficiently
    model the relationship between a premise and a hypothesis during encoding and
    inference. We also introduce a sophisticated ensemble strategy to combine our
    proposed models, which noticeably improves final predictions. Finally, we demonstrate
    how the results can be improved further with an additional preprocessing step.
    Our evaluation shows that DR-BiLSTM obtains the best single model and ensemble
    model results achieving the new state-of-the-art scores on the Stanford NLI dataset.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1132.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1132.Notes.pdf
  - filename: http://vimeo.com/277672495
    type: video
    url: http://vimeo.com/277672495
  author:
  - first: Reza
    full: Reza Ghaeini
    id: reza-ghaeini
    last: Ghaeini
  - first: Sadid A.
    full: Sadid A. Hasan
    id: sadid-a-hasan
    last: Hasan
  - first: Vivek
    full: Vivek Datla
    id: vivek-datla
    last: Datla
  - first: Joey
    full: Joey Liu
    id: joey-liu
    last: Liu
  - first: Kathy
    full: Kathy Lee
    id: kathy-lee
    last: Lee
  - first: Ashequl
    full: Ashequl Qadir
    id: ashequl-qadir
    last: Qadir
  - first: Yuan
    full: Yuan Ling
    id: yuan-ling
    last: Ling
  - first: Aaditya
    full: Aaditya Prakash
    id: aaditya-prakash
    last: Prakash
  - first: Xiaoli
    full: Xiaoli Fern
    id: xiaoli-fern
    last: Fern
  - first: Oladimeji
    full: Oladimeji Farri
    id: oladimeji-farri
    last: Farri
  author_string: Reza Ghaeini, Sadid A. Hasan, Vivek Datla, Joey Liu, Kathy Lee, Ashequl
    Qadir, Yuan Ling, Aaditya Prakash, Xiaoli Fern, Oladimeji Farri
  bibkey: ghaeini-etal-2018-dr
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1132
  month: June
  page_first: '1460'
  page_last: '1469'
  pages: "1460\u20131469"
  paper_id: '132'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1132.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1132.jpg
  title: 'DR-BiLSTM: Dependent Reading Bidirectional LSTM for Natural Language Inference'
  title_html: '<span class="acl-fixed-case">DR</span>-<span class="acl-fixed-case">B</span>i<span
    class="acl-fixed-case">LSTM</span>: Dependent Reading Bidirectional <span class="acl-fixed-case">LSTM</span>
    for Natural Language Inference'
  url: https://www.aclweb.org/anthology/N18-1132
  year: '2018'
N18-1133:
  abstract: 'We introduce KBGAN, an adversarial learning framework to improve the
    performances of a wide range of existing knowledge graph embedding models. Because
    knowledge graphs typically only contain positive facts, sampling useful negative
    training examples is a nontrivial task. Replacing the head or tail entity of a
    fact with a uniformly randomly selected entity is a conventional method for generating
    negative facts, but the majority of the generated negative facts can be easily
    discriminated from positive facts, and will contribute little towards the training.
    Inspired by generative adversarial networks (GANs), we use one knowledge graph
    embedding model as a negative sample generator to assist the training of our desired
    model, which acts as the discriminator in GANs. This framework is independent
    of the concrete form of generator and discriminator, and therefore can utilize
    a wide variety of knowledge graph embedding models as its building blocks. In
    experiments, we adversarially train two translation-based models, TRANSE and TRANSD,
    each with assistance from one of the two probability-based models, DISTMULT and
    COMPLEX. We evaluate the performances of KBGAN on the link prediction task, using
    three knowledge base completion datasets: FB15k-237, WN18 and WN18RR. Experimental
    results show that adversarial training substantially improves the performances
    of target embedding models under various settings.'
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1133.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1133.Notes.pdf
  - filename: http://vimeo.com/277671743
    type: video
    url: http://vimeo.com/277671743
  author:
  - first: Liwei
    full: Liwei Cai
    id: liwei-cai
    last: Cai
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  author_string: Liwei Cai, William Yang Wang
  bibkey: cai-wang-2018-kbgan
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1133
  month: June
  page_first: '1470'
  page_last: '1480'
  pages: "1470\u20131480"
  paper_id: '133'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1133.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1133.jpg
  title: 'KBGAN: Adversarial Learning for Knowledge Graph Embeddings'
  title_html: '<span class="acl-fixed-case">KBGAN</span>: Adversarial Learning for
    Knowledge Graph Embeddings'
  url: https://www.aclweb.org/anthology/N18-1133
  year: '2018'
N18-1134:
  abstract: An essential step in FrameNet Semantic Role Labeling is the Frame Identification
    (FrameId) task, which aims at disambiguating a situation around a predicate. Whilst
    current FrameId methods rely on textual representations only, we hypothesize that
    FrameId can profit from a richer understanding of the situational context. Such
    contextual information can be obtained from common sense knowledge, which is more
    present in images than in text. In this paper, we extend a state-of-the-art FrameId
    system in order to effectively leverage multimodal representations. We conduct
    a comprehensive evaluation on the English FrameNet and its German counterpart
    SALSA. Our analysis shows that for the German data, textual representations are
    still competitive with multimodal ones. However on the English data, our multimodal
    FrameId approach outperforms its unimodal counterpart, setting a new state of
    the art. Its benefits are particularly apparent in dealing with ambiguous and
    rare instances, the main source of errors of current systems. For research purposes,
    we release (a) the implementation of our system, (b) our evaluation splits for
    SALSA 2.0, and (c) the embeddings for synsets and IMAGINED words.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/277672746
    type: video
    url: http://vimeo.com/277672746
  author:
  - first: Teresa
    full: Teresa Botschen
    id: teresa-botschen
    last: Botschen
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  - first: Jan-Christoph
    full: Jan-Christoph Klie
    id: jan-christoph-klie
    last: Klie
  - first: Hatem
    full: Hatem Mousselly-Sergieh
    id: hatem-mousselly-sergieh
    last: Mousselly-Sergieh
  - first: Stefan
    full: Stefan Roth
    id: stefan-roth
    last: Roth
  author_string: Teresa Botschen, Iryna Gurevych, Jan-Christoph Klie, Hatem Mousselly-Sergieh,
    Stefan Roth
  bibkey: botschen-etal-2018-multimodal
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1134
  month: June
  page_first: '1481'
  page_last: '1491'
  pages: "1481\u20131491"
  paper_id: '134'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1134.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1134.jpg
  title: Multimodal Frame Identification with Multilingual Evaluation
  title_html: Multimodal Frame Identification with Multilingual Evaluation
  url: https://www.aclweb.org/anthology/N18-1134
  year: '2018'
N18-1135:
  abstract: "We present a new approach to learning a semantic parser from multiple\
    \ datasets, even when the target semantic formalisms are drastically different\
    \ and the underlying corpora do not overlap. We handle such \u201Cdisjoint\u201D\
    \ data by treating annotations for unobserved formalisms as latent structured\
    \ variables. Building on state-of-the-art baselines, we show improvements both\
    \ in frame-semantic parsing and semantic dependency parsing by modeling them jointly."
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/277672994
    type: video
    url: http://vimeo.com/277672994
  author:
  - first: Hao
    full: Hao Peng
    id: hao-peng
    last: Peng
  - first: Sam
    full: Sam Thomson
    id: sam-thomson
    last: Thomson
  - first: Swabha
    full: Swabha Swayamdipta
    id: swabha-swayamdipta
    last: Swayamdipta
  - first: Noah A.
    full: Noah A. Smith
    id: noah-a-smith
    last: Smith
  author_string: Hao Peng, Sam Thomson, Swabha Swayamdipta, Noah A. Smith
  bibkey: peng-etal-2018-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1135
  month: June
  page_first: '1492'
  page_last: '1502'
  pages: "1492\u20131502"
  paper_id: '135'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1135.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1135.jpg
  title: Learning Joint Semantic Parsers from Disjoint Data
  title_html: Learning Joint Semantic Parsers from Disjoint Data
  url: https://www.aclweb.org/anthology/N18-1135
  year: '2018'
N18-1136:
  abstract: Recognizing that even correct translations are not always semantically
    equivalent, we automatically detect meaning divergences in parallel sentence pairs
    with a deep neural model of bilingual semantic similarity which can be trained
    for any parallel corpus without any manual annotation. We show that our semantic
    model detects divergences more accurately than models based on surface features
    derived from word alignments, and that these divergences matter for neural machine
    translation.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1136.Datasets.tgz
    type: dataset
    url: https://www.aclweb.org/anthology/attachments/N18-1136.Datasets.tgz
  - filename: http://vimeo.com/277672916
    type: video
    url: http://vimeo.com/277672916
  author:
  - first: Yogarshi
    full: Yogarshi Vyas
    id: yogarshi-vyas
    last: Vyas
  - first: Xing
    full: Xing Niu
    id: xing-niu
    last: Niu
  - first: Marine
    full: Marine Carpuat
    id: marine-carpuat
    last: Carpuat
  author_string: Yogarshi Vyas, Xing Niu, Marine Carpuat
  bibkey: vyas-etal-2018-identifying
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1136
  month: June
  page_first: '1503'
  page_last: '1515'
  pages: "1503\u20131515"
  paper_id: '136'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1136.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1136.jpg
  title: Identifying Semantic Divergences in Parallel Text without Annotations
  title_html: Identifying Semantic Divergences in Parallel Text without Annotations
  url: https://www.aclweb.org/anthology/N18-1136
  year: '2018'
N18-1137:
  abstract: A core step in statistical data-to-text generation concerns learning correspondences
    between structured data representations (e.g., facts in a database) and associated
    texts. In this paper we aim to bootstrap generators from large scale datasets
    where the data (e.g., DBPedia facts) and related texts (e.g., Wikipedia abstracts)
    are loosely aligned. We tackle this challenging task by introducing a special-purpose
    content selection mechanism. We use multi-instance learning to automatically discover
    correspondences between data and text pairs and show how these can be used to
    enhance the content signal while training an encoder-decoder architecture. Experimental
    results demonstrate that models trained with content-specific objectives improve
    upon a vanilla encoder-decoder which solely relies on soft attention.
  address: New Orleans, Louisiana
  attachment:
  - filename: https://vimeo.com/277348853
    type: video
    url: https://vimeo.com/277348853
  author:
  - first: Laura
    full: Laura Perez-Beltrachini
    id: laura-perez-beltrachini
    last: Perez-Beltrachini
  - first: Mirella
    full: Mirella Lapata
    id: mirella-lapata
    last: Lapata
  author_string: Laura Perez-Beltrachini, Mirella Lapata
  bibkey: perez-beltrachini-lapata-2018-bootstrapping
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1137
  month: June
  page_first: '1516'
  page_last: '1527'
  pages: "1516\u20131527"
  paper_id: '137'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1137.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1137.jpg
  title: Bootstrapping Generators from Noisy Data
  title_html: Bootstrapping Generators from Noisy Data
  url: https://www.aclweb.org/anthology/N18-1137
  year: '2018'
N18-1138:
  abstract: "Supervised training of abstractive language generation models results\
    \ in learning conditional probabilities over language sequences based on the supervised\
    \ training signal. When the training signal contains a variety of writing styles,\
    \ such models may end up learning an \u2018average\u2019 style that is directly\
    \ influenced by the training data make-up and cannot be controlled by the needs\
    \ of an application. We describe a family of model architectures capable of capturing\
    \ both generic language characteristics via shared model parameters, as well as\
    \ particular style characteristics via private model parameters. Such models are\
    \ able to generate language according to a specific learned style, while still\
    \ taking advantage of their power to model generic language phenomena. Furthermore,\
    \ we describe an extension that uses a mixture of output distributions from all\
    \ learned styles to perform on-the-fly style adaptation based on the textual input\
    \ alone. Experimentally, we find that the proposed models consistently outperform\
    \ models that encapsulate single-style or average-style language generation capabilities."
  address: New Orleans, Louisiana
  attachment:
  - filename: https://vimeo.com/277348640
    type: video
    url: https://vimeo.com/277348640
  author:
  - first: Ye
    full: Ye Zhang
    id: ye-zhang
    last: Zhang
  - first: Nan
    full: Nan Ding
    id: nan-ding
    last: Ding
  - first: Radu
    full: Radu Soricut
    id: radu-soricut
    last: Soricut
  author_string: Ye Zhang, Nan Ding, Radu Soricut
  bibkey: zhang-etal-2018-shaped
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1138
  month: June
  page_first: '1528'
  page_last: '1538'
  pages: "1528\u20131538"
  paper_id: '138'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1138.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1138.jpg
  title: 'SHAPED: Shared-Private Encoder-Decoder for Text Style Adaptation'
  title_html: '<span class="acl-fixed-case">SHAPED</span>: Shared-Private Encoder-Decoder
    for Text Style Adaptation'
  url: https://www.aclweb.org/anthology/N18-1138
  year: '2018'
N18-1139:
  abstract: 'In this work, we focus on the task of generating natural language descriptions
    from a structured table of facts containing fields (such as nationality, occupation,
    etc) and values (such as Indian, actor, director, etc). One simple choice is to
    treat the table as a sequence of fields and values and then use a standard seq2seq
    model for this task. However, such a model is too generic and does not exploit
    task specific characteristics. For example, while generating descriptions from
    a table, a human would attend to information at two levels: (i) the fields (macro
    level) and (ii) the values within the field (micro level). Further, a human would
    continue attending to a field for a few timesteps till all the information from
    that field has been rendered and then never return back to this field (because
    there is nothing left to say about it). To capture this behavior we use (i) a
    fused bifocal attention mechanism which exploits and combines this micro and macro
    level information and (ii) a gated orthogonalization mechanism which tries to
    ensure that a field is remembered for a few time steps and then forgotten. We
    experiment with a recently released dataset which contains fact tables about people
    and their corresponding one line biographical descriptions in English. In addition,
    we also introduce two similar datasets for French and German. Our experiments
    show that the proposed model gives 21% relative improvement over a recently proposed
    state of the art method and 10% relative improvement over basic seq2seq models.
    The code and the datasets developed as a part of this work are publicly available
    on https://github.com/PrekshaNema25/StructuredData_To_Descriptions'
  address: New Orleans, Louisiana
  author:
  - first: Preksha
    full: Preksha Nema
    id: preksha-nema
    last: Nema
  - first: Shreyas
    full: Shreyas Shetty
    id: shreyas-shetty
    last: Shetty
  - first: Parag
    full: Parag Jain
    id: parag-jain
    last: Jain
  - first: Anirban
    full: Anirban Laha
    id: anirban-laha
    last: Laha
  - first: Karthik
    full: Karthik Sankaranarayanan
    id: karthik-sankaranarayanan
    last: Sankaranarayanan
  - first: Mitesh M.
    full: Mitesh M. Khapra
    id: mitesh-m-khapra
    last: Khapra
  author_string: Preksha Nema, Shreyas Shetty, Parag Jain, Anirban Laha, Karthik Sankaranarayanan,
    Mitesh M. Khapra
  bibkey: nema-etal-2018-generating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1139
  month: June
  page_first: '1539'
  page_last: '1550'
  pages: "1539\u20131550"
  paper_id: '139'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1139.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1139.jpg
  title: Generating Descriptions from Structured Data Using a Bifocal Attention Mechanism
    and Gated Orthogonalization
  title_html: Generating Descriptions from Structured Data Using a Bifocal Attention
    Mechanism and Gated Orthogonalization
  url: https://www.aclweb.org/anthology/N18-1139
  year: '2018'
N18-1140:
  abstract: We present a new dataset for machine comprehension in the medical domain.
    Our dataset uses clinical case reports with around 100,000 gap-filling queries
    about these cases. We apply several baselines and state-of-the-art neural readers
    to the dataset, and observe a considerable gap in performance (20% F1) between
    the best human and machine readers. We analyze the skills required for successful
    answering and show how reader performance varies depending on the applicable skills.
    We find that inferences using domain knowledge and object tracking are the most
    frequently required skills, and that recognizing omitted information and spatio-temporal
    reasoning are the most difficult for the machines.
  address: New Orleans, Louisiana
  author:
  - first: Simon
    full: "Simon \u0160uster"
    id: simon-suster
    last: "\u0160uster"
  - first: Walter
    full: Walter Daelemans
    id: walter-daelemans
    last: Daelemans
  author_string: "Simon \u0160uster, Walter Daelemans"
  bibkey: suster-daelemans-2018-clicr
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1140
  month: June
  page_first: '1551'
  page_last: '1563'
  pages: "1551\u20131563"
  paper_id: '140'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1140.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1140.jpg
  title: 'CliCR: a Dataset of Clinical Case Reports for Machine Reading Comprehension'
  title_html: '<span class="acl-fixed-case">C</span>li<span class="acl-fixed-case">CR</span>:
    a Dataset of Clinical Case Reports for Machine Reading Comprehension'
  url: https://www.aclweb.org/anthology/N18-1140
  year: '2018'
N18-1141:
  abstract: Question answering (QA) and question generation (QG) are closely related
    tasks that could improve each other; however, the connection of these two tasks
    is not well explored in literature. In this paper, we give a systematic study
    that seeks to leverage the connection to improve both QA and QG. We present a
    training algorithm that generalizes both Generative Adversarial Network (GAN)
    and Generative Domain-Adaptive Nets (GDAN) under the question answering scenario.
    The two key ideas are improving the QG model with QA through incorporating additional
    QA-specific signal as the loss function, and improving the QA model with QG through
    adding artificially generated training instances. We conduct experiments on both
    document based and knowledge based question answering tasks. We have two main
    findings. Firstly, the performance of a QG model (e.g in terms of BLEU score)
    could be easily improved by a QA model via policy gradient. Secondly, directly
    applying GAN that regards all the generated questions as negative instances could
    not improve the accuracy of the QA model. Learning when to regard generated questions
    as positive instances could bring performance boost.
  address: New Orleans, Louisiana
  author:
  - first: Duyu
    full: Duyu Tang
    id: duyu-tang
    last: Tang
  - first: Nan
    full: Nan Duan
    id: nan-duan
    last: Duan
  - first: Zhao
    full: Zhao Yan
    id: zhao-yan
    last: Yan
  - first: Zhirui
    full: Zhirui Zhang
    id: zhirui-zhang
    last: Zhang
  - first: Yibo
    full: Yibo Sun
    id: yibo-sun
    last: Sun
  - first: Shujie
    full: Shujie Liu
    id: shujie-liu
    last: Liu
  - first: Yuanhua
    full: Yuanhua Lv
    id: yuanhua-lv
    last: Lv
  - first: Ming
    full: Ming Zhou
    id: ming-zhou
    last: Zhou
  author_string: Duyu Tang, Nan Duan, Zhao Yan, Zhirui Zhang, Yibo Sun, Shujie Liu,
    Yuanhua Lv, Ming Zhou
  bibkey: tang-etal-2018-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1141
  month: June
  page_first: '1564'
  page_last: '1574'
  pages: "1564\u20131574"
  paper_id: '141'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1141.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1141.jpg
  title: Learning to Collaborate for Question Answering and Asking
  title_html: Learning to Collaborate for Question Answering and Asking
  url: https://www.aclweb.org/anthology/N18-1141
  year: '2018'
N18-1142:
  abstract: In this paper, we propose a novel end-to-end neural architecture for ranking
    candidate answers, that adapts a hierarchical recurrent neural network and a latent
    topic clustering module. With our proposed model, a text is encoded to a vector
    representation from an word-level to a chunk-level to effectively capture the
    entire meaning. In particular, by adapting the hierarchical structure, our model
    shows very small performance degradations in longer text comprehension while other
    state-of-the-art recurrent neural network models suffer from it. Additionally,
    the latent topic clustering module extracts semantic information from target samples.
    This clustering module is useful for any text related tasks by allowing each data
    sample to find its nearest topic cluster, thus helping the neural network model
    analyze the entire data. We evaluate our models on the Ubuntu Dialogue Corpus
    and consumer electronic domain question answering dataset, which is related to
    Samsung products. The proposed model shows state-of-the-art results for ranking
    question-answer pairs.
  address: New Orleans, Louisiana
  author:
  - first: Seunghyun
    full: Seunghyun Yoon
    id: seunghyun-yoon
    last: Yoon
  - first: Joongbo
    full: Joongbo Shin
    id: joongbo-shin
    last: Shin
  - first: Kyomin
    full: Kyomin Jung
    id: kyomin-jung
    last: Jung
  author_string: Seunghyun Yoon, Joongbo Shin, Kyomin Jung
  bibkey: yoon-etal-2018-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1142
  month: June
  page_first: '1575'
  page_last: '1584'
  pages: "1575\u20131584"
  paper_id: '142'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1142.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1142.jpg
  title: Learning to Rank Question-Answer Pairs Using Hierarchical Recurrent Encoder
    with Latent Topic Clustering
  title_html: Learning to Rank Question-Answer Pairs Using Hierarchical Recurrent
    Encoder with Latent Topic Clustering
  url: https://www.aclweb.org/anthology/N18-1142
  year: '2018'
N18-1143:
  abstract: Although transfer learning has been shown to be successful for tasks like
    object and speech recognition, its applicability to question answering (QA) has
    yet to be well-studied. In this paper, we conduct extensive experiments to investigate
    the transferability of knowledge learned from a source QA dataset to a target
    dataset using two QA models. The performance of both models on a TOEFL listening
    comprehension test (Tseng et al., 2016) and MCTest (Richardson et al., 2013) is
    significantly improved via a simple transfer learning technique from MovieQA (Tapaswi
    et al., 2016). In particular, one of the models achieves the state-of-the-art
    on all target datasets; for the TOEFL listening comprehension test, it outperforms
    the previous best model by 7%. Finally, we show that transfer learning is helpful
    even in unsupervised scenarios when correct answers for target QA dataset examples
    are not available.
  address: New Orleans, Louisiana
  author:
  - first: Yu-An
    full: Yu-An Chung
    id: yu-an-chung
    last: Chung
  - first: Hung-Yi
    full: Hung-Yi Lee
    id: hung-yi-lee
    last: Lee
  - first: James
    full: James Glass
    id: james-glass
    last: Glass
  author_string: Yu-An Chung, Hung-Yi Lee, James Glass
  bibkey: chung-etal-2018-supervised
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1143
  month: June
  page_first: '1585'
  page_last: '1594'
  pages: "1585\u20131594"
  paper_id: '143'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1143.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1143.jpg
  title: Supervised and Unsupervised Transfer Learning for Question Answering
  title_html: Supervised and Unsupervised Transfer Learning for Question Answering
  url: https://www.aclweb.org/anthology/N18-1143
  year: '2018'
N18-1144:
  abstract: We present a new dataset and models for comprehending paragraphs about
    processes (e.g., photosynthesis), an important genre of text describing a dynamic
    world. The new dataset, ProPara, is the first to contain natural (rather than
    machine-generated) text about a changing world along with a full annotation of
    entity states (location and existence) during those changes (81k datapoints).
    The end-task, tracking the location and existence of entities through the text,
    is challenging because the causal effects of actions are often implicit and need
    to be inferred. We find that previous models that have worked well on synthetic
    data achieve only mediocre performance on ProPara, and introduce two new neural
    models that exploit alternative mechanisms for state prediction, in particular
    using LSTM input encoding and span prediction. The new models improve accuracy
    by up to 19%. We are releasing the ProPara dataset and our models to the community.
  address: New Orleans, Louisiana
  author:
  - first: Bhavana
    full: Bhavana Dalvi
    id: bhavana-dalvi
    last: Dalvi
  - first: Lifu
    full: Lifu Huang
    id: lifu-huang
    last: Huang
  - first: Niket
    full: Niket Tandon
    id: niket-tandon
    last: Tandon
  - first: Wen-tau
    full: Wen-tau Yih
    id: wen-tau-yih
    last: Yih
  - first: Peter
    full: Peter Clark
    id: peter-clark
    last: Clark
  author_string: Bhavana Dalvi, Lifu Huang, Niket Tandon, Wen-tau Yih, Peter Clark
  bibkey: dalvi-etal-2018-tracking
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1144
  month: June
  page_first: '1595'
  page_last: '1604'
  pages: "1595\u20131604"
  paper_id: '144'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1144.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1144.jpg
  title: 'Tracking State Changes in Procedural Text: a Challenge Dataset and Models
    for Process Paragraph Comprehension'
  title_html: 'Tracking State Changes in Procedural Text: a Challenge Dataset and
    Models for Process Paragraph Comprehension'
  url: https://www.aclweb.org/anthology/N18-1144
  year: '2018'
N18-1145:
  abstract: With the rise of e-commerce, people are accustomed to writing their reviews
    after receiving the goods. These comments are so important that a bad review can
    have a direct impact on others buying. Besides, the abundant information within
    user reviews is very useful for extracting user preferences and item properties.
    In this paper, we investigate the approach to effectively utilize review information
    for recommender systems. The proposed model is named LSTM-Topic matrix factorization
    (LTMF) which integrates both LSTM and Topic Modeling for review understanding.
    In the experiments on popular review dataset Amazon , our LTMF model outperforms
    previous proposed HFT model and ConvMF model in rating prediction. Furthermore,
    LTMF shows the better ability on making topic clustering than traditional topic
    model based method, which implies integrating the information from deep learning
    and topic modeling is a meaningful approach to make a better understanding of
    reviews.
  address: New Orleans, Louisiana
  author:
  - first: Mingmin
    full: Mingmin Jin
    id: mingmin-jin
    last: Jin
  - first: Xin
    full: Xin Luo
    id: xin-luo
    last: Luo
  - first: Huiling
    full: Huiling Zhu
    id: huiling-zhu
    last: Zhu
  - first: Hankz Hankui
    full: Hankz Hankui Zhuo
    id: hankz-hankui-zhuo
    last: Zhuo
  author_string: Mingmin Jin, Xin Luo, Huiling Zhu, Hankz Hankui Zhuo
  bibkey: jin-etal-2018-combining
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1145
  month: June
  page_first: '1605'
  page_last: '1614'
  pages: "1605\u20131614"
  paper_id: '145'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1145.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1145.jpg
  title: Combining Deep Learning and Topic Modeling for Review Understanding in Context-Aware
    Recommendation
  title_html: Combining Deep Learning and Topic Modeling for Review Understanding
    in Context-Aware Recommendation
  url: https://www.aclweb.org/anthology/N18-1145
  year: '2018'
N18-1146:
  abstract: 'NLP algorithms are increasingly used in computational social science
    to take linguistic observations and predict outcomes like human preferences or
    actions. Making these social models transparent and interpretable often requires
    identifying features in the input that predict outcomes while also controlling
    for potential confounds. We formalize this need as a new task: inducing a lexicon
    that is predictive of a set of target variables yet uncorrelated to a set of confounding
    variables. We introduce two deep learning algorithms for the task. The first uses
    a bifurcated architecture to separate the explanatory power of the text and confounds.
    The second uses an adversarial discriminator to force confound-invariant text
    encodings. Both elicit lexicons from learned weights and attentional scores. We
    use them to induce lexicons that are predictive of timely responses to consumer
    complaints (controlling for product), enrollment from course descriptions (controlling
    for subject), and sales from product descriptions (controlling for seller). In
    each domain our algorithms pick words that are associated with narrative persuasion;
    more predictive and less confound-related than those of standard feature weighting
    and lexicon induction techniques like regression and log odds.'
  address: New Orleans, Louisiana
  author:
  - first: Reid
    full: Reid Pryzant
    id: reid-pryzant
    last: Pryzant
  - first: Kelly
    full: Kelly Shen
    id: kelly-shen
    last: Shen
  - first: Dan
    full: Dan Jurafsky
    id: dan-jurafsky
    last: Jurafsky
  - first: Stefan
    full: Stefan Wagner
    id: stefan-wagner
    last: Wagner
  author_string: Reid Pryzant, Kelly Shen, Dan Jurafsky, Stefan Wagner
  bibkey: pryzant-etal-2018-deconfounded
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1146
  month: June
  page_first: '1615'
  page_last: '1625'
  pages: "1615\u20131625"
  paper_id: '146'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1146.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1146.jpg
  title: Deconfounded Lexicon Induction for Interpretable Social Science
  title_html: Deconfounded Lexicon Induction for Interpretable Social Science
  url: https://www.aclweb.org/anthology/N18-1146
  year: '2018'
N18-1147:
  abstract: "This paper describes a novel application of NLP models to detect denial\
    \ of service attacks using only social media as evidence. Individual networks\
    \ are often slow in reporting attacks, so a detection system from public data\
    \ could better assist a response to a broad attack across multiple services. We\
    \ explore NLP methods to use social media as an indirect measure of network service\
    \ status. We describe two learning frameworks for this task: a feed-forward neural\
    \ network and a partially labeled LDA model. Both models outperform previous work\
    \ by significant margins (20% F1 score). We further show that the topic-based\
    \ model enables the first fine-grained analysis of how the public reacts to ongoing\
    \ network attacks, discovering multiple \u201Cstages\u201D of observation. This\
    \ is the first model that both detects network attacks (with best performance)\
    \ and provides an analysis of when and how the public interprets service outages.\
    \ We describe the models, present experiments on the largest twitter DDoS corpus\
    \ to date, and conclude with an analysis of public reactions based on the learned\
    \ model\u2019s output."
  address: New Orleans, Louisiana
  author:
  - first: Nathanael
    full: Nathanael Chambers
    id: nathanael-chambers
    last: Chambers
  - first: Ben
    full: Ben Fry
    id: ben-fry
    last: Fry
  - first: James
    full: James McMasters
    id: james-mcmasters
    last: McMasters
  author_string: Nathanael Chambers, Ben Fry, James McMasters
  bibkey: chambers-etal-2018-detecting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1147
  month: June
  page_first: '1626'
  page_last: '1635'
  pages: "1626\u20131635"
  paper_id: '147'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1147.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1147.jpg
  title: 'Detecting Denial-of-Service Attacks from Social Media Text: Applying NLP
    to Computer Security'
  title_html: 'Detecting Denial-of-Service Attacks from Social Media Text: Applying
    <span class="acl-fixed-case">NLP</span> to Computer Security'
  url: https://www.aclweb.org/anthology/N18-1147
  year: '2018'
N18-1148:
  abstract: Estimating label proportions in a target corpus is a type of measurement
    that is useful for answering certain types of social-scientific questions. While
    past work has described a number of relevant approaches, nearly all are based
    on an assumption which we argue is invalid for many problems, particularly when
    dealing with human annotations. In this paper, we identify and differentiate between
    two relevant data generating scenarios (intrinsic vs. extrinsic labels), introduce
    a simple but novel method which emphasizes the importance of calibration, and
    then analyze and experimentally validate the appropriateness of various methods
    for each of the two scenarios.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1148.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1148.Notes.pdf
  author:
  - first: Dallas
    full: Dallas Card
    id: dallas-card
    last: Card
  - first: Noah A.
    full: Noah A. Smith
    id: noah-a-smith
    last: Smith
  author_string: Dallas Card, Noah A. Smith
  bibkey: card-smith-2018-importance
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1148
  month: June
  page_first: '1636'
  page_last: '1646'
  pages: "1636\u20131646"
  paper_id: '148'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1148.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1148.jpg
  title: The Importance of Calibration for Estimating Proportions from Annotations
  title_html: The Importance of Calibration for Estimating Proportions from Annotations
  url: https://www.aclweb.org/anthology/N18-1148
  year: '2018'
N18-1149:
  abstract: "Peer reviewing is a central component in the scientific publishing process.\
    \ We present the first public dataset of scientific peer reviews available for\
    \ research purposes (PeerRead v1),1 providing an opportunity to study this important\
    \ artifact. The dataset consists of 14.7K paper drafts and the corresponding accept/reject\
    \ decisions in top-tier venues including ACL, NIPS and ICLR. The dataset also\
    \ includes 10.7K textual peer reviews written by experts for a subset of the papers.\
    \ We describe the data collection process and report interesting observed phenomena\
    \ in the peer reviews. We also propose two novel NLP tasks based on this dataset\
    \ and provide simple baseline models. In the first task, we show that simple models\
    \ can predict whether a paper is accepted with up to 21% error reduction compared\
    \ to the majority baseline. In the second task, we predict the numerical scores\
    \ of review aspects and show that simple models can outperform the mean baseline\
    \ for aspects with high variance such as \u2018originality\u2019 and \u2018impact\u2019\
    ."
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1149.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1149.Notes.pdf
  author:
  - first: Dongyeop
    full: Dongyeop Kang
    id: dongyeop-kang
    last: Kang
  - first: Waleed
    full: Waleed Ammar
    id: waleed-ammar
    last: Ammar
  - first: Bhavana
    full: Bhavana Dalvi
    id: bhavana-dalvi
    last: Dalvi
  - first: Madeleine
    full: Madeleine van Zuylen
    id: madeleine-van-zuylen
    last: van Zuylen
  - first: Sebastian
    full: Sebastian Kohlmeier
    id: sebastian-kohlmeier
    last: Kohlmeier
  - first: Eduard
    full: Eduard Hovy
    id: eduard-hovy
    last: Hovy
  - first: Roy
    full: Roy Schwartz
    id: roy-schwartz
    last: Schwartz
  author_string: Dongyeop Kang, Waleed Ammar, Bhavana Dalvi, Madeleine van Zuylen,
    Sebastian Kohlmeier, Eduard Hovy, Roy Schwartz
  bibkey: kang-etal-2018-dataset
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1149
  month: June
  page_first: '1647'
  page_last: '1661'
  pages: "1647\u20131661"
  paper_id: '149'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1149.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1149.jpg
  title: 'A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications'
  title_html: 'A Dataset of Peer Reviews (<span class="acl-fixed-case">P</span>eer<span
    class="acl-fixed-case">R</span>ead): Collection, Insights and <span class="acl-fixed-case">NLP</span>
    Applications'
  url: https://www.aclweb.org/anthology/N18-1149
  year: '2018'
N18-1150:
  abstract: We present deep communicating agents in an encoder-decoder architecture
    to address the challenges of representing a long document for abstractive summarization.
    With deep communicating agents, the task of encoding a long text is divided across
    multiple collaborating agents, each in charge of a subsection of the input text.
    These encoders are connected to a single decoder, trained end-to-end using reinforcement
    learning to generate a focused and coherent summary. Empirical results demonstrate
    that multiple communicating encoders lead to a higher quality summary compared
    to several strong baselines, including those based on a single encoder or multiple
    non-communicating encoders.
  address: New Orleans, Louisiana
  author:
  - first: Asli
    full: Asli Celikyilmaz
    id: asli-celikyilmaz
    last: Celikyilmaz
  - first: Antoine
    full: Antoine Bosselut
    id: antoine-bosselut
    last: Bosselut
  - first: Xiaodong
    full: Xiaodong He
    id: xiaodong-he
    last: He
  - first: Yejin
    full: Yejin Choi
    id: yejin-choi
    last: Choi
  author_string: Asli Celikyilmaz, Antoine Bosselut, Xiaodong He, Yejin Choi
  bibkey: celikyilmaz-etal-2018-deep
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1150
  month: June
  page_first: '1662'
  page_last: '1675'
  pages: "1662\u20131675"
  paper_id: '150'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1150.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1150.jpg
  title: Deep Communicating Agents for Abstractive Summarization
  title_html: Deep Communicating Agents for Abstractive Summarization
  url: https://www.aclweb.org/anthology/N18-1150
  year: '2018'
N18-1151:
  abstract: Existing keyphrase extraction methods suffer from data sparsity problem
    when they are conducted on short and informal texts, especially microblog messages.
    Enriching context is one way to alleviate this problem. Considering that conversations
    are formed by reposting and replying messages, they provide useful clues for recognizing
    essential content in target posts and are therefore helpful for keyphrase identification.
    In this paper, we present a neural keyphrase extraction framework for microblog
    posts that takes their conversation context into account, where four types of
    neural encoders, namely, averaged embedding, RNN, attention, and memory networks,
    are proposed to represent the conversation context. Experimental results on Twitter
    and Weibo datasets show that our framework with such encoders outperforms state-of-the-art
    approaches.
  address: New Orleans, Louisiana
  author:
  - first: Yingyi
    full: Yingyi Zhang
    id: yingyi-zhang
    last: Zhang
  - first: Jing
    full: Jing Li
    id: jing-li
    last: Li
  - first: Yan
    full: Yan Song
    id: yan-song
    last: Song
  - first: Chengzhi
    full: Chengzhi Zhang
    id: chengzhi-zhang
    last: Zhang
  author_string: Yingyi Zhang, Jing Li, Yan Song, Chengzhi Zhang
  bibkey: zhang-etal-2018-encoding
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1151
  month: June
  page_first: '1676'
  page_last: '1686'
  pages: "1676\u20131686"
  paper_id: '151'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1151.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1151.jpg
  title: Encoding Conversation Context for Neural Keyphrase Extraction from Microblog
    Posts
  title_html: Encoding Conversation Context for Neural Keyphrase Extraction from Microblog
    Posts
  url: https://www.aclweb.org/anthology/N18-1151
  year: '2018'
N18-1152:
  abstract: Automatic evaluation systems in the field of automatic summarization have
    been relying on the availability of gold standard summaries for over ten years.
    Gold standard summaries are expensive to obtain and often require the availability
    of domain experts to achieve high quality. In this paper, we propose an alternative
    evaluation approach based on pairwise preferences of sentences. In comparison
    to gold standard summaries, they are simpler and cheaper to obtain. In our experiments,
    we show that humans are able to provide useful feedback in the form of pairwise
    preferences. The new framework performs better than the three most popular versions
    of ROUGE with less expensive human input. We also show that our framework can
    reuse already available evaluation data and achieve even better results.
  address: New Orleans, Louisiana
  author:
  - first: Markus
    full: Markus Zopf
    id: markus-zopf
    last: Zopf
  author_string: Markus Zopf
  bibkey: zopf-2018-estimating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1152
  month: June
  page_first: '1687'
  page_last: '1696'
  pages: "1687\u20131696"
  paper_id: '152'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1152.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1152.jpg
  title: Estimating Summary Quality with Pairwise Preferences
  title_html: Estimating Summary Quality with Pairwise Preferences
  url: https://www.aclweb.org/anthology/N18-1152
  year: '2018'
N18-1153:
  abstract: Summarizing a document requires identifying the important parts of the
    document with an objective of providing a quick overview to a reader. However,
    a long article can span several topics and a single summary cannot do justice
    to all the topics. Further, the interests of readers can vary and the notion of
    importance can change across them. Existing summarization algorithms generate
    a single summary and are not capable of generating multiple summaries tuned to
    the interests of the readers. In this paper, we propose an attention based RNN
    framework to generate multiple summaries of a single document tuned to different
    topics of interest. Our method outperforms existing baselines and our results
    suggest that the attention of generative networks can be successfully biased to
    look at sentences relevant to a topic and effectively used to generate topic-tuned
    summaries.
  address: New Orleans, Louisiana
  author:
  - first: Kundan
    full: Kundan Krishna
    id: kundan-krishna
    last: Krishna
  - first: Balaji Vasan
    full: Balaji Vasan Srinivasan
    id: balaji-vasan-srinivasan
    last: Srinivasan
  author_string: Kundan Krishna, Balaji Vasan Srinivasan
  bibkey: krishna-srinivasan-2018-generating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1153
  month: June
  page_first: '1697'
  page_last: '1705'
  pages: "1697\u20131705"
  paper_id: '153'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1153.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1153.jpg
  title: Generating Topic-Oriented Summaries Using Neural Attention
  title_html: Generating Topic-Oriented Summaries Using Neural Attention
  url: https://www.aclweb.org/anthology/N18-1153
  year: '2018'
N18-1154:
  abstract: In order to alleviate data sparsity and overfitting problems in maximum
    likelihood estimation (MLE) for sequence prediction tasks, we propose the Generative
    Bridging Network (GBN), in which a novel bridge module is introduced to assist
    the training of the sequence prediction model (the generator network). Unlike
    MLE directly maximizing the conditional likelihood, the bridge extends the point-wise
    ground truth to a bridge distribution conditioned on it, and the generator is
    optimized to minimize their KL-divergence. Three different GBNs, namely uniform
    GBN, language-model GBN and coaching GBN, are proposed to penalize confidence,
    enhance language smoothness and relieve learning burden. Experiments conducted
    on two recognized sequence prediction tasks (machine translation and abstractive
    text summarization) show that our proposed GBNs can yield significant improvements
    over strong baselines. Furthermore, by analyzing samples drawn from different
    bridges, expected influences on the generator are verified.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1154.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1154.Notes.pdf
  author:
  - first: Wenhu
    full: Wenhu Chen
    id: wenhu-chen
    last: Chen
  - first: Guanlin
    full: Guanlin Li
    id: guanlin-li
    last: Li
  - first: Shuo
    full: Shuo Ren
    id: shuo-ren
    last: Ren
  - first: Shujie
    full: Shujie Liu
    id: shujie-liu
    last: Liu
  - first: Zhirui
    full: Zhirui Zhang
    id: zhirui-zhang
    last: Zhang
  - first: Mu
    full: Mu Li
    id: mu-li
    last: Li
  - first: Ming
    full: Ming Zhou
    id: ming-zhou
    last: Zhou
  author_string: Wenhu Chen, Guanlin Li, Shuo Ren, Shujie Liu, Zhirui Zhang, Mu Li,
    Ming Zhou
  bibkey: chen-etal-2018-generative
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1154
  month: June
  page_first: '1706'
  page_last: '1715'
  pages: "1706\u20131715"
  paper_id: '154'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1154.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1154.jpg
  title: Generative Bridging Network for Neural Sequence Prediction
  title_html: Generative Bridging Network for Neural Sequence Prediction
  url: https://www.aclweb.org/anthology/N18-1154
  year: '2018'
N18-1155:
  abstract: A sentence compression method using LSTM can generate fluent compressed
    sentences. However, the performance of this method is significantly degraded when
    compressing longer sentences since it does not explicitly handle syntactic features.
    To solve this problem, we propose a higher-order syntactic attention network (HiSAN)
    that can handle higher-order dependency features as an attention distribution
    on LSTM hidden states. Furthermore, to avoid the influence of incorrect parse
    results, we trained HiSAN by maximizing jointly the probability of a correct output
    with the attention distribution. Experimental results on Google sentence compression
    dataset showed that our method achieved the best performance on F1 as well as
    ROUGE-1,2 and L scores, 83.2, 82.9, 75.8 and 82.7, respectively. In human evaluation,
    our methods also outperformed baseline methods in both readability and informativeness.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1155.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1155.Notes.pdf
  author:
  - first: Hidetaka
    full: Hidetaka Kamigaito
    id: hidetaka-kamigaito
    last: Kamigaito
  - first: Katsuhiko
    full: Katsuhiko Hayashi
    id: katsuhiko-hayashi
    last: Hayashi
  - first: Tsutomu
    full: Tsutomu Hirao
    id: tsutomu-hirao
    last: Hirao
  - first: Masaaki
    full: Masaaki Nagata
    id: masaaki-nagata
    last: Nagata
  author_string: Hidetaka Kamigaito, Katsuhiko Hayashi, Tsutomu Hirao, Masaaki Nagata
  bibkey: kamigaito-etal-2018-higher
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1155
  month: June
  page_first: '1716'
  page_last: '1726'
  pages: "1716\u20131726"
  paper_id: '155'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1155.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1155.jpg
  title: Higher-Order Syntactic Attention Network for Longer Sentence Compression
  title_html: Higher-Order Syntactic Attention Network for Longer Sentence Compression
  url: https://www.aclweb.org/anthology/N18-1155
  year: '2018'
N18-1156:
  abstract: Storyline generation aims to extract events described on news articles
    under a certain news topic and reveal how those events evolve over time. Most
    approaches to storyline generation first train supervised models to extract events
    from news articles published in different time periods and then link relevant
    extracted events into coherent stories. They are domain dependent and cannot deal
    with unseen event types. To tackle this problem, approaches based on probabilistic
    graphic models jointly model the generations of events and storylines without
    the use of annotated data. However, the parameter inference procedure is too complex
    and models often require long time to converge. In this paper, we propose a novel
    neural network based approach to extract structured representations and evolution
    patterns of storylines without using annotated data. In this model, title and
    main body of a news article are assumed to share the similar storyline distribution.
    Moreover, similar documents described in neighboring time periods are assumed
    to share similar storyline distributions. Based on these assumptions, structured
    representations and evolution patterns of storylines can be extracted. The proposed
    model has been evaluated on three news corpora and the experimental results show
    that it outperforms state-of-the-art approaches for storyline generation on both
    accuracy and efficiency.
  address: New Orleans, Louisiana
  author:
  - first: Deyu
    full: Deyu Zhou
    id: deyu-zhou
    last: Zhou
  - first: Linsen
    full: Linsen Guo
    id: linsen-guo
    last: Guo
  - first: Yulan
    full: Yulan He
    id: yulan-he
    last: He
  author_string: Deyu Zhou, Linsen Guo, Yulan He
  bibkey: zhou-etal-2018-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1156
  month: June
  page_first: '1727'
  page_last: '1736'
  pages: "1727\u20131736"
  paper_id: '156'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1156.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1156.jpg
  title: Neural Storyline Extraction Model for Storyline Generation from News Articles
  title_html: Neural Storyline Extraction Model for Storyline Generation from News
    Articles
  url: https://www.aclweb.org/anthology/N18-1156
  year: '2018'
N18-1157:
  abstract: 'Submodular maximization with the greedy algorithm has been studied as
    an effective approach to extractive summarization. This approach is known to have
    three advantages: its applicability to many useful submodular objective functions,
    the efficiency of the greedy algorithm, and the provable performance guarantee.
    However, when it comes to compressive summarization, we are currently missing
    a counterpart of the extractive method based on submodularity. In this paper,
    we propose a fast greedy method for compressive summarization. Our method is applicable
    to any monotone submodular objective function, including many functions well-suited
    for document summarization. We provide an approximation guarantee of our greedy
    algorithm. Experiments show that our method is about 100 to 400 times faster than
    an existing method based on integer-linear-programming (ILP) formulations and
    that our method empirically achieves more than 95%-approximation.'
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1157.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1157.Notes.pdf
  author:
  - first: Shinsaku
    full: Shinsaku Sakaue
    id: shinsaku-sakaue
    last: Sakaue
  - first: Tsutomu
    full: Tsutomu Hirao
    id: tsutomu-hirao
    last: Hirao
  - first: Masaaki
    full: Masaaki Nishino
    id: masaaki-nishino
    last: Nishino
  - first: Masaaki
    full: Masaaki Nagata
    id: masaaki-nagata
    last: Nagata
  author_string: Shinsaku Sakaue, Tsutomu Hirao, Masaaki Nishino, Masaaki Nagata
  bibkey: sakaue-etal-2018-provable
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1157
  month: June
  page_first: '1737'
  page_last: '1746'
  pages: "1737\u20131746"
  paper_id: '157'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1157.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1157.jpg
  title: Provable Fast Greedy Compressive Summarization with Any Monotone Submodular
    Function
  title_html: Provable Fast Greedy Compressive Summarization with Any Monotone Submodular
    Function
  url: https://www.aclweb.org/anthology/N18-1157
  year: '2018'
N18-1158:
  abstract: Single document summarization is the task of producing a shorter version
    of a document while preserving its principal information content. In this paper
    we conceptualize extractive summarization as a sentence ranking task and propose
    a novel training algorithm which globally optimizes the ROUGE evaluation metric
    through a reinforcement learning objective. We use our algorithm to train a neural
    summarization model on the CNN and DailyMail datasets and demonstrate experimentally
    that it outperforms state-of-the-art extractive and abstractive systems when evaluated
    automatically and by humans.
  address: New Orleans, Louisiana
  author:
  - first: Shashi
    full: Shashi Narayan
    id: shashi-narayan
    last: Narayan
  - first: Shay B.
    full: Shay B. Cohen
    id: shay-b-cohen
    last: Cohen
  - first: Mirella
    full: Mirella Lapata
    id: mirella-lapata
    last: Lapata
  author_string: Shashi Narayan, Shay B. Cohen, Mirella Lapata
  bibkey: narayan-etal-2018-ranking
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1158
  month: June
  page_first: '1747'
  page_last: '1759'
  pages: "1747\u20131759"
  paper_id: '158'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1158.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1158.jpg
  title: Ranking Sentences for Extractive Summarization with Reinforcement Learning
  title_html: Ranking Sentences for Extractive Summarization with Reinforcement Learning
  url: https://www.aclweb.org/anthology/N18-1158
  year: '2018'
N18-1159:
  abstract: This work introduces a new problem, relational summarization, in which
    the goal is to generate a natural language summary of the relationship between
    two lexical items in a corpus, without reference to a knowledge base. Motivated
    by the needs of novel user interfaces, we define the task and give examples of
    its application. We also present a new query-focused method for finding natural
    language sentences which express relationships. Our method allows for summarization
    of more than two times more query pairs than baseline relation extractors, while
    returning measurably more readable output. Finally, to help guide future work,
    we analyze the challenges of relational summarization using both a news and a
    social media corpus.
  address: New Orleans, Louisiana
  author:
  - first: Abram
    full: Abram Handler
    id: abram-handler
    last: Handler
  - first: Brendan
    full: "Brendan O\u2019Connor"
    id: brendan-oconnor
    last: "O\u2019Connor"
  author_string: "Abram Handler, Brendan O\u2019Connor"
  bibkey: handler-oconnor-2018-relational
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1159
  month: June
  page_first: '1760'
  page_last: '1769'
  pages: "1760\u20131769"
  paper_id: '159'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1159.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1159.jpg
  title: Relational Summarization for Corpus Analysis
  title_html: Relational Summarization for Corpus Analysis
  url: https://www.aclweb.org/anthology/N18-1159
  year: '2018'
N18-1160:
  abstract: "This work takes a first step toward movie content analysis by tackling\
    \ the novel task of movie overview generation. Overviews are natural language\
    \ texts that give a first impression of a movie, describing aspects such as its\
    \ genre, plot, mood, or artistic style. We create a dataset that consists of movie\
    \ scripts, attribute-value pairs for the movies\u2019 aspects, as well as overviews,\
    \ which we extract from an online database. We present a novel end-to-end model\
    \ for overview generation, consisting of a multi-label encoder for identifying\
    \ screenplay attributes, and an LSTM decoder to generate natural language sentences\
    \ conditioned on the identified attributes. Automatic and human evaluation show\
    \ that the encoder is able to reliably assign good labels for the movie\u2019\
    s attributes, and the overviews provide descriptions of the movie\u2019s content\
    \ which are informative and faithful."
  address: New Orleans, Louisiana
  author:
  - first: Philip John
    full: Philip John Gorinski
    id: philip-gorinski
    last: Gorinski
  - first: Mirella
    full: Mirella Lapata
    id: mirella-lapata
    last: Lapata
  author_string: Philip John Gorinski, Mirella Lapata
  bibkey: gorinski-lapata-2018-whats
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1160
  month: June
  page_first: '1770'
  page_last: '1781'
  pages: "1770\u20131781"
  paper_id: '160'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1160.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1160.jpg
  title: "What\u2019s This Movie About? A Joint Neural Network Architecture for Movie\
    \ Content Analysis"
  title_html: "What\u2019s This Movie About? A Joint Neural Network Architecture for\
    \ Movie Content Analysis"
  url: https://www.aclweb.org/anthology/N18-1160
  year: '2018'
N18-1161:
  abstract: The task of automatic text summarization is to generate a short text that
    summarizes the most important information in a given set of documents. Sentence
    regression is an emerging branch in automatic text summarizations. Its key idea
    is to estimate the importance of information via learned utility scores for individual
    sentences. These scores are then used for selecting sentences from the source
    documents, typically according to a greedy selection strategy. Recently proposed
    state-of-the-art models learn to predict ROUGE recall scores of individual sentences,
    which seems reasonable since the final summaries are evaluated according to ROUGE
    recall. In this paper, we show in extensive experiments that following this intuition
    leads to suboptimal results and that learning to predict ROUGE precision scores
    leads to better results. The crucial difference is to aim not at covering as much
    information as possible but at wasting as little space as possible in every greedy
    step.
  address: New Orleans, Louisiana
  author:
  - first: Markus
    full: Markus Zopf
    id: markus-zopf
    last: Zopf
  - first: Eneldo
    full: "Eneldo Loza Menc\xEDa"
    id: eneldo-loza-mencia
    last: "Loza Menc\xEDa"
  - first: Johannes
    full: "Johannes F\xFCrnkranz"
    id: johannes-furnkranz
    last: "F\xFCrnkranz"
  author_string: "Markus Zopf, Eneldo Loza Menc\xEDa, Johannes F\xFCrnkranz"
  bibkey: zopf-etal-2018-scores
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1161
  month: June
  page_first: '1782'
  page_last: '1791'
  pages: "1782\u20131791"
  paper_id: '161'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1161.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1161.jpg
  title: Which Scores to Predict in Sentence Regression for Text Summarization?
  title_html: Which Scores to Predict in Sentence Regression for Text Summarization?
  url: https://www.aclweb.org/anthology/N18-1161
  year: '2018'
N18-1162:
  abstract: Variational autoencoders (VAE) combined with hierarchical RNNs have emerged
    as a powerful framework for conversation modeling. However, they suffer from the
    notorious degeneration problem, where the decoders learn to ignore latent variables
    and reduce to vanilla RNNs. We empirically show that this degeneracy occurs mostly
    due to two reasons. First, the expressive power of hierarchical RNN decoders is
    often high enough to model the data using only its decoding distributions without
    relying on the latent variables. Second, the conditional VAE structure whose generation
    process is conditioned on a context, makes the range of training targets very
    sparse; that is, the RNN decoders can easily overfit to the training data ignoring
    the latent variables. To solve the degeneration problem, we propose a novel model
    named Variational Hierarchical Conversation RNNs (VHCR), involving two key ideas
    of (1) using a hierarchical structure of latent variables, and (2) exploiting
    an utterance drop regularization. With evaluations on two datasets of Cornell
    Movie Dialog and Ubuntu Dialog Corpus, we show that our VHCR successfully utilizes
    latent variables and outperforms state-of-the-art models for conversation generation.
    Moreover, it can perform several new utterance control tasks, thanks to its hierarchical
    latent structure.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1162.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1162.Notes.pdf
  - filename: http://vimeo.com/277671819
    type: video
    url: http://vimeo.com/277671819
  author:
  - first: Yookoon
    full: Yookoon Park
    id: yookoon-park
    last: Park
  - first: Jaemin
    full: Jaemin Cho
    id: jaemin-cho
    last: Cho
  - first: Gunhee
    full: Gunhee Kim
    id: gunhee-kim
    last: Kim
  author_string: Yookoon Park, Jaemin Cho, Gunhee Kim
  bibkey: park-etal-2018-hierarchical
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1162
  month: June
  page_first: '1792'
  page_last: '1801'
  pages: "1792\u20131801"
  paper_id: '162'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1162.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1162.jpg
  title: A Hierarchical Latent Structure for Variational Conversation Modeling
  title_html: A Hierarchical Latent Structure for Variational Conversation Modeling
  url: https://www.aclweb.org/anthology/N18-1162
  year: '2018'
N18-1163:
  abstract: Virtual agents are becoming a prominent channel of interaction in customer
    service. Not all customer interactions are smooth, however, and some can become
    almost comically bad. In such instances, a human agent might need to step in and
    salvage the conversation. Detecting bad conversations is important since disappointing
    customer service may threaten customer loyalty and impact revenue. In this paper,
    we outline an approach to detecting such egregious conversations, using behavioral
    cues from the user, patterns in agent responses, and user-agent interaction. Using
    logs of two commercial systems, we show that using these features improves the
    detection F1-score by around 20% over using textual features alone. In addition,
    we show that those features are common across two quite different domains and,
    arguably, universal.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/277671902
    type: video
    url: http://vimeo.com/277671902
  author:
  - first: Tommy
    full: Tommy Sandbank
    id: tommy-sandbank
    last: Sandbank
  - first: Michal
    full: Michal Shmueli-Scheuer
    id: michal-shmueli-scheuer
    last: Shmueli-Scheuer
  - first: Jonathan
    full: Jonathan Herzig
    id: jonathan-herzig
    last: Herzig
  - first: David
    full: David Konopnicki
    id: david-konopnicki
    last: Konopnicki
  - first: John
    full: John Richards
    id: john-richards
    last: Richards
  - first: David
    full: David Piorkowski
    id: david-piorkowski
    last: Piorkowski
  author_string: Tommy Sandbank, Michal Shmueli-Scheuer, Jonathan Herzig, David Konopnicki,
    John Richards, David Piorkowski
  bibkey: sandbank-etal-2018-detecting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1163
  month: June
  page_first: '1802'
  page_last: '1811'
  pages: "1802\u20131811"
  paper_id: '163'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1163.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1163.jpg
  title: Detecting Egregious Conversations between Customers and Virtual Agents
  title_html: Detecting Egregious Conversations between Customers and Virtual Agents
  url: https://www.aclweb.org/anthology/N18-1163
  year: '2018'
N18-1164:
  abstract: An enormous amount of conversation occurs online every day, such as on
    chat platforms where multiple conversations may take place concurrently. Interleaved
    conversations lead to difficulties in not only following discussions but also
    retrieving relevant information from simultaneous messages. Conversation disentanglement
    aims to separate intermingled messages into detached conversations. In this paper,
    we propose to leverage representation learning for conversation disentanglement.
    A Siamese hierarchical convolutional neural network (SHCNN), which integrates
    local and more global representations of a message, is first presented to estimate
    the conversation-level similarity between closely posted messages. With the estimated
    similarity scores, our algorithm for conversation identification by similarity
    ranking (CISIR) then derives conversations based on high-confidence message pairs
    and pairwise redundancy. Experiments were conducted with four publicly available
    datasets of conversations from Reddit and IRC channels. The experimental results
    show that our approach significantly outperforms comparative baselines in both
    pairwise similarity estimation and conversation disentanglement.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/277671673
    type: video
    url: http://vimeo.com/277671673
  author:
  - first: Jyun-Yu
    full: Jyun-Yu Jiang
    id: jyun-yu-jiang
    last: Jiang
  - first: Francine
    full: Francine Chen
    id: francine-chen
    last: Chen
  - first: Yan-Ying
    full: Yan-Ying Chen
    id: yan-ying-chen
    last: Chen
  - first: Wei
    full: Wei Wang
    id: wei-wang
    last: Wang
  author_string: Jyun-Yu Jiang, Francine Chen, Yan-Ying Chen, Wei Wang
  bibkey: jiang-etal-2018-learning-disentangle
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1164
  month: June
  page_first: '1812'
  page_last: '1822'
  pages: "1812\u20131822"
  paper_id: '164'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1164.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1164.jpg
  title: Learning to Disentangle Interleaved Conversational Threads with a Siamese
    Hierarchical Network and Similarity Ranking
  title_html: Learning to Disentangle Interleaved Conversational Threads with a <span
    class="acl-fixed-case">S</span>iamese Hierarchical Network and Similarity Ranking
  url: https://www.aclweb.org/anthology/N18-1164
  year: '2018'
N18-1165:
  abstract: Inferring missing links in knowledge graphs (KG) has attracted a lot of
    attention from the research community. In this paper, we tackle a practical query
    answering task involving predicting the relation of a given entity pair. We frame
    this prediction problem as an inference problem in a probabilistic graphical model
    and aim at resolving it from a variational inference perspective. In order to
    model the relation between the query entity pair, we assume that there exists
    an underlying latent variable (paths connecting two nodes) in the KG, which carries
    the equivalent semantics of their relations. However, due to the intractability
    of connections in large KGs, we propose to use variation inference to maximize
    the evidence lower bound. More specifically, our framework (Diva) is composed
    of three modules, i.e. a posterior approximator, a prior (path finder), and a
    likelihood (path reasoner). By using variational inference, we are able to incorporate
    them closely into a unified architecture and jointly optimize them to perform
    KG reasoning. With active interactions among these sub-modules, Diva is better
    at handling noise and coping with more complex reasoning scenarios. In order to
    evaluate our method, we conduct the experiment of the link prediction task on
    multiple datasets and achieve state-of-the-art performances on both datasets.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/277673049
    type: video
    url: http://vimeo.com/277673049
  author:
  - first: Wenhu
    full: Wenhu Chen
    id: wenhu-chen
    last: Chen
  - first: Wenhan
    full: Wenhan Xiong
    id: wenhan-xiong
    last: Xiong
  - first: Xifeng
    full: Xifeng Yan
    id: xifeng-yan
    last: Yan
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  author_string: Wenhu Chen, Wenhan Xiong, Xifeng Yan, William Yang Wang
  bibkey: chen-etal-2018-variational
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1165
  month: June
  page_first: '1823'
  page_last: '1832'
  pages: "1823\u20131832"
  paper_id: '165'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1165.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1165.jpg
  title: Variational Knowledge Graph Reasoning
  title_html: Variational Knowledge Graph Reasoning
  url: https://www.aclweb.org/anthology/N18-1165
  year: '2018'
N18-1166:
  abstract: "Recognizing temporal relations among events and time expressions has\
    \ been an essential but challenging task in natural language processing. Conventional\
    \ annotation of judging temporal relations puts a heavy load on annotators. In\
    \ reality, the existing annotated corpora include annotations on only \u201Csalient\u201D\
    \ event pairs, or on pairs in a fixed window of sentences. In this paper, we propose\
    \ a new approach to obtain temporal relations from absolute time value (a.k.a.\
    \ time anchors), which is suitable for texts containing rich temporal information\
    \ such as news articles. We start from time anchors for events and time expressions,\
    \ and temporal relation annotations are induced automatically by computing relative\
    \ order of two time anchors. This proposal shows several advantages over the current\
    \ methods for temporal relation annotation: it requires less annotation effort,\
    \ can induce inter-sentence relations easily, and increases informativeness of\
    \ temporal relations. We compare the empirical statistics and automatic recognition\
    \ results with our data against a previous temporal relation corpus. We also reveal\
    \ that our data contributes to a significant improvement of the downstream time\
    \ anchor prediction task, demonstrating 14.1 point increase in overall accuracy."
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/277672694
    type: video
    url: http://vimeo.com/277672694
  author:
  - first: Fei
    full: Fei Cheng
    id: fei-cheng
    last: Cheng
  - first: Yusuke
    full: Yusuke Miyao
    id: yusuke-miyao
    last: Miyao
  author_string: Fei Cheng, Yusuke Miyao
  bibkey: cheng-miyao-2018-inducing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1166
  month: June
  page_first: '1833'
  page_last: '1843'
  pages: "1833\u20131843"
  paper_id: '166'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1166.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1166.jpg
  title: Inducing Temporal Relations from Time Anchor Annotation
  title_html: Inducing Temporal Relations from Time Anchor Annotation
  url: https://www.aclweb.org/anthology/N18-1166
  year: '2018'
N18-1167:
  abstract: "Entity Linking (EL) systems aim to automatically map mentions of an entity\
    \ in text to the corresponding entity in a Knowledge Graph (KG). Degree of connectivity\
    \ of an entity in the KG directly affects an EL system\u2019s ability to correctly\
    \ link mentions in text to the entity in KG. This causes many EL systems to perform\
    \ well for entities well connected to other entities in KG, bringing into focus\
    \ the role of KG density in EL. In this paper, we propose Entity Linking using\
    \ Densified Knowledge Graphs (ELDEN). ELDEN is an EL system which first densifies\
    \ the KG with co-occurrence statistics from a large text corpus, and then uses\
    \ the densified KG to train entity embeddings. Entity similarity measured using\
    \ these trained entity embeddings result in improved EL. ELDEN outperforms state-of-the-art\
    \ EL system on benchmark datasets. Due to such densification, ELDEN performs well\
    \ for sparsely connected entities in the KG too. ELDEN\u2019s approach is simple,\
    \ yet effective. We have made ELDEN\u2019s code and data publicly available."
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1167.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1167.Notes.pdf
  - filename: http://vimeo.com/277673016
    type: video
    url: http://vimeo.com/277673016
  author:
  - first: Priya
    full: Priya Radhakrishnan
    id: priya-radhakrishnan
    last: Radhakrishnan
  - first: Partha
    full: Partha Talukdar
    id: partha-talukdar
    last: Talukdar
  - first: Vasudeva
    full: Vasudeva Varma
    id: vasudeva-varma
    last: Varma
  author_string: Priya Radhakrishnan, Partha Talukdar, Vasudeva Varma
  bibkey: radhakrishnan-etal-2018-elden
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1167
  month: June
  page_first: '1844'
  page_last: '1853'
  pages: "1844\u20131853"
  paper_id: '167'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1167.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1167.jpg
  title: 'ELDEN: Improved Entity Linking Using Densified Knowledge Graphs'
  title_html: '<span class="acl-fixed-case">ELDEN</span>: Improved Entity Linking
    Using Densified Knowledge Graphs'
  url: https://www.aclweb.org/anthology/N18-1167
  year: '2018'
N18-1168:
  abstract: In this paper, we propose to study the problem of court view generation
    from the fact description in a criminal case. The task aims to improve the interpretability
    of charge prediction systems and help automatic legal document generation. We
    formulate this task as a text-to-text natural language generation (NLG) problem.
    Sequence-to-sequence model has achieved cutting-edge performances in many NLG
    tasks. However, due to the non-distinctions of fact descriptions, it is hard for
    Seq2Seq model to generate charge-discriminative court views. In this work, we
    explore charge labels to tackle this issue. We propose a label-conditioned Seq2Seq
    model with attention for this problem, to decode court views conditioned on encoded
    charge labels. Experimental results show the effectiveness of our method.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/277673836
    type: video
    url: http://vimeo.com/277673836
  author:
  - first: Hai
    full: Hai Ye
    id: hai-ye
    last: Ye
  - first: Xin
    full: Xin Jiang
    id: xin-jiang
    last: Jiang
  - first: Zhunchen
    full: Zhunchen Luo
    id: zhunchen-luo
    last: Luo
  - first: Wenhan
    full: Wenhan Chao
    id: wenhan-chao
    last: Chao
  author_string: Hai Ye, Xin Jiang, Zhunchen Luo, Wenhan Chao
  bibkey: ye-etal-2018-interpretable
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1168
  month: June
  page_first: '1854'
  page_last: '1864'
  pages: "1854\u20131864"
  paper_id: '168'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1168.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1168.jpg
  title: 'Interpretable Charge Predictions for Criminal Cases: Learning to Generate
    Court Views from Fact Descriptions'
  title_html: 'Interpretable Charge Predictions for Criminal Cases: Learning to Generate
    Court Views from Fact Descriptions'
  url: https://www.aclweb.org/anthology/N18-1168
  year: '2018'
N18-1169:
  abstract: "We consider the task of text attribute transfer: transforming a sentence\
    \ to alter a specific attribute (e.g., sentiment) while preserving its attribute-independent\
    \ content (e.g., \u201Cscreen is just the right size\u201D to \u201Cscreen is\
    \ too small\u201D). Our training data includes only sentences labeled with their\
    \ attribute (e.g., positive and negative), but not pairs of sentences that only\
    \ differ in the attributes, so we must learn to disentangle attributes from attribute-independent\
    \ content in an unsupervised way. Previous work using adversarial methods has\
    \ struggled to produce high-quality outputs. In this paper, we propose simpler\
    \ methods motivated by the observation that text attributes are often marked by\
    \ distinctive phrases (e.g., \u201Ctoo small\u201D). Our strongest method extracts\
    \ content words by deleting phrases associated with the sentence\u2019s original\
    \ attribute value, retrieves new phrases associated with the target attribute,\
    \ and uses a neural model to fluently combine these into a final output. Based\
    \ on human evaluation, our best method generates grammatical and appropriate responses\
    \ on 22% more inputs than the best previous system, averaged over three attribute\
    \ transfer datasets: altering sentiment of reviews on Yelp, altering sentiment\
    \ of reviews on Amazon, and altering image captions to be more romantic or humorous."
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1169.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1169.Notes.pdf
  - filename: http://vimeo.com/277673818
    type: video
    url: http://vimeo.com/277673818
  author:
  - first: Juncen
    full: Juncen Li
    id: juncen-li
    last: Li
  - first: Robin
    full: Robin Jia
    id: robin-jia
    last: Jia
  - first: He
    full: He He
    id: he-he
    last: He
  - first: Percy
    full: Percy Liang
    id: percy-liang
    last: Liang
  author_string: Juncen Li, Robin Jia, He He, Percy Liang
  bibkey: li-etal-2018-delete
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1169
  month: June
  page_first: '1865'
  page_last: '1874'
  pages: "1865\u20131874"
  paper_id: '169'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1169.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1169.jpg
  title: 'Delete, Retrieve, Generate: a Simple Approach to Sentiment and Style Transfer'
  title_html: 'Delete, Retrieve, Generate: a Simple Approach to Sentiment and Style
    Transfer'
  url: https://www.aclweb.org/anthology/N18-1169
  year: '2018'
N18-1170:
  abstract: "We propose syntactically controlled paraphrase networks (SCPNs) and use\
    \ them to generate adversarial examples. Given a sentence and a target syntactic\
    \ form (e.g., a constituency parse), SCPNs are trained to produce a paraphrase\
    \ of the sentence with the desired syntax. We show it is possible to create training\
    \ data for this task by first doing backtranslation at a very large scale, and\
    \ then using a parser to label the syntactic transformations that naturally occur\
    \ during this process. Such data allows us to train a neural encoder-decoder model\
    \ with extra inputs to specify the target syntax. A combination of automated and\
    \ human evaluations show that SCPNs generate paraphrases that follow their target\
    \ specifications without decreasing paraphrase quality when compared to baseline\
    \ (uncontrolled) paraphrase systems. Furthermore, they are more capable of generating\
    \ syntactically adversarial examples that both (1) \u201Cfool\u201D pretrained\
    \ models and (2) improve the robustness of these models to syntactic variation\
    \ when used to augment their training data."
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/277673796
    type: video
    url: http://vimeo.com/277673796
  author:
  - first: Mohit
    full: Mohit Iyyer
    id: mohit-iyyer
    last: Iyyer
  - first: John
    full: John Wieting
    id: john-wieting
    last: Wieting
  - first: Kevin
    full: Kevin Gimpel
    id: kevin-gimpel
    last: Gimpel
  - first: Luke
    full: Luke Zettlemoyer
    id: luke-zettlemoyer
    last: Zettlemoyer
  author_string: Mohit Iyyer, John Wieting, Kevin Gimpel, Luke Zettlemoyer
  bibkey: iyyer-etal-2018-adversarial
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1170
  month: June
  page_first: '1875'
  page_last: '1885'
  pages: "1875\u20131885"
  paper_id: '170'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1170.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1170.jpg
  title: Adversarial Example Generation with Syntactically Controlled Paraphrase Networks
  title_html: Adversarial Example Generation with Syntactically Controlled Paraphrase
    Networks
  url: https://www.aclweb.org/anthology/N18-1170
  year: '2018'
N18-1171:
  abstract: "Sentiment analysis is used as a proxy to measure human emotion, where\
    \ the objective is to categorize text according to some predefined notion of sentiment.\
    \ Sentiment analysis datasets are typically constructed with gold-standard sentiment\
    \ labels, assigned based on the results of manual annotations. When working with\
    \ such annotations, it is common for dataset constructors to discard \u201Cnoisy\u201D\
    \ or \u201Ccontroversial\u201D data where there is significant disagreement on\
    \ the proper label. In datasets constructed for the purpose of Twitter sentiment\
    \ analysis (TSA), these controversial examples can compose over 30% of the originally\
    \ annotated data. We argue that the removal of such data is a problematic trend\
    \ because, when performing real-time sentiment classification of short-text, an\
    \ automated system cannot know a priori which samples would fall into this category\
    \ of disputed sentiment. We therefore propose the notion of a \u201Ccomplicated\u201D\
    \ class of sentiment to categorize such text, and argue that its inclusion in\
    \ the short-text sentiment analysis framework will improve the quality of automated\
    \ sentiment analysis systems as they are implemented in real-world settings. We\
    \ motivate this argument by building and analyzing a new publicly available TSA\
    \ dataset of over 7,000 tweets annotated with 5x coverage, named MTSA. Our analysis\
    \ of classifier performance over our dataset offers insights into sentiment analysis\
    \ dataset and model design, how current techniques would perform in the real world,\
    \ and how researchers should handle difficult data."
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1171.Datasets.zip
    type: dataset
    url: https://www.aclweb.org/anthology/attachments/N18-1171.Datasets.zip
  - filename: http://vimeo.com/277672337
    type: video
    url: http://vimeo.com/277672337
  author:
  - first: Kian
    full: Kian Kenyon-Dean
    id: kian-kenyon-dean
    last: Kenyon-Dean
  - first: Eisha
    full: Eisha Ahmed
    id: eisha-ahmed
    last: Ahmed
  - first: Scott
    full: Scott Fujimoto
    id: scott-fujimoto
    last: Fujimoto
  - first: Jeremy
    full: Jeremy Georges-Filteau
    id: jeremy-georges-filteau
    last: Georges-Filteau
  - first: Christopher
    full: Christopher Glasz
    id: christopher-glasz
    last: Glasz
  - first: Barleen
    full: Barleen Kaur
    id: barleen-kaur
    last: Kaur
  - first: Auguste
    full: Auguste Lalande
    id: auguste-lalande
    last: Lalande
  - first: Shruti
    full: Shruti Bhanderi
    id: shruti-bhanderi
    last: Bhanderi
  - first: Robert
    full: Robert Belfer
    id: robert-belfer
    last: Belfer
  - first: Nirmal
    full: Nirmal Kanagasabai
    id: nirmal-kanagasabai
    last: Kanagasabai
  - first: Roman
    full: Roman Sarrazingendron
    id: roman-sarrazingendron
    last: Sarrazingendron
  - first: Rohit
    full: Rohit Verma
    id: rohit-verma
    last: Verma
  - first: Derek
    full: Derek Ruths
    id: derek-ruths
    last: Ruths
  author_string: Kian Kenyon-Dean, Eisha Ahmed, Scott Fujimoto, Jeremy Georges-Filteau,
    Christopher Glasz, Barleen Kaur, Auguste Lalande, Shruti Bhanderi, Robert Belfer,
    Nirmal Kanagasabai, Roman Sarrazingendron, Rohit Verma, Derek Ruths
  bibkey: kenyon-dean-etal-2018-sentiment
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1171
  month: June
  page_first: '1886'
  page_last: '1895'
  pages: "1886\u20131895"
  paper_id: '171'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1171.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1171.jpg
  title: "Sentiment Analysis: It\u2019s Complicated!"
  title_html: "Sentiment Analysis: It\u2019s Complicated!"
  url: https://www.aclweb.org/anthology/N18-1171
  year: '2018'
N18-1172:
  abstract: We combine multi-task learning and semi-supervised learning by inducing
    a joint embedding space between disparate label spaces and learning transfer functions
    between label embeddings, enabling us to jointly leverage unlabelled data and
    auxiliary, annotated datasets. We evaluate our approach on a variety of tasks
    with disparate label spaces. We outperform strong single and multi-task baselines
    and achieve a new state of the art for aspect-based and topic-based sentiment
    analysis.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/277671362
    type: video
    url: http://vimeo.com/277671362
  author:
  - first: Isabelle
    full: Isabelle Augenstein
    id: isabelle-augenstein
    last: Augenstein
  - first: Sebastian
    full: Sebastian Ruder
    id: sebastian-ruder
    last: Ruder
  - first: Anders
    full: "Anders S\xF8gaard"
    id: anders-sogaard
    last: "S\xF8gaard"
  author_string: "Isabelle Augenstein, Sebastian Ruder, Anders S\xF8gaard"
  bibkey: augenstein-etal-2018-multi
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1172
  month: June
  page_first: '1896'
  page_last: '1906'
  pages: "1896\u20131906"
  paper_id: '172'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1172.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1172.jpg
  title: Multi-Task Learning of Pairwise Sequence Classification Tasks over Disparate
    Label Spaces
  title_html: Multi-Task Learning of Pairwise Sequence Classification Tasks over Disparate
    Label Spaces
  url: https://www.aclweb.org/anthology/N18-1172
  year: '2018'
N18-1173:
  abstract: Predicting the emotional value of lexical items is a well-known problem
    in sentiment analysis. While research has focused on polarity for quite a long
    time, meanwhile this early focus has been shifted to more expressive emotion representation
    models (such as Basic Emotions or Valence-Arousal-Dominance). This change resulted
    in a proliferation of heterogeneous formats and, in parallel, often small-sized,
    non-interoperable resources (lexicons and corpus annotations). In particular,
    the limitations in size hampered the application of deep learning methods in this
    area because they typically require large amounts of input data. We here present
    a solution to get around this language data bottleneck by rephrasing word emotion
    induction as a multi-task learning problem. In this approach, the prediction of
    each independent emotion dimension is considered as an individual task and hidden
    layers are shared between these dimensions. We investigate whether multi-task
    learning is more advantageous than single-task learning for emotion prediction
    by comparing our model against a wide range of alternative emotion and polarity
    induction methods featuring 9 typologically diverse languages and a total of 15
    conditions. Our model turns out to outperform each one of them. Against all odds,
    the proposed deep learning approach yields the largest gain on the smallest data
    sets, merely composed of one thousand samples.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/277671439
    type: video
    url: http://vimeo.com/277671439
  author:
  - first: Sven
    full: Sven Buechel
    id: sven-buechel
    last: Buechel
  - first: Udo
    full: Udo Hahn
    id: udo-hahn
    last: Hahn
  author_string: Sven Buechel, Udo Hahn
  bibkey: buechel-hahn-2018-word
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1173
  month: June
  page_first: '1907'
  page_last: '1918'
  pages: "1907\u20131918"
  paper_id: '173'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1173.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1173.jpg
  title: Word Emotion Induction for Multiple Languages as a Deep Multi-Task Learning
    Problem
  title_html: Word Emotion Induction for Multiple Languages as a Deep Multi-Task Learning
    Problem
  url: https://www.aclweb.org/anthology/N18-1173
  year: '2018'
N18-1174:
  abstract: "We often talk about events that impact us positively or negatively. For\
    \ example \u201CI got a job\u201D is good news, but \u201CI lost my job\u201D\
    \ is bad news. When we discuss an event, we not only understand its affective\
    \ polarity but also the reason why the event is beneficial or detrimental. For\
    \ example, getting or losing a job has affective polarity primarily because it\
    \ impacts us financially. Our work aims to categorize affective events based upon\
    \ human need categories that often explain people\u2019s motivations and desires:\
    \ PHYSIOLOGICAL, HEALTH, LEISURE, SOCIAL, FINANCIAL, COGNITION, and FREEDOM. We\
    \ create classification models based on event expressions as well as models that\
    \ use contexts surrounding event mentions. We also design a co-training model\
    \ that learns from unlabeled data by simultaneously training event expression\
    \ and event context classifiers in an iterative learning process. Our results\
    \ show that co-training performs well, producing substantially better results\
    \ than the individual classifiers."
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/277671591
    type: video
    url: http://vimeo.com/277671591
  author:
  - first: Haibo
    full: Haibo Ding
    id: haibo-ding
    last: Ding
  - first: Ellen
    full: Ellen Riloff
    id: ellen-riloff
    last: Riloff
  author_string: Haibo Ding, Ellen Riloff
  bibkey: ding-riloff-2018-human
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1174
  month: June
  page_first: '1919'
  page_last: '1929'
  pages: "1919\u20131929"
  paper_id: '174'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1174.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1174.jpg
  title: Human Needs Categorization of Affective Events Using Labeled and Unlabeled
    Data
  title_html: Human Needs Categorization of Affective Events Using Labeled and Unlabeled
    Data
  url: https://www.aclweb.org/anthology/N18-1174
  year: '2018'
N18-1175:
  abstract: Reasoning is a crucial part of natural language argumentation. To comprehend
    an argument, one must analyze its warrant, which explains why its claim follows
    from its premises. As arguments are highly contextualized, warrants are usually
    presupposed and left implicit. Thus, the comprehension does not only require language
    understanding and logic skills, but also depends on common sense. In this paper
    we develop a methodology for reconstructing warrants systematically. We operationalize
    it in a scalable crowdsourcing process, resulting in a freely licensed dataset
    with warrants for 2k authentic arguments from news comments. On this basis, we
    present a new challenging task, the argument reasoning comprehension task. Given
    an argument with a claim and a premise, the goal is to choose the correct implicit
    warrant from two options. Both warrants are plausible and lexically close, but
    lead to contradicting claims. A solution to this task will define a substantial
    step towards automatic warrant reconstruction. However, experiments with several
    neural attention and language models reveal that current approaches do not suffice.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1175.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1175.Notes.pdf
  - filename: http://vimeo.com/277672779
    type: video
    url: http://vimeo.com/277672779
  author:
  - first: Ivan
    full: Ivan Habernal
    id: ivan-habernal
    last: Habernal
  - first: Henning
    full: Henning Wachsmuth
    id: henning-wachsmuth
    last: Wachsmuth
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  - first: Benno
    full: Benno Stein
    id: benno-stein
    last: Stein
  author_string: Ivan Habernal, Henning Wachsmuth, Iryna Gurevych, Benno Stein
  bibkey: habernal-etal-2018-argument
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1175
  month: June
  page_first: '1930'
  page_last: '1940'
  pages: "1930\u20131940"
  paper_id: '175'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1175.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1175.jpg
  title: 'The Argument Reasoning Comprehension Task: Identification and Reconstruction
    of Implicit Warrants'
  title_html: 'The Argument Reasoning Comprehension Task: Identification and Reconstruction
    of Implicit Warrants'
  url: https://www.aclweb.org/anthology/N18-1175
  year: '2018'
N18-1176:
  abstract: We explore deception detection in interview dialogues. We analyze a set
    of linguistic features in both truthful and deceptive responses to interview questions.
    We also study the perception of deception, identifying characteristics of statements
    that are perceived as truthful or deceptive by interviewers. Our analysis show
    significant differences between truthful and deceptive question responses, as
    well as variations in deception patterns across gender and native language. This
    analysis motivated our selection of features for machine learning experiments
    aimed at classifying globally deceptive speech. Our best classification performance
    is 72.74% F1-Score (about 17% better than human performance), which is achieved
    using a combination of linguistic features and individual traits.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/277672970
    type: video
    url: http://vimeo.com/277672970
  author:
  - first: Sarah Ita
    full: Sarah Ita Levitan
    id: sarah-ita-levitan
    last: Levitan
  - first: Angel
    full: Angel Maredia
    id: angel-maredia
    last: Maredia
  - first: Julia
    full: Julia Hirschberg
    id: julia-hirschberg
    last: Hirschberg
  author_string: Sarah Ita Levitan, Angel Maredia, Julia Hirschberg
  bibkey: levitan-etal-2018-linguistic
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1176
  month: June
  page_first: '1941'
  page_last: '1950'
  pages: "1941\u20131950"
  paper_id: '176'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1176.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1176.jpg
  title: Linguistic Cues to Deception and Perceived Deception in Interview Dialogues
  title_html: Linguistic Cues to Deception and Perceived Deception in Interview Dialogues
  url: https://www.aclweb.org/anthology/N18-1176
  year: '2018'
N18-1177:
  abstract: We show that explicit pragmatic inference aids in correctly generating
    and following natural language instructions for complex, sequential tasks. Our
    pragmatics-enabled models reason about why speakers produce certain instructions,
    and about how listeners will react upon hearing them. Like previous pragmatic
    models, we use learned base listener and speaker models to build a pragmatic speaker
    that uses the base listener to simulate the interpretation of candidate descriptions,
    and a pragmatic listener that reasons counterfactually about alternative descriptions.
    We extend these models to tasks with sequential structure. Evaluation of language
    generation and interpretation shows that pragmatic inference improves state-of-the-art
    listener models (at correctly interpreting human instructions) and speaker models
    (at producing instructions correctly interpreted by humans) in diverse settings.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/277672896
    type: video
    url: http://vimeo.com/277672896
  author:
  - first: Daniel
    full: Daniel Fried
    id: daniel-fried
    last: Fried
  - first: Jacob
    full: Jacob Andreas
    id: jacob-andreas
    last: Andreas
  - first: Dan
    full: Dan Klein
    id: dan-klein
    last: Klein
  author_string: Daniel Fried, Jacob Andreas, Dan Klein
  bibkey: fried-etal-2018-unified
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1177
  month: June
  page_first: '1951'
  page_last: '1963'
  pages: "1951\u20131963"
  paper_id: '177'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1177.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1177.jpg
  title: Unified Pragmatic Models for Generating and Following Instructions
  title_html: Unified Pragmatic Models for Generating and Following Instructions
  url: https://www.aclweb.org/anthology/N18-1177
  year: '2018'
N18-1178:
  abstract: "Election manifestos document the intentions, motives, and views of political\
    \ parties. They are often used for analysing a party\u2019s fine-grained position\
    \ on a particular issue, as well as for coarse-grained positioning of a party\
    \ on the left\u2013right spectrum. In this paper we propose a two-stage model\
    \ for automatically performing both levels of analysis over manifestos. In the\
    \ first step we employ a hierarchical multi-task structured deep model to predict\
    \ fine- and coarse-grained positions, and in the second step we perform post-hoc\
    \ calibration of coarse-grained positions using probabilistic soft logic. We empirically\
    \ show that the proposed model outperforms state-of-art approaches at both granularities\
    \ using manifestos from twelve countries, written in ten different languages."
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/277672945
    type: video
    url: http://vimeo.com/277672945
  author:
  - first: Shivashankar
    full: Shivashankar Subramanian
    id: shivashankar-subramanian
    last: Subramanian
  - first: Trevor
    full: Trevor Cohn
    id: trevor-cohn
    last: Cohn
  - first: Timothy
    full: Timothy Baldwin
    id: timothy-baldwin
    last: Baldwin
  author_string: Shivashankar Subramanian, Trevor Cohn, Timothy Baldwin
  bibkey: subramanian-etal-2018-hierarchical
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1178
  month: June
  page_first: '1964'
  page_last: '1974'
  pages: "1964\u20131974"
  paper_id: '178'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1178.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1178.jpg
  title: Hierarchical Structured Model for Fine-to-Coarse Manifesto Text Analysis
  title_html: Hierarchical Structured Model for Fine-to-Coarse Manifesto Text Analysis
  url: https://www.aclweb.org/anthology/N18-1178
  year: '2018'
N18-1179:
  abstract: "Natural Language Inference is a challenging task that has received substantial\
    \ attention, and state-of-the-art models now achieve impressive test set performance\
    \ in the form of accuracy scores. Here, we go beyond this single evaluation metric\
    \ to examine robustness to semantically-valid alterations to the input data. We\
    \ identify three factors - insensitivity, polarity and unseen pairs - and compare\
    \ their impact on three SNLI models under a variety of conditions. Our results\
    \ demonstrate a number of strengths and weaknesses in the models\u2019 ability\
    \ to generalise to new in-domain instances. In particular, while strong performance\
    \ is possible on unseen hypernyms, unseen antonyms are more challenging for all\
    \ the models. More generally, the models suffer from an insensitivity to certain\
    \ small but semantically significant alterations, and are also often influenced\
    \ by simple statistical correlations between words and training labels. Overall,\
    \ we show that evaluations of NLI models can benefit from studying the influence\
    \ of factors intrinsic to the models or found in the dataset used."
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1179.Datasets.zip
    type: dataset
    url: https://www.aclweb.org/anthology/attachments/N18-1179.Datasets.zip
  - filename: http://vimeo.com/277673944
    type: video
    url: http://vimeo.com/277673944
  author:
  - first: Ivan
    full: Ivan Sanchez
    id: ivan-sanchez
    last: Sanchez
  - first: Jeff
    full: Jeff Mitchell
    id: jeff-mitchell
    last: Mitchell
  - first: Sebastian
    full: Sebastian Riedel
    id: sebastian-riedel
    last: Riedel
  author_string: Ivan Sanchez, Jeff Mitchell, Sebastian Riedel
  bibkey: sanchez-etal-2018-behavior
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1179
  month: June
  page_first: '1975'
  page_last: '1985'
  pages: "1975\u20131985"
  paper_id: '179'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1179.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1179.jpg
  title: 'Behavior Analysis of NLI Models: Uncovering the Influence of Three Factors
    on Robustness'
  title_html: 'Behavior Analysis of <span class="acl-fixed-case">NLI</span> Models:
    Uncovering the Influence of Three Factors on Robustness'
  url: https://www.aclweb.org/anthology/N18-1179
  year: '2018'
N18-1180:
  abstract: "We present a novel approach for determining learners\u2019 second language\
    \ proficiency which utilizes behavioral traces of eye movements during reading.\
    \ Our approach provides stand-alone eyetracking based English proficiency scores\
    \ which reflect the extent to which the learner\u2019s gaze patterns in reading\
    \ are similar to those of native English speakers. We show that our scores correlate\
    \ strongly with standardized English proficiency tests. We also demonstrate that\
    \ gaze information can be used to accurately predict the outcomes of such tests.\
    \ Our approach yields the strongest performance when the test taker is presented\
    \ with a suite of sentences for which we have eyetracking data from other readers.\
    \ However, it remains effective even using eyetracking with sentences for which\
    \ eye movement data have not been previously collected. By deriving proficiency\
    \ as an automatic byproduct of eye movements during ordinary reading, our approach\
    \ offers a potentially valuable new tool for second language proficiency assessment.\
    \ More broadly, our results open the door to future methods for inferring reader\
    \ characteristics from the behavioral traces of reading."
  address: New Orleans, Louisiana
  author:
  - first: Yevgeni
    full: Yevgeni Berzak
    id: yevgeni-berzak
    last: Berzak
  - first: Boris
    full: Boris Katz
    id: boris-katz
    last: Katz
  - first: Roger
    full: Roger Levy
    id: roger-levy
    last: Levy
  author_string: Yevgeni Berzak, Boris Katz, Roger Levy
  bibkey: berzak-etal-2018-assessing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1180
  month: June
  page_first: '1986'
  page_last: '1996'
  pages: "1986\u20131996"
  paper_id: '180'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1180.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1180.jpg
  title: Assessing Language Proficiency from Eye Movements in Reading
  title_html: Assessing Language Proficiency from Eye Movements in Reading
  url: https://www.aclweb.org/anthology/N18-1180
  year: '2018'
N18-1181:
  abstract: Speakers often have more than one way to express the same meaning. What
    general principles govern speaker choice in the face of optionality when near
    semantically invariant alternation exists? Studies have shown that optional reduction
    in language is sensitive to contextual predictability, such that more predictable
    a linguistic unit is, the more likely it is to get reduced. Yet it is unclear
    whether these cases of speaker choice are driven by audience design versus toward
    facilitating production. Here we argue that for a different optionality phenomenon,
    namely classifier choice in Mandarin Chinese, Uniform Information Density and
    at least one plausible variant of availability-based production make opposite
    predictions regarding the relationship between the predictability of the upcoming
    material and speaker choices. In a corpus analysis of Mandarin Chinese, we show
    that the distribution of speaker choices supports the availability-based production
    account and not the Uniform Information Density.
  address: New Orleans, Louisiana
  author:
  - first: Meilin
    full: Meilin Zhan
    id: meilin-zhan
    last: Zhan
  - first: Roger
    full: Roger Levy
    id: roger-levy
    last: Levy
  author_string: Meilin Zhan, Roger Levy
  bibkey: zhan-levy-2018-comparing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1181
  month: June
  page_first: '1997'
  page_last: '2005'
  pages: "1997\u20132005"
  paper_id: '181'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1181.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1181.jpg
  title: Comparing Theories of Speaker Choice Using a Model of Classifier Production
    in Mandarin Chinese
  title_html: Comparing Theories of Speaker Choice Using a Model of Classifier Production
    in <span class="acl-fixed-case">M</span>andarin <span class="acl-fixed-case">C</span>hinese
  url: https://www.aclweb.org/anthology/N18-1181
  year: '2018'
N18-1182:
  abstract: "Automatic identification of spurious instances (those with potentially\
    \ wrong labels in datasets) can improve the quality of existing language resources,\
    \ especially when annotations are obtained through crowdsourcing or automatically\
    \ generated based on coded rankings. In this paper, we present effective approaches\
    \ inspired by queueing theory and psychology of learning to automatically identify\
    \ spurious instances in datasets. Our approaches discriminate instances based\
    \ on their \u201Cdifficulty to learn,\u201D determined by a downstream learner.\
    \ Our methods can be applied to any dataset assuming the existence of a neural\
    \ network model for the target task of the dataset. Our best approach outperforms\
    \ competing state-of-the-art baselines and has a MAP of 0.85 and 0.22 in identifying\
    \ spurious instances in synthetic and carefully-crowdsourced real-world datasets\
    \ respectively."
  address: New Orleans, Louisiana
  author:
  - first: Hadi
    full: Hadi Amiri
    id: hadi-amiri
    last: Amiri
  - first: Timothy
    full: Timothy Miller
    id: timothy-miller
    last: Miller
  - first: Guergana
    full: Guergana Savova
    id: guergana-savova
    last: Savova
  author_string: Hadi Amiri, Timothy Miller, Guergana Savova
  bibkey: amiri-etal-2018-spotting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1182
  month: June
  page_first: '2006'
  page_last: '2016'
  pages: "2006\u20132016"
  paper_id: '182'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1182.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1182.jpg
  title: Spotting Spurious Data with Neural Networks
  title_html: Spotting Spurious Data with Neural Networks
  url: https://www.aclweb.org/anthology/N18-1182
  year: '2018'
N18-1183:
  abstract: This paper explores the time course of lexical memory retrieval by modeling
    fluent language production. The duration of retrievals is predicted using the
    ACT-R cognitive architecture. In a large-scale observational study of a spoken
    corpus, we find that language production at a time point preceding a word is sped
    up or slowed down depending on activation of that word. This computational analysis
    has consequences for the theoretical model of language production. The results
    point to interference between lexical and phonological stages as well as a quantifiable
    buffer for lexical information that opens up the possibility of non-sequential
    retrievals.
  address: New Orleans, Louisiana
  author:
  - first: Jeremy
    full: Jeremy Cole
    id: jeremy-cole
    last: Cole
  - first: David
    full: David Reitter
    id: david-reitter
    last: Reitter
  author_string: Jeremy Cole, David Reitter
  bibkey: cole-reitter-2018-timing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1183
  month: June
  page_first: '2017'
  page_last: '2027'
  pages: "2017\u20132027"
  paper_id: '183'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1183.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1183.jpg
  title: The Timing of Lexical Memory Retrievals in Language Production
  title_html: The Timing of Lexical Memory Retrievals in Language Production
  url: https://www.aclweb.org/anthology/N18-1183
  year: '2018'
N18-1184:
  abstract: When learning POS taggers and syntactic chunkers for low-resource languages,
    different resources may be available, and often all we have is a small tag dictionary,
    motivating type-constrained unsupervised induction. Even small dictionaries can
    improve the performance of unsupervised induction algorithms. This paper shows
    that performance can be further improved by including data that is readily available
    or can be easily obtained for most languages, i.e., eye-tracking, speech, or keystroke
    logs (or any combination thereof). We project information from all these data
    sources into shared spaces, in which the union of words is represented. For English
    unsupervised POS induction, the additional information, which is not required
    at test time, leads to an average error reduction on Ontonotes domains of 1.5%
    over systems augmented with state-of-the-art word embeddings. On Penn Treebank
    the best model achieves 5.4% error reduction over a word embeddings baseline.
    We also achieve significant improvements for syntactic chunk induction. Our analysis
    shows that improvements are even bigger when the available tag dictionaries are
    smaller.
  address: New Orleans, Louisiana
  author:
  - first: Maria
    full: Maria Barrett
    id: maria-barrett
    last: Barrett
  - first: Ana Valeria
    full: "Ana Valeria Gonz\xE1lez-Gardu\xF1o"
    id: ana-valeria-gonzalez-garduno
    last: "Gonz\xE1lez-Gardu\xF1o"
  - first: Lea
    full: Lea Frermann
    id: lea-frermann
    last: Frermann
  - first: Anders
    full: "Anders S\xF8gaard"
    id: anders-sogaard
    last: "S\xF8gaard"
  author_string: "Maria Barrett, Ana Valeria Gonz\xE1lez-Gardu\xF1o, Lea Frermann,\
    \ Anders S\xF8gaard"
  bibkey: barrett-etal-2018-unsupervised
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1184
  month: June
  page_first: '2028'
  page_last: '2038'
  pages: "2028\u20132038"
  paper_id: '184'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1184.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1184.jpg
  title: Unsupervised Induction of Linguistic Categories with Records of Reading,
    Speaking, and Writing
  title_html: Unsupervised Induction of Linguistic Categories with Records of Reading,
    Speaking, and Writing
  url: https://www.aclweb.org/anthology/N18-1184
  year: '2018'
N18-1185:
  abstract: This paper presents a new corpus and a robust deep learning architecture
    for a task in reading comprehension, passage completion, on multiparty dialog.
    Given a dialog in text and a passage containing factual descriptions about the
    dialog where mentions of the characters are replaced by blanks, the task is to
    fill the blanks with the most appropriate character names that reflect the contexts
    in the dialog. Since there is no dataset that challenges the task of passage completion
    in this genre, we create a corpus by selecting transcripts from a TV show that
    comprise 1,681 dialogs, generating passages for each dialog through crowdsourcing,
    and annotating mentions of characters in both the dialog and the passages. Given
    this dataset, we build a deep neural model that integrates rich feature extraction
    from convolutional neural networks into sequence modeling in recurrent neural
    networks, optimized by utterance and dialog level attentions. Our model outperforms
    the previous state-of-the-art model on this task in a different genre using bidirectional
    LSTM, showing a 13.0+% improvement for longer dialogs. Our analysis shows the
    effectiveness of the attention mechanisms and suggests a direction to machine
    comprehension on multiparty dialog.
  address: New Orleans, Louisiana
  author:
  - first: Kaixin
    full: Kaixin Ma
    id: kaixin-ma
    last: Ma
  - first: Tomasz
    full: Tomasz Jurczyk
    id: tomasz-jurczyk
    last: Jurczyk
  - first: Jinho D.
    full: Jinho D. Choi
    id: jinho-d-choi
    last: Choi
  author_string: Kaixin Ma, Tomasz Jurczyk, Jinho D. Choi
  bibkey: ma-etal-2018-challenging
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1185
  month: June
  page_first: '2039'
  page_last: '2048'
  pages: "2039\u20132048"
  paper_id: '185'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1185.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1185.jpg
  title: 'Challenging Reading Comprehension on Daily Conversation: Passage Completion
    on Multiparty Dialog'
  title_html: 'Challenging Reading Comprehension on Daily Conversation: Passage Completion
    on Multiparty Dialog'
  url: https://www.aclweb.org/anthology/N18-1185
  year: '2018'
N18-1186:
  abstract: "In this paper, we propose a generalizable dialog generation approach\
    \ that adapts multi-turn reasoning, one recent advancement in the field of document\
    \ comprehension, to generate responses (\u201Canswers\u201D) by taking current\
    \ conversation session context as a \u201Cdocument\u201D and current query as\
    \ a \u201Cquestion\u201D. The major idea is to represent a conversation session\
    \ into memories upon which attention-based memory reading mechanism can be performed\
    \ multiple times, so that (1) user\u2019s query is properly extended by contextual\
    \ clues and (2) optimal responses are step-by-step generated. Considering that\
    \ the speakers of one conversation are not limited to be one, we separate the\
    \ single memory used for document comprehension into different groups for speaker-specific\
    \ topic and opinion embedding. Namely, we utilize the queries\u2019 memory, the\
    \ responses\u2019 memory, and their unified memory, following the time sequence\
    \ of the conversation session. Experiments on Japanese 10-sentence (5-round) conversation\
    \ modeling show impressive results on how multi-turn reasoning can produce more\
    \ diverse and acceptable responses than state-of-the-art single-turn and non-reasoning\
    \ baselines."
  address: New Orleans, Louisiana
  author:
  - first: Xianchao
    full: Xianchao Wu
    id: xianchao-wu
    last: Wu
  - first: Ander
    full: "Ander Mart\xEDnez"
    id: ander-martinez
    last: "Mart\xEDnez"
  - first: Momo
    full: Momo Klyen
    id: momo-klyen
    last: Klyen
  author_string: "Xianchao Wu, Ander Mart\xEDnez, Momo Klyen"
  bibkey: wu-etal-2018-dialog
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1186
  month: June
  page_first: '2049'
  page_last: '2059'
  pages: "2049\u20132059"
  paper_id: '186'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1186.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1186.jpg
  title: Dialog Generation Using Multi-Turn Reasoning Neural Networks
  title_html: Dialog Generation Using Multi-Turn Reasoning Neural Networks
  url: https://www.aclweb.org/anthology/N18-1186
  year: '2018'
N18-1187:
  abstract: "In this work, we present a hybrid learning method for training task-oriented\
    \ dialogue systems through online user interactions. Popular methods for learning\
    \ task-oriented dialogues include applying reinforcement learning with user feedback\
    \ on supervised pre-training models. Efficiency of such learning method may suffer\
    \ from the mismatch of dialogue state distribution between offline training and\
    \ online interactive learning stages. To address this challenge, we propose a\
    \ hybrid imitation and reinforcement learning method, with which a dialogue agent\
    \ can effectively learn from its interaction with users by learning from human\
    \ teaching and feedback. We design a neural network based task-oriented dialogue\
    \ agent that can be optimized end-to-end with the proposed learning method. Experimental\
    \ results show that our end-to-end dialogue agent can learn effectively from the\
    \ mistake it makes via imitation learning from user teaching. Applying reinforcement\
    \ learning with user feedback after the imitation learning stage further improves\
    \ the agent\u2019s capability in successfully completing a task."
  address: New Orleans, Louisiana
  author:
  - first: Bing
    full: Bing Liu
    id: bing-liu
    last: Liu
  - first: Gokhan
    full: "Gokhan T\xFCr"
    id: gokhan-tur
    last: "T\xFCr"
  - first: Dilek
    full: "Dilek Hakkani-T\xFCr"
    id: dilek-hakkani-tur
    last: "Hakkani-T\xFCr"
  - first: Pararth
    full: Pararth Shah
    id: pararth-shah
    last: Shah
  - first: Larry
    full: Larry Heck
    id: larry-heck
    last: Heck
  author_string: "Bing Liu, Gokhan T\xFCr, Dilek Hakkani-T\xFCr, Pararth Shah, Larry\
    \ Heck"
  bibkey: liu-etal-2018-dialogue
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1187
  month: June
  page_first: '2060'
  page_last: '2069'
  pages: "2060\u20132069"
  paper_id: '187'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1187.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1187.jpg
  title: Dialogue Learning with Human Teaching and Feedback in End-to-End Trainable
    Task-Oriented Dialogue Systems
  title_html: Dialogue Learning with Human Teaching and Feedback in End-to-End Trainable
    Task-Oriented Dialogue Systems
  url: https://www.aclweb.org/anthology/N18-1187
  year: '2018'
N18-1188:
  abstract: It has been proven that automatic conversational agents can be built up
    using the Endto-End Neural Response Generation (NRG) framework, and such a data-driven
    methodology requires a large number of dialog pairs for model training and reasonable
    evaluation metrics for testing. This paper proposes a Large Scale Domain-Specific
    Conversational Corpus (LSDSCC) composed of high-quality queryresponse pairs extracted
    from the domainspecific online forum, with thorough preprocessing and cleansing
    procedures. Also, a testing set, including multiple diverse responses annotated
    for each query, is constructed, and on this basis, the metrics for measuring the
    diversity of generated results are further presented. We evaluate the performances
    of neural dialog models with the widely applied diversity boosting strategies
    on the proposed dataset. The experimental results have shown that our proposed
    corpus can be taken as a new benchmark dataset for the NRG task, and the presented
    metrics are promising to guide the optimization of NRG models by quantifying the
    diversity of the generated responses reasonably.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1188.Datasets.zip
    type: dataset
    url: https://www.aclweb.org/anthology/attachments/N18-1188.Datasets.zip
  - filename: N18-1188.Software.zip
    type: software
    url: https://www.aclweb.org/anthology/attachments/N18-1188.Software.zip
  author:
  - first: Zhen
    full: Zhen Xu
    id: zhen-xu
    last: Xu
  - first: Nan
    full: Nan Jiang
    id: nan-jiang
    last: Jiang
  - first: Bingquan
    full: Bingquan Liu
    id: bingquan-liu
    last: Liu
  - first: Wenge
    full: Wenge Rong
    id: wenge-rong
    last: Rong
  - first: Bowen
    full: Bowen Wu
    id: bowen-wu
    last: Wu
  - first: Baoxun
    full: Baoxun Wang
    id: baoxun-wang
    last: Wang
  - first: Zhuoran
    full: Zhuoran Wang
    id: zhuoran-wang
    last: Wang
  - first: Xiaolong
    full: Xiaolong Wang
    id: xiaolong-wang
    last: Wang
  author_string: Zhen Xu, Nan Jiang, Bingquan Liu, Wenge Rong, Bowen Wu, Baoxun Wang,
    Zhuoran Wang, Xiaolong Wang
  bibkey: xu-etal-2018-lsdscc
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1188
  month: June
  page_first: '2070'
  page_last: '2080'
  pages: "2070\u20132080"
  paper_id: '188'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1188.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1188.jpg
  title: 'LSDSCC: a Large Scale Domain-Specific Conversational Corpus for Response
    Generation with Diversity Oriented Evaluation Metrics'
  title_html: '<span class="acl-fixed-case">LSDSCC</span>: a Large Scale Domain-Specific
    Conversational Corpus for Response Generation with Diversity Oriented Evaluation
    Metrics'
  url: https://www.aclweb.org/anthology/N18-1188
  year: '2018'
N18-1189:
  abstract: "Coding EMRs with diagnosis and procedure codes is an indispensable task\
    \ for billing, secondary data analyses, and monitoring health trends. Both speed\
    \ and accuracy of coding are critical. While coding errors could lead to more\
    \ patient-side financial burden and misinterpretation of a patient\u2019s well-being,\
    \ timely coding is also needed to avoid backlogs and additional costs for the\
    \ healthcare facility. In this paper, we present a new neural network architecture\
    \ that combines ideas from few-shot learning matching networks, multi-label loss\
    \ functions, and convolutional neural networks for text classification to significantly\
    \ outperform other state-of-the-art models. Our evaluations are conducted using\
    \ a well known de-identified EMR dataset (MIMIC) with a variety of multi-label\
    \ performance measures."
  address: New Orleans, Louisiana
  author:
  - first: Anthony
    full: Anthony Rios
    id: anthony-rios
    last: Rios
  - first: Ramakanth
    full: Ramakanth Kavuluru
    id: ramakanth-kavuluru
    last: Kavuluru
  author_string: Anthony Rios, Ramakanth Kavuluru
  bibkey: rios-kavuluru-2018-emr
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1189
  month: June
  page_first: '2081'
  page_last: '2091'
  pages: "2081\u20132091"
  paper_id: '189'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1189.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1189.jpg
  title: EMR Coding with Semi-Parametric Multi-Head Matching Networks
  title_html: <span class="acl-fixed-case">EMR</span> Coding with Semi-Parametric
    Multi-Head Matching Networks
  url: https://www.aclweb.org/anthology/N18-1189
  year: '2018'
N18-1190:
  abstract: Despite the recent popularity of word embedding methods, there is only
    a small body of work exploring the limitations of these representations. In this
    paper, we consider one aspect of embedding spaces, namely their stability. We
    show that even relatively high frequency words (100-200 occurrences) are often
    unstable. We provide empirical evidence for how various factors contribute to
    the stability of word embeddings, and we analyze the effects of stability on downstream
    tasks.
  address: New Orleans, Louisiana
  author:
  - first: Laura
    full: Laura Wendlandt
    id: laura-burdick
    last: Wendlandt
  - first: Jonathan K.
    full: Jonathan K. Kummerfeld
    id: jonathan-k-kummerfeld
    last: Kummerfeld
  - first: Rada
    full: Rada Mihalcea
    id: rada-mihalcea
    last: Mihalcea
  author_string: Laura Wendlandt, Jonathan K. Kummerfeld, Rada Mihalcea
  bibkey: wendlandt-etal-2018-factors
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1190
  month: June
  page_first: '2092'
  page_last: '2102'
  pages: "2092\u20132102"
  paper_id: '190'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1190.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1190.jpg
  title: Factors Influencing the Surprising Instability of Word Embeddings
  title_html: Factors Influencing the Surprising Instability of Word Embeddings
  url: https://www.aclweb.org/anthology/N18-1190
  year: '2018'
N18-1191:
  abstract: We investigate the task of mining relevant stocks given a topic of concern
    on emerging capital markets, for which there is lack of structural understanding.
    Deep learning is leveraged to mine evidences from large scale textual data, which
    contain valuable market information. In particular, distributed word similarities
    trained over large scale raw texts are taken as a basis of relevance measuring,
    and deep reinforcement learning is leveraged to learn a strategy of topic expansion,
    given a small amount of manually labeled data from financial analysts. Results
    on two Chinese stock market datasets show that our method outperforms a strong
    baseline using information retrieval techniques.
  address: New Orleans, Louisiana
  author:
  - first: Qi
    full: Qi Liu
    id: qi-liu
    last: Liu
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  author_string: Qi Liu, Yue Zhang
  bibkey: liu-zhang-2018-mining
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1191
  month: June
  page_first: '2103'
  page_last: '2112'
  pages: "2103\u20132112"
  paper_id: '191'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1191.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1191.jpg
  title: Mining Evidences for Concept Stock Recommendation
  title_html: Mining Evidences for Concept Stock Recommendation
  url: https://www.aclweb.org/anthology/N18-1191
  year: '2018'
N18-1192:
  abstract: Long short-term memory (LSTM) language model (LM) has been widely investigated
    for automatic speech recognition (ASR) and natural language processing (NLP).
    Although excellent performance is obtained for large vocabulary tasks, tremendous
    memory consumption prohibits the use of LSTM LM in low-resource devices. The memory
    consumption mainly comes from the word embedding layer. In this paper, a novel
    binarized LSTM LM is proposed to address the problem. Words are encoded into binary
    vectors and other LSTM parameters are further binarized to achieve high memory
    compression. This is the first effort to investigate binary LSTM for large vocabulary
    LM. Experiments on both English and Chinese LM and ASR tasks showed that can achieve
    a compression ratio of 11.3 without any loss of LM and ASR performances and a
    compression ratio of 31.6 with acceptable minor performance degradation.
  address: New Orleans, Louisiana
  author:
  - first: Xuan
    full: Xuan Liu
    id: xuan-liu
    last: Liu
  - first: Di
    full: Di Cao
    id: di-cao
    last: Cao
  - first: Kai
    full: Kai Yu
    id: kai-yu
    last: Yu
  author_string: Xuan Liu, Di Cao, Kai Yu
  bibkey: liu-etal-2018-binarized
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1192
  month: June
  page_first: '2113'
  page_last: '2121'
  pages: "2113\u20132121"
  paper_id: '192'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1192.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1192.jpg
  title: Binarized LSTM Language Model
  title_html: Binarized <span class="acl-fixed-case">LSTM</span> Language Model
  url: https://www.aclweb.org/anthology/N18-1192
  year: '2018'
N18-1193:
  abstract: "Emotion recognition in conversations is crucial for the development of\
    \ empathetic machines. Present methods mostly ignore the role of inter-speaker\
    \ dependency relations while classifying emotions in conversations. In this paper,\
    \ we address recognizing utterance-level emotions in dyadic conversational videos.\
    \ We propose a deep neural framework, termed Conversational Memory Network (CMN),\
    \ which leverages contextual information from the conversation history. In particular,\
    \ CMN uses multimodal approach comprising audio, visual and textual features with\
    \ gated recurrent units to model past utterances of each speaker into memories.\
    \ These memories are then merged using attention-based hops to capture inter-speaker\
    \ dependencies. Experiments show a significant improvement of 3 \u2212 4% in accuracy\
    \ over the state of the art."
  address: New Orleans, Louisiana
  author:
  - first: Devamanyu
    full: Devamanyu Hazarika
    id: devamanyu-hazarika
    last: Hazarika
  - first: Soujanya
    full: Soujanya Poria
    id: soujanya-poria
    last: Poria
  - first: Amir
    full: Amir Zadeh
    id: amir-zadeh
    last: Zadeh
  - first: Erik
    full: Erik Cambria
    id: erik-cambria
    last: Cambria
  - first: Louis-Philippe
    full: Louis-Philippe Morency
    id: louis-philippe-morency
    last: Morency
  - first: Roger
    full: Roger Zimmermann
    id: roger-zimmermann
    last: Zimmermann
  author_string: Devamanyu Hazarika, Soujanya Poria, Amir Zadeh, Erik Cambria, Louis-Philippe
    Morency, Roger Zimmermann
  bibkey: hazarika-etal-2018-conversational
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1193
  month: June
  page_first: '2122'
  page_last: '2132'
  pages: "2122\u20132132"
  paper_id: '193'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1193.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1193.jpg
  title: Conversational Memory Network for Emotion Recognition in Dyadic Dialogue
    Videos
  title_html: Conversational Memory Network for Emotion Recognition in Dyadic Dialogue
    Videos
  url: https://www.aclweb.org/anthology/N18-1193
  year: '2018'
N18-1194:
  abstract: Spoken language understanding (SLU) is an essential component in conversational
    systems. Most SLU components treats each utterance independently, and then the
    following components aggregate the multi-turn information in the separate phases.
    In order to avoid error propagation and effectively utilize contexts, prior work
    leveraged history for contextual SLU. However, most previous models only paid
    attention to the related content in history utterances, ignoring their temporal
    information. In the dialogues, it is intuitive that the most recent utterances
    are more important than the least recent ones, in other words, time-aware attention
    should be in a decaying manner. Therefore, this paper designs and investigates
    various types of time-decay attention on the sentence-level and speaker-level,
    and further proposes a flexible universal time-decay attention mechanism. The
    experiments on the benchmark Dialogue State Tracking Challenge (DSTC4) dataset
    show that the proposed time-decay attention mechanisms significantly improve the
    state-of-the-art model for contextual understanding performance.
  address: New Orleans, Louisiana
  author:
  - first: Shang-Yu
    full: Shang-Yu Su
    id: shang-yu-su
    last: Su
  - first: Pei-Chieh
    full: Pei-Chieh Yuan
    id: pei-chieh-yuan
    last: Yuan
  - first: Yun-Nung
    full: Yun-Nung Chen
    id: yun-nung-chen
    last: Chen
  author_string: Shang-Yu Su, Pei-Chieh Yuan, Yun-Nung Chen
  bibkey: su-etal-2018-time
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1194
  month: June
  page_first: '2133'
  page_last: '2142'
  pages: "2133\u20132142"
  paper_id: '194'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1194.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1194.jpg
  title: 'How Time Matters: Learning Time-Decay Attention for Contextual Spoken Language
    Understanding in Dialogues'
  title_html: 'How Time Matters: Learning Time-Decay Attention for Contextual Spoken
    Language Understanding in Dialogues'
  url: https://www.aclweb.org/anthology/N18-1194
  year: '2018'
N18-1195:
  abstract: Using a case study, we show that variation in oral reading rate across
    passages for professional narrators is consistent across readers and much of it
    can be explained using features of the texts being read. While text complexity
    is a poor predictor of the reading rate, a substantial share of variability can
    be explained by timing and story-based factors with performance reaching r=0.75
    for unseen passages and narrator.
  address: New Orleans, Louisiana
  author:
  - first: Anastassia
    full: Anastassia Loukina
    id: anastassia-loukina
    last: Loukina
  - first: Van Rynald T.
    full: Van Rynald T. Liceralde
    id: van-rynald-t-liceralde
    last: Liceralde
  - first: Beata
    full: Beata Beigman Klebanov
    id: beata-beigman-klebanov
    last: Beigman Klebanov
  author_string: Anastassia Loukina, Van Rynald T. Liceralde, Beata Beigman Klebanov
  bibkey: loukina-etal-2018-towards
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1195
  month: June
  page_first: '2143'
  page_last: '2154'
  pages: "2143\u20132154"
  paper_id: '195'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1195.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1195.jpg
  title: Towards Understanding Text Factors in Oral Reading
  title_html: Towards Understanding Text Factors in Oral Reading
  url: https://www.aclweb.org/anthology/N18-1195
  year: '2018'
N18-1196:
  abstract: 'Contextual influences on language often exhibit substantial cross-lingual
    regularities; for example, we are more verbose in situations that require finer
    distinctions. However, these regularities are sometimes obscured by semantic and
    syntactic differences. Using a newly-collected dataset of color reference games
    in Mandarin Chinese (which we release to the public), we confirm that a variety
    of constructions display the same sensitivity to contextual difficulty in Chinese
    and English. We then show that a neural speaker agent trained on bilingual data
    with a simple multitask learning approach displays more human-like patterns of
    context dependence and is more pragmatically informative than its monolingual
    Chinese counterpart. Moreover, this is not at the expense of language-specific
    semantic understanding: the resulting speaker model learns the different basic
    color term systems of English and Chinese (with noteworthy cross-lingual influences),
    and it can identify synonyms between the two languages using vector analogy operations
    on its output layer, despite having no exposure to parallel data.'
  address: New Orleans, Louisiana
  author:
  - first: Will
    full: Will Monroe
    id: will-monroe
    last: Monroe
  - first: Jennifer
    full: Jennifer Hu
    id: jennifer-hu
    last: Hu
  - first: Andrew
    full: Andrew Jong
    id: andrew-jong
    last: Jong
  - first: Christopher
    full: Christopher Potts
    id: christopher-potts
    last: Potts
  author_string: Will Monroe, Jennifer Hu, Andrew Jong, Christopher Potts
  bibkey: monroe-etal-2018-generating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1196
  month: June
  page_first: '2155'
  page_last: '2165'
  pages: "2155\u20132165"
  paper_id: '196'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1196.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1196.jpg
  title: Generating Bilingual Pragmatic Color References
  title_html: Generating Bilingual Pragmatic Color References
  url: https://www.aclweb.org/anthology/N18-1196
  year: '2018'
N18-1197:
  abstract: "The named concepts and compositional operators present in natural language\
    \ provide a rich source of information about the abstractions humans use to navigate\
    \ the world. Can this linguistic background knowledge improve the generality and\
    \ efficiency of learned classifiers and control policies? This paper aims to show\
    \ that using the space of natural language strings as a parameter space is an\
    \ effective way to capture natural task structure. In a pretraining phase, we\
    \ learn a language interpretation model that transforms inputs (e.g. images) into\
    \ outputs (e.g. labels) given natural language descriptions. To learn a new concept\
    \ (e.g. a classifier), we search directly in the space of descriptions to minimize\
    \ the interpreter\u2019s loss on training examples. Crucially, our models do not\
    \ require language data to learn these concepts: language is used only in pretraining\
    \ to impose structure on subsequent learning. Results on image classification,\
    \ text editing, and reinforcement learning show that, in all settings, models\
    \ with a linguistic parameterization outperform those without."
  address: New Orleans, Louisiana
  author:
  - first: Jacob
    full: Jacob Andreas
    id: jacob-andreas
    last: Andreas
  - first: Dan
    full: Dan Klein
    id: dan-klein
    last: Klein
  - first: Sergey
    full: Sergey Levine
    id: sergey-levine
    last: Levine
  author_string: Jacob Andreas, Dan Klein, Sergey Levine
  bibkey: andreas-etal-2018-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1197
  month: June
  page_first: '2166'
  page_last: '2179'
  pages: "2166\u20132179"
  paper_id: '197'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1197.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1197.jpg
  title: Learning with Latent Language
  title_html: Learning with Latent Language
  url: https://www.aclweb.org/anthology/N18-1197
  year: '2018'
N18-1198:
  abstract: "The use of explicit object detectors as an intermediate step to image\
    \ captioning \u2013 which used to constitute an essential stage in early work\
    \ \u2013 is often bypassed in the currently dominant end-to-end approaches, where\
    \ the language model is conditioned directly on a mid-level image embedding. We\
    \ argue that explicit detections provide rich semantic information, and can thus\
    \ be used as an interpretable representation to better understand why end-to-end\
    \ image captioning systems work well. We provide an in-depth analysis of end-to-end\
    \ image captioning by exploring a variety of cues that can be derived from such\
    \ object detections. Our study reveals that end-to-end image captioning systems\
    \ rely on matching image representations to generate captions, and that encoding\
    \ the frequency, size and position of objects are complementary and all play a\
    \ role in forming a good image representation. It also reveals that different\
    \ object categories contribute in different ways towards image captioning."
  address: New Orleans, Louisiana
  author:
  - first: Josiah
    full: Josiah Wang
    id: josiah-wang
    last: Wang
  - first: Pranava Swaroop
    full: Pranava Swaroop Madhyastha
    id: pranava-swaroop-madhyastha
    last: Madhyastha
  - first: Lucia
    full: Lucia Specia
    id: lucia-specia
    last: Specia
  author_string: Josiah Wang, Pranava Swaroop Madhyastha, Lucia Specia
  bibkey: wang-etal-2018-object
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1198
  month: June
  page_first: '2180'
  page_last: '2193'
  pages: "2180\u20132193"
  paper_id: '198'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1198.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1198.jpg
  title: Object Counts! Bringing Explicit Detections Back into Image Captioning
  title_html: Object Counts! Bringing Explicit Detections Back into Image Captioning
  url: https://www.aclweb.org/anthology/N18-1198
  year: '2018'
N18-1199:
  abstract: Multimodal machine learning algorithms aim to learn visual-textual correspondences.
    Previous work suggests that concepts with concrete visual manifestations may be
    easier to learn than concepts with abstract ones. We give an algorithm for automatically
    computing the visual concreteness of words and topics within multimodal datasets.
    We apply the approach in four settings, ranging from image captions to images/text
    scraped from historical books. In addition to enabling explorations of concepts
    in multimodal datasets, our concreteness scores predict the capacity of machine
    learning algorithms to learn textual/visual relationships. We find that 1) concrete
    concepts are indeed easier to learn; 2) the large number of algorithms we consider
    have similar failure cases; 3) the precise positive relationship between concreteness
    and performance varies between datasets. We conclude with recommendations for
    using concreteness scores to facilitate future multimodal research.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1199.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1199.Notes.pdf
  author:
  - first: Jack
    full: Jack Hessel
    id: jack-hessel
    last: Hessel
  - first: David
    full: David Mimno
    id: david-mimno
    last: Mimno
  - first: Lillian
    full: Lillian Lee
    id: lillian-lee
    last: Lee
  author_string: Jack Hessel, David Mimno, Lillian Lee
  bibkey: hessel-etal-2018-quantifying
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1199
  month: June
  page_first: '2194'
  page_last: '2205'
  pages: "2194\u20132205"
  paper_id: '199'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1199.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1199.jpg
  title: Quantifying the Visual Concreteness of Words and Topics in Multimodal Datasets
  title_html: Quantifying the Visual Concreteness of Words and Topics in Multimodal
    Datasets
  url: https://www.aclweb.org/anthology/N18-1199
  year: '2018'
N18-1200:
  abstract: We propose a new model for speaker naming in movies that leverages visual,
    textual, and acoustic modalities in an unified optimization framework. To evaluate
    the performance of our model, we introduce a new dataset consisting of six episodes
    of the Big Bang Theory TV show and eighteen full movies covering different genres.
    Our experiments show that our multimodal model significantly outperforms several
    competitive baselines on the average weighted F-score metric. To demonstrate the
    effectiveness of our framework, we design an end-to-end memory network model that
    leverages our speaker naming model and achieves state-of-the-art results on the
    subtitles task of the MovieQA 2017 Challenge.
  address: New Orleans, Louisiana
  author:
  - first: Mahmoud
    full: Mahmoud Azab
    id: mahmoud-azab
    last: Azab
  - first: Mingzhe
    full: Mingzhe Wang
    id: mingzhe-wang
    last: Wang
  - first: Max
    full: Max Smith
    id: max-smith
    last: Smith
  - first: Noriyuki
    full: Noriyuki Kojima
    id: noriyuki-kojima
    last: Kojima
  - first: Jia
    full: Jia Deng
    id: jia-deng
    last: Deng
  - first: Rada
    full: Rada Mihalcea
    id: rada-mihalcea
    last: Mihalcea
  author_string: Mahmoud Azab, Mingzhe Wang, Max Smith, Noriyuki Kojima, Jia Deng,
    Rada Mihalcea
  bibkey: azab-etal-2018-speaker
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1200
  month: June
  page_first: '2206'
  page_last: '2216'
  pages: "2206\u20132216"
  paper_id: '200'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1200.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1200.jpg
  title: Speaker Naming in Movies
  title_html: Speaker Naming in Movies
  url: https://www.aclweb.org/anthology/N18-1200
  year: '2018'
N18-1201:
  abstract: Visual Question Answering (VQA) is a well-known and challenging task that
    requires systems to jointly reason about natural language and vision. Deep learning
    models in various forms have been the standard for solving VQA. However, some
    of these VQA models are better at certain types of image-question pairs than other
    models. Ensembling VQA models intelligently to leverage their diverse expertise
    is, therefore, advantageous. Stacking With Auxiliary Features (SWAF) is an intelligent
    ensembling technique which learns to combine the results of multiple models using
    features of the current problem as context. We propose four categories of auxiliary
    features for ensembling for VQA. Three out of the four categories of features
    can be inferred from an image-question pair and do not require querying the component
    models. The fourth category of auxiliary features uses model-specific explanations.
    In this paper, we describe how we use these various categories of auxiliary features
    to improve performance for VQA. Using SWAF to effectively ensemble three recent
    systems, we obtain a new state-of-the-art. Our work also highlights the advantages
    of explainable AI models.
  address: New Orleans, Louisiana
  author:
  - first: Nazneen Fatema
    full: Nazneen Fatema Rajani
    id: nazneen-fatema-rajani
    last: Rajani
  - first: Raymond
    full: Raymond Mooney
    id: raymond-mooney
    last: Mooney
  author_string: Nazneen Fatema Rajani, Raymond Mooney
  bibkey: rajani-mooney-2018-stacking
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1201
  month: June
  page_first: '2217'
  page_last: '2226'
  pages: "2217\u20132226"
  paper_id: '201'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1201.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1201.jpg
  title: Stacking with Auxiliary Features for Visual Question Answering
  title_html: Stacking with Auxiliary Features for Visual Question Answering
  url: https://www.aclweb.org/anthology/N18-1201
  year: '2018'
N18-1202:
  abstract: We introduce a new type of deep contextualized word representation that
    models both (1) complex characteristics of word use (e.g., syntax and semantics),
    and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).
    Our word vectors are learned functions of the internal states of a deep bidirectional
    language model (biLM), which is pre-trained on a large text corpus. We show that
    these representations can be easily added to existing models and significantly
    improve the state of the art across six challenging NLP problems, including question
    answering, textual entailment and sentiment analysis. We also present an analysis
    showing that exposing the deep internals of the pre-trained network is crucial,
    allowing downstream models to mix different types of semi-supervision signals.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1202.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1202.Notes.pdf
  - filename: http://vimeo.com/277672840
    type: video
    url: http://vimeo.com/277672840
  author:
  - first: Matthew
    full: Matthew Peters
    id: matthew-peters
    last: Peters
  - first: Mark
    full: Mark Neumann
    id: mark-neumann
    last: Neumann
  - first: Mohit
    full: Mohit Iyyer
    id: mohit-iyyer
    last: Iyyer
  - first: Matt
    full: Matt Gardner
    id: matt-gardner
    last: Gardner
  - first: Christopher
    full: Christopher Clark
    id: christopher-clark
    last: Clark
  - first: Kenton
    full: Kenton Lee
    id: kenton-lee
    last: Lee
  - first: Luke
    full: Luke Zettlemoyer
    id: luke-zettlemoyer
    last: Zettlemoyer
  author_string: Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher
    Clark, Kenton Lee, Luke Zettlemoyer
  bibkey: peters-etal-2018-deep
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1202
  month: June
  page_first: '2227'
  page_last: '2237'
  pages: "2227\u20132237"
  paper_id: '202'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1202.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1202.jpg
  title: Deep Contextualized Word Representations
  title_html: Deep Contextualized Word Representations
  url: https://www.aclweb.org/anthology/N18-1202
  year: '2018'
N18-1203:
  abstract: We propose a context-dependent model to map utterances within an interaction
    to executable formal queries. To incorporate interaction history, the model maintains
    an interaction-level encoder that updates after each turn, and can copy sub-sequences
    of previously predicted queries during generation. Our approach combines implicit
    and explicit modeling of references between utterances. We evaluate our model
    on the ATIS flight planning interactions, and demonstrate the benefits of modeling
    context and explicit references.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-1203.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-1203.Notes.pdf
  - filename: http://vimeo.com/277672864
    type: video
    url: http://vimeo.com/277672864
  author:
  - first: Alane
    full: Alane Suhr
    id: alane-suhr
    last: Suhr
  - first: Srinivasan
    full: Srinivasan Iyer
    id: srinivasan-iyer
    last: Iyer
  - first: Yoav
    full: Yoav Artzi
    id: yoav-artzi
    last: Artzi
  author_string: Alane Suhr, Srinivasan Iyer, Yoav Artzi
  bibkey: suhr-etal-2018-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1203
  month: June
  page_first: '2238'
  page_last: '2249'
  pages: "2238\u20132249"
  paper_id: '203'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1203.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1203.jpg
  title: Learning to Map Context-Dependent Sentences to Executable Formal Queries
  title_html: Learning to Map Context-Dependent Sentences to Executable Formal Queries
  url: https://www.aclweb.org/anthology/N18-1203
  year: '2018'
N18-1204:
  abstract: 'We introduce an approach to neural text generation that explicitly represents
    entities mentioned in the text. Entity representations are vectors that are updated
    as the text proceeds; they are designed specifically for narrative text like fiction
    or news stories. Our experiments demonstrate that modeling entities offers a benefit
    in two automatic evaluations: mention generation (in which a model chooses which
    entity to mention next and which words to use in the mention) and selection between
    a correct next sentence and a distractor from later in the same story. We also
    conduct a human evaluation on automatically generated text in story contexts;
    this study supports our emphasis on entities and suggests directions for further
    research.'
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/277672801
    type: video
    url: http://vimeo.com/277672801
  author:
  - first: Elizabeth
    full: Elizabeth Clark
    id: elizabeth-clark
    last: Clark
  - first: Yangfeng
    full: Yangfeng Ji
    id: yangfeng-ji
    last: Ji
  - first: Noah A.
    full: Noah A. Smith
    id: noah-a-smith
    last: Smith
  author_string: Elizabeth Clark, Yangfeng Ji, Noah A. Smith
  bibkey: clark-etal-2018-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1204
  month: June
  page_first: '2250'
  page_last: '2260'
  pages: "2250\u20132260"
  paper_id: '204'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1204.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1204.jpg
  title: Neural Text Generation in Stories Using Entity Representations as Context
  title_html: Neural Text Generation in Stories Using Entity Representations as Context
  url: https://www.aclweb.org/anthology/N18-1204
  year: '2018'
N18-1205:
  abstract: We investigate the computational complexity of various problems for simple
    recurrent neural networks (RNNs) as formal models for recognizing weighted languages.
    We focus on the single-layer, ReLU-activation, rational-weight RNNs with softmax,
    which are commonly used in natural language processing applications. We show that
    most problems for such RNNs are undecidable, including consistency, equivalence,
    minimization, and the determination of the highest-weighted string. However, for
    consistent RNNs the last problem becomes decidable, although the solution length
    can surpass all computable bounds. If additionally the string is limited to polynomial
    length, the problem becomes NP-complete. In summary, this shows that approximations
    and heuristic algorithms are necessary in practical applications of those RNNs.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/277672723
    type: video
    url: http://vimeo.com/277672723
  author:
  - first: Yining
    full: Yining Chen
    id: yining-chen
    last: Chen
  - first: Sorcha
    full: Sorcha Gilroy
    id: sorcha-gilroy
    last: Gilroy
  - first: Andreas
    full: Andreas Maletti
    id: andreas-maletti
    last: Maletti
  - first: Jonathan
    full: Jonathan May
    id: jonathan-may
    last: May
  - first: Kevin
    full: Kevin Knight
    id: kevin-knight
    last: Knight
  author_string: Yining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan May, Kevin
    Knight
  bibkey: chen-etal-2018-recurrent
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)'
  doi: 10.18653/v1/N18-1205
  month: June
  page_first: '2261'
  page_last: '2271'
  pages: "2261\u20132271"
  paper_id: '205'
  parent_volume_id: N18-1
  pdf: https://www.aclweb.org/anthology/N18-1205.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-1205.jpg
  title: Recurrent Neural Networks as Weighted Language Recognizers
  title_html: Recurrent Neural Networks as Weighted Language Recognizers
  url: https://www.aclweb.org/anthology/N18-1205
  year: '2018'
N18-2000:
  address: New Orleans, Louisiana
  author:
  - first: Marilyn
    full: Marilyn Walker
    id: marilyn-walker
    last: Walker
  - first: Heng
    full: Heng Ji
    id: heng-ji
    last: Ji
  - first: Amanda
    full: Amanda Stent
    id: amanda-stent
    last: Stent
  author_string: Marilyn Walker, Heng Ji, Amanda Stent
  bibkey: naacl-2018-2018-north
  bibtype: proceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  doi: 10.18653/v1/N18-2
  month: June
  paper_id: '0'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2000.jpg
  title: 'Proceedings of the 2018 Conference of the North American Chapter of the
    Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  title_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  url: https://www.aclweb.org/anthology/N18-2000
  year: '2018'
N18-2001:
  abstract: Most current models of word representations (e.g., GloVe) have successfully
    captured fine-grained semantics. However, semantic similarity exhibited in these
    word embeddings is not suitable for resolving bridging anaphora, which requires
    the knowledge of associative similarity (i.e., relatedness) instead of semantic
    similarity information between synonyms or hypernyms. We create word embeddings
    (embeddings_PP) to capture such relatedness by exploring the syntactic structure
    of noun phrases. We demonstrate that using embeddings _PP alone achieves around
    30% of accuracy for bridging anaphora resolution on the ISNotes corpus. Furthermore,
    we achieve a substantial gain over the state-of-the-art system (Hou et al., 2013b)
    for bridging antecedent selection.
  address: New Orleans, Louisiana
  author:
  - first: Yufang
    full: Yufang Hou
    id: yufang-hou
    last: Hou
  author_string: Yufang Hou
  bibkey: hou-2018-enhanced
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2001
  month: June
  page_first: '1'
  page_last: '7'
  pages: "1\u20137"
  paper_id: '1'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2001.jpg
  title: Enhanced Word Representations for Bridging Anaphora Resolution
  title_html: Enhanced Word Representations for Bridging Anaphora Resolution
  url: https://www.aclweb.org/anthology/N18-2001
  year: '2018'
N18-2002:
  abstract: "We present an empirical study of gender bias in coreference resolution\
    \ systems. We first introduce a novel, Winograd schema-style set of minimal pair\
    \ sentences that differ only by pronoun gender. With these \u201CWinogender schemas,\u201D\
    \ we evaluate and confirm systematic gender bias in three publicly-available coreference\
    \ resolution systems, and correlate this bias with real-world and textual gender\
    \ statistics."
  address: New Orleans, Louisiana
  author:
  - first: Rachel
    full: Rachel Rudinger
    id: rachel-rudinger
    last: Rudinger
  - first: Jason
    full: Jason Naradowsky
    id: jason-naradowsky
    last: Naradowsky
  - first: Brian
    full: Brian Leonard
    id: brian-leonard
    last: Leonard
  - first: Benjamin
    full: Benjamin Van Durme
    id: benjamin-van-durme
    last: Van Durme
  author_string: Rachel Rudinger, Jason Naradowsky, Brian Leonard, Benjamin Van Durme
  bibkey: rudinger-etal-2018-gender
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2002
  month: June
  page_first: '8'
  page_last: '14'
  pages: "8\u201314"
  paper_id: '2'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2002.jpg
  title: Gender Bias in Coreference Resolution
  title_html: Gender Bias in Coreference Resolution
  url: https://www.aclweb.org/anthology/N18-2002
  year: '2018'
N18-2003:
  abstract: In this paper, we introduce a new benchmark for co-reference resolution
    focused on gender bias, WinoBias. Our corpus contains Winograd-schema style sentences
    with entities corresponding to people referred by their occupation (e.g. the nurse,
    the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich,
    and a neural coreference system all link gendered pronouns to pro-stereotypical
    entities with higher accuracy than anti-stereotypical entities, by an average
    difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach
    that, in combination with existing word-embedding debiasing techniques, removes
    the bias demonstrated by these systems in WinoBias without significantly affecting
    their performance on existing datasets.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-2003.Datasets.zip
    type: dataset
    url: https://www.aclweb.org/anthology/attachments/N18-2003.Datasets.zip
  author:
  - first: Jieyu
    full: Jieyu Zhao
    id: jieyu-zhao
    last: Zhao
  - first: Tianlu
    full: Tianlu Wang
    id: tianlu-wang
    last: Wang
  - first: Mark
    full: Mark Yatskar
    id: mark-yatskar
    last: Yatskar
  - first: Vicente
    full: Vicente Ordonez
    id: vicente-ordonez
    last: Ordonez
  - first: Kai-Wei
    full: Kai-Wei Chang
    id: kai-wei-chang
    last: Chang
  author_string: Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, Kai-Wei Chang
  bibkey: zhao-etal-2018-gender
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2003
  month: June
  page_first: '15'
  page_last: '20'
  pages: "15\u201320"
  paper_id: '3'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2003.jpg
  title: 'Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods'
  title_html: 'Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods'
  url: https://www.aclweb.org/anthology/N18-2003
  year: '2018'
N18-2004:
  abstract: "A reasonable approach for fact checking a claim involves retrieving potentially\
    \ relevant documents from different sources (e.g., news websites, social media,\
    \ etc.), determining the stance of each document with respect to the claim, and\
    \ finally making a prediction about the claim\u2019s factuality by aggregating\
    \ the strength of the stances, while taking the reliability of the source into\
    \ account. Moreover, a fact checking system should be able to explain its decision\
    \ by providing relevant extracts (rationales) from the documents. Yet, this setup\
    \ is not directly supported by existing datasets, which treat fact checking, document\
    \ retrieval, source credibility, stance detection and rationale extraction as\
    \ independent tasks. In this paper, we support the interdependencies between these\
    \ tasks as annotations in the same corpus. We implement this setup on an Arabic\
    \ fact checking corpus, the first of its kind."
  address: New Orleans, Louisiana
  author:
  - first: Ramy
    full: Ramy Baly
    id: ramy-baly
    last: Baly
  - first: Mitra
    full: Mitra Mohtarami
    id: mitra-mohtarami
    last: Mohtarami
  - first: James
    full: James Glass
    id: james-glass
    last: Glass
  - first: "Llu\xEDs"
    full: "Llu\xEDs M\xE0rquez"
    id: lluis-marquez
    last: "M\xE0rquez"
  - first: Alessandro
    full: Alessandro Moschitti
    id: alessandro-moschitti
    last: Moschitti
  - first: Preslav
    full: Preslav Nakov
    id: preslav-nakov
    last: Nakov
  author_string: "Ramy Baly, Mitra Mohtarami, James Glass, Llu\xEDs M\xE0rquez, Alessandro\
    \ Moschitti, Preslav Nakov"
  bibkey: baly-etal-2018-integrating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2004
  month: June
  page_first: '21'
  page_last: '27'
  pages: "21\u201327"
  paper_id: '4'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2004.jpg
  title: Integrating Stance Detection and Fact Checking in a Unified Corpus
  title_html: Integrating Stance Detection and Fact Checking in a Unified Corpus
  url: https://www.aclweb.org/anthology/N18-2004
  year: '2018'
N18-2005:
  abstract: Online reviews have become a popular portal among customers making decisions
    about purchasing products. A number of corpora of reviews have been widely investigated
    in NLP in general, and, in particular, in argument mining. This is a subset of
    NLP that deals with extracting arguments and the relations among them from user-based
    content. A major problem faced by argument mining research is the lack of human-annotated
    data. In this paper, we investigate the use of weakly supervised and semi-supervised
    methods for automatically annotating data, and thus providing large annotated
    datasets. We do this by building on previous work that explores the classification
    of opinions present in reviews based whether the stance is expressed explicitly
    or implicitly. In the work described here, we automatically annotate stance as
    implicit or explicit and our results show that the datasets we generate, although
    noisy, can be used to learn better models for implicit/explicit opinion classification.
  address: New Orleans, Louisiana
  author:
  - first: Pavithra
    full: Pavithra Rajendran
    id: pavithra-rajendran
    last: Rajendran
  - first: Danushka
    full: Danushka Bollegala
    id: danushka-bollegala
    last: Bollegala
  - first: Simon
    full: Simon Parsons
    id: simon-parsons
    last: Parsons
  author_string: Pavithra Rajendran, Danushka Bollegala, Simon Parsons
  bibkey: rajendran-etal-2018-something
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2005
  month: June
  page_first: '28'
  page_last: '34'
  pages: "28\u201334"
  paper_id: '5'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2005.jpg
  title: Is Something Better than Nothing? Automatically Predicting Stance-based Arguments
    Using Deep Learning and Small Labelled Dataset
  title_html: Is Something Better than Nothing? Automatically Predicting Stance-based
    Arguments Using Deep Learning and Small Labelled Dataset
  url: https://www.aclweb.org/anthology/N18-2005
  year: '2018'
N18-2006:
  abstract: We investigate whether and where multi-task learning (MTL) can improve
    performance on NLP problems related to argumentation mining (AM), in particular
    argument component identification. Our results show that MTL performs particularly
    well (and better than single-task learning) when little training data is available
    for the main task, a common scenario in AM. Our findings challenge previous assumptions
    that conceptualizations across AM datasets are divergent and that MTL is difficult
    for semantic or higher-level tasks.
  address: New Orleans, Louisiana
  author:
  - first: Claudia
    full: Claudia Schulz
    id: claudia-schulz
    last: Schulz
  - first: Steffen
    full: Steffen Eger
    id: steffen-eger
    last: Eger
  - first: Johannes
    full: Johannes Daxenberger
    id: johannes-daxenberger
    last: Daxenberger
  - first: Tobias
    full: Tobias Kahse
    id: tobias-kahse
    last: Kahse
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Claudia Schulz, Steffen Eger, Johannes Daxenberger, Tobias Kahse,
    Iryna Gurevych
  bibkey: schulz-etal-2018-multi
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2006
  month: June
  page_first: '35'
  page_last: '41'
  pages: "35\u201341"
  paper_id: '6'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2006.jpg
  title: Multi-Task Learning for Argumentation Mining in Low-Resource Settings
  title_html: Multi-Task Learning for Argumentation Mining in Low-Resource Settings
  url: https://www.aclweb.org/anthology/N18-2006
  year: '2018'
N18-2007:
  abstract: "Many problems in NLP require aggregating information from multiple mentions\
    \ of the same entity which may be far apart in the text. Existing Recurrent Neural\
    \ Network (RNN) layers are biased towards short-term dependencies and hence not\
    \ suited to such tasks. We present a recurrent layer which is instead biased towards\
    \ coreferent dependencies. The layer uses coreference annotations extracted from\
    \ an external system to connect entity mentions belonging to the same cluster.\
    \ Incorporating this layer into a state-of-the-art reading comprehension model\
    \ improves performance on three datasets \u2013 Wikihop, LAMBADA and the bAbi\
    \ AI tasks \u2013 with large gains when training data is scarce."
  address: New Orleans, Louisiana
  author:
  - first: Bhuwan
    full: Bhuwan Dhingra
    id: bhuwan-dhingra
    last: Dhingra
  - first: Qiao
    full: Qiao Jin
    id: qiao-jin
    last: Jin
  - first: Zhilin
    full: Zhilin Yang
    id: zhilin-yang
    last: Yang
  - first: William
    full: William Cohen
    id: william-cohen
    last: Cohen
  - first: Ruslan
    full: Ruslan Salakhutdinov
    id: ruslan-salakhutdinov
    last: Salakhutdinov
  author_string: Bhuwan Dhingra, Qiao Jin, Zhilin Yang, William Cohen, Ruslan Salakhutdinov
  bibkey: dhingra-etal-2018-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2007
  month: June
  page_first: '42'
  page_last: '48'
  pages: "42\u201348"
  paper_id: '7'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2007.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2007.jpg
  title: Neural Models for Reasoning over Multiple Mentions Using Coreference
  title_html: Neural Models for Reasoning over Multiple Mentions Using Coreference
  url: https://www.aclweb.org/anthology/N18-2007
  year: '2018'
N18-2008:
  abstract: Despite myriad efforts in the literature designing neural dialogue generation
    systems in recent years, very few consider putting restrictions on the response
    itself. They learn from collections of past responses and generate one based on
    a given utterance without considering, speech act, desired style or emotion to
    be expressed. In this research, we address the problem of forcing the dialogue
    generation to express emotion. We present three models that either concatenate
    the desired emotion with the source input during the learning, or push the emotion
    in the decoder. The results, evaluated with an emotion tagger, are encouraging
    with all three models, but present better outcome and promise with our model that
    adds the emotion vector in the decoder.
  address: New Orleans, Louisiana
  author:
  - first: Chenyang
    full: Chenyang Huang
    id: chenyang-huang
    last: Huang
  - first: Osmar
    full: "Osmar Za\xEFane"
    id: osmar-r-zaiane
    last: "Za\xEFane"
  - first: Amine
    full: Amine Trabelsi
    id: amine-trabelsi
    last: Trabelsi
  - first: Nouha
    full: Nouha Dziri
    id: nouha-dziri
    last: Dziri
  author_string: "Chenyang Huang, Osmar Za\xEFane, Amine Trabelsi, Nouha Dziri"
  bibkey: huang-etal-2018-automatic
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2008
  month: June
  page_first: '49'
  page_last: '54'
  pages: "49\u201354"
  paper_id: '8'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2008.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2008.jpg
  title: Automatic Dialogue Generation with Expressed Emotions
  title_html: Automatic Dialogue Generation with Expressed Emotions
  url: https://www.aclweb.org/anthology/N18-2008
  year: '2018'
N18-2009:
  abstract: Neural network models, based on the attentional encoder-decoder model,
    have good capability in abstractive text summarization. However, these models
    are hard to be controlled in the process of generation, which leads to a lack
    of key information. We propose a guiding generation model that combines the extractive
    method and the abstractive method. Firstly, we obtain keywords from the text by
    a extractive model. Then, we introduce a Key Information Guide Network (KIGN),
    which encodes the keywords to the key information representation, to guide the
    process of generation. In addition, we use a prediction-guide mechanism, which
    can obtain the long-term value for future decoding, to further guide the summary
    generation. We evaluate our model on the CNN/Daily Mail dataset. The experimental
    results show that our model leads to significant improvements.
  address: New Orleans, Louisiana
  author:
  - first: Chenliang
    full: Chenliang Li
    id: chenliang-li
    last: Li
  - first: Weiran
    full: Weiran Xu
    id: weiran-xu
    last: Xu
  - first: Si
    full: Si Li
    id: si-li
    last: Li
  - first: Sheng
    full: Sheng Gao
    id: sheng-gao
    last: Gao
  author_string: Chenliang Li, Weiran Xu, Si Li, Sheng Gao
  bibkey: li-etal-2018-guiding
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2009
  month: June
  page_first: '55'
  page_last: '60'
  pages: "55\u201360"
  paper_id: '9'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2009.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2009.jpg
  title: Guiding Generation for Abstractive Text Summarization Based on Key Information
    Guide Network
  title_html: Guiding Generation for Abstractive Text Summarization Based on Key Information
    Guide Network
  url: https://www.aclweb.org/anthology/N18-2009
  year: '2018'
N18-2010:
  abstract: 'Natural language generation (NLG) is a critical component in spoken dialogue
    systems. Classic NLG can be divided into two phases: (1) sentence planning: deciding
    on the overall sentence structure, (2) surface realization: determining specific
    word forms and flattening the sentence structure into a string. Many simple NLG
    models are based on recurrent neural networks (RNN) and sequence-to-sequence (seq2seq)
    model, which basically contains a encoder-decoder structure; these NLG models
    generate sentences from scratch by jointly optimizing sentence planning and surface
    realization using a simple cross entropy loss training criterion. However, the
    simple encoder-decoder architecture usually suffers from generating complex and
    long sentences, because the decoder has to learn all grammar and diction knowledge.
    This paper introduces a hierarchical decoding NLG model based on linguistic patterns
    in different levels, and shows that the proposed method outperforms the traditional
    one with a smaller model size. Furthermore, the design of the hierarchical decoding
    is flexible and easily-extendible in various NLG systems.'
  address: New Orleans, Louisiana
  author:
  - first: Shang-Yu
    full: Shang-Yu Su
    id: shang-yu-su
    last: Su
  - first: Kai-Ling
    full: Kai-Ling Lo
    id: kai-ling-lo
    last: Lo
  - first: Yi-Ting
    full: Yi-Ting Yeh
    id: yi-ting-yeh
    last: Yeh
  - first: Yun-Nung
    full: Yun-Nung Chen
    id: yun-nung-chen
    last: Chen
  author_string: Shang-Yu Su, Kai-Ling Lo, Yi-Ting Yeh, Yun-Nung Chen
  bibkey: su-etal-2018-natural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2010
  month: June
  page_first: '61'
  page_last: '66'
  pages: "61\u201366"
  paper_id: '10'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2010.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2010.jpg
  title: Natural Language Generation by Hierarchical Decoding with Linguistic Patterns
  title_html: Natural Language Generation by Hierarchical Decoding with Linguistic
    Patterns
  url: https://www.aclweb.org/anthology/N18-2010
  year: '2018'
N18-2011:
  abstract: We present the first neural poetry translation system. Unlike previous
    works that often fail to produce any translation for fixed rhyme and rhythm patterns,
    our system always translates a source text to an English poem. Human evaluation
    of the translations ranks the quality as acceptable 78.2% of the time.
  address: New Orleans, Louisiana
  author:
  - first: Marjan
    full: Marjan Ghazvininejad
    id: marjan-ghazvininejad
    last: Ghazvininejad
  - first: Yejin
    full: Yejin Choi
    id: yejin-choi
    last: Choi
  - first: Kevin
    full: Kevin Knight
    id: kevin-knight
    last: Knight
  author_string: Marjan Ghazvininejad, Yejin Choi, Kevin Knight
  bibkey: ghazvininejad-etal-2018-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2011
  month: June
  page_first: '67'
  page_last: '71'
  pages: "67\u201371"
  paper_id: '11'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2011.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2011.jpg
  title: Neural Poetry Translation
  title_html: Neural Poetry Translation
  url: https://www.aclweb.org/anthology/N18-2011
  year: '2018'
N18-2012:
  abstract: Human evaluation for natural language generation (NLG) often suffers from
    inconsistent user ratings. While previous research tends to attribute this problem
    to individual user preferences, we show that the quality of human judgements can
    also be improved by experimental design. We present a novel rank-based magnitude
    estimation method (RankME), which combines the use of continuous scales and relative
    assessments. We show that RankME significantly improves the reliability and consistency
    of human ratings compared to traditional evaluation methods. In addition, we show
    that it is possible to evaluate NLG systems according to multiple, distinct criteria,
    which is important for error analysis. Finally, we demonstrate that RankME, in
    combination with Bayesian estimation of system quality, is a cost-effective alternative
    for ranking multiple NLG systems.
  address: New Orleans, Louisiana
  author:
  - first: Jekaterina
    full: Jekaterina Novikova
    id: jekaterina-novikova
    last: Novikova
  - first: "Ond\u0159ej"
    full: "Ond\u0159ej Du\u0161ek"
    id: ondrej-dusek
    last: "Du\u0161ek"
  - first: Verena
    full: Verena Rieser
    id: verena-rieser
    last: Rieser
  author_string: "Jekaterina Novikova, Ond\u0159ej Du\u0161ek, Verena Rieser"
  bibkey: novikova-etal-2018-rankme
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2012
  month: June
  page_first: '72'
  page_last: '78'
  pages: "72\u201378"
  paper_id: '12'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2012.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2012.jpg
  title: 'RankME: Reliable Human Ratings for Natural Language Generation'
  title_html: '<span class="acl-fixed-case">R</span>ank<span class="acl-fixed-case">ME</span>:
    Reliable Human Ratings for Natural Language Generation'
  url: https://www.aclweb.org/anthology/N18-2012
  year: '2018'
N18-2013:
  abstract: Sentence simplification aims to simplify the content and structure of
    complex sentences, and thus make them easier to interpret for human readers, and
    easier to process for downstream NLP applications. Recent advances in neural machine
    translation have paved the way for novel approaches to the task. In this paper,
    we adapt an architecture with augmented memory capacities called Neural Semantic
    Encoders (Munkhdalai and Yu, 2017) for sentence simplification. Our experiments
    demonstrate the effectiveness of our approach on different simplification datasets,
    both in terms of automatic evaluation measures and human judgments.
  address: New Orleans, Louisiana
  author:
  - first: Tu
    full: Tu Vu
    id: tu-vu
    last: Vu
  - first: Baotian
    full: Baotian Hu
    id: baotian-hu
    last: Hu
  - first: Tsendsuren
    full: Tsendsuren Munkhdalai
    id: tsendsuren-munkhdalai
    last: Munkhdalai
  - first: Hong
    full: Hong Yu
    id: hong-yu
    last: Yu
  author_string: Tu Vu, Baotian Hu, Tsendsuren Munkhdalai, Hong Yu
  bibkey: vu-etal-2018-sentence
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2013
  month: June
  page_first: '79'
  page_last: '85'
  pages: "79\u201385"
  paper_id: '13'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2013.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2013.jpg
  title: Sentence Simplification with Memory-Augmented Neural Networks
  title_html: Sentence Simplification with Memory-Augmented Neural Networks
  url: https://www.aclweb.org/anthology/N18-2013
  year: '2018'
N18-2014:
  abstract: We present a corpus of 240 argumentative essays written by non-native
    speakers of English annotated for metaphor. The corpus is made publicly available.
    We provide benchmark performance of state-of-the-art systems on this new corpus,
    and explore the relationship between writing proficiency and metaphor use.
  address: New Orleans, Louisiana
  author:
  - first: Beata
    full: Beata Beigman Klebanov
    id: beata-beigman-klebanov
    last: Beigman Klebanov
  - first: Chee Wee (Ben)
    full: Chee Wee (Ben) Leong
    id: chee-wee-leong
    last: Leong
  - first: Michael
    full: Michael Flor
    id: michael-flor
    last: Flor
  author_string: Beata Beigman Klebanov, Chee Wee (Ben) Leong, Michael Flor
  bibkey: beigman-klebanov-etal-2018-corpus
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2014
  month: June
  page_first: '86'
  page_last: '91'
  pages: "86\u201391"
  paper_id: '14'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2014.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2014.jpg
  title: A Corpus of Non-Native Written English Annotated for Metaphor
  title_html: A Corpus of Non-Native Written <span class="acl-fixed-case">E</span>nglish
    Annotated for Metaphor
  url: https://www.aclweb.org/anthology/N18-2014
  year: '2018'
N18-2015:
  abstract: "In the Story Cloze Test, a system is presented with a 4-sentence prompt\
    \ to a story, and must determine which one of two potential endings is the \u2018\
    right\u2019 ending to the story. Previous work has shown that ignoring the training\
    \ set and training a model on the validation set can achieve high accuracy on\
    \ this task due to stylistic differences between the story endings in the training\
    \ set and validation and test sets. Following this approach, we present a simpler\
    \ fully-neural approach to the Story Cloze Test using skip-thought embeddings\
    \ of the stories in a feed-forward network that achieves close to state-of-the-art\
    \ performance on this task without any feature engineering. We also find that\
    \ considering just the last sentence of the prompt instead of the whole prompt\
    \ yields higher accuracy with our approach."
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-2015.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-2015.Notes.pdf
  author:
  - first: Siddarth
    full: Siddarth Srinivasan
    id: siddarth-srinivasan
    last: Srinivasan
  - first: Richa
    full: Richa Arora
    id: richa-arora
    last: Arora
  - first: Mark
    full: Mark Riedl
    id: mark-riedl
    last: Riedl
  author_string: Siddarth Srinivasan, Richa Arora, Mark Riedl
  bibkey: srinivasan-etal-2018-simple
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2015
  month: June
  page_first: '92'
  page_last: '96'
  pages: "92\u201396"
  paper_id: '15'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2015.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2015.jpg
  title: A Simple and Effective Approach to the Story Cloze Test
  title_html: A Simple and Effective Approach to the Story Cloze Test
  url: https://www.aclweb.org/anthology/N18-2015
  year: '2018'
N18-2016:
  abstract: We describe an effort to annotate a corpus of natural language instructions
    consisting of 622 wet lab protocols to facilitate automatic or semi-automatic
    conversion of protocols into a machine-readable format and benefit biological
    research. Experimental results demonstrate the utility of our corpus for developing
    machine learning approaches to shallow semantic parsing of instructional texts.
    We make our annotated Wet Lab Protocol Corpus available to the research community.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-2016.Datasets.zip
    type: dataset
    url: https://www.aclweb.org/anthology/attachments/N18-2016.Datasets.zip
  author:
  - first: Chaitanya
    full: Chaitanya Kulkarni
    id: chaitanya-kulkarni
    last: Kulkarni
  - first: Wei
    full: Wei Xu
    id: wei-xu
    last: Xu
  - first: Alan
    full: Alan Ritter
    id: alan-ritter
    last: Ritter
  - first: Raghu
    full: Raghu Machiraju
    id: raghu-machiraju
    last: Machiraju
  author_string: Chaitanya Kulkarni, Wei Xu, Alan Ritter, Raghu Machiraju
  bibkey: kulkarni-etal-2018-annotated
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2016
  month: June
  page_first: '97'
  page_last: '106'
  pages: "97\u2013106"
  paper_id: '16'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2016.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2016.jpg
  title: An Annotated Corpus for Machine Reading of Instructions in Wet Lab Protocols
  title_html: An Annotated Corpus for Machine Reading of Instructions in Wet Lab Protocols
  url: https://www.aclweb.org/anthology/N18-2016
  year: '2018'
N18-2017:
  abstract: Large-scale datasets for natural language inference are created by presenting
    crowd workers with a sentence (premise), and asking them to generate three new
    sentences (hypotheses) that it entails, contradicts, or is logically neutral with
    respect to. We show that, in a significant portion of such data, this protocol
    leaves clues that make it possible to identify the label by looking only at the
    hypothesis, without observing the premise. Specifically, we show that a simple
    text categorization model can correctly classify the hypothesis alone in about
    67% of SNLI (Bowman et. al, 2015) and 53% of MultiNLI (Williams et. al, 2017).
    Our analysis reveals that specific linguistic phenomena such as negation and vagueness
    are highly correlated with certain inference classes. Our findings suggest that
    the success of natural language inference models to date has been overestimated,
    and that the task remains a hard open problem.
  address: New Orleans, Louisiana
  author:
  - first: Suchin
    full: Suchin Gururangan
    id: suchin-gururangan
    last: Gururangan
  - first: Swabha
    full: Swabha Swayamdipta
    id: swabha-swayamdipta
    last: Swayamdipta
  - first: Omer
    full: Omer Levy
    id: omer-levy
    last: Levy
  - first: Roy
    full: Roy Schwartz
    id: roy-schwartz
    last: Schwartz
  - first: Samuel
    full: Samuel Bowman
    id: samuel-bowman
    last: Bowman
  - first: Noah A.
    full: Noah A. Smith
    id: noah-a-smith
    last: Smith
  author_string: Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel
    Bowman, Noah A. Smith
  bibkey: gururangan-etal-2018-annotation
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2017
  month: June
  page_first: '107'
  page_last: '112'
  pages: "107\u2013112"
  paper_id: '17'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2017.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2017.jpg
  title: Annotation Artifacts in Natural Language Inference Data
  title_html: Annotation Artifacts in Natural Language Inference Data
  url: https://www.aclweb.org/anthology/N18-2017
  year: '2018'
N18-2018:
  abstract: Humor is an essential but most fascinating element in personal communication.
    How to build computational models to discover the structures of humor, recognize
    humor and even generate humor remains a challenge and there have been yet few
    attempts on it. In this paper, we construct and collect four datasets with distinct
    joke types in both English and Chinese and conduct learning experiments on humor
    recognition. We implement a Convolutional Neural Network (CNN) with extensive
    filter size, number and Highway Networks to increase the depth of networks. Results
    show that our model outperforms in recognition of different types of humor with
    benchmarks collected in both English and Chinese languages on accuracy, precision,
    and recall in comparison to previous works.
  address: New Orleans, Louisiana
  author:
  - first: Peng-Yu
    full: Peng-Yu Chen
    id: peng-yu-chen
    last: Chen
  - first: Von-Wun
    full: Von-Wun Soo
    id: von-wun-soo
    last: Soo
  author_string: Peng-Yu Chen, Von-Wun Soo
  bibkey: chen-soo-2018-humor
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2018
  month: June
  page_first: '113'
  page_last: '117'
  pages: "113\u2013117"
  paper_id: '18'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2018.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2018.jpg
  title: Humor Recognition Using Deep Learning
  title_html: Humor Recognition Using Deep Learning
  url: https://www.aclweb.org/anthology/N18-2018
  year: '2018'
N18-2019:
  abstract: "Hate speech detection is a critical, yet challenging problem in Natural\
    \ Language Processing (NLP). Despite the existence of numerous studies dedicated\
    \ to the development of NLP hate speech detection approaches, the accuracy is\
    \ still poor. The central problem is that social media posts are short and noisy,\
    \ and most existing hate speech detection solutions take each post as an isolated\
    \ input instance, which is likely to yield high false positive and negative rates.\
    \ In this paper, we radically improve automated hate speech detection by presenting\
    \ a novel model that leverages intra-user and inter-user representation learning\
    \ for robust hate speech detection on Twitter. In addition to the target Tweet,\
    \ we collect and analyze the user\u2019s historical posts to model intra-user\
    \ Tweet representations. To suppress the noise in a single Tweet, we also model\
    \ the similar Tweets posted by all other users with reinforced inter-user representation\
    \ learning techniques. Experimentally, we show that leveraging these two representations\
    \ can significantly improve the f-score of a strong bidirectional LSTM baseline\
    \ model by 10.1%."
  address: New Orleans, Louisiana
  author:
  - first: Jing
    full: Jing Qian
    id: jing-qian
    last: Qian
  - first: Mai
    full: Mai ElSherief
    id: mai-elsherief
    last: ElSherief
  - first: Elizabeth
    full: Elizabeth Belding
    id: elizabeth-belding
    last: Belding
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  author_string: Jing Qian, Mai ElSherief, Elizabeth Belding, William Yang Wang
  bibkey: qian-etal-2018-leveraging
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2019
  month: June
  page_first: '118'
  page_last: '123'
  pages: "118\u2013123"
  paper_id: '19'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2019.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2019.jpg
  title: Leveraging Intra-User and Inter-User Representation Learning for Automated
    Hate Speech Detection
  title_html: Leveraging Intra-User and Inter-User Representation Learning for Automated
    Hate Speech Detection
  url: https://www.aclweb.org/anthology/N18-2019
  year: '2018'
N18-2020:
  abstract: "We propose USim, a semantic measure for Grammatical Error Correction\
    \ (that measures the semantic faithfulness of the output to the source, thereby\
    \ complementing existing reference-less measures (RLMs) for measuring the output\u2019\
    s grammaticality. USim operates by comparing the semantic symbolic structure of\
    \ the source and the correction, without relying on manually-curated references.\
    \ Our experiments establish the validity of USim, by showing that the semantic\
    \ structures can be consistently applied to ungrammatical text, that valid corrections\
    \ obtain a high USim similarity score to the source, and that invalid corrections\
    \ obtain a lower score."
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-2020.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-2020.Notes.pdf
  author:
  - first: Leshem
    full: Leshem Choshen
    id: leshem-choshen
    last: Choshen
  - first: Omri
    full: Omri Abend
    id: omri-abend
    last: Abend
  author_string: Leshem Choshen, Omri Abend
  bibkey: choshen-abend-2018-reference
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2020
  month: June
  page_first: '124'
  page_last: '129'
  pages: "124\u2013129"
  paper_id: '20'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2020.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2020.jpg
  title: Reference-less Measure of Faithfulness for Grammatical Error Correction
  title_html: Reference-less Measure of Faithfulness for Grammatical Error Correction
  url: https://www.aclweb.org/anthology/N18-2020
  year: '2018'
N18-2021:
  abstract: This paper introduces rank-based training of structured prediction energy
    networks (SPENs). Our method samples from output structures using gradient descent
    and minimizes the ranking violation of the sampled structures with respect to
    a scalar scoring function defined with domain knowledge. We have successfully
    trained SPEN for citation field extraction without any labeled data instances,
    where the only source of supervision is a simple human-written scoring function.
    Such scoring functions are often easy to provide; the SPEN then furnishes an efficient
    structured prediction inference procedure.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276400814
    type: video
    url: http://vimeo.com/276400814
  author:
  - first: Amirmohammad
    full: Amirmohammad Rooshenas
    id: amirmohammad-rooshenas
    last: Rooshenas
  - first: Aishwarya
    full: Aishwarya Kamath
    id: aishwarya-kamath
    last: Kamath
  - first: Andrew
    full: Andrew McCallum
    id: andrew-mccallum
    last: McCallum
  author_string: Amirmohammad Rooshenas, Aishwarya Kamath, Andrew McCallum
  bibkey: rooshenas-etal-2018-training
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2021
  month: June
  page_first: '130'
  page_last: '135'
  pages: "130\u2013135"
  paper_id: '21'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2021.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2021.jpg
  title: Training Structured Prediction Energy Networks with Indirect Supervision
  title_html: Training Structured Prediction Energy Networks with Indirect Supervision
  url: https://www.aclweb.org/anthology/N18-2021
  year: '2018'
N18-2022:
  abstract: Political identity is often manifested in language variation, but the
    relationship between the two is still relatively unexplored from a quantitative
    perspective. This study examines the use of Catalan, a language local to the semi-autonomous
    region of Catalonia in Spain, on Twitter in discourse related to the 2017 independence
    referendum. We corroborate prior findings that pro-independence tweets are more
    likely to include the local language than anti-independence tweets. We also find
    that Catalan is used more often in referendum-related discourse than in other
    contexts, contrary to prior findings on language variation. This suggests a strong
    role for the Catalan language in the expression of Catalonian political identity.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276426716
    type: video
    url: http://vimeo.com/276426716
  author:
  - first: Ian
    full: Ian Stewart
    id: ian-stewart
    last: Stewart
  - first: Yuval
    full: Yuval Pinter
    id: yuval-pinter
    last: Pinter
  - first: Jacob
    full: Jacob Eisenstein
    id: jacob-eisenstein
    last: Eisenstein
  author_string: Ian Stewart, Yuval Pinter, Jacob Eisenstein
  bibkey: stewart-etal-2018-si
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2022
  month: June
  page_first: '136'
  page_last: '141'
  pages: "136\u2013141"
  paper_id: '22'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2022.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2022.jpg
  title: Si O No, Que Penses? Catalonian Independence and Linguistic Identity on Social
    Media
  title_html: Si O No, Que Penses? <span class="acl-fixed-case">C</span>atalonian
    Independence and Linguistic Identity on Social Media
  url: https://www.aclweb.org/anthology/N18-2022
  year: '2018'
N18-2023:
  abstract: Non-projective parsing can be useful to handle cycles and reentrancy in
    AMR graphs. We explore this idea and introduce a greedy left-to-right non-projective
    transition-based parser. At each parsing configuration, an oracle decides whether
    to create a concept or whether to connect a pair of existing concepts. The algorithm
    handles reentrancy and arbitrary cycles natively, i.e. within the transition system
    itself. The model is evaluated on the LDC2015E86 corpus, obtaining results close
    to the state of the art, including a Smatch of 64%, and showing good behavior
    on reentrant edges.
  address: New Orleans, Louisiana
  author:
  - first: David
    full: David Vilares
    id: david-vilares
    last: Vilares
  - first: Carlos
    full: "Carlos G\xF3mez-Rodr\xEDguez"
    id: carlos-gomez-rodriguez
    last: "G\xF3mez-Rodr\xEDguez"
  author_string: "David Vilares, Carlos G\xF3mez-Rodr\xEDguez"
  bibkey: vilares-gomez-rodriguez-2018-transition
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2023
  month: June
  page_first: '142'
  page_last: '149'
  pages: "142\u2013149"
  paper_id: '23'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2023.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2023.jpg
  title: A Transition-Based Algorithm for Unrestricted AMR Parsing
  title_html: A Transition-Based Algorithm for Unrestricted <span class="acl-fixed-case">AMR</span>
    Parsing
  url: https://www.aclweb.org/anthology/N18-2023
  year: '2018'
N18-2024:
  abstract: "We present a computational model to detect and distinguish analogies\
    \ in meaning shifts between German base and complex verbs. In contrast to corpus-based\
    \ studies, a novel dataset demonstrates that \u201Cregular\u201D shifts represent\
    \ the smallest class. Classification experiments relying on a standard similarity\
    \ model successfully distinguish between four types of shifts, with verb classes\
    \ boosting the performance, and affective features for abstractness, emotion and\
    \ sentiment representing the most salient indicators."
  address: New Orleans, Louisiana
  author:
  - first: Maximilian
    full: "Maximilian K\xF6per"
    id: maximilian-koper
    last: "K\xF6per"
  - first: Sabine
    full: Sabine Schulte im Walde
    id: sabine-schulte-im-walde
    last: Schulte im Walde
  author_string: "Maximilian K\xF6per, Sabine Schulte im Walde"
  bibkey: koper-schulte-im-walde-2018-analogies
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2024
  month: June
  page_first: '150'
  page_last: '156'
  pages: "150\u2013156"
  paper_id: '24'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2024.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2024.jpg
  title: 'Analogies in Complex Verb Meaning Shifts: the Effect of Affect in Semantic
    Similarity Models'
  title_html: 'Analogies in Complex Verb Meaning Shifts: the Effect of Affect in Semantic
    Similarity Models'
  url: https://www.aclweb.org/anthology/N18-2024
  year: '2018'
N18-2025:
  abstract: Sentence pair modeling is critical for many NLP tasks, such as paraphrase
    identification, semantic textual similarity, and natural language inference. Most
    state-of-the-art neural models for these tasks rely on pretrained word embedding
    and compose sentence-level semantics in varied ways; however, few works have attempted
    to verify whether we really need pretrained embeddings in these tasks. In this
    paper, we study how effective subword-level (character and character n-gram) representations
    are in sentence pair modeling. Though it is well-known that subword models are
    effective in tasks with single sentence input, including language modeling and
    machine translation, they have not been systematically studied in sentence pair
    modeling tasks where the semantic and string similarities between texts matter.
    Our experiments show that subword models without any pretrained word embedding
    can achieve new state-of-the-art results on two social media datasets and competitive
    results on news data for paraphrase identification.
  address: New Orleans, Louisiana
  author:
  - first: Wuwei
    full: Wuwei Lan
    id: wuwei-lan
    last: Lan
  - first: Wei
    full: Wei Xu
    id: wei-xu
    last: Xu
  author_string: Wuwei Lan, Wei Xu
  bibkey: lan-xu-2018-character
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2025
  month: June
  page_first: '157'
  page_last: '163'
  pages: "157\u2013163"
  paper_id: '25'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2025.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2025.jpg
  title: Character-Based Neural Networks for Sentence Pair Modeling
  title_html: Character-Based Neural Networks for Sentence Pair Modeling
  url: https://www.aclweb.org/anthology/N18-2025
  year: '2018'
N18-2026:
  abstract: This paper presents models to predict event durations. We introduce aspectual
    features that capture deeper linguistic information than previous work, and experiment
    with neural networks. Our analysis shows that tense, aspect and temporal structure
    of the clause provide useful clues, and that an LSTM ensemble captures relevant
    context around the event.
  address: New Orleans, Louisiana
  author:
  - first: Alakananda
    full: Alakananda Vempala
    id: alakananda-vempala
    last: Vempala
  - first: Eduardo
    full: Eduardo Blanco
    id: eduardo-blanco
    last: Blanco
  - first: Alexis
    full: Alexis Palmer
    id: alexis-palmer
    last: Palmer
  author_string: Alakananda Vempala, Eduardo Blanco, Alexis Palmer
  bibkey: vempala-etal-2018-determining
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2026
  month: June
  page_first: '164'
  page_last: '168'
  pages: "164\u2013168"
  paper_id: '26'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2026.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2026.jpg
  title: 'Determining Event Durations: Models and Error Analysis'
  title_html: 'Determining Event Durations: Models and Error Analysis'
  url: https://www.aclweb.org/anthology/N18-2026
  year: '2018'
N18-2027:
  abstract: We propose a framework that extends synchronic polysemy annotation to
    diachronic changes in lexical meaning, to counteract the lack of resources for
    evaluating computational models of lexical semantic change. Our framework exploits
    an intuitive notion of semantic relatedness, and distinguishes between innovative
    and reductive meaning changes with high inter-annotator agreement. The resulting
    test set for German comprises ratings from five annotators for the relatedness
    of 1,320 use pairs across 22 target words.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-2027.Datasets.zip
    type: dataset
    url: https://www.aclweb.org/anthology/attachments/N18-2027.Datasets.zip
  author:
  - first: Dominik
    full: Dominik Schlechtweg
    id: dominik-schlechtweg
    last: Schlechtweg
  - first: Sabine
    full: Sabine Schulte im Walde
    id: sabine-schulte-im-walde
    last: Schulte im Walde
  - first: Stefanie
    full: Stefanie Eckmann
    id: stefanie-eckmann
    last: Eckmann
  author_string: Dominik Schlechtweg, Sabine Schulte im Walde, Stefanie Eckmann
  bibkey: schlechtweg-etal-2018-diachronic
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2027
  month: June
  page_first: '169'
  page_last: '174'
  pages: "169\u2013174"
  paper_id: '27'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2027.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2027.jpg
  title: 'Diachronic Usage Relatedness (DURel): A Framework for the Annotation of
    Lexical Semantic Change'
  title_html: 'Diachronic Usage Relatedness (<span class="acl-fixed-case">DUR</span>el):
    A Framework for the Annotation of Lexical Semantic Change'
  url: https://www.aclweb.org/anthology/N18-2027
  year: '2018'
N18-2028:
  abstract: In this paper, we present directional skip-gram (DSG), a simple but effective
    enhancement of the skip-gram model by explicitly distinguishing left and right
    context in word prediction. In doing so, a direction vector is introduced for
    each word, whose embedding is thus learned by not only word co-occurrence patterns
    in its context, but also the directions of its contextual words. Theoretical and
    empirical studies on complexity illustrate that our model can be trained as efficient
    as the original skip-gram model, when compared to other extensions of the skip-gram
    model. Experimental results show that our model outperforms others on different
    datasets in semantic (word similarity measurement) and syntactic (part-of-speech
    tagging) evaluations, respectively.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-2028.Software.zip
    type: software
    url: https://www.aclweb.org/anthology/attachments/N18-2028.Software.zip
  author:
  - first: Yan
    full: Yan Song
    id: yan-song
    last: Song
  - first: Shuming
    full: Shuming Shi
    id: shuming-shi
    last: Shi
  - first: Jing
    full: Jing Li
    id: jing-li
    last: Li
  - first: Haisong
    full: Haisong Zhang
    id: haisong-zhang
    last: Zhang
  author_string: Yan Song, Shuming Shi, Jing Li, Haisong Zhang
  bibkey: song-etal-2018-directional
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2028
  month: June
  page_first: '175'
  page_last: '180'
  pages: "175\u2013180"
  paper_id: '28'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2028.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2028.jpg
  title: 'Directional Skip-Gram: Explicitly Distinguishing Left and Right Context
    for Word Embeddings'
  title_html: 'Directional Skip-Gram: Explicitly Distinguishing Left and Right Context
    for Word Embeddings'
  url: https://www.aclweb.org/anthology/N18-2028
  year: '2018'
N18-2029:
  abstract: We present a simple and effective feed-forward neural architecture for
    discriminating between lexico-semantic relations (synonymy, antonymy, hypernymy,
    and meronymy). Our Specialization Tensor Model (STM) simultaneously produces multiple
    different specializations of input distributional word vectors, tailored for predicting
    lexico-semantic relations for word pairs. STM outperforms more complex state-of-the-art
    architectures on two benchmark datasets and exhibits stable performance across
    languages. We also show that, if coupled with a bilingual distributional space,
    the proposed model can transfer the prediction of lexico-semantic relations to
    a resource-lean target language without any training data.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-2029.Datasets.zip
    type: dataset
    url: https://www.aclweb.org/anthology/attachments/N18-2029.Datasets.zip
  - filename: N18-2029.Software.zip
    type: software
    url: https://www.aclweb.org/anthology/attachments/N18-2029.Software.zip
  author:
  - first: Goran
    full: "Goran Glava\u0161"
    id: goran-glavas
    last: "Glava\u0161"
  - first: Ivan
    full: "Ivan Vuli\u0107"
    id: ivan-vulic
    last: "Vuli\u0107"
  author_string: "Goran Glava\u0161, Ivan Vuli\u0107"
  bibkey: glavas-vulic-2018-discriminating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2029
  month: June
  page_first: '181'
  page_last: '187'
  pages: "181\u2013187"
  paper_id: '29'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2029.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2029.jpg
  title: Discriminating between Lexico-Semantic Relations with the Specialization
    Tensor Model
  title_html: Discriminating between Lexico-Semantic Relations with the Specialization
    Tensor Model
  url: https://www.aclweb.org/anthology/N18-2029
  year: '2018'
N18-2030:
  abstract: Bilingual word embeddings are useful for bilingual lexicon induction,
    the task of mining translations of given words. Many studies have shown that bilingual
    word embeddings perform well for bilingual lexicon induction but they focused
    on frequent words in general domains. For many applications, bilingual lexicon
    induction of rare and domain-specific words is of critical importance. Therefore,
    we design a new task to evaluate bilingual word embeddings on rare words in different
    domains. We show that state-of-the-art approaches fail on this task and present
    simple new techniques to improve bilingual word embeddings for mining rare words.
    We release new gold standard datasets and code to stimulate research on this task.
  address: New Orleans, Louisiana
  author:
  - first: Fabienne
    full: Fabienne Braune
    id: fabienne-braune
    last: Braune
  - first: Viktor
    full: Viktor Hangya
    id: viktor-hangya
    last: Hangya
  - first: Tobias
    full: Tobias Eder
    id: tobias-eder
    last: Eder
  - first: Alexander
    full: Alexander Fraser
    id: alexander-fraser
    last: Fraser
  author_string: Fabienne Braune, Viktor Hangya, Tobias Eder, Alexander Fraser
  bibkey: braune-etal-2018-evaluating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2030
  month: June
  page_first: '188'
  page_last: '193'
  pages: "188\u2013193"
  paper_id: '30'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2030.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2030.jpg
  title: Evaluating bilingual word embeddings on the long tail
  title_html: Evaluating bilingual word embeddings on the long tail
  url: https://www.aclweb.org/anthology/N18-2030
  year: '2018'
N18-2031:
  abstract: Creating accurate meta-embeddings from pre-trained source embeddings has
    received attention lately. Methods based on global and locally-linear transformation
    and concatenation have shown to produce accurate meta-embeddings. In this paper,
    we show that the arithmetic mean of two distinct word embedding sets yields a
    performant meta-embedding that is comparable or better than more complex meta-embedding
    learning methods. The result seems counter-intuitive given that vector spaces
    in different source embeddings are not comparable and cannot be simply averaged.
    We give insight into why averaging can still produce accurate meta-embedding despite
    the incomparability of the source vector spaces.
  address: New Orleans, Louisiana
  author:
  - first: Joshua
    full: Joshua Coates
    id: joshua-coates
    last: Coates
  - first: Danushka
    full: Danushka Bollegala
    id: danushka-bollegala
    last: Bollegala
  author_string: Joshua Coates, Danushka Bollegala
  bibkey: coates-bollegala-2018-frustratingly
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2031
  month: June
  page_first: '194'
  page_last: '198'
  pages: "194\u2013198"
  paper_id: '31'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2031.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2031.jpg
  title: "Frustratingly Easy Meta-Embedding \u2013 Computing Meta-Embeddings by Averaging\
    \ Source Word Embeddings"
  title_html: "Frustratingly Easy Meta-Embedding \u2013 Computing Meta-Embeddings\
    \ by Averaging Source Word Embeddings"
  url: https://www.aclweb.org/anthology/N18-2031
  year: '2018'
N18-2032:
  abstract: 'We present two novel datasets for the low-resource language Vietnamese
    to assess models of semantic similarity: ViCon comprises pairs of synonyms and
    antonyms across word classes, thus offering data to distinguish between similarity
    and dissimilarity. ViSim-400 provides degrees of similarity across five semantic
    relations, as rated by human judges. The two datasets are verified through standard
    co-occurrence and neural network models, showing results comparable to the respective
    English datasets.'
  address: New Orleans, Louisiana
  author:
  - first: Kim Anh
    full: Kim Anh Nguyen
    id: kim-anh-nguyen
    last: Nguyen
  - first: Sabine
    full: Sabine Schulte im Walde
    id: sabine-schulte-im-walde
    last: Schulte im Walde
  - first: Ngoc Thang
    full: Ngoc Thang Vu
    id: ngoc-thang-vu
    last: Vu
  author_string: Kim Anh Nguyen, Sabine Schulte im Walde, Ngoc Thang Vu
  bibkey: nguyen-etal-2018-introducing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2032
  month: June
  page_first: '199'
  page_last: '205'
  pages: "199\u2013205"
  paper_id: '32'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2032.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2032.jpg
  title: Introducing Two Vietnamese Datasets for Evaluating Semantic Models of (Dis-)Similarity
    and Relatedness
  title_html: Introducing Two <span class="acl-fixed-case">V</span>ietnamese Datasets
    for Evaluating Semantic Models of (Dis-)Similarity and Relatedness
  url: https://www.aclweb.org/anthology/N18-2032
  year: '2018'
N18-2033:
  abstract: "Compositional Distributional Semantic Models (CDSMs) model the meaning\
    \ of phrases and sentences in vector space. They have been predominantly evaluated\
    \ on limited, artificial tasks such as semantic sentence similarity on hand-constructed\
    \ datasets. This paper argues for lexical substitution (LexSub) as a means to\
    \ evaluate CDSMs. LexSub is a more natural task, enables us to evaluate meaning\
    \ composition at the level of individual words, and provides a common ground to\
    \ compare CDSMs with dedicated LexSub models. We create a LexSub dataset for CDSM\
    \ evaluation from a corpus with manual \u201Call-words\u201D LexSub annotation.\
    \ Our experiments indicate that the Practical Lexical Function CDSM outperforms\
    \ simple component-wise CDSMs and performs on par with the context2vec LexSub\
    \ model using the same context."
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-2033.Datasets.zip
    type: dataset
    url: https://www.aclweb.org/anthology/attachments/N18-2033.Datasets.zip
  author:
  - first: Maja
    full: Maja Buljan
    id: maja-buljan
    last: Buljan
  - first: Sebastian
    full: "Sebastian Pad\xF3"
    id: sebastian-pado
    last: "Pad\xF3"
  - first: Jan
    full: "Jan \u0160najder"
    id: jan-snajder
    last: "\u0160najder"
  author_string: "Maja Buljan, Sebastian Pad\xF3, Jan \u0160najder"
  bibkey: buljan-etal-2018-lexical
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2033
  month: June
  page_first: '206'
  page_last: '211'
  pages: "206\u2013211"
  paper_id: '33'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2033.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2033.jpg
  title: Lexical Substitution for Evaluating Compositional Distributional Models
  title_html: Lexical Substitution for Evaluating Compositional Distributional Models
  url: https://www.aclweb.org/anthology/N18-2033
  year: '2018'
N18-2034:
  abstract: We present a simple extension of the GloVe representation learning model
    that begins with general-purpose representations and updates them based on data
    from a specialized domain. We show that the resulting representations can lead
    to faster learning and better results on a variety of tasks.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-2034.Software.zip
    type: software
    url: https://www.aclweb.org/anthology/attachments/N18-2034.Software.zip
  author:
  - first: Nicholas
    full: Nicholas Dingwall
    id: nicholas-dingwall
    last: Dingwall
  - first: Christopher
    full: Christopher Potts
    id: christopher-potts
    last: Potts
  author_string: Nicholas Dingwall, Christopher Potts
  bibkey: dingwall-potts-2018-mittens
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2034
  month: June
  page_first: '212'
  page_last: '217'
  pages: "212\u2013217"
  paper_id: '34'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2034.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2034.jpg
  title: 'Mittens: an Extension of GloVe for Learning Domain-Specialized Representations'
  title_html: '<span class="acl-fixed-case">M</span>ittens: an Extension of <span
    class="acl-fixed-case">G</span>lo<span class="acl-fixed-case">V</span>e for Learning
    Domain-Specialized Representations'
  url: https://www.aclweb.org/anthology/N18-2034
  year: '2018'
N18-2035:
  abstract: Automatic interpretation of the relation between the constituents of a
    noun compound, e.g. olive oil (source) and baby oil (purpose) is an important
    task for many NLP applications. Recent approaches are typically based on either
    noun-compound representations or paraphrases. While the former has initially shown
    promising results, recent work suggests that the success stems from memorizing
    single prototypical words for each relation. We explore a neural paraphrasing
    approach that demonstrates superior performance when such memorization is not
    possible.
  address: New Orleans, Louisiana
  author:
  - first: Vered
    full: Vered Shwartz
    id: vered-shwartz
    last: Shwartz
  - first: Chris
    full: Chris Waterson
    id: chris-waterson
    last: Waterson
  author_string: Vered Shwartz, Chris Waterson
  bibkey: shwartz-waterson-2018-olive
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2035
  month: June
  page_first: '218'
  page_last: '224'
  pages: "218\u2013224"
  paper_id: '35'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2035.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2035.jpg
  title: 'Olive Oil is Made of Olives, Baby Oil is Made for Babies: Interpreting Noun
    Compounds Using Paraphrases in a Neural Model'
  title_html: 'Olive Oil is Made <i>of</i> Olives, Baby Oil is Made <i>for</i> Babies:
    Interpreting Noun Compounds Using Paraphrases in a Neural Model'
  url: https://www.aclweb.org/anthology/N18-2035
  year: '2018'
N18-2036:
  abstract: Pleonasms are words that are redundant. To aid the development of systems
    that detect pleonasms in text, we introduce an annotated corpus of semantic pleonasms.
    We validate the integrity of the corpus with interannotator agreement analyses.
    We also compare it against alternative resources in terms of their effects on
    several automatic redundancy detection methods.
  address: New Orleans, Louisiana
  author:
  - first: Omid
    full: Omid Kashefi
    id: omid-kashefi
    last: Kashefi
  - first: Andrew T.
    full: Andrew T. Lucas
    id: andrew-t-lucas
    last: Lucas
  - first: Rebecca
    full: Rebecca Hwa
    id: rebecca-hwa
    last: Hwa
  author_string: Omid Kashefi, Andrew T. Lucas, Rebecca Hwa
  bibkey: kashefi-etal-2018-semantic
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2036
  month: June
  page_first: '225'
  page_last: '230'
  pages: "225\u2013230"
  paper_id: '36'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2036.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2036.jpg
  title: Semantic Pleonasm Detection
  title_html: Semantic Pleonasm Detection
  url: https://www.aclweb.org/anthology/N18-2036
  year: '2018'
N18-2037:
  abstract: "Semantic Verbal Fluency tests have been used in the detection of certain\
    \ clinical conditions, like Dementia. In particular, given a sequence of semantically\
    \ related words, a large number of switches from one semantic class to another\
    \ has been linked to clinical conditions. In this work, we investigate three similarity\
    \ measures for automatically identifying switches in semantic chains: semantic\
    \ similarity from a manually constructed resource, and word association strength\
    \ and semantic relatedness, both calculated from corpora. This information is\
    \ used for building classifiers to distinguish healthy controls from clinical\
    \ cases with early stages of Alzheimer\u2019s Disease and Mild Cognitive Deficits.\
    \ The overall results indicate that for clinical conditions the classifiers that\
    \ use these similarity measures outperform those that use a gold standard taxonomy."
  address: New Orleans, Louisiana
  author:
  - first: Felipe
    full: Felipe Paula
    id: felipe-paula
    last: Paula
  - first: Rodrigo
    full: Rodrigo Wilkens
    id: rodrigo-wilkens
    last: Wilkens
  - first: Marco
    full: Marco Idiart
    id: marco-idiart
    last: Idiart
  - first: Aline
    full: Aline Villavicencio
    id: aline-villavicencio
    last: Villavicencio
  author_string: Felipe Paula, Rodrigo Wilkens, Marco Idiart, Aline Villavicencio
  bibkey: paula-etal-2018-similarity
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2037
  month: June
  page_first: '231'
  page_last: '235'
  pages: "231\u2013235"
  paper_id: '37'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2037.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2037.jpg
  title: Similarity Measures for the Detection of Clinical Conditions with Verbal
    Fluency Tasks
  title_html: Similarity Measures for the Detection of Clinical Conditions with Verbal
    Fluency Tasks
  url: https://www.aclweb.org/anthology/N18-2037
  year: '2018'
N18-2038:
  abstract: Sluice resolution in English is the problem of finding antecedents of
    wh-fronted ellipses. Previous work has relied on hand-crafted features over syntax
    trees that scale poorly to other languages and domains; in particular, to dialogue,
    which is one of the most interesting applications of sluice resolution. Syntactic
    information is arguably important for sluice resolution, but we show that multi-task
    learning with partial parsing as auxiliary tasks effectively closes the gap and
    buys us an additional 9% error reduction over previous work. Since we are not
    directly relying on features from partial parsers, our system is more robust to
    domain shifts, giving a 26% error reduction on embedded sluices in dialogue.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-2038.Datasets.zip
    type: dataset
    url: https://www.aclweb.org/anthology/attachments/N18-2038.Datasets.zip
  author:
  - first: Ola
    full: "Ola R\xF8nning"
    id: ola-ronning
    last: "R\xF8nning"
  - first: Daniel
    full: Daniel Hardt
    id: daniel-hardt
    last: Hardt
  - first: Anders
    full: "Anders S\xF8gaard"
    id: anders-sogaard
    last: "S\xF8gaard"
  author_string: "Ola R\xF8nning, Daniel Hardt, Anders S\xF8gaard"
  bibkey: ronning-etal-2018-sluice
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2038
  month: June
  page_first: '236'
  page_last: '241'
  pages: "236\u2013241"
  paper_id: '38'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2038.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2038.jpg
  title: Sluice Resolution without Hand-Crafted Features over Brittle Syntax Trees
  title_html: Sluice Resolution without Hand-Crafted Features over Brittle Syntax
    Trees
  url: https://www.aclweb.org/anthology/N18-2038
  year: '2018'
N18-2039:
  abstract: There are some important problems in the evaluation of word embeddings
    using standard word analogy tests. In particular, in virtue of the assumptions
    made by systems generating the embeddings, these remain tests over randomness.
    We show that even supposing there were such word analogy regularities that should
    be detected in the word embeddings obtained via unsupervised means, standard word
    analogy test implementation practices provide distorted or contrived results.
    We raise concerns regarding the use of Principal Component Analysis to 2 or 3
    dimensions as a provision of visual evidence for the existence of word analogy
    relations in embeddings. Finally, we propose some solutions to these problems.
  address: New Orleans, Louisiana
  author:
  - first: Natalie
    full: Natalie Schluter
    id: natalie-schluter
    last: Schluter
  author_string: Natalie Schluter
  bibkey: schluter-2018-word
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2039
  month: June
  page_first: '242'
  page_last: '246'
  pages: "242\u2013246"
  paper_id: '39'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2039.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2039.jpg
  title: The Word Analogy Testing Caveat
  title_html: The Word Analogy Testing Caveat
  url: https://www.aclweb.org/anthology/N18-2039
  year: '2018'
N18-2040:
  abstract: This paper presents the first AMR parser built on the Chinese AMR bank.
    By applying a transition-based AMR parsing framework to Chinese, we first investigate
    how well the transitions first designed for English AMR parsing generalize to
    Chinese and provide a comparative analysis between the transitions for English
    and Chinese. We then perform a detailed error analysis to identify the major challenges
    in Chinese AMR parsing that we hope will inform future research in this area.
  address: New Orleans, Louisiana
  author:
  - first: Chuan
    full: Chuan Wang
    id: chuan-wang
    last: Wang
  - first: Bin
    full: Bin Li
    id: bin-li
    last: Li
  - first: Nianwen
    full: Nianwen Xue
    id: nianwen-xue
    last: Xue
  author_string: Chuan Wang, Bin Li, Nianwen Xue
  bibkey: wang-etal-2018-transition
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2040
  month: June
  page_first: '247'
  page_last: '252'
  pages: "247\u2013252"
  paper_id: '40'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2040.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2040.jpg
  title: Transition-Based Chinese AMR Parsing
  title_html: Transition-Based <span class="acl-fixed-case">C</span>hinese <span class="acl-fixed-case">AMR</span>
    Parsing
  url: https://www.aclweb.org/anthology/N18-2040
  year: '2018'
N18-2041:
  abstract: We propose a novel two-layered attention network based on Bidirectional
    Long Short-Term Memory for sentiment analysis. The novel two-layered attention
    network takes advantage of the external knowledge bases to improve the sentiment
    prediction. It uses the Knowledge Graph Embedding generated using the WordNet.
    We build our model by combining the two-layered attention network with the supervised
    model based on Support Vector Regression using a Multilayer Perceptron network
    for sentiment analysis. We evaluate our model on the benchmark dataset of SemEval
    2017 Task 5. Experimental results show that the proposed model surpasses the top
    system of SemEval 2017 Task 5. The model performs significantly better by improving
    the state-of-the-art system at SemEval 2017 Task 5 by 1.7 and 3.7 points for sub-tracks
    1 and 2 respectively.
  address: New Orleans, Louisiana
  author:
  - first: Abhishek
    full: Abhishek Kumar
    id: abhishek-kumar
    last: Kumar
  - first: Daisuke
    full: Daisuke Kawahara
    id: daisuke-kawahara
    last: Kawahara
  - first: Sadao
    full: Sadao Kurohashi
    id: sadao-kurohashi
    last: Kurohashi
  author_string: Abhishek Kumar, Daisuke Kawahara, Sadao Kurohashi
  bibkey: kumar-etal-2018-knowledge
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2041
  month: June
  page_first: '253'
  page_last: '258'
  pages: "253\u2013258"
  paper_id: '41'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2041.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2041.jpg
  title: Knowledge-Enriched Two-Layered Attention Network for Sentiment Analysis
  title_html: Knowledge-Enriched Two-Layered Attention Network for Sentiment Analysis
  url: https://www.aclweb.org/anthology/N18-2041
  year: '2018'
N18-2042:
  abstract: "Books have the power to make us feel happiness, sadness, pain, surprise,\
    \ or sorrow. An author\u2019s dexterity in the use of these emotions captivates\
    \ readers and makes it difficult for them to put the book down. In this paper,\
    \ we model the flow of emotions over a book using recurrent neural networks and\
    \ quantify its usefulness in predicting success in books. We obtained the best\
    \ weighted F1-score of 69% for predicting books\u2019 success in a multitask setting\
    \ (simultaneously predicting success and genre of books)."
  address: New Orleans, Louisiana
  author:
  - first: Suraj
    full: Suraj Maharjan
    id: suraj-maharjan
    last: Maharjan
  - first: Sudipta
    full: Sudipta Kar
    id: sudipta-kar
    last: Kar
  - first: Manuel
    full: Manuel Montes
    id: manuel-montes
    last: Montes
  - first: Fabio A.
    full: "Fabio A. Gonz\xE1lez"
    id: fabio-a-gonzalez
    last: "Gonz\xE1lez"
  - first: Thamar
    full: Thamar Solorio
    id: thamar-solorio
    last: Solorio
  author_string: "Suraj Maharjan, Sudipta Kar, Manuel Montes, Fabio A. Gonz\xE1lez,\
    \ Thamar Solorio"
  bibkey: maharjan-etal-2018-letting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2042
  month: June
  page_first: '259'
  page_last: '265'
  pages: "259\u2013265"
  paper_id: '42'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2042.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2042.jpg
  title: 'Letting Emotions Flow: Success Prediction by Modeling the Flow of Emotions
    in Books'
  title_html: 'Letting Emotions Flow: Success Prediction by Modeling the Flow of Emotions
    in Books'
  url: https://www.aclweb.org/anthology/N18-2042
  year: '2018'
N18-2043:
  abstract: Aspect-based Sentiment Analysis is a fine-grained task of sentiment classification
    for multiple aspects in a sentence. Present neural-based models exploit aspect
    and its contextual information in the sentence but largely ignore the inter-aspect
    dependencies. In this paper, we incorporate this pattern by simultaneous classification
    of all aspects in a sentence along with temporal dependency processing of their
    corresponding sentence representations using recurrent networks. Results on the
    benchmark SemEval 2014 dataset suggest the effectiveness of our proposed approach.
  address: New Orleans, Louisiana
  author:
  - first: Devamanyu
    full: Devamanyu Hazarika
    id: devamanyu-hazarika
    last: Hazarika
  - first: Soujanya
    full: Soujanya Poria
    id: soujanya-poria
    last: Poria
  - first: Prateek
    full: Prateek Vij
    id: prateek-vij
    last: Vij
  - first: Gangeshwar
    full: Gangeshwar Krishnamurthy
    id: gangeshwar-krishnamurthy
    last: Krishnamurthy
  - first: Erik
    full: Erik Cambria
    id: erik-cambria
    last: Cambria
  - first: Roger
    full: Roger Zimmermann
    id: roger-zimmermann
    last: Zimmermann
  author_string: Devamanyu Hazarika, Soujanya Poria, Prateek Vij, Gangeshwar Krishnamurthy,
    Erik Cambria, Roger Zimmermann
  bibkey: hazarika-etal-2018-modeling
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2043
  month: June
  page_first: '266'
  page_last: '270'
  pages: "266\u2013270"
  paper_id: '43'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2043.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2043.jpg
  title: Modeling Inter-Aspect Dependencies for Aspect-Based Sentiment Analysis
  title_html: Modeling Inter-Aspect Dependencies for Aspect-Based Sentiment Analysis
  url: https://www.aclweb.org/anthology/N18-2043
  year: '2018'
N18-2044:
  abstract: "In recent past, social media has emerged as an active platform in the\
    \ context of healthcare and medicine. In this paper, we present a study where\
    \ medical user\u2019s opinions on health-related issues are analyzed to capture\
    \ the medical sentiment at a blog level. The medical sentiments can be studied\
    \ in various facets such as medical condition, treatment, and medication that\
    \ characterize the overall health status of the user. Considering these facets,\
    \ we treat analysis of this information as a multi-task classification problem.\
    \ In this paper, we adopt a novel adversarial learning approach for our multi-task\
    \ learning framework to learn the sentiment\u2019s strengths expressed in a medical\
    \ blog. Our evaluation shows promising results for our target tasks."
  address: New Orleans, Louisiana
  author:
  - first: Shweta
    full: Shweta Yadav
    id: shweta-yadav
    last: Yadav
  - first: Asif
    full: Asif Ekbal
    id: asif-ekbal
    last: Ekbal
  - first: Sriparna
    full: Sriparna Saha
    id: sriparna-saha
    last: Saha
  - first: Pushpak
    full: Pushpak Bhattacharyya
    id: pushpak-bhattacharyya
    last: Bhattacharyya
  - first: Amit
    full: Amit Sheth
    id: amit-sheth
    last: Sheth
  author_string: Shweta Yadav, Asif Ekbal, Sriparna Saha, Pushpak Bhattacharyya, Amit
    Sheth
  bibkey: yadav-etal-2018-multi
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2044
  month: June
  page_first: '271'
  page_last: '277'
  pages: "271\u2013277"
  paper_id: '44'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2044.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2044.jpg
  title: Multi-Task Learning Framework for Mining Crowd Intelligence towards Clinical
    Treatment
  title_html: Multi-Task Learning Framework for Mining Crowd Intelligence towards
    Clinical Treatment
  url: https://www.aclweb.org/anthology/N18-2044
  year: '2018'
N18-2045:
  abstract: "While neural networks have been shown to achieve impressive results for\
    \ sentence-level sentiment analysis, targeted aspect-based sentiment analysis\
    \ (TABSA) \u2014 extraction of fine-grained opinion polarity w.r.t. a pre-defined\
    \ set of aspects \u2014 remains a difficult task. Motivated by recent advances\
    \ in memory-augmented models for machine reading, we propose a novel architecture,\
    \ utilising external \u201Cmemory chains\u201D with a delayed memory update mechanism\
    \ to track entities. On a TABSA task, the proposed model demonstrates substantial\
    \ improvements over state-of-the-art approaches, including those using external\
    \ knowledge bases."
  address: New Orleans, Louisiana
  author:
  - first: Fei
    full: Fei Liu
    id: fei-liu-unimelb
    last: Liu
  - first: Trevor
    full: Trevor Cohn
    id: trevor-cohn
    last: Cohn
  - first: Timothy
    full: Timothy Baldwin
    id: timothy-baldwin
    last: Baldwin
  author_string: Fei Liu, Trevor Cohn, Timothy Baldwin
  bibkey: liu-etal-2018-recurrent
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2045
  month: June
  page_first: '278'
  page_last: '283'
  pages: "278\u2013283"
  paper_id: '45'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2045.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2045.jpg
  title: Recurrent Entity Networks with Delayed Memory Update for Targeted Aspect-Based
    Sentiment Analysis
  title_html: Recurrent Entity Networks with Delayed Memory Update for Targeted Aspect-Based
    Sentiment Analysis
  url: https://www.aclweb.org/anthology/N18-2045
  year: '2018'
N18-2046:
  abstract: 'We combine two of the most popular approaches to automated Grammatical
    Error Correction (GEC): GEC based on Statistical Machine Translation (SMT) and
    GEC based on Neural Machine Translation (NMT). The hybrid system achieves new
    state-of-the-art results on the CoNLL-2014 and JFLEG benchmarks. This GEC system
    preserves the accuracy of SMT output and, at the same time, generates more fluent
    sentences as it typical for NMT. Our analysis shows that the created systems are
    closer to reaching human-level performance than any other GEC system reported
    so far.'
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276373827
    type: video
    url: http://vimeo.com/276373827
  author:
  - first: Roman
    full: Roman Grundkiewicz
    id: roman-grundkiewicz
    last: Grundkiewicz
  - first: Marcin
    full: Marcin Junczys-Dowmunt
    id: marcin-junczys-dowmunt
    last: Junczys-Dowmunt
  author_string: Roman Grundkiewicz, Marcin Junczys-Dowmunt
  bibkey: grundkiewicz-junczys-dowmunt-2018-near
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2046
  month: June
  page_first: '284'
  page_last: '290'
  pages: "284\u2013290"
  paper_id: '46'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2046.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2046.jpg
  title: Near Human-Level Performance in Grammatical Error Correction with Hybrid
    Machine Translation
  title_html: Near Human-Level Performance in Grammatical Error Correction with Hybrid
    Machine Translation
  url: https://www.aclweb.org/anthology/N18-2046
  year: '2018'
N18-2047:
  abstract: We examine the problem of question answering over knowledge graphs, focusing
    on simple questions that can be answered by the lookup of a single fact. Adopting
    a straightforward decomposition of the problem into entity detection, entity linking,
    relation prediction, and evidence combination, we explore simple yet strong baselines.
    On the popular SimpleQuestions dataset, we find that basic LSTMs and GRUs plus
    a few heuristics yield accuracies that approach the state of the art, and techniques
    that do not use neural networks also perform reasonably well. These results show
    that gains from sophisticated deep learning techniques proposed in the literature
    are quite modest and that some previous models exhibit unnecessary complexity.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276433908
    type: video
    url: http://vimeo.com/276433908
  author:
  - first: Salman
    full: Salman Mohammed
    id: salman-mohammed
    last: Mohammed
  - first: Peng
    full: Peng Shi
    id: peng-shi
    last: Shi
  - first: Jimmy
    full: Jimmy Lin
    id: jimmy-lin
    last: Lin
  author_string: Salman Mohammed, Peng Shi, Jimmy Lin
  bibkey: mohammed-etal-2018-strong
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2047
  month: June
  page_first: '291'
  page_last: '296'
  pages: "291\u2013296"
  paper_id: '47'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2047.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2047.jpg
  title: Strong Baselines for Simple Question Answering over Knowledge Graphs with
    and without Neural Networks
  title_html: Strong Baselines for Simple Question Answering over Knowledge Graphs
    with and without Neural Networks
  url: https://www.aclweb.org/anthology/N18-2047
  year: '2018'
N18-2048:
  abstract: "Entrainment has been shown to occur for various linguistic features individually.\
    \ Motivated by cognitive theories regarding linguistic entrainment, we analyze\
    \ speakers\u2019 overall entrainment behaviors and search for an underlying structure.\
    \ We consider various measures of both acoustic-prosodic and lexical entrainment,\
    \ measuring the latter with a novel application of two previously introduced methods\
    \ in addition to a standard high-frequency word measure. We present a negative\
    \ result of our search, finding no meaningful correlations, clusters, or principal\
    \ components in various entrainment measures, and discuss practical and theoretical\
    \ implications."
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/277631568
    type: video
    url: http://vimeo.com/277631568
  author:
  - first: Andreas
    full: Andreas Weise
    id: andreas-weise
    last: Weise
  - first: Rivka
    full: Rivka Levitan
    id: rivka-levitan
    last: Levitan
  author_string: Andreas Weise, Rivka Levitan
  bibkey: weise-levitan-2018-looking
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2048
  month: June
  page_first: '297'
  page_last: '302'
  pages: "297\u2013302"
  paper_id: '48'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2048.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2048.jpg
  title: Looking for Structure in Lexical and Acoustic-Prosodic Entrainment Behaviors
  title_html: Looking for Structure in Lexical and Acoustic-Prosodic Entrainment Behaviors
  url: https://www.aclweb.org/anthology/N18-2048
  year: '2018'
N18-2049:
  abstract: 'Distributional data tells us that a man can swallow candy, but not that
    a man can swallow a paintball, since this is never attested. However both are
    physically plausible events. This paper introduces the task of semantic plausibility:
    recognizing plausible but possibly novel events. We present a new crowdsourced
    dataset of semantic plausibility judgments of single events such as man swallow
    paintball. Simple models based on distributional representations perform poorly
    on this task, despite doing well on selection preference, but injecting manually
    elicited knowledge about entity properties provides a substantial performance
    boost. Our error analysis shows that our new dataset is a great testbed for semantic
    plausibility models: more sophisticated knowledge representation and propagation
    could address many of the remaining errors.'
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-2049.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-2049.Notes.pdf
  - filename: http://vimeo.com/276898113
    type: video
    url: http://vimeo.com/276898113
  author:
  - first: Su
    full: Su Wang
    id: su-wang
    last: Wang
  - first: Greg
    full: Greg Durrett
    id: greg-durrett
    last: Durrett
  - first: Katrin
    full: Katrin Erk
    id: katrin-erk
    last: Erk
  author_string: Su Wang, Greg Durrett, Katrin Erk
  bibkey: wang-etal-2018-modeling
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2049
  month: June
  page_first: '303'
  page_last: '308'
  pages: "303\u2013308"
  paper_id: '49'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2049.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2049.jpg
  title: Modeling Semantic Plausibility by Injecting World Knowledge
  title_html: Modeling Semantic Plausibility by Injecting World Knowledge
  url: https://www.aclweb.org/anthology/N18-2049
  year: '2018'
N18-2050:
  abstract: "Intent detection and slot filling are two main tasks for building a spoken\
    \ language understanding(SLU) system. Multiple deep learning based models have\
    \ demonstrated good results on these tasks . The most effective algorithms are\
    \ based on the structures of sequence to sequence models (or \u201Cencoder-decoder\u201D\
    \ models), and generate the intents and semantic tags either using separate models.\
    \ Most of the previous studies, however, either treat the intent detection and\
    \ slot filling as two separate parallel tasks, or use a sequence to sequence model\
    \ to generate both semantic tags and intent. None of the approaches consider the\
    \ cross-impact between the intent detection task and the slot filling task. In\
    \ this paper, new Bi-model based RNN semantic frame parsing network structures\
    \ are designed to perform the intent detection and slot filling tasks jointly,\
    \ by considering their cross-impact to each other using two correlated bidirectional\
    \ LSTMs (BLSTM). Our Bi-model structure with a decoder achieves state-of-art result\
    \ on the benchmark ATIS data, with about 0.5% intent accuracy improvement and\
    \ 0.9 % slot filling improvement."
  address: New Orleans, Louisiana
  author:
  - first: Yu
    full: Yu Wang
    id: yu-wang
    last: Wang
  - first: Yilin
    full: Yilin Shen
    id: yilin-shen
    last: Shen
  - first: Hongxia
    full: Hongxia Jin
    id: hongxia-jin
    last: Jin
  author_string: Yu Wang, Yilin Shen, Hongxia Jin
  bibkey: wang-etal-2018-bi
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2050
  month: June
  page_first: '309'
  page_last: '314'
  pages: "309\u2013314"
  paper_id: '50'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2050.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2050.jpg
  title: A Bi-Model Based RNN Semantic Frame Parsing Model for Intent Detection and
    Slot Filling
  title_html: A Bi-Model Based <span class="acl-fixed-case">RNN</span> Semantic Frame
    Parsing Model for Intent Detection and Slot Filling
  url: https://www.aclweb.org/anthology/N18-2050
  year: '2018'
N18-2051:
  abstract: Taxonomies are often used to look up the concepts they contain in text
    documents (for instance, to classify a document). The more comprehensive the taxonomy,
    the higher recall the application has that uses the taxonomy. In this paper, we
    explore automatic taxonomy augmentation with paraphrases. We compare two state-of-the-art
    paraphrase models based on Moses, a statistical Machine Translation system, and
    a sequence-to-sequence neural network, trained on a paraphrase datasets with respect
    to their abilities to add novel nodes to an existing taxonomy from the risk domain.
    We conduct component-based and task-based evaluations. Our results show that paraphrasing
    is a viable method to enrich a taxonomy with more terms, and that Moses consistently
    outperforms the sequence-to-sequence neural model. To the best of our knowledge,
    this is the first approach to augment taxonomies with paraphrases.
  address: New Orleans, Louisiana
  author:
  - first: Vassilis
    full: Vassilis Plachouras
    id: vassilis-plachouras
    last: Plachouras
  - first: Fabio
    full: Fabio Petroni
    id: fabio-petroni
    last: Petroni
  - first: Timothy
    full: Timothy Nugent
    id: timothy-nugent
    last: Nugent
  - first: Jochen L.
    full: Jochen L. Leidner
    id: jochen-l-leidner
    last: Leidner
  author_string: Vassilis Plachouras, Fabio Petroni, Timothy Nugent, Jochen L. Leidner
  bibkey: plachouras-etal-2018-comparison
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2051
  month: June
  page_first: '315'
  page_last: '320'
  pages: "315\u2013320"
  paper_id: '51'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2051.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2051.jpg
  title: A Comparison of Two Paraphrase Models for Taxonomy Augmentation
  title_html: A Comparison of Two Paraphrase Models for Taxonomy Augmentation
  url: https://www.aclweb.org/anthology/N18-2051
  year: '2018'
N18-2052:
  abstract: 'This paper introduces a new dataset of term annotation. Given that even
    experts vary significantly in their understanding of termhood, and that term identification
    is mostly performed as a binary task, we offer a novel perspective to explore
    the common, natural understanding of what constitutes a term: Laypeople annotate
    single-word and multi-word terms, across four domains and across four task definitions.
    Analyses based on inter-annotator agreement offer insights into differences in
    term specificity, term granularity and subtermhood.'
  address: New Orleans, Louisiana
  author:
  - first: Anna
    full: "Anna H\xE4tty"
    id: anna-hatty
    last: "H\xE4tty"
  - first: Sabine
    full: Sabine Schulte im Walde
    id: sabine-schulte-im-walde
    last: Schulte im Walde
  author_string: "Anna H\xE4tty, Sabine Schulte im Walde"
  bibkey: hatty-schulte-im-walde-2018-laypeople
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2052
  month: June
  page_first: '321'
  page_last: '326'
  pages: "321\u2013326"
  paper_id: '52'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2052.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2052.jpg
  title: A Laypeople Study on Terminology Identification across Domains and Task Definitions
  title_html: A Laypeople Study on Terminology Identification across Domains and Task
    Definitions
  url: https://www.aclweb.org/anthology/N18-2052
  year: '2018'
N18-2053:
  abstract: In this paper, we propose a novel embedding model, named ConvKB, for knowledge
    base completion. Our model ConvKB advances state-of-the-art models by employing
    a convolutional neural network, so that it can capture global relationships and
    transitional characteristics between entities and relations in knowledge bases.
    In ConvKB, each triple (head entity, relation, tail entity) is represented as
    a 3-column matrix where each column vector represents a triple element. This 3-column
    matrix is then fed to a convolution layer where multiple filters are operated
    on the matrix to generate different feature maps. These feature maps are then
    concatenated into a single feature vector representing the input triple. The feature
    vector is multiplied with a weight vector via a dot product to return a score.
    This score is then used to predict whether the triple is valid or not. Experiments
    show that ConvKB achieves better link prediction performance than previous state-of-the-art
    embedding models on two benchmark datasets WN18RR and FB15k-237.
  address: New Orleans, Louisiana
  author:
  - first: Dai Quoc
    full: Dai Quoc Nguyen
    id: dai-quoc-nguyen
    last: Nguyen
  - first: Tu Dinh
    full: Tu Dinh Nguyen
    id: tu-dinh-nguyen
    last: Nguyen
  - first: Dat Quoc
    full: Dat Quoc Nguyen
    id: dat-quoc-nguyen
    last: Nguyen
  - first: Dinh
    full: Dinh Phung
    id: dinh-phung
    last: Phung
  author_string: Dai Quoc Nguyen, Tu Dinh Nguyen, Dat Quoc Nguyen, Dinh Phung
  bibkey: nguyen-etal-2018-novel
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2053
  month: June
  page_first: '327'
  page_last: '333'
  pages: "327\u2013333"
  paper_id: '53'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2053.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2053.jpg
  title: A Novel Embedding Model for Knowledge Base Completion Based on Convolutional
    Neural Network
  title_html: A Novel Embedding Model for Knowledge Base Completion Based on Convolutional
    Neural Network
  url: https://www.aclweb.org/anthology/N18-2053
  year: '2018'
N18-2054:
  abstract: Cross-language article linking (CLAL) is the task of finding corresponding
    article pairs of different languages across encyclopedias. This task is a difficult
    disambiguation problem in which one article must be selected among several candidate
    articles with similar titles and contents. Existing works focus on engineering
    text-based or link-based features for this task, which is a time-consuming job,
    and some of these features are only applicable within the same encyclopedia. In
    this paper, we address these problems by proposing cross-encyclopedia entity embedding.
    Unlike other works, our proposed method does not rely on known cross-language
    pairs. We apply our method to CLAL between English Wikipedia and Chinese Baidu
    Baike. Our features improve performance relative to the baseline by 29.62%. Tested
    30 times, our system achieved an average improvement of 2.76% over the current
    best system (26.86% over baseline), a statistically significant result.
  address: New Orleans, Louisiana
  author:
  - first: Chun-Kai
    full: Chun-Kai Wu
    id: chun-kai-wu
    last: Wu
  - first: Richard Tzong-Han
    full: Richard Tzong-Han Tsai
    id: richard-tzong-han-tsai
    last: Tsai
  author_string: Chun-Kai Wu, Richard Tzong-Han Tsai
  bibkey: wu-tsai-2018-cross
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2054
  month: June
  page_first: '334'
  page_last: '339'
  pages: "334\u2013339"
  paper_id: '54'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2054.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2054.jpg
  title: Cross-language Article Linking Using Cross-Encyclopedia Entity Embedding
  title_html: Cross-language Article Linking Using Cross-Encyclopedia Entity Embedding
  url: https://www.aclweb.org/anthology/N18-2054
  year: '2018'
N18-2055:
  abstract: Identifying the most dominant and central event of a document, which governs
    and connects other foreground and background events in the document, is useful
    for many applications, such as text summarization, storyline generation and text
    segmentation. We observed that the central event of a document usually has many
    coreferential event mentions that are scattered throughout the document for enabling
    a smooth transition of subtopics. Our empirical experiments, using gold event
    coreference relations, have shown that the central event of a document can be
    well identified by mining properties of event coreference chains. But the performance
    drops when switching to system predicted event coreference relations. In addition,
    we found that the central event can be more accurately identified by further considering
    the number of sub-events as well as the realis status of an event.
  address: New Orleans, Louisiana
  author:
  - first: Prafulla Kumar
    full: Prafulla Kumar Choubey
    id: prafulla-kumar-choubey
    last: Choubey
  - first: Kaushik
    full: Kaushik Raju
    id: kaushik-raju
    last: Raju
  - first: Ruihong
    full: Ruihong Huang
    id: ruihong-huang
    last: Huang
  author_string: Prafulla Kumar Choubey, Kaushik Raju, Ruihong Huang
  bibkey: choubey-etal-2018-identifying
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2055
  month: June
  page_first: '340'
  page_last: '345'
  pages: "340\u2013345"
  paper_id: '55'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2055.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2055.jpg
  title: Identifying the Most Dominant Event in a News Article by Mining Event Coreference
    Relations
  title_html: Identifying the Most Dominant Event in a News Article by Mining Event
    Coreference Relations
  url: https://www.aclweb.org/anthology/N18-2055
  year: '2018'
N18-2056:
  abstract: Entity recognition is a widely benchmarked task in natural language processing
    due to its massive applications. The state-of-the-art solution applies a neural
    architecture named BiLSTM-CRF to model the language sequences. In this paper,
    we propose an entity recognition system that improves this neural architecture
    with two novel techniques. The first technique is Multi-Task Data Selection, which
    ensures the consistency of data distribution and labeling guidelines between source
    and target datasets. The other one is constrained decoding using knowledge base.
    The decoder of the model operates at the document level, and leverages global
    and external information sources to further improve performance. Extensive experiments
    have been conducted to show the advantages of each technique. Our system achieves
    state-of-the-art results on the English entity recognition task in KBP 2017 official
    evaluation, and it also yields very strong results in other languages.
  address: New Orleans, Louisiana
  author:
  - first: Huasha
    full: Huasha Zhao
    id: huasha-zhao
    last: Zhao
  - first: Yi
    full: Yi Yang
    id: yi-yang
    last: Yang
  - first: Qiong
    full: Qiong Zhang
    id: qiong-zhang
    last: Zhang
  - first: Luo
    full: Luo Si
    id: luo-si
    last: Si
  author_string: Huasha Zhao, Yi Yang, Qiong Zhang, Luo Si
  bibkey: zhao-etal-2018-improve
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2056
  month: June
  page_first: '346'
  page_last: '351'
  pages: "346\u2013351"
  paper_id: '56'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2056.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2056.jpg
  title: Improve Neural Entity Recognition via Multi-Task Data Selection and Constrained
    Decoding
  title_html: Improve Neural Entity Recognition via Multi-Task Data Selection and
    Constrained Decoding
  url: https://www.aclweb.org/anthology/N18-2056
  year: '2018'
N18-2057:
  abstract: We propose a novel approach to semi-supervised learning for information
    extraction that uses ladder networks (Rasmus et al., 2015). In particular, we
    focus on the task of named entity classification, defined as identifying the correct
    label (e.g., person or organization name) of an entity mention in a given context.
    Our approach is simple, efficient and has the benefit of being robust to semantic
    drift, a dominant problem in most semi-supervised learning systems. We empirically
    demonstrate the superior performance of our system compared to the state-of-the-art
    on two standard datasets for named entity classification. We obtain between 62%
    and 200% improvement over the state-of-art baseline on these two datasets.
  address: New Orleans, Louisiana
  author:
  - first: Ajay
    full: Ajay Nagesh
    id: ajay-nagesh
    last: Nagesh
  - first: Mihai
    full: Mihai Surdeanu
    id: mihai-surdeanu
    last: Surdeanu
  author_string: Ajay Nagesh, Mihai Surdeanu
  bibkey: nagesh-surdeanu-2018-keep
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2057
  month: June
  page_first: '352'
  page_last: '358'
  pages: "352\u2013358"
  paper_id: '57'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2057.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2057.jpg
  title: 'Keep Your Bearings: Lightly-Supervised Information Extraction with Ladder
    Networks That Avoids Semantic Drift'
  title_html: 'Keep Your Bearings: Lightly-Supervised Information Extraction with
    Ladder Networks That Avoids Semantic Drift'
  url: https://www.aclweb.org/anthology/N18-2057
  year: '2018'
N18-2058:
  abstract: Supervised event extraction systems are limited in their accuracy due
    to the lack of available training data. We present a method for self-training
    event extraction systems by bootstrapping additional training data. This is done
    by taking advantage of the occurrence of multiple mentions of the same event instances
    across newswire articles from multiple sources. If our system can make a high-confidence
    extraction of some mentions in such a cluster, it can then acquire diverse training
    examples by adding the other mentions as well. Our experiments show significant
    performance improvements on multiple event extractors over ACE 2005 and TAC-KBP
    2015 datasets.
  address: New Orleans, Louisiana
  author:
  - first: James
    full: James Ferguson
    id: james-ferguson
    last: Ferguson
  - first: Colin
    full: Colin Lockard
    id: colin-lockard
    last: Lockard
  - first: Daniel
    full: Daniel Weld
    id: daniel-s-weld
    last: Weld
  - first: Hannaneh
    full: Hannaneh Hajishirzi
    id: hannaneh-hajishirzi
    last: Hajishirzi
  author_string: James Ferguson, Colin Lockard, Daniel Weld, Hannaneh Hajishirzi
  bibkey: ferguson-etal-2018-semi
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2058
  month: June
  page_first: '359'
  page_last: '364'
  pages: "359\u2013364"
  paper_id: '58'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2058.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2058.jpg
  title: Semi-Supervised Event Extraction with Paraphrase Clusters
  title_html: Semi-Supervised Event Extraction with Paraphrase Clusters
  url: https://www.aclweb.org/anthology/N18-2058
  year: '2018'
N18-2059:
  abstract: Relation classification is an important semantic processing task in the
    field of natural language processing. In this paper, we propose the task of relation
    classification for Chinese literature text. A new dataset of Chinese literature
    text is constructed to facilitate the study in this task. We present a novel model,
    named Structure Regularized Bidirectional Recurrent Convolutional Neural Network
    (SR-BRCNN), to identify the relation between entities. The proposed model learns
    relation representations along the shortest dependency path (SDP) extracted from
    the structure regularized dependency tree, which has the benefits of reducing
    the complexity of the whole model. Experimental results show that the proposed
    method significantly improves the F1 score by 10.3, and outperforms the state-of-the-art
    approaches on Chinese literature text.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-2059.Datasets.zip
    type: dataset
    url: https://www.aclweb.org/anthology/attachments/N18-2059.Datasets.zip
  author:
  - first: Ji
    full: Ji Wen
    id: ji-wen
    last: Wen
  - first: Xu
    full: Xu Sun
    id: xu-sun
    last: Sun
  - first: Xuancheng
    full: Xuancheng Ren
    id: xuancheng-ren
    last: Ren
  - first: Qi
    full: Qi Su
    id: qi-su
    last: Su
  author_string: Ji Wen, Xu Sun, Xuancheng Ren, Qi Su
  bibkey: wen-etal-2018-structure
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2059
  month: June
  page_first: '365'
  page_last: '370'
  pages: "365\u2013370"
  paper_id: '59'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2059.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2059.jpg
  title: Structure Regularized Neural Network for Entity Relation Classification for
    Chinese Literature Text
  title_html: Structure Regularized Neural Network for Entity Relation Classification
    for <span class="acl-fixed-case">C</span>hinese Literature Text
  url: https://www.aclweb.org/anthology/N18-2059
  year: '2018'
N18-2060:
  abstract: Medical professionals search the published literature by specifying the
    type of patients, the medical intervention(s) and the outcome measure(s) of interest.
    In this paper we demonstrate how features encoding syntactic patterns improve
    the performance of state-of-the-art sequence tagging models (both neural and linear)
    for information extraction of these medically relevant categories. We present
    an analysis of the type of patterns exploited and of the semantic space induced
    for these, i.e., the distributed representations learned for identified multi-token
    patterns. We show that these learned representations differ substantially from
    those of the constituent unigrams, suggesting that the patterns capture contextual
    information that is otherwise lost.
  address: New Orleans, Louisiana
  author:
  - first: Roma
    full: Roma Patel
    id: roma-patel
    last: Patel
  - first: Yinfei
    full: Yinfei Yang
    id: yinfei-yang
    last: Yang
  - first: Iain
    full: Iain Marshall
    id: iain-marshall
    last: Marshall
  - first: Ani
    full: Ani Nenkova
    id: ani-nenkova
    last: Nenkova
  - first: Byron
    full: Byron Wallace
    id: byron-c-wallace
    last: Wallace
  author_string: Roma Patel, Yinfei Yang, Iain Marshall, Ani Nenkova, Byron Wallace
  bibkey: patel-etal-2018-syntactic
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2060
  month: June
  page_first: '371'
  page_last: '377'
  pages: "371\u2013377"
  paper_id: '60'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2060.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2060.jpg
  title: Syntactic Patterns Improve Information Extraction for Medical Search
  title_html: Syntactic Patterns Improve Information Extraction for Medical Search
  url: https://www.aclweb.org/anthology/N18-2060
  year: '2018'
N18-2061:
  abstract: Automatically identifying definitional knowledge in text corpora (Definition
    Extraction or DE) is an important task with direct applications in, among others,
    Automatic Glossary Generation, Taxonomy Learning, Question Answering and Semantic
    Search. It is generally cast as a binary classification problem between definitional
    and non-definitional sentences. In this paper we present a set of neural architectures
    combining Convolutional and Recurrent Neural Networks, which are further enriched
    by incorporating linguistic information via syntactic dependencies. Our experimental
    results in the task of sentence classification, on two benchmarking DE datasets
    (one generic, one domain-specific), show that these models obtain consistent state
    of the art results. Furthermore, we demonstrate that models trained on clean Wikipedia-like
    definitions can successfully be applied to more noisy domain-specific corpora.
  address: New Orleans, Louisiana
  author:
  - first: Luis
    full: Luis Espinosa-Anke
    id: luis-espinosa-anke
    last: Espinosa-Anke
  - first: Steven
    full: Steven Schockaert
    id: steven-schockaert
    last: Schockaert
  author_string: Luis Espinosa-Anke, Steven Schockaert
  bibkey: espinosa-anke-schockaert-2018-syntactically
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2061
  month: June
  page_first: '378'
  page_last: '385'
  pages: "378\u2013385"
  paper_id: '61'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2061.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2061.jpg
  title: Syntactically Aware Neural Architectures for Definition Extraction
  title_html: Syntactically Aware Neural Architectures for Definition Extraction
  url: https://www.aclweb.org/anthology/N18-2061
  year: '2018'
N18-2062:
  abstract: We propose an efficient dynamic oracle for training the 2-Planar transition-based
    parser, a linear-time parser with over 99% coverage on non-projective syntactic
    corpora. This novel approach outperforms the static training strategy in the vast
    majority of languages tested and scored better on most datasets than the arc-hybrid
    parser enhanced with the Swap transition, which can handle unrestricted non-projectivity.
  address: New Orleans, Louisiana
  author:
  - first: Daniel
    full: "Daniel Fern\xE1ndez-Gonz\xE1lez"
    id: daniel-fernandez-gonzalez
    last: "Fern\xE1ndez-Gonz\xE1lez"
  - first: Carlos
    full: "Carlos G\xF3mez-Rodr\xEDguez"
    id: carlos-gomez-rodriguez
    last: "G\xF3mez-Rodr\xEDguez"
  author_string: "Daniel Fern\xE1ndez-Gonz\xE1lez, Carlos G\xF3mez-Rodr\xEDguez"
  bibkey: fernandez-gonzalez-gomez-rodriguez-2018-dynamic
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2062
  month: June
  page_first: '386'
  page_last: '392'
  pages: "386\u2013392"
  paper_id: '62'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2062.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2062.jpg
  title: A Dynamic Oracle for Linear-Time 2-Planar Dependency Parsing
  title_html: A Dynamic Oracle for Linear-Time 2-Planar Dependency Parsing
  url: https://www.aclweb.org/anthology/N18-2062
  year: '2018'
N18-2063:
  abstract: "We evaluate the performance of state-of-the-art algorithms for automatic\
    \ cognate detection by comparing how useful automatically inferred cognates are\
    \ for the task of phylogenetic inference compared to classical manually annotated\
    \ cognate sets. Our findings suggest that phylogenies inferred from automated\
    \ cognate sets come close to phylogenies inferred from expert-annotated ones,\
    \ although on average, the latter are still superior. We conclude that future\
    \ work on phylogenetic reconstruction can profit much from automatic cognate detection.\
    \ Especially where scholars are merely interested in exploring the bigger picture\
    \ of a language family\u2019s phylogeny, algorithms for automatic cognate detection\
    \ are a useful complement for current research on language phylogenies."
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-2063.Datasets.zip
    type: dataset
    url: https://www.aclweb.org/anthology/attachments/N18-2063.Datasets.zip
  - filename: N18-2063.Software.zip
    type: software
    url: https://www.aclweb.org/anthology/attachments/N18-2063.Software.zip
  author:
  - first: Taraka
    full: Taraka Rama
    id: taraka-rama
    last: Rama
  - first: Johann-Mattis
    full: Johann-Mattis List
    id: johann-mattis-list
    last: List
  - first: Johannes
    full: Johannes Wahle
    id: johannes-wahle
    last: Wahle
  - first: Gerhard
    full: "Gerhard J\xE4ger"
    id: gerhard-jager
    last: "J\xE4ger"
  author_string: "Taraka Rama, Johann-Mattis List, Johannes Wahle, Gerhard J\xE4ger"
  bibkey: rama-etal-2018-automatic
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2063
  month: June
  page_first: '393'
  page_last: '400'
  pages: "393\u2013400"
  paper_id: '63'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2063.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2063.jpg
  title: Are Automatic Methods for Cognate Detection Good Enough for Phylogenetic
    Reconstruction in Historical Linguistics?
  title_html: Are Automatic Methods for Cognate Detection Good Enough for Phylogenetic
    Reconstruction in Historical Linguistics?
  url: https://www.aclweb.org/anthology/N18-2063
  year: '2018'
N18-2064:
  abstract: This work introduces a new strategy to compare the numerous conventions
    that have been proposed over the years for expressing dependency structures and
    discover the one for which a parser will achieve the highest parsing performance.
    Instead of associating each sentence in the training set with a single gold reference
    we propose to consider a set of references encoding alternative syntactic representations.
    Training a parser with a dynamic oracle will then automatically select among all
    alternatives the reference that will be predicted with the highest accuracy. Experiments
    on the UD corpora show the validity of this approach.
  address: New Orleans, Louisiana
  author:
  - first: Guillaume
    full: Guillaume Wisniewski
    id: guillaume-wisniewski
    last: Wisniewski
  - first: "Oph\xE9lie"
    full: "Oph\xE9lie Lacroix"
    id: ophelie-lacroix
    last: Lacroix
  - first: "Fran\xE7ois"
    full: "Fran\xE7ois Yvon"
    id: francois-yvon
    last: Yvon
  author_string: "Guillaume Wisniewski, Oph\xE9lie Lacroix, Fran\xE7ois Yvon"
  bibkey: wisniewski-etal-2018-automatically
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2064
  month: June
  page_first: '401'
  page_last: '406'
  pages: "401\u2013406"
  paper_id: '64'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2064.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2064.jpg
  title: Automatically Selecting the Best Dependency Annotation Design with Dynamic
    Oracles
  title_html: Automatically Selecting the Best Dependency Annotation Design with Dynamic
    Oracles
  url: https://www.aclweb.org/anthology/N18-2064
  year: '2018'
N18-2065:
  abstract: In formal logic-based approaches to Recognizing Textual Entailment (RTE),
    a Combinatory Categorial Grammar (CCG) parser is used to parse input premises
    and hypotheses to obtain their logical formulas. Here, it is important that the
    parser processes the sentences consistently; failing to recognize the similar
    syntactic structure results in inconsistent predicate argument structures among
    them, in which case the succeeding theorem proving is doomed to failure. In this
    work, we present a simple method to extend an existing CCG parser to parse a set
    of sentences consistently, which is achieved with an inter-sentence modeling with
    Markov Random Fields (MRF). When combined with existing logic-based systems, our
    method always shows improvement in the RTE experiments on English and Japanese
    languages.
  address: New Orleans, Louisiana
  author:
  - first: Masashi
    full: Masashi Yoshikawa
    id: masashi-yoshikawa
    last: Yoshikawa
  - first: Koji
    full: Koji Mineshima
    id: koji-mineshima
    last: Mineshima
  - first: Hiroshi
    full: Hiroshi Noji
    id: hiroshi-noji
    last: Noji
  - first: Daisuke
    full: Daisuke Bekki
    id: daisuke-bekki
    last: Bekki
  author_string: Masashi Yoshikawa, Koji Mineshima, Hiroshi Noji, Daisuke Bekki
  bibkey: yoshikawa-etal-2018-consistent
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2065
  month: June
  page_first: '407'
  page_last: '412'
  pages: "407\u2013412"
  paper_id: '65'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2065.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2065.jpg
  title: Consistent CCG Parsing over Multiple Sentences for Improved Logical Reasoning
  title_html: Consistent <span class="acl-fixed-case">CCG</span> Parsing over Multiple
    Sentences for Improved Logical Reasoning
  url: https://www.aclweb.org/anthology/N18-2065
  year: '2018'
N18-2066:
  abstract: Because the most common transition systems are projective, training a
    transition-based dependency parser often implies to either ignore or rewrite the
    non-projective training examples, which has an adverse impact on accuracy. In
    this work, we propose a simple modification of dynamic oracles, which enables
    the use of non-projective data when training projective parsers. Evaluation on
    73 treebanks shows that our method achieves significant gains (+2 to +7 UAS for
    the most non-projective languages) and consistently outperforms traditional projectivization
    and pseudo-projectivization approaches.
  address: New Orleans, Louisiana
  author:
  - first: Lauriane
    full: Lauriane Aufrant
    id: lauriane-aufrant
    last: Aufrant
  - first: Guillaume
    full: Guillaume Wisniewski
    id: guillaume-wisniewski
    last: Wisniewski
  - first: "Fran\xE7ois"
    full: "Fran\xE7ois Yvon"
    id: francois-yvon
    last: Yvon
  author_string: "Lauriane Aufrant, Guillaume Wisniewski, Fran\xE7ois Yvon"
  bibkey: aufrant-etal-2018-exploiting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2066
  month: June
  page_first: '413'
  page_last: '419'
  pages: "413\u2013419"
  paper_id: '66'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2066.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2066.jpg
  title: Exploiting Dynamic Oracles to Train Projective Dependency Parsers on Non-Projective
    Trees
  title_html: Exploiting Dynamic Oracles to Train Projective Dependency Parsers on
    Non-Projective Trees
  url: https://www.aclweb.org/anthology/N18-2066
  year: '2018'
N18-2067:
  abstract: "We generalize Cohen, G\xF3mez-Rodr\xEDguez, and Satta\u2019s (2011) parser\
    \ to a family of non-projective transition-based dependency parsers allowing polynomial-time\
    \ exact inference. This includes novel parsers with better coverage than Cohen\
    \ et al. (2011), and even a variant that reduces time complexity to O(n6), improving\
    \ over the known bounds in exact inference for non-projective transition-based\
    \ parsing. We hope that this piece of theoretical work inspires design of novel\
    \ transition systems with better coverage and better run-time guarantees. , improving\
    \ over the known bounds in exact inference for non-projective transition-based\
    \ parsing. We hope that this piece of theoretical work inspires design of novel\
    \ transition systems with better coverage and better run-time guarantees."
  address: New Orleans, Louisiana
  author:
  - first: Tianze
    full: Tianze Shi
    id: tianze-shi
    last: Shi
  - first: Carlos
    full: "Carlos G\xF3mez-Rodr\xEDguez"
    id: carlos-gomez-rodriguez
    last: "G\xF3mez-Rodr\xEDguez"
  - first: Lillian
    full: Lillian Lee
    id: lillian-lee
    last: Lee
  author_string: "Tianze Shi, Carlos G\xF3mez-Rodr\xEDguez, Lillian Lee"
  bibkey: shi-etal-2018-improving
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2067
  month: June
  page_first: '420'
  page_last: '425'
  pages: "420\u2013425"
  paper_id: '67'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2067.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2067.jpg
  title: Improving Coverage and Runtime Complexity for Exact Inference in Non-Projective
    Transition-Based Dependency Parsers
  title_html: Improving Coverage and Runtime Complexity for Exact Inference in Non-Projective
    Transition-Based Dependency Parsers
  url: https://www.aclweb.org/anthology/N18-2067
  year: '2018'
N18-2068:
  abstract: "One of the most outstanding properties of multiword expressions (MWEs),\
    \ especially verbal ones (VMWEs), important both in theoretical models and applications,\
    \ is their idiosyncratic variability. Some MWEs are always continuous, while some\
    \ others admit certain types of insertions. Components of some MWEs are rarely\
    \ or never modified, while some others admit either specific or unrestricted modification.\
    \ This unpredictable variability profile of MWEs hinders modeling and processing\
    \ them as \u201Cwords-with-spaces\u201D on the one hand, and as regular syntactic\
    \ structures on the other hand. Since variability of MWEs is a matter of scale\
    \ rather than a binary property, we propose a 2-dimensional language-independent\
    \ measure of variability dedicated to verbal MWEs based on syntactic and discontinuity-related\
    \ clues. We assess its relevance with respect to a linguistic benchmark and its\
    \ utility for the tasks of VMWE classification and variant identification on a\
    \ French corpus."
  address: New Orleans, Louisiana
  author:
  - first: Caroline
    full: Caroline Pasquer
    id: caroline-pasquer
    last: Pasquer
  - first: Agata
    full: Agata Savary
    id: agata-savary
    last: Savary
  - first: Jean-Yves
    full: Jean-Yves Antoine
    id: jean-yves-antoine
    last: Antoine
  - first: Carlos
    full: Carlos Ramisch
    id: carlos-ramisch
    last: Ramisch
  author_string: Caroline Pasquer, Agata Savary, Jean-Yves Antoine, Carlos Ramisch
  bibkey: pasquer-etal-2018-towards
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2068
  month: June
  page_first: '426'
  page_last: '432'
  pages: "426\u2013432"
  paper_id: '68'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2068.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2068.jpg
  title: Towards a Variability Measure for Multiword Expressions
  title_html: Towards a Variability Measure for Multiword Expressions
  url: https://www.aclweb.org/anthology/N18-2068
  year: '2018'
N18-2069:
  abstract: We address the task of detecting foiled image captions, i.e. identifying
    whether a caption contains a word that has been deliberately replaced by a semantically
    similar word, thus rendering it inaccurate with respect to the image being described.
    Solving this problem should in principle require a fine-grained understanding
    of images to detect subtle perturbations in captions. In such contexts, encoding
    sufficiently descriptive image information becomes a key challenge. In this paper,
    we demonstrate that it is possible to solve this task using simple, interpretable
    yet powerful representations based on explicit object information over multilayer
    perceptron models. Our models achieve state-of-the-art performance on a recently
    published dataset, with scores exceeding those achieved by humans on the task.
    We also measure the upper-bound performance of our models using gold standard
    annotations. Our study and analysis reveals that the simpler model performs well
    even without image information, suggesting that the dataset contains strong linguistic
    bias.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276898131
    type: video
    url: http://vimeo.com/276898131
  author:
  - first: Pranava Swaroop
    full: Pranava Swaroop Madhyastha
    id: pranava-swaroop-madhyastha
    last: Madhyastha
  - first: Josiah
    full: Josiah Wang
    id: josiah-wang
    last: Wang
  - first: Lucia
    full: Lucia Specia
    id: lucia-specia
    last: Specia
  author_string: Pranava Swaroop Madhyastha, Josiah Wang, Lucia Specia
  bibkey: madhyastha-etal-2018-defoiling
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2069
  month: June
  page_first: '433'
  page_last: '438'
  pages: "433\u2013438"
  paper_id: '69'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2069.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2069.jpg
  title: Defoiling Foiled Image Captions
  title_html: Defoiling Foiled Image Captions
  url: https://www.aclweb.org/anthology/N18-2069
  year: '2018'
N18-2070:
  abstract: "We combine a neural image captioner with a Rational Speech Acts (RSA)\
    \ model to make a system that is pragmatically informative: its objective is to\
    \ produce captions that are not merely true but also distinguish their inputs\
    \ from similar images. Previous attempts to combine RSA with neural image captioning\
    \ require an inference which normalizes over the entire set of possible utterances.\
    \ This poses a serious problem of efficiency, previously solved by sampling a\
    \ small subset of possible utterances. We instead solve this problem by implementing\
    \ a version of RSA which operates at the level of characters (\u201Ca\u201D, \u201C\
    b\u201D, \u201Cc\u201D, ...) during the unrolling of the caption. We find that\
    \ the utterance-level effect of referential captions can be obtained with only\
    \ character-level decisions. Finally, we introduce an automatic method for testing\
    \ the performance of pragmatic speaker models, and show that our model outperforms\
    \ a non-pragmatic baseline as well as a word-level RSA captioner."
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276898147
    type: video
    url: http://vimeo.com/276898147
  author:
  - first: Reuben
    full: Reuben Cohn-Gordon
    id: reuben-cohn-gordon
    last: Cohn-Gordon
  - first: Noah
    full: Noah Goodman
    id: noah-goodman
    last: Goodman
  - first: Christopher
    full: Christopher Potts
    id: christopher-potts
    last: Potts
  author_string: Reuben Cohn-Gordon, Noah Goodman, Christopher Potts
  bibkey: cohn-gordon-etal-2018-pragmatically
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2070
  month: June
  page_first: '439'
  page_last: '443'
  pages: "439\u2013443"
  paper_id: '70'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2070.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2070.jpg
  title: Pragmatically Informative Image Captioning with Character-Level Inference
  title_html: Pragmatically Informative Image Captioning with Character-Level Inference
  url: https://www.aclweb.org/anthology/N18-2070
  year: '2018'
N18-2071:
  abstract: Visual reasoning with compositional natural language instructions, e.g.,
    based on the newly-released Cornell Natural Language Visual Reasoning (NLVR) dataset,
    is a challenging task, where the model needs to have the ability to create an
    accurate mapping between the diverse phrases and the several objects placed in
    complex arrangements in the image. Further, this mapping needs to be processed
    to answer the question in the statement given the ordering and relationship of
    the objects across three similar images. In this paper, we propose a novel end-to-end
    neural model for the NLVR task, where we first use joint bidirectional attention
    to build a two-way conditioning between the visual information and the language
    phrases. Next, we use an RL-based pointer network to sort and process the varying
    number of unordered objects (so as to match the order of the statement phrases)
    in each of the three images and then pool over the three decisions. Our model
    achieves strong improvements (of 4-6% absolute) over the state-of-the-art on both
    the structured representation and raw image versions of the dataset.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/276898161
    type: video
    url: http://vimeo.com/276898161
  author:
  - first: Hao
    full: Hao Tan
    id: hao-tan
    last: Tan
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  author_string: Hao Tan, Mohit Bansal
  bibkey: tan-bansal-2018-object
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2071
  month: June
  page_first: '444'
  page_last: '451'
  pages: "444\u2013451"
  paper_id: '71'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2071.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2071.jpg
  title: Object Ordering with Bidirectional Matchings for Visual Reasoning
  title_html: Object Ordering with Bidirectional Matchings for Visual Reasoning
  url: https://www.aclweb.org/anthology/N18-2071
  year: '2018'
N18-2072:
  abstract: We propose a novel data augmentation for labeled sentences called contextual
    augmentation. We assume an invariance that sentences are natural even if the words
    in the sentences are replaced with other words with paradigmatic relations. We
    stochastically replace words with other words that are predicted by a bi-directional
    language model at the word positions. Words predicted according to a context are
    numerous but appropriate for the augmentation of the original words. Furthermore,
    we retrofit a language model with a label-conditional architecture, which allows
    the model to augment sentences without breaking the label-compatibility. Through
    the experiments for six various different text classification tasks, we demonstrate
    that the proposed method improves classifiers based on the convolutional or recurrent
    neural networks.
  address: New Orleans, Louisiana
  author:
  - first: Sosuke
    full: Sosuke Kobayashi
    id: sosuke-kobayashi
    last: Kobayashi
  author_string: Sosuke Kobayashi
  bibkey: kobayashi-2018-contextual
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2072
  month: June
  page_first: '452'
  page_last: '457'
  pages: "452\u2013457"
  paper_id: '72'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2072.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2072.jpg
  title: 'Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations'
  title_html: 'Contextual Augmentation: Data Augmentation by Words with Paradigmatic
    Relations'
  url: https://www.aclweb.org/anthology/N18-2072
  year: '2018'
N18-2073:
  abstract: "Cross-lingual information retrieval (CLIR) is a document retrieval task\
    \ where the documents are written in a language different from that of the user\u2019\
    s query. This is a challenging problem for data-driven approaches due to the general\
    \ lack of labeled training data. We introduce a large-scale dataset derived from\
    \ Wikipedia to support CLIR research in 25 languages. Further, we present a simple\
    \ yet effective neural learning-to-rank model that shares representations across\
    \ languages and reduces the data requirement. This model can exploit training\
    \ data in, for example, Japanese-English CLIR to improve the results of Swahili-English\
    \ CLIR."
  address: New Orleans, Louisiana
  author:
  - first: Shota
    full: Shota Sasaki
    id: shota-sasaki
    last: Sasaki
  - first: Shuo
    full: Shuo Sun
    id: shuo-sun
    last: Sun
  - first: Shigehiko
    full: Shigehiko Schamoni
    id: shigehiko-schamoni
    last: Schamoni
  - first: Kevin
    full: Kevin Duh
    id: kevin-duh
    last: Duh
  - first: Kentaro
    full: Kentaro Inui
    id: kentaro-inui
    last: Inui
  author_string: Shota Sasaki, Shuo Sun, Shigehiko Schamoni, Kevin Duh, Kentaro Inui
  bibkey: sasaki-etal-2018-cross
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2073
  month: June
  page_first: '458'
  page_last: '463'
  pages: "458\u2013463"
  paper_id: '73'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2073.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2073.jpg
  title: Cross-Lingual Learning-to-Rank with Shared Representations
  title_html: Cross-Lingual Learning-to-Rank with Shared Representations
  url: https://www.aclweb.org/anthology/N18-2073
  year: '2018'
N18-2074:
  abstract: Relying entirely on an attention mechanism, the Transformer introduced
    by Vaswani et al. (2017) achieves state-of-the-art results for machine translation.
    In contrast to recurrent and convolutional neural networks, it does not explicitly
    model relative or absolute position information in its structure. Instead, it
    requires adding representations of absolute positions to its inputs. In this work
    we present an alternative approach, extending the self-attention mechanism to
    efficiently consider representations of the relative positions, or distances between
    sequence elements. On the WMT 2014 English-to-German and English-to-French translation
    tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute
    position representations, respectively. Notably, we observe that combining relative
    and absolute position representations yields no further improvement in translation
    quality. We describe an efficient implementation of our method and cast it as
    an instance of relation-aware self-attention mechanisms that can generalize to
    arbitrary graph-labeled inputs.
  address: New Orleans, Louisiana
  author:
  - first: Peter
    full: Peter Shaw
    id: peter-shaw
    last: Shaw
  - first: Jakob
    full: Jakob Uszkoreit
    id: jakob-uszkoreit
    last: Uszkoreit
  - first: Ashish
    full: Ashish Vaswani
    id: ashish-vaswani
    last: Vaswani
  author_string: Peter Shaw, Jakob Uszkoreit, Ashish Vaswani
  bibkey: shaw-etal-2018-self
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2074
  month: June
  page_first: '464'
  page_last: '468'
  pages: "464\u2013468"
  paper_id: '74'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2074.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2074.jpg
  title: Self-Attention with Relative Position Representations
  title_html: Self-Attention with Relative Position Representations
  url: https://www.aclweb.org/anthology/N18-2074
  year: '2018'
N18-2075:
  abstract: Text segmentation, the task of dividing a document into contiguous segments
    based on its semantic structure, is a longstanding challenge in language understanding.
    Previous work on text segmentation focused on unsupervised methods such as clustering
    or graph search, due to the paucity in labeled data. In this work, we formulate
    text segmentation as a supervised learning problem, and present a large new dataset
    for text segmentation that is automatically extracted and labeled from Wikipedia.
    Moreover, we develop a segmentation model based on this dataset and show that
    it generalizes well to unseen natural text.
  address: New Orleans, Louisiana
  author:
  - first: Omri
    full: Omri Koshorek
    id: omri-koshorek
    last: Koshorek
  - first: Adir
    full: Adir Cohen
    id: adir-cohen
    last: Cohen
  - first: Noam
    full: Noam Mor
    id: noam-mor
    last: Mor
  - first: Michael
    full: Michael Rotman
    id: michael-rotman
    last: Rotman
  - first: Jonathan
    full: Jonathan Berant
    id: jonathan-berant
    last: Berant
  author_string: Omri Koshorek, Adir Cohen, Noam Mor, Michael Rotman, Jonathan Berant
  bibkey: koshorek-etal-2018-text
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2075
  month: June
  page_first: '469'
  page_last: '473'
  pages: "469\u2013473"
  paper_id: '75'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2075.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2075.jpg
  title: Text Segmentation as a Supervised Learning Task
  title_html: Text Segmentation as a Supervised Learning Task
  url: https://www.aclweb.org/anthology/N18-2075
  year: '2018'
N18-2076:
  abstract: Most real world language problems require learning from heterogenous corpora,
    raising the problem of learning robust models which generalise well to both similar
    (in domain) and dissimilar (out of domain) instances to those seen in training.
    This requires learning an underlying task, while not learning irrelevant signals
    and biases specific to individual domains. We propose a novel method to optimise
    both in- and out-of-domain accuracy based on joint learning of a structured neural
    model with domain-specific and domain-general components, coupled with adversarial
    training for domain. Evaluating on multi-domain language identification and multi-domain
    sentiment analysis, we show substantial improvements over standard domain adaptation
    techniques, and domain-adversarial training.
  address: New Orleans, Louisiana
  author:
  - first: Yitong
    full: Yitong Li
    id: yitong-li
    last: Li
  - first: Timothy
    full: Timothy Baldwin
    id: timothy-baldwin
    last: Baldwin
  - first: Trevor
    full: Trevor Cohn
    id: trevor-cohn
    last: Cohn
  author_string: Yitong Li, Timothy Baldwin, Trevor Cohn
  bibkey: li-etal-2018-whats
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2076
  month: June
  page_first: '474'
  page_last: '479'
  pages: "474\u2013479"
  paper_id: '76'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2076.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2076.jpg
  title: "What\u2019s in a Domain? Learning Domain-Robust Text Representations using\
    \ Adversarial Training"
  title_html: "What\u2019s in a Domain? Learning Domain-Robust Text Representations\
    \ using Adversarial Training"
  url: https://www.aclweb.org/anthology/N18-2076
  year: '2018'
N18-2077:
  abstract: We propose a variant of a well-known machine translation (MT) evaluation
    metric, HyTER (Dreyer and Marcu, 2012), which exploits reference translations
    enriched with meaning equivalent expressions. The original HyTER metric relied
    on hand-crafted paraphrase networks which restricted its applicability to new
    data. We test, for the first time, HyTER with automatically built paraphrase lattices.
    We show that although the metric obtains good results on small and carefully curated
    data with both manually and automatically selected substitutes, it achieves medium
    performance on much larger and noisier datasets, demonstrating the limits of the
    metric for tuning and evaluation of current MT systems.
  address: New Orleans, Louisiana
  author:
  - first: Marianna
    full: Marianna Apidianaki
    id: marianna-apidianaki
    last: Apidianaki
  - first: Guillaume
    full: Guillaume Wisniewski
    id: guillaume-wisniewski
    last: Wisniewski
  - first: Anne
    full: Anne Cocos
    id: anne-cocos
    last: Cocos
  - first: Chris
    full: Chris Callison-Burch
    id: chris-callison-burch
    last: Callison-Burch
  author_string: Marianna Apidianaki, Guillaume Wisniewski, Anne Cocos, Chris Callison-Burch
  bibkey: apidianaki-etal-2018-automated
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2077
  month: June
  page_first: '480'
  page_last: '485'
  pages: "480\u2013485"
  paper_id: '77'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2077.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2077.jpg
  title: Automated Paraphrase Lattice Creation for HyTER Machine Translation Evaluation
  title_html: Automated Paraphrase Lattice Creation for <span class="acl-fixed-case">H</span>y<span
    class="acl-fixed-case">TER</span> Machine Translation Evaluation
  url: https://www.aclweb.org/anthology/N18-2077
  year: '2018'
N18-2078:
  abstract: "Semantic representations have long been argued as potentially useful\
    \ for enforcing meaning preservation and improving generalization performance\
    \ of machine translation methods. In this work, we are the first to incorporate\
    \ information about predicate-argument structure of source sentences (namely,\
    \ semantic-role representations) into neural machine translation. We use Graph\
    \ Convolutional Networks (GCNs) to inject a semantic bias into sentence encoders\
    \ and achieve improvements in BLEU scores over the linguistic-agnostic and syntax-aware\
    \ versions on the English\u2013German language pair."
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-2078.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-2078.Notes.pdf
  author:
  - first: Diego
    full: Diego Marcheggiani
    id: diego-marcheggiani
    last: Marcheggiani
  - first: Joost
    full: Joost Bastings
    id: joost-bastings
    last: Bastings
  - first: Ivan
    full: Ivan Titov
    id: ivan-titov
    last: Titov
  author_string: Diego Marcheggiani, Joost Bastings, Ivan Titov
  bibkey: marcheggiani-etal-2018-exploiting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2078
  month: June
  page_first: '486'
  page_last: '492'
  pages: "486\u2013492"
  paper_id: '78'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2078.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2078.jpg
  title: Exploiting Semantics in Neural Machine Translation with Graph Convolutional
    Networks
  title_html: Exploiting Semantics in Neural Machine Translation with Graph Convolutional
    Networks
  url: https://www.aclweb.org/anthology/N18-2078
  year: '2018'
N18-2079:
  abstract: We address the problem of simultaneous translation by modifying the Neural
    MT decoder to operate with dynamically built encoder and attention. We propose
    a tunable agent which decides the best segmentation strategy for a user-defined
    BLEU loss and Average Proportion (AP) constraint. Our agent outperforms previously
    proposed Wait-if-diff and Wait-if-worse agents (Cho and Esipova, 2016) on BLEU
    with a lower latency. Secondly we proposed data-driven changes to Neural MT training
    to better match the incremental decoding framework.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-2079.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-2079.Notes.pdf
  author:
  - first: Fahim
    full: Fahim Dalvi
    id: fahim-dalvi
    last: Dalvi
  - first: Nadir
    full: Nadir Durrani
    id: nadir-durrani
    last: Durrani
  - first: Hassan
    full: Hassan Sajjad
    id: hassan-sajjad
    last: Sajjad
  - first: Stephan
    full: Stephan Vogel
    id: stephan-vogel
    last: Vogel
  author_string: Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Stephan Vogel
  bibkey: dalvi-etal-2018-incremental
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2079
  month: June
  page_first: '493'
  page_last: '499'
  pages: "493\u2013499"
  paper_id: '79'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2079.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2079.jpg
  title: Incremental Decoding and Training Methods for Simultaneous Translation in
    Neural Machine Translation
  title_html: Incremental Decoding and Training Methods for Simultaneous Translation
    in Neural Machine Translation
  url: https://www.aclweb.org/anthology/N18-2079
  year: '2018'
N18-2080:
  abstract: In this paper we explore the use of Learning Hidden Unit Contribution
    for the task of neural machine translation. The method was initially proposed
    in the context of speech recognition for adapting a general system to the specific
    acoustic characteristics of each speaker. Similar in spirit, in a machine translation
    framework we want to adapt a general system to a specific domain. We show that
    the proposed method achieves improvements of up to 2.6 BLEU points over a general
    system, and up to 6 BLEU points if the initial system has only been trained on
    out-of-domain data, a situation which may easily happen in practice. The good
    performance together with its short training time and small memory footprint make
    it a very attractive solution for domain adaptation.
  address: New Orleans, Louisiana
  author:
  - first: David
    full: David Vilar
    id: david-vilar
    last: Vilar
  author_string: David Vilar
  bibkey: vilar-2018-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2080
  month: June
  page_first: '500'
  page_last: '505'
  pages: "500\u2013505"
  paper_id: '80'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2080.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2080.jpg
  title: Learning Hidden Unit Contribution for Adapting Neural Machine Translation
    Models
  title_html: Learning Hidden Unit Contribution for Adapting Neural Machine Translation
    Models
  url: https://www.aclweb.org/anthology/N18-2080
  year: '2018'
N18-2081:
  abstract: Despite the impressive quality improvements yielded by neural machine
    translation (NMT) systems, controlling their translation output to adhere to user-provided
    terminology constraints remains an open problem. We describe our approach to constrained
    neural decoding based on finite-state machines and multi-stack decoding which
    supports target-side constraints as well as constraints with corresponding aligned
    input text spans. We demonstrate the performance of our framework on multiple
    translation tasks and motivate the need for constrained decoding with attentions
    as a means of reducing misplacement and duplication when translating user constraints.
  address: New Orleans, Louisiana
  author:
  - first: Eva
    full: Eva Hasler
    id: eva-hasler
    last: Hasler
  - first: "Adri\xE0"
    full: "Adri\xE0 de Gispert"
    id: adria-de-gispert
    last: de Gispert
  - first: Gonzalo
    full: Gonzalo Iglesias
    id: gonzalo-iglesias
    last: Iglesias
  - first: Bill
    full: Bill Byrne
    id: bill-byrne
    last: Byrne
  author_string: "Eva Hasler, Adri\xE0 de Gispert, Gonzalo Iglesias, Bill Byrne"
  bibkey: hasler-etal-2018-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2081
  month: June
  page_first: '506'
  page_last: '512'
  pages: "506\u2013512"
  paper_id: '81'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2081.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2081.jpg
  title: Neural Machine Translation Decoding with Terminology Constraints
  title_html: Neural Machine Translation Decoding with Terminology Constraints
  url: https://www.aclweb.org/anthology/N18-2081
  year: '2018'
N18-2082:
  abstract: We propose a process for investigating the extent to which sentence representations
    arising from neural machine translation (NMT) systems encode distinct semantic
    phenomena. We use these representations as features to train a natural language
    inference (NLI) classifier based on datasets recast from existing semantic annotations.
    In applying this process to a representative NMT system, we find its encoder appears
    most suited to supporting inferences at the syntax-semantics interface, as compared
    to anaphora resolution requiring world knowledge. We conclude with a discussion
    on the merits and potential deficiencies of the existing process, and how it may
    be improved and extended as a broader framework for evaluating semantic coverage
  address: New Orleans, Louisiana
  author:
  - first: Adam
    full: Adam Poliak
    id: adam-poliak
    last: Poliak
  - first: Yonatan
    full: Yonatan Belinkov
    id: yonatan-belinkov
    last: Belinkov
  - first: James
    full: James Glass
    id: james-glass
    last: Glass
  - first: Benjamin
    full: Benjamin Van Durme
    id: benjamin-van-durme
    last: Van Durme
  author_string: Adam Poliak, Yonatan Belinkov, James Glass, Benjamin Van Durme
  bibkey: poliak-etal-2018-evaluation
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2082
  month: June
  page_first: '513'
  page_last: '523'
  pages: "513\u2013523"
  paper_id: '82'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2082.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2082.jpg
  title: On the Evaluation of Semantic Phenomena in Neural Machine Translation Using
    Natural Language Inference
  title_html: On the Evaluation of Semantic Phenomena in Neural Machine Translation
    Using Natural Language Inference
  url: https://www.aclweb.org/anthology/N18-2082
  year: '2018'
N18-2083:
  abstract: We present a method for improving word alignments using word similarities.
    This method is based on encouraging common alignment links between semantically
    similar words. We use word vectors trained on monolingual data to estimate similarity.
    Our experiments on translating fifteen languages into English show consistent
    BLEU score improvements across the languages.
  address: New Orleans, Louisiana
  author:
  - first: Nima
    full: Nima Pourdamghani
    id: nima-pourdamghani
    last: Pourdamghani
  - first: Marjan
    full: Marjan Ghazvininejad
    id: marjan-ghazvininejad
    last: Ghazvininejad
  - first: Kevin
    full: Kevin Knight
    id: kevin-knight
    last: Knight
  author_string: Nima Pourdamghani, Marjan Ghazvininejad, Kevin Knight
  bibkey: pourdamghani-etal-2018-using
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2083
  month: June
  page_first: '524'
  page_last: '528'
  pages: "524\u2013528"
  paper_id: '83'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2083.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2083.jpg
  title: Using Word Vectors to Improve Word Alignments for Low Resource Machine Translation
  title_html: Using Word Vectors to Improve Word Alignments for Low Resource Machine
    Translation
  url: https://www.aclweb.org/anthology/N18-2083
  year: '2018'
N18-2084:
  abstract: "The performance of Neural Machine Translation (NMT) systems often suffers\
    \ in low-resource scenarios where sufficiently large-scale parallel corpora cannot\
    \ be obtained. Pre-trained word embeddings have proven to be invaluable for improving\
    \ performance in natural language analysis tasks, which often suffer from paucity\
    \ of data. However, their utility for NMT has not been extensively explored. In\
    \ this work, we perform five sets of experiments that analyze when we can expect\
    \ pre-trained word embeddings to help in NMT tasks. We show that such embeddings\
    \ can be surprisingly effective in some cases \u2013 providing gains of up to\
    \ 20 BLEU points in the most favorable setting."
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-2084.Software.tgz
    type: software
    url: https://www.aclweb.org/anthology/attachments/N18-2084.Software.tgz
  author:
  - first: Ye
    full: Ye Qi
    id: ye-qi
    last: Qi
  - first: Devendra
    full: Devendra Sachan
    id: devendra-sachan
    last: Sachan
  - first: Matthieu
    full: Matthieu Felix
    id: matthieu-felix
    last: Felix
  - first: Sarguna
    full: Sarguna Padmanabhan
    id: sarguna-padmanabhan
    last: Padmanabhan
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  author_string: Ye Qi, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, Graham
    Neubig
  bibkey: qi-etal-2018-pre
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2084
  month: June
  page_first: '529'
  page_last: '535'
  pages: "529\u2013535"
  paper_id: '84'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2084.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2084.jpg
  title: When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?
  title_html: When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine
    Translation?
  url: https://www.aclweb.org/anthology/N18-2084
  year: '2018'
N18-2085:
  abstract: 'For general modeling methods applied to diverse languages, a natural
    question is: how well should we expect our models to work on languages with differing
    typological profiles? In this work, we develop an evaluation framework for fair
    cross-linguistic comparison of language models, using translated text so that
    all models are asked to predict approximately the same information. We then conduct
    a study on 21 languages, demonstrating that in some languages, the textual expression
    of the information is harder to predict with both n-gram and LSTM language models.
    We show complex inflectional morphology to be a cause of performance differences
    among languages.'
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-2085.Poster.pdf
    type: poster
    url: https://www.aclweb.org/anthology/attachments/N18-2085.Poster.pdf
  author:
  - first: Ryan
    full: Ryan Cotterell
    id: ryan-cotterell
    last: Cotterell
  - first: Sebastian J.
    full: Sebastian J. Mielke
    id: sebastian-j-mielke
    last: Mielke
  - first: Jason
    full: Jason Eisner
    id: jason-eisner
    last: Eisner
  - first: Brian
    full: Brian Roark
    id: brian-roark
    last: Roark
  author_string: Ryan Cotterell, Sebastian J. Mielke, Jason Eisner, Brian Roark
  bibkey: cotterell-etal-2018-languages
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2085
  month: June
  page_first: '536'
  page_last: '541'
  pages: "536\u2013541"
  paper_id: '85'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2085.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2085.jpg
  title: Are All Languages Equally Hard to Language-Model?
  title_html: Are All Languages Equally Hard to Language-Model?
  url: https://www.aclweb.org/anthology/N18-2085
  year: '2018'
N18-2086:
  abstract: We analyze the complexity of the problem of determining whether a set
    of phonemes forms a natural class and, if so, that of finding the minimal feature
    specification for the class. A standard assumption in phonology is that finding
    a minimal feature specification is an automatic part of acquisition and generalization.
    We find that the natural class decision problem is tractable (i.e. is in P), while
    the minimization problem is not; the decision version of the problem which determines
    whether a natural class can be defined with k features or less is NP-complete.
    We also show that, empirically, a greedy algorithm for finding minimal feature
    specifications will sometimes fail, and thus cannot be assumed to be the basis
    for human performance in solving the problem. features or less is NP-complete.
    We also show that, empirically, a greedy algorithm for finding minimal feature
    specifications will sometimes fail, and thus cannot be assumed to be the basis
    for human performance in solving the problem.
  address: New Orleans, Louisiana
  author:
  - first: Hubie
    full: Hubie Chen
    id: hubie-chen
    last: Chen
  - first: Mans
    full: Mans Hulden
    id: mans-hulden
    last: Hulden
  author_string: Hubie Chen, Mans Hulden
  bibkey: chen-hulden-2018-computational
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2086
  month: June
  page_first: '542'
  page_last: '547'
  pages: "542\u2013547"
  paper_id: '86'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2086.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2086.jpg
  title: The Computational Complexity of Distinctive Feature Minimization in Phonology
  title_html: The Computational Complexity of Distinctive Feature Minimization in
    Phonology
  url: https://www.aclweb.org/anthology/N18-2086
  year: '2018'
N18-2087:
  abstract: "Lexical ambiguity makes it difficult to compute useful statistics of\
    \ a corpus. A given word form might represent any of several morphological feature\
    \ bundles. One can, however, use unsupervised learning (as in EM) to fit a model\
    \ that probabilistically disambiguates word forms. We present such an approach,\
    \ which employs a neural network to smoothly model a prior distribution over feature\
    \ bundles (even rare ones). Although this basic model does not consider a token\u2019\
    s context, that very property allows it to operate on a simple list of unigram\
    \ type counts, partitioning each count among different analyses of that unigram.\
    \ We discuss evaluation metrics for this novel task and report results on 5 languages."
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-2087.Poster.pdf
    type: poster
    url: https://www.aclweb.org/anthology/attachments/N18-2087.Poster.pdf
  author:
  - first: Ryan
    full: Ryan Cotterell
    id: ryan-cotterell
    last: Cotterell
  - first: Christo
    full: Christo Kirov
    id: christo-kirov
    last: Kirov
  - first: Sebastian J.
    full: Sebastian J. Mielke
    id: sebastian-j-mielke
    last: Mielke
  - first: Jason
    full: Jason Eisner
    id: jason-eisner
    last: Eisner
  author_string: Ryan Cotterell, Christo Kirov, Sebastian J. Mielke, Jason Eisner
  bibkey: cotterell-etal-2018-unsupervised
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2087
  month: June
  page_first: '548'
  page_last: '553'
  pages: "548\u2013553"
  paper_id: '87'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2087.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2087.jpg
  title: Unsupervised Disambiguation of Syncretism in Inflected Lexicons
  title_html: Unsupervised Disambiguation of Syncretism in Inflected Lexicons
  url: https://www.aclweb.org/anthology/N18-2087
  year: '2018'
N18-2088:
  abstract: Reading a document and extracting an answer to a question about its content
    has attracted substantial attention recently. While most work has focused on the
    interaction between the question and the document, in this work we evaluate the
    importance of context when the question and document are processed independently.
    We take a standard neural architecture for this task, and show that by providing
    rich contextualized word representations from a large pre-trained language model
    as well as allowing the model to choose between context-dependent and context-independent
    word representations, we can obtain dramatic improvements and reach performance
    comparable to state-of-the-art on the competitive SQuAD dataset.
  address: New Orleans, Louisiana
  author:
  - first: Shimi
    full: Shimi Salant
    id: shimi-salant
    last: Salant
  - first: Jonathan
    full: Jonathan Berant
    id: jonathan-berant
    last: Berant
  author_string: Shimi Salant, Jonathan Berant
  bibkey: salant-berant-2018-contextualized
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2088
  month: June
  page_first: '554'
  page_last: '559'
  pages: "554\u2013559"
  paper_id: '88'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2088.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2088.jpg
  title: Contextualized Word Representations for Reading Comprehension
  title_html: Contextualized Word Representations for Reading Comprehension
  url: https://www.aclweb.org/anthology/N18-2088
  year: '2018'
N18-2089:
  abstract: We introduce Question-Answer Meaning Representations (QAMRs), which represent
    the predicate-argument structure of a sentence as a set of question-answer pairs.
    We develop a crowdsourcing scheme to show that QAMRs can be labeled with very
    little training, and gather a dataset with over 5,000 sentences and 100,000 questions.
    A qualitative analysis demonstrates that the crowd-generated question-answer pairs
    cover the vast majority of predicate-argument relationships in existing datasets
    (including PropBank, NomBank, and QA-SRL) along with many previously under-resourced
    ones, including implicit arguments and relations. We also report baseline models
    for question generation and answering, and summarize a recent approach for using
    QAMR labels to improve an Open IE system. These results suggest the freely available
    QAMR data and annotation scheme should support significant future work.
  address: New Orleans, Louisiana
  author:
  - first: Julian
    full: Julian Michael
    id: julian-michael
    last: Michael
  - first: Gabriel
    full: Gabriel Stanovsky
    id: gabriel-stanovsky
    last: Stanovsky
  - first: Luheng
    full: Luheng He
    id: luheng-he
    last: He
  - first: Ido
    full: Ido Dagan
    id: ido-dagan
    last: Dagan
  - first: Luke
    full: Luke Zettlemoyer
    id: luke-zettlemoyer
    last: Zettlemoyer
  author_string: Julian Michael, Gabriel Stanovsky, Luheng He, Ido Dagan, Luke Zettlemoyer
  bibkey: michael-etal-2018-crowdsourcing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2089
  month: June
  page_first: '560'
  page_last: '568'
  pages: "560\u2013568"
  paper_id: '89'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2089.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2089.jpg
  title: Crowdsourcing Question-Answer Meaning Representations
  title_html: Crowdsourcing Question-Answer Meaning Representations
  url: https://www.aclweb.org/anthology/N18-2089
  year: '2018'
N18-2090:
  abstract: The task of natural question generation is to generate a corresponding
    question given the input passage (fact) and answer. It is useful for enlarging
    the training set of QA systems. Previous work has adopted sequence-to-sequence
    models that take a passage with an additional bit to indicate answer position
    as input. However, they do not explicitly model the information between answer
    and other context within the passage. We propose a model that matches the answer
    with the passage before generating the question. Experiments show that our model
    outperforms the existing state of the art using rich features.
  address: New Orleans, Louisiana
  author:
  - first: Linfeng
    full: Linfeng Song
    id: linfeng-song
    last: Song
  - first: Zhiguo
    full: Zhiguo Wang
    id: zhiguo-wang
    last: Wang
  - first: Wael
    full: Wael Hamza
    id: wael-hamza
    last: Hamza
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  - first: Daniel
    full: Daniel Gildea
    id: daniel-gildea
    last: Gildea
  author_string: Linfeng Song, Zhiguo Wang, Wael Hamza, Yue Zhang, Daniel Gildea
  bibkey: song-etal-2018-leveraging
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2090
  month: June
  page_first: '569'
  page_last: '574'
  pages: "569\u2013574"
  paper_id: '90'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2090.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2090.jpg
  title: Leveraging Context Information for Natural Question Generation
  title_html: Leveraging Context Information for Natural Question Generation
  url: https://www.aclweb.org/anthology/N18-2090
  year: '2018'
N18-2091:
  abstract: "It is shown that many published models for the Stanford Question Answering\
    \ Dataset (Rajpurkar et al., 2016) lack robustness, suffering an over 50% decrease\
    \ in F1 score during adversarial evaluation based on the AddSent (Jia and Liang,\
    \ 2017) algorithm. It has also been shown that retraining models on data generated\
    \ by AddSent has limited effect on their robustness. We propose a novel alternative\
    \ adversary-generation algorithm, AddSentDiverse, that significantly increases\
    \ the variance within the adversarial training data by providing effective examples\
    \ that punish the model for making certain superficial assumptions. Further, in\
    \ order to improve robustness to AddSent\u2019s semantic perturbations (e.g.,\
    \ antonyms), we jointly improve the model\u2019s semantic-relationship learning\
    \ capabilities in addition to our AddSentDiverse-based adversarial training data\
    \ augmentation. With these additions, we show that we can make a state-of-the-art\
    \ model significantly more robust, achieving a 36.5% increase in F1 score under\
    \ many different types of adversarial evaluation while maintaining performance\
    \ on the regular SQuAD task."
  address: New Orleans, Louisiana
  author:
  - first: Yicheng
    full: Yicheng Wang
    id: yicheng-wang
    last: Wang
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  author_string: Yicheng Wang, Mohit Bansal
  bibkey: wang-bansal-2018-robust
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2091
  month: June
  page_first: '575'
  page_last: '581'
  pages: "575\u2013581"
  paper_id: '91'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2091.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2091.jpg
  title: Robust Machine Comprehension Models via Adversarial Training
  title_html: Robust Machine Comprehension Models via Adversarial Training
  url: https://www.aclweb.org/anthology/N18-2091
  year: '2018'
N18-2092:
  abstract: Recent success of deep learning models for the task of extractive Question
    Answering (QA) is hinged on the availability of large annotated corpora. However,
    large domain specific annotated corpora are limited and expensive to construct.
    In this work, we envision a system where the end user specifies a set of base
    documents and only a few labelled examples. Our system exploits the document structure
    to create cloze-style questions from these base documents; pre-trains a powerful
    neural network on the cloze style questions; and further fine-tunes the model
    on the labeled examples. We evaluate our proposed system across three diverse
    datasets from different domains, and find it to be highly effective with very
    little labeled data. We attain more than 50% F1 score on SQuAD and TriviaQA with
    less than a thousand labelled examples. We are also releasing a set of 3.2M cloze-style
    questions for practitioners to use while building QA systems.
  address: New Orleans, Louisiana
  author:
  - first: Bhuwan
    full: Bhuwan Dhingra
    id: bhuwan-dhingra
    last: Dhingra
  - first: Danish
    full: Danish Danish
    id: danish-danish
    last: Danish
  - first: Dheeraj
    full: Dheeraj Rajagopal
    id: dheeraj-rajagopal
    last: Rajagopal
  author_string: Bhuwan Dhingra, Danish Danish, Dheeraj Rajagopal
  bibkey: dhingra-etal-2018-simple
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2092
  month: June
  page_first: '582'
  page_last: '587'
  pages: "582\u2013587"
  paper_id: '92'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2092.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2092.jpg
  title: Simple and Effective Semi-Supervised Question Answering
  title_html: Simple and Effective Semi-Supervised Question Answering
  url: https://www.aclweb.org/anthology/N18-2092
  year: '2018'
N18-2093:
  abstract: "Interacting with relational databases through natural language helps\
    \ users with any background easily query and analyze a vast amount of data. This\
    \ requires a system that understands users\u2019 questions and converts them to\
    \ SQL queries automatically. In this paper, we present a novel approach TypeSQL\
    \ which formats the problem as a slot filling task in a more reasonable way. In\
    \ addition, TypeSQL utilizes type information to better understand rare entities\
    \ and numbers in the questions. We experiment this idea on the WikiSQL dataset\
    \ and outperform the prior art by 6% in much shorter time. We also show that accessing\
    \ the content of databases can significantly improve the performance when users\u2019\
    \ queries are not well-formed. TypeSQL can reach 82.6% accuracy, a 17.5% absolute\
    \ improvement compared to the previous content-sensitive model."
  address: New Orleans, Louisiana
  author:
  - first: Tao
    full: Tao Yu
    id: tao-yu
    last: Yu
  - first: Zifan
    full: Zifan Li
    id: zifan-li
    last: Li
  - first: Zilin
    full: Zilin Zhang
    id: zilin-zhang
    last: Zhang
  - first: Rui
    full: Rui Zhang
    id: rui-zhang
    last: Zhang
  - first: Dragomir
    full: Dragomir Radev
    id: dragomir-radev
    last: Radev
  author_string: Tao Yu, Zifan Li, Zilin Zhang, Rui Zhang, Dragomir Radev
  bibkey: yu-etal-2018-typesql
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2093
  month: June
  page_first: '588'
  page_last: '594'
  pages: "588\u2013594"
  paper_id: '93'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2093.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2093.jpg
  title: 'TypeSQL: Knowledge-Based Type-Aware Neural Text-to-SQL Generation'
  title_html: '<span class="acl-fixed-case">T</span>ype<span class="acl-fixed-case">SQL</span>:
    Knowledge-Based Type-Aware Neural Text-to-<span class="acl-fixed-case">SQL</span>
    Generation'
  url: https://www.aclweb.org/anthology/N18-2093
  year: '2018'
N18-2094:
  abstract: 'This paper addresses the problem of community membership detection using
    only text features in a scenario where a small number of positive labeled examples
    defines the community. The solution introduces an unsupervised proxy task for
    learning user embeddings: user re-identification. Experiments with 16 different
    communities show that the resulting embeddings are more effective for community
    membership identification than common unsupervised representations.'
  address: New Orleans, Louisiana
  author:
  - first: Aaron
    full: Aaron Jaech
    id: aaron-jaech
    last: Jaech
  - first: Shobhit
    full: Shobhit Hathi
    id: shobhit-hathi
    last: Hathi
  - first: Mari
    full: Mari Ostendorf
    id: mari-ostendorf
    last: Ostendorf
  author_string: Aaron Jaech, Shobhit Hathi, Mari Ostendorf
  bibkey: jaech-etal-2018-community
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2094
  month: June
  page_first: '595'
  page_last: '601'
  pages: "595\u2013601"
  paper_id: '94'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2094.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2094.jpg
  title: Community Member Retrieval on Social Media Using Textual Information
  title_html: Community Member Retrieval on Social Media Using Textual Information
  url: https://www.aclweb.org/anthology/N18-2094
  year: '2018'
N18-2095:
  abstract: "With the growing amount of reviews in e-commerce websites, it is critical\
    \ to assess the helpfulness of reviews and recommend them accordingly to consumers.\
    \ Recent studies on review helpfulness require plenty of labeled samples for each\
    \ domain/category of interests. However, such an approach based on close-world\
    \ assumption is not always practical, especially for domains with limited reviews\
    \ or the \u201Cout-of-vocabulary\u201D problem. Therefore, we propose a convolutional\
    \ neural network (CNN) based model which leverages both word-level and character-based\
    \ representations. To transfer knowledge between domains, we further extend our\
    \ model to jointly model different domains with auxiliary domain discriminators.\
    \ On the Amazon product review dataset, our approach significantly outperforms\
    \ the state of the art in terms of both accuracy and cross-domain robustness."
  address: New Orleans, Louisiana
  author:
  - first: Cen
    full: Cen Chen
    id: cen-chen
    last: Chen
  - first: Yinfei
    full: Yinfei Yang
    id: yinfei-yang
    last: Yang
  - first: Jun
    full: Jun Zhou
    id: jun-zhou
    last: Zhou
  - first: Xiaolong
    full: Xiaolong Li
    id: xiaolong-li
    last: Li
  - first: Forrest Sheng
    full: Forrest Sheng Bao
    id: forrest-bao
    last: Bao
  author_string: Cen Chen, Yinfei Yang, Jun Zhou, Xiaolong Li, Forrest Sheng Bao
  bibkey: chen-etal-2018-cross
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2095
  month: June
  page_first: '602'
  page_last: '607'
  pages: "602\u2013607"
  paper_id: '95'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2095.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2095.jpg
  title: Cross-Domain Review Helpfulness Prediction Based on Convolutional Neural
    Networks with Auxiliary Domain Discriminators
  title_html: Cross-Domain Review Helpfulness Prediction Based on Convolutional Neural
    Networks with Auxiliary Domain Discriminators
  url: https://www.aclweb.org/anthology/N18-2095
  year: '2018'
N18-2096:
  abstract: "Social media is known for its multi-cultural and multilingual interactions,\
    \ a natural product of which is code-mixing. Multilingual speakers mix languages\
    \ they tweet to address a different audience, express certain feelings, or attract\
    \ attention. This paper presents a large-scale analysis of 6 million tweets produced\
    \ by 27 thousand multilingual users speaking 12 other languages besides English.\
    \ We rely on this corpus to build predictive models to infer non-English languages\
    \ that users speak exclusively from their English tweets. Unlike native language\
    \ identification task, we rely on large amounts of informal social media communications\
    \ rather than ESL essays. We contrast the predictive power of the state-of-the-art\
    \ machine learning models trained on lexical, syntactic, and stylistic signals\
    \ with neural network models learned from word, character and byte representations\
    \ extracted from English only tweets. We report that content, style and syntax\
    \ are the most predictive of non-English languages that users speak on Twitter.\
    \ Neural network models learned from byte representations of user content combined\
    \ with transfer learning yield the best performance. Finally, by analyzing cross-lingual\
    \ transfer \u2013 the influence of non-English languages on various levels of\
    \ linguistic performance in English, we present novel findings on stylistic and\
    \ syntactic variations across speakers of 12 languages in social media."
  address: New Orleans, Louisiana
  author:
  - first: Svitlana
    full: Svitlana Volkova
    id: svitlana-volkova
    last: Volkova
  - first: Stephen
    full: Stephen Ranshous
    id: stephen-ranshous
    last: Ranshous
  - first: Lawrence
    full: Lawrence Phillips
    id: lawrence-phillips
    last: Phillips
  author_string: Svitlana Volkova, Stephen Ranshous, Lawrence Phillips
  bibkey: volkova-etal-2018-predicting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2096
  month: June
  page_first: '608'
  page_last: '614'
  pages: "608\u2013614"
  paper_id: '96'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2096.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2096.jpg
  title: Predicting Foreign Language Usage from English-Only Social Media Posts
  title_html: Predicting Foreign Language Usage from <span class="acl-fixed-case">E</span>nglish-Only
    Social Media Posts
  url: https://www.aclweb.org/anthology/N18-2096
  year: '2018'
N18-2097:
  abstract: Neural abstractive summarization models have led to promising results
    in summarizing relatively short documents. We propose the first model for abstractive
    summarization of single, longer-form documents (e.g., research papers). Our approach
    consists of a new hierarchical encoder that models the discourse structure of
    a document, and an attentive discourse-aware decoder to generate the summary.
    Empirical results on two large-scale datasets of scientific papers show that our
    model significantly outperforms state-of-the-art models.
  address: New Orleans, Louisiana
  author:
  - first: Arman
    full: Arman Cohan
    id: arman-cohan
    last: Cohan
  - first: Franck
    full: Franck Dernoncourt
    id: franck-dernoncourt
    last: Dernoncourt
  - first: Doo Soon
    full: Doo Soon Kim
    id: doo-soon-kim
    last: Kim
  - first: Trung
    full: Trung Bui
    id: trung-bui
    last: Bui
  - first: Seokhwan
    full: Seokhwan Kim
    id: seokhwan-kim
    last: Kim
  - first: Walter
    full: Walter Chang
    id: walter-chang
    last: Chang
  - first: Nazli
    full: Nazli Goharian
    id: nazli-goharian
    last: Goharian
  author_string: Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan
    Kim, Walter Chang, Nazli Goharian
  bibkey: cohan-etal-2018-discourse
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2097
  month: June
  page_first: '615'
  page_last: '621'
  pages: "615\u2013621"
  paper_id: '97'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2097.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2097.jpg
  title: A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents
  title_html: A Discourse-Aware Attention Model for Abstractive Summarization of Long
    Documents
  url: https://www.aclweb.org/anthology/N18-2097
  year: '2018'
N18-2098:
  abstract: Structured data summarization involves generation of natural language
    summaries from structured input data. In this work, we consider summarizing structured
    data occurring in the form of tables as they are prevalent across a wide variety
    of domains. We formulate the standard table summarization problem, which deals
    with tables conforming to a single predefined schema. To this end, we propose
    a mixed hierarchical attention based encoder-decoder model which is able to leverage
    the structure in addition to the content of the tables. Our experiments on the
    publicly available weathergov dataset show around 18 BLEU (around 30%) improvement
    over the current state-of-the-art.
  address: New Orleans, Louisiana
  author:
  - first: Parag
    full: Parag Jain
    id: parag-jain
    last: Jain
  - first: Anirban
    full: Anirban Laha
    id: anirban-laha
    last: Laha
  - first: Karthik
    full: Karthik Sankaranarayanan
    id: karthik-sankaranarayanan
    last: Sankaranarayanan
  - first: Preksha
    full: Preksha Nema
    id: preksha-nema
    last: Nema
  - first: Mitesh M.
    full: Mitesh M. Khapra
    id: mitesh-m-khapra
    last: Khapra
  - first: Shreyas
    full: Shreyas Shetty
    id: shreyas-shetty
    last: Shetty
  author_string: Parag Jain, Anirban Laha, Karthik Sankaranarayanan, Preksha Nema,
    Mitesh M. Khapra, Shreyas Shetty
  bibkey: jain-etal-2018-mixed
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2098
  month: June
  page_first: '622'
  page_last: '627'
  pages: "622\u2013627"
  paper_id: '98'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2098.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2098.jpg
  title: A Mixed Hierarchical Attention Based Encoder-Decoder Approach for Standard
    Table Summarization
  title_html: A Mixed Hierarchical Attention Based Encoder-Decoder Approach for Standard
    Table Summarization
  url: https://www.aclweb.org/anthology/N18-2098
  year: '2018'
N18-2099:
  abstract: Most summarization research focuses on summarizing the entire given text,
    but in practice readers are often interested in only one aspect of the document
    or conversation. We propose targeted summarization as an umbrella category for
    summarization tasks that intentionally consider only parts of the input data.
    This covers query-based summarization, update summarization, and a new task we
    propose where the goal is to summarize a particular aspect of a document. However,
    collecting data for this new task is hard because directly asking annotators (e.g.,
    crowd workers) to write summaries leads to data with low accuracy when there are
    a large number of facts to include. We introduce a novel crowdsourcing workflow,
    Pin-Refine, that allows us to collect high-quality summaries for our task, a necessary
    step for the development of automatic systems.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-2099.Datasets.zip
    type: dataset
    url: https://www.aclweb.org/anthology/attachments/N18-2099.Datasets.zip
  author:
  - first: Youxuan
    full: Youxuan Jiang
    id: youxuan-jiang
    last: Jiang
  - first: Catherine
    full: Catherine Finegan-Dollak
    id: catherine-finegan-dollak
    last: Finegan-Dollak
  - first: Jonathan K.
    full: Jonathan K. Kummerfeld
    id: jonathan-k-kummerfeld
    last: Kummerfeld
  - first: Walter
    full: Walter Lasecki
    id: walter-lasecki
    last: Lasecki
  author_string: Youxuan Jiang, Catherine Finegan-Dollak, Jonathan K. Kummerfeld,
    Walter Lasecki
  bibkey: jiang-etal-2018-effective
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2099
  month: June
  page_first: '628'
  page_last: '633'
  pages: "628\u2013633"
  paper_id: '99'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2099.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2099.jpg
  title: Effective Crowdsourcing for a New Type of Summarization Task
  title_html: Effective Crowdsourcing for a New Type of Summarization Task
  url: https://www.aclweb.org/anthology/N18-2099
  year: '2018'
N18-2100:
  abstract: Keyphrase extraction is a fundamental task in natural language processing
    that facilitates mapping of documents to a set of representative phrases. In this
    paper, we present an unsupervised technique (Key2Vec) that leverages phrase embeddings
    for ranking keyphrases extracted from scientific articles. Specifically, we propose
    an effective way of processing text documents for training multi-word phrase embeddings
    that are used for thematic representation of scientific articles and ranking of
    keyphrases extracted from them using theme-weighted PageRank. Evaluations are
    performed on benchmark datasets producing state-of-the-art results.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-2100.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-2100.Notes.pdf
  author:
  - first: Debanjan
    full: Debanjan Mahata
    id: debanjan-mahata
    last: Mahata
  - first: John
    full: John Kuriakose
    id: john-kuriakose
    last: Kuriakose
  - first: Rajiv Ratn
    full: Rajiv Ratn Shah
    id: rajiv-shah
    last: Shah
  - first: Roger
    full: Roger Zimmermann
    id: roger-zimmermann
    last: Zimmermann
  author_string: Debanjan Mahata, John Kuriakose, Rajiv Ratn Shah, Roger Zimmermann
  bibkey: mahata-etal-2018-key2vec
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2100
  month: June
  page_first: '634'
  page_last: '639'
  pages: "634\u2013639"
  paper_id: '100'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2100.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2100.jpg
  title: 'Key2Vec: Automatic Ranked Keyphrase Extraction from Scientific Articles
    using Phrase Embeddings'
  title_html: '<span class="acl-fixed-case">K</span>ey2<span class="acl-fixed-case">V</span>ec:
    Automatic Ranked Keyphrase Extraction from Scientific Articles using Phrase Embeddings'
  url: https://www.aclweb.org/anthology/N18-2100
  year: '2018'
N18-2101:
  abstract: 'While Wikipedia exists in 287 languages, its content is unevenly distributed
    among them. In this work, we investigate the generation of open domain Wikipedia
    summaries in underserved languages using structured data from Wikidata. To this
    end, we propose a neural network architecture equipped with copy actions that
    learns to generate single-sentence and comprehensible textual summaries from Wikidata
    triples. We demonstrate the effectiveness of the proposed approach by evaluating
    it against a set of baselines on two languages of different natures: Arabic, a
    morphological rich language with a larger vocabulary than English, and Esperanto,
    a constructed language known for its easy acquisition.'
  address: New Orleans, Louisiana
  author:
  - first: "Lucie-Aim\xE9e"
    full: "Lucie-Aim\xE9e Kaffee"
    id: lucie-aimee-kaffee
    last: Kaffee
  - first: Hady
    full: Hady Elsahar
    id: hady-elsahar
    last: Elsahar
  - first: Pavlos
    full: Pavlos Vougiouklis
    id: pavlos-vougiouklis
    last: Vougiouklis
  - first: Christophe
    full: Christophe Gravier
    id: christophe-gravier
    last: Gravier
  - first: "Fr\xE9d\xE9rique"
    full: "Fr\xE9d\xE9rique Laforest"
    id: frederique-laforest
    last: Laforest
  - first: Jonathon
    full: Jonathon Hare
    id: jonathon-hare
    last: Hare
  - first: Elena
    full: Elena Simperl
    id: elena-simperl
    last: Simperl
  author_string: "Lucie-Aim\xE9e Kaffee, Hady Elsahar, Pavlos Vougiouklis, Christophe\
    \ Gravier, Fr\xE9d\xE9rique Laforest, Jonathon Hare, Elena Simperl"
  bibkey: kaffee-etal-2018-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2101
  month: June
  page_first: '640'
  page_last: '645'
  pages: "640\u2013645"
  paper_id: '101'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2101.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2101.jpg
  title: Learning to Generate Wikipedia Summaries for Underserved Languages from Wikidata
  title_html: Learning to Generate <span class="acl-fixed-case">W</span>ikipedia Summaries
    for Underserved Languages from <span class="acl-fixed-case">W</span>ikidata
  url: https://www.aclweb.org/anthology/N18-2101
  year: '2018'
N18-2102:
  abstract: 'Abstractive text summarization is the task of compressing and rewriting
    a long document into a short summary while maintaining saliency, directed logical
    entailment, and non-redundancy. In this work, we address these three important
    aspects of a good summary via a reinforcement learning approach with two novel
    reward functions: ROUGESal and Entail, on top of a coverage-based baseline. The
    ROUGESal reward modifies the ROUGE metric by up-weighting the salient phrases/words
    detected via a keyphrase classifier. The Entail reward gives high (length-normalized)
    scores to logically-entailed summaries using an entailment classifier. Further,
    we show superior performance improvement when these rewards are combined with
    traditional metric (ROUGE) based rewards, via our novel and effective multi-reward
    approach of optimizing multiple rewards simultaneously in alternate mini-batches.
    Our method achieves the new state-of-the-art results on CNN/Daily Mail dataset
    as well as strong improvements in a test-only transfer setup on DUC-2002.'
  address: New Orleans, Louisiana
  author:
  - first: Ramakanth
    full: Ramakanth Pasunuru
    id: ramakanth-pasunuru
    last: Pasunuru
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  author_string: Ramakanth Pasunuru, Mohit Bansal
  bibkey: pasunuru-bansal-2018-multi
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2102
  month: June
  page_first: '646'
  page_last: '653'
  pages: "646\u2013653"
  paper_id: '102'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2102.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2102.jpg
  title: Multi-Reward Reinforced Summarization with Saliency and Entailment
  title_html: Multi-Reward Reinforced Summarization with Saliency and Entailment
  url: https://www.aclweb.org/anthology/N18-2102
  year: '2018'
N18-2103:
  abstract: "Supervised summarization systems usually rely on supervision at the sentence\
    \ or n-gram level provided by automatic metrics like ROUGE, which act as noisy\
    \ proxies for human judgments. In this work, we learn a summary-level scoring\
    \ function \U0001D703 including human judgments as supervision and automatically\
    \ generated data as regularization. We extract summaries with a genetic algorithm\
    \ using including human judgments as supervision and automatically generated data\
    \ as regularization. We extract summaries with a genetic algorithm using \U0001D703\
    \ as a fitness function. We observe strong and promising performances across datasets\
    \ in both automatic and manual evaluation. as a fitness function. We observe strong\
    \ and promising performances across datasets in both automatic and manual evaluation."
  address: New Orleans, Louisiana
  author:
  - first: Maxime
    full: Maxime Peyrard
    id: maxime-peyrard
    last: Peyrard
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Maxime Peyrard, Iryna Gurevych
  bibkey: peyrard-gurevych-2018-objective
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2103
  month: June
  page_first: '654'
  page_last: '660'
  pages: "654\u2013660"
  paper_id: '103'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2103.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2103.jpg
  title: Objective Function Learning to Match Human Judgements for Optimization-Based
    Summarization
  title_html: Objective Function Learning to Match Human Judgements for Optimization-Based
    Summarization
  url: https://www.aclweb.org/anthology/N18-2103
  year: '2018'
N18-2104:
  abstract: We propose a simple but highly effective automatic evaluation measure
    of summarization, pruned Basic Elements (pBE). Although the BE concept is widely
    used for the automated evaluation of summaries, its weakness is that it redundantly
    matches basic elements. To avoid this redundancy, pBE prunes basic elements by
    (1) disregarding frequency count of basic elements and (2) reducing semantically
    overlapped basic elements based on word similarity. Even though it is simple,
    pBE outperforms ROUGE in DUC datasets in most cases and achieves the highest rank
    correlation coefficient in TAC 2011 AESOP task.
  address: New Orleans, Louisiana
  author:
  - first: Ukyo
    full: Ukyo Honda
    id: ukyo-honda
    last: Honda
  - first: Tsutomu
    full: Tsutomu Hirao
    id: tsutomu-hirao
    last: Hirao
  - first: Masaaki
    full: Masaaki Nagata
    id: masaaki-nagata
    last: Nagata
  author_string: Ukyo Honda, Tsutomu Hirao, Masaaki Nagata
  bibkey: honda-etal-2018-pruning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2104
  month: June
  page_first: '661'
  page_last: '666'
  pages: "661\u2013666"
  paper_id: '104'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2104.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2104.jpg
  title: Pruning Basic Elements for Better Automatic Evaluation of Summaries
  title_html: Pruning Basic Elements for Better Automatic Evaluation of Summaries
  url: https://www.aclweb.org/anthology/N18-2104
  year: '2018'
N18-2105:
  abstract: We propose an unsupervised keyphrase extraction model that encodes topical
    information within a multipartite graph structure. Our model represents keyphrase
    candidates and topics in a single graph and exploits their mutually reinforcing
    relationship to improve candidate ranking. We further introduce a novel mechanism
    to incorporate keyphrase selection preferences into the model. Experiments conducted
    on three widely used datasets show significant improvements over state-of-the-art
    graph-based models.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-2105.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-2105.Notes.pdf
  author:
  - first: Florian
    full: Florian Boudin
    id: florian-boudin
    last: Boudin
  author_string: Florian Boudin
  bibkey: boudin-2018-unsupervised
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2105
  month: June
  page_first: '667'
  page_last: '672'
  pages: "667\u2013672"
  paper_id: '105'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2105.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2105.jpg
  title: Unsupervised Keyphrase Extraction with Multipartite Graphs
  title_html: Unsupervised Keyphrase Extraction with Multipartite Graphs
  url: https://www.aclweb.org/anthology/N18-2105
  year: '2018'
N18-2106:
  abstract: 'People can identify correspondences between narratives in everyday life.
    For example, an analogy with the Cinderella story may be made in describing the
    unexpected success of an underdog in seemingly different stories. We present a
    new task and dataset for story understanding: identifying instances of similar
    narratives from a collection of narrative texts. We present an initial approach
    for this problem, which finds correspondences between narratives in terms of plot
    events, and resemblances between characters and their social relationships. Our
    approach yields an 8% absolute improvement in performance over a competitive information-retrieval
    baseline on a novel dataset of plot summaries of 577 movie remakes from Wikipedia.'
  address: New Orleans, Louisiana
  author:
  - first: Snigdha
    full: Snigdha Chaturvedi
    id: snigdha-chaturvedi
    last: Chaturvedi
  - first: Shashank
    full: Shashank Srivastava
    id: shashank-srivastava
    last: Srivastava
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Snigdha Chaturvedi, Shashank Srivastava, Dan Roth
  bibkey: chaturvedi-etal-2018-heard
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2106
  month: June
  page_first: '673'
  page_last: '678'
  pages: "673\u2013678"
  paper_id: '106'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2106.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2106.jpg
  title: Where Have I Heard This Story Before? Identifying Narrative Similarity in
    Movie Remakes
  title_html: Where Have <span class="acl-fixed-case">I</span> Heard This Story Before?
    Identifying Narrative Similarity in Movie Remakes
  url: https://www.aclweb.org/anthology/N18-2106
  year: '2018'
N18-2107:
  abstract: Emojis are small images that are commonly included in social media text
    messages. The combination of visual and textual content in the same message builds
    up a modern way of communication, that automatic systems are not used to deal
    with. In this paper we extend recent advances in emoji prediction by putting forward
    a multimodal approach that is able to predict emojis in Instagram posts. Instagram
    posts are composed of pictures together with texts which sometimes include emojis.
    We show that these emojis can be predicted by using the text, but also using the
    picture. Our main finding is that incorporating the two synergistic modalities,
    in a combined model, improves accuracy in an emoji prediction task. This result
    demonstrates that these two modalities (text and images) encode different information
    on the use of emojis and therefore can complement each other.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/277671532
    type: video
    url: http://vimeo.com/277671532
  author:
  - first: Francesco
    full: Francesco Barbieri
    id: francesco-barbieri
    last: Barbieri
  - first: Miguel
    full: Miguel Ballesteros
    id: miguel-ballesteros
    last: Ballesteros
  - first: Francesco
    full: Francesco Ronzano
    id: francesco-ronzano
    last: Ronzano
  - first: Horacio
    full: Horacio Saggion
    id: horacio-saggion
    last: Saggion
  author_string: Francesco Barbieri, Miguel Ballesteros, Francesco Ronzano, Horacio
    Saggion
  bibkey: barbieri-etal-2018-multimodal
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2107
  month: June
  page_first: '679'
  page_last: '686'
  pages: "679\u2013686"
  paper_id: '107'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2107.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2107.jpg
  title: Multimodal Emoji Prediction
  title_html: Multimodal Emoji Prediction
  url: https://www.aclweb.org/anthology/N18-2107
  year: '2018'
N18-2108:
  abstract: We introduce a fully-differentiable approximation to higher-order inference
    for coreference resolution. Our approach uses the antecedent distribution from
    a span-ranking architecture as an attention mechanism to iteratively refine span
    representations. This enables the model to softly consider multiple hops in the
    predicted clusters. To alleviate the computational cost of this iterative process,
    we introduce a coarse-to-fine approach that incorporates a less accurate but more
    efficient bilinear factor, enabling more aggressive pruning without hurting accuracy.
    Compared to the existing state-of-the-art span-ranking approach, our model significantly
    improves accuracy on the English OntoNotes benchmark, while being far more computationally
    efficient.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/277672817
    type: video
    url: http://vimeo.com/277672817
  author:
  - first: Kenton
    full: Kenton Lee
    id: kenton-lee
    last: Lee
  - first: Luheng
    full: Luheng He
    id: luheng-he
    last: He
  - first: Luke
    full: Luke Zettlemoyer
    id: luke-zettlemoyer
    last: Zettlemoyer
  author_string: Kenton Lee, Luheng He, Luke Zettlemoyer
  bibkey: lee-etal-2018-higher
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2108
  month: June
  page_first: '687'
  page_last: '692'
  pages: "687\u2013692"
  paper_id: '108'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2108.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2108.jpg
  title: Higher-Order Coreference Resolution with Coarse-to-Fine Inference
  title_html: Higher-Order Coreference Resolution with Coarse-to-Fine Inference
  url: https://www.aclweb.org/anthology/N18-2108
  year: '2018'
N18-2109:
  abstract: We present a novel transition system, based on the Covington non-projective
    parser, introducing non-local transitions that can directly create arcs involving
    nodes to the left of the current focus positions. This avoids the need for long
    sequences of No-Arcs transitions to create long-distance arcs, thus alleviating
    error propagation. The resulting parser outperforms the original version and achieves
    the best accuracy on the Stanford Dependencies conversion of the Penn Treebank
    among greedy transition-based parsers.
  address: New Orleans, Louisiana
  attachment:
  - filename: http://vimeo.com/277673868
    type: video
    url: http://vimeo.com/277673868
  author:
  - first: Daniel
    full: "Daniel Fern\xE1ndez-Gonz\xE1lez"
    id: daniel-fernandez-gonzalez
    last: "Fern\xE1ndez-Gonz\xE1lez"
  - first: Carlos
    full: "Carlos G\xF3mez-Rodr\xEDguez"
    id: carlos-gomez-rodriguez
    last: "G\xF3mez-Rodr\xEDguez"
  author_string: "Daniel Fern\xE1ndez-Gonz\xE1lez, Carlos G\xF3mez-Rodr\xEDguez"
  bibkey: fernandez-gonzalez-gomez-rodriguez-2018-non
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2109
  month: June
  page_first: '693'
  page_last: '700'
  pages: "693\u2013700"
  paper_id: '109'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2109.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2109.jpg
  title: Non-Projective Dependency Parsing with Non-Local Transitions
  title_html: Non-Projective Dependency Parsing with Non-Local Transitions
  url: https://www.aclweb.org/anthology/N18-2109
  year: '2018'
N18-2110:
  abstract: "Alzheimer\u2019s disease (AD) is an irreversible and progressive brain\
    \ disease that can be stopped or slowed down with medical treatment. Language\
    \ changes serve as a sign that a patient\u2019s cognitive functions have been\
    \ impacted, potentially leading to early diagnosis. In this work, we use NLP techniques\
    \ to classify and analyze the linguistic characteristics of AD patients using\
    \ the DementiaBank dataset. We apply three neural models based on CNNs, LSTM-RNNs,\
    \ and their combination, to distinguish between language samples from AD and control\
    \ patients. We achieve a new independent benchmark accuracy for the AD classification\
    \ task. More importantly, we next interpret what these neural models have learned\
    \ about the linguistic characteristics of AD patients, via analysis based on activation\
    \ clustering and first-derivative saliency techniques. We then perform novel automatic\
    \ pattern discovery inside activation clusters, and consolidate AD patients\u2019\
    \ distinctive grammar patterns. Additionally, we show that first derivative saliency\
    \ can not only rediscover previous language patterns of AD patients, but also\
    \ shed light on the limitations of neural models. Lastly, we also include analysis\
    \ of gender-separated AD data."
  address: New Orleans, Louisiana
  author:
  - first: Sweta
    full: Sweta Karlekar
    id: sweta-karlekar
    last: Karlekar
  - first: Tong
    full: Tong Niu
    id: tong-niu
    last: Niu
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  author_string: Sweta Karlekar, Tong Niu, Mohit Bansal
  bibkey: karlekar-etal-2018-detecting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2110
  month: June
  page_first: '701'
  page_last: '707'
  pages: "701\u2013707"
  paper_id: '110'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2110.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2110.jpg
  title: "Detecting Linguistic Characteristics of Alzheimer\u2019s Dementia by Interpreting\
    \ Neural Models"
  title_html: "Detecting Linguistic Characteristics of <span class=\"acl-fixed-case\"\
    >A</span>lzheimer\u2019s Dementia by Interpreting Neural Models"
  url: https://www.aclweb.org/anthology/N18-2110
  year: '2018'
N18-2111:
  abstract: An essential aspect to understanding narratives is to grasp the interaction
    between characters in a story and the actions they take. We examine whether computational
    models can capture this interaction, when both character attributes and actions
    are expressed as complex natural language descriptions. We propose role-playing
    games as a testbed for this problem, and introduce a large corpus of game transcripts
    collected from online discussion forums. Using neural language models which combine
    character and action descriptions from these stories, we show that we can learn
    the latent ties. Action sequences are better predicted when the character performing
    the action is also taken into account, and vice versa for character attributes.
  address: New Orleans, Louisiana
  author:
  - first: Annie
    full: Annie Louis
    id: annie-louis
    last: Louis
  - first: Charles
    full: Charles Sutton
    id: charles-sutton
    last: Sutton
  author_string: Annie Louis, Charles Sutton
  bibkey: louis-sutton-2018-deep
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2111
  month: June
  page_first: '708'
  page_last: '713'
  pages: "708\u2013713"
  paper_id: '111'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2111.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2111.jpg
  title: 'Deep Dungeons and Dragons: Learning Character-Action Interactions from Role-Playing
    Game Transcripts'
  title_html: 'Deep Dungeons and Dragons: Learning Character-Action Interactions from
    Role-Playing Game Transcripts'
  url: https://www.aclweb.org/anthology/N18-2111
  year: '2018'
N18-2112:
  abstract: Reinforcement learning (RL) is a promising approach to solve dialogue
    policy optimisation. Traditional RL algorithms, however, fail to scale to large
    domains due to the curse of dimensionality. We propose a novel Dialogue Management
    architecture, based on Feudal RL, which decomposes the decision into two steps;
    a first step where a master policy selects a subset of primitive actions, and
    a second step where a primitive action is chosen from the selected subset. The
    structural information included in the domain ontology is used to abstract the
    dialogue state space, taking the decisions at each step using different parts
    of the abstracted state. This, combined with an information sharing mechanism
    between slots, increases the scalability to large domains. We show that an implementation
    of this approach, based on Deep-Q Networks, significantly outperforms previous
    state of the art in several dialogue domains and environments, without the need
    of any additional reward signal.
  address: New Orleans, Louisiana
  author:
  - first: "I\xF1igo"
    full: "I\xF1igo Casanueva"
    id: inigo-casanueva
    last: Casanueva
  - first: "Pawe\u0142"
    full: "Pawe\u0142 Budzianowski"
    id: pawel-budzianowski
    last: Budzianowski
  - first: Pei-Hao
    full: Pei-Hao Su
    id: pei-hao-su
    last: Su
  - first: Stefan
    full: Stefan Ultes
    id: stefan-ultes
    last: Ultes
  - first: Lina M.
    full: Lina M. Rojas-Barahona
    id: lina-m-rojas-barahona
    last: Rojas-Barahona
  - first: Bo-Hsiang
    full: Bo-Hsiang Tseng
    id: bo-hsiang-tseng
    last: Tseng
  - first: Milica
    full: "Milica Ga\u0161i\u0107"
    id: milica-gasic
    last: "Ga\u0161i\u0107"
  author_string: "I\xF1igo Casanueva, Pawe\u0142 Budzianowski, Pei-Hao Su, Stefan\
    \ Ultes, Lina M. Rojas-Barahona, Bo-Hsiang Tseng, Milica Ga\u0161i\u0107"
  bibkey: casanueva-etal-2018-feudal
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2112
  month: June
  page_first: '714'
  page_last: '719'
  pages: "714\u2013719"
  paper_id: '112'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2112.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2112.jpg
  title: Feudal Reinforcement Learning for Dialogue Management in Large Domains
  title_html: Feudal Reinforcement Learning for Dialogue Management in Large Domains
  url: https://www.aclweb.org/anthology/N18-2112
  year: '2018'
N18-2113:
  abstract: "We highlight several issues in the evaluation of historical text normalization\
    \ systems that make it hard to tell how well these systems would actually work\
    \ in practice\u2014i.e., for new datasets or languages; in comparison to more\
    \ na\xEFve systems; or as a preprocessing step for downstream NLP tools. We illustrate\
    \ these issues and exemplify our proposed evaluation practices by comparing two\
    \ neural models against a na\xEFve baseline system. We show that the neural models\
    \ generalize well to unseen words in tests on five languages; nevertheless, they\
    \ provide no clear benefit over the na\xEFve baseline for downstream POS tagging\
    \ of an English historical collection. We conclude that future work should include\
    \ more rigorous evaluation, including both intrinsic and extrinsic measures where\
    \ possible."
  address: New Orleans, Louisiana
  author:
  - first: Alexander
    full: Alexander Robertson
    id: alexander-robertson
    last: Robertson
  - first: Sharon
    full: Sharon Goldwater
    id: sharon-goldwater
    last: Goldwater
  author_string: Alexander Robertson, Sharon Goldwater
  bibkey: robertson-goldwater-2018-evaluating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2113
  month: June
  page_first: '720'
  page_last: '725'
  pages: "720\u2013725"
  paper_id: '113'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2113.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2113.jpg
  title: 'Evaluating Historical Text Normalization Systems: How Well Do They Generalize?'
  title_html: 'Evaluating Historical Text Normalization Systems: How Well Do They
    Generalize?'
  url: https://www.aclweb.org/anthology/N18-2113
  year: '2018'
N18-2114:
  abstract: Multi-task learning with Convolutional Neural Network (CNN) has shown
    great success in many Natural Language Processing (NLP) tasks. This success can
    be largely attributed to the feature sharing by fusing some layers among tasks.
    However, most existing approaches just fully or proportionally share the features
    without distinguishing the helpfulness of them. By that the network would be confused
    by the helpless even harmful features, generating undesired interference between
    tasks. In this paper, we introduce gate mechanism into multi-task CNN and propose
    a new Gated Sharing Unit, which can filter the feature flows between tasks and
    greatly reduce the interference. Experiments on 9 text classification datasets
    shows that our approach can learn selection rules automatically and gain a great
    improvement over strong baselines.
  address: New Orleans, Louisiana
  author:
  - first: Liqiang
    full: Liqiang Xiao
    id: liqiang-xiao
    last: Xiao
  - first: Honglun
    full: Honglun Zhang
    id: honglun-zhang
    last: Zhang
  - first: Wenqing
    full: Wenqing Chen
    id: wenqing-chen
    last: Chen
  author_string: Liqiang Xiao, Honglun Zhang, Wenqing Chen
  bibkey: xiao-etal-2018-gated
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2114
  month: June
  page_first: '726'
  page_last: '731'
  pages: "726\u2013731"
  paper_id: '114'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2114.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2114.jpg
  title: Gated Multi-Task Network for Text Classification
  title_html: Gated Multi-Task Network for Text Classification
  url: https://www.aclweb.org/anthology/N18-2114
  year: '2018'
N18-2115:
  abstract: "In conventional supervised training, a model is trained to fit all the\
    \ training examples. However, having a monolithic model may not always be the\
    \ best strategy, as examples could vary widely. In this work, we explore a different\
    \ learning protocol that treats each example as a unique pseudo-task, by reducing\
    \ the original learning problem to a few-shot meta-learning scenario with the\
    \ help of a domain-dependent relevance function. When evaluated on the WikiSQL\
    \ dataset, our approach leads to faster convergence and achieves 1.1%\u20135.4%\
    \ absolute accuracy gains over the non-meta-learning counterparts."
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-2115.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-2115.Notes.pdf
  author:
  - first: Po-Sen
    full: Po-Sen Huang
    id: po-sen-huang
    last: Huang
  - first: Chenglong
    full: Chenglong Wang
    id: chenglong-wang
    last: Wang
  - first: Rishabh
    full: Rishabh Singh
    id: rishabh-singh
    last: Singh
  - first: Wen-tau
    full: Wen-tau Yih
    id: wen-tau-yih
    last: Yih
  - first: Xiaodong
    full: Xiaodong He
    id: xiaodong-he
    last: He
  author_string: Po-Sen Huang, Chenglong Wang, Rishabh Singh, Wen-tau Yih, Xiaodong
    He
  bibkey: huang-etal-2018-natural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2115
  month: June
  page_first: '732'
  page_last: '738'
  pages: "732\u2013738"
  paper_id: '115'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2115.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2115.jpg
  title: Natural Language to Structured Query Generation via Meta-Learning
  title_html: Natural Language to Structured Query Generation via Meta-Learning
  url: https://www.aclweb.org/anthology/N18-2115
  year: '2018'
N18-2116:
  abstract: Word embedding parameters often dominate overall model sizes in neural
    methods for natural language processing. We reduce deployed model sizes of text
    classifiers by learning a hard word clustering in an end-to-end manner. We use
    the Gumbel-Softmax distribution to maximize over the latent clustering while minimizing
    the task loss. We propose variations that selectively assign additional parameters
    to words, which further improves accuracy while still remaining parameter-efficient.
  address: New Orleans, Louisiana
  author:
  - first: Mingda
    full: Mingda Chen
    id: mingda-chen
    last: Chen
  - first: Kevin
    full: Kevin Gimpel
    id: kevin-gimpel
    last: Gimpel
  author_string: Mingda Chen, Kevin Gimpel
  bibkey: chen-gimpel-2018-smaller
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2116
  month: June
  page_first: '739'
  page_last: '745'
  pages: "739\u2013745"
  paper_id: '116'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2116.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2116.jpg
  title: Smaller Text Classifiers with Discriminative Cluster Embeddings
  title_html: Smaller Text Classifiers with Discriminative Cluster Embeddings
  url: https://www.aclweb.org/anthology/N18-2116
  year: '2018'
N18-2117:
  abstract: "Neuropsychological examinations are an important screening tool for the\
    \ presence of cognitive conditions (e.g. Alzheimer\u2019s, Parkinson\u2019s Disease),\
    \ and require a trained tester to conduct the exam through spoken interactions\
    \ with the subject. While audio is relatively easy to record, it remains a challenge\
    \ to automatically diarize (who spoke when?), decode (what did they say?), and\
    \ assess a subject\u2019s cognitive health. This paper demonstrates a method to\
    \ determine the cognitive health (impaired or not) of 92 subjects, from audio\
    \ that was diarized using an automatic speech recognition system trained on TED\
    \ talks and on the structured language used by testers and subjects. Using leave-one-out\
    \ cross validation and logistic regression modeling we show that even with noisily\
    \ decoded data (81% WER) we can still perform accurate enough diarization (0.02%\
    \ confusion rate) to determine the cognitive state of a subject (0.76 AUC)."
  address: New Orleans, Louisiana
  author:
  - first: Tuka
    full: Tuka Al Hanai
    id: tuka-al-hanai
    last: Al Hanai
  - first: Rhoda
    full: Rhoda Au
    id: rhoda-au
    last: Au
  - first: James
    full: James Glass
    id: james-glass
    last: Glass
  author_string: Tuka Al Hanai, Rhoda Au, James Glass
  bibkey: al-hanai-etal-2018-role
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2117
  month: June
  page_first: '746'
  page_last: '752'
  pages: "746\u2013752"
  paper_id: '117'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2117.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2117.jpg
  title: Role-specific Language Models for Processing Recorded Neuropsychological
    Exams
  title_html: Role-specific Language Models for Processing Recorded Neuropsychological
    Exams
  url: https://www.aclweb.org/anthology/N18-2117
  year: '2018'
N18-2118:
  abstract: Attention-based recurrent neural network models for joint intent detection
    and slot filling have achieved the state-of-the-art performance, while they have
    independent attention weights. Considering that slot and intent have the strong
    relationship, this paper proposes a slot gate that focuses on learning the relationship
    between intent and slot attention vectors in order to obtain better semantic frame
    results by the global optimization. The experiments show that our proposed model
    significantly improves sentence-level semantic frame accuracy with 4.2% and 1.9%
    relative improvement compared to the attentional model on benchmark ATIS and Snips
    datasets respectively
  address: New Orleans, Louisiana
  author:
  - first: Chih-Wen
    full: Chih-Wen Goo
    id: chih-wen-goo
    last: Goo
  - first: Guang
    full: Guang Gao
    id: guang-gao
    last: Gao
  - first: Yun-Kai
    full: Yun-Kai Hsu
    id: yun-kai-hsu
    last: Hsu
  - first: Chih-Li
    full: Chih-Li Huo
    id: chih-li-huo
    last: Huo
  - first: Tsung-Chieh
    full: Tsung-Chieh Chen
    id: tsung-chieh-chen
    last: Chen
  - first: Keng-Wei
    full: Keng-Wei Hsu
    id: keng-wei-hsu
    last: Hsu
  - first: Yun-Nung
    full: Yun-Nung Chen
    id: yun-nung-chen
    last: Chen
  author_string: Chih-Wen Goo, Guang Gao, Yun-Kai Hsu, Chih-Li Huo, Tsung-Chieh Chen,
    Keng-Wei Hsu, Yun-Nung Chen
  bibkey: goo-etal-2018-slot
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2118
  month: June
  page_first: '753'
  page_last: '757'
  pages: "753\u2013757"
  paper_id: '118'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2118.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2118.jpg
  title: Slot-Gated Modeling for Joint Slot Filling and Intent Prediction
  title_html: Slot-Gated Modeling for Joint Slot Filling and Intent Prediction
  url: https://www.aclweb.org/anthology/N18-2118
  year: '2018'
N18-2119:
  abstract: Recent research in language and vision has developed models for predicting
    and disambiguating verbs from images. Here, we ask whether the predictions made
    by such models correspond to human intuitions about visual verbs. We show that
    the image regions a verb prediction model identifies as salient for a given verb
    correlate with the regions fixated by human observers performing a verb classification
    task.
  address: New Orleans, Louisiana
  author:
  - first: Spandana
    full: Spandana Gella
    id: spandana-gella
    last: Gella
  - first: Frank
    full: Frank Keller
    id: frank-keller
    last: Keller
  author_string: Spandana Gella, Frank Keller
  bibkey: gella-keller-2018-evaluation
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2119
  month: June
  page_first: '758'
  page_last: '763'
  pages: "758\u2013763"
  paper_id: '119'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2119.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2119.jpg
  title: An Evaluation of Image-Based Verb Prediction Models against Human Eye-Tracking
    Data
  title_html: An Evaluation of Image-Based Verb Prediction Models against Human Eye-Tracking
    Data
  url: https://www.aclweb.org/anthology/N18-2119
  year: '2018'
N18-2120:
  abstract: Automatic colorization is the process of adding color to greyscale images.
    We condition this process on language, allowing end users to manipulate a colorized
    image by feeding in different captions. We present two different architectures
    for language-conditioned colorization, both of which produce more accurate and
    plausible colorizations than a language-agnostic version. Furthermore, we demonstrate
    through crowdsourced experiments that we can dramatically alter colorizations
    simply by manipulating descriptive color words in captions.
  address: New Orleans, Louisiana
  author:
  - first: Varun
    full: Varun Manjunatha
    id: varun-manjunatha
    last: Manjunatha
  - first: Mohit
    full: Mohit Iyyer
    id: mohit-iyyer
    last: Iyyer
  - first: Jordan
    full: Jordan Boyd-Graber
    id: jordan-boyd-graber
    last: Boyd-Graber
  - first: Larry
    full: Larry Davis
    id: larry-davis
    last: Davis
  author_string: Varun Manjunatha, Mohit Iyyer, Jordan Boyd-Graber, Larry Davis
  bibkey: manjunatha-etal-2018-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2120
  month: June
  page_first: '764'
  page_last: '769'
  pages: "764\u2013769"
  paper_id: '120'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2120.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2120.jpg
  title: Learning to Color from Language
  title_html: Learning to Color from Language
  url: https://www.aclweb.org/anthology/N18-2120
  year: '2018'
N18-2121:
  abstract: Wit is a form of rich interaction that is often grounded in a specific
    situation (e.g., a comment in response to an event). In this work, we attempt
    to build computational models that can produce witty descriptions for a given
    image. Inspired by a cognitive account of humor appreciation, we employ linguistic
    wordplay, specifically puns, in image descriptions. We develop two approaches
    which involve retrieving witty descriptions for a given image from a large corpus
    of sentences, or generating them via an encoder-decoder neural network architecture.
    We compare our approach against meaningful baseline approaches via human studies
    and show substantial improvements. Moreover, in a Turing test style evaluation,
    people find the image descriptions generated by our model to be slightly wittier
    than human-written witty descriptions when the human is subject to similar constraints
    as the model regarding word usage and style.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-2121.Datasets.zip
    type: dataset
    url: https://www.aclweb.org/anthology/attachments/N18-2121.Datasets.zip
  author:
  - first: Arjun
    full: Arjun Chandrasekaran
    id: arjun-chandrasekaran
    last: Chandrasekaran
  - first: Devi
    full: Devi Parikh
    id: devi-parikh
    last: Parikh
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  author_string: Arjun Chandrasekaran, Devi Parikh, Mohit Bansal
  bibkey: chandrasekaran-etal-2018-punny
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2121
  month: June
  page_first: '770'
  page_last: '775'
  pages: "770\u2013775"
  paper_id: '121'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2121.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2121.jpg
  title: 'Punny Captions: Witty Wordplay in Image Descriptions'
  title_html: 'Punny Captions: Witty Wordplay in Image Descriptions'
  url: https://www.aclweb.org/anthology/N18-2121
  year: '2018'
N18-2122:
  abstract: Word vector models learn about semantics through corpora. Convolutional
    Neural Networks (CNNs) can learn about semantics through images. At the most abstract
    level, some of the information in these models must be shared, as they model the
    same real-world phenomena. Here we employ techniques previously used to detect
    semantic representations in the human brain to detect semantic representations
    in CNNs. We show the accumulation of semantic information in the layers of the
    CNN, and discover that, for misclassified images, the correct class can be recovered
    in intermediate layers of a CNN.
  address: New Orleans, Louisiana
  attachment:
  - filename: N18-2122.Notes.pdf
    type: note
    url: https://www.aclweb.org/anthology/attachments/N18-2122.Notes.pdf
  author:
  - first: Dhanush
    full: Dhanush Dharmaretnam
    id: dhanush-dharmaretnam
    last: Dharmaretnam
  - first: Alona
    full: Alona Fyshe
    id: alona-fyshe
    last: Fyshe
  author_string: Dhanush Dharmaretnam, Alona Fyshe
  bibkey: dharmaretnam-fyshe-2018-emergence
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2122
  month: June
  page_first: '776'
  page_last: '780'
  pages: "776\u2013780"
  paper_id: '122'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2122.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2122.jpg
  title: The Emergence of Semantics in Neural Network Representations of Visual Information
  title_html: The Emergence of Semantics in Neural Network Representations of Visual
    Information
  url: https://www.aclweb.org/anthology/N18-2122
  year: '2018'
N18-2123:
  abstract: "We present an empirical analysis of state-of-the-art systems for referring\
    \ expression recognition \u2013 the task of identifying the object in an image\
    \ referred to by a natural language expression \u2013 with the goal of gaining\
    \ insight into how these systems reason about language and vision. Surprisingly,\
    \ we find strong evidence that even sophisticated and linguistically-motivated\
    \ models for this task may ignore linguistic structure, instead relying on shallow\
    \ correlations introduced by unintended biases in the data selection and annotation\
    \ process. For example, we show that a system trained and tested on the input\
    \ image without the input referring expression can achieve a precision of 71.2%\
    \ in top-2 predictions. Furthermore, a system that predicts only the object category\
    \ given the input can achieve a precision of 84.2% in top-2 predictions. These\
    \ surprisingly positive results for what should be deficient prediction scenarios\
    \ suggest that careful analysis of what our models are learning \u2013 and further,\
    \ how our data is constructed \u2013 is critical as we seek to make substantive\
    \ progress on grounded language tasks."
  address: New Orleans, Louisiana
  author:
  - first: Volkan
    full: Volkan Cirik
    id: volkan-cirik
    last: Cirik
  - first: Louis-Philippe
    full: Louis-Philippe Morency
    id: louis-philippe-morency
    last: Morency
  - first: Taylor
    full: Taylor Berg-Kirkpatrick
    id: taylor-berg-kirkpatrick
    last: Berg-Kirkpatrick
  author_string: Volkan Cirik, Louis-Philippe Morency, Taylor Berg-Kirkpatrick
  bibkey: cirik-etal-2018-visual
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2123
  month: June
  page_first: '781'
  page_last: '787'
  pages: "781\u2013787"
  paper_id: '123'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2123.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2123.jpg
  title: 'Visual Referring Expression Recognition: What Do Systems Actually Learn?'
  title_html: 'Visual Referring Expression Recognition: What Do Systems Actually Learn?'
  url: https://www.aclweb.org/anthology/N18-2123
  year: '2018'
N18-2124:
  abstract: Extraction of spatial relations from sentences with complex/nesting relationships
    is very challenging as often needs resolving inherent semantic ambiguities. We
    seek help from visual modality to fill the information gap in the text modality
    and resolve spatial semantic ambiguities. We use various recent vision and language
    datasets and techniques to train inter-modality alignment models, visual relationship
    classifiers and propose a novel global inference model to integrate these components
    into our structured output prediction model for spatial role and relation extraction.
    Our global inference model enables us to utilize the visual and geometric relationships
    between objects and improves the state-of-art results of spatial information extraction
    from text.
  address: New Orleans, Louisiana
  author:
  - first: Taher
    full: Taher Rahgooy
    id: taher-rahgooy
    last: Rahgooy
  - first: Umar
    full: Umar Manzoor
    id: umar-manzoor
    last: Manzoor
  - first: Parisa
    full: Parisa Kordjamshidi
    id: parisa-kordjamshidi
    last: Kordjamshidi
  author_string: Taher Rahgooy, Umar Manzoor, Parisa Kordjamshidi
  bibkey: rahgooy-etal-2018-visually
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2124
  month: June
  page_first: '788'
  page_last: '794'
  pages: "788\u2013794"
  paper_id: '124'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2124.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2124.jpg
  title: Visually Guided Spatial Relation Extraction from Text
  title_html: Visually Guided Spatial Relation Extraction from Text
  url: https://www.aclweb.org/anthology/N18-2124
  year: '2018'
N18-2125:
  abstract: A major challenge for video captioning is to combine audio and visual
    cues. Existing multi-modal fusion methods have shown encouraging results in video
    understanding. However, the temporal structures of multiple modalities at different
    granularities are rarely explored, and how to selectively fuse the multi-modal
    representations at different levels of details remains uncharted. In this paper,
    we propose a novel hierarchically aligned cross-modal attention (HACA) framework
    to learn and selectively fuse both global and local temporal dynamics of different
    modalities. Furthermore, for the first time, we validate the superior performance
    of the deep audio features on the video captioning task. Finally, our HACA model
    significantly outperforms the previous best systems and achieves new state-of-the-art
    results on the widely used MSR-VTT dataset.
  address: New Orleans, Louisiana
  author:
  - first: Xin
    full: Xin Wang
    id: xin-wang
    last: Wang
  - first: Yuan-Fang
    full: Yuan-Fang Wang
    id: yuan-fang-wang
    last: Wang
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  author_string: Xin Wang, Yuan-Fang Wang, William Yang Wang
  bibkey: wang-etal-2018-watch
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    2 (Short Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 2 (Short Papers)'
  doi: 10.18653/v1/N18-2125
  month: June
  page_first: '795'
  page_last: '801'
  pages: "795\u2013801"
  paper_id: '125'
  parent_volume_id: N18-2
  pdf: https://www.aclweb.org/anthology/N18-2125.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-2125.jpg
  title: 'Watch, Listen, and Describe: Globally and Locally Aligned Cross-Modal Attentions
    for Video Captioning'
  title_html: 'Watch, Listen, and Describe: Globally and Locally Aligned Cross-Modal
    Attentions for Video Captioning'
  url: https://www.aclweb.org/anthology/N18-2125
  year: '2018'
N18-3000:
  address: New Orleans - Louisiana
  author:
  - first: Srinivas
    full: Srinivas Bangalore
    id: srinivas-bangalore
    last: Bangalore
  - first: Jennifer
    full: Jennifer Chu-Carroll
    id: jennifer-chu-carroll
    last: Chu-Carroll
  - first: Yunyao
    full: Yunyao Li
    id: yunyao-li
    last: Li
  author_string: Srinivas Bangalore, Jennifer Chu-Carroll, Yunyao Li
  bibkey: naacl-2018-2018-north-american
  bibtype: proceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  doi: 10.18653/v1/N18-3
  month: June
  paper_id: '0'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3000.jpg
  title: 'Proceedings of the 2018 Conference of the North American Chapter of the
    Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  title_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  url: https://www.aclweb.org/anthology/N18-3000
  year: '2018'
N18-3001:
  abstract: In recent years the use of electronic medical records has accelerated
    resulting in large volumes of medical data when a patient visits a healthcare
    facility. As a first step towards reimbursement healthcare institutions need to
    associate ICD-10 billing codes to these documents. This is done by trained clinical
    coders who may use a computer assisted solution for shortlisting of codes. In
    this work, we present our work to build a machine learning based scalable system
    for predicting ICD-10 codes from electronic medical records. We address data imbalance
    issues by implementing two system architectures using convolutional neural networks
    and logistic regression models. We illustrate the pros and cons of those system
    designs and show that the best performance can be achieved by leveraging the advantages
    of both using a system combination approach.
  address: New Orleans - Louisiana
  attachment:
  - filename: http://vimeo.com/277630837
    type: video
    url: http://vimeo.com/277630837
  author:
  - first: Marilisa
    full: Marilisa Amoia
    id: marilisa-amoia
    last: Amoia
  - first: Frank
    full: Frank Diehl
    id: frank-diehl
    last: Diehl
  - first: Jesus
    full: Jesus Gimenez
    id: jesus-gimenez
    last: Gimenez
  - first: Joel
    full: Joel Pinto
    id: joel-pinto
    last: Pinto
  - first: Raphael
    full: Raphael Schumann
    id: raphael-schumann
    last: Schumann
  - first: Fabian
    full: Fabian Stemmer
    id: fabian-stemmer
    last: Stemmer
  - first: Paul
    full: Paul Vozila
    id: paul-vozila
    last: Vozila
  - first: Yi
    full: Yi Zhang
    id: yi-zhang
    last: Zhang
  author_string: Marilisa Amoia, Frank Diehl, Jesus Gimenez, Joel Pinto, Raphael Schumann,
    Fabian Stemmer, Paul Vozila, Yi Zhang
  bibkey: amoia-etal-2018-scalable
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3001
  month: June
  page_first: '1'
  page_last: '7'
  pages: "1\u20137"
  paper_id: '1'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3001.jpg
  title: Scalable Wide and Deep Learning for Computer Assisted Coding
  title_html: Scalable Wide and Deep Learning for Computer Assisted Coding
  url: https://www.aclweb.org/anthology/N18-3001
  year: '2018'
N18-3002:
  abstract: Matching a seller listed item to an appropriate product has become a fundamental
    and one of the most significant step for e-commerce platforms for product based
    experience. It has a huge impact on making the search effective, search engine
    optimization, providing product reviews and product price estimation etc. along
    with many other advantages for a better user experience. As significant and vital
    it has become, the challenge to tackle the complexity has become huge with the
    exponential growth of individual and business sellers trading millions of products
    everyday. We explored two approaches; classification based on shallow neural network
    and similarity based on deep siamese network. These models outperform the baseline
    by more than 5% in term of accuracy and are capable of extremely efficient training
    and inference.
  address: New Orleans - Louisiana
  attachment:
  - filename: http://vimeo.com/277630843
    type: video
    url: http://vimeo.com/277630843
  author:
  - first: Kashif
    full: Kashif Shah
    id: kashif-shah
    last: Shah
  - first: Selcuk
    full: Selcuk Kopru
    id: selcuk-kopru
    last: Kopru
  - first: Jean-David
    full: Jean-David Ruvini
    id: jean-david-ruvini
    last: Ruvini
  author_string: Kashif Shah, Selcuk Kopru, Jean-David Ruvini
  bibkey: shah-etal-2018-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3002
  month: June
  page_first: '8'
  page_last: '15'
  pages: "8\u201315"
  paper_id: '2'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3002.jpg
  title: Neural Network based Extreme Classification and Similarity Models for Product
    Matching
  title_html: Neural Network based Extreme Classification and Similarity Models for
    Product Matching
  url: https://www.aclweb.org/anthology/N18-3002
  year: '2018'
N18-3003:
  abstract: Intelligent personal digital assistants (IPDAs), a popular real-life application
    with spoken language understanding capabilities, can cover potentially thousands
    of overlapping domains for natural language understanding, and the task of finding
    the best domain to handle an utterance becomes a challenging problem on a large
    scale. In this paper, we propose a set of efficient and scalable shortlisting-reranking
    neural models for effective large-scale domain classification for IPDAs. The shortlisting
    stage focuses on efficiently trimming all domains down to a list of k-best candidate
    domains, and the reranking stage performs a list-wise reranking of the initial
    k-best domains with additional contextual information. We show the effectiveness
    of our approach with extensive experiments on 1,500 IPDA domains.
  address: New Orleans - Louisiana
  attachment:
  - filename: http://vimeo.com/277630853
    type: video
    url: http://vimeo.com/277630853
  author:
  - first: Young-Bum
    full: Young-Bum Kim
    id: young-bum-kim
    last: Kim
  - first: Dongchan
    full: Dongchan Kim
    id: dongchan-kim
    last: Kim
  - first: Joo-Kyung
    full: Joo-Kyung Kim
    id: joo-kyung-kim
    last: Kim
  - first: Ruhi
    full: Ruhi Sarikaya
    id: ruhi-sarikaya
    last: Sarikaya
  author_string: Young-Bum Kim, Dongchan Kim, Joo-Kyung Kim, Ruhi Sarikaya
  bibkey: kim-etal-2018-scalable
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3003
  month: June
  page_first: '16'
  page_last: '24'
  pages: "16\u201324"
  paper_id: '3'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3003.jpg
  title: A Scalable Neural Shortlisting-Reranking Approach for Large-Scale Domain
    Classification in Natural Language Understanding
  title_html: A Scalable Neural Shortlisting-Reranking Approach for Large-Scale Domain
    Classification in Natural Language Understanding
  url: https://www.aclweb.org/anthology/N18-3003
  year: '2018'
N18-3004:
  abstract: In task-oriented dialog, agents need to generate both fluent natural language
    responses and correct external actions like database queries and updates. Our
    paper makes the first attempt at evaluating state of the art models on a large
    real world task with human users. We show that methods that achieve state of the
    art performance on synthetic datasets, perform poorly in real world dialog tasks.
    We propose a hybrid model, where nearest neighbor is used to generate fluent responses
    and Seq2Seq type models ensure dialogue coherency and generate accurate external
    actions. The hybrid model on the customer support data achieves a 78% relative
    improvement in fluency, and a 200% improvement in accuracy of external calls.
  address: New Orleans - Louisiana
  attachment:
  - filename: http://vimeo.com/277631110
    type: video
    url: http://vimeo.com/277631110
  author:
  - first: Rashmi
    full: Rashmi Gangadharaiah
    id: rashmi-gangadharaiah
    last: Gangadharaiah
  - first: Balakrishnan
    full: Balakrishnan Narayanaswamy
    id: balakrishnan-narayanaswamy
    last: Narayanaswamy
  - first: Charles
    full: Charles Elkan
    id: charles-elkan
    last: Elkan
  author_string: Rashmi Gangadharaiah, Balakrishnan Narayanaswamy, Charles Elkan
  bibkey: gangadharaiah-etal-2018-need
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3004
  month: June
  page_first: '25'
  page_last: '32'
  pages: "25\u201332"
  paper_id: '4'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3004.jpg
  title: What we need to learn if we want to do and not just talk
  title_html: What we need to learn if we want to do and not just talk
  url: https://www.aclweb.org/anthology/N18-3004
  year: '2018'
N18-3005:
  abstract: Industrial dialogue systems such as Apple Siri and Google Now rely on
    large scale diverse and robust training data to enable their sophisticated conversation
    capability. Crowdsourcing provides a scalable and inexpensive way of data collection
    but collecting high quality data efficiently requires thoughtful orchestration
    of the crowdsourcing jobs. Prior study of this topic have focused on tasks only
    in the academia settings with limited scope or only provide intrinsic dataset
    analysis, lacking indication on how it affects the trained model performance.
    In this paper, we present a study of crowdsourcing methods for a user intent classification
    task in our deployed dialogue system. Our task requires classification of 47 possible
    user intents and contains many intent pairs with subtle differences. We consider
    different crowdsourcing job types and job prompts and analyze quantitatively the
    quality of the collected data and the downstream model performance on a test set
    of real user queries from production logs. Our observation provides insights into
    designing efficient crowdsourcing jobs and provide recommendations for future
    dialogue system data collection process.
  address: New Orleans - Louisiana
  attachment:
  - filename: http://vimeo.com/277631102
    type: video
    url: http://vimeo.com/277631102
  author:
  - first: Yiping
    full: Yiping Kang
    id: yiping-kang
    last: Kang
  - first: Yunqi
    full: Yunqi Zhang
    id: yunqi-zhang
    last: Zhang
  - first: Jonathan K.
    full: Jonathan K. Kummerfeld
    id: jonathan-k-kummerfeld
    last: Kummerfeld
  - first: Lingjia
    full: Lingjia Tang
    id: lingjia-tang
    last: Tang
  - first: Jason
    full: Jason Mars
    id: jason-mars
    last: Mars
  author_string: Yiping Kang, Yunqi Zhang, Jonathan K. Kummerfeld, Lingjia Tang, Jason
    Mars
  bibkey: kang-etal-2018-data
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3005
  month: June
  page_first: '33'
  page_last: '40'
  pages: "33\u201340"
  paper_id: '5'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3005.jpg
  title: 'Data Collection for Dialogue System: A Startup Perspective'
  title_html: 'Data Collection for Dialogue System: A Startup Perspective'
  url: https://www.aclweb.org/anthology/N18-3005
  year: '2018'
N18-3006:
  abstract: End-to-end neural models show great promise towards building conversational
    agents that are trained from data and on-line experience using supervised and
    reinforcement learning. However, these models require a large corpus of dialogues
    to learn effectively. For goal-oriented dialogues, such datasets are expensive
    to collect and annotate, since each task involves a separate schema and database
    of entities. Further, the Wizard-of-Oz approach commonly used for dialogue collection
    does not provide sufficient coverage of salient dialogue flows, which is critical
    for guaranteeing an acceptable task completion rate in consumer-facing conversational
    agents. In this paper, we study a recently proposed approach for building an agent
    for arbitrary tasks by combining dialogue self-play and crowd-sourcing to generate
    fully-annotated dialogues with diverse and natural utterances. We discuss the
    advantages of this approach for industry applications of conversational agents,
    wherein an agent can be rapidly bootstrapped to deploy in front of users and further
    optimized via interactive learning from actual users of the system.
  address: New Orleans - Louisiana
  attachment:
  - filename: http://vimeo.com/277631118
    type: video
    url: http://vimeo.com/277631118
  author:
  - first: Pararth
    full: Pararth Shah
    id: pararth-shah
    last: Shah
  - first: Dilek
    full: "Dilek Hakkani-T\xFCr"
    id: dilek-hakkani-tur
    last: "Hakkani-T\xFCr"
  - first: Bing
    full: Bing Liu
    id: bing-liu
    last: Liu
  - first: Gokhan
    full: "Gokhan T\xFCr"
    id: gokhan-tur
    last: "T\xFCr"
  author_string: "Pararth Shah, Dilek Hakkani-T\xFCr, Bing Liu, Gokhan T\xFCr"
  bibkey: shah-etal-2018-bootstrapping
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3006
  month: June
  page_first: '41'
  page_last: '51'
  pages: "41\u201351"
  paper_id: '6'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3006.jpg
  title: Bootstrapping a Neural Conversational Agent with Dialogue Self-Play, Crowdsourcing
    and On-Line Reinforcement Learning
  title_html: Bootstrapping a Neural Conversational Agent with Dialogue Self-Play,
    Crowdsourcing and On-Line Reinforcement Learning
  url: https://www.aclweb.org/anthology/N18-3006
  year: '2018'
N18-3007:
  abstract: 'At eBay, we are automatically generating a large amount of natural language
    titles for eCommerce browse pages using machine translation (MT) technology. While
    automatic approaches can generate millions of titles very fast, they are prone
    to errors. We therefore develop quality estimation (QE) methods which can automatically
    detect titles with low quality in order to prevent them from going live. In this
    paper, we present different approaches: The first one is a Random Forest (RF)
    model that explores hand-crafted, robust features, which are a mix of established
    features commonly used in Machine Translation Quality Estimation (MTQE) and new
    features developed specifically for our task. The second model is based on Siamese
    Networks (SNs) which embed the metadata input sequence and the generated title
    in the same space and do not require hand-crafted features at all. We thoroughly
    evaluate and compare those approaches on in-house data. While the RF models are
    competitive for scenarios with smaller amounts of training data and somewhat more
    robust, they are clearly outperformed by the SN models when the amount of training
    data is larger.'
  address: New Orleans - Louisiana
  author:
  - first: Nicola
    full: Nicola Ueffing
    id: nicola-ueffing
    last: Ueffing
  - first: "Jos\xE9 G."
    full: "Jos\xE9 G. C. de Souza"
    id: jose-g-c-de-souza
    last: C. de Souza
  - first: Gregor
    full: Gregor Leusch
    id: gregor-leusch
    last: Leusch
  author_string: "Nicola Ueffing, Jos\xE9 G. C. de Souza, Gregor Leusch"
  bibkey: ueffing-etal-2018-quality
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3007
  month: June
  page_first: '52'
  page_last: '59'
  pages: "52\u201359"
  paper_id: '7'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3007.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3007.jpg
  title: Quality Estimation for Automatically Generated Titles of eCommerce Browse
    Pages
  title_html: Quality Estimation for Automatically Generated Titles of e<span class="acl-fixed-case">C</span>ommerce
    Browse Pages
  url: https://www.aclweb.org/anthology/N18-3007
  year: '2018'
N18-3008:
  abstract: "In large-scale educational assessments, the use of automated scoring\
    \ has recently become quite common. While the majority of student responses can\
    \ be processed and scored without difficulty, there are a small number of responses\
    \ that have atypical characteristics that make it difficult for an automated scoring\
    \ system to assign a correct score. We describe a pipeline that detects and processes\
    \ these kinds of responses at run-time. We present the most frequent kinds of\
    \ what are called non-scorable responses along with effective filtering models\
    \ based on various NLP and speech processing technologies. We give an overview\
    \ of two operational automated scoring systems \u2014one for essay scoring and\
    \ one for speech scoring\u2014 and describe the filtering models they use. Finally,\
    \ we present an evaluation and analysis of filtering models used for spoken responses\
    \ in an assessment of language proficiency."
  address: New Orleans - Louisiana
  author:
  - first: Su-Youn
    full: Su-Youn Yoon
    id: su-youn-yoon
    last: Yoon
  - first: Aoife
    full: Aoife Cahill
    id: aoife-cahill
    last: Cahill
  - first: Anastassia
    full: Anastassia Loukina
    id: anastassia-loukina
    last: Loukina
  - first: Klaus
    full: Klaus Zechner
    id: klaus-zechner
    last: Zechner
  - first: Brian
    full: Brian Riordan
    id: brian-riordan
    last: Riordan
  - first: Nitin
    full: Nitin Madnani
    id: nitin-madnani
    last: Madnani
  author_string: Su-Youn Yoon, Aoife Cahill, Anastassia Loukina, Klaus Zechner, Brian
    Riordan, Nitin Madnani
  bibkey: yoon-etal-2018-atypical
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3008
  month: June
  page_first: '60'
  page_last: '67'
  pages: "60\u201367"
  paper_id: '8'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3008.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3008.jpg
  title: Atypical Inputs in Educational Applications
  title_html: Atypical Inputs in Educational Applications
  url: https://www.aclweb.org/anthology/N18-3008
  year: '2018'
N18-3009:
  abstract: Reviews of products or services on Internet marketplace websites contain
    a rich amount of information. Users often wish to survey reviews or review snippets
    from the perspective of a certain aspect, which has resulted in a large body of
    work on aspect identification and extraction from such corpora. In this work,
    we evaluate a newly-proposed neural model for aspect extraction on two practical
    tasks. The first is to extract canonical sentences of various aspects from reviews,
    and is judged by human evaluators against alternatives. A k-means baseline does
    remarkably well in this setting. The second experiment focuses on the suitability
    of the recovered aspect distributions to represent users by the reviews they have
    written. Through a set of review reranking experiments, we find that aspect-based
    profiles can largely capture notions of user preferences, by showing that divergent
    users generate markedly different review rankings. -means baseline does remarkably
    well in this setting. The second experiment focuses on the suitability of the
    recovered aspect distributions to represent users by the reviews they have written.
    Through a set of review reranking experiments, we find that aspect-based profiles
    can largely capture notions of user preferences, by showing that divergent users
    generate markedly different review rankings.
  address: New Orleans - Louisiana
  author:
  - first: Christopher
    full: Christopher Mitcheltree
    id: christopher-mitcheltree
    last: Mitcheltree
  - first: Skyler
    full: Skyler Wharton
    id: skyler-wharton
    last: Wharton
  - first: Avneesh
    full: Avneesh Saluja
    id: avneesh-saluja
    last: Saluja
  author_string: Christopher Mitcheltree, Skyler Wharton, Avneesh Saluja
  bibkey: mitcheltree-etal-2018-using
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3009
  month: June
  page_first: '68'
  page_last: '75'
  pages: "68\u201375"
  paper_id: '9'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3009.pdf
  publisher: Association for Computational Linguistics
  revision:
  - id: '1'
    url: https://www.aclweb.org/anthology/N18-3009v1.pdf
    value: N18-3009v1
  - explanation: Changed the first name of an author.
    id: '2'
    url: https://www.aclweb.org/anthology/N18-3009v2.pdf
    value: N18-3009v2
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3009.jpg
  title: Using Aspect Extraction Approaches to Generate Review Summaries and User
    Profiles
  title_html: Using Aspect Extraction Approaches to Generate Review Summaries and
    User Profiles
  url: https://www.aclweb.org/anthology/N18-3009
  year: '2018'
N18-3010:
  abstract: The rise of enterprise applications over unstructured and semi-structured
    documents poses new challenges to text understanding systems across multiple dimensions.
    We present SystemT, a declarative text understanding system that addresses these
    challenges and has been deployed in a wide range of enterprise applications. We
    highlight the design considerations and decisions behind SystemT in addressing
    the needs of the enterprise setting. We also summarize the impact of SystemT on
    business and education.
  address: New Orleans - Louisiana
  author:
  - first: Laura
    full: Laura Chiticariu
    id: laura-chiticariu
    last: Chiticariu
  - first: Marina
    full: Marina Danilevsky
    id: marina-danilevsky
    last: Danilevsky
  - first: Yunyao
    full: Yunyao Li
    id: yunyao-li
    last: Li
  - first: Frederick
    full: Frederick Reiss
    id: frederick-reiss
    last: Reiss
  - first: Huaiyu
    full: Huaiyu Zhu
    id: huaiyu-zhu
    last: Zhu
  author_string: Laura Chiticariu, Marina Danilevsky, Yunyao Li, Frederick Reiss,
    Huaiyu Zhu
  bibkey: chiticariu-etal-2018-systemt
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3010
  month: June
  page_first: '76'
  page_last: '83'
  pages: "76\u201383"
  paper_id: '10'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3010.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3010.jpg
  title: 'SystemT: Declarative Text Understanding for Enterprise'
  title_html: '<span class="acl-fixed-case">S</span>ystem<span class="acl-fixed-case">T</span>:
    Declarative Text Understanding for Enterprise'
  url: https://www.aclweb.org/anthology/N18-3010
  year: '2018'
N18-3011:
  abstract: We describe a deployed scalable system for organizing published scientific
    literature into a heterogeneous graph to facilitate algorithmic manipulation and
    discovery. The resulting literature graph consists of more than 280M nodes, representing
    papers, authors, entities and various interactions between them (e.g., authorships,
    citations, entity mentions). We reduce literature graph construction into familiar
    NLP tasks (e.g., entity extraction and linking), point out research challenges
    due to differences from standard formulations of these tasks, and report empirical
    results for each task. The methods described in this paper are used to enable
    semantic features in www.semanticscholar.org.
  address: New Orleans - Louisiana
  author:
  - first: Waleed
    full: Waleed Ammar
    id: waleed-ammar
    last: Ammar
  - first: Dirk
    full: Dirk Groeneveld
    id: dirk-groeneveld
    last: Groeneveld
  - first: Chandra
    full: Chandra Bhagavatula
    id: chandra-bhagavatula
    last: Bhagavatula
  - first: Iz
    full: Iz Beltagy
    id: iz-beltagy
    last: Beltagy
  - first: Miles
    full: Miles Crawford
    id: miles-crawford
    last: Crawford
  - first: Doug
    full: Doug Downey
    id: doug-downey
    last: Downey
  - first: Jason
    full: Jason Dunkelberger
    id: jason-dunkelberger
    last: Dunkelberger
  - first: Ahmed
    full: Ahmed Elgohary
    id: ahmed-elgohary
    last: Elgohary
  - first: Sergey
    full: Sergey Feldman
    id: sergey-feldman
    last: Feldman
  - first: Vu
    full: Vu Ha
    id: vu-ha
    last: Ha
  - first: Rodney
    full: Rodney Kinney
    id: rodney-kinney
    last: Kinney
  - first: Sebastian
    full: Sebastian Kohlmeier
    id: sebastian-kohlmeier
    last: Kohlmeier
  - first: Kyle
    full: Kyle Lo
    id: kyle-lo
    last: Lo
  - first: Tyler
    full: Tyler Murray
    id: tyler-murray
    last: Murray
  - first: Hsu-Han
    full: Hsu-Han Ooi
    id: hsu-han-ooi
    last: Ooi
  - first: Matthew
    full: Matthew Peters
    id: matthew-peters
    last: Peters
  - first: Joanna
    full: Joanna Power
    id: joanna-power
    last: Power
  - first: Sam
    full: Sam Skjonsberg
    id: sam-skjonsberg
    last: Skjonsberg
  - first: Lucy
    full: Lucy Wang
    id: lucy-wang
    last: Wang
  - first: Chris
    full: Chris Wilhelm
    id: chris-wilhelm
    last: Wilhelm
  - first: Zheng
    full: Zheng Yuan
    id: zheng-yuan
    last: Yuan
  - first: Madeleine
    full: Madeleine van Zuylen
    id: madeleine-van-zuylen
    last: van Zuylen
  - first: Oren
    full: Oren Etzioni
    id: oren-etzioni
    last: Etzioni
  author_string: Waleed Ammar, Dirk Groeneveld, Chandra Bhagavatula, Iz Beltagy, Miles
    Crawford, Doug Downey, Jason Dunkelberger, Ahmed Elgohary, Sergey Feldman, Vu
    Ha, Rodney Kinney, Sebastian Kohlmeier, Kyle Lo, Tyler Murray, Hsu-Han Ooi, Matthew
    Peters, Joanna Power, Sam Skjonsberg, Lucy Wang, Chris Wilhelm, Zheng Yuan, Madeleine
    van Zuylen, Oren Etzioni
  bibkey: ammar-etal-2018-construction
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3011
  month: June
  page_first: '84'
  page_last: '91'
  pages: "84\u201391"
  paper_id: '11'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3011.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3011.jpg
  title: Construction of the Literature Graph in Semantic Scholar
  title_html: Construction of the Literature Graph in Semantic Scholar
  url: https://www.aclweb.org/anthology/N18-3011
  year: '2018'
N18-3012:
  abstract: "We present the first real-world application of methods for improving\
    \ neural machine translation (NMT) with human reinforcement, based on explicit\
    \ and implicit user feedback collected on the eBay e-commerce platform. Previous\
    \ work has been confined to simulation experiments, whereas in this paper we work\
    \ with real logged feedback for offline bandit learning of NMT parameters. We\
    \ conduct a thorough analysis of the available explicit user judgments\u2014five-star\
    \ ratings of translation quality\u2014and show that they are not reliable enough\
    \ to yield significant improvements in bandit learning. In contrast, we successfully\
    \ utilize implicit task-based feedback collected in a cross-lingual search task\
    \ to improve task-specific and machine translation quality metrics."
  address: New Orleans - Louisiana
  attachment:
  - filename: http://vimeo.com/277631383
    type: video
    url: http://vimeo.com/277631383
  author:
  - first: Julia
    full: Julia Kreutzer
    id: julia-kreutzer
    last: Kreutzer
  - first: Shahram
    full: Shahram Khadivi
    id: shahram-khadivi
    last: Khadivi
  - first: Evgeny
    full: Evgeny Matusov
    id: evgeny-matusov
    last: Matusov
  - first: Stefan
    full: Stefan Riezler
    id: stefan-riezler
    last: Riezler
  author_string: Julia Kreutzer, Shahram Khadivi, Evgeny Matusov, Stefan Riezler
  bibkey: kreutzer-etal-2018-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3012
  month: June
  page_first: '92'
  page_last: '105'
  pages: "92\u2013105"
  paper_id: '12'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3012.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3012.jpg
  title: Can Neural Machine Translation be Improved with User Feedback?
  title_html: Can Neural Machine Translation be Improved with User Feedback?
  url: https://www.aclweb.org/anthology/N18-3012
  year: '2018'
N18-3013:
  abstract: We describe a batched beam decoding algorithm for NMT with LMBR n-gram
    posteriors, showing that LMBR techniques still yield gains on top of the best
    recently reported results with Transformers. We also discuss acceleration strategies
    for deployment, and the effect of the beam size and batching on memory and speed.
  address: New Orleans - Louisiana
  attachment:
  - filename: http://vimeo.com/277631374
    type: video
    url: http://vimeo.com/277631374
  author:
  - first: Gonzalo
    full: Gonzalo Iglesias
    id: gonzalo-iglesias
    last: Iglesias
  - first: William
    full: William Tambellini
    id: william-tambellini
    last: Tambellini
  - first: "Adri\xE0"
    full: "Adri\xE0 De Gispert"
    id: adria-de-gispert
    last: De Gispert
  - first: Eva
    full: Eva Hasler
    id: eva-hasler
    last: Hasler
  - first: Bill
    full: Bill Byrne
    id: bill-byrne
    last: Byrne
  author_string: "Gonzalo Iglesias, William Tambellini, Adri\xE0 De Gispert, Eva Hasler,\
    \ Bill Byrne"
  bibkey: iglesias-etal-2018-accelerating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3013
  month: June
  page_first: '106'
  page_last: '113'
  pages: "106\u2013113"
  paper_id: '13'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3013.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3013.jpg
  title: Accelerating NMT Batched Beam Decoding with LMBR Posteriors for Deployment
  title_html: Accelerating <span class="acl-fixed-case">NMT</span> Batched Beam Decoding
    with <span class="acl-fixed-case">LMBR</span> Posteriors for Deployment
  url: https://www.aclweb.org/anthology/N18-3013
  year: '2018'
N18-3014:
  abstract: Neural machine translation has achieved levels of fluency and adequacy
    that would have been surprising a short time ago. Output quality is extremely
    relevant for industry purposes, however it is equally important to produce results
    in the shortest time possible, mainly for latency-sensitive applications and to
    control cloud hosting costs. In this paper we show the effectiveness of translating
    with 8-bit quantization for models that have been trained using 32-bit floating
    point values. Results show that 8-bit translation makes a non-negligible impact
    in terms of speed with no degradation in accuracy and adequacy.
  address: New Orleans - Louisiana
  attachment:
  - filename: http://vimeo.com/277631395
    type: video
    url: http://vimeo.com/277631395
  author:
  - first: Jerry
    full: Jerry Quinn
    id: jerry-quinn
    last: Quinn
  - first: Miguel
    full: Miguel Ballesteros
    id: miguel-ballesteros
    last: Ballesteros
  author_string: Jerry Quinn, Miguel Ballesteros
  bibkey: quinn-ballesteros-2018-pieces
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3014
  month: June
  page_first: '114'
  page_last: '120'
  pages: "114\u2013120"
  paper_id: '14'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3014.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3014.jpg
  title: 'Pieces of Eight: 8-bit Neural Machine Translation'
  title_html: 'Pieces of Eight: 8-bit Neural Machine Translation'
  url: https://www.aclweb.org/anthology/N18-3014
  year: '2018'
N18-3015:
  abstract: "A typical workflow to document clinical encounters entails dictating\
    \ a summary, running speech recognition, and post-processing the resulting text\
    \ into a formatted letter. Post-processing entails a host of transformations including\
    \ punctuation restoration, truecasing, marking sections and headers, converting\
    \ dates and numerical expressions, parsing lists, etc. In conventional implementations,\
    \ most of these tasks are accomplished by individual modules. We introduce a novel\
    \ holistic approach to post-processing that relies on machine callytranslation.\
    \ We show how this technique outperforms an alternative conventional system\u2014\
    even learning to correct speech recognition errors during post-processing\u2014\
    while being much simpler to maintain."
  address: New Orleans - Louisiana
  attachment:
  - filename: http://vimeo.com/277631480
    type: video
    url: http://vimeo.com/277631480
  author:
  - first: Gregory
    full: Gregory Finley
    id: gregory-finley
    last: Finley
  - first: Wael
    full: Wael Salloum
    id: wael-salloum
    last: Salloum
  - first: Najmeh
    full: Najmeh Sadoughi
    id: najmeh-sadoughi
    last: Sadoughi
  - first: Erik
    full: Erik Edwards
    id: erik-edwards
    last: Edwards
  - first: Amanda
    full: Amanda Robinson
    id: amanda-robinson
    last: Robinson
  - first: Nico
    full: Nico Axtmann
    id: nico-axtmann
    last: Axtmann
  - first: Michael
    full: Michael Brenndoerfer
    id: michael-brenndoerfer
    last: Brenndoerfer
  - first: Mark
    full: Mark Miller
    id: mark-miller
    last: Miller
  - first: David
    full: David Suendermann-Oeft
    id: david-suendermann-oeft
    last: Suendermann-Oeft
  author_string: Gregory Finley, Wael Salloum, Najmeh Sadoughi, Erik Edwards, Amanda
    Robinson, Nico Axtmann, Michael Brenndoerfer, Mark Miller, David Suendermann-Oeft
  bibkey: finley-etal-2018-dictations
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3015
  month: June
  page_first: '121'
  page_last: '128'
  pages: "121\u2013128"
  paper_id: '15'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3015.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3015.jpg
  title: From dictations to clinical reports using machine translation
  title_html: From dictations to clinical reports using machine translation
  url: https://www.aclweb.org/anthology/N18-3015
  year: '2018'
N18-3016:
  abstract: "We address the problem of determining entity-oriented polarity in business\
    \ news. This can be viewed as classifying the polarity of the sentiment expressed\
    \ toward a given mention of a company in a news article. We present a complete,\
    \ end-to-end approach to the problem. We introduce a new dataset of over 17,000\
    \ manually labeled documents, which is substantially larger than any currently\
    \ available resources. We propose a benchmark solution based on convolutional\
    \ neural networks for classifying entity-oriented polarity. Although our dataset\
    \ is much larger than those currently available, it is small on the scale of datasets\
    \ commonly used for training robust neural network models. To compensate for this,\
    \ we use transfer learning\u2014pre-train the model on a much larger dataset,\
    \ annotated for a related but different classification task, in order to learn\
    \ a good representation for business text, and then fine-tune it on the smaller\
    \ polarity dataset."
  address: New Orleans - Louisiana
  author:
  - first: Lidia
    full: Lidia Pivovarova
    id: lidia-pivovarova
    last: Pivovarova
  - first: Arto
    full: Arto Klami
    id: arto-klami
    last: Klami
  - first: Roman
    full: Roman Yangarber
    id: roman-yangarber
    last: Yangarber
  author_string: Lidia Pivovarova, Arto Klami, Roman Yangarber
  bibkey: pivovarova-etal-2018-benchmarks
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3016
  month: June
  page_first: '129'
  page_last: '136'
  pages: "129\u2013136"
  paper_id: '16'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3016.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3016.jpg
  title: Benchmarks and models for entity-oriented polarity detection
  title_html: Benchmarks and models for entity-oriented polarity detection
  url: https://www.aclweb.org/anthology/N18-3016
  year: '2018'
N18-3017:
  abstract: This paper investigates the use of Machine Translation (MT) to bootstrap
    a Natural Language Understanding (NLU) system for a new language for the use case
    of a large-scale voice-controlled device. The goal is to decrease the cost and
    time needed to get an annotated corpus for the new language, while still having
    a large enough coverage of user requests. Different methods of filtering MT data
    in order to keep utterances that improve NLU performance and language-specific
    post-processing methods are investigated. These methods are tested in a large-scale
    NLU task with translating around 10 millions training utterances from English
    to German. The results show a large improvement for using MT data over a grammar-based
    and over an in-house data collection baseline, while reducing the manual effort
    greatly. Both filtering and post-processing approaches improve results further.
  address: New Orleans - Louisiana
  attachment:
  - filename: http://vimeo.com/277669655
    type: video
    url: http://vimeo.com/277669655
  author:
  - first: Judith
    full: Judith Gaspers
    id: judith-gaspers
    last: Gaspers
  - first: Penny
    full: Penny Karanasou
    id: penny-karanasou
    last: Karanasou
  - first: Rajen
    full: Rajen Chatterjee
    id: rajen-chatterjee
    last: Chatterjee
  author_string: Judith Gaspers, Penny Karanasou, Rajen Chatterjee
  bibkey: gaspers-etal-2018-selecting
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3017
  month: June
  page_first: '137'
  page_last: '144'
  pages: "137\u2013144"
  paper_id: '17'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3017.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3017.jpg
  title: Selecting Machine-Translated Data for Quick Bootstrapping of a Natural Language
    Understanding System
  title_html: Selecting Machine-Translated Data for Quick Bootstrapping of a Natural
    Language Understanding System
  url: https://www.aclweb.org/anthology/N18-3017
  year: '2018'
N18-3018:
  abstract: Fast expansion of natural language functionality of intelligent virtual
    agents is critical for achieving engaging and informative interactions. However,
    developing accurate models for new natural language domains is a time and data
    intensive process. We propose efficient deep neural network architectures that
    maximally re-use available resources through transfer learning. Our methods are
    applied for expanding the understanding capabilities of a popular commercial agent
    and are evaluated on hundreds of new domains, designed by internal or external
    developers. We demonstrate that our proposed methods significantly increase accuracy
    in low resource settings and enable rapid development of accurate models with
    less data.
  address: New Orleans - Louisiana
  attachment:
  - filename: http://vimeo.com/277669529
    type: video
    url: http://vimeo.com/277669529
  author:
  - first: Anuj Kumar
    full: Anuj Kumar Goyal
    id: anuj-kumar-goyal
    last: Goyal
  - first: Angeliki
    full: Angeliki Metallinou
    id: angeliki-metallinou
    last: Metallinou
  - first: Spyros
    full: Spyros Matsoukas
    id: spyros-matsoukas
    last: Matsoukas
  author_string: Anuj Kumar Goyal, Angeliki Metallinou, Spyros Matsoukas
  bibkey: goyal-etal-2018-fast
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3018
  month: June
  page_first: '145'
  page_last: '152'
  pages: "145\u2013152"
  paper_id: '18'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3018.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3018.jpg
  title: Fast and Scalable Expansion of Natural Language Understanding Functionality
    for Intelligent Agents
  title_html: Fast and Scalable Expansion of Natural Language Understanding Functionality
    for Intelligent Agents
  url: https://www.aclweb.org/anthology/N18-3018
  year: '2018'
N18-3019:
  abstract: Slot tagging, the task of detecting entities in input user utterances,
    is a key component of natural language understanding systems for personal digital
    assistants. Since each new domain requires a different set of slots, the annotation
    costs for labeling data for training slot tagging models increases rapidly as
    the number of domains grow. To tackle this, we describe Bag of Experts (BoE) architectures
    for model reuse for both LSTM and CRF based models. Extensive experimentation
    over a dataset of 10 domains drawn from data relevant to our commercial personal
    digital assistant shows that our BoE models outperform the baseline models with
    a statistically significant average margin of 5.06% in absolute F1-score when
    training with 2000 instances per domain, and achieve an even higher improvement
    of 12.16% when only 25% of the training data is used.
  address: New Orleans - Louisiana
  attachment:
  - filename: http://vimeo.com/277669612
    type: video
    url: http://vimeo.com/277669612
  author:
  - first: Rahul
    full: Rahul Jha
    id: rahul-jha
    last: Jha
  - first: Alex
    full: Alex Marin
    id: alex-marin
    last: Marin
  - first: Suvamsh
    full: Suvamsh Shivaprasad
    id: suvamsh-shivaprasad
    last: Shivaprasad
  - first: Imed
    full: Imed Zitouni
    id: imed-zitouni
    last: Zitouni
  author_string: Rahul Jha, Alex Marin, Suvamsh Shivaprasad, Imed Zitouni
  bibkey: jha-etal-2018-bag
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3019
  month: June
  page_first: '153'
  page_last: '161'
  pages: "153\u2013161"
  paper_id: '19'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3019.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3019.jpg
  title: Bag of Experts Architectures for Model Reuse in Conversational Language Understanding
  title_html: Bag of Experts Architectures for Model Reuse in Conversational Language
    Understanding
  url: https://www.aclweb.org/anthology/N18-3019
  year: '2018'
N18-3020:
  abstract: To provide better access of the inventory to buyers and better search
    engine optimization, e-Commerce websites are automatically generating millions
    of browse pages. A browse page consists of a set of slot name/value pairs within
    a given category, grouping multiple items which share some characteristics. These
    browse pages require a title describing the content of the page. Since the number
    of browse pages are huge, manual creation of these titles is infeasible. Previous
    statistical and neural approaches depend heavily on the availability of large
    amounts of data in a language. In this research, we apply sequence-to-sequence
    models to generate titles for high-resource as well as low-resource languages
    by leveraging transfer learning. We train these models on multi-lingual data,
    thereby creating one joint model which can generate titles in various different
    languages. Performance of the title generation system is evaluated on three different
    languages; English, German, and French, with a particular focus on low-resourced
    French language.
  address: New Orleans - Louisiana
  attachment:
  - filename: http://vimeo.com/277669567
    type: video
    url: http://vimeo.com/277669567
  author:
  - first: Prashant
    full: Prashant Mathur
    id: prashant-mathur
    last: Mathur
  - first: Nicola
    full: Nicola Ueffing
    id: nicola-ueffing
    last: Ueffing
  - first: Gregor
    full: Gregor Leusch
    id: gregor-leusch
    last: Leusch
  author_string: Prashant Mathur, Nicola Ueffing, Gregor Leusch
  bibkey: mathur-etal-2018-multi
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3020
  month: June
  page_first: '162'
  page_last: '169'
  pages: "162\u2013169"
  paper_id: '20'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3020.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3020.jpg
  title: Multi-lingual neural title generation for e-Commerce browse pages
  title_html: Multi-lingual neural title generation for e-Commerce browse pages
  url: https://www.aclweb.org/anthology/N18-3020
  year: '2018'
N18-3021:
  abstract: As a specialized example of information extraction, part name extraction
    is an area that presents unique challenges. Part names are typically multi-word
    terms longer than two words. There is little consistency in how terms are described
    in noisy free text, with variations spawned by typos, ad hoc abbreviations, acronyms,
    and incomplete names. This makes search and analyses of parts in these data extremely
    challenging. In this paper, we present our algorithm, PANDA (Part Name Discovery
    Analytics), based on a unique method that exploits statistical, linguistic and
    machine learning techniques to discover part names in noisy text such as that
    in manufacturing quality documentation, supply chain management records, service
    communication logs, and maintenance reports. Experiments show that PANDA is scalable
    and outperforms existing techniques significantly.
  address: New Orleans - Louisiana
  attachment:
  - filename: http://vimeo.com/277669493
    type: video
    url: http://vimeo.com/277669493
  author:
  - first: Nobal Bikram
    full: Nobal Bikram Niraula
    id: nobal-bikram-niraula
    last: Niraula
  - first: Daniel
    full: Daniel Whyatt
    id: daniel-whyatt
    last: Whyatt
  - first: Anne
    full: Anne Kao
    id: anne-kao
    last: Kao
  author_string: Nobal Bikram Niraula, Daniel Whyatt, Anne Kao
  bibkey: niraula-etal-2018-novel
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3021
  month: June
  page_first: '170'
  page_last: '176'
  pages: "170\u2013176"
  paper_id: '21'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3021.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3021.jpg
  title: A Novel Approach to Part Name Discovery in Noisy Text
  title_html: A Novel Approach to Part Name Discovery in Noisy Text
  url: https://www.aclweb.org/anthology/N18-3021
  year: '2018'
N18-3022:
  abstract: "This paper introduces a meaning representation for spoken language understanding.\
    \ The Alexa meaning representation language (AMRL), unlike previous approaches,\
    \ which factor spoken utterances into domains, provides a common representation\
    \ for how people communicate in spoken language. AMRL is a rooted graph, links\
    \ to a large-scale ontology, supports cross-domain queries, fine-grained types,\
    \ complex utterances and composition. A spoken language dataset has been collected\
    \ for Alexa, which contains \u223C20k examples across eight domains. A version\
    \ of this meaning representation was released to developers at a trade show in\
    \ 2016."
  address: New Orleans - Louisiana
  attachment:
  - filename: http://vimeo.com/277669699
    type: video
    url: http://vimeo.com/277669699
  author:
  - first: Thomas
    full: Thomas Kollar
    id: thomas-kollar
    last: Kollar
  - first: Danielle
    full: Danielle Berry
    id: danielle-berry
    last: Berry
  - first: Lauren
    full: Lauren Stuart
    id: lauren-stuart
    last: Stuart
  - first: Karolina
    full: Karolina Owczarzak
    id: karolina-owczarzak
    last: Owczarzak
  - first: Tagyoung
    full: Tagyoung Chung
    id: tagyoung-chung
    last: Chung
  - first: Lambert
    full: Lambert Mathias
    id: lambert-mathias
    last: Mathias
  - first: Michael
    full: Michael Kayser
    id: michael-kayser
    last: Kayser
  - first: Bradford
    full: Bradford Snow
    id: bradford-snow
    last: Snow
  - first: Spyros
    full: Spyros Matsoukas
    id: spyros-matsoukas
    last: Matsoukas
  author_string: Thomas Kollar, Danielle Berry, Lauren Stuart, Karolina Owczarzak,
    Tagyoung Chung, Lambert Mathias, Michael Kayser, Bradford Snow, Spyros Matsoukas
  bibkey: kollar-etal-2018-alexa
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3022
  month: June
  page_first: '177'
  page_last: '184'
  pages: "177\u2013184"
  paper_id: '22'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3022.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3022.jpg
  title: The Alexa Meaning Representation Language
  title_html: The <span class="acl-fixed-case">A</span>lexa Meaning Representation
    Language
  url: https://www.aclweb.org/anthology/N18-3022
  year: '2018'
N18-3023:
  abstract: 'Spoken Language Understanding (SLU), which extracts semantic information
    from speech, is not flawless, specially in practical applications. The reliability
    of the output of an SLU system can be evaluated using a semantic confidence measure.
    Confidence measures are a solution to improve the quality of spoken dialogue systems,
    by rejecting low-confidence SLU results. In this study we discuss real-world applications
    of confidence scoring in a customer service scenario. We build confidence models
    for three major types of dialogue states that are considered as different domains:
    how may I help you, number capture, and confirmation. Practical challenges to
    train domain-dependent confidence models, including data limitations, are discussed,
    and it is shown that feature engineering plays an important role to improve performance.
    We explore a wide variety of predictor features based on speech recognition, intent
    classification, and high-level domain knowledge, and find the combined feature
    set with the best rejection performance for each application.'
  address: New Orleans - Louisiana
  author:
  - first: Mahnoosh
    full: Mahnoosh Mehrabani
    id: mahnoosh-mehrabani
    last: Mehrabani
  - first: David
    full: David Thomson
    id: david-thomson
    last: Thomson
  - first: Benjamin
    full: Benjamin Stern
    id: benjamin-stern
    last: Stern
  author_string: Mahnoosh Mehrabani, David Thomson, Benjamin Stern
  bibkey: mehrabani-etal-2018-practical
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3023
  month: June
  page_first: '185'
  page_last: '192'
  pages: "185\u2013192"
  paper_id: '23'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3023.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3023.jpg
  title: Practical Application of Domain Dependent Confidence Measurement for Spoken
    Language Understanding Systems
  title_html: Practical Application of Domain Dependent Confidence Measurement for
    Spoken Language Understanding Systems
  url: https://www.aclweb.org/anthology/N18-3023
  year: '2018'
N18-3024:
  abstract: The overwhelming success of the Web and mobile technologies has enabled
    millions to share their opinions publicly at any time. But the same success also
    endangers this freedom of speech due to closing down of participatory sites misused
    by individuals or interest groups. We propose to support manual moderation by
    proactively drawing the attention of our moderators to article discussions that
    most likely need their intervention. To this end, we predict which articles will
    receive a high number of comments. In contrast to existing work, we enrich the
    article with metadata, extract semantic and linguistic features, and exploit annotated
    data from a foreign language corpus. Our logistic regression model improves F1-scores
    by over 80% in comparison to state-of-the-art approaches.
  address: New Orleans - Louisiana
  attachment:
  - filename: http://vimeo.com/277674125
    type: video
    url: http://vimeo.com/277674125
  author:
  - first: Carl
    full: Carl Ambroselli
    id: carl-ambroselli
    last: Ambroselli
  - first: Julian
    full: Julian Risch
    id: julian-risch
    last: Risch
  - first: Ralf
    full: Ralf Krestel
    id: ralf-krestel
    last: Krestel
  - first: Andreas
    full: Andreas Loos
    id: andreas-loos
    last: Loos
  author_string: Carl Ambroselli, Julian Risch, Ralf Krestel, Andreas Loos
  bibkey: ambroselli-etal-2018-prediction
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3024
  month: June
  page_first: '193'
  page_last: '199'
  pages: "193\u2013199"
  paper_id: '24'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3024.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3024.jpg
  title: 'Prediction for the Newsroom: Which Articles Will Get the Most Comments?'
  title_html: 'Prediction for the Newsroom: Which Articles Will Get the Most Comments?'
  url: https://www.aclweb.org/anthology/N18-3024
  year: '2018'
N18-3025:
  abstract: In this paper we introduce the notion of Demand-Weighted Completeness,
    allowing estimation of the completeness of a knowledge base with respect to how
    it is used. Defining an entity by its classes, we employ usage data to predict
    the distribution over relations for that entity. For example, instances of person
    in a knowledge base may require a birth date, name and nationality to be considered
    complete. These predicted relation distributions enable detection of important
    gaps in the knowledge base, and define the required facts for unseen entities.
    Such characterisation of the knowledge base can also quantify how usage and completeness
    change over time. We demonstrate a method to measure Demand-Weighted Completeness,
    and show that a simple neural network model performs well at this prediction task.
  address: New Orleans - Louisiana
  attachment:
  - filename: http://vimeo.com/277674060
    type: video
    url: http://vimeo.com/277674060
  author:
  - first: Andrew
    full: Andrew Hopkinson
    id: andrew-hopkinson
    last: Hopkinson
  - first: Amit
    full: Amit Gurdasani
    id: amit-gurdasani
    last: Gurdasani
  - first: Dave
    full: Dave Palfrey
    id: dave-palfrey
    last: Palfrey
  - first: Arpit
    full: Arpit Mittal
    id: arpit-mittal
    last: Mittal
  author_string: Andrew Hopkinson, Amit Gurdasani, Dave Palfrey, Arpit Mittal
  bibkey: hopkinson-etal-2018-demand
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3025
  month: June
  page_first: '200'
  page_last: '207'
  pages: "200\u2013207"
  paper_id: '25'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3025.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3025.jpg
  title: Demand-Weighted Completeness Prediction for a Knowledge Base
  title_html: Demand-Weighted Completeness Prediction for a Knowledge Base
  url: https://www.aclweb.org/anthology/N18-3025
  year: '2018'
N18-3026:
  abstract: "Query auto completion (QAC) systems are a standard part of search engines\
    \ in industry, helping users formulate their query. Such systems update their\
    \ suggestions after the user types each character, predicting the user\u2019s\
    \ intent using various signals \u2013 one of the most common being popularity.\
    \ Recently, deep learning approaches have been proposed for the QAC task, to specifically\
    \ address the main limitation of previous popularity-based methods: the inability\
    \ to predict unseen queries. In this work we improve previous methods based on\
    \ neural language modeling, with the goal of building an end-to-end system. We\
    \ particularly focus on using real-world data by integrating user information\
    \ for personalized suggestions when possible. We also make use of time information\
    \ and study how to increase diversity in the suggestions while studying the impact\
    \ on scalability. Our empirical results demonstrate a marked improvement on two\
    \ separate datasets over previous best methods in both accuracy and scalability,\
    \ making a step towards neural query auto-completion in production search engines."
  address: New Orleans - Louisiana
  attachment:
  - filename: http://vimeo.com/277674087
    type: video
    url: http://vimeo.com/277674087
  author:
  - first: Nicolas
    full: Nicolas Fiorini
    id: nicolas-fiorini
    last: Fiorini
  - first: Zhiyong
    full: Zhiyong Lu
    id: zhiyong-lu
    last: Lu
  author_string: Nicolas Fiorini, Zhiyong Lu
  bibkey: fiorini-lu-2018-personalized
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3026
  month: June
  page_first: '208'
  page_last: '215'
  pages: "208\u2013215"
  paper_id: '26'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3026.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3026.jpg
  title: Personalized neural language models for real-world query auto completion
  title_html: Personalized neural language models for real-world query auto completion
  url: https://www.aclweb.org/anthology/N18-3026
  year: '2018'
N18-3027:
  abstract: 'Job boards and professional social networks heavily use recommender systems
    in order to better support users in exploring job advertisements. Detecting the
    similarity between job advertisements is important for job recommendation systems
    as it allows, for example, the application of item-to-item based recommendations.
    In this work, we research the usage of dense vector representations to enhance
    a large-scale job recommendation system and to rank German job advertisements
    regarding their similarity. We follow a two-folded evaluation scheme: (1) we exploit
    historic user interactions to automatically create a dataset of similar jobs that
    enables an offline evaluation. (2) In addition, we conduct an online A/B test
    and evaluate the best performing method on our platform reaching more than 1 million
    users. We achieve the best results by combining job titles with full-text job
    descriptions. In particular, this method builds dense document representation
    using words of the titles to weigh the importance of words of the full-text description.
    In the online evaluation, this approach allows us to increase the click-through
    rate on job recommendations for active users by 8.0%.'
  address: New Orleans - Louisiana
  author:
  - first: Ahmed
    full: Ahmed Elsafty
    id: ahmed-elsafty
    last: Elsafty
  - first: Martin
    full: Martin Riedl
    id: martin-riedl
    last: Riedl
  - first: Chris
    full: Chris Biemann
    id: chris-biemann
    last: Biemann
  author_string: Ahmed Elsafty, Martin Riedl, Chris Biemann
  bibkey: elsafty-etal-2018-document
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    3 (Industry Papers)'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 3 (Industry Papers)'
  doi: 10.18653/v1/N18-3027
  month: June
  page_first: '216'
  page_last: '224'
  pages: "216\u2013224"
  paper_id: '27'
  parent_volume_id: N18-3
  pdf: https://www.aclweb.org/anthology/N18-3027.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-3027.jpg
  title: Document-based Recommender System for Job Postings using Dense Representations
  title_html: Document-based Recommender System for Job Postings using Dense Representations
  url: https://www.aclweb.org/anthology/N18-3027
  year: '2018'
N18-4000:
  address: New Orleans, Louisiana, USA
  author:
  - first: Silvio Ricardo
    full: Silvio Ricardo Cordeiro
    id: silvio-cordeiro
    last: Cordeiro
  - first: Shereen
    full: Shereen Oraby
    id: shereen-oraby
    last: Oraby
  - first: Umashanthi
    full: Umashanthi Pavalanathan
    id: umashanthi-pavalanathan
    last: Pavalanathan
  - first: Kyeongmin
    full: Kyeongmin Rim
    id: kyeongmin-rim
    last: Rim
  author_string: Silvio Ricardo Cordeiro, Shereen Oraby, Umashanthi Pavalanathan,
    Kyeongmin Rim
  bibkey: naacl-2018-2018-north-american-chapter
  bibtype: proceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N18-4
  month: June
  paper_id: '0'
  parent_volume_id: N18-4
  pdf: https://www.aclweb.org/anthology/N18-4000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-4000.jpg
  title: 'Proceedings of the 2018 Conference of the North American Chapter of the
    Association for Computational Linguistics: Student Research Workshop'
  title_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  url: https://www.aclweb.org/anthology/N18-4000
  year: '2018'
N18-4001:
  abstract: "Conversation is a joint social process, with participants cooperating\
    \ to exchange information. This process is helped along through linguistic alignment:\
    \ participants\u2019 adoption of each other\u2019s word use. This alignment is\
    \ robust, appearing many settings, and is nearly always positive. We create an\
    \ alignment model for examining alignment in Twitter conversations across antagonistic\
    \ groups. This model finds that some word categories, specifically pronouns used\
    \ to establish group identity and common ground, are negatively aligned. This\
    \ negative alignment is observed despite other categories, which are less related\
    \ to the group dynamics, showing the standard positive alignment. This suggests\
    \ that alignment is strongly biased toward cooperative alignment, but that different\
    \ linguistic features can show substantially different behaviors."
  address: New Orleans, Louisiana, USA
  attachment:
  - filename: http://vimeo.com/277631295
    type: video
    url: http://vimeo.com/277631295
  author:
  - first: Hagyeong
    full: Hagyeong Shin
    id: hagyeong-shin
    last: Shin
  - first: Gabriel
    full: Gabriel Doyle
    id: gabriel-doyle
    last: Doyle
  author_string: Hagyeong Shin, Gabriel Doyle
  bibkey: shin-doyle-2018-alignment
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N18-4001
  month: June
  page_first: '1'
  page_last: '8'
  pages: "1\u20138"
  paper_id: '1'
  parent_volume_id: N18-4
  pdf: https://www.aclweb.org/anthology/N18-4001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-4001.jpg
  title: Alignment, Acceptance, and Rejection of Group Identities in Online Political
    Discourse
  title_html: Alignment, Acceptance, and Rejection of Group Identities in Online Political
    Discourse
  url: https://www.aclweb.org/anthology/N18-4001
  year: '2018'
N18-4002:
  abstract: This paper presents two novel datasets and a random-forest classifier
    to automatically predict literal vs. non-literal language usage for a highly frequent
    type of multi-word expression in a low-resource language, i.e., Estonian. We demonstrate
    the value of language-specific indicators induced from theoretical linguistic
    research, which outperform a high majority baseline when combined with language-independent
    features of non-literal language (such as abstractness).
  address: New Orleans, Louisiana, USA
  author:
  - first: Eleri
    full: Eleri Aedmaa
    id: eleri-aedmaa
    last: Aedmaa
  - first: Maximilian
    full: "Maximilian K\xF6per"
    id: maximilian-koper
    last: "K\xF6per"
  - first: Sabine
    full: Sabine Schulte im Walde
    id: sabine-schulte-im-walde
    last: Schulte im Walde
  author_string: "Eleri Aedmaa, Maximilian K\xF6per, Sabine Schulte im Walde"
  bibkey: aedmaa-etal-2018-combining
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N18-4002
  month: June
  page_first: '9'
  page_last: '16'
  pages: "9\u201316"
  paper_id: '2'
  parent_volume_id: N18-4
  pdf: https://www.aclweb.org/anthology/N18-4002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-4002.jpg
  title: Combining Abstractness and Language-specific Theoretical Indicators for Detecting
    Non-Literal Usage of Estonian Particle Verbs
  title_html: Combining Abstractness and Language-specific Theoretical Indicators
    for Detecting Non-Literal Usage of <span class="acl-fixed-case">E</span>stonian
    Particle Verbs
  url: https://www.aclweb.org/anthology/N18-4002
  year: '2018'
N18-4003:
  abstract: "Frame induction is the automatic creation of frame-semantic resources\
    \ similar to FrameNet or PropBank, which map lexical units of a language to frame\
    \ representations of each lexical unit\u2019s semantics. For verbs, these representations\
    \ usually include a specification of their argument slots and of the selectional\
    \ restrictions that apply to each slot. Verbs that participate in diathesis alternations\
    \ have different syntactic realizations whose semantics are closely related, but\
    \ not identical. We discuss the influence that such alternations have on frame\
    \ induction, compare several possible frame structures for verbs in the causative\
    \ alternation, and propose a systematic analysis of alternating verbs that encodes\
    \ their similarities as well as their differences."
  address: New Orleans, Louisiana, USA
  author:
  - first: Esther
    full: Esther Seyffarth
    id: esther-seyffarth
    last: Seyffarth
  author_string: Esther Seyffarth
  bibkey: seyffarth-2018-verb
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N18-4003
  month: June
  page_first: '17'
  page_last: '24'
  pages: "17\u201324"
  paper_id: '3'
  parent_volume_id: N18-4
  pdf: https://www.aclweb.org/anthology/N18-4003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-4003.jpg
  title: Verb Alternations and Their Impact on Frame Induction
  title_html: Verb Alternations and Their Impact on Frame Induction
  url: https://www.aclweb.org/anthology/N18-4003
  year: '2018'
N18-4004:
  abstract: We introduce an automatic system that performs well on two common-sense
    reasoning tasks, the Winograd Schema Challenge (WSC) and the Choice of Plausible
    Alternatives (COPA). Problem instances from these tasks require diverse, complex
    forms of inference and knowledge to solve. Our method uses a knowledge-hunting
    module to gather text from the web, which serves as evidence for candidate problem
    resolutions. Given an input problem, our system generates relevant queries to
    send to a search engine. It extracts and classifies knowledge from the returned
    results and weighs it to make a resolution. Our approach improves F1 performance
    on the WSC by 0.16 over the previous best and is competitive with the state-of-the-art
    on COPA, demonstrating its general applicability.
  address: New Orleans, Louisiana, USA
  attachment:
  - filename: http://vimeo.com/277631281
    type: video
    url: http://vimeo.com/277631281
  author:
  - first: Ali
    full: Ali Emami
    id: ali-emami
    last: Emami
  - first: Adam
    full: Adam Trischler
    id: adam-trischler
    last: Trischler
  - first: Kaheer
    full: Kaheer Suleman
    id: kaheer-suleman
    last: Suleman
  - first: Jackie Chi Kit
    full: Jackie Chi Kit Cheung
    id: jackie-chi-kit-cheung
    last: Cheung
  author_string: Ali Emami, Adam Trischler, Kaheer Suleman, Jackie Chi Kit Cheung
  bibkey: emami-etal-2018-generalized
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N18-4004
  month: June
  page_first: '25'
  page_last: '31'
  pages: "25\u201331"
  paper_id: '4'
  parent_volume_id: N18-4
  pdf: https://www.aclweb.org/anthology/N18-4004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-4004.jpg
  title: A Generalized Knowledge Hunting Framework for the Winograd Schema Challenge
  title_html: A Generalized Knowledge Hunting Framework for the <span class="acl-fixed-case">W</span>inograd
    Schema Challenge
  url: https://www.aclweb.org/anthology/N18-4004
  year: '2018'
N18-4005:
  abstract: We propose a method to study the variation lying between different word
    embeddings models trained with different parameters. We explore the variation
    between models trained with only one varying parameter by observing the distributional
    neighbors variation and show how changing only one parameter can have a massive
    impact on a given semantic space. We show that the variation is not affecting
    all words of the semantic space equally. Variation is influenced by parameters
    such as setting a parameter to its minimum or maximum value but it also depends
    on the corpus intrinsic features such as the frequency of a word. We identify
    semantic classes of words remaining stable across the models trained and specific
    words having high variation.
  address: New Orleans, Louisiana, USA
  author:
  - first: "B\xE9n\xE9dicte"
    full: "B\xE9n\xE9dicte Pierrejean"
    id: benedicte-pierrejean
    last: Pierrejean
  - first: Ludovic
    full: Ludovic Tanguy
    id: ludovic-tanguy
    last: Tanguy
  author_string: "B\xE9n\xE9dicte Pierrejean, Ludovic Tanguy"
  bibkey: pierrejean-tanguy-2018-towards
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N18-4005
  month: June
  page_first: '32'
  page_last: '39'
  pages: "32\u201339"
  paper_id: '5'
  parent_volume_id: N18-4
  pdf: https://www.aclweb.org/anthology/N18-4005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-4005.jpg
  title: 'Towards Qualitative Word Embeddings Evaluation: Measuring Neighbors Variation'
  title_html: 'Towards Qualitative Word Embeddings Evaluation: Measuring Neighbors
    Variation'
  url: https://www.aclweb.org/anthology/N18-4005
  year: '2018'
N18-4006:
  abstract: We investigate the effect of various dependency-based word embeddings
    on distinguishing between functional and domain similarity, word similarity rankings,
    and two downstream tasks in English. Variations include word embeddings trained
    using context windows from Stanford and Universal dependencies at several levels
    of enhancement (ranging from unlabeled, to Enhanced++ dependencies). Results are
    compared to basic linear contexts and evaluated on several datasets. We found
    that embeddings trained with Universal and Stanford dependency contexts excel
    at different tasks, and that enhanced dependencies often improve performance.
  address: New Orleans, Louisiana, USA
  author:
  - first: Sean
    full: Sean MacAvaney
    id: sean-macavaney
    last: MacAvaney
  - first: Amir
    full: Amir Zeldes
    id: amir-zeldes
    last: Zeldes
  author_string: Sean MacAvaney, Amir Zeldes
  bibkey: macavaney-zeldes-2018-deeper
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N18-4006
  month: June
  page_first: '40'
  page_last: '45'
  pages: "40\u201345"
  paper_id: '6'
  parent_volume_id: N18-4
  pdf: https://www.aclweb.org/anthology/N18-4006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-4006.jpg
  title: A Deeper Look into Dependency-Based Word Embeddings
  title_html: A Deeper Look into Dependency-Based Word Embeddings
  url: https://www.aclweb.org/anthology/N18-4006
  year: '2018'
N18-4007:
  abstract: This research proposal describes two algorithms that are aimed at learning
    word embeddings for data sparse and sentiment rich data sets. The goal is to use
    word embeddings adapted for domain specific data sets in downstream applications
    such as sentiment classification. The first approach learns word embeddings in
    a supervised fashion via SWESA (Supervised Word Embeddings for Sentiment Analysis),
    an algorithm for sentiment analysis on data sets that are of modest size. SWESA
    leverages document labels to jointly learn polarity-aware word embeddings and
    a classifier to classify unseen documents. In the second approach domain adapted
    (DA) word embeddings are learned by exploiting the specificity of domain specific
    data sets and the breadth of generic word embeddings. The new embeddings are formed
    by aligning corresponding word vectors using Canonical Correlation Analysis (CCA)
    or the related nonlinear Kernel CCA. Experimental results on binary sentiment
    classification tasks using both approaches for standard data sets are presented.
  address: New Orleans, Louisiana, USA
  author:
  - first: Prathusha
    full: Prathusha Kameswara Sarma
    id: prathusha-kameswara-sarma
    last: Kameswara Sarma
  author_string: Prathusha Kameswara Sarma
  bibkey: kameswara-sarma-2018-learning
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N18-4007
  month: June
  page_first: '46'
  page_last: '53'
  pages: "46\u201353"
  paper_id: '7'
  parent_volume_id: N18-4
  pdf: https://www.aclweb.org/anthology/N18-4007.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-4007.jpg
  title: Learning Word Embeddings for Data Sparse and Sentiment Rich Data Sets
  title_html: Learning Word Embeddings for Data Sparse and Sentiment Rich Data Sets
  url: https://www.aclweb.org/anthology/N18-4007
  year: '2018'
N18-4008:
  abstract: "Igbo is a low-resource language spoken by approximately 30 million people\
    \ worldwide. It is the native language of the Igbo people of south-eastern Nigeria.\
    \ In Igbo language, diacritics - orthographic and tonal - play a huge role in\
    \ the distinguishing the meaning and pronunciation of words. Omitting diacritics\
    \ in texts often leads to lexical ambiguity. Diacritic restoration is a pre-processing\
    \ task that replaces missing diacritics on words from which they have been removed.\
    \ In this work, we applied embedding models to the diacritic restoration task\
    \ and compared their performances to those of n-gram models. Although word embedding\
    \ models have been successfully applied to various NLP tasks, it has not been\
    \ used, to our knowledge, for diacritic restoration. Two classes of word embeddings\
    \ models were used: those projected from the English embedding space; and those\
    \ trained with Igbo bible corpus (\u2248 1m). Our best result, 82.49%, is an improvement\
    \ on the baseline n-gram models."
  address: New Orleans, Louisiana, USA
  attachment:
  - filename: http://vimeo.com/276458723
    type: video
    url: http://vimeo.com/276458723
  author:
  - first: Ignatius
    full: Ignatius Ezeani
    id: ignatius-ezeani
    last: Ezeani
  - first: Mark
    full: Mark Hepple
    id: mark-hepple
    last: Hepple
  - first: Ikechukwu
    full: Ikechukwu Onyenwe
    id: ikechukwu-onyenwe
    last: Onyenwe
  - first: Enemouh
    full: Enemouh Chioma
    id: enemouh-chioma
    last: Chioma
  author_string: Ignatius Ezeani, Mark Hepple, Ikechukwu Onyenwe, Enemouh Chioma
  bibkey: ezeani-etal-2018-igbo
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N18-4008
  month: June
  page_first: '54'
  page_last: '60'
  pages: "54\u201360"
  paper_id: '8'
  parent_volume_id: N18-4
  pdf: https://www.aclweb.org/anthology/N18-4008.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-4008.jpg
  title: Igbo Diacritic Restoration using Embedding Models
  title_html: <span class="acl-fixed-case">I</span>gbo Diacritic Restoration using
    Embedding Models
  url: https://www.aclweb.org/anthology/N18-4008
  year: '2018'
N18-4009:
  abstract: "The aim of this thesis is to perform a Native Language Identification\
    \ (NLI) task where we identify an English learner\u2019s native language background\
    \ based only on the learner\u2019s English writing samples. We focus on the use\
    \ of English grammatical morphemes across four proficiency levels. The outcome\
    \ of the computational task is connected to a position in second language acquisition\
    \ research that holds all learners acquire English grammatical morphemes in the\
    \ same order, regardless of native language background. We use the NLI task as\
    \ a tool to uncover cross-linguistic influence on the developmental trajectory\
    \ of morphemes. We perform a cross-corpus evaluation across proficiency levels\
    \ to increase the reliability and validity of the linguistic features that predict\
    \ the native language background. We include native English data to determine\
    \ the different morpheme patterns used by native versus non-native English speakers.\
    \ Furthermore, we conduct a human NLI task to determine the type and magnitude\
    \ of language transfer cues used by human raters versus the classifier."
  address: New Orleans, Louisiana, USA
  attachment:
  - filename: http://vimeo.com/276460505
    type: video
    url: http://vimeo.com/276460505
  author:
  - first: Alexandra
    full: Alexandra Lavrentovich
    id: alexandra-lavrentovich
    last: Lavrentovich
  author_string: Alexandra Lavrentovich
  bibkey: lavrentovich-2018-using
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N18-4009
  month: June
  page_first: '61'
  page_last: '66'
  pages: "61\u201366"
  paper_id: '9'
  parent_volume_id: N18-4
  pdf: https://www.aclweb.org/anthology/N18-4009.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-4009.jpg
  title: Using Classifier Features to Determine Language Transfer on Morphemes
  title_html: Using Classifier Features to Determine Language Transfer on Morphemes
  url: https://www.aclweb.org/anthology/N18-4009
  year: '2018'
N18-4010:
  abstract: In this thesis proposal, we address the limitations of conventional pipeline
    design of task-oriented dialog systems and propose end-to-end learning solutions.
    We design neural network based dialog system that is able to robustly track dialog
    state, interface with knowledge bases, and incorporate structured query results
    into system responses to successfully complete task-oriented dialog. In learning
    such neural network based dialog systems, we propose hybrid offline training and
    online interactive learning methods. We introduce a multi-task learning method
    in pre-training the dialog agent in a supervised manner using task-oriented dialog
    corpora. The supervised training agent can further be improved via interacting
    with users and learning online from user demonstration and feedback with imitation
    and reinforcement learning. In addressing the sample efficiency issue with online
    policy learning, we further propose a method by combining the learning-from-user
    and learning-from-simulation approaches to improve the online interactive learning
    efficiency.
  address: New Orleans, Louisiana, USA
  attachment:
  - filename: http://vimeo.com/276463184
    type: video
    url: http://vimeo.com/276463184
  author:
  - first: Bing
    full: Bing Liu
    id: bing-liu
    last: Liu
  - first: Ian
    full: Ian Lane
    id: ian-lane
    last: Lane
  author_string: Bing Liu, Ian Lane
  bibkey: liu-lane-2018-end
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N18-4010
  month: June
  page_first: '67'
  page_last: '73'
  pages: "67\u201373"
  paper_id: '10'
  parent_volume_id: N18-4
  pdf: https://www.aclweb.org/anthology/N18-4010.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-4010.jpg
  title: End-to-End Learning of Task-Oriented Dialogs
  title_html: End-to-End Learning of Task-Oriented Dialogs
  url: https://www.aclweb.org/anthology/N18-4010
  year: '2018'
N18-4011:
  abstract: "Most of the health documents, including patient education materials and\
    \ discharge notes, are usually flooded with medical jargons and contain a lot\
    \ of generic information about the health issue. In addition, patients are only\
    \ provided with the doctor\u2019s perspective of what happened to them in the\
    \ hospital while the care procedure performed by nurses during their entire hospital\
    \ stay is nowhere included. The main focus of this research is to generate personalized\
    \ hospital-stay summaries for patients by combining information from physician\
    \ discharge notes and nursing plan of care. It uses a metric to identify medical\
    \ concepts that are Complex, extracts definitions for the concept from three external\
    \ knowledge sources, and provides the simplest definition to the patient. It also\
    \ takes various features of the patient into account, like their concerns and\
    \ strengths, ability to understand basic health information, level of engagement\
    \ in taking care of their health, and familiarity with the health issue and personalizes\
    \ the content of the summaries accordingly. Our evaluation showed that the summaries\
    \ contain 80% of the medical concepts that are considered as being important by\
    \ both doctor and nurses. Three patient advisors (i.e. individuals who are trained\
    \ in understanding patient experience extensively) verified the usability of our\
    \ summaries and mentioned that they would like to get such summaries when they\
    \ are discharged from hospital."
  address: New Orleans, Louisiana, USA
  attachment:
  - filename: http://vimeo.com/277631289
    type: video
    url: http://vimeo.com/277631289
  author:
  - first: Sabita
    full: Sabita Acharya
    id: sabita-acharya
    last: Acharya
  - first: Barbara
    full: Barbara Di Eugenio
    id: barbara-di-eugenio
    last: Di Eugenio
  - first: Andrew
    full: Andrew Boyd
    id: andrew-boyd
    last: Boyd
  - first: Richard
    full: Richard Cameron
    id: richard-cameron
    last: Cameron
  - first: Karen
    full: Karen Dunn Lopez
    id: karen-dunn-lopez
    last: Dunn Lopez
  - first: Pamela
    full: Pamela Martyn-Nemeth
    id: pamela-martyn-nemeth
    last: Martyn-Nemeth
  - first: Carolyn
    full: Carolyn Dickens
    id: carolyn-dickens
    last: Dickens
  - first: Amer
    full: Amer Ardati
    id: amer-ardati
    last: Ardati
  author_string: Sabita Acharya, Barbara Di Eugenio, Andrew Boyd, Richard Cameron,
    Karen Dunn Lopez, Pamela Martyn-Nemeth, Carolyn Dickens, Amer Ardati
  bibkey: acharya-etal-2018-towards
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N18-4011
  month: June
  page_first: '74'
  page_last: '82'
  pages: "74\u201382"
  paper_id: '11'
  parent_volume_id: N18-4
  pdf: https://www.aclweb.org/anthology/N18-4011.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-4011.jpg
  title: Towards Generating Personalized Hospitalization Summaries
  title_html: Towards Generating Personalized Hospitalization Summaries
  url: https://www.aclweb.org/anthology/N18-4011
  year: '2018'
N18-4012:
  abstract: 'Gated-Attention (GA) Reader has been effective for reading comprehension.
    GA Reader makes two assumptions: (1) a uni-directional attention that uses an
    input query to gate token encodings of a document; (2) encoding at the cloze position
    of an input query is considered for answer prediction. In this paper, we propose
    Collaborative Gating (CG) and Self-Belief Aggregation (SBA) to address the above
    assumptions respectively. In CG, we first use an input document to gate token
    encodings of an input query so that the influence of irrelevant query tokens may
    be reduced. Then the filtered query is used to gate token encodings of an document
    in a collaborative fashion. In SBA, we conjecture that query tokens other than
    the cloze token may be informative for answer prediction. We apply self-attention
    to link the cloze token with other tokens in a query so that the importance of
    query tokens with respect to the cloze position are weighted. Then their evidences
    are weighted, propagated and aggregated for better reading comprehension. Experiments
    show that our approaches advance the state-of-theart results in CNN, Daily Mail,
    and Who Did What public test sets.'
  address: New Orleans, Louisiana, USA
  author:
  - first: Haohui
    full: Haohui Deng
    id: haohui-deng
    last: Deng
  - first: Yik-Cheung
    full: Yik-Cheung Tam
    id: yik-cheung-tam
    last: Tam
  author_string: Haohui Deng, Yik-Cheung Tam
  bibkey: deng-tam-2018-read
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N18-4012
  month: June
  page_first: '83'
  page_last: '91'
  pages: "83\u201391"
  paper_id: '12'
  parent_volume_id: N18-4
  pdf: https://www.aclweb.org/anthology/N18-4012.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-4012.jpg
  title: Read and Comprehend by Gated-Attention Reader with More Belief
  title_html: Read and Comprehend by Gated-Attention Reader with More Belief
  url: https://www.aclweb.org/anthology/N18-4012
  year: '2018'
N18-4013:
  abstract: Latent tree learning models learn to parse a sentence without syntactic
    supervision, and use that parse to build the sentence representation. Existing
    work on such models has shown that, while they perform well on tasks like sentence
    classification, they do not learn grammars that conform to any plausible semantic
    or syntactic formalism (Williams et al., 2018a). Studying the parsing ability
    of such models in natural language can be challenging due to the inherent complexities
    of natural language, like having several valid parses for a single sentence. In
    this paper we introduce ListOps, a toy dataset created to study the parsing ability
    of latent tree models. ListOps sequences are in the style of prefix arithmetic.
    The dataset is designed to have a single correct parsing strategy that a system
    needs to learn to succeed at the task. We show that the current leading latent
    tree models are unable to learn to parse and succeed at ListOps. These models
    achieve accuracies worse than purely sequential RNNs.
  address: New Orleans, Louisiana, USA
  author:
  - first: Nikita
    full: Nikita Nangia
    id: nikita-nangia
    last: Nangia
  - first: Samuel
    full: Samuel Bowman
    id: samuel-bowman
    last: Bowman
  author_string: Nikita Nangia, Samuel Bowman
  bibkey: nangia-bowman-2018-listops
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N18-4013
  month: June
  page_first: '92'
  page_last: '99'
  pages: "92\u201399"
  paper_id: '13'
  parent_volume_id: N18-4
  pdf: https://www.aclweb.org/anthology/N18-4013.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-4013.jpg
  title: 'ListOps: A Diagnostic Dataset for Latent Tree Learning'
  title_html: '<span class="acl-fixed-case">L</span>ist<span class="acl-fixed-case">O</span>ps:
    A Diagnostic Dataset for Latent Tree Learning'
  url: https://www.aclweb.org/anthology/N18-4013
  year: '2018'
N18-4014:
  abstract: 'Neural machine translation (NMT) has a drawback in that can generate
    only high-frequency words owing to the computational costs of the softmax function
    in the output layer. In Japanese-English NMT, Japanese predicate conjugation causes
    an increase in vocabulary size. For example, one verb can have as many as 19 surface
    varieties. In this research, we focus on predicate conjugation for compressing
    the vocabulary size in Japanese. The vocabulary list is filled with the various
    forms of verbs. We propose methods using predicate conjugation information without
    discarding linguistic information. The proposed methods can generate low-frequency
    words and deal with unknown words. Two methods were considered to introduce conjugation
    information: the first considers it as a token (conjugation token) and the second
    considers it as an embedded vector (conjugation feature). The results using these
    methods demonstrate that the vocabulary size can be compressed by approximately
    86.1% (Tanaka corpus) and the NMT models can output the words not in the training
    data set. Furthermore, BLEU scores improved by 0.91 points in Japanese-to-English
    translation, and 0.32 points in English-to-Japanese translation with ASPEC.'
  address: New Orleans, Louisiana, USA
  author:
  - first: Michiki
    full: Michiki Kurosawa
    id: michiki-kurosawa
    last: Kurosawa
  - first: Yukio
    full: Yukio Matsumura
    id: yukio-matsumura
    last: Matsumura
  - first: Hayahide
    full: Hayahide Yamagishi
    id: hayahide-yamagishi
    last: Yamagishi
  - first: Mamoru
    full: Mamoru Komachi
    id: mamoru-komachi
    last: Komachi
  author_string: Michiki Kurosawa, Yukio Matsumura, Hayahide Yamagishi, Mamoru Komachi
  bibkey: kurosawa-etal-2018-japanese
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N18-4014
  month: June
  page_first: '100'
  page_last: '105'
  pages: "100\u2013105"
  paper_id: '14'
  parent_volume_id: N18-4
  pdf: https://www.aclweb.org/anthology/N18-4014.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-4014.jpg
  title: Japanese Predicate Conjugation for Neural Machine Translation
  title_html: <span class="acl-fixed-case">J</span>apanese Predicate Conjugation for
    Neural Machine Translation
  url: https://www.aclweb.org/anthology/N18-4014
  year: '2018'
N18-4015:
  abstract: Sentence representations can capture a wide range of information that
    cannot be captured by local features based on character or word N-grams. This
    paper examines the usefulness of universal sentence representations for evaluating
    the quality of machine translation. Al-though it is difficult to train sentence
    representations using small-scale translation datasets with manual evaluation,
    sentence representations trained from large-scale data in other tasks can improve
    the automatic evaluation of machine translation. Experimental results of the WMT-2016
    dataset show that the proposed method achieves state-of-the-art performance with
    sentence representation features only.
  address: New Orleans, Louisiana, USA
  author:
  - first: Hiroki
    full: Hiroki Shimanaka
    id: hiroki-shimanaka
    last: Shimanaka
  - first: Tomoyuki
    full: Tomoyuki Kajiwara
    id: tomoyuki-kajiwara
    last: Kajiwara
  - first: Mamoru
    full: Mamoru Komachi
    id: mamoru-komachi
    last: Komachi
  author_string: Hiroki Shimanaka, Tomoyuki Kajiwara, Mamoru Komachi
  bibkey: shimanaka-etal-2018-metric
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N18-4015
  month: June
  page_first: '106'
  page_last: '111'
  pages: "106\u2013111"
  paper_id: '15'
  parent_volume_id: N18-4
  pdf: https://www.aclweb.org/anthology/N18-4015.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-4015.jpg
  title: Metric for Automatic Machine Translation Evaluation based on Universal Sentence
    Representations
  title_html: Metric for Automatic Machine Translation Evaluation based on Universal
    Sentence Representations
  url: https://www.aclweb.org/anthology/N18-4015
  year: '2018'
N18-4016:
  abstract: "Resources for the non-English languages are scarce and this paper addresses\
    \ this problem in the context of machine translation, by automatically extracting\
    \ parallel sentence pairs from the multilingual articles available on the Internet.\
    \ In this paper, we have used an end-to-end Siamese bidirectional recurrent neural\
    \ network to generate parallel sentences from comparable multilingual articles\
    \ in Wikipedia. Subsequently, we have showed that using the harvested dataset\
    \ improved BLEU scores on both NMT and phrase-based SMT systems for the low-resource\
    \ language pairs: English\u2013Hindi and English\u2013Tamil, when compared to\
    \ training exclusively on the limited bilingual corpora collected for these language\
    \ pairs."
  address: New Orleans, Louisiana, USA
  author:
  - first: Sree Harsha
    full: Sree Harsha Ramesh
    id: sree-harsha-ramesh
    last: Ramesh
  - first: Krishna Prasad
    full: Krishna Prasad Sankaranarayanan
    id: krishna-prasad-sankaranarayanan
    last: Sankaranarayanan
  author_string: Sree Harsha Ramesh, Krishna Prasad Sankaranarayanan
  bibkey: ramesh-sankaranarayanan-2018-neural
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N18-4016
  month: June
  page_first: '112'
  page_last: '119'
  pages: "112\u2013119"
  paper_id: '16'
  parent_volume_id: N18-4
  pdf: https://www.aclweb.org/anthology/N18-4016.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-4016.jpg
  title: Neural Machine Translation for Low Resource Languages using Bilingual Lexicon
    Induced from Comparable Corpora
  title_html: Neural Machine Translation for Low Resource Languages using Bilingual
    Lexicon Induced from Comparable Corpora
  url: https://www.aclweb.org/anthology/N18-4016
  year: '2018'
N18-4017:
  abstract: In recent years, there have been amazing advances in deep learning methods
    for machine reading. In machine reading, the machine reader has to extract the
    answer from the given ground truth paragraph. Recently, the state-of-the-art machine
    reading models achieve human level performance in SQuAD which is a reading comprehension-style
    question answering (QA) task. The success of machine reading has inspired researchers
    to combine Information Retrieval with machine reading to tackle open-domain QA.
    However, these systems perform poorly compared to reading comprehension-style
    QA because it is difficult to retrieve the pieces of paragraphs that contain the
    answer to the question. In this study, we propose two neural network rankers that
    assign scores to different passages based on their likelihood of containing the
    answer to a given question. Additionally, we analyze the relative importance of
    semantic similarity and word level relevance matching in open-domain QA.
  address: New Orleans, Louisiana, USA
  author:
  - first: Phu Mon
    full: Phu Mon Htut
    id: phu-mon-htut
    last: Htut
  - first: Samuel
    full: Samuel Bowman
    id: samuel-bowman
    last: Bowman
  - first: Kyunghyun
    full: Kyunghyun Cho
    id: kyunghyun-cho
    last: Cho
  author_string: Phu Mon Htut, Samuel Bowman, Kyunghyun Cho
  bibkey: htut-etal-2018-training
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N18-4017
  month: June
  page_first: '120'
  page_last: '127'
  pages: "120\u2013127"
  paper_id: '17'
  parent_volume_id: N18-4
  pdf: https://www.aclweb.org/anthology/N18-4017.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-4017.jpg
  title: Training a Ranking Function for Open-Domain Question Answering
  title_html: Training a Ranking Function for Open-Domain Question Answering
  url: https://www.aclweb.org/anthology/N18-4017
  year: '2018'
N18-4018:
  abstract: Emotion Prediction is a Natural Language Processing (NLP) task dealing
    with detection and classification of emotions in various monolingual and bilingual
    texts. While some work has been done on code-mixed social media text and in emotion
    prediction separately, our work is the first attempt which aims at identifying
    the emotion associated with Hindi-English code-mixed social media text. In this
    paper, we analyze the problem of emotion identification in code-mixed content
    and present a Hindi-English code-mixed corpus extracted from twitter and annotated
    with the associated emotion. For every tweet in the dataset, we annotate the source
    language of all the words present, and also the causal language of the expressed
    emotion. Finally, we propose a supervised classification system which uses various
    machine learning techniques for detecting the emotion associated with the text
    using a variety of character level, word level, and lexicon based features.
  address: New Orleans, Louisiana, USA
  author:
  - first: Deepanshu
    full: Deepanshu Vijay
    id: deepanshu-vijay
    last: Vijay
  - first: Aditya
    full: Aditya Bohra
    id: aditya-bohra
    last: Bohra
  - first: Vinay
    full: Vinay Singh
    id: vinay-singh
    last: Singh
  - first: Syed Sarfaraz
    full: Syed Sarfaraz Akhtar
    id: syed-sarfaraz-akhtar
    last: Akhtar
  - first: Manish
    full: Manish Shrivastava
    id: manish-shrivastava
    last: Shrivastava
  author_string: Deepanshu Vijay, Aditya Bohra, Vinay Singh, Syed Sarfaraz Akhtar,
    Manish Shrivastava
  bibkey: vijay-etal-2018-corpus
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N18-4018
  month: June
  page_first: '128'
  page_last: '135'
  pages: "128\u2013135"
  paper_id: '18'
  parent_volume_id: N18-4
  pdf: https://www.aclweb.org/anthology/N18-4018.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-4018.jpg
  title: Corpus Creation and Emotion Prediction for Hindi-English Code-Mixed Social
    Media Text
  title_html: Corpus Creation and Emotion Prediction for <span class="acl-fixed-case">H</span>indi-<span
    class="acl-fixed-case">E</span>nglish Code-Mixed Social Media Text
  url: https://www.aclweb.org/anthology/N18-4018
  year: '2018'
N18-4019:
  abstract: While labor issues and quality assurance in crowdwork are increasingly
    studied, how annotators make sense of texts and how they are personally impacted
    by doing so are not. We study these questions via a narrative-sorting annotation
    task, where carefully selected (by sequentiality, topic, emotional content, and
    length) collections of tweets serve as examples of everyday storytelling. As readers
    process these narratives, we measure their facial expressions, galvanic skin response,
    and self-reported reactions. From the perspective of annotator well-being, a reassuring
    outcome was that the sorting task did not cause a measurable stress response,
    however readers reacted to humor. In terms of sensemaking, readers were more confident
    when sorting sequential, target-topical, and highly emotional tweets. As crowdsourcing
    becomes more common, this research sheds light onto the perceptive capabilities
    and emotional impact of human readers.
  address: New Orleans, Louisiana, USA
  author:
  - first: McKenna
    full: McKenna Tornblad
    id: mckenna-tornblad
    last: Tornblad
  - first: Luke
    full: Luke Lapresi
    id: luke-lapresi
    last: Lapresi
  - first: Christopher
    full: Christopher Homan
    id: christopher-homan
    last: Homan
  - first: Raymond
    full: Raymond Ptucha
    id: raymond-ptucha
    last: Ptucha
  - first: Cecilia
    full: Cecilia Ovesdotter Alm
    id: cecilia-ovesdotter-alm
    last: Ovesdotter Alm
  author_string: McKenna Tornblad, Luke Lapresi, Christopher Homan, Raymond Ptucha,
    Cecilia Ovesdotter Alm
  bibkey: tornblad-etal-2018-sensing
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N18-4019
  month: June
  page_first: '136'
  page_last: '143'
  pages: "136\u2013143"
  paper_id: '19'
  parent_volume_id: N18-4
  pdf: https://www.aclweb.org/anthology/N18-4019.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-4019.jpg
  title: Sensing and Learning Human Annotators Engaged in Narrative Sensemaking
  title_html: <span class="acl-fixed-case">S</span>ensing and Learning Human Annotators
    Engaged in Narrative Sensemaking
  url: https://www.aclweb.org/anthology/N18-4019
  year: '2018'
N18-4020:
  abstract: Image caption generation has gathered widespread interest in the artificial
    intelligence community. Automatic generation of an image description requires
    both computer vision and natural language processing techniques. While, there
    has been advanced research in the English caption generation, research on generating
    Arabic descriptions of an image is extremely limited. Semitic languages like Arabic
    are heavily influenced by root-words. We leverage this critical dependency of
    Arabic to generate captions of an image directly in Arabic using root-word based
    Recurrent Neural Network and Deep Neural Networks. Experimental results on dataset
    from various Middle Eastern newspaper websites allow us to report the first BLEU
    score for direct Arabic caption generation. We also compare the results of our
    approach with BLEU score captions generated in English and translated in Arabic.
    Experimental results confirm that generating image captions using root-words directly
    in Arabic significantly outperforms the English-Arabic translated captions using
    state-of-the-art methods.
  address: New Orleans, Louisiana, USA
  author:
  - first: Vasu
    full: Vasu Jindal
    id: vasu-jindal
    last: Jindal
  author_string: Vasu Jindal
  bibkey: jindal-2018-generating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Student Research Workshop'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Student Research Workshop'
  doi: 10.18653/v1/N18-4020
  month: June
  page_first: '144'
  page_last: '151'
  pages: "144\u2013151"
  paper_id: '20'
  parent_volume_id: N18-4
  pdf: https://www.aclweb.org/anthology/N18-4020.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-4020.jpg
  title: Generating Image Captions in Arabic using Root-Word Based Recurrent Neural
    Networks and Deep Neural Networks
  title_html: Generating Image Captions in <span class="acl-fixed-case">A</span>rabic
    using Root-Word Based Recurrent Neural Networks and Deep Neural Networks
  url: https://www.aclweb.org/anthology/N18-4020
  year: '2018'
N18-5000:
  address: New Orleans, Louisiana
  author:
  - first: Yang
    full: Yang Liu
    id: yang-liu-icsi
    last: Liu
  - first: Tim
    full: Tim Paek
    id: tim-paek
    last: Paek
  - first: Manasi
    full: Manasi Patwardhan
    id: manasi-patwardhan
    last: Patwardhan
  author_string: Yang Liu, Tim Paek, Manasi Patwardhan
  bibkey: naacl-2018-2018-north-american-chapter-association
  bibtype: proceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Demonstrations'
  doi: 10.18653/v1/N18-5
  month: June
  paper_id: '0'
  parent_volume_id: N18-5
  pdf: https://www.aclweb.org/anthology/N18-5000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-5000.jpg
  title: 'Proceedings of the 2018 Conference of the North American Chapter of the
    Association for Computational Linguistics: Demonstrations'
  title_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Demonstrations'
  url: https://www.aclweb.org/anthology/N18-5000
  year: '2018'
N18-5001:
  abstract: "This paper presents NLP Lean Programming framework (NLPf), a new framework\
    \ for creating custom Natural Language Processing (NLP) models and pipelines by\
    \ utilizing common software development build systems. This approach allows developers\
    \ to train and integrate domain-specific NLP pipelines into their applications\
    \ seamlessly. Additionally, NLPf provides an annotation tool which improves the\
    \ annotation process significantly by providing a well-designed GUI and sophisticated\
    \ way of using input devices. Due to NLPf\u2019s properties developers and domain\
    \ experts are able to build domain-specific NLP application more effectively.\
    \ Project page: https://gitlab.com/schrieveslaach/NLPf Video Tutorial: https://www.youtube.com/watch?v=44UJspVebTA\
    \ (Demonstration starts at 11:40 min) This paper is related to: - Interfaces and\
    \ resources to support linguistic annotation - Software architectures and reusable\
    \ components - Software tools for evaluation or error analysis"
  address: New Orleans, Louisiana
  author:
  - first: Marc
    full: Marc Schreiber
    id: marc-schreiber
    last: Schreiber
  - first: Bodo
    full: Bodo Kraft
    id: bodo-kraft
    last: Kraft
  - first: Albert
    full: "Albert Z\xFCndorf"
    id: albert-zundorf
    last: "Z\xFCndorf"
  author_string: "Marc Schreiber, Bodo Kraft, Albert Z\xFCndorf"
  bibkey: schreiber-etal-2018-nlp
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Demonstrations'
  doi: 10.18653/v1/N18-5001
  month: June
  page_first: '1'
  page_last: '5'
  pages: "1\u20135"
  paper_id: '1'
  parent_volume_id: N18-5
  pdf: https://www.aclweb.org/anthology/N18-5001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-5001.jpg
  title: 'NLP Lean Programming Framework: Developing NLP Applications More Effectively'
  title_html: '<span class="acl-fixed-case">NLP</span> Lean Programming Framework:
    Developing <span class="acl-fixed-case">NLP</span> Applications More Effectively'
  url: https://www.aclweb.org/anthology/N18-5001
  year: '2018'
N18-5002:
  abstract: "We demonstrate the serverless deployment of neural networks for model\
    \ inferencing in NLP applications using Amazon\u2019s Lambda service for feedforward\
    \ evaluation and DynamoDB for storing word embeddings. Our architecture realizes\
    \ a pay-per-request pricing model, requiring zero ongoing costs for maintaining\
    \ server instances. All virtual machine management is handled behind the scenes\
    \ by the cloud provider without any direct developer intervention. We describe\
    \ a number of techniques that allow efficient use of serverless resources, and\
    \ evaluations confirm that our design is both scalable and inexpensive."
  address: New Orleans, Louisiana
  author:
  - first: Zhucheng
    full: Zhucheng Tu
    id: zhucheng-tu
    last: Tu
  - first: Mengping
    full: Mengping Li
    id: mengping-li
    last: Li
  - first: Jimmy
    full: Jimmy Lin
    id: jimmy-lin
    last: Lin
  author_string: Zhucheng Tu, Mengping Li, Jimmy Lin
  bibkey: tu-etal-2018-pay
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Demonstrations'
  doi: 10.18653/v1/N18-5002
  month: June
  page_first: '6'
  page_last: '10'
  pages: "6\u201310"
  paper_id: '2'
  parent_volume_id: N18-5
  pdf: https://www.aclweb.org/anthology/N18-5002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-5002.jpg
  title: Pay-Per-Request Deployment of Neural Network Models Using Serverless Architectures
  title_html: Pay-Per-Request Deployment of Neural Network Models Using Serverless
    Architectures
  url: https://www.aclweb.org/anthology/N18-5002
  year: '2018'
N18-5003:
  abstract: "A medical scribe is a clinical professional who charts patient\u2013\
    physician encounters in real time, relieving physicians of most of their administrative\
    \ burden and substantially increasing productivity and job satisfaction. We present\
    \ a complete implementation of an automated medical scribe. Our system can serve\
    \ either as a scalable, standardized, and economical alternative to human scribes;\
    \ or as an assistive tool for them, providing a first draft of a report along\
    \ with a convenient means to modify it. This solution is, to our knowledge, the\
    \ first automated scribe ever presented and relies upon multiple speech and language\
    \ technologies, including speaker diarization, medical speech recognition, knowledge\
    \ extraction, and natural language generation."
  address: New Orleans, Louisiana
  author:
  - first: Gregory
    full: Gregory Finley
    id: gregory-finley
    last: Finley
  - first: Erik
    full: Erik Edwards
    id: erik-edwards
    last: Edwards
  - first: Amanda
    full: Amanda Robinson
    id: amanda-robinson
    last: Robinson
  - first: Michael
    full: Michael Brenndoerfer
    id: michael-brenndoerfer
    last: Brenndoerfer
  - first: Najmeh
    full: Najmeh Sadoughi
    id: najmeh-sadoughi
    last: Sadoughi
  - first: James
    full: James Fone
    id: james-fone
    last: Fone
  - first: Nico
    full: Nico Axtmann
    id: nico-axtmann
    last: Axtmann
  - first: Mark
    full: Mark Miller
    id: mark-miller
    last: Miller
  - first: David
    full: David Suendermann-Oeft
    id: david-suendermann-oeft
    last: Suendermann-Oeft
  author_string: Gregory Finley, Erik Edwards, Amanda Robinson, Michael Brenndoerfer,
    Najmeh Sadoughi, James Fone, Nico Axtmann, Mark Miller, David Suendermann-Oeft
  bibkey: finley-etal-2018-automated
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Demonstrations'
  doi: 10.18653/v1/N18-5003
  month: June
  page_first: '11'
  page_last: '15'
  pages: "11\u201315"
  paper_id: '3'
  parent_volume_id: N18-5
  pdf: https://www.aclweb.org/anthology/N18-5003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-5003.jpg
  title: An automated medical scribe for documenting clinical encounters
  title_html: An automated medical scribe for documenting clinical encounters
  url: https://www.aclweb.org/anthology/N18-5003
  year: '2018'
N18-5004:
  abstract: We present CL Scholar, the ACL Anthology knowledge graph miner to facilitate
    high-quality search and exploration of current research progress in the computational
    linguistics community. In contrast to previous works, periodically crawling, indexing
    and processing of new incoming articles is completely automated in the current
    system. CL Scholar utilizes both textual and network information for knowledge
    graph construction. As an additional novel initiative, CL Scholar supports more
    than 1200 scholarly natural language queries along with standard keyword-based
    search on constructed knowledge graph. It answers binary, statistical and list
    based natural language queries. The current system is deployed at http://cnerg.iitkgp.ac.in/aclakg.
    We also provide REST API support along with bulk download facility. Our code and
    data are available at https://github.com/CLScholar.
  address: New Orleans, Louisiana
  author:
  - first: Mayank
    full: Mayank Singh
    id: mayank-singh
    last: Singh
  - first: Pradeep
    full: Pradeep Dogga
    id: pradeep-dogga
    last: Dogga
  - first: Sohan
    full: Sohan Patro
    id: sohan-patro
    last: Patro
  - first: Dhiraj
    full: Dhiraj Barnwal
    id: dhiraj-barnwal
    last: Barnwal
  - first: Ritam
    full: Ritam Dutt
    id: ritam-dutt
    last: Dutt
  - first: Rajarshi
    full: Rajarshi Haldar
    id: rajarshi-haldar
    last: Haldar
  - first: Pawan
    full: Pawan Goyal
    id: pawan-goyal
    last: Goyal
  - first: Animesh
    full: Animesh Mukherjee
    id: animesh-mukherjee
    last: Mukherjee
  author_string: Mayank Singh, Pradeep Dogga, Sohan Patro, Dhiraj Barnwal, Ritam Dutt,
    Rajarshi Haldar, Pawan Goyal, Animesh Mukherjee
  bibkey: singh-etal-2018-cl
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Demonstrations'
  doi: 10.18653/v1/N18-5004
  month: June
  page_first: '16'
  page_last: '20'
  pages: "16\u201320"
  paper_id: '4'
  parent_volume_id: N18-5
  pdf: https://www.aclweb.org/anthology/N18-5004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-5004.jpg
  title: 'CL Scholar: The ACL Anthology Knowledge Graph Miner'
  title_html: '<span class="acl-fixed-case">CL</span> Scholar: The <span class="acl-fixed-case">ACL</span>
    Anthology Knowledge Graph Miner'
  url: https://www.aclweb.org/anthology/N18-5004
  year: '2018'
N18-5005:
  abstract: Argument mining is a core technology for enabling argument search in large
    corpora. However, most current approaches fall short when applied to heterogeneous
    texts. In this paper, we present an argument retrieval system capable of retrieving
    sentential arguments for any given controversial topic. By analyzing the highest-ranked
    results extracted from Web sources, we found that our system covers 89% of arguments
    found in expert-curated lists of arguments from an online debate portal, and also
    identifies additional valid arguments.
  address: New Orleans, Louisiana
  author:
  - first: Christian
    full: Christian Stab
    id: christian-stab
    last: Stab
  - first: Johannes
    full: Johannes Daxenberger
    id: johannes-daxenberger
    last: Daxenberger
  - first: Chris
    full: Chris Stahlhut
    id: chris-stahlhut
    last: Stahlhut
  - first: Tristan
    full: Tristan Miller
    id: tristan-miller
    last: Miller
  - first: Benjamin
    full: Benjamin Schiller
    id: benjamin-schiller
    last: Schiller
  - first: Christopher
    full: Christopher Tauchmann
    id: christopher-tauchmann
    last: Tauchmann
  - first: Steffen
    full: Steffen Eger
    id: steffen-eger
    last: Eger
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Christian Stab, Johannes Daxenberger, Chris Stahlhut, Tristan Miller,
    Benjamin Schiller, Christopher Tauchmann, Steffen Eger, Iryna Gurevych
  bibkey: stab-etal-2018-argumentext
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Demonstrations'
  doi: 10.18653/v1/N18-5005
  month: June
  page_first: '21'
  page_last: '25'
  pages: "21\u201325"
  paper_id: '5'
  parent_volume_id: N18-5
  pdf: https://www.aclweb.org/anthology/N18-5005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-5005.jpg
  title: 'ArgumenText: Searching for Arguments in Heterogeneous Sources'
  title_html: '<span class="acl-fixed-case">A</span>rgumen<span class="acl-fixed-case">T</span>ext:
    Searching for Arguments in Heterogeneous Sources'
  url: https://www.aclweb.org/anthology/N18-5005
  year: '2018'
N18-5006:
  abstract: We present ClaimRank, an online system for detecting check-worthy claims.
    While originally trained on political debates, the system can work for any kind
    of text, e.g., interviews or just regular news articles. Its aim is to facilitate
    manual fact-checking efforts by prioritizing the claims that fact-checkers should
    consider first. ClaimRank supports both Arabic and English, it is trained on actual
    annotations from nine reputable fact-checking organizations (PolitiFact, FactCheck,
    ABC, CNN, NPR, NYT, Chicago Tribune, The Guardian, and Washington Post), and thus
    it can mimic the claim selection strategies for each and any of them, as well
    as for the union of them all.
  address: New Orleans, Louisiana
  author:
  - first: Israa
    full: Israa Jaradat
    id: israa-jaradat
    last: Jaradat
  - first: Pepa
    full: Pepa Gencheva
    id: pepa-gencheva
    last: Gencheva
  - first: Alberto
    full: "Alberto Barr\xF3n-Cede\xF1o"
    id: alberto-barron-cedeno
    last: "Barr\xF3n-Cede\xF1o"
  - first: "Llu\xEDs"
    full: "Llu\xEDs M\xE0rquez"
    id: lluis-marquez
    last: "M\xE0rquez"
  - first: Preslav
    full: Preslav Nakov
    id: preslav-nakov
    last: Nakov
  author_string: "Israa Jaradat, Pepa Gencheva, Alberto Barr\xF3n-Cede\xF1o, Llu\xED\
    s M\xE0rquez, Preslav Nakov"
  bibkey: jaradat-etal-2018-claimrank
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Demonstrations'
  doi: 10.18653/v1/N18-5006
  month: June
  page_first: '26'
  page_last: '30'
  pages: "26\u201330"
  paper_id: '6'
  parent_volume_id: N18-5
  pdf: https://www.aclweb.org/anthology/N18-5006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-5006.jpg
  title: 'ClaimRank: Detecting Check-Worthy Claims in Arabic and English'
  title_html: '<span class="acl-fixed-case">C</span>laim<span class="acl-fixed-case">R</span>ank:
    Detecting Check-Worthy Claims in <span class="acl-fixed-case">A</span>rabic and
    <span class="acl-fixed-case">E</span>nglish'
  url: https://www.aclweb.org/anthology/N18-5006
  year: '2018'
N18-5007:
  abstract: "The proliferation of fake news and filter bubbles makes it increasingly\
    \ difficult to form an unbiased, balanced opinion towards a topic. To ameliorate\
    \ this, we propose 360\xB0 Stance Detection, a tool that aggregates news with\
    \ multiple perspectives on a topic. It presents them on a spectrum ranging from\
    \ support to opposition, enabling the user to base their opinion on multiple pieces\
    \ of diverse evidence."
  address: New Orleans, Louisiana
  author:
  - first: Sebastian
    full: Sebastian Ruder
    id: sebastian-ruder
    last: Ruder
  - first: John
    full: John Glover
    id: john-glover
    last: Glover
  - first: Afshin
    full: Afshin Mehrabani
    id: afshin-mehrabani
    last: Mehrabani
  - first: Parsa
    full: Parsa Ghaffari
    id: parsa-ghaffari
    last: Ghaffari
  author_string: Sebastian Ruder, John Glover, Afshin Mehrabani, Parsa Ghaffari
  bibkey: ruder-etal-2018-360deg
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Demonstrations'
  doi: 10.18653/v1/N18-5007
  month: June
  page_first: '31'
  page_last: '35'
  pages: "31\u201335"
  paper_id: '7'
  parent_volume_id: N18-5
  pdf: https://www.aclweb.org/anthology/N18-5007.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-5007.jpg
  title: "360\xB0 Stance Detection"
  title_html: "360\xB0 Stance Detection"
  url: https://www.aclweb.org/anthology/N18-5007
  year: '2018'
N18-5008:
  abstract: We introduce DebugSL, a visual (Web) debugging tool for sentiment lexicons
    (SLs). Its core component implements our algorithms for the automatic detection
    of polarity inconsistencies in SLs. An inconsistency is a set of words and/or
    word-senses whose polarity assignments cannot all be simultaneously satisfied.
    DebugSL finds inconsistencies of small sizes in SLs and has a rich user interface
    which helps users in the correction process. The project source code is available
    at https://github.com/atschneid/DebugSL A screencast of DebugSL can be viewed
    at https://cis.temple.edu/~edragut/DebugSL.webm
  address: New Orleans, Louisiana
  author:
  - first: Andrew
    full: Andrew Schneider
    id: andrew-schneider
    last: Schneider
  - first: John
    full: John Male
    id: john-male
    last: Male
  - first: Saroja
    full: Saroja Bhogadhi
    id: saroja-bhogadhi
    last: Bhogadhi
  - first: Eduard
    full: Eduard Dragut
    id: eduard-dragut
    last: Dragut
  author_string: Andrew Schneider, John Male, Saroja Bhogadhi, Eduard Dragut
  bibkey: schneider-etal-2018-debugsl
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Demonstrations'
  doi: 10.18653/v1/N18-5008
  month: June
  page_first: '36'
  page_last: '40'
  pages: "36\u201340"
  paper_id: '8'
  parent_volume_id: N18-5
  pdf: https://www.aclweb.org/anthology/N18-5008.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-5008.jpg
  title: 'DebugSL: An Interactive Tool for Debugging Sentiment Lexicons'
  title_html: '<span class="acl-fixed-case">D</span>ebug<span class="acl-fixed-case">SL</span>:
    An Interactive Tool for Debugging Sentiment Lexicons'
  url: https://www.aclweb.org/anthology/N18-5008
  year: '2018'
N18-5009:
  abstract: We demonstrate ELISA-EDL, a state-of-the-art re-trainable system to extract
    entity mentions from low-resource languages, link them to external English knowledge
    bases, and visualize locations related to disaster topics on a world heatmap.
    We make all of our data sets, resources and system training and testing APIs publicly
    available for research purpose.
  address: New Orleans, Louisiana
  author:
  - first: Boliang
    full: Boliang Zhang
    id: boliang-zhang
    last: Zhang
  - first: Ying
    full: Ying Lin
    id: ying-lin
    last: Lin
  - first: Xiaoman
    full: Xiaoman Pan
    id: xiaoman-pan
    last: Pan
  - first: Di
    full: Di Lu
    id: di-lu
    last: Lu
  - first: Jonathan
    full: Jonathan May
    id: jonathan-may
    last: May
  - first: Kevin
    full: Kevin Knight
    id: kevin-knight
    last: Knight
  - first: Heng
    full: Heng Ji
    id: heng-ji
    last: Ji
  author_string: Boliang Zhang, Ying Lin, Xiaoman Pan, Di Lu, Jonathan May, Kevin
    Knight, Heng Ji
  bibkey: zhang-etal-2018-elisa
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Demonstrations'
  doi: 10.18653/v1/N18-5009
  month: June
  page_first: '41'
  page_last: '45'
  pages: "41\u201345"
  paper_id: '9'
  parent_volume_id: N18-5
  pdf: https://www.aclweb.org/anthology/N18-5009.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-5009.jpg
  title: 'ELISA-EDL: A Cross-lingual Entity Extraction, Linking and Localization System'
  title_html: '<span class="acl-fixed-case">ELISA</span>-<span class="acl-fixed-case">EDL</span>:
    A Cross-lingual Entity Extraction, Linking and Localization System'
  url: https://www.aclweb.org/anthology/N18-5009
  year: '2018'
N18-5010:
  abstract: We present a system for resolving entities and disambiguating locations
    based on publicly available web data in the domain of ancient Hindu Temples. Scarce,
    unstructured information poses a challenge to Entity Resolution(ER) and snippet
    ranking. Additionally, because the same set of entities may be associated with
    multiple locations, Location Disambiguation(LD) is a problem. The mentions and
    descriptions of temples exist in the order of hundreds of thousands, with such
    data generated by various users in various forms such as text (Wikipedia pages),
    videos (YouTube videos), blogs, etc. We demonstrate an integrated approach using
    a combination of grammar rules for parsing and unsupervised (clustering) algorithms
    to resolve entity and locations with high confidence. A demo of our system is
    accessible at tinyurl.com/templedemos. Our system is open source and available
    on GitHub.
  address: New Orleans, Louisiana
  author:
  - first: Ayush
    full: Ayush Maheshwari
    id: ayush-maheshwari
    last: Maheshwari
  - first: Vishwajeet
    full: Vishwajeet Kumar
    id: vishwajeet-kumar
    last: Kumar
  - first: Ganesh
    full: Ganesh Ramakrishnan
    id: ganesh-ramakrishnan
    last: Ramakrishnan
  - first: J. Saketha
    full: J. Saketha Nath
    id: j-saketha-nath
    last: Nath
  author_string: Ayush Maheshwari, Vishwajeet Kumar, Ganesh Ramakrishnan, J. Saketha
    Nath
  bibkey: maheshwari-etal-2018-entity
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Demonstrations'
  doi: 10.18653/v1/N18-5010
  month: June
  page_first: '46'
  page_last: '50'
  pages: "46\u201350"
  paper_id: '10'
  parent_volume_id: N18-5
  pdf: https://www.aclweb.org/anthology/N18-5010.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-5010.jpg
  title: Entity Resolution and Location Disambiguation in the Ancient Hindu Temples
    Domain using Web Data
  title_html: Entity Resolution and Location Disambiguation in the Ancient <span class="acl-fixed-case">H</span>indu
    Temples Domain using Web Data
  url: https://www.aclweb.org/anthology/N18-5010
  year: '2018'
N18-5011:
  abstract: "Madly Ambiguous is an open source, online game aimed at teaching audiences\
    \ of all ages about structural ambiguity and why it\u2019s hard for computers.\
    \ After a brief introduction to structural ambiguity, users are challenged to\
    \ complete a sentence in a way that tricks the computer into guessing an incorrect\
    \ interpretation. Behind the scenes are two different NLP-based methods for classifying\
    \ the user\u2019s input, one representative of classic rule-based approaches to\
    \ disambiguation and the other representative of recent neural network approaches.\
    \ Qualitative feedback from the system\u2019s use in online, classroom, and science\
    \ museum settings indicates that it is engaging and successful in conveying the\
    \ intended take home messages. A demo of Madly Ambiguous can be played at http://madlyambiguous.osu.edu."
  address: New Orleans, Louisiana
  author:
  - first: Ajda
    full: Ajda Gokcen
    id: ajda-gokcen
    last: Gokcen
  - first: Ethan
    full: Ethan Hill
    id: ethan-hill
    last: Hill
  - first: Michael
    full: Michael White
    id: michael-white
    last: White
  author_string: Ajda Gokcen, Ethan Hill, Michael White
  bibkey: gokcen-etal-2018-madly
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Demonstrations'
  doi: 10.18653/v1/N18-5011
  month: June
  page_first: '51'
  page_last: '55'
  pages: "51\u201355"
  paper_id: '11'
  parent_volume_id: N18-5
  pdf: https://www.aclweb.org/anthology/N18-5011.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-5011.jpg
  title: "Madly Ambiguous: A Game for Learning about Structural Ambiguity and Why\
    \ It\u2019s Hard for Computers"
  title_html: "Madly Ambiguous: A Game for Learning about Structural Ambiguity and\
    \ Why It\u2019s Hard for Computers"
  url: https://www.aclweb.org/anthology/N18-5011
  year: '2018'
N18-5012:
  abstract: "We present an easy-to-use and fast toolkit, namely VnCoreNLP\u2014a Java\
    \ NLP annotation pipeline for Vietnamese. Our VnCoreNLP supports key natural language\
    \ processing (NLP) tasks including word segmentation, part-of-speech (POS) tagging,\
    \ named entity recognition (NER) and dependency parsing, and obtains state-of-the-art\
    \ (SOTA) results for these tasks. We release VnCoreNLP to provide rich linguistic\
    \ annotations to facilitate research work on Vietnamese NLP. Our VnCoreNLP is\
    \ open-source and available at: https://github.com/vncorenlp/VnCoreNLP"
  address: New Orleans, Louisiana
  author:
  - first: Thanh
    full: Thanh Vu
    id: thanh-vu
    last: Vu
  - first: Dat Quoc
    full: Dat Quoc Nguyen
    id: dat-quoc-nguyen
    last: Nguyen
  - first: Dai Quoc
    full: Dai Quoc Nguyen
    id: dai-quoc-nguyen
    last: Nguyen
  - first: Mark
    full: Mark Dras
    id: mark-dras
    last: Dras
  - first: Mark
    full: Mark Johnson
    id: mark-johnson
    last: Johnson
  author_string: Thanh Vu, Dat Quoc Nguyen, Dai Quoc Nguyen, Mark Dras, Mark Johnson
  bibkey: vu-etal-2018-vncorenlp
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Demonstrations'
  doi: 10.18653/v1/N18-5012
  month: June
  page_first: '56'
  page_last: '60'
  pages: "56\u201360"
  paper_id: '12'
  parent_volume_id: N18-5
  pdf: https://www.aclweb.org/anthology/N18-5012.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-5012.jpg
  title: 'VnCoreNLP: A Vietnamese Natural Language Processing Toolkit'
  title_html: '<span class="acl-fixed-case">V</span>n<span class="acl-fixed-case">C</span>ore<span
    class="acl-fixed-case">NLP</span>: A <span class="acl-fixed-case">V</span>ietnamese
    Natural Language Processing Toolkit'
  url: https://www.aclweb.org/anthology/N18-5012
  year: '2018'
N18-5013:
  abstract: We demonstrate a JavaScript implementation of a convolutional neural network
    that performs feedforward inference completely in the browser. Such a deployment
    means that models can run completely on the client, on a wide range of devices,
    without making backend server requests. This design is useful for applications
    with stringent latency requirements or low connectivity. Our evaluations show
    the feasibility of JavaScript as a deployment target. Furthermore, an in-browser
    implementation enables seamless integration with the JavaScript ecosystem for
    information visualization, providing opportunities to visually inspect neural
    networks and better understand their inner workings.
  address: New Orleans, Louisiana
  author:
  - first: Yiyun
    full: Yiyun Liang
    id: yiyun-liang
    last: Liang
  - first: Zhucheng
    full: Zhucheng Tu
    id: zhucheng-tu
    last: Tu
  - first: Laetitia
    full: Laetitia Huang
    id: laetitia-huang
    last: Huang
  - first: Jimmy
    full: Jimmy Lin
    id: jimmy-lin
    last: Lin
  author_string: Yiyun Liang, Zhucheng Tu, Laetitia Huang, Jimmy Lin
  bibkey: liang-etal-2018-cnns
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Demonstrations'
  doi: 10.18653/v1/N18-5013
  month: June
  page_first: '61'
  page_last: '65'
  pages: "61\u201365"
  paper_id: '13'
  parent_volume_id: N18-5
  pdf: https://www.aclweb.org/anthology/N18-5013.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-5013.jpg
  title: 'CNNs for NLP in the Browser: Client-Side Deployment and Visualization Opportunities'
  title_html: '<span class="acl-fixed-case">CNN</span>s for <span class="acl-fixed-case">NLP</span>
    in the Browser: Client-Side Deployment and Visualization Opportunities'
  url: https://www.aclweb.org/anthology/N18-5013
  year: '2018'
N18-5014:
  abstract: We present an architecture that generates medical texts while learning
    an informative, continuous representation with discriminative features. During
    training the input to the system is a dataset of captions for medical X-Rays.
    The acquired continuous representations are of particular interest for use in
    many machine learning techniques where the discrete and high-dimensional nature
    of textual input is an obstacle. We use an Adversarially Regularized Autoencoder
    to create realistic text in both an unconditional and conditional setting. We
    show that this technique is applicable to medical texts which often contain syntactic
    and domain-specific shorthands. A quantitative evaluation shows that we achieve
    a lower model perplexity than a traditional LSTM generator.
  address: New Orleans, Louisiana
  author:
  - first: Graham
    full: Graham Spinks
    id: graham-spinks
    last: Spinks
  - first: Marie-Francine
    full: Marie-Francine Moens
    id: marie-francine-moens
    last: Moens
  author_string: Graham Spinks, Marie-Francine Moens
  bibkey: spinks-moens-2018-generating
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Demonstrations'
  doi: 10.18653/v1/N18-5014
  month: June
  page_first: '66'
  page_last: '70'
  pages: "66\u201370"
  paper_id: '14'
  parent_volume_id: N18-5
  pdf: https://www.aclweb.org/anthology/N18-5014.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-5014.jpg
  title: Generating Continuous Representations of Medical Texts
  title_html: Generating Continuous Representations of Medical Texts
  url: https://www.aclweb.org/anthology/N18-5014
  year: '2018'
N18-5015:
  abstract: Machine Translation systems are usually evaluated and compared using automated
    evaluation metrics such as BLEU and METEOR to score the generated translations
    against human translations. However, the interaction with the output from the
    metrics is relatively limited and results are commonly a single score along with
    a few additional statistics. Whilst this may be enough for system comparison it
    does not provide much useful feedback or a means for inspecting translations and
    their respective scores. VisEval Metric Viewer VEMV is a tool designed to provide
    visualisation of multiple evaluation scores so they can be easily interpreted
    by a user. VEMV takes in the source, reference, and hypothesis files as parameters,
    and scores the hypotheses using several popular evaluation metrics simultaneously.
    Scores are produced at both the sentence and dataset level and results are written
    locally to a series of HTML files that can be viewed on a web browser. The individual
    scored sentences can easily be inspected using powerful search and selection functions
    and results can be visualised with graphical representations of the scores and
    distributions.
  address: New Orleans, Louisiana
  author:
  - first: David
    full: David Steele
    id: david-steele
    last: Steele
  - first: Lucia
    full: Lucia Specia
    id: lucia-specia
    last: Specia
  author_string: David Steele, Lucia Specia
  bibkey: steele-specia-2018-vis
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Demonstrations'
  doi: 10.18653/v1/N18-5015
  month: June
  page_first: '71'
  page_last: '75'
  pages: "71\u201375"
  paper_id: '15'
  parent_volume_id: N18-5
  pdf: https://www.aclweb.org/anthology/N18-5015.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-5015.jpg
  title: 'Vis-Eval Metric Viewer: A Visualisation Tool for Inspecting and Evaluating
    Metric Scores of Machine Translation Output'
  title_html: 'Vis-Eval Metric Viewer: A Visualisation Tool for Inspecting and Evaluating
    Metric Scores of Machine Translation Output'
  url: https://www.aclweb.org/anthology/N18-5015
  year: '2018'
N18-5016:
  abstract: Having an understanding of interpersonal relationships is helpful in many
    contexts. Our system seeks to assist humans with that task, using textual information
    (e.g., case notes, speech transcripts, posts, books) as input. Specifically, our
    system first extracts qualitative and quantitative information elements (which
    we call signals) about interactions among persons, aggregates those to provide
    a condensed view of relationships and then enables users to explore all facets
    of the resulting social (multi-)graph through a visual interface.
  address: New Orleans, Louisiana
  author:
  - first: "L\xE9a"
    full: "L\xE9a Deleris"
    id: lea-deleris
    last: Deleris
  - first: Francesca
    full: Francesca Bonin
    id: francesca-bonin
    last: Bonin
  - first: Elizabeth
    full: Elizabeth Daly
    id: elizabeth-daly
    last: Daly
  - first: "St\xE9phane"
    full: "St\xE9phane Deparis"
    id: stephane-deparis
    last: Deparis
  - first: Yufang
    full: Yufang Hou
    id: yufang-hou
    last: Hou
  - first: Charles
    full: Charles Jochim
    id: charles-jochim
    last: Jochim
  - first: Yassine
    full: Yassine Lassoued
    id: yassine-lassoued
    last: Lassoued
  - first: Killian
    full: Killian Levacher
    id: killian-levacher
    last: Levacher
  author_string: "L\xE9a Deleris, Francesca Bonin, Elizabeth Daly, St\xE9phane Deparis,\
    \ Yufang Hou, Charles Jochim, Yassine Lassoued, Killian Levacher"
  bibkey: deleris-etal-2018-know
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Demonstrations'
  doi: 10.18653/v1/N18-5016
  month: June
  page_first: '76'
  page_last: '80'
  pages: "76\u201380"
  paper_id: '16'
  parent_volume_id: N18-5
  pdf: https://www.aclweb.org/anthology/N18-5016.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-5016.jpg
  title: 'Know Who Your Friends Are: Understanding Social Connections from Unstructured
    Text'
  title_html: 'Know Who Your <span class="acl-fixed-case">F</span>riends Are: Understanding
    Social Connections from Unstructured Text'
  url: https://www.aclweb.org/anthology/N18-5016
  year: '2018'
N18-5017:
  abstract: 'This paper presents a web-based information system, RiskFinder, for facilitating
    the analyses of soft and hard information in financial reports. In particular,
    the system broadens the analyses from the word level to sentence level, which
    makes the system useful for practitioner communities and unprecedented among financial
    academics. The proposed system has four main components: 1) a Form 10-K risk-sentiment
    dataset, consisting of a set of risk-labeled financial sentences and pre-trained
    sentence embeddings; 2) metadata, including basic information on each company
    that published the Form 10-K financial report as well as several relevant financial
    measures; 3) an interface that highlights risk-related sentences in the financial
    reports based on the latest sentence embedding techniques; 4) a visualization
    of financial time-series data for a corresponding company. This paper also conducts
    some case studies to showcase that the system can be of great help in capturing
    valuable insight within large amounts of textual information. The system is now
    online available at https://cfda.csie.org/RiskFinder/.'
  address: New Orleans, Louisiana
  author:
  - first: Yu-Wen
    full: Yu-Wen Liu
    id: yu-wen-liu
    last: Liu
  - first: Liang-Chih
    full: Liang-Chih Liu
    id: liang-chih-liu
    last: Liu
  - first: Chuan-Ju
    full: Chuan-Ju Wang
    id: chuan-ju-wang
    last: Wang
  - first: Ming-Feng
    full: Ming-Feng Tsai
    id: ming-feng-tsai
    last: Tsai
  author_string: Yu-Wen Liu, Liang-Chih Liu, Chuan-Ju Wang, Ming-Feng Tsai
  bibkey: liu-etal-2018-riskfinder
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Demonstrations'
  doi: 10.18653/v1/N18-5017
  month: June
  page_first: '81'
  page_last: '85'
  pages: "81\u201385"
  paper_id: '17'
  parent_volume_id: N18-5
  pdf: https://www.aclweb.org/anthology/N18-5017.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-5017.jpg
  title: 'RiskFinder: A Sentence-level Risk Detector for Financial Reports'
  title_html: '<span class="acl-fixed-case">R</span>isk<span class="acl-fixed-case">F</span>inder:
    A Sentence-level Risk Detector for Financial Reports'
  url: https://www.aclweb.org/anthology/N18-5017
  year: '2018'
N18-5018:
  abstract: "We demonstrate an intelligent conversational agent system designed for\
    \ advancing human-machine collaborative tasks. The agent is able to interpret\
    \ a user\u2019s communicative intent from both their verbal utterances and non-verbal\
    \ behaviors, such as gestures. The agent is also itself able to communicate both\
    \ with natural language and gestures, through its embodiment as an avatar thus\
    \ facilitating natural symmetric multi-modal interactions. We demonstrate two\
    \ intelligent agents with specialized skills in the Blocks World as use-cases\
    \ of our system."
  address: New Orleans, Louisiana
  author:
  - first: Sujeong
    full: Sujeong Kim
    id: sujeong-kim
    last: Kim
  - first: David
    full: David Salter
    id: david-salter
    last: Salter
  - first: Luke
    full: Luke DeLuccia
    id: luke-deluccia
    last: DeLuccia
  - first: Kilho
    full: Kilho Son
    id: kilho-son
    last: Son
  - first: Mohamed R.
    full: Mohamed R. Amer
    id: mohamed-r-amer
    last: Amer
  - first: Amir
    full: Amir Tamrakar
    id: amir-tamrakar
    last: Tamrakar
  author_string: Sujeong Kim, David Salter, Luke DeLuccia, Kilho Son, Mohamed R. Amer,
    Amir Tamrakar
  bibkey: kim-etal-2018-smilee
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Demonstrations'
  doi: 10.18653/v1/N18-5018
  month: June
  page_first: '86'
  page_last: '90'
  pages: "86\u201390"
  paper_id: '18'
  parent_volume_id: N18-5
  pdf: https://www.aclweb.org/anthology/N18-5018.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-5018.jpg
  title: 'SMILEE: Symmetric Multi-modal Interactions with Language-gesture Enabled
    (AI) Embodiment'
  title_html: '<span class="acl-fixed-case">SMILEE</span>: Symmetric Multi-modal Interactions
    with Language-gesture Enabled (<span class="acl-fixed-case">AI</span>) Embodiment'
  url: https://www.aclweb.org/anthology/N18-5018
  year: '2018'
N18-5019:
  abstract: We describe the vision and current version of a Natural Language Processing
    system aimed at group decision making facilitation. Borrowing from the scientific
    field of Decision Analysis, its essential role is to identify alternatives and
    criteria associated with a given decision, to keep track of who proposed them
    and of the expressed sentiment towards them. Based on this information, the system
    can help identify agreement and dissent or recommend an alternative. Overall,
    it seeks to help a group reach a decision in a natural yet auditable fashion.
  address: New Orleans, Louisiana
  author:
  - first: "L\xE9a"
    full: "L\xE9a Deleris"
    id: lea-deleris
    last: Deleris
  - first: Debasis
    full: Debasis Ganguly
    id: debasis-ganguly
    last: Ganguly
  - first: Killian
    full: Killian Levacher
    id: killian-levacher
    last: Levacher
  - first: Martin
    full: Martin Stephenson
    id: martin-stephenson
    last: Stephenson
  - first: Francesca
    full: Francesca Bonin
    id: francesca-bonin
    last: Bonin
  author_string: "L\xE9a Deleris, Debasis Ganguly, Killian Levacher, Martin Stephenson,\
    \ Francesca Bonin"
  bibkey: deleris-etal-2018-decision
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Demonstrations'
  doi: 10.18653/v1/N18-5019
  month: June
  page_first: '91'
  page_last: '95'
  pages: "91\u201395"
  paper_id: '19'
  parent_volume_id: N18-5
  pdf: https://www.aclweb.org/anthology/N18-5019.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-5019.jpg
  title: Decision Conversations Decoded
  title_html: Decision Conversations Decoded
  url: https://www.aclweb.org/anthology/N18-5019
  year: '2018'
N18-5020:
  abstract: We present Sounding Board, a social chatbot that won the 2017 Amazon Alexa
    Prize. The system architecture consists of several components including spoken
    language processing, dialogue management, language generation, and content management,
    with emphasis on user-centric and content-driven design. We also share insights
    gained from large-scale online logs based on 160,000 conversations with real-world
    users.
  address: New Orleans, Louisiana
  author:
  - first: Hao
    full: Hao Fang
    id: hao-fang
    last: Fang
  - first: Hao
    full: Hao Cheng
    id: hao-cheng
    last: Cheng
  - first: Maarten
    full: Maarten Sap
    id: maarten-sap
    last: Sap
  - first: Elizabeth
    full: Elizabeth Clark
    id: elizabeth-clark
    last: Clark
  - first: Ari
    full: Ari Holtzman
    id: ari-holtzman
    last: Holtzman
  - first: Yejin
    full: Yejin Choi
    id: yejin-choi
    last: Choi
  - first: Noah A.
    full: Noah A. Smith
    id: noah-a-smith
    last: Smith
  - first: Mari
    full: Mari Ostendorf
    id: mari-ostendorf
    last: Ostendorf
  author_string: Hao Fang, Hao Cheng, Maarten Sap, Elizabeth Clark, Ari Holtzman,
    Yejin Choi, Noah A. Smith, Mari Ostendorf
  bibkey: fang-etal-2018-sounding
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Demonstrations'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Demonstrations'
  doi: 10.18653/v1/N18-5020
  month: June
  page_first: '96'
  page_last: '100'
  pages: "96\u2013100"
  paper_id: '20'
  parent_volume_id: N18-5
  pdf: https://www.aclweb.org/anthology/N18-5020.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-5020.jpg
  title: 'Sounding Board: A User-Centric and Content-Driven Social Chatbot'
  title_html: 'Sounding Board: A User-Centric and Content-Driven Social Chatbot'
  url: https://www.aclweb.org/anthology/N18-5020
  year: '2018'
N18-6000:
  address: New Orleans, Louisiana
  author:
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  - first: Rebecca
    full: Rebecca Passonneau
    id: rebecca-j-passonneau
    last: Passonneau
  author_string: Mohit Bansal, Rebecca Passonneau
  bibkey: naacl-2018-2018-north-american-chapter-association-linguistics
  bibtype: proceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Tutorial Abstracts'
  doi: 10.18653/v1/N18-6
  month: June
  paper_id: '0'
  parent_volume_id: N18-6
  pdf: https://www.aclweb.org/anthology/N18-6000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-6000.jpg
  title: 'Proceedings of the 2018 Conference of the North American Chapter of the
    Association for Computational Linguistics: Tutorial Abstracts'
  title_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Tutorial Abstracts'
  url: https://www.aclweb.org/anthology/N18-6000
  year: '2018'
N18-6001:
  abstract: As computers and information grow a more integral part of our world, it
    is becoming more and more important for humans to be able to interact with their
    computers in complex ways. One way to do so is by programming, but the ability
    to understand and generate programming languages is a highly specialized skill.
    As a result, in the past several years there has been an increasing research interest
    in methods that focus on the intersection of programming and natural language,
    allowing users to use natural language to interact with computers in the complex
    ways that programs allow us to do. In this tutorial, we will focus on machine
    learning models of programs and natural language focused on making this goal a
    reality. First, we will discuss the similarities and differences between programming
    and natural language. Then we will discuss methods that have been designed to
    cover a variety of tasks in this field, including automatic explanation of programs
    in natural language (code-to-language), automatic generation of programs from
    natural language specifications (language-to-code), modeling the natural language
    elements of source code, and analysis of communication in collaborative programming
    communities. The tutorial will be aimed at NLP researchers and practitioners,
    aiming to describe the interesting opportunities that models at the intersection
    of natural and programming languages provide, and also how their techniques could
    provide benefit to the practice of software engineering as a whole.
  address: New Orleans, Louisiana
  attachment:
  - filename: https://vimeo.com/279154245
    type: video
    url: https://vimeo.com/279154245
  author:
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  - first: Miltiadis
    full: Miltiadis Allamanis
    id: miltiadis-allamanis
    last: Allamanis
  author_string: Graham Neubig, Miltiadis Allamanis
  bibkey: neubig-allamanis-2018-modelling
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Tutorial Abstracts'
  doi: 10.18653/v1/N18-6001
  month: June
  page_first: '1'
  page_last: '3'
  pages: "1\u20133"
  paper_id: '1'
  parent_volume_id: N18-6
  pdf: https://www.aclweb.org/anthology/N18-6001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-6001.jpg
  title: Modelling Natural Language, Programs, and their Intersection
  title_html: Modelling Natural Language, Programs, and their Intersection
  url: https://www.aclweb.org/anthology/N18-6001
  year: '2018'
N18-6002:
  abstract: Text production is a key component of many NLP applications. In data-driven
    approaches, it is used for instance, to generate dialogue turns from dialogue
    moves, to verbalise the content of Knowledge bases or to generate natural English
    sentences from rich linguistic representations, such as dependency trees or Abstract
    Meaning Representations. In text-driven methods on the other hand, text production
    is at work in sentence compression, sentence fusion, paraphrasing, sentence (or
    text) simplification, text summarisation and end-to-end dialogue systems. Following
    the success of encoder-decoder models in modeling sequence-rewriting tasks such
    as machine translation, deep learning models have successfully been applied to
    the various text production tasks. In this tutorial, we will cover the fundamentals
    and the state-of-the-art research on neural models for text production. Each text
    production task raises a slightly different communication goal (e.g, how to take
    the dialogue context into account when producing a dialogue turn; how to detect
    and merge relevant information when summarising a text; or how to produce a well-formed
    text that correctly capture the information contained in some input data in the
    case of data-to-text generation). We will outline the constraints specific to
    each subtasks and examine how the existing neural models account for them.
  address: New Orleans, Louisiana
  author:
  - first: Claire
    full: Claire Gardent
    id: claire-gardent
    last: Gardent
  - first: Shashi
    full: Shashi Narayan
    id: shashi-narayan
    last: Narayan
  author_string: Claire Gardent, Shashi Narayan
  bibkey: gardent-narayan-2018-deep
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Tutorial Abstracts'
  doi: 10.18653/v1/N18-6002
  month: June
  page_first: '4'
  page_last: '9'
  pages: "4\u20139"
  paper_id: '2'
  parent_volume_id: N18-6
  pdf: https://www.aclweb.org/anthology/N18-6002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-6002.jpg
  title: Deep Learning Approaches to Text Production
  title_html: Deep Learning Approaches to Text Production
  url: https://www.aclweb.org/anthology/N18-6002
  year: '2018'
N18-6003:
  abstract: "In today\u2019s information-based society, there is abundant knowledge\
    \ out there carried in the form of natural language texts (e.g., news articles,\
    \ social media posts, scientific publications), which spans across various domains\
    \ (e.g., corporate documents, advertisements, legal acts, medical reports), which\
    \ grows at an astonishing rate. Yet this knowledge is mostly inaccessible to computers\
    \ and overwhelming for human experts to absorb. How to turn such massive and unstructured\
    \ text data into structured, actionable knowledge, and furthermore, how to teach\
    \ machines learn to reason and complete the extracted knowledge is a grand challenge\
    \ to the research community. Traditional IE systems assume abundant human annotations\
    \ for training high quality machine learning models, which is impractical when\
    \ trying to deploy IE systems to a broad range of domains, settings and languages.\
    \ In the first part of the tutorial, we introduce how to extract structured facts\
    \ (i.e., entities and their relations for types of interest) from text corpora\
    \ to construct knowledge bases, with a focus on methods that are weakly-supervised\
    \ and domain-independent for timely knowledge base construction across various\
    \ application domains. In the second part, we introduce how to leverage other\
    \ knowledge, such as the distributional statistics of characters and words, the\
    \ annotations for other tasks and other domains, and the linguistics and problem\
    \ structures, to combat the problem of inadequate supervision, and conduct low-resource\
    \ information extraction. In the third part, we describe recent advances in knowledge\
    \ base reasoning. We start with the gentle introduction to the literature, focusing\
    \ on path-based and embedding based methods. We then describe DeepPath, a recent\
    \ attempt of using deep reinforcement learning to combine the best of both worlds\
    \ for knowledge base reasoning."
  address: New Orleans, Louisiana
  attachment:
  - filename: https://vimeo.com/279154243
    type: video
    url: https://vimeo.com/279154243
  author:
  - first: Xiang
    full: Xiang Ren
    id: xiang-ren
    last: Ren
  - first: Nanyun
    full: Nanyun Peng
    id: nanyun-peng
    last: Peng
  - first: William Yang
    full: William Yang Wang
    id: william-yang-wang
    last: Wang
  author_string: Xiang Ren, Nanyun Peng, William Yang Wang
  bibkey: ren-etal-2018-scalable
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Tutorial Abstracts'
  doi: 10.18653/v1/N18-6003
  month: June
  page_first: '10'
  page_last: '16'
  pages: "10\u201316"
  paper_id: '3'
  parent_volume_id: N18-6
  pdf: https://www.aclweb.org/anthology/N18-6003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-6003.jpg
  title: Scalable Construction and Reasoning of Massive Knowledge Bases
  title_html: Scalable Construction and Reasoning of Massive Knowledge Bases
  url: https://www.aclweb.org/anthology/N18-6003
  year: '2018'
N18-6004:
  abstract: 'Incorporating linguistic, world and common sense knowledge into AI/NLP
    systems is currently an important research area, with several open problems and
    challenges. At the same time, processing and storing this knowledge in lexical
    resources is not a straightforward task. We propose to address these complementary
    goals from two methodological perspectives: the use of NLP methods to help the
    process of constructing and enriching lexical resources and the use of lexical
    resources for improving NLP applications. This tutorial may be useful for two
    main types of audience: those working on language resources who are interested
    in becoming acquainted with automatic NLP techniques, with the end goal of speeding
    and/or easing up the process of resource curation; and on the other hand, researchers
    in NLP who would like to benefit from the knowledge of lexical resources to improve
    their systems and models.'
  address: New Orleans, Louisiana
  attachment:
  - filename: https://vimeo.com/279154260
    type: video
    url: https://vimeo.com/279154260
  author:
  - first: Jose
    full: Jose Camacho-Collados
    id: jose-camacho-collados
    last: Camacho-Collados
  - first: Luis
    full: Luis Espinosa Anke
    id: luis-espinosa-anke
    last: Espinosa Anke
  - first: Mohammad Taher
    full: Mohammad Taher Pilehvar
    id: mohammad-taher-pilehvar
    last: Pilehvar
  author_string: Jose Camacho-Collados, Luis Espinosa Anke, Mohammad Taher Pilehvar
  bibkey: camacho-collados-etal-2018-interplay
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Tutorial Abstracts'
  doi: 10.18653/v1/N18-6004
  month: June
  page_first: '17'
  page_last: '23'
  pages: "17\u201323"
  paper_id: '4'
  parent_volume_id: N18-6
  pdf: https://www.aclweb.org/anthology/N18-6004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-6004.jpg
  title: The interplay between lexical resources and Natural Language Processing
  title_html: The interplay between lexical resources and Natural Language Processing
  url: https://www.aclweb.org/anthology/N18-6004
  year: '2018'
N18-6005:
  abstract: As language technologies have become increasingly prevalent, there is
    a growing awareness that decisions we make about our data, methods, and tools
    are often tied up with their impact on people and societies. This tutorial will
    provide an overview of real-world applications of language technologies and the
    potential ethical implications associated with them. We will discuss philosophical
    foundations of ethical research along with state of the art techniques. Through
    this tutorial, we intend to provide the NLP researcher with an overview of tools
    to ensure that the data, algorithms, and models that they build are socially responsible.
    These tools will include a checklist of common pitfalls that one should avoid
    (e.g., demographic bias in data collection), as well as methods to adequately
    mitigate these issues (e.g., adjusting sampling rates or de-biasing through regularization).
    The tutorial is based on a new course on Ethics and NLP developed at Carnegie
    Mellon University.
  address: New Orleans, Louisiana
  author:
  - first: Yulia
    full: Yulia Tsvetkov
    id: yulia-tsvetkov
    last: Tsvetkov
  - first: Vinodkumar
    full: Vinodkumar Prabhakaran
    id: vinodkumar-prabhakaran
    last: Prabhakaran
  - first: Rob
    full: Rob Voigt
    id: rob-voigt
    last: Voigt
  author_string: Yulia Tsvetkov, Vinodkumar Prabhakaran, Rob Voigt
  bibkey: tsvetkov-etal-2018-socially
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Tutorial Abstracts'
  doi: 10.18653/v1/N18-6005
  month: June
  page_first: '24'
  page_last: '26'
  pages: "24\u201326"
  paper_id: '5'
  parent_volume_id: N18-6
  pdf: https://www.aclweb.org/anthology/N18-6005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-6005.jpg
  title: Socially Responsible NLP
  title_html: Socially Responsible <span class="acl-fixed-case">NLP</span>
  url: https://www.aclweb.org/anthology/N18-6005
  year: '2018'
N18-6006:
  abstract: 'Spoken Dialogue Systems (SDS) have great commercial potential as they
    promise to revolutionise the way in which humans interact with machines. The advent
    of deep learning led to substantial developments in this area of NLP research,
    and the goal of this tutorial is to familiarise the research community with the
    recent advances in what some call the most difficult problem in NLP. From a research
    perspective, the design of spoken dialogue systems provides a number of significant
    challenges, as these systems depend on: a) solving several difficult NLP and decision-making
    tasks; and b) combining these into a functional dialogue system pipeline. A key
    long-term goal of dialogue system research is to enable open-domain systems that
    can converse about arbitrary topics and assist humans with completing a wide range
    of tasks. Furthermore, such systems need to autonomously learn on-line to improve
    their performance and recover from errors using both signals from their environment
    and from implicit and explicit user feedback. While the design of such systems
    has traditionally been modular, domain and language-specific, advances in deep
    learning have alleviated many of the design problems. The main purpose of this
    tutorial is to encourage dialogue research in the NLP community by providing the
    research background, a survey of available resources, and giving key insights
    to application of state-of-the-art SDS methodology into industry-scale conversational
    AI systems. We plan to introduce researchers to the pipeline framework for modelling
    goal-oriented dialogue systems, which includes three key components: 1) Language
    Understanding; 2) Dialogue Management; and 3) Language Generation. The differences
    between goal-oriented dialogue systems and chat-bot style conversational agents
    will be explained in order to show the motivation behind the design of both, with
    the main focus on the pipeline SDS framework. For each key component, we will
    define the research problem, provide a brief literature review and introduce the
    current state-of-the-art approaches. Complementary resources (e.g. available datasets
    and toolkits) will also be discussed. Finally, future work, outstanding challenges,
    and current industry practices will be presented. All of the presented material
    will be made available online for future reference.'
  address: New Orleans, Louisiana
  attachment:
  - filename: https://vimeo.com/279154253
    type: video
    url: https://vimeo.com/279154253
  author:
  - first: Pei-Hao
    full: Pei-Hao Su
    id: pei-hao-su
    last: Su
  - first: Nikola
    full: "Nikola Mrk\u0161i\u0107"
    id: nikola-mrksic
    last: "Mrk\u0161i\u0107"
  - first: "I\xF1igo"
    full: "I\xF1igo Casanueva"
    id: inigo-casanueva
    last: Casanueva
  - first: Ivan
    full: "Ivan Vuli\u0107"
    id: ivan-vulic
    last: "Vuli\u0107"
  author_string: "Pei-Hao Su, Nikola Mrk\u0161i\u0107, I\xF1igo Casanueva, Ivan Vuli\u0107"
  bibkey: su-etal-2018-deep
  bibtype: inproceedings
  booktitle: 'Proceedings of the 2018 Conference of the North American Chapter of
    the Association for Computational Linguistics: Tutorial Abstracts'
  booktitle_html: 'Proceedings of the 2018 Conference of the North <span class="acl-fixed-case">A</span>merican
    Chapter of the Association for Computational Linguistics: Tutorial Abstracts'
  doi: 10.18653/v1/N18-6006
  month: June
  page_first: '27'
  page_last: '32'
  pages: "27\u201332"
  paper_id: '6'
  parent_volume_id: N18-6
  pdf: https://www.aclweb.org/anthology/N18-6006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/N18-6006.jpg
  title: Deep Learning for Conversational AI
  title_html: Deep Learning for Conversational <span class="acl-fixed-case">AI</span>
  url: https://www.aclweb.org/anthology/N18-6006
  year: '2018'
