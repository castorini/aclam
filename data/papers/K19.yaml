K19-1000:
  address: Hong Kong, China
  author:
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  - first: Aline
    full: Aline Villavicencio
    id: aline-villavicencio
    last: Villavicencio
  author_string: Mohit Bansal, Aline Villavicencio
  bibkey: conll-2019-natural
  bibtype: proceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  month: November
  paper_id: '0'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1000.jpg
  title: Proceedings of the 23rd Conference on Computational Natural Language Learning
    (CoNLL)
  title_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  url: https://www.aclweb.org/anthology/K19-1000
  year: '2019'
K19-1001:
  abstract: Extensive research has recently shown that recurrent neural language models
    are able to process a wide range of grammatical phenomena. How these models are
    able to perform these remarkable feats so well, however, is still an open question.
    To gain more insight into what information LSTMs base their decisions on, we propose
    a generalisation of Contextual Decomposition (GCD). In particular, this setup
    enables us to accurately distil which part of a prediction stems from semantic
    heuristics, which part truly emanates from syntactic cues and which part arise
    from the model biases themselves instead. We investigate this technique on tasks
    pertaining to syntactic agreement and co-reference resolution and discover that
    the model strongly relies on a default reasoning effect to perform these tasks.
  address: Hong Kong, China
  attachment:
  - filename: K19-1001.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-1001.Attachment.pdf
  author:
  - first: Jaap
    full: Jaap Jumelet
    id: jaap-jumelet
    last: Jumelet
  - first: Willem
    full: Willem Zuidema
    id: willem-zuidema
    last: Zuidema
  - first: Dieuwke
    full: Dieuwke Hupkes
    id: dieuwke-hupkes
    last: Hupkes
  author_string: Jaap Jumelet, Willem Zuidema, Dieuwke Hupkes
  bibkey: jumelet-etal-2019-analysing
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1001
  month: November
  page_first: '1'
  page_last: '11'
  pages: "1\u201311"
  paper_id: '1'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1001.jpg
  title: 'Analysing Neural Language Models: Contextual Decomposition Reveals Default
    Reasoning in Number and Gender Assignment'
  title_html: 'Analysing Neural Language Models: Contextual Decomposition Reveals
    Default Reasoning in Number and Gender Assignment'
  url: https://www.aclweb.org/anthology/K19-1001
  year: '2019'
K19-1002:
  abstract: Supertagging is a sequence prediction task where each word is assigned
    a piece of complex syntactic structure called a supertag. We provide a novel approach
    to multi-task learning for Tree Adjoining Grammar (TAG) supertagging by deconstructing
    these complex supertags in order to define a set of related but auxiliary sequence
    prediction tasks. Our multi-task prediction framework is trained over the exactly
    same training data used to train the original supertagger where each auxiliary
    task provides an alternative view on the original prediction task. Our experimental
    results show that our multi-task approach significantly improves TAG supertagging
    with a new state-of-the-art accuracy score of 91.39% on the Penn Treebank supertagging
    dataset.
  address: Hong Kong, China
  author:
  - first: Zhenqi
    full: Zhenqi Zhu
    id: zhenqi-zhu
    last: Zhu
  - first: Anoop
    full: Anoop Sarkar
    id: anoop-sarkar
    last: Sarkar
  author_string: Zhenqi Zhu, Anoop Sarkar
  bibkey: zhu-sarkar-2019-deconstructing
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1002
  month: November
  page_first: '12'
  page_last: '21'
  pages: "12\u201321"
  paper_id: '2'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1002.jpg
  title: Deconstructing Supertagging into Multi-Task Sequence Prediction
  title_html: Deconstructing Supertagging into Multi-Task Sequence Prediction
  url: https://www.aclweb.org/anthology/K19-1002
  year: '2019'
K19-1003:
  abstract: We present a method for applying a neural network trained on one (resource-rich)
    language for a given task to other (resource-poor) languages. We accomplish this
    by inducing a mapping from pre-trained cross-lingual word embeddings to the embedding
    layer of the neural network trained on the resource-rich language. To perform
    element-wise cross-task embedding projection, we invent locally linear mapping
    which assumes and preserves the local topology across the semantic spaces before
    and after the projection. Experimental results on topic classification task and
    sentiment analysis task showed that the fully task-specific multilingual model
    obtained using our method outperformed the existing multilingual models with embedding
    layers fixed to pre-trained cross-lingual word embeddings.
  address: Hong Kong, China
  author:
  - first: Jin
    full: Jin Sakuma
    id: jin-sakuma
    last: Sakuma
  - first: Naoki
    full: Naoki Yoshinaga
    id: naoki-yoshinaga
    last: Yoshinaga
  author_string: Jin Sakuma, Naoki Yoshinaga
  bibkey: sakuma-yoshinaga-2019-multilingual
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1003
  month: November
  page_first: '22'
  page_last: '32'
  pages: "22\u201332"
  paper_id: '3'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1003.jpg
  title: Multilingual Model Using Cross-Task Embedding Projection
  title_html: Multilingual Model Using Cross-Task Embedding Projection
  url: https://www.aclweb.org/anthology/K19-1003
  year: '2019'
K19-1004:
  abstract: In this paper, we present a thorough investigation on methods that align
    pre-trained contextualized embeddings into shared cross-lingual context-aware
    embedding space, providing strong reference benchmarks for future context-aware
    crosslingual models. We propose a novel and challenging task, Bilingual Token-level
    Sense Retrieval (BTSR). It specifically evaluates the accurate alignment of words
    with the same meaning in cross-lingual non-parallel contexts, currently not evaluated
    by existing tasks such as Bilingual Contextual Word Similarity and Sentence Retrieval.
    We show how the proposed BTSR task highlights the merits of different alignment
    methods. In particular, we find that using context average type-level alignment
    is effective in transferring monolingual contextualized embeddings cross-lingually
    especially in non-parallel contexts, and at the same time improves the monolingual
    space. Furthermore, aligning independently trained models yields better performance
    than aligning multilingual embeddings with shared vocabulary.
  address: Hong Kong, China
  author:
  - first: Qianchu
    full: Qianchu Liu
    id: qianchu-liu
    last: Liu
  - first: Diana
    full: Diana McCarthy
    id: diana-mccarthy
    last: McCarthy
  - first: Ivan
    full: "Ivan Vuli\u0107"
    id: ivan-vulic
    last: "Vuli\u0107"
  - first: Anna
    full: Anna Korhonen
    id: anna-korhonen
    last: Korhonen
  author_string: "Qianchu Liu, Diana McCarthy, Ivan Vuli\u0107, Anna Korhonen"
  bibkey: liu-etal-2019-investigating
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1004
  month: November
  page_first: '33'
  page_last: '43'
  pages: "33\u201343"
  paper_id: '4'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1004.jpg
  title: Investigating Cross-Lingual Alignment Methods for Contextualized Embeddings
    with Token-Level Evaluation
  title_html: Investigating Cross-Lingual Alignment Methods for Contextualized Embeddings
    with Token-Level Evaluation
  url: https://www.aclweb.org/anthology/K19-1004
  year: '2019'
K19-1005:
  abstract: Producing diverse paraphrases of a sentence is a challenging task. Natural
    paraphrase corpora are scarce and limited, while existing large-scale resources
    are automatically generated via back-translation and rely on beam search, which
    tends to lack diversity. We describe ParaBank 2, a new resource that contains
    multiple diverse sentential paraphrases, produced from a bilingual corpus using
    negative constraints, inference sampling, and clustering.We show that ParaBank
    2 significantly surpasses prior work in both lexical and syntactic diversity while
    being meaning-preserving, as measured by human judgments and standardized metrics.
    Further, we illustrate how such paraphrastic resources may be used to refine contextualized
    encoders, leading to improvements in downstream tasks.
  address: Hong Kong, China
  author:
  - first: J. Edward
    full: J. Edward Hu
    id: j-edward-hu
    last: Hu
  - first: Abhinav
    full: Abhinav Singh
    id: abhinav-singh
    last: Singh
  - first: Nils
    full: Nils Holzenberger
    id: nils-holzenberger
    last: Holzenberger
  - first: Matt
    full: Matt Post
    id: matt-post
    last: Post
  - first: Benjamin
    full: Benjamin Van Durme
    id: benjamin-van-durme
    last: Van Durme
  author_string: J. Edward Hu, Abhinav Singh, Nils Holzenberger, Matt Post, Benjamin
    Van Durme
  bibkey: hu-etal-2019-large
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1005
  month: November
  page_first: '44'
  page_last: '54'
  pages: "44\u201354"
  paper_id: '5'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1005.jpg
  title: Large-Scale, Diverse, Paraphrastic Bitexts via Sampling and Clustering
  title_html: Large-Scale, Diverse, Paraphrastic Bitexts via Sampling and Clustering
  url: https://www.aclweb.org/anthology/K19-1005
  year: '2019'
K19-1006:
  abstract: "Systems that can associate images with their spoken audio captions are\
    \ an important step towards visually grounded language learning. We describe a\
    \ scalable method to automatically generate diverse audio for image captioning\
    \ datasets. This supports pretraining deep networks for encoding both audio and\
    \ images, which we do via a dual encoder that learns to align latent representations\
    \ from both modalities. We show that a masked margin softmax loss for such models\
    \ is superior to the standard triplet loss. We fine-tune these models on the Flickr8k\
    \ Audio Captions Corpus and obtain state-of-the-art results\u2014improving recall\
    \ in the top 10 from 29.6% to 49.5%. We also obtain human ratings on retrieval\
    \ outputs to better assess the impact of incidentally matching image-caption pairs\
    \ that were not associated in the data, finding that automatic evaluation substantially\
    \ underestimates the quality of the retrieved results."
  address: Hong Kong, China
  author:
  - first: Gabriel
    full: Gabriel Ilharco
    id: gabriel-ilharco
    last: Ilharco
  - first: Yuan
    full: Yuan Zhang
    id: yuan-zhang
    last: Zhang
  - first: Jason
    full: Jason Baldridge
    id: jason-baldridge
    last: Baldridge
  author_string: Gabriel Ilharco, Yuan Zhang, Jason Baldridge
  bibkey: ilharco-etal-2019-large
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1006
  month: November
  page_first: '55'
  page_last: '65'
  pages: "55\u201365"
  paper_id: '6'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1006.jpg
  title: Large-Scale Representation Learning from Visually Grounded Untranscribed
    Speech
  title_html: Large-Scale Representation Learning from Visually Grounded Untranscribed
    Speech
  url: https://www.aclweb.org/anthology/K19-1006
  year: '2019'
K19-1007:
  abstract: "Neural language models (LMs) perform well on tasks that require sensitivity\
    \ to syntactic structure. Drawing on the syntactic priming paradigm from psycholinguistics,\
    \ we propose a novel technique to analyze the representations that enable such\
    \ success. By establishing a gradient similarity metric between structures, this\
    \ technique allows us to reconstruct the organization of the LMs\u2019 syntactic\
    \ representational space. We use this technique to demonstrate that LSTM LMs\u2019\
    \ representations of different types of sentences with relative clauses are organized\
    \ hierarchically in a linguistically interpretable manner, suggesting that the\
    \ LMs track abstract properties of the sentence."
  address: Hong Kong, China
  author:
  - first: Grusha
    full: Grusha Prasad
    id: grusha-prasad
    last: Prasad
  - first: Marten
    full: Marten van Schijndel
    id: marten-van-schijndel
    last: van Schijndel
  - first: Tal
    full: Tal Linzen
    id: tal-linzen
    last: Linzen
  author_string: Grusha Prasad, Marten van Schijndel, Tal Linzen
  bibkey: prasad-etal-2019-using
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1007
  month: November
  page_first: '66'
  page_last: '76'
  pages: "66\u201376"
  paper_id: '7'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1007.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1007.jpg
  title: Using Priming to Uncover the Organization of Syntactic Representations in
    Neural Language Models
  title_html: Using Priming to Uncover the Organization of Syntactic Representations
    in Neural Language Models
  url: https://www.aclweb.org/anthology/K19-1007
  year: '2019'
K19-1008:
  abstract: Computational research on error detection in second language speakers
    has mainly addressed clear grammatical anomalies typical to learners at the beginner-to-intermediate
    level. We focus instead on acquisition of subtle semantic nuances of English indefinite
    pronouns by non-native speakers at varying levels of proficiency. We first lay
    out theoretical, linguistically motivated hypotheses, and supporting empirical
    evidence, on the nature of the challenges posed by indefinite pronouns to English
    learners. We then suggest and evaluate an automatic approach for detection of
    atypical usage patterns, demonstrating that deep learning architectures are promising
    for this task involving nuanced semantic anomalies.
  address: Hong Kong, China
  attachment:
  - filename: K19-1008.Supplementary_Material.zip
    type: supplementary-material
    url: https://www.aclweb.org/anthology/attachments/K19-1008.Supplementary_Material.zip
  author:
  - first: Ella
    full: Ella Rabinovich
    id: ella-rabinovich
    last: Rabinovich
  - first: Julia
    full: Julia Watson
    id: julia-watson
    last: Watson
  - first: Barend
    full: Barend Beekhuizen
    id: barend-beekhuizen
    last: Beekhuizen
  - first: Suzanne
    full: Suzanne Stevenson
    id: suzanne-stevenson
    last: Stevenson
  author_string: Ella Rabinovich, Julia Watson, Barend Beekhuizen, Suzanne Stevenson
  bibkey: rabinovich-etal-2019-say
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1008
  month: November
  page_first: '77'
  page_last: '86'
  pages: "77\u201386"
  paper_id: '8'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1008.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1008.jpg
  title: 'Say Anything: Automatic Semantic Infelicity Detection in L2 English Indefinite
    Pronouns'
  title_html: 'Say Anything: Automatic Semantic Infelicity Detection in <span class="acl-fixed-case">L</span>2
    <span class="acl-fixed-case">E</span>nglish Indefinite Pronouns'
  url: https://www.aclweb.org/anthology/K19-1008
  year: '2019'
K19-1009:
  abstract: "Image captioning models are usually evaluated on their ability to describe\
    \ a held-out set of images, not on their ability to generalize to unseen concepts.\
    \ We study the problem of compositional generalization, which measures how well\
    \ a model composes unseen combinations of concepts when describing images. State-of-the-art\
    \ image captioning models show poor generalization performance on this task. We\
    \ propose a multi-task model to address the poor performance, that combines caption\
    \ generation and image\u2013sentence ranking, and uses a decoding mechanism that\
    \ re-ranks the captions according their similarity to the image. This model is\
    \ substantially better at generalizing to unseen combinations of concepts compared\
    \ to state-of-the-art captioning models."
  address: Hong Kong, China
  attachment:
  - filename: K19-1009.Supplementary_Material.pdf
    type: supplementary-material
    url: https://www.aclweb.org/anthology/attachments/K19-1009.Supplementary_Material.pdf
  author:
  - first: Mitja
    full: Mitja Nikolaus
    id: mitja-nikolaus
    last: Nikolaus
  - first: Mostafa
    full: Mostafa Abdou
    id: mostafa-abdou
    last: Abdou
  - first: Matthew
    full: Matthew Lamm
    id: matthew-lamm
    last: Lamm
  - first: Rahul
    full: Rahul Aralikatte
    id: rahul-aralikatte
    last: Aralikatte
  - first: Desmond
    full: Desmond Elliott
    id: desmond-elliott
    last: Elliott
  author_string: Mitja Nikolaus, Mostafa Abdou, Matthew Lamm, Rahul Aralikatte, Desmond
    Elliott
  bibkey: nikolaus-etal-2019-compositional
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1009
  month: November
  page_first: '87'
  page_last: '98'
  pages: "87\u201398"
  paper_id: '9'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1009.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1009.jpg
  title: Compositional Generalization in Image Captioning
  title_html: Compositional Generalization in Image Captioning
  url: https://www.aclweb.org/anthology/K19-1009
  year: '2019'
K19-1010:
  abstract: 'We introduce a new embedding model to represent movie characters and
    their interactions in a dialogue by encoding in the same representation the language
    used by these characters as well as information about the other participants in
    the dialogue. We evaluate the performance of these new character embeddings on
    two tasks: (1) character relatedness, using a dataset we introduce consisting
    of a dense character interaction matrix for 4,378 unique character pairs over
    22 hours of dialogue from eighteen movies; and (2) character relation classification,
    for fine- and coarse-grained relations, as well as sentiment relations. Our experiments
    show that our model significantly outperforms the traditional Word2Vec continuous
    bag-of-words and skip-gram models, demonstrating the effectiveness of the character
    embeddings we introduce. We further show how these embeddings can be used in conjunction
    with a visual question answering system to improve over previous results.'
  address: Hong Kong, China
  author:
  - first: Mahmoud
    full: Mahmoud Azab
    id: mahmoud-azab
    last: Azab
  - first: Noriyuki
    full: Noriyuki Kojima
    id: noriyuki-kojima
    last: Kojima
  - first: Jia
    full: Jia Deng
    id: jia-deng
    last: Deng
  - first: Rada
    full: Rada Mihalcea
    id: rada-mihalcea
    last: Mihalcea
  author_string: Mahmoud Azab, Noriyuki Kojima, Jia Deng, Rada Mihalcea
  bibkey: azab-etal-2019-representing
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1010
  month: November
  page_first: '99'
  page_last: '109'
  pages: "99\u2013109"
  paper_id: '10'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1010.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1010.jpg
  title: Representing Movie Characters in Dialogues
  title_html: Representing Movie Characters in Dialogues
  url: https://www.aclweb.org/anthology/K19-1010
  year: '2019'
K19-1011:
  abstract: Research on the bilingual lexicon has uncovered fascinating interactions
    between the lexicons of the native language and of the second language in bilingual
    speakers. In particular, it has been found that the lexicon of the underlying
    native language affects the organisation of the second language. In the spirit
    of interpreting current distributed representations, this paper investigates two
    models of cross-lingual word embeddings, comparing them to the shared-translation
    effect and the cross-lingual coactivation effects of false and true friends (cognates)
    found in humans. We find that the similarity structure of the cross-lingual word
    embeddings space yields the same effects as the human bilingual lexicon.
  address: Hong Kong, China
  attachment:
  - filename: K19-1011.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-1011.Attachment.zip
  author:
  - first: Paola
    full: Paola Merlo
    id: paola-merlo
    last: Merlo
  - first: Maria
    full: Maria Andueza Rodriguez
    id: maria-andueza-rodriguez
    last: Andueza Rodriguez
  author_string: Paola Merlo, Maria Andueza Rodriguez
  bibkey: merlo-andueza-rodriguez-2019-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1011
  month: November
  page_first: '110'
  page_last: '120'
  pages: "110\u2013120"
  paper_id: '11'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1011.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1011.jpg
  title: Cross-Lingual Word Embeddings and the Structure of the Human Bilingual Lexicon
  title_html: Cross-Lingual Word Embeddings and the Structure of the Human Bilingual
    Lexicon
  url: https://www.aclweb.org/anthology/K19-1011
  year: '2019'
K19-1012:
  abstract: "We propose algorithms to train production-quality n-gram language models\
    \ using federated learning. Federated learning is a distributed computation platform\
    \ that can be used to train global models for portable devices such as smart phones.\
    \ Federated learning is especially relevant for applications handling privacy-sensitive\
    \ data, such as virtual keyboards, because training is performed without the users\u2019\
    \ data ever leaving their devices. While the principles of federated learning\
    \ are fairly generic, its methodology assumes that the underlying models are neural\
    \ networks. However, virtual keyboards are typically powered by n-gram language\
    \ models for latency reasons. We propose to train a recurrent neural network language\
    \ model using the decentralized FederatedAveraging algorithm and to approximate\
    \ this federated model server-side with an n-gram model that can be deployed to\
    \ devices for fast inference. Our technical contributions include ways of handling\
    \ large vocabularies, algorithms to correct capitalization errors in user data,\
    \ and efficient finite state transducer algorithms to convert word language models\
    \ to word-piece language models and vice versa. The n-gram language models trained\
    \ with federated learning are compared to n-grams trained with traditional server-based\
    \ algorithms using A/B tests on tens of millions of users of a virtual keyboard.\
    \ Results are presented for two languages, American English and Brazilian Portuguese.\
    \ This work demonstrates that high-quality n-gram language models can be trained\
    \ directly on client mobile devices without sensitive training data ever leaving\
    \ the devices."
  address: Hong Kong, China
  author:
  - first: Mingqing
    full: Mingqing Chen
    id: mingqing-chen
    last: Chen
  - first: Ananda Theertha
    full: Ananda Theertha Suresh
    id: ananda-theertha-suresh
    last: Suresh
  - first: Rajiv
    full: Rajiv Mathews
    id: rajiv-mathews
    last: Mathews
  - first: Adeline
    full: Adeline Wong
    id: adeline-wong
    last: Wong
  - first: Cyril
    full: Cyril Allauzen
    id: cyril-allauzen
    last: Allauzen
  - first: "Fran\xE7oise"
    full: "Fran\xE7oise Beaufays"
    id: francoise-beaufays
    last: Beaufays
  - first: Michael
    full: Michael Riley
    id: michael-riley
    last: Riley
  author_string: "Mingqing Chen, Ananda Theertha Suresh, Rajiv Mathews, Adeline Wong,\
    \ Cyril Allauzen, Fran\xE7oise Beaufays, Michael Riley"
  bibkey: chen-etal-2019-federated
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1012
  month: November
  page_first: '121'
  page_last: '130'
  pages: "121\u2013130"
  paper_id: '12'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1012.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1012.jpg
  title: Federated Learning of N-Gram Language Models
  title_html: Federated Learning of N-Gram Language Models
  url: https://www.aclweb.org/anthology/K19-1012
  year: '2019'
K19-1013:
  abstract: "Conceptual spaces are geometric representations of meaning that were\
    \ proposed by G \u0308ardenfors (2000). They share many similarities with the\
    \ vector space embeddings that are commonly used in natural language processing.\
    \ However, rather than representing entities in a single vector space, conceptual\
    \ spaces are usually decomposed into several facets, each of which is then modelled\
    \ as a relatively low dimensional vector space. Unfortunately, the problem of\
    \ learning such conceptual spaces has thus far only received limited attention.\
    \ To address this gap, we analyze how, and to what extent, a given vector space\
    \ embedding can be decomposed into meaningful facets in an unsupervised fashion.\
    \ While this problem is highly challenging, we show that useful facets can be\
    \ discovered by relying on word embeddings to group semantically related features."
  address: Hong Kong, China
  author:
  - first: Rana
    full: Rana Alshaikh
    id: rana-alshaikh
    last: Alshaikh
  - first: Zied
    full: Zied Bouraoui
    id: zied-bouraoui
    last: Bouraoui
  - first: Steven
    full: Steven Schockaert
    id: steven-schockaert
    last: Schockaert
  author_string: Rana Alshaikh, Zied Bouraoui, Steven Schockaert
  bibkey: alshaikh-etal-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1013
  month: November
  page_first: '131'
  page_last: '139'
  pages: "131\u2013139"
  paper_id: '13'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1013.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1013.jpg
  title: Learning Conceptual Spaces with Disentangled Facets
  title_html: Learning Conceptual Spaces with Disentangled Facets
  url: https://www.aclweb.org/anthology/K19-1013
  year: '2019'
K19-1014:
  abstract: "We conduct a manual error analysis of the CoNLL-SIGMORPHON Shared Task\
    \ on Morphological Reinflection. This task involves natural language generation:\
    \ systems are given a word in citation form (e.g., hug) and asked to produce the\
    \ corresponding inflected form (e.g., the simple past hugged). We propose an error\
    \ taxonomy and use it to annotate errors made by the top two systems across twelve\
    \ languages. Many of the observed errors are related to inflectional patterns\
    \ sensitive to inherent linguistic properties such as animacy or affect; many\
    \ others are failures to predict truly unpredictable inflectional behaviors. We\
    \ also find nearly one quarter of the residual \u201Cerrors\u201D reflect errors\
    \ in the gold data."
  address: Hong Kong, China
  author:
  - first: Kyle
    full: Kyle Gorman
    id: kyle-gorman
    last: Gorman
  - first: Arya D.
    full: Arya D. McCarthy
    id: arya-d-mccarthy
    last: McCarthy
  - first: Ryan
    full: Ryan Cotterell
    id: ryan-cotterell
    last: Cotterell
  - first: Ekaterina
    full: Ekaterina Vylomova
    id: ekaterina-vylomova
    last: Vylomova
  - first: Miikka
    full: Miikka Silfverberg
    id: miikka-silfverberg
    last: Silfverberg
  - first: Magdalena
    full: Magdalena Markowska
    id: magdalena-markowska
    last: Markowska
  author_string: Kyle Gorman, Arya D. McCarthy, Ryan Cotterell, Ekaterina Vylomova,
    Miikka Silfverberg, Magdalena Markowska
  bibkey: gorman-etal-2019-weird
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1014
  month: November
  page_first: '140'
  page_last: '151'
  pages: "140\u2013151"
  paper_id: '14'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1014.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1014.jpg
  title: 'Weird Inflects but OK: Making Sense of Morphological Generation Errors'
  title_html: 'Weird Inflects but <span class="acl-fixed-case">OK</span>: Making Sense
    of Morphological Generation Errors'
  url: https://www.aclweb.org/anthology/K19-1014
  year: '2019'
K19-1015:
  abstract: Bilingual word embeddings have been widely used to capture the correspondence
    of lexical semantics in different human languages. However, the cross-lingual
    correspondence between sentences and words is less studied, despite that this
    correspondence can significantly benefit many applications such as crosslingual
    semantic search and textual inference. To bridge this gap, we propose a neural
    embedding model that leverages bilingual dictionaries. The proposed model is trained
    to map the lexical definitions to the cross-lingual target words, for which we
    explore with different sentence encoding techniques. To enhance the learning process
    on limited resources, our model adopts several critical learning strategies, including
    multi-task learning on different bridges of languages, and joint learning of the
    dictionary model with a bilingual word embedding model. We conduct experiments
    on two new tasks. In the cross-lingual reverse dictionary retrieval task, we demonstrate
    that our model is capable of comprehending bilingual concepts based on descriptions,
    and the proposed learning strategies are effective. In the bilingual paraphrase
    identification task, we show that our model effectively associates sentences in
    different languages via a shared embedding space, and outperforms existing approaches
    in identifying bilingual paraphrases.
  address: Hong Kong, China
  author:
  - first: Muhao
    full: Muhao Chen
    id: muhao-chen
    last: Chen
  - first: Yingtao
    full: Yingtao Tian
    id: yingtao-tian
    last: Tian
  - first: Haochen
    full: Haochen Chen
    id: haochen-chen
    last: Chen
  - first: Kai-Wei
    full: Kai-Wei Chang
    id: kai-wei-chang
    last: Chang
  - first: Steven
    full: Steven Skiena
    id: steven-skiena
    last: Skiena
  - first: Carlo
    full: Carlo Zaniolo
    id: carlo-zaniolo
    last: Zaniolo
  author_string: Muhao Chen, Yingtao Tian, Haochen Chen, Kai-Wei Chang, Steven Skiena,
    Carlo Zaniolo
  bibkey: chen-etal-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1015
  month: November
  page_first: '152'
  page_last: '162'
  pages: "152\u2013162"
  paper_id: '15'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1015.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1015.jpg
  title: Learning to Represent Bilingual Dictionaries
  title_html: Learning to Represent Bilingual Dictionaries
  url: https://www.aclweb.org/anthology/K19-1015
  year: '2019'
K19-1016:
  abstract: We propose a method called reverse mapping bytepair encoding, which maps
    named-entity information and other word-level linguistic features back to subwords
    during the encoding procedure of bytepair encoding (BPE). We employ this method
    to the Generative Pre-trained Transformer (OpenAI GPT) by adding a weighted linear
    layer after the embedding layer. We also propose a new model architecture named
    as the multi-channel separate transformer to employ a training process without
    parameter-sharing. Evaluation on Stories Cloze, RTE, SciTail and SST-2 datasets
    demonstrates the effectiveness of our approach.
  address: Hong Kong, China
  author:
  - first: Chaodong
    full: Chaodong Tong
    id: chaodong-tong
    last: Tong
  - first: Huailiang
    full: Huailiang Peng
    id: huailiang-peng
    last: Peng
  - first: Qiong
    full: Qiong Dai
    id: qiong-dai
    last: Dai
  - first: Lei
    full: Lei Jiang
    id: lei-jiang
    last: Jiang
  - first: Jianghua
    full: Jianghua Huang
    id: jianghua-huang
    last: Huang
  author_string: Chaodong Tong, Huailiang Peng, Qiong Dai, Lei Jiang, Jianghua Huang
  bibkey: tong-etal-2019-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1016
  month: November
  page_first: '163'
  page_last: '173'
  pages: "163\u2013173"
  paper_id: '16'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1016.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1016.jpg
  title: Improving Natural Language Understanding by Reverse Mapping Bytepair Encoding
  title_html: Improving Natural Language Understanding by Reverse Mapping Bytepair
    Encoding
  url: https://www.aclweb.org/anthology/K19-1016
  year: '2019'
K19-1017:
  abstract: Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport,
    2013) is a typologically-informed, broad-coverage semantic annotation scheme that
    describes coarse-grained predicate-argument structure but currently lacks semantic
    roles. We argue that lexicon-free annotation of the semantic roles marked by prepositions,
    as formulated by Schneider et al. (2018), is complementary and suitable for integration
    within UCCA. We show empirically for English that the schemes, though annotated
    independently, are compatible and can be combined in a single semantic graph.
    A comparison of several approaches to parsing the integrated representation lays
    the groundwork for future research on this task.
  address: Hong Kong, China
  attachment:
  - filename: K19-1017.Supplementary_Material.pdf
    type: supplementary-material
    url: https://www.aclweb.org/anthology/attachments/K19-1017.Supplementary_Material.pdf
  - filename: K19-1017.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-1017.Attachment.pdf
  author:
  - first: Jakob
    full: Jakob Prange
    id: jakob-prange
    last: Prange
  - first: Nathan
    full: Nathan Schneider
    id: nathan-schneider
    last: Schneider
  - first: Omri
    full: Omri Abend
    id: omri-abend
    last: Abend
  author_string: Jakob Prange, Nathan Schneider, Omri Abend
  bibkey: prange-etal-2019-made
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1017
  month: November
  page_first: '174'
  page_last: '185'
  pages: "174\u2013185"
  paper_id: '17'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1017.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1017.jpg
  title: 'Made for Each Other: Broad-Coverage Semantic Structures Meet Preposition
    Supersenses'
  title_html: 'Made for Each Other: Broad-Coverage Semantic Structures Meet Preposition
    Supersenses'
  url: https://www.aclweb.org/anthology/K19-1017
  year: '2019'
K19-1018:
  abstract: "Though languages can evolve slowly, they can also react strongly to dramatic\
    \ world events. By studying the connection between words and events, it is possible\
    \ to identify which events change our vocabulary and in what way. In this work,\
    \ we tackle the task of creating timelines - records of historical \u201Cturning\
    \ points\u201D, represented by either words or events, to understand the dynamics\
    \ of a target word. Our approach identifies these points by leveraging both static\
    \ and time-varying word embeddings to measure the influence of words and events.\
    \ In addition to quantifying changes, we show how our technique can help isolate\
    \ semantic changes. Our qualitative and quantitative evaluations show that we\
    \ are able to capture this semantic change and event influence."
  address: Hong Kong, China
  attachment:
  - filename: K19-1018.Supplementary_Material.zip
    type: supplementary-material
    url: https://www.aclweb.org/anthology/attachments/K19-1018.Supplementary_Material.zip
  author:
  - first: Guy D.
    full: Guy D. Rosin
    id: guy-d-rosin
    last: Rosin
  - first: Kira
    full: Kira Radinsky
    id: kira-radinsky
    last: Radinsky
  author_string: Guy D. Rosin, Kira Radinsky
  bibkey: rosin-radinsky-2019-generating
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1018
  month: November
  page_first: '186'
  page_last: '195'
  pages: "186\u2013195"
  paper_id: '18'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1018.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1018.jpg
  title: Generating Timelines by Modeling Semantic Change
  title_html: Generating Timelines by Modeling Semantic Change
  url: https://www.aclweb.org/anthology/K19-1018
  year: '2019'
K19-1019:
  abstract: "Phenomenon-specific \u201Cadversarial\u201D datasets have been recently\
    \ designed to perform targeted stress-tests for particular inference types. Recent\
    \ work (Liu et al., 2019a) proposed that such datasets can be utilized for training\
    \ NLI and other types of models, often allowing to learn the phenomenon in focus\
    \ and improve on the challenge dataset, indicating a \u201Cblind spot\u201D in\
    \ the original training data. Yet, although a model can improve in such a training\
    \ process, it might still be vulnerable to other challenge datasets targeting\
    \ the same phenomenon but drawn from a different distribution, such as having\
    \ a different syntactic complexity level. In this work, we extend this method\
    \ to drive conclusions about a model\u2019s ability to learn and generalize a\
    \ target phenomenon rather than to \u201Clearn\u201D a dataset, by controlling\
    \ additional aspects in the adversarial datasets. We demonstrate our approach\
    \ on two inference phenomena \u2013 dative alternation and numerical reasoning,\
    \ elaborating, and in some cases contradicting, the results of Liu et al.. Our\
    \ methodology enables building better challenge datasets for creating more robust\
    \ models, and may yield better model understanding and subsequent overarching\
    \ improvements."
  address: Hong Kong, China
  author:
  - first: Ohad
    full: Ohad Rozen
    id: ohad-rozen
    last: Rozen
  - first: Vered
    full: Vered Shwartz
    id: vered-shwartz
    last: Shwartz
  - first: Roee
    full: Roee Aharoni
    id: roee-aharoni
    last: Aharoni
  - first: Ido
    full: Ido Dagan
    id: ido-dagan
    last: Dagan
  author_string: Ohad Rozen, Vered Shwartz, Roee Aharoni, Ido Dagan
  bibkey: rozen-etal-2019-diversify
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1019
  month: November
  page_first: '196'
  page_last: '205'
  pages: "196\u2013205"
  paper_id: '19'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1019.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1019.jpg
  title: 'Diversify Your Datasets: Analyzing Generalization via Controlled Variance
    in Adversarial Datasets'
  title_html: 'Diversify Your Datasets: Analyzing Generalization via Controlled Variance
    in Adversarial Datasets'
  url: https://www.aclweb.org/anthology/K19-1019
  year: '2019'
K19-1020:
  abstract: "We present a fully unsupervised crosslingual semantic textual similarity\
    \ (STS) metric, based on contextual embeddings extracted from BERT \u2013 Bidirectional\
    \ Encoder Representations from Transformers (Devlin et al., 2019). The goal of\
    \ crosslingual STS is to measure to what degree two segments of text in different\
    \ languages express the same meaning. Not only is it a key task in crosslingual\
    \ natural language understanding (XLU), it is also particularly useful for identifying\
    \ parallel resources for training and evaluating downstream multilingual natural\
    \ language processing (NLP) applications, such as machine translation. Most previous\
    \ crosslingual STS methods relied heavily on existing parallel resources, thus\
    \ leading to a circular dependency problem. With the advent of massively multilingual\
    \ context representation models such as BERT, which are trained on the concatenation\
    \ of non-parallel data from each language, we show that the deadlock around parallel\
    \ resources can be broken. We perform intrinsic evaluations on crosslingual STS\
    \ data sets and extrinsic evaluations on parallel corpus filtering and human translation\
    \ equivalence assessment tasks. Our results show that the unsupervised crosslingual\
    \ STS metric using BERT without fine-tuning achieves performance on par with supervised\
    \ or weakly supervised approaches."
  address: Hong Kong, China
  author:
  - first: Chi-kiu
    full: Chi-kiu Lo
    id: chi-kiu-lo
    last: Lo
  - first: Michel
    full: Michel Simard
    id: michel-simard
    last: Simard
  author_string: Chi-kiu Lo, Michel Simard
  bibkey: lo-simard-2019-fully
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1020
  month: November
  page_first: '206'
  page_last: '215'
  pages: "206\u2013215"
  paper_id: '20'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1020.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1020.jpg
  title: Fully Unsupervised Crosslingual Semantic Textual Similarity Metric Based
    on BERT for Identifying Parallel Data
  title_html: Fully Unsupervised Crosslingual Semantic Textual Similarity Metric Based
    on <span class="acl-fixed-case">BERT</span> for Identifying Parallel Data
  url: https://www.aclweb.org/anthology/K19-1020
  year: '2019'
K19-1021:
  abstract: 'Recent work has validated the importance of subword information for word
    representation learning. Since subwords increase parameter sharing ability in
    neural models, their value should be even more pronounced in low-data regimes.
    In this work, we therefore provide a comprehensive analysis focused on the usefulness
    of subwords for word representation learning in truly low-resource scenarios and
    for three representative morphological tasks: fine-grained entity typing, morphological
    tagging, and named entity recognition. We conduct a systematic study that spans
    several dimensions of comparison: 1) type of data scarcity which can stem from
    the lack of task-specific training data, or even from the lack of unannotated
    data required to train word embeddings, or both; 2) language type by working with
    a sample of 16 typologically diverse languages including some truly low-resource
    ones (e.g. Rusyn, Buryat, and Zulu); 3) the choice of the subword-informed word
    representation method. Our main results show that subword-informed models are
    universally useful across all language types, with large gains over subword-agnostic
    embeddings. They also suggest that the effective use of subwords largely depends
    on the language (type) and the task at hand, as well as on the amount of available
    data for training the embeddings and task-based models, where having sufficient
    in-task data is a more critical requirement.'
  address: Hong Kong, China
  author:
  - first: Yi
    full: Yi Zhu
    id: yi-zhu
    last: Zhu
  - first: Benjamin
    full: Benjamin Heinzerling
    id: benjamin-heinzerling
    last: Heinzerling
  - first: Ivan
    full: "Ivan Vuli\u0107"
    id: ivan-vulic
    last: "Vuli\u0107"
  - first: Michael
    full: Michael Strube
    id: michael-strube
    last: Strube
  - first: Roi
    full: Roi Reichart
    id: roi-reichart
    last: Reichart
  - first: Anna
    full: Anna Korhonen
    id: anna-korhonen
    last: Korhonen
  author_string: "Yi Zhu, Benjamin Heinzerling, Ivan Vuli\u0107, Michael Strube, Roi\
    \ Reichart, Anna Korhonen"
  bibkey: zhu-etal-2019-importance
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1021
  month: November
  page_first: '216'
  page_last: '226'
  pages: "216\u2013226"
  paper_id: '21'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1021.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1021.jpg
  title: On the Importance of Subword Information for Morphological Tasks in Truly
    Low-Resource Languages
  title_html: On the Importance of Subword Information for Morphological Tasks in
    Truly Low-Resource Languages
  url: https://www.aclweb.org/anthology/K19-1021
  year: '2019'
K19-1022:
  abstract: 'Recurrent neural network grammars generate sentences using phrase-structure
    syntax and perform very well on both parsing and language modeling. To explore
    whether generative dependency models are similarly effective, we propose two new
    generative models of dependency syntax. Both models use recurrent neural nets
    to avoid making explicit independence assumptions, but they differ in the order
    used to construct the trees: one builds the tree bottom-up and the other top-down,
    which profoundly changes the estimation problem faced by the learner. We evaluate
    the two models on three typologically different languages: English, Arabic, and
    Japanese. While both generative models improve parsing performance over a discriminative
    baseline, they are significantly less effective than non-syntactic LSTM language
    models. Surprisingly, little difference between the construction orders is observed
    for either parsing or language modeling.'
  address: Hong Kong, China
  author:
  - first: Austin
    full: Austin Matthews
    id: austin-matthews
    last: Matthews
  - first: Graham
    full: Graham Neubig
    id: graham-neubig
    last: Neubig
  - first: Chris
    full: Chris Dyer
    id: chris-dyer
    last: Dyer
  author_string: Austin Matthews, Graham Neubig, Chris Dyer
  bibkey: matthews-etal-2019-comparing
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1022
  month: November
  page_first: '227'
  page_last: '237'
  pages: "227\u2013237"
  paper_id: '22'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1022.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1022.jpg
  title: Comparing Top-Down and Bottom-Up Neural Generative Dependency Models
  title_html: Comparing Top-Down and Bottom-Up Neural Generative Dependency Models
  url: https://www.aclweb.org/anthology/K19-1022
  year: '2019'
K19-1023:
  abstract: We present a new method for transition-based parsing where a solution
    is a pair made of a dependency tree and a derivation graph describing the construction
    of the former. From this representation we are able to derive an efficient parsing
    algorithm and design a neural network that learns vertex representations and arc
    scores. Experimentally, although we only train via local classifiers, our approach
    improves over previous arc-hybrid systems and reach state-of-the-art parsing accuracy.
  address: Hong Kong, China
  author:
  - first: Joseph
    full: Joseph Le Roux
    id: joseph-le-roux
    last: Le Roux
  - first: Antoine
    full: Antoine Rozenknop
    id: antoine-rozenknop
    last: Rozenknop
  - first: Mathieu
    full: Mathieu Lacroix
    id: mathieu-lacroix
    last: Lacroix
  author_string: Joseph Le Roux, Antoine Rozenknop, Mathieu Lacroix
  bibkey: le-roux-etal-2019-representation
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1023
  month: November
  page_first: '238'
  page_last: '248'
  pages: "238\u2013248"
  paper_id: '23'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1023.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1023.jpg
  title: Representation Learning and Dynamic Programming for Arc-Hybrid Parsing
  title_html: Representation Learning and Dynamic Programming for Arc-Hybrid Parsing
  url: https://www.aclweb.org/anthology/K19-1023
  year: '2019'
K19-1024:
  abstract: "Debate motions (proposals) tabled in the UK Parliament contain information\
    \ about the stated policy preferences of the Members of Parliament who propose\
    \ them, and are key to the analysis of all subsequent speeches given in response\
    \ to them. We attempt to automatically label debate motions with codes from a\
    \ pre-existing coding scheme developed by political scientists for the annotation\
    \ and analysis of political parties\u2019 manifestos. We develop annotation guidelines\
    \ for the task of applying these codes to debate motions at two levels of granularity\
    \ and produce a dataset of manually labelled examples. We evaluate the annotation\
    \ process and the reliability and utility of the labelling scheme, finding that\
    \ inter-annotator agreement is comparable with that of other studies conducted\
    \ on manifesto data. Moreover, we test a variety of ways of automatically labelling\
    \ motions with the codes, ranging from similarity matching to neural classification\
    \ methods, and evaluate them against the gold standard labels. From these experiments,\
    \ we note that established supervised baselines are not always able to improve\
    \ over simple lexical heuristics. At the same time, we detect a clear and evident\
    \ benefit when employing BERT, a state-of-the-art deep language representation\
    \ model, even in classification scenarios with over 30 different labels and limited\
    \ amounts of training data."
  address: Hong Kong, China
  author:
  - first: Gavin
    full: Gavin Abercrombie
    id: gavin-abercrombie
    last: Abercrombie
  - first: Federico
    full: Federico Nanni
    id: federico-nanni
    last: Nanni
  - first: Riza
    full: Riza Batista-Navarro
    id: riza-theresa-batista-navarro
    last: Batista-Navarro
  - first: Simone Paolo
    full: Simone Paolo Ponzetto
    id: simone-paolo-ponzetto
    last: Ponzetto
  author_string: Gavin Abercrombie, Federico Nanni, Riza Batista-Navarro, Simone Paolo
    Ponzetto
  bibkey: abercrombie-etal-2019-policy
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1024
  month: November
  page_first: '249'
  page_last: '259'
  pages: "249\u2013259"
  paper_id: '24'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1024.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1024.jpg
  title: Policy Preference Detection in Parliamentary Debate Motions
  title_html: Policy Preference Detection in Parliamentary Debate Motions
  url: https://www.aclweb.org/anthology/K19-1024
  year: '2019'
K19-1025:
  abstract: Neural Machine Translation (NMT) optimized by Maximum Likelihood Estimation
    (MLE) lacks the guarantee of translation adequacy. To alleviate this problem,
    we propose an NMT approach that heightens the adequacy in machine translation
    by transferring the semantic knowledge learned from bilingual sentence alignment.
    Specifically, we first design a discriminator that learns to estimate sentence
    aligning score over translation candidates, and then the learned semantic knowledge
    is transfered to the NMT model under an adversarial learning framework. We also
    propose a gated self-attention based encoder for sentence embedding. Furthermore,
    an N-pair training loss is introduced in our framework to aid the discriminator
    in better capturing lexical evidence in translation candidates. Experimental results
    show that our proposed method outperforms baseline NMT models on Chinese-to-English
    and English-to-German translation tasks. Further analysis also indicates the detailed
    semantic knowledge transfered from the discriminator to the NMT model.
  address: Hong Kong, China
  author:
  - first: Xuewen
    full: Xuewen Shi
    id: xuewen-shi
    last: Shi
  - first: Heyan
    full: Heyan Huang
    id: he-yan-huang
    last: Huang
  - first: Wenguan
    full: Wenguan Wang
    id: wenguan-wang
    last: Wang
  - first: Ping
    full: Ping Jian
    id: ping-jian
    last: Jian
  - first: Yi-Kun
    full: Yi-Kun Tang
    id: yi-kun-tang
    last: Tang
  author_string: Xuewen Shi, Heyan Huang, Wenguan Wang, Ping Jian, Yi-Kun Tang
  bibkey: shi-etal-2019-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1025
  month: November
  page_first: '260'
  page_last: '270'
  pages: "260\u2013270"
  paper_id: '25'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1025.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1025.jpg
  title: Improving Neural Machine Translation by Achieving Knowledge Transfer with
    Sentence Alignment Learning
  title_html: Improving Neural Machine Translation by Achieving Knowledge Transfer
    with Sentence Alignment Learning
  url: https://www.aclweb.org/anthology/K19-1025
  year: '2019'
K19-1026:
  abstract: Training code-switched language models is difficult due to lack of data
    and complexity in the grammatical structure. Linguistic constraint theories have
    been used for decades to generate artificial code-switching sentences to cope
    with this issue. However, this require external word alignments or constituency
    parsers that create erroneous results on distant languages. We propose a sequence-to-sequence
    model using a copy mechanism to generate code-switching data by leveraging parallel
    monolingual translations from a limited source of code-switching data. The model
    learns how to combine words from parallel sentences and identifies when to switch
    one language to the other. Moreover, it captures code-switching constraints by
    attending and aligning the words in inputs, without requiring any external knowledge.
    Based on experimental results, the language model trained with the generated sentences
    achieves state-of-the-art performance and improves end-to-end automatic speech
    recognition.
  address: Hong Kong, China
  attachment:
  - filename: K19-1026.Supplementary_Material.pdf
    type: supplementary-material
    url: https://www.aclweb.org/anthology/attachments/K19-1026.Supplementary_Material.pdf
  author:
  - first: Genta Indra
    full: Genta Indra Winata
    id: genta-indra-winata
    last: Winata
  - first: Andrea
    full: Andrea Madotto
    id: andrea-madotto
    last: Madotto
  - first: Chien-Sheng
    full: Chien-Sheng Wu
    id: chien-sheng-wu
    last: Wu
  - first: Pascale
    full: Pascale Fung
    id: pascale-fung
    last: Fung
  author_string: Genta Indra Winata, Andrea Madotto, Chien-Sheng Wu, Pascale Fung
  bibkey: winata-etal-2019-code
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1026
  month: November
  page_first: '271'
  page_last: '280'
  pages: "271\u2013280"
  paper_id: '26'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1026.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1026.jpg
  title: Code-Switched Language Models Using Neural Based Synthetic Data from Parallel
    Sentences
  title_html: Code-Switched Language Models Using Neural Based Synthetic Data from
    Parallel Sentences
  url: https://www.aclweb.org/anthology/K19-1026
  year: '2019'
K19-1027:
  abstract: "In this paper, we alleviate the local optimality of back-translation\
    \ by learning a policy (takes the form of an encoder-decoder and is defined by\
    \ its parameters) with future rewarding under the reinforcement learning framework,\
    \ which aims to optimize the global word predictions for unsupervised neural machine\
    \ translation. To this end, we design a novel reward function to characterize\
    \ high-quality translations from two aspects: n-gram matching and semantic adequacy.\
    \ The n-gram matching is defined as an alternative for the discrete BLEU metric,\
    \ and the semantic adequacy is used to measure the adequacy of conveying the meaning\
    \ of the source sentence to the target. During training, our model strives for\
    \ earning higher rewards by learning to produce grammatically more accurate and\
    \ semantically more adequate translations. Besides, a variational inference network\
    \ (VIN) is proposed to constrain the corresponding sentences in two languages\
    \ have the same or similar latent semantic code. On the widely used WMT\u2019\
    14 English-French, WMT\u201916 English-German and NIST Chinese-to-English benchmarks,\
    \ our models respectively obtain 27.59/27.15, 19.65/23.42 and 22.40 BLEU points\
    \ without using any labeled data, demonstrating consistent improvements over previous\
    \ unsupervised NMT models."
  address: Hong Kong, China
  author:
  - first: Xiangpeng
    full: Xiangpeng Wei
    id: xiangpeng-wei
    last: Wei
  - first: Yue
    full: Yue Hu
    id: yue-hu
    last: Hu
  - first: Luxi
    full: Luxi Xing
    id: luxi-xing
    last: Xing
  - first: Li
    full: Li Gao
    id: li-gao
    last: Gao
  author_string: Xiangpeng Wei, Yue Hu, Luxi Xing, Li Gao
  bibkey: wei-etal-2019-unsupervised
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1027
  month: November
  page_first: '281'
  page_last: '290'
  pages: "281\u2013290"
  paper_id: '27'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1027.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1027.jpg
  title: Unsupervised Neural Machine Translation with Future Rewarding
  title_html: Unsupervised Neural Machine Translation with Future Rewarding
  url: https://www.aclweb.org/anthology/K19-1027
  year: '2019'
K19-1028:
  abstract: We show that the state-of-the-art Transformer MT model is not biased towards
    monotonic reordering (unlike previous recurrent neural network models), but that
    nevertheless, long-distance dependencies remain a challenge for the model. Since
    most dependencies are short-distance, common evaluation metrics will be little
    influenced by how well systems perform on them. We therefore propose an automatic
    approach for extracting challenge sets rich with long-distance dependencies, and
    argue that evaluation using this methodology provides a complementary perspective
    on system performance. To support our claim, we compile challenge sets for English-German
    and German-English, which are much larger than any previously released challenge
    set for MT. The extracted sets are large enough to allow reliable automatic evaluation,
    which makes the proposed approach a scalable and practical solution for evaluating
    MT performance on the long-tail of syntactic phenomena.
  address: Hong Kong, China
  attachment:
  - filename: K19-1028.Supplementary_Material.pdf
    type: supplementary-material
    url: https://www.aclweb.org/anthology/attachments/K19-1028.Supplementary_Material.pdf
  author:
  - first: Leshem
    full: Leshem Choshen
    id: leshem-choshen
    last: Choshen
  - first: Omri
    full: Omri Abend
    id: omri-abend
    last: Abend
  author_string: Leshem Choshen, Omri Abend
  bibkey: choshen-abend-2019-automatically
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1028
  month: November
  page_first: '291'
  page_last: '303'
  pages: "291\u2013303"
  paper_id: '28'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1028.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1028.jpg
  title: Automatically Extracting Challenge Sets for Non-Local Phenomena in Neural
    Machine Translation
  title_html: Automatically Extracting Challenge Sets for Non-Local Phenomena in Neural
    Machine Translation
  url: https://www.aclweb.org/anthology/K19-1028
  year: '2019'
K19-1029:
  abstract: "Despite advances in dependency parsing, languages with small treebanks\
    \ still present challenges. We assess recent approaches to multilingual contextual\
    \ word representations (CWRs), and compare them for crosslingual transfer from\
    \ a language with a large treebank to a language with a small or nonexistent treebank,\
    \ by sharing parameters between languages in the parser itself. We experiment\
    \ with a diverse selection of languages in both simulated and truly low-resource\
    \ scenarios, and show that multilingual CWRs greatly facilitate low-resource dependency\
    \ parsing even without crosslingual supervision such as dictionaries or parallel\
    \ text. Furthermore, we examine the non-contextual part of the learned language\
    \ models (which we call a \u201Cdecontextual probe\u201D) to demonstrate that\
    \ polyglot language models better encode crosslingual lexical correspondence compared\
    \ to aligned monolingual language models. This analysis provides further evidence\
    \ that polyglot training is an effective approach to crosslingual transfer."
  address: Hong Kong, China
  attachment:
  - filename: K19-1029.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-1029.Attachment.zip
  author:
  - first: Phoebe
    full: Phoebe Mulcaire
    id: phoebe-mulcaire
    last: Mulcaire
  - first: Jungo
    full: Jungo Kasai
    id: jungo-kasai
    last: Kasai
  - first: Noah A.
    full: Noah A. Smith
    id: noah-a-smith
    last: Smith
  author_string: Phoebe Mulcaire, Jungo Kasai, Noah A. Smith
  bibkey: mulcaire-etal-2019-low
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1029
  month: November
  page_first: '304'
  page_last: '315'
  pages: "304\u2013315"
  paper_id: '29'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1029.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1029.jpg
  title: Low-Resource Parsing with Crosslingual Contextualized Representations
  title_html: Low-Resource Parsing with Crosslingual Contextualized Representations
  url: https://www.aclweb.org/anthology/K19-1029
  year: '2019'
K19-1030:
  abstract: Recently, pre-trained language models have achieved remarkable success
    in a broad range of natural language processing tasks. However, in multilingual
    setting, it is extremely resource-consuming to pre-train a deep language model
    over large-scale corpora for each language. Instead of exhaustively pre-training
    monolingual language models independently, an alternative solution is to pre-train
    a powerful multilingual deep language model over large-scale corpora in hundreds
    of languages. However, the vocabulary size for each language in such a model is
    relatively small, especially for low-resource languages. This limitation inevitably
    hinders the performance of these multilingual models on tasks such as sequence
    labeling, wherein in-depth token-level or sentence-level understanding is essential.
    In this paper, inspired by previous methods designed for monolingual settings,
    we investigate two approaches (i.e., joint mapping and mixture mapping) based
    on a pre-trained multilingual model BERT for addressing the out-of-vocabulary
    (OOV) problem on a variety of tasks, including part-of-speech tagging, named entity
    recognition, machine translation quality estimation, and machine reading comprehension.
    Experimental results show that using mixture mapping is more promising. To the
    best of our knowledge, this is the first work that attempts to address and discuss
    the OOV issue in multilingual settings.
  address: Hong Kong, China
  author:
  - first: Hai
    full: Hai Wang
    id: hai-wang
    last: Wang
  - first: Dian
    full: Dian Yu
    id: dian-yu
    last: Yu
  - first: Kai
    full: Kai Sun
    id: kai-sun
    last: Sun
  - first: Jianshu
    full: Jianshu Chen
    id: jianshu-chen
    last: Chen
  - first: Dong
    full: Dong Yu
    id: dong-yu
    last: Yu
  author_string: Hai Wang, Dian Yu, Kai Sun, Jianshu Chen, Dong Yu
  bibkey: wang-etal-2019-improving-pre
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1030
  month: November
  page_first: '316'
  page_last: '327'
  pages: "316\u2013327"
  paper_id: '30'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1030.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1030.jpg
  title: Improving Pre-Trained Multilingual Model with Vocabulary Expansion
  title_html: Improving Pre-Trained Multilingual Model with Vocabulary Expansion
  url: https://www.aclweb.org/anthology/K19-1030
  year: '2019'
K19-1031:
  abstract: Long sentences have been one of the major challenges in neural machine
    translation (NMT). Although some approaches such as the attention mechanism have
    partially remedied the problem, we found that the current standard NMT model,
    Transformer, has difficulty in translating long sentences compared to the former
    standard, Recurrent Neural Network (RNN)-based model. One of the key differences
    of these NMT models is how the model handles position information which is essential
    to process sequential data. In this study, we focus on the position information
    type of NMT models, and hypothesize that relative position is better than absolute
    position. To examine the hypothesis, we propose RNN-Transformer which replaces
    positional encoding layer of Transformer by RNN, and then compare RNN-based model
    and four variants of Transformer. Experiments on ASPEC English-to-Japanese and
    WMT2014 English-to-German translation tasks demonstrate that relative position
    helps translating sentences longer than those in the training data. Further experiments
    on length-controlled training data reveal that absolute position actually causes
    overfitting to the sentence length.
  address: Hong Kong, China
  author:
  - first: Masato
    full: Masato Neishi
    id: masato-neishi
    last: Neishi
  - first: Naoki
    full: Naoki Yoshinaga
    id: naoki-yoshinaga
    last: Yoshinaga
  author_string: Masato Neishi, Naoki Yoshinaga
  bibkey: neishi-yoshinaga-2019-relation
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1031
  month: November
  page_first: '328'
  page_last: '338'
  pages: "328\u2013338"
  paper_id: '31'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1031.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1031.jpg
  title: On the Relation between Position Information and Sentence Length in Neural
    Machine Translation
  title_html: On the Relation between Position Information and Sentence Length in
    Neural Machine Translation
  url: https://www.aclweb.org/anthology/K19-1031
  year: '2019'
K19-1032:
  abstract: "In this paper, we study how word-like units are represented and activated\
    \ in a recurrent neural model of visually grounded speech. The model used in our\
    \ experiments is trained to project an image and its spoken description in a common\
    \ representation space. We show that a recurrent model trained on spoken sentences\
    \ implicitly segments its input into word-like units and reliably maps them to\
    \ their correct visual referents. We introduce a methodology originating from\
    \ linguistics to analyse the representation learned by neural networks \u2013\
    \ the gating paradigm \u2013 and show that the correct representation of a word\
    \ is only activated if the network has access to first phoneme of the target word,\
    \ suggesting that the network does not rely on a global acoustic pattern. Furthermore,\
    \ we find out that not all speech frames (MFCC vectors in our case) play an equal\
    \ role in the final encoded representation of a given word, but that some frames\
    \ have a crucial effect on it. Finally we suggest that word representation could\
    \ be activated through a process of lexical competition."
  address: Hong Kong, China
  attachment:
  - filename: K19-1032.Supplementary_Material.pdf
    type: supplementary-material
    url: https://www.aclweb.org/anthology/attachments/K19-1032.Supplementary_Material.pdf
  author:
  - first: William N.
    full: William N. Havard
    id: william-n-havard
    last: Havard
  - first: Jean-Pierre
    full: Jean-Pierre Chevrot
    id: jean-pierre-chevrot
    last: Chevrot
  - first: Laurent
    full: Laurent Besacier
    id: laurent-besacier
    last: Besacier
  author_string: William N. Havard, Jean-Pierre Chevrot, Laurent Besacier
  bibkey: havard-etal-2019-word
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1032
  month: November
  page_first: '339'
  page_last: '348'
  pages: "339\u2013348"
  paper_id: '32'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1032.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1032.jpg
  title: Word Recognition, Competition, and Activation in a Model of Visually Grounded
    Speech
  title_html: Word Recognition, Competition, and Activation in a Model of Visually
    Grounded Speech
  url: https://www.aclweb.org/anthology/K19-1032
  year: '2019'
K19-1033:
  abstract: Quantitative reasoning is a higher-order reasoning skill that any intelligent
    natural language understanding system can reasonably be expected to handle. We
    present EQUATE (Evaluating Quantitative Understanding Aptitude in Textual Entailment),
    a new framework for quantitative reasoning in textual entailment. We benchmark
    the performance of 9 published NLI models on EQUATE, and find that on average,
    state-of-the-art methods do not achieve an absolute improvement over a majority-class
    baseline, suggesting that they do not implicitly learn to reason with quantities.
    We establish a new baseline Q-REAS that manipulates quantities symbolically. In
    comparison to the best performing NLI model, it achieves success on numerical
    reasoning tests (+24.2 %), but has limited verbal reasoning capabilities (-8.1
    %). We hope our evaluation framework will support the development of models of
    quantitative reasoning in language understanding.
  address: Hong Kong, China
  author:
  - first: Abhilasha
    full: Abhilasha Ravichander
    id: abhilasha-ravichander
    last: Ravichander
  - first: Aakanksha
    full: Aakanksha Naik
    id: aakanksha-naik
    last: Naik
  - first: Carolyn
    full: Carolyn Rose
    id: carolyn-rose
    last: Rose
  - first: Eduard
    full: Eduard Hovy
    id: eduard-hovy
    last: Hovy
  author_string: Abhilasha Ravichander, Aakanksha Naik, Carolyn Rose, Eduard Hovy
  bibkey: ravichander-etal-2019-equate
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1033
  month: November
  page_first: '349'
  page_last: '361'
  pages: "349\u2013361"
  paper_id: '33'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1033.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1033.jpg
  title: 'EQUATE: A Benchmark Evaluation Framework for Quantitative Reasoning in Natural
    Language Inference'
  title_html: '<span class="acl-fixed-case">EQUATE</span>: A Benchmark Evaluation
    Framework for Quantitative Reasoning in Natural Language Inference'
  url: https://www.aclweb.org/anthology/K19-1033
  year: '2019'
K19-1034:
  abstract: In the field of metaphor detection, deep learning systems are the ubiquitous
    and achieve strong performance on many tasks. However, due to the complicated
    procedures for manually identifying metaphors, the datasets available are relatively
    small and fraught with complications. We show that using syntactic features and
    lexical resources can automatically provide additional high-quality training data
    for metaphoric language, and this data can cover gaps and inconsistencies in metaphor
    annotation, improving state-of-the-art word-level metaphor identification. This
    novel application of automatically improving training data improves classification
    across numerous tasks, and reconfirms the necessity of high-quality data for deep
    learning frameworks.
  address: Hong Kong, China
  author:
  - first: Kevin
    full: Kevin Stowe
    id: kevin-stowe
    last: Stowe
  - first: Sarah
    full: Sarah Moeller
    id: sarah-moeller
    last: Moeller
  - first: Laura
    full: Laura Michaelis
    id: laura-michaelis
    last: Michaelis
  - first: Martha
    full: Martha Palmer
    id: martha-palmer
    last: Palmer
  author_string: Kevin Stowe, Sarah Moeller, Laura Michaelis, Martha Palmer
  bibkey: stowe-etal-2019-linguistic
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1034
  month: November
  page_first: '362'
  page_last: '371'
  pages: "362\u2013371"
  paper_id: '34'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1034.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1034.jpg
  title: Linguistic Analysis Improves Neural Metaphor Detection
  title_html: Linguistic Analysis Improves Neural Metaphor Detection
  url: https://www.aclweb.org/anthology/K19-1034
  year: '2019'
K19-1035:
  abstract: Cross-lingual transfer learning has become an important weapon to battle
    the unavailability of annotated resources for low-resource languages. One of the
    fundamental techniques to transfer across languages is learning language-agnostic
    representations, in the form of word embeddings or contextual encodings. In this
    work, we propose to leverage unannotated sentences from auxiliary languages to
    help learning language-agnostic representations. Specifically, we explore adversarial
    training for learning contextual encoders that produce invariant representations
    across languages to facilitate cross-lingual transfer. We conduct experiments
    on cross-lingual dependency parsing where we train a dependency parser on a source
    language and transfer it to a wide range of target languages. Experiments on 28
    target languages demonstrate that adversarial training significantly improves
    the overall transfer performances under several different settings. We conduct
    a careful analysis to evaluate the language-agnostic representations resulted
    from adversarial training.
  address: Hong Kong, China
  author:
  - first: Wasi Uddin
    full: Wasi Uddin Ahmad
    id: wasi-uddin-ahmad
    last: Ahmad
  - first: Zhisong
    full: Zhisong Zhang
    id: zhisong-zhang
    last: Zhang
  - first: Xuezhe
    full: Xuezhe Ma
    id: xuezhe-ma
    last: Ma
  - first: Kai-Wei
    full: Kai-Wei Chang
    id: kai-wei-chang
    last: Chang
  - first: Nanyun
    full: Nanyun Peng
    id: nanyun-peng
    last: Peng
  author_string: Wasi Uddin Ahmad, Zhisong Zhang, Xuezhe Ma, Kai-Wei Chang, Nanyun
    Peng
  bibkey: ahmad-etal-2019-cross
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1035
  month: November
  page_first: '372'
  page_last: '382'
  pages: "372\u2013382"
  paper_id: '35'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1035.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1035.jpg
  title: Cross-Lingual Dependency Parsing with Unlabeled Auxiliary Languages
  title_html: Cross-Lingual Dependency Parsing with Unlabeled Auxiliary Languages
  url: https://www.aclweb.org/anthology/K19-1035
  year: '2019'
K19-1036:
  abstract: Recognising dialogue acts (DA) is important for many natural language
    processing tasks such as dialogue generation and intention recognition. In this
    paper, we propose a dual-attention hierarchical recurrent neural network for DA
    classification. Our model is partially inspired by the observation that conversational
    utterances are normally associated with both a DA and a topic, where the former
    captures the social act and the latter describes the subject matter. However,
    such a dependency between DAs and topics has not been utilised by most existing
    systems for DA classification. With a novel dual task-specific attention mechanism,
    our model is able, for utterances, to capture information about both DAs and topics,
    as well as information about the interactions between them. Experimental results
    show that by modelling topic as an auxiliary task, our model can significantly
    improve DA classification, yielding better or comparable performance to the state-of-the-art
    method on three public datasets.
  address: Hong Kong, China
  author:
  - first: Ruizhe
    full: Ruizhe Li
    id: ruizhe-li
    last: Li
  - first: Chenghua
    full: Chenghua Lin
    id: chenghua-lin
    last: Lin
  - first: Matthew
    full: Matthew Collinson
    id: matthew-collinson
    last: Collinson
  - first: Xiao
    full: Xiao Li
    id: xiao-li
    last: Li
  - first: Guanyi
    full: Guanyi Chen
    id: guanyi-chen
    last: Chen
  author_string: Ruizhe Li, Chenghua Lin, Matthew Collinson, Xiao Li, Guanyi Chen
  bibkey: li-etal-2019-dual
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1036
  month: November
  page_first: '383'
  page_last: '392'
  pages: "383\u2013392"
  paper_id: '36'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1036.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1036.jpg
  title: A Dual-Attention Hierarchical Recurrent Neural Network for Dialogue Act Classification
  title_html: A Dual-Attention Hierarchical Recurrent Neural Network for Dialogue
    Act Classification
  url: https://www.aclweb.org/anthology/K19-1036
  year: '2019'
K19-1037:
  abstract: "Reflective listening\u2013demonstrating that you have heard your conversational\
    \ partner\u2013is key to effective communication. Expert human communicators often\
    \ mimic and rephrase their conversational partner, e.g., when responding to sentimental\
    \ stories or to questions they don\u2019t know the answer to. We introduce a new\
    \ task and an associated dataset wherein dialogue agents similarly mimic and rephrase\
    \ a user\u2019s request to communicate sympathy (I\u2019m sorry to hear that)\
    \ or lack of knowledge (I do not know that). We study what makes a rephrasal response\
    \ good against a set of qualitative metrics. We then evaluate three models for\
    \ generating responses: a syntax-aware rule-based system, a seq2seq LSTM neural\
    \ models with attention (S2SA), and the same neural model augmented with a copy\
    \ mechanism (S2SA+C). In a human evaluation, we find that S2SA+C and the rule-based\
    \ system are comparable and approach human-generated response quality. In addition,\
    \ experiences with a live deployment of S2SA+C in a customer support setting suggest\
    \ that this generation task is a practical contribution to real world conversational\
    \ agents."
  address: Hong Kong, China
  attachment:
  - filename: K19-1037.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-1037.Attachment.zip
  author:
  - first: Justin
    full: Justin Dieter
    id: justin-dieter
    last: Dieter
  - first: Tian
    full: Tian Wang
    id: tian-wang
    last: Wang
  - first: Arun Tejasvi
    full: Arun Tejasvi Chaganty
    id: arun-tejasvi-chaganty
    last: Chaganty
  - first: Gabor
    full: Gabor Angeli
    id: gabor-angeli
    last: Angeli
  - first: Angel X.
    full: Angel X. Chang
    id: angel-chang
    last: Chang
  author_string: Justin Dieter, Tian Wang, Arun Tejasvi Chaganty, Gabor Angeli, Angel
    X. Chang
  bibkey: dieter-etal-2019-mimic
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1037
  month: November
  page_first: '393'
  page_last: '403'
  pages: "393\u2013403"
  paper_id: '37'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1037.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1037.jpg
  title: 'Mimic and Rephrase: Reflective Listening in Open-Ended Dialogue'
  title_html: 'Mimic and Rephrase: Reflective Listening in Open-Ended Dialogue'
  url: https://www.aclweb.org/anthology/K19-1037
  year: '2019'
K19-1038:
  abstract: Pyramid evaluation was developed to assess the content of paragraph length
    summaries of source texts. A pyramid lists the distinct units of content found
    in several reference summaries, weights content units by how many reference summaries
    they occur in, and produces three scores based on the weighted content of new
    summaries. We present an automated method that is more efficient, more transparent,
    and more complete than previous automated pyramid methods. It is tested on a new
    dataset of student summaries, and historical NIST data from extractive summarizers.
  address: Hong Kong, China
  author:
  - first: Yanjun
    full: Yanjun Gao
    id: yanjun-gao
    last: Gao
  - first: Chen
    full: Chen Sun
    id: chen-sun
    last: Sun
  - first: Rebecca J.
    full: Rebecca J. Passonneau
    id: rebecca-j-passonneau
    last: Passonneau
  author_string: Yanjun Gao, Chen Sun, Rebecca J. Passonneau
  bibkey: gao-etal-2019-automated
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1038
  month: November
  page_first: '404'
  page_last: '418'
  pages: "404\u2013418"
  paper_id: '38'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1038.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1038.jpg
  title: Automated Pyramid Summarization Evaluation
  title_html: Automated Pyramid Summarization Evaluation
  url: https://www.aclweb.org/anthology/K19-1038
  year: '2019'
K19-1039:
  abstract: "Instructional videos get high-traffic on video sharing platforms, and\
    \ prior work suggests that providing time-stamped, subtask annotations (e.g.,\
    \ \u201Cheat the oil in the pan\u201D) improves user experiences. However, current\
    \ automatic annotation methods based on visual features alone perform only slightly\
    \ better than constant prediction. Taking cues from prior work, we show that we\
    \ can improve performance significantly by considering automatic speech recognition\
    \ (ASR) tokens as input. Furthermore, jointly modeling ASR tokens and visual features\
    \ results in higher performance compared to training individually on either modality.\
    \ We find that unstated background information is better explained by visual features,\
    \ whereas fine-grained distinctions (e.g., \u201Cadd oil\u201D vs. \u201Cadd olive\
    \ oil\u201D) are disambiguated more easily via ASR tokens."
  address: Hong Kong, China
  author:
  - first: Jack
    full: Jack Hessel
    id: jack-hessel
    last: Hessel
  - first: Bo
    full: Bo Pang
    id: bo-pang
    last: Pang
  - first: Zhenhai
    full: Zhenhai Zhu
    id: zhenhai-zhu
    last: Zhu
  - first: Radu
    full: Radu Soricut
    id: radu-soricut
    last: Soricut
  author_string: Jack Hessel, Bo Pang, Zhenhai Zhu, Radu Soricut
  bibkey: hessel-etal-2019-case
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1039
  month: November
  page_first: '419'
  page_last: '429'
  pages: "419\u2013429"
  paper_id: '39'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1039.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1039.jpg
  title: A Case Study on Combining ASR and Visual Features for Generating Instructional
    Video Captions
  title_html: A Case Study on Combining <span class="acl-fixed-case">ASR</span> and
    Visual Features for Generating Instructional Video Captions
  url: https://www.aclweb.org/anthology/K19-1039
  year: '2019'
K19-1040:
  abstract: Grounding referring expressions to objects in an environment has traditionally
    been considered a one-off, ahistorical task. However, in realistic applications
    of grounding, multiple users will repeatedly refer to the same set of objects.
    As a result, past referring expressions for objects can provide strong signals
    for grounding subsequent referring expressions. We therefore reframe the grounding
    problem from the perspective of coreference detection and propose a neural network
    that detects when two expressions are referring to the same object. The network
    combines information from vision and past referring expressions to resolve which
    object is being referred to. Our experiments show that detecting referring expression
    coreference is an effective way to ground objects described by subtle visual properties,
    which standard visual grounding models have difficulty capturing. We also show
    the ability to detect object coreference allows the grounding model to perform
    well even when it encounters object categories not seen in the training data.
  address: Hong Kong, China
  attachment:
  - filename: K19-1040.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-1040.Attachment.pdf
  author:
  - first: Subhro
    full: Subhro Roy
    id: subhro-roy
    last: Roy
  - first: Michael
    full: Michael Noseworthy
    id: michael-noseworthy
    last: Noseworthy
  - first: Rohan
    full: Rohan Paul
    id: rohan-paul
    last: Paul
  - first: Daehyung
    full: Daehyung Park
    id: daehyung-park
    last: Park
  - first: Nicholas
    full: Nicholas Roy
    id: nicholas-roy
    last: Roy
  author_string: Subhro Roy, Michael Noseworthy, Rohan Paul, Daehyung Park, Nicholas
    Roy
  bibkey: roy-etal-2019-leveraging
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1040
  month: November
  page_first: '430'
  page_last: '440'
  pages: "430\u2013440"
  paper_id: '40'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1040.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1040.jpg
  title: Leveraging Past References for Robust Language Grounding
  title_html: Leveraging Past References for Robust Language Grounding
  url: https://www.aclweb.org/anthology/K19-1040
  year: '2019'
K19-1041:
  abstract: This paper addresses the problem of comprehending procedural commonsense
    knowledge. This is a challenging task as it requires identifying key entities,
    keeping track of their state changes, and understanding temporal and causal relations.
    Contrary to most of the previous work, in this study, we do not rely on strong
    inductive bias and explore the question of how multimodality can be exploited
    to provide a complementary semantic signal. Towards this end, we introduce a new
    entity-aware neural comprehension model augmented with external relational memory
    units. Our model learns to dynamically update entity states in relation to each
    other while reading the text instructions. Our experimental analysis on the visual
    reasoning tasks in the recently proposed RecipeQA dataset reveals that our approach
    improves the accuracy of the previously reported models by a large margin. Moreover,
    we find that our model learns effective dynamic representations of entities even
    though we do not use any supervision at the level of entity states.
  address: Hong Kong, China
  author:
  - first: Mustafa Sercan
    full: Mustafa Sercan Amac
    id: mustafa-sercan-amac
    last: Amac
  - first: Semih
    full: Semih Yagcioglu
    id: semih-yagcioglu
    last: Yagcioglu
  - first: Aykut
    full: Aykut Erdem
    id: aykut-erdem
    last: Erdem
  - first: Erkut
    full: Erkut Erdem
    id: erkut-erdem
    last: Erdem
  author_string: Mustafa Sercan Amac, Semih Yagcioglu, Aykut Erdem, Erkut Erdem
  bibkey: amac-etal-2019-procedural
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1041
  month: November
  page_first: '441'
  page_last: '451'
  pages: "441\u2013451"
  paper_id: '41'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1041.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1041.jpg
  title: Procedural Reasoning Networks for Understanding Multimodal Procedures
  title_html: Procedural Reasoning Networks for Understanding Multimodal Procedures
  url: https://www.aclweb.org/anthology/K19-1041
  year: '2019'
K19-1042:
  abstract: One of the goals of natural language understanding is to develop models
    that map sentences into meaning representations. However, training such models
    requires expensive annotation of complex structures, which hinders their adoption.
    Learning to actively-learn(LTAL) is a recent paradigm for reducing the amount
    of labeled data by learning a policy that selects which samples should be labeled.
    In this work, we examine LTAL for learning semantic representations, such as QA-SRL.
    We show that even an oracle policy that is allowed to pick examples that maximize
    performance on the test set (and constitutes an upper bound on the potential of
    LTAL), does not substantially improve performance compared to a random policy.
    We investigate factors that could explain this finding and show that a distinguishing
    characteristic of successful applications of LTAL is the interaction between optimization
    and the oracle policy selection process. In successful applications of LTAL, the
    examples selected by the oracle policy do not substantially depend on the optimization
    procedure, while in our setup the stochastic nature of optimization strongly affects
    the examples selected by the oracle. We conclude that the current applicability
    of LTAL for improving data efficiency in learning semantic meaning representations
    is limited.
  address: Hong Kong, China
  author:
  - first: Omri
    full: Omri Koshorek
    id: omri-koshorek
    last: Koshorek
  - first: Gabriel
    full: Gabriel Stanovsky
    id: gabriel-stanovsky
    last: Stanovsky
  - first: Yichu
    full: Yichu Zhou
    id: yichu-zhou
    last: Zhou
  - first: Vivek
    full: Vivek Srikumar
    id: vivek-srikumar
    last: Srikumar
  - first: Jonathan
    full: Jonathan Berant
    id: jonathan-berant
    last: Berant
  author_string: Omri Koshorek, Gabriel Stanovsky, Yichu Zhou, Vivek Srikumar, Jonathan
    Berant
  bibkey: koshorek-etal-2019-limits
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1042
  month: November
  page_first: '452'
  page_last: '462'
  pages: "452\u2013462"
  paper_id: '42'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1042.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1042.jpg
  title: On the Limits of Learning to Actively Learn Semantic Representations
  title_html: On the Limits of Learning to Actively Learn Semantic Representations
  url: https://www.aclweb.org/anthology/K19-1042
  year: '2019'
K19-1043:
  abstract: "Many natural languages assign grammatical gender also to inanimate nouns\
    \ in the language. In such languages, words that relate to the gender-marked nouns\
    \ are inflected to agree with the noun\u2019s gender. We show that this affects\
    \ the word representations of inanimate nouns, resulting in nouns with the same\
    \ gender being closer to each other than nouns with different gender. While \u201C\
    embedding debiasing\u201D methods fail to remove the effect, we demonstrate that\
    \ a careful application of methods that neutralize grammatical gender signals\
    \ from the words\u2019 context when training word embeddings is effective in removing\
    \ it. Fixing the grammatical gender bias yields a positive effect on the quality\
    \ of the resulting word embeddings, both in monolingual and cross-lingual settings.\
    \ We note that successfully removing gender signals, while achievable, is not\
    \ trivial to do and that a language-specific morphological analyzer, together\
    \ with careful usage of it, are essential for achieving good results."
  address: Hong Kong, China
  attachment:
  - filename: K19-1043.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-1043.Attachment.pdf
  author:
  - first: Hila
    full: Hila Gonen
    id: hila-gonen
    last: Gonen
  - first: Yova
    full: Yova Kementchedjhieva
    id: yova-kementchedjhieva
    last: Kementchedjhieva
  - first: Yoav
    full: Yoav Goldberg
    id: yoav-goldberg
    last: Goldberg
  author_string: Hila Gonen, Yova Kementchedjhieva, Yoav Goldberg
  bibkey: gonen-etal-2019-grammatical
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1043
  month: November
  page_first: '463'
  page_last: '471'
  pages: "463\u2013471"
  paper_id: '43'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1043.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1043.jpg
  title: How Does Grammatical Gender Affect Noun Representations in Gender-Marking
    Languages?
  title_html: How Does Grammatical Gender Affect Noun Representations in Gender-Marking
    Languages?
  url: https://www.aclweb.org/anthology/K19-1043
  year: '2019'
K19-1044:
  abstract: Active learning (AL) is a technique for reducing manual annotation effort
    during the annotation of training data for machine learning classifiers. For NLP
    tasks, pool-based and stream-based sampling techniques have been used to select
    new instances for AL while gen erating new, artificial instances via Membership
    Query Synthesis was, up to know, considered to be infeasible for NLP problems.
    We present the first successfull attempt to use Membership Query Synthesis for
    generating AL queries, using Variational Autoencoders for query generation. We
    evaluate our approach in a text classification task and demonstrate that query
    synthesis shows competitive performance to pool-based AL strategies while substantially
    reducing annotation time
  address: Hong Kong, China
  author:
  - first: Raphael
    full: Raphael Schumann
    id: raphael-schumann
    last: Schumann
  - first: Ines
    full: Ines Rehbein
    id: ines-rehbein
    last: Rehbein
  author_string: Raphael Schumann, Ines Rehbein
  bibkey: schumann-rehbein-2019-active
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1044
  month: November
  page_first: '472'
  page_last: '481'
  pages: "472\u2013481"
  paper_id: '44'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1044.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1044.jpg
  title: Active Learning via Membership Query Synthesis for Semi-Supervised Sentence
    Classification
  title_html: Active Learning via Membership Query Synthesis for Semi-Supervised Sentence
    Classification
  url: https://www.aclweb.org/anthology/K19-1044
  year: '2019'
K19-1045:
  abstract: Inference in structured prediction involves finding the best output structure
    for an input, subject to certain constraints. Many current approaches use sequential
    inference, which constructs the output in a left-to-right manner. However, there
    is no general framework to specify constraints in these approaches. We present
    a principled approach for incorporating constraints into sequential inference
    algorithms. Our approach expresses constraints using an automaton, which is traversed
    in lock-step during inference, guiding the search to valid outputs. We show that
    automata can express commonly used constraints and are easily incorporated into
    sequential inference. When it is more natural to represent constraints as a set
    of automata, our algorithm uses an active set method for demonstrably fast and
    efficient inference. We experimentally show the benefits of our algorithm on constituency
    parsing and semantic role labeling. For parsing, unlike unconstrained approaches,
    our algorithm always generates valid output, incurring only a small drop in performance.
    For semantic role labeling, imposing constraints using our algorithm corrects
    common errors, improving F1 by 1.5 points. These benefits increase in low-resource
    settings. Our active set method achieves a 5.2x relative speed-up over a naive
    approach.
  address: Hong Kong, China
  attachment:
  - filename: K19-1045.Supplementary_Material.zip
    type: supplementary-material
    url: https://www.aclweb.org/anthology/attachments/K19-1045.Supplementary_Material.zip
  author:
  - first: Daniel
    full: Daniel Deutsch
    id: daniel-deutsch
    last: Deutsch
  - first: Shyam
    full: Shyam Upadhyay
    id: shyam-upadhyay
    last: Upadhyay
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Daniel Deutsch, Shyam Upadhyay, Dan Roth
  bibkey: deutsch-etal-2019-general
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1045
  month: November
  page_first: '482'
  page_last: '492'
  pages: "482\u2013492"
  paper_id: '45'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1045.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1045.jpg
  title: A General-Purpose Algorithm for Constrained Sequential Inference
  title_html: A General-Purpose Algorithm for Constrained Sequential Inference
  url: https://www.aclweb.org/anthology/K19-1045
  year: '2019'
K19-1046:
  abstract: 'Automated fact-checking based on machine learning is a promising approach
    to identify false information distributed on the web. In order to achieve satisfactory
    performance, machine learning methods require a large corpus with reliable annotations
    for the different tasks in the fact-checking process. Having analyzed existing
    fact-checking corpora, we found that none of them meets these criteria in full.
    They are either too small in size, do not provide detailed annotations, or are
    limited to a single domain. Motivated by this gap, we present a new substantially
    sized mixed-domain corpus with annotations of good quality for the core fact-checking
    tasks: document retrieval, evidence extraction, stance detection, and claim validation.
    To aid future corpus construction, we describe our methodology for corpus creation
    and annotation, and demonstrate that it results in substantial inter-annotator
    agreement. As baselines for future research, we perform experiments on our corpus
    with a number of model architectures that reach high performance in similar problem
    settings. Finally, to support the development of future models, we provide a detailed
    error analysis for each of the tasks. Our results show that the realistic, multi-domain
    setting defined by our data poses new challenges for the existing models, providing
    opportunities for considerable improvement by future systems.'
  address: Hong Kong, China
  attachment:
  - filename: K19-1046.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-1046.Attachment.pdf
  - filename: K19-1046.Supplementary_Material.pdf
    type: supplementary-material
    url: https://www.aclweb.org/anthology/attachments/K19-1046.Supplementary_Material.pdf
  author:
  - first: Andreas
    full: Andreas Hanselowski
    id: andreas-hanselowski
    last: Hanselowski
  - first: Christian
    full: Christian Stab
    id: christian-stab
    last: Stab
  - first: Claudia
    full: Claudia Schulz
    id: claudia-schulz
    last: Schulz
  - first: Zile
    full: Zile Li
    id: zile-li
    last: Li
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Andreas Hanselowski, Christian Stab, Claudia Schulz, Zile Li, Iryna
    Gurevych
  bibkey: hanselowski-etal-2019-richly
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1046
  month: November
  page_first: '493'
  page_last: '503'
  pages: "493\u2013503"
  paper_id: '46'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1046.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1046.jpg
  title: A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking
  title_html: A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking
  url: https://www.aclweb.org/anthology/K19-1046
  year: '2019'
K19-1047:
  abstract: "Different news articles about the same topic often offer a variety of\
    \ perspectives: an article written about gun violence might emphasize gun control,\
    \ while another might promote 2nd Amendment rights, and yet a third might focus\
    \ on mental health issues. In communication research, these different perspectives\
    \ are known as \u201Cframes\u201D, which, when used in news media will influence\
    \ the opinion of their readers in multiple ways. In this paper, we present a method\
    \ for effectively detecting frames in news headlines. Our training and performance\
    \ evaluation is based on a new dataset of news headlines related to the issue\
    \ of gun violence in the United States. This Gun Violence Frame Corpus (GVFC)\
    \ was curated and annotated by journalism and communication experts. Our proposed\
    \ approach sets a new state-of-the-art performance for multiclass news frame detection,\
    \ significantly outperforming a recent baseline by 35.9% absolute difference in\
    \ accuracy. We apply our frame detection approach in a large scale study of 88k\
    \ news headlines about the coverage of gun violence in the U.S. between 2016 and\
    \ 2018."
  address: Hong Kong, China
  author:
  - first: Siyi
    full: Siyi Liu
    id: siyi-liu
    last: Liu
  - first: Lei
    full: Lei Guo
    id: lei-guo
    last: Guo
  - first: Kate
    full: Kate Mays
    id: kate-mays
    last: Mays
  - first: Margrit
    full: Margrit Betke
    id: margrit-betke
    last: Betke
  - first: Derry Tanti
    full: Derry Tanti Wijaya
    id: derry-tanti-wijaya
    last: Wijaya
  author_string: Siyi Liu, Lei Guo, Kate Mays, Margrit Betke, Derry Tanti Wijaya
  bibkey: liu-etal-2019-detecting
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1047
  month: November
  page_first: '504'
  page_last: '514'
  pages: "504\u2013514"
  paper_id: '47'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1047.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1047.jpg
  title: Detecting Frames in News Headlines and Its Application to Analyzing News
    Framing Trends Surrounding U.S. Gun Violence
  title_html: Detecting Frames in News Headlines and Its Application to Analyzing
    News Framing Trends Surrounding <span class="acl-fixed-case">U</span>.<span class="acl-fixed-case">S</span>.
    Gun Violence
  url: https://www.aclweb.org/anthology/K19-1047
  year: '2019'
K19-1048:
  abstract: "Named entity recognition (NER) identifies typed entity mentions in raw\
    \ text. While the task is well-established, there is no universally used tagset:\
    \ often, datasets are annotated for use in downstream applications and accordingly\
    \ only cover a small set of entity types relevant to a particular task. For instance,\
    \ in the biomedical domain, one corpus might annotate genes, another chemicals,\
    \ and another diseases\u2014despite the texts in each corpus containing references\
    \ to all three types of entities. In this paper, we propose a deep structured\
    \ model to integrate these \u201Cpartially annotated\u201D datasets to jointly\
    \ identify all entity types appearing in the training corpora. By leveraging multiple\
    \ datasets, the model can learn robust input representations; by building a joint\
    \ structured model, it avoids potential conflicts caused by combining several\
    \ models\u2019 predictions at test time. Experiments show that the proposed model\
    \ significantly outperforms strong multi-task learning baselines when training\
    \ on multiple, partially annotated datasets and testing on datasets that contain\
    \ tags from more than one of the training corpora."
  address: Hong Kong, China
  attachment:
  - filename: K19-1048.Supplementary_Material.pdf
    type: supplementary-material
    url: https://www.aclweb.org/anthology/attachments/K19-1048.Supplementary_Material.pdf
  - filename: K19-1048.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-1048.Attachment.pdf
  author:
  - first: Xiao
    full: Xiao Huang
    id: xiao-huang
    last: Huang
  - first: Li
    full: Li Dong
    id: li-dong
    last: Dong
  - first: Elizabeth
    full: Elizabeth Boschee
    id: elizabeth-boschee
    last: Boschee
  - first: Nanyun
    full: Nanyun Peng
    id: nanyun-peng
    last: Peng
  author_string: Xiao Huang, Li Dong, Elizabeth Boschee, Nanyun Peng
  bibkey: huang-etal-2019-learning-unified
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1048
  month: November
  page_first: '515'
  page_last: '527'
  pages: "515\u2013527"
  paper_id: '48'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1048.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1048.jpg
  title: Learning a Unified Named Entity Tagger from Multiple Partially Annotated
    Corpora for Efficient Adaptation
  title_html: Learning a Unified Named Entity Tagger from Multiple Partially Annotated
    Corpora for Efficient Adaptation
  url: https://www.aclweb.org/anthology/K19-1048
  year: '2019'
K19-1049:
  abstract: We show that it is feasible to perform entity linking by training a dual
    encoder (two-tower) model that encodes mentions and entities in the same dense
    vector space, where candidate entities are retrieved by approximate nearest neighbor
    search. Unlike prior work, this setup does not rely on an alias table followed
    by a re-ranker, and is thus the first fully learned entity retrieval model. We
    show that our dual encoder, trained using only anchor-text links in Wikipedia,
    outperforms discrete alias table and BM25 baselines, and is competitive with the
    best comparable results on the standard TACKBP-2010 dataset. In addition, it can
    retrieve candidates extremely fast, and generalizes well to a new dataset derived
    from Wikinews. On the modeling side, we demonstrate the dramatic value of an unsupervised
    negative mining algorithm for this task.
  address: Hong Kong, China
  author:
  - first: Daniel
    full: Daniel Gillick
    id: dan-gillick
    last: Gillick
  - first: Sayali
    full: Sayali Kulkarni
    id: sayali-kulkarni
    last: Kulkarni
  - first: Larry
    full: Larry Lansing
    id: larry-lansing
    last: Lansing
  - first: Alessandro
    full: Alessandro Presta
    id: alessandro-presta
    last: Presta
  - first: Jason
    full: Jason Baldridge
    id: jason-baldridge
    last: Baldridge
  - first: Eugene
    full: Eugene Ie
    id: eugene-ie
    last: Ie
  - first: Diego
    full: Diego Garcia-Olano
    id: diego-garcia-olano
    last: Garcia-Olano
  author_string: Daniel Gillick, Sayali Kulkarni, Larry Lansing, Alessandro Presta,
    Jason Baldridge, Eugene Ie, Diego Garcia-Olano
  bibkey: gillick-etal-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1049
  month: November
  page_first: '528'
  page_last: '537'
  pages: "528\u2013537"
  paper_id: '49'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1049.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1049.jpg
  title: Learning Dense Representations for Entity Retrieval
  title_html: Learning Dense Representations for Entity Retrieval
  url: https://www.aclweb.org/anthology/K19-1049
  year: '2019'
K19-1050:
  abstract: An interesting method of evaluating word representations is by how much
    they reflect the semantic representations in the human brain. However, most, if
    not all, previous works only focus on small datasets and a single modality. In
    this paper, we present the first multi-modal framework for evaluating English
    word representations based on cognitive lexical semantics. Six types of word embeddings
    are evaluated by fitting them to 15 datasets of eye-tracking, EEG and fMRI signals
    recorded during language processing. To achieve a global score over all evaluation
    hypotheses, we apply statistical significance testing accounting for the multiple
    comparisons problem. This framework is easily extensible and available to include
    other intrinsic and extrinsic evaluation methods. We find strong correlations
    in the results between cognitive datasets, across recording modalities and to
    their performance on extrinsic NLP tasks.
  address: Hong Kong, China
  attachment:
  - filename: K19-1050.Supplementary_Material.zip
    type: supplementary-material
    url: https://www.aclweb.org/anthology/attachments/K19-1050.Supplementary_Material.zip
  - filename: K19-1050.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-1050.Attachment.zip
  author:
  - first: Nora
    full: Nora Hollenstein
    id: nora-hollenstein
    last: Hollenstein
  - first: Antonio
    full: Antonio de la Torre
    id: antonio-de-la-torre
    last: de la Torre
  - first: Nicolas
    full: Nicolas Langer
    id: nicolas-langer
    last: Langer
  - first: Ce
    full: Ce Zhang
    id: ce-zhang
    last: Zhang
  author_string: Nora Hollenstein, Antonio de la Torre, Nicolas Langer, Ce Zhang
  bibkey: hollenstein-etal-2019-cognival
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1050
  month: November
  page_first: '538'
  page_last: '549'
  pages: "538\u2013549"
  paper_id: '50'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1050.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1050.jpg
  title: 'CogniVal: A Framework for Cognitive Word Embedding Evaluation'
  title_html: '<span class="acl-fixed-case">C</span>ogni<span class="acl-fixed-case">V</span>al:
    A Framework for Cognitive Word Embedding Evaluation'
  url: https://www.aclweb.org/anthology/K19-1050
  year: '2019'
K19-1051:
  abstract: "Story understanding requires developing expectations of what events come\
    \ next in text. Prior knowledge \u2013 both statistical and declarative \u2013\
    \ is essential in guiding such expectations. While existing semantic language\
    \ models (SemLM) capture event co-occurrence information by modeling event sequences\
    \ as semantic frames, entities, and other semantic units, this paper aims at augmenting\
    \ them with causal knowledge (i.e., one event is likely to lead to another). Such\
    \ knowledge is modeled at the frame and entity level, and can be obtained either\
    \ statistically from text or stated declaratively. The proposed method, KnowSemLM,\
    \ infuses this knowledge into a semantic LM by joint training and inference, and\
    \ is shown to be effective on both the event cloze test and story/referent prediction\
    \ tasks."
  address: Hong Kong, China
  author:
  - first: Haoruo
    full: Haoruo Peng
    id: haoruo-peng
    last: Peng
  - first: Qiang
    full: Qiang Ning
    id: qiang-ning
    last: Ning
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Haoruo Peng, Qiang Ning, Dan Roth
  bibkey: peng-etal-2019-knowsemlm
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1051
  month: November
  page_first: '550'
  page_last: '562'
  pages: "550\u2013562"
  paper_id: '51'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1051.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1051.jpg
  title: 'KnowSemLM: A Knowledge Infused Semantic Language Model'
  title_html: '<span class="acl-fixed-case">K</span>now<span class="acl-fixed-case">S</span>em<span
    class="acl-fixed-case">LM</span>: A Knowledge Infused Semantic Language Model'
  url: https://www.aclweb.org/anthology/K19-1051
  year: '2019'
K19-1052:
  abstract: This study proposes a Neural Attentive Bag-of-Entities model, which is
    a neural network model that performs text classification using entities in a knowledge
    base. Entities provide unambiguous and relevant semantic signals that are beneficial
    for text classification. We combine simple high-recall entity detection based
    on a dictionary, to detect entities in a document, with a novel neural attention
    mechanism that enables the model to focus on a small number of unambiguous and
    relevant entities. We tested the effectiveness of our model using two standard
    text classification datasets (i.e., the 20 Newsgroups and R8 datasets) and a popular
    factoid question answering dataset based on a trivia quiz game. As a result, our
    model achieved state-of-the-art results on all datasets. The source code of the
    proposed model is available online at https://github.com/wikipedia2vec/wikipedia2vec.
  address: Hong Kong, China
  author:
  - first: Ikuya
    full: Ikuya Yamada
    id: ikuya-yamada
    last: Yamada
  - first: Hiroyuki
    full: Hiroyuki Shindo
    id: hiroyuki-shindo
    last: Shindo
  author_string: Ikuya Yamada, Hiroyuki Shindo
  bibkey: yamada-shindo-2019-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1052
  month: November
  page_first: '563'
  page_last: '573'
  pages: "563\u2013573"
  paper_id: '52'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1052.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1052.jpg
  title: Neural Attentive Bag-of-Entities Model for Text Classification
  title_html: Neural Attentive Bag-of-Entities Model for Text Classification
  url: https://www.aclweb.org/anthology/K19-1052
  year: '2019'
K19-1053:
  abstract: The official voting records of United States congresspeople are preserved
    as roll call votes. Prediction of voting behavior of politicians for whom no voting
    record exists, such as individuals running for office, is important for forecasting
    key political decisions. Prior work has relied on past votes cast to predict future
    votes, and thus fails to predict voting patterns for politicians without voting
    records. We address this by augmenting a prior state of the art model with multiple
    sources of external knowledge so as to enable prediction on unseen politicians.
    The sources of knowledge we use are news text and Freebase, a manually curated
    knowledge base. We propose augmentations based on unigram features for news text,
    and a knowledge base embedding method followed by a neural network composition
    for relations from Freebase. Empirical evaluation of these approaches indicate
    that the proposed models outperform the prior system for politicians with complete
    historical voting records by 1.0% point of accuracy (8.7% error reduction) and
    for politicians without voting records by 33.4% points of accuracy (66.7% error
    reduction). We also show that the knowledge base augmented approach outperforms
    the news text augmented approach by 4.2% points of accuracy.
  address: Hong Kong, China
  author:
  - first: Pallavi
    full: Pallavi Patil
    id: pallavi-patil
    last: Patil
  - first: Kriti
    full: Kriti Myer
    id: kriti-myer
    last: Myer
  - first: Ronak
    full: Ronak Zala
    id: ronak-zala
    last: Zala
  - first: Arpit
    full: Arpit Singh
    id: arpit-singh
    last: Singh
  - first: Sheshera
    full: Sheshera Mysore
    id: sheshera-mysore
    last: Mysore
  - first: Andrew
    full: Andrew McCallum
    id: andrew-mccallum
    last: McCallum
  - first: Adrian
    full: Adrian Benton
    id: adrian-benton
    last: Benton
  - first: Amanda
    full: Amanda Stent
    id: amanda-stent
    last: Stent
  author_string: Pallavi Patil, Kriti Myer, Ronak Zala, Arpit Singh, Sheshera Mysore,
    Andrew McCallum, Adrian Benton, Amanda Stent
  bibkey: patil-etal-2019-roll
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1053
  month: November
  page_first: '574'
  page_last: '581'
  pages: "574\u2013581"
  paper_id: '53'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1053.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1053.jpg
  title: Roll Call Vote Prediction with Knowledge Augmented Models
  title_html: Roll Call Vote Prediction with Knowledge Augmented Models
  url: https://www.aclweb.org/anthology/K19-1053
  year: '2019'
K19-1054:
  abstract: We propose BeamSeg, a joint model for segmentation and topic identification
    of documents from the same domain. The model assumes that lexical cohesion can
    be observed across documents, meaning that segments describing the same topic
    use a similar lexical distribution over the vocabulary. The model implements lexical
    cohesion in an unsupervised Bayesian setting by drawing from the same language
    model segments with the same topic. Contrary to previous approaches, we assume
    that language models are not independent, since the vocabulary changes in consecutive
    segments are expected to be smooth and not abrupt. We achieve this by using a
    dynamic Dirichlet prior that takes into account data contributions from other
    topics. BeamSeg also models segment length properties of documents based on modality
    (textbooks, slides, etc.). The evaluation is carried out in three datasets. In
    two of them, improvements of up to 4.8% and 7.3% are obtained in the segmentation
    and topic identifications tasks, indicating that both tasks should be jointly
    modeled.
  address: Hong Kong, China
  author:
  - first: Pedro
    full: Pedro Mota
    id: pedro-mota
    last: Mota
  - first: Maxine
    full: Maxine Eskenazi
    id: maxine-eskenazi
    last: Eskenazi
  - first: "Lu\xEDsa"
    full: "Lu\xEDsa Coheur"
    id: luisa-coheur
    last: Coheur
  author_string: "Pedro Mota, Maxine Eskenazi, Lu\xEDsa Coheur"
  bibkey: mota-etal-2019-beamseg
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1054
  month: November
  page_first: '582'
  page_last: '592'
  pages: "582\u2013592"
  paper_id: '54'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1054.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1054.jpg
  title: 'BeamSeg: A Joint Model for Multi-Document Segmentation and Topic Identification'
  title_html: '<span class="acl-fixed-case">B</span>eam<span class="acl-fixed-case">S</span>eg:
    A Joint Model for Multi-Document Segmentation and Topic Identification'
  url: https://www.aclweb.org/anthology/K19-1054
  year: '2019'
K19-1055:
  abstract: This paper focuses on how to extract multiple relational facts from unstructured
    text. Neural encoder-decoder models have provided a viable new approach for jointly
    extracting relations and entity pairs. However, these models either fail to deal
    with entity overlapping among relational facts, or neglect to produce the whole
    entity pairs. In this work, we propose a novel architecture that augments the
    encoder and decoder in two elegant ways. First, we apply a binary CNN classifier
    for each relation, which identifies all possible relations maintained in the text,
    while retaining the target relation representation to aid entity pair recognition.
    Second, we perform a multi-head attention over the text and a triplet attention
    with the target relation interacting with every token of the text to precisely
    produce all possible entity pairs in a sequential manner. Experiments on three
    benchmark datasets show that our proposed method successfully addresses the multiple
    relations and multiple entity pairs even with complex overlapping and significantly
    outperforms the state-of-the-art methods.
  address: Hong Kong, China
  author:
  - first: Jiayu
    full: Jiayu Chen
    id: jiayu-chen
    last: Chen
  - first: Caixia
    full: Caixia Yuan
    id: caixia-yuan
    last: Yuan
  - first: Xiaojie
    full: Xiaojie Wang
    id: xiaojie-wang
    last: Wang
  - first: Ziwei
    full: Ziwei Bai
    id: ziwei-bai
    last: Bai
  author_string: Jiayu Chen, Caixia Yuan, Xiaojie Wang, Ziwei Bai
  bibkey: chen-etal-2019-mrmep
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1055
  month: November
  page_first: '593'
  page_last: '602'
  pages: "593\u2013602"
  paper_id: '55'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1055.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1055.jpg
  title: 'MrMep: Joint Extraction of Multiple Relations and Multiple Entity Pairs
    Based on Triplet Attention'
  title_html: '<span class="acl-fixed-case">M</span>r<span class="acl-fixed-case">M</span>ep:
    Joint Extraction of Multiple Relations and Multiple Entity Pairs Based on Triplet
    Attention'
  url: https://www.aclweb.org/anthology/K19-1055
  year: '2019'
K19-1056:
  abstract: Relation extraction is the task of determining the relation between two
    entities in a sentence. Distantly-supervised models are popular for this task.
    However, sentences can be long and two entities can be located far from each other
    in a sentence. The pieces of evidence supporting the presence of a relation between
    two entities may not be very direct, since the entities may be connected via some
    indirect links such as a third entity or via co-reference. Relation extraction
    in such scenarios becomes more challenging as we need to capture the long-distance
    interactions among the entities and other words in the sentence. Also, the words
    in a sentence do not contribute equally in identifying the relation between the
    two entities. To address this issue, we propose a novel and effective attention
    model which incorporates syntactic information of the sentence and a multi-factor
    attention mechanism. Experiments on the New York Times corpus show that our proposed
    model outperforms prior state-of-the-art models.
  address: Hong Kong, China
  author:
  - first: Tapas
    full: Tapas Nayak
    id: tapas-nayak
    last: Nayak
  - first: Hwee Tou
    full: Hwee Tou Ng
    id: hwee-tou-ng
    last: Ng
  author_string: Tapas Nayak, Hwee Tou Ng
  bibkey: nayak-ng-2019-effective
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1056
  month: November
  page_first: '603'
  page_last: '612'
  pages: "603\u2013612"
  paper_id: '56'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1056.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1056.jpg
  title: Effective Attention Modeling for Neural Relation Extraction
  title_html: Effective Attention Modeling for Neural Relation Extraction
  url: https://www.aclweb.org/anthology/K19-1056
  year: '2019'
K19-1057:
  abstract: Event Detection (ED) is one of the most important task in the field of
    information extraction. The goal of ED is to find triggers in sentences and classify
    them into different event types. In previous works, the information of entity
    types are commonly utilized to benefit event detection. However, the sequential
    features of entity types have not been well utilized yet in the existing ED methods.
    In this paper, we propose a novel ED approach which learns sequential features
    from word sequences and entity type sequences separately, and combines these two
    types of sequential features with the help of a trigger-entity interaction learning
    module. The experimental results demonstrate that our proposed approach outperforms
    the state-of-the-art methods.
  address: Hong Kong, China
  author:
  - first: Yuze
    full: Yuze Ji
    id: yuze-ji
    last: Ji
  - first: Youfang
    full: Youfang Lin
    id: youfang-lin
    last: Lin
  - first: Jianwei
    full: Jianwei Gao
    id: jianwei-gao
    last: Gao
  - first: Huaiyu
    full: Huaiyu Wan
    id: huaiyu-wan
    last: Wan
  author_string: Yuze Ji, Youfang Lin, Jianwei Gao, Huaiyu Wan
  bibkey: ji-etal-2019-exploiting
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1057
  month: November
  page_first: '613'
  page_last: '623'
  pages: "613\u2013623"
  paper_id: '57'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1057.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1057.jpg
  title: Exploiting the Entity Type Sequence to Benefit Event Detection
  title_html: Exploiting the Entity Type Sequence to Benefit Event Detection
  url: https://www.aclweb.org/anthology/K19-1057
  year: '2019'
K19-1058:
  abstract: Recent developments in Named Entity Recognition (NER) have resulted in
    better and better models. However, is there a glass ceiling? Do we know which
    types of errors are still hard or even impossible to correct? In this paper, we
    present a detailed analysis of the types of errors in state-of-the-art machine
    learning (ML) methods. Our study illustrates weak and strong points of the Stanford,
    CMU, FLAIR, ELMO and BERT models, as well as their shared limitations. We also
    introduce new techniques for improving annotation, training process, and for checking
    model quality and stability.
  address: Hong Kong, China
  attachment:
  - filename: K19-1058.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-1058.Attachment.pdf
  - filename: K19-1058.Supplementary_Material.pdf
    type: supplementary-material
    url: https://www.aclweb.org/anthology/attachments/K19-1058.Supplementary_Material.pdf
  author:
  - first: Tomasz
    full: Tomasz Stanislawek
    id: tomasz-stanislawek
    last: Stanislawek
  - first: Anna
    full: "Anna Wr\xF3blewska"
    id: anna-wroblewska
    last: "Wr\xF3blewska"
  - first: Alicja
    full: "Alicja W\xF3jcicka"
    id: alicja-wojcicka
    last: "W\xF3jcicka"
  - first: Daniel
    full: Daniel Ziembicki
    id: daniel-ziembicki
    last: Ziembicki
  - first: Przemyslaw
    full: Przemyslaw Biecek
    id: przemyslaw-biecek
    last: Biecek
  author_string: "Tomasz Stanislawek, Anna Wr\xF3blewska, Alicja W\xF3jcicka, Daniel\
    \ Ziembicki, Przemyslaw Biecek"
  bibkey: stanislawek-etal-2019-named
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1058
  month: November
  page_first: '624'
  page_last: '633'
  pages: "624\u2013633"
  paper_id: '58'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1058.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1058.jpg
  title: Named Entity Recognition - Is There a Glass Ceiling?
  title_html: Named Entity Recognition - Is There a Glass Ceiling?
  url: https://www.aclweb.org/anthology/K19-1058
  year: '2019'
K19-1059:
  abstract: Document embeddings, created with methods ranging from simple heuristics
    to statistical and deep models, are widely applicable. Bag-of-vectors models for
    documents include the mean and quadratic approaches (Torki, 2018). We present
    evidence that quadratic statistics alone, without the mean information, can offer
    superior accuracy, fast document comparison, and compact document representations.
    In matching news articles to their comment threads, low-rank representations of
    only 3-4 times the size of the mean vector give most accurate matching, and in
    standard sentence comparison tasks, results are state of the art despite faster
    computation. Similarity measures are discussed, and the Frobenius product implicit
    in the proposed method is contrasted to Wasserstein or Bures metric from the transportation
    theory. We also shortly demonstrate matching of unordered word lists to documents,
    to measure topicality or sentiment of documents.
  address: Hong Kong, China
  author:
  - first: Jarkko
    full: Jarkko Lagus
    id: jarkko-lagus
    last: Lagus
  - first: Janne
    full: Janne Sinkkonen
    id: janne-sinkkonen
    last: Sinkkonen
  - first: Arto
    full: Arto Klami
    id: arto-klami
    last: Klami
  author_string: Jarkko Lagus, Janne Sinkkonen, Arto Klami
  bibkey: lagus-etal-2019-low
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1059
  month: November
  page_first: '634'
  page_last: '644'
  pages: "634\u2013644"
  paper_id: '59'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1059.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1059.jpg
  title: Low-Rank Approximations of Second-Order Document Representations
  title_html: Low-Rank Approximations of Second-Order Document Representations
  url: https://www.aclweb.org/anthology/K19-1059
  year: '2019'
K19-1060:
  abstract: Supervised machine learning assumes the availability of fully-labeled
    data, but in many cases, such as low-resource languages, the only data available
    is partially annotated. We study the problem of Named Entity Recognition (NER)
    with partially annotated training data in which a fraction of the named entities
    are labeled, and all other tokens, entities or otherwise, are labeled as non-entity
    by default. In order to train on this noisy dataset, we need to distinguish between
    the true and false negatives. To this end, we introduce a constraint-driven iterative
    algorithm that learns to detect false negatives in the noisy set and downweigh
    them, resulting in a weighted training set. With this set, we train a weighted
    NER model. We evaluate our algorithm with weighted variants of neural and non-neural
    NER models on data in 8 languages from several language and script families, showing
    strong ability to learn from partial data. Finally, to show real-world efficacy,
    we evaluate on a Bengali NER corpus annotated by non-speakers, outperforming the
    prior state-of-the-art by over 5 points F1.
  address: Hong Kong, China
  author:
  - first: Stephen
    full: Stephen Mayhew
    id: stephen-mayhew
    last: Mayhew
  - first: Snigdha
    full: Snigdha Chaturvedi
    id: snigdha-chaturvedi
    last: Chaturvedi
  - first: Chen-Tse
    full: Chen-Tse Tsai
    id: chen-tse-tsai
    last: Tsai
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Stephen Mayhew, Snigdha Chaturvedi, Chen-Tse Tsai, Dan Roth
  bibkey: mayhew-etal-2019-named
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1060
  month: November
  page_first: '645'
  page_last: '655'
  pages: "645\u2013655"
  paper_id: '60'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1060.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1060.jpg
  title: Named Entity Recognition with Partially Annotated Training Data
  title_html: Named Entity Recognition with Partially Annotated Training Data
  url: https://www.aclweb.org/anthology/K19-1060
  year: '2019'
K19-1061:
  abstract: Event trigger extraction is an information extraction task of practical
    utility, yet it is challenging due to the difficulty of disambiguating word sense
    meaning. Previous approaches rely extensively on hand-crafted language-specific
    features and are applied mainly to English for which annotated datasets and Natural
    Language Processing (NLP) tools are available. However, the availability of such
    resources varies from one language to another. Recently, contextualized Bidirectional
    Encoder Representations from Transformers (BERT) models have established state-of-the-art
    performance for a variety of NLP tasks. However, there has not been much effort
    in exploring language transfer using BERT for event extraction. In this work,
    we treat event trigger extraction as a sequence tagging problem and propose a
    cross-lingual framework for training it without any hand-crafted features. We
    experiment with different flavors of transfer learning from high-resourced to
    low-resourced languages and compare the performance of different multilingual
    embeddings for event trigger extraction. Our results show that training in a multilingual
    setting outperforms language-specific models for both English and Chinese. Our
    work is the first to experiment with two event architecture variants in a cross-lingual
    setting, to show the effectiveness of contextualized embeddings obtained using
    BERT, and to explore and analyze its performance on Arabic.
  address: Hong Kong, China
  attachment:
  - filename: K19-1061.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-1061.Attachment.pdf
  - filename: K19-1061.Supplementary_Material.pdf
    type: supplementary-material
    url: https://www.aclweb.org/anthology/attachments/K19-1061.Supplementary_Material.pdf
  author:
  - first: Meryem
    full: "Meryem M\u2019hamdi"
    id: meryem-mhamdi
    last: "M\u2019hamdi"
  - first: Marjorie
    full: Marjorie Freedman
    id: marjorie-freedman
    last: Freedman
  - first: Jonathan
    full: Jonathan May
    id: jonathan-may
    last: May
  author_string: "Meryem M\u2019hamdi, Marjorie Freedman, Jonathan May"
  bibkey: mhamdi-etal-2019-contextualized
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1061
  month: November
  page_first: '656'
  page_last: '665'
  pages: "656\u2013665"
  paper_id: '61'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1061.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1061.jpg
  title: Contextualized Cross-Lingual Event Trigger Extraction with Minimal Resources
  title_html: Contextualized Cross-Lingual Event Trigger Extraction with Minimal Resources
  url: https://www.aclweb.org/anthology/K19-1061
  year: '2019'
K19-1062:
  abstract: We propose a novel deep structured learning framework for event temporal
    relation extraction. The model consists of 1) a recurrent neural network (RNN)
    to learn scoring functions for pair-wise relations, and 2) a structured support
    vector machine (SSVM) to make joint predictions. The neural network automatically
    learns representations that account for long-term contexts to provide robust features
    for the structured model, while the SSVM incorporates domain knowledge such as
    transitive closure of temporal relations as constraints to make better globally
    consistent decisions. By jointly training the two components, our model combines
    the benefits of both data-driven learning and knowledge exploitation. Experimental
    results on three high-quality event temporal relation datasets (TCR, MATRES, and
    TB-Dense) demonstrate that incorporated with pre-trained contextualized embeddings,
    the proposed model achieves significantly better performances than the state-of-the-art
    methods on all three datasets. We also provide thorough ablation studies to investigate
    our model.
  address: Hong Kong, China
  author:
  - first: Rujun
    full: Rujun Han
    id: rujun-han
    last: Han
  - first: I-Hung
    full: I-Hung Hsu
    id: i-hung-hsu
    last: Hsu
  - first: Mu
    full: Mu Yang
    id: mu-yang
    last: Yang
  - first: Aram
    full: Aram Galstyan
    id: aram-galstyan
    last: Galstyan
  - first: Ralph
    full: Ralph Weischedel
    id: ralph-weischedel
    last: Weischedel
  - first: Nanyun
    full: Nanyun Peng
    id: nanyun-peng
    last: Peng
  author_string: Rujun Han, I-Hung Hsu, Mu Yang, Aram Galstyan, Ralph Weischedel,
    Nanyun Peng
  bibkey: han-etal-2019-deep
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1062
  month: November
  page_first: '666'
  page_last: '106'
  pages: "666\u2013106"
  paper_id: '62'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1062.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1062.jpg
  title: Deep Structured Neural Network for Event Temporal Relation Extraction
  title_html: Deep Structured Neural Network for Event Temporal Relation Extraction
  url: https://www.aclweb.org/anthology/K19-1062
  year: '2019'
K19-1063:
  abstract: "A typical architecture for end-to-end entity linking systems consists\
    \ of three steps: mention detection, candidate generation and entity disambiguation.\
    \ In this study we investigate the following questions: (a) Can all those steps\
    \ be learned jointly with a model for contextualized text-representations, i.e.\
    \ BERT? (b) How much entity knowledge is already contained in pretrained BERT?\
    \ (c) Does additional entity knowledge improve BERT\u2019s performance in downstream\
    \ tasks? To this end we propose an extreme simplification of the entity linking\
    \ setup that works surprisingly well: simply cast it as a per token classification\
    \ over the entire entity vocabulary (over 700K classes in our case). We show on\
    \ an entity linking benchmark that (i) this model improves the entity representations\
    \ over plain BERT, (ii) that it outperforms entity linking architectures that\
    \ optimize the tasks separately and (iii) that it only comes second to the current\
    \ state-of-the-art that does mention detection and entity disambiguation jointly.\
    \ Additionally, we investigate the usefulness of entity-aware token-representations\
    \ in the text-understanding benchmark GLUE, as well as the question answering\
    \ benchmarks SQUAD~V2 and SWAG and also the EN-DE WMT14 machine translation benchmark.\
    \ To our surprise, we find that most of those benchmarks do not benefit from additional\
    \ entity knowledge, except for a task with very small training data, the RTE task\
    \ in GLUE, which improves by 2%. classes in our case). We show on an entity linking\
    \ benchmark that (i) this model improves the entity representations over plain\
    \ BERT, (ii) that it outperforms entity linking architectures that optimize the\
    \ tasks separately and (iii) that it only comes second to the current state-of-the-art\
    \ that does mention detection and entity disambiguation jointly. Additionally,\
    \ we investigate the usefulness of entity-aware token-representations in the text-understanding\
    \ benchmark GLUE, as well as the question answering benchmarks SQUAD~V2 and SWAG\
    \ and also the EN-DE WMT14 machine translation benchmark. To our surprise, we\
    \ find that most of those benchmarks do not benefit from additional entity knowledge,\
    \ except for a task with very small training data, the RTE task in GLUE, which\
    \ improves by 2%."
  address: Hong Kong, China
  author:
  - first: Samuel
    full: Samuel Broscheit
    id: samuel-broscheit
    last: Broscheit
  author_string: Samuel Broscheit
  bibkey: broscheit-2019-investigating
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1063
  month: November
  page_first: '677'
  page_last: '685'
  pages: "677\u2013685"
  paper_id: '63'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1063.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1063.jpg
  title: Investigating Entity Knowledge in BERT with Simple Neural End-To-End Entity
    Linking
  title_html: Investigating Entity Knowledge in <span class="acl-fixed-case">BERT</span>
    with Simple Neural End-To-End Entity Linking
  url: https://www.aclweb.org/anthology/K19-1063
  year: '2019'
K19-1064:
  abstract: Implicit discourse relations are not only more challenging to classify,
    but also to annotate, than their explicit counterparts. We tackle situations where
    training data for implicit relations are lacking, and exploit domain adaptation
    from explicit relations (Ji et al., 2015). We present an unsupervised adversarial
    domain adaptive network equipped with a reconstruction component. Our system outperforms
    prior works and other adversarial benchmarks for unsupervised domain adaptation.
    Additionally, we extend our system to take advantage of labeled data if some are
    available.
  address: Hong Kong, China
  author:
  - first: Hsin-Ping
    full: Hsin-Ping Huang
    id: hsin-ping-huang
    last: Huang
  - first: Junyi Jessy
    full: Junyi Jessy Li
    id: junyi-jessy-li
    last: Li
  author_string: Hsin-Ping Huang, Junyi Jessy Li
  bibkey: huang-li-2019-unsupervised
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1064
  month: November
  page_first: '686'
  page_last: '695'
  pages: "686\u2013695"
  paper_id: '64'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1064.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1064.jpg
  title: Unsupervised Adversarial Domain Adaptation for Implicit Discourse Relation
    Classification
  title_html: Unsupervised Adversarial Domain Adaptation for Implicit Discourse Relation
    Classification
  url: https://www.aclweb.org/anthology/K19-1064
  year: '2019'
K19-1065:
  abstract: 'Remarkable success has been achieved in the last few years on some limited
    machine reading comprehension (MRC) tasks. However, it is still difficult to interpret
    the predictions of existing MRC models. In this paper, we focus on extracting
    evidence sentences that can explain or support the answers of multiple-choice
    MRC tasks, where the majority of answer options cannot be directly extracted from
    reference documents. Due to the lack of ground truth evidence sentence labels
    in most cases, we apply distant supervision to generate imperfect labels and then
    use them to train an evidence sentence extractor. To denoise the noisy labels,
    we apply a recently proposed deep probabilistic logic learning framework to incorporate
    both sentence-level and cross-sentence linguistic indicators for indirect supervision.
    We feed the extracted evidence sentences into existing MRC models and evaluate
    the end-to-end performance on three challenging multiple-choice MRC datasets:
    MultiRC, RACE, and DREAM, achieving comparable or better performance than the
    same models that take as input the full reference document. To the best of our
    knowledge, this is the first work extracting evidence sentences for multiple-choice
    MRC.'
  address: Hong Kong, China
  attachment:
  - filename: K19-1065.Supplementary_Material.pdf
    type: supplementary-material
    url: https://www.aclweb.org/anthology/attachments/K19-1065.Supplementary_Material.pdf
  - filename: K19-1065.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-1065.Attachment.pdf
  author:
  - first: Hai
    full: Hai Wang
    id: hai-wang
    last: Wang
  - first: Dian
    full: Dian Yu
    id: dian-yu
    last: Yu
  - first: Kai
    full: Kai Sun
    id: kai-sun
    last: Sun
  - first: Jianshu
    full: Jianshu Chen
    id: jianshu-chen
    last: Chen
  - first: Dong
    full: Dong Yu
    id: dong-yu
    last: Yu
  - first: David
    full: David McAllester
    id: david-mcallester
    last: McAllester
  - first: Dan
    full: Dan Roth
    id: dan-roth
    last: Roth
  author_string: Hai Wang, Dian Yu, Kai Sun, Jianshu Chen, Dong Yu, David McAllester,
    Dan Roth
  bibkey: wang-etal-2019-evidence
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1065
  month: November
  page_first: '696'
  page_last: '707'
  pages: "696\u2013707"
  paper_id: '65'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1065.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1065.jpg
  title: Evidence Sentence Extraction for Machine Reading Comprehension
  title_html: Evidence Sentence Extraction for Machine Reading Comprehension
  url: https://www.aclweb.org/anthology/K19-1065
  year: '2019'
K19-1066:
  abstract: "Conversational AI systems are gaining a lot of attention recently in\
    \ both industrial and scientific domains, providing a natural way of interaction\
    \ between customers and adaptive intelligent systems. A key requirement in these\
    \ systems is the ability to understand the user\u2019s intent and provide adequate\
    \ responses to them. One of the greatest challenges of language understanding\
    \ (LU) services is efficient utterance (sentence) representation in vector space,\
    \ which is an essential step for most ML tasks. In this paper, we propose a novel\
    \ approach for generating vector space representations of utterances using pair-wise\
    \ similarity metrics. The proposed approach uses only a few corpora to tune the\
    \ weights of the similarity metric without relying on external general purpose\
    \ ontologies. Our experiments confirm that the generated vectors can improve the\
    \ performance of LU services in unsupervised, semi-supervised and supervised learning\
    \ tasks."
  address: Hong Kong, China
  author:
  - first: Ashraf
    full: Ashraf Mahgoub
    id: ashraf-mahgoub
    last: Mahgoub
  - first: Youssef
    full: Youssef Shahin
    id: youssef-shahin
    last: Shahin
  - first: Riham
    full: Riham Mansour
    id: riham-mansour
    last: Mansour
  - first: Saurabh
    full: Saurabh Bagchi
    id: saurabh-bagchi
    last: Bagchi
  author_string: Ashraf Mahgoub, Youssef Shahin, Riham Mansour, Saurabh Bagchi
  bibkey: mahgoub-etal-2019-simvecs
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1066
  month: November
  page_first: '708'
  page_last: '717'
  pages: "708\u2013717"
  paper_id: '66'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1066.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1066.jpg
  title: 'SimVecs: Similarity-Based Vectors for Utterance Representation in Conversational
    AI Systems'
  title_html: '<span class="acl-fixed-case">S</span>im<span class="acl-fixed-case">V</span>ecs:
    Similarity-Based Vectors for Utterance Representation in Conversational <span
    class="acl-fixed-case">AI</span> Systems'
  url: https://www.aclweb.org/anthology/K19-1066
  year: '2019'
K19-1067:
  abstract: "Conventional chatbots focus on two-party response generation, which simplifies\
    \ the real dialogue scene. In this paper, we strive toward a novel task of Response\
    \ Generation on Multi-Party Chatbot (RGMPC), where the generated responses heavily\
    \ rely on the interlocutors\u2019 roles (e.g., speaker and addressee) and their\
    \ utterances. Unfortunately, complex interactions among the interlocutors\u2019\
    \ roles make it challenging to precisely capture conversational contexts and interlocutors\u2019\
    \ information. Facing this challenge, we present a response generation model which\
    \ incorporates Interlocutor-aware Contexts into Recurrent Encoder-Decoder frameworks\
    \ (ICRED) for RGMPC. Specifically, we employ interactive representations to capture\
    \ dialogue contexts for different interlocutors. Moreover, we leverage an addressee\
    \ memory to enhance contextual interlocutor information for the target addressee.\
    \ Finally, we construct a corpus for RGMPC based on an existing open-access dataset.\
    \ Automatic and manual evaluations demonstrate that the ICRED remarkably outperforms\
    \ strong baselines."
  address: Hong Kong, China
  author:
  - first: Cao
    full: Cao Liu
    id: cao-liu
    last: Liu
  - first: Kang
    full: Kang Liu
    id: kang-liu
    last: Liu
  - first: Shizhu
    full: Shizhu He
    id: shizhu-he
    last: He
  - first: Zaiqing
    full: Zaiqing Nie
    id: zaiqing-nie
    last: Nie
  - first: Jun
    full: Jun Zhao
    id: jun-zhao
    last: Zhao
  author_string: Cao Liu, Kang Liu, Shizhu He, Zaiqing Nie, Jun Zhao
  bibkey: liu-etal-2019-incorporating-interlocutor
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1067
  month: November
  page_first: '718'
  page_last: '727'
  pages: "718\u2013727"
  paper_id: '67'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1067.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1067.jpg
  title: Incorporating Interlocutor-Aware Context into Response Generation on Multi-Party
    Chatbots
  title_html: Incorporating Interlocutor-Aware Context into Response Generation on
    Multi-Party Chatbots
  url: https://www.aclweb.org/anthology/K19-1067
  year: '2019'
K19-1068:
  abstract: We introduce Episodic Memory QA, the task of answering personal user questions
    grounded on memory graph (MG), where episodic memories and related entity nodes
    are connected via relational edges. We create a new benchmark dataset first by
    generating synthetic memory graphs with simulated attributes, and by composing
    100K QA pairs for the generated MG with bootstrapped scripts. To address the unique
    challenges for the proposed task, we propose Memory Graph Networks (MGN), a novel
    extension of memory networks to enable dynamic expansion of memory slots through
    graph traversals, thus able to answer queries in which contexts from multiple
    linked episodes and external knowledge are required. We then propose the Episodic
    Memory QA Net with multiple module networks to effectively handle various question
    types. Empirical results show improvement over the QA baselines in top-k answer
    prediction accuracy in the proposed task. The proposed model also generates a
    graph walk path and attention vectors for each predicted answer, providing a natural
    way to explain its QA reasoning.
  address: Hong Kong, China
  author:
  - first: Seungwhan
    full: Seungwhan Moon
    id: seungwhan-moon
    last: Moon
  - first: Pararth
    full: Pararth Shah
    id: pararth-shah
    last: Shah
  - first: Anuj
    full: Anuj Kumar
    id: anuj-kumar
    last: Kumar
  - first: Rajen
    full: Rajen Subba
    id: rajen-subba
    last: Subba
  author_string: Seungwhan Moon, Pararth Shah, Anuj Kumar, Rajen Subba
  bibkey: moon-etal-2019-memory-graph
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1068
  month: November
  page_first: '728'
  page_last: '736'
  pages: "728\u2013736"
  paper_id: '68'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1068.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1068.jpg
  title: Memory Graph Networks for Explainable Memory-grounded Question Answering
  title_html: Memory Graph Networks for Explainable Memory-grounded Question Answering
  url: https://www.aclweb.org/anthology/K19-1068
  year: '2019'
K19-1069:
  abstract: We consider the importance of different utterances in the context for
    selecting the response usually depends on the current query. In this paper, we
    propose the model TripleNet to fully model the task with the triple <context,
    query, response> instead of <context, response > in previous works. The heart
    of TripleNet is a novel attention mechanism named triple attention to model the
    relationships within the triple at four levels. The new mechanism updates the
    representation of each element based on the attention with the other two concurrently
    and symmetrically.We match the triple <C, Q, R> centered on the response from
    char to context level for prediction.Experimental results on two large-scale multi-turn
    response selection datasets show that the proposed model can significantly outperform
    the state-of-the-art methods.
  address: Hong Kong, China
  author:
  - first: Wentao
    full: Wentao Ma
    id: wentao-ma
    last: Ma
  - first: Yiming
    full: Yiming Cui
    id: yiming-cui
    last: Cui
  - first: Nan
    full: Nan Shao
    id: nan-shao
    last: Shao
  - first: Su
    full: Su He
    id: su-he
    last: He
  - first: Wei-Nan
    full: Wei-Nan Zhang
    id: weinan-zhang
    last: Zhang
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  - first: Shijin
    full: Shijin Wang
    id: shijin-wang
    last: Wang
  - first: Guoping
    full: Guoping Hu
    id: guoping-hu
    last: Hu
  author_string: Wentao Ma, Yiming Cui, Nan Shao, Su He, Wei-Nan Zhang, Ting Liu,
    Shijin Wang, Guoping Hu
  bibkey: ma-etal-2019-triplenet
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1069
  month: November
  page_first: '737'
  page_last: '746'
  pages: "737\u2013746"
  paper_id: '69'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1069.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1069.jpg
  title: 'TripleNet: Triple Attention Network for Multi-Turn Response Selection in
    Retrieval-Based Chatbots'
  title_html: '<span class="acl-fixed-case">T</span>riple<span class="acl-fixed-case">N</span>et:
    Triple Attention Network for Multi-Turn Response Selection in Retrieval-Based
    Chatbots'
  url: https://www.aclweb.org/anthology/K19-1069
  year: '2019'
K19-1070:
  abstract: "Machine reading comprehension (MRC) has attracted significant amounts\
    \ of research attention recently, due to an increase of challenging reading comprehension\
    \ datasets. In this paper, we aim to improve a MRC model\u2019s ability to determine\
    \ whether a question has an answer in a given context (e.g. the recently proposed\
    \ SQuAD 2.0 task). The relation module consists of both semantic extraction and\
    \ relational information. We first extract high level semantics as objects from\
    \ both question and context with multi-head self-attentive pooling. These semantic\
    \ objects are then passed to a relation network, which generates relationship\
    \ scores for each object pair in a sentence. These scores are used to determine\
    \ whether a question is non-answerable. We test the relation module on the SQuAD\
    \ 2.0 dataset using both the BiDAF and BERT models as baseline readers. We obtain\
    \ 1.8% gain of F1 accuracy on top of the BiDAF reader, and 1.0% on top of the\
    \ BERT base model. These results show the effectiveness of our relation module\
    \ on MRC."
  address: Hong Kong, China
  author:
  - first: Kevin
    full: Kevin Huang
    id: kevin-huang
    last: Huang
  - first: Yun
    full: Yun Tang
    id: yun-tang
    last: Tang
  - first: Jing
    full: Jing Huang
    id: jing-huang
    last: Huang
  - first: Xiaodong
    full: Xiaodong He
    id: xiaodong-he
    last: He
  - first: Bowen
    full: Bowen Zhou
    id: bowen-zhou
    last: Zhou
  author_string: Kevin Huang, Yun Tang, Jing Huang, Xiaodong He, Bowen Zhou
  bibkey: huang-etal-2019-relation
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1070
  month: November
  page_first: '747'
  page_last: '756'
  pages: "747\u2013756"
  paper_id: '70'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1070.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1070.jpg
  title: Relation Module for Non-Answerable Predictions on Reading Comprehension
  title_html: Relation Module for Non-Answerable Predictions on Reading Comprehension
  url: https://www.aclweb.org/anthology/K19-1070
  year: '2019'
K19-1071:
  abstract: Task oriented language understanding (LU) in human-to-machine (H2M) conversations
    has been extensively studied for personal digital assistants. In this work, we
    extend the task oriented LU problem to human-to-human (H2H) conversations, focusing
    on the slot tagging task. Recent advances on LU in H2M conversations have shown
    accuracy improvements by adding encoded knowledge from different sources. Inspired
    by this, we explore several variants of a bidirectional LSTM architecture that
    relies on different knowledge sources, such as Web data, search engine click logs,
    expert feedback from H2M models, as well as previous utterances in the conversation.
    We also propose ensemble techniques that aggregate these different knowledge sources
    into a single model. Experimental evaluation on a four-turn Twitter dataset in
    the restaurant and music domains shows improvements in the slot tagging F1-score
    of up to 6.09% compared to existing approaches.
  address: Hong Kong, China
  author:
  - first: Kunho
    full: Kunho Kim
    id: kunho-kim
    last: Kim
  - first: Rahul
    full: Rahul Jha
    id: rahul-jha
    last: Jha
  - first: Kyle
    full: Kyle Williams
    id: kyle-williams
    last: Williams
  - first: Alex
    full: Alex Marin
    id: alex-marin
    last: Marin
  - first: Imed
    full: Imed Zitouni
    id: imed-zitouni
    last: Zitouni
  author_string: Kunho Kim, Rahul Jha, Kyle Williams, Alex Marin, Imed Zitouni
  bibkey: kim-etal-2019-slot
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1071
  month: November
  page_first: '757'
  page_last: '767'
  pages: "757\u2013767"
  paper_id: '71'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1071.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1071.jpg
  title: Slot Tagging for Task Oriented Spoken Language Understanding in Human-to-Human
    Conversation Scenarios
  title_html: Slot Tagging for Task Oriented Spoken Language Understanding in Human-to-Human
    Conversation Scenarios
  url: https://www.aclweb.org/anthology/K19-1071
  year: '2019'
K19-1072:
  abstract: This paper describes a novel approach for the task of end-to-end argument
    labeling in shallow discourse parsing. Our method describes a decomposition of
    the overall labeling task into subtasks and a general distance-based aggregation
    procedure. For learning these subtasks, we train a recurrent neural network and
    gradually replace existing components of our baseline by our model. The model
    is trained and evaluated on the Penn Discourse Treebank 2 corpus. While it is
    not as good as knowledge-intense approaches, it clearly outperforms other models
    that are also trained without additional linguistic features.
  address: Hong Kong, China
  author:
  - first: "Ren\xE9"
    full: "Ren\xE9 Knaebel"
    id: rene-knaebel
    last: Knaebel
  - first: Manfred
    full: Manfred Stede
    id: manfred-stede
    last: Stede
  - first: Sebastian
    full: Sebastian Stober
    id: sebastian-stober
    last: Stober
  author_string: "Ren\xE9 Knaebel, Manfred Stede, Sebastian Stober"
  bibkey: knaebel-etal-2019-window
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1072
  month: November
  page_first: '768'
  page_last: '777'
  pages: "768\u2013777"
  paper_id: '72'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1072.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1072.jpg
  title: Window-Based Neural Tagging for Shallow Discourse Argument Labeling
  title_html: Window-Based Neural Tagging for Shallow Discourse Argument Labeling
  url: https://www.aclweb.org/anthology/K19-1072
  year: '2019'
K19-1073:
  abstract: Content of text data are often influenced by contextual factors which
    often evolve over time (e.g., content of social media are often influenced by
    topics covered in the major news streams). Existing language models do not consider
    the influence of such related evolving topics, and thus are not optimal. In this
    paper, we propose to incorporate such topical-influence into a language model
    to both improve its accuracy and enable cross-stream analysis of topical influences.
    Specifically, we propose a novel language model called Topical Influence Language
    Model (TILM), which is a novel extension of a neural language model to capture
    the influences on the contents in one text stream by the evolving topics in another
    related (or possibly same) text stream. Experimental results on six different
    text stream data comprised of conference paper titles show that the incorporation
    of evolving topical influence into a language model is beneficial and TILM outperforms
    multiple baselines in a challenging task of text forecasting. In addition to serving
    as a language model, TILM further enables interesting analysis of topical influence
    among multiple text streams.
  address: Hong Kong, China
  attachment:
  - filename: K19-1073.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-1073.Attachment.zip
  author:
  - first: Shubhra Kanti
    full: Shubhra Kanti Karmaker Santu
    id: shubhra-kanti-karmaker-santu
    last: Karmaker Santu
  - first: Kalyan
    full: Kalyan Veeramachaneni
    id: kalyan-veeramachaneni
    last: Veeramachaneni
  - first: Chengxiang
    full: Chengxiang Zhai
    id: chengxiang-zhai
    last: Zhai
  author_string: Shubhra Kanti Karmaker Santu, Kalyan Veeramachaneni, Chengxiang Zhai
  bibkey: karmaker-santu-etal-2019-tilm
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1073
  month: November
  page_first: '778'
  page_last: '788'
  pages: "778\u2013788"
  paper_id: '73'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1073.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1073.jpg
  title: 'TILM: Neural Language Models with Evolving Topical Influence'
  title_html: '<span class="acl-fixed-case">TILM</span>: Neural Language Models with
    Evolving Topical Influence'
  url: https://www.aclweb.org/anthology/K19-1073
  year: '2019'
K19-1074:
  abstract: In this paper, we propose a novel pretraining-based encoder-decoder framework,
    which can generate the output sequence based on the input sequence in a two-stage
    manner. For the encoder of our model, we encode the input sequence into context
    representations using BERT. For the decoder, there are two stages in our model,
    in the first stage, we use a Transformer-based decoder to generate a draft output
    sequence. In the second stage, we mask each word of the draft sequence and feed
    it to BERT, then by combining the input sequence and the draft representation
    generated by BERT, we use a Transformer-based decoder to predict the refined word
    for each masked position. To the best of our knowledge, our approach is the first
    method which applies the BERT into text generation tasks. As the first step in
    this direction, we evaluate our proposed method on the text summarization task.
    Experimental results show that our model achieves new state-of-the-art on both
    CNN/Daily Mail and New York Times datasets.
  address: Hong Kong, China
  author:
  - first: Haoyu
    full: Haoyu Zhang
    id: haoyu-zhang
    last: Zhang
  - first: Jingjing
    full: Jingjing Cai
    id: jingjing-cai
    last: Cai
  - first: Jianjun
    full: Jianjun Xu
    id: jianjun-xu
    last: Xu
  - first: Ji
    full: Ji Wang
    id: ji-wang
    last: Wang
  author_string: Haoyu Zhang, Jingjing Cai, Jianjun Xu, Ji Wang
  bibkey: zhang-etal-2019-pretraining
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1074
  month: November
  page_first: '789'
  page_last: '797'
  pages: "789\u2013797"
  paper_id: '74'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1074.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1074.jpg
  title: Pretraining-Based Natural Language Generation for Text Summarization
  title_html: Pretraining-Based Natural Language Generation for Text Summarization
  url: https://www.aclweb.org/anthology/K19-1074
  year: '2019'
K19-1075:
  abstract: Hierarchical neural networks are often used to model inherent structures
    within dialogues. For goal-oriented dialogues, these models miss a mechanism adhering
    to the goals and neglect the distinct conversational patterns between two interlocutors.
    In this work, we propose Goal-Embedded Dual Hierarchical Attentional Encoder-Decoder
    (G-DuHA) able to center around goals and capture interlocutor-level disparity
    while modeling goal-oriented dialogues. Experiments on dialogue generation, response
    generation, and human evaluations demonstrate that the proposed model successfully
    generates higher-quality, more diverse and goal-centric dialogues. Moreover, we
    apply data augmentation via goal-oriented dialogue generation for task-oriented
    dialog systems with better performance achieved.
  address: Hong Kong, China
  attachment:
  - filename: K19-1075.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-1075.Attachment.zip
  author:
  - first: Yi-An
    full: Yi-An Lai
    id: yi-an-lai
    last: Lai
  - first: Arshit
    full: Arshit Gupta
    id: arshit-gupta
    last: Gupta
  - first: Yi
    full: Yi Zhang
    id: yi-zhang
    last: Zhang
  author_string: Yi-An Lai, Arshit Gupta, Yi Zhang
  bibkey: lai-etal-2019-goal
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1075
  month: November
  page_first: '798'
  page_last: '811'
  pages: "798\u2013811"
  paper_id: '75'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1075.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1075.jpg
  title: Goal-Embedded Dual Hierarchical Model for Task-Oriented Dialogue Generation
  title_html: Goal-Embedded Dual Hierarchical Model for Task-Oriented Dialogue Generation
  url: https://www.aclweb.org/anthology/K19-1075
  year: '2019'
K19-1076:
  abstract: Automatic question generation (QG) is a useful yet challenging task in
    NLP. Recent neural network-based approaches represent the state-of-the-art in
    this task. In this work, we attempt to strengthen them significantly by adopting
    a holistic and novel generator-evaluator framework that directly optimizes objectives
    that reward semantics and structure. The generator is a sequence-to-sequence model
    that incorporates the structure and semantics of the question being generated.
    The generator predicts an answer in the passage that the question can pivot on.
    Employing the copy and coverage mechanisms, it also acknowledges other contextually
    important (and possibly rare) keywords in the passage that the question needs
    to conform to, while not redundantly repeating words. The evaluator model evaluates
    and assigns a reward to each predicted question based on its conformity to the
    structure of ground-truth questions. We propose two novel QG-specific reward functions
    for text conformity and answer conformity of the generated question. The evaluator
    also employs structure-sensitive rewards based on evaluation measures such as
    BLEU, GLEU, and ROUGE-L, which are suitable for QG. In contrast, most of the previous
    works only optimize the cross-entropy loss, which can induce inconsistencies between
    training (objective) and testing (evaluation) measures. Our evaluation shows that
    our approach significantly outperforms state-of-the-art systems on the widely-used
    SQuAD benchmark as per both automatic and human evaluation.
  address: Hong Kong, China
  author:
  - first: Vishwajeet
    full: Vishwajeet Kumar
    id: vishwajeet-kumar
    last: Kumar
  - first: Ganesh
    full: Ganesh Ramakrishnan
    id: ganesh-ramakrishnan
    last: Ramakrishnan
  - first: Yuan-Fang
    full: Yuan-Fang Li
    id: yuan-fang-li
    last: Li
  author_string: Vishwajeet Kumar, Ganesh Ramakrishnan, Yuan-Fang Li
  bibkey: kumar-etal-2019-putting
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1076
  month: November
  page_first: '812'
  page_last: '821'
  pages: "812\u2013821"
  paper_id: '76'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1076.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1076.jpg
  title: 'Putting the Horse before the Cart: A Generator-Evaluator Framework for Question
    Generation from Text'
  title_html: 'Putting the Horse before the Cart: A Generator-Evaluator Framework
    for Question Generation from Text'
  url: https://www.aclweb.org/anthology/K19-1076
  year: '2019'
K19-1077:
  abstract: Various Seq2Seq learning models designed for machine translation were
    applied for abstractive summarization task recently. Despite these models provide
    high ROUGE scores, they are limited to generate comprehensive summaries with a
    high level of abstraction due to its degenerated attention distribution. We introduce
    Diverse Convolutional Seq2Seq Model(DivCNN Seq2Seq) using Determinantal Point
    Processes methods(Micro DPPs and Macro DPPs) to produce attention distribution
    considering both quality and diversity. Without breaking the end to end architecture,
    DivCNN Seq2Seq achieves a higher level of comprehensiveness compared to vanilla
    models and strong baselines. All the reproducible codes and datasets are available
    online.
  address: Hong Kong, China
  attachment:
  - filename: K19-1077.Supplementary_Material.zip
    type: supplementary-material
    url: https://www.aclweb.org/anthology/attachments/K19-1077.Supplementary_Material.zip
  author:
  - first: Lei
    full: Lei Li
    id: lei-li
    last: Li
  - first: Wei
    full: Wei Liu
    id: wei-liu
    last: Liu
  - first: Marina
    full: Marina Litvak
    id: marina-litvak
    last: Litvak
  - first: Natalia
    full: Natalia Vanetik
    id: natalia-vanetik
    last: Vanetik
  - first: Zuying
    full: Zuying Huang
    id: zuying-huang
    last: Huang
  author_string: Lei Li, Wei Liu, Marina Litvak, Natalia Vanetik, Zuying Huang
  bibkey: li-etal-2019-conclusion
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1077
  month: November
  page_first: '822'
  page_last: '832'
  pages: "822\u2013832"
  paper_id: '77'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1077.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1077.jpg
  title: 'In Conclusion Not Repetition: Comprehensive Abstractive Summarization with
    Diversified Attention Based on Determinantal Point Processes'
  title_html: 'In Conclusion Not Repetition: Comprehensive Abstractive Summarization
    with Diversified Attention Based on Determinantal Point Processes'
  url: https://www.aclweb.org/anthology/K19-1077
  year: '2019'
K19-1078:
  abstract: Abstractive text summarization aims at generating human-like summaries
    by understanding and paraphrasing the given input content. Recent efforts based
    on sequence-to-sequence networks only allow the generation of a single summary.
    However, it is often desirable to accommodate the psycho-linguistic preferences
    of the intended audience while generating the summaries. In this work, we present
    a reinforcement learning based approach to generate formality-tailored summaries
    for an input article. Our novel input-dependent reward function aids in training
    the model with stylistic feedback on sampled and ground-truth summaries together.
    Once trained, the same model can generate formal and informal summary variants.
    Our automated and qualitative evaluations show the viability of the proposed framework.
  address: Hong Kong, China
  attachment:
  - filename: K19-1078.Supplementary_Material.zip
    type: supplementary-material
    url: https://www.aclweb.org/anthology/attachments/K19-1078.Supplementary_Material.zip
  author:
  - first: Kushal
    full: Kushal Chawla
    id: kushal-chawla
    last: Chawla
  - first: Balaji Vasan
    full: Balaji Vasan Srinivasan
    id: balaji-vasan-srinivasan
    last: Srinivasan
  - first: Niyati
    full: Niyati Chhaya
    id: niyati-chhaya
    last: Chhaya
  author_string: Kushal Chawla, Balaji Vasan Srinivasan, Niyati Chhaya
  bibkey: chawla-etal-2019-generating
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1078
  month: November
  page_first: '833'
  page_last: '842'
  pages: "833\u2013842"
  paper_id: '78'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1078.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1078.jpg
  title: Generating Formality-Tuned Summaries Using Input-Dependent Rewards
  title_html: Generating Formality-Tuned Summaries Using Input-Dependent Rewards
  url: https://www.aclweb.org/anthology/K19-1078
  year: '2019'
K19-1079:
  abstract: Large neural language models trained on massive amounts of text have emerged
    as a formidable strategy for Natural Language Understanding tasks. However, the
    strength of these models as Natural Language Generators is less clear. Though
    anecdotal evidence suggests that these models generate better quality text, there
    has been no detailed study characterizing their generation abilities. In this
    work, we compare the performance of an extensively pretrained model, OpenAI GPT2-117
    (Radford et al., 2019), to a state-of-the-art neural story generation model (Fan
    et al., 2018). By evaluating the generated text across a wide variety of automatic
    metrics, we characterize the ways in which pretrained models do, and do not, make
    better storytellers. We find that although GPT2-117 conditions more strongly on
    context, is more sensitive to ordering of events, and uses more unusual words,
    it is just as likely to produce repetitive and under-diverse text when using likelihood-maximizing
    decoding algorithms.
  address: Hong Kong, China
  author:
  - first: Abigail
    full: Abigail See
    id: abigail-see
    last: See
  - first: Aneesh
    full: Aneesh Pappu
    id: aneesh-pappu
    last: Pappu
  - first: Rohun
    full: Rohun Saxena
    id: rohun-saxena
    last: Saxena
  - first: Akhila
    full: Akhila Yerukola
    id: akhila-yerukola
    last: Yerukola
  - first: Christopher D.
    full: Christopher D. Manning
    id: christopher-d-manning
    last: Manning
  author_string: Abigail See, Aneesh Pappu, Rohun Saxena, Akhila Yerukola, Christopher
    D. Manning
  bibkey: see-etal-2019-massively
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1079
  month: November
  page_first: '843'
  page_last: '861'
  pages: "843\u2013861"
  paper_id: '79'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1079.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1079.jpg
  title: Do Massively Pretrained Language Models Make Better Storytellers?
  title_html: Do Massively Pretrained Language Models Make Better Storytellers?
  url: https://www.aclweb.org/anthology/K19-1079
  year: '2019'
K19-1080:
  abstract: Residual has been widely applied to build deep neural networks with enhanced
    feature propagation and improved accuracy. In the literature, multiple variants
    of residual structure are proposed. However, most of them are manually designed
    for particular tasks and datasets and the combination of existing residual structures
    has not been well studied. In this work, we propose the Self-Adaptive Scaling
    (SAS) approach that automatically learns the design of residual structure from
    data. The proposed approach makes the best of various residual structures, resulting
    in a general architecture covering several existing ones. In this manner, we construct
    a learnable residual structure which can be easily integrated into a wide range
    of residual-based models. We evaluate our approach on various tasks concerning
    different modalities, including machine translation (IWSLT-2015 EN-VI and WMT-2014
    EN-DE, EN-FR), image classification (CIFAR-10 and CIFAR-100), and image captioning
    (MSCOCO). Empirical results show that the proposed approach consistently improves
    the residual-based models and exhibits desirable generalization ability. In particular,
    by incorporating the proposed approach to the Transformer model, we establish
    new state-of-the-arts on the IWSLT-2015 EN-VI low-resource machine translation
    dataset.
  address: Hong Kong, China
  author:
  - first: Fenglin
    full: Fenglin Liu
    id: fenglin-liu
    last: Liu
  - first: Meng
    full: Meng Gao
    id: meng-gao
    last: Gao
  - first: Yuanxin
    full: Yuanxin Liu
    id: yuanxin-liu
    last: Liu
  - first: Kai
    full: Kai Lei
    id: kai-lei
    last: Lei
  author_string: Fenglin Liu, Meng Gao, Yuanxin Liu, Kai Lei
  bibkey: liu-etal-2019-self
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1080
  month: November
  page_first: '862'
  page_last: '870'
  pages: "862\u2013870"
  paper_id: '80'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1080.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1080.jpg
  title: Self-Adaptive Scaling for Learnable Residual Structure
  title_html: Self-Adaptive Scaling for Learnable Residual Structure
  url: https://www.aclweb.org/anthology/K19-1080
  year: '2019'
K19-1081:
  abstract: The Specialized Information Service Biodiversity Research (BIOfid) has
    been launched to mobilize valuable biological data from printed literature hidden
    in German libraries for over the past 250 years. In this project, we annotate
    German texts converted by OCR from historical scientific literature on the biodiversity
    of plants, birds, moths and butterflies. Our work enables the automatic extraction
    of biological information previously buried in the mass of papers and volumes.
    For this purpose, we generated training data for the tasks of Named Entity Recognition
    (NER) and Taxa Recognition (TR) in biological documents. We use this data to train
    a number of leading machine learning tools and create a gold standard for TR in
    biodiversity literature. More specifically, we perform a practical analysis of
    our newly generated BIOfid dataset through various downstream-task evaluations
    and establish a new state of the art for TR with 80.23% F-score. In this sense,
    our paper lays the foundations for future work in the field of information extraction
    in biology texts.
  address: Hong Kong, China
  attachment:
  - filename: K19-1081.Supplementary_Material.zip
    type: supplementary-material
    url: https://www.aclweb.org/anthology/attachments/K19-1081.Supplementary_Material.zip
  author:
  - first: Sajawel
    full: Sajawel Ahmed
    id: sajawel-ahmed
    last: Ahmed
  - first: Manuel
    full: Manuel Stoeckel
    id: manuel-stoeckel
    last: Stoeckel
  - first: Christine
    full: Christine Driller
    id: christine-driller
    last: Driller
  - first: Adrian
    full: Adrian Pachzelt
    id: adrian-pachzelt
    last: Pachzelt
  - first: Alexander
    full: Alexander Mehler
    id: alexander-mehler
    last: Mehler
  author_string: Sajawel Ahmed, Manuel Stoeckel, Christine Driller, Adrian Pachzelt,
    Alexander Mehler
  bibkey: ahmed-etal-2019-biofid
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1081
  month: November
  page_first: '871'
  page_last: '880'
  pages: "871\u2013880"
  paper_id: '81'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1081.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1081.jpg
  title: 'BIOfid Dataset: Publishing a German Gold Standard for Named Entity Recognition
    in Historical Biodiversity Literature'
  title_html: '<span class="acl-fixed-case">BIO</span>fid Dataset: Publishing a <span
    class="acl-fixed-case">G</span>erman Gold Standard for Named Entity Recognition
    in Historical Biodiversity Literature'
  url: https://www.aclweb.org/anthology/K19-1081
  year: '2019'
K19-1082:
  abstract: The prevalence of informal language such as slang presents challenges
    for natural language systems, particularly in the automatic discovery of flexible
    word usages. Previous work has explored slang in terms of dictionary construction,
    sentiment analysis, word formation, and interpretation, but scarce research has
    attempted the basic problem of slang detection and identification. We examine
    the extent to which deep learning methods support automatic detection and identification
    of slang from natural sentences using a combination of bidirectional recurrent
    neural networks, conditional random field, and multilayer perceptron. We test
    these models based on a comprehensive set of linguistic features in sentence-level
    detection and token-level identification of slang. We found that a prominent feature
    of slang is the surprising use of words across syntactic categories or syntactic
    shift (e.g., verb-noun). Our best models detect the presence of slang at the sentence
    level with an F1-score of 0.80 and identify its exact position at the token level
    with an F1-Score of 0.50.
  address: Hong Kong, China
  author:
  - first: Zhengqi
    full: Zhengqi Pei
    id: zhengqi-pei
    last: Pei
  - first: Zhewei
    full: Zhewei Sun
    id: zhewei-sun
    last: Sun
  - first: Yang
    full: Yang Xu
    id: yang-xu
    last: Xu
  author_string: Zhengqi Pei, Zhewei Sun, Yang Xu
  bibkey: pei-etal-2019-slang
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1082
  month: November
  page_first: '881'
  page_last: '889'
  pages: "881\u2013889"
  paper_id: '82'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1082.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1082.jpg
  title: Slang Detection and Identification
  title_html: Slang Detection and Identification
  url: https://www.aclweb.org/anthology/K19-1082
  year: '2019'
K19-1083:
  abstract: "In sequence modeling tasks the token order matters, but this information\
    \ can be partially lost due to the discretization of the sequence into data points.\
    \ In this paper, we study the imbalance between the way certain token pairs are\
    \ included in data points and others are not. We denote this a token order imbalance\
    \ (TOI) and we link the partial sequence information loss to a diminished performance\
    \ of the system as a whole, both in text and speech processing tasks. We then\
    \ provide a mechanism to leverage the full token order information\u2014Alleviated\
    \ TOI\u2014by iteratively overlapping the token composition of data points. For\
    \ recurrent networks, we use prime numbers for the batch size to avoid redundancies\
    \ when building batches from overlapped data points. The proposed method achieved\
    \ state of the art performance in both text and speech related tasks."
  address: Hong Kong, China
  author:
  - first: "No\xE9mien"
    full: "No\xE9mien Kocher"
    id: noemien-kocher
    last: Kocher
  - first: Christian
    full: Christian Scuito
    id: christian-scuito
    last: Scuito
  - first: Lorenzo
    full: Lorenzo Tarantino
    id: lorenzo-tarantino
    last: Tarantino
  - first: Alexandros
    full: Alexandros Lazaridis
    id: alexandros-lazaridis
    last: Lazaridis
  - first: Andreas
    full: Andreas Fischer
    id: andreas-fischer
    last: Fischer
  - first: Claudiu
    full: Claudiu Musat
    id: claudiu-musat
    last: Musat
  author_string: "No\xE9mien Kocher, Christian Scuito, Lorenzo Tarantino, Alexandros\
    \ Lazaridis, Andreas Fischer, Claudiu Musat"
  bibkey: kocher-etal-2019-alleviating
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1083
  month: November
  page_first: '890'
  page_last: '899'
  pages: "890\u2013899"
  paper_id: '83'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1083.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1083.jpg
  title: Alleviating Sequence Information Loss with Data Overlapping and Prime Batch
    Sizes
  title_html: Alleviating Sequence Information Loss with Data Overlapping and Prime
    Batch Sizes
  url: https://www.aclweb.org/anthology/K19-1083
  year: '2019'
K19-1084:
  abstract: Standard autoregressive seq2seq models are easily trained by max-likelihood,
    but tend to show poor results under small-data conditions. We introduce a class
    of seq2seq models, GAMs (Global Autoregressive Models), which combine an autoregressive
    component with a log-linear component, allowing the use of global a priori features
    to compensate for lack of data. We train these models in two steps. In the first
    step, we obtain an unnormalized GAM that maximizes the likelihood of the data,
    but is improper for fast inference or evaluation. In the second step, we use this
    GAM to train (by distillation) a second autoregressive model that approximates
    the normalized distribution associated with the GAM, and can be used for fast
    inference and evaluation. Our experiments focus on language modelling under synthetic
    conditions and show a strong perplexity reduction of using the second autoregressive
    model over the standard one.
  address: Hong Kong, China
  attachment:
  - filename: K19-1084.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-1084.Attachment.pdf
  - filename: K19-1084.Supplementary_Material.pdf
    type: supplementary-material
    url: https://www.aclweb.org/anthology/attachments/K19-1084.Supplementary_Material.pdf
  author:
  - first: Tetiana
    full: Tetiana Parshakova
    id: tetiana-parshakova
    last: Parshakova
  - first: Jean-Marc
    full: Jean-Marc Andreoli
    id: jean-marc-andreoli
    last: Andreoli
  - first: Marc
    full: Marc Dymetman
    id: marc-dymetman
    last: Dymetman
  author_string: Tetiana Parshakova, Jean-Marc Andreoli, Marc Dymetman
  bibkey: parshakova-etal-2019-global
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1084
  month: November
  page_first: '900'
  page_last: '909'
  pages: "900\u2013909"
  paper_id: '84'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1084.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1084.jpg
  title: Global Autoregressive Models for Data-Efficient Sequence Learning
  title_html: Global Autoregressive Models for Data-Efficient Sequence Learning
  url: https://www.aclweb.org/anthology/K19-1084
  year: '2019'
K19-1085:
  abstract: Answer selection aims at identifying the correct answer for a given question
    from a set of potentially correct answers. Contrary to previous works, which typically
    focus on the semantic similarity between a question and its answer, our hypothesis
    is that question-answer pairs are often in analogical relation to each other.
    Using analogical inference as our use case, we propose a framework and a neural
    network architecture for learning dedicated sentence embeddings that preserve
    analogical properties in the semantic space. We evaluate the proposed method on
    benchmark datasets for answer selection and demonstrate that our sentence embeddings
    indeed capture analogical properties better than conventional embeddings, and
    that analogy-based question answering outperforms a comparable similarity-based
    technique.
  address: Hong Kong, China
  author:
  - first: "A\xEFssatou"
    full: "A\xEFssatou Diallo"
    id: aissatou-diallo
    last: Diallo
  - first: Markus
    full: Markus Zopf
    id: markus-zopf
    last: Zopf
  - first: Johannes
    full: "Johannes F\xFCrnkranz"
    id: johannes-furnkranz
    last: "F\xFCrnkranz"
  author_string: "A\xEFssatou Diallo, Markus Zopf, Johannes F\xFCrnkranz"
  bibkey: diallo-etal-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1085
  month: November
  page_first: '910'
  page_last: '919'
  pages: "910\u2013919"
  paper_id: '85'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1085.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1085.jpg
  title: Learning Analogy-Preserving Sentence Embeddings for Answer Selection
  title_html: Learning Analogy-Preserving Sentence Embeddings for Answer Selection
  url: https://www.aclweb.org/anthology/K19-1085
  year: '2019'
K19-1086:
  abstract: We propose a simple and effective method to inject word-level information
    into character-aware neural language models. Unlike previous approaches which
    usually inject word-level information at the input of a long short-term memory
    (LSTM) network, we inject it into the softmax function. The resultant model can
    be seen as a combination of character-aware language model and simple word-level
    language model. Our injection method can also be used together with previous methods.
    Through the experiments on 14 typologically diverse languages, we empirically
    show that our injection method, when used together with the previous methods,
    works better than the previous methods, including a gating mechanism, averaging,
    and concatenation of word vectors. We also provide a comprehensive comparison
    of these injection methods.
  address: Hong Kong, China
  author:
  - first: Yukun
    full: Yukun Feng
    id: yukun-feng
    last: Feng
  - first: Hidetaka
    full: Hidetaka Kamigaito
    id: hidetaka-kamigaito
    last: Kamigaito
  - first: Hiroya
    full: Hiroya Takamura
    id: hiroya-takamura
    last: Takamura
  - first: Manabu
    full: Manabu Okumura
    id: manabu-okumura
    last: Okumura
  author_string: Yukun Feng, Hidetaka Kamigaito, Hiroya Takamura, Manabu Okumura
  bibkey: feng-etal-2019-simple
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1086
  month: November
  page_first: '920'
  page_last: '928'
  pages: "920\u2013928"
  paper_id: '86'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1086.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1086.jpg
  title: A Simple and Effective Method for Injecting Word-Level Information into Character-Aware
    Neural Language Models
  title_html: A Simple and Effective Method for Injecting Word-Level Information into
    Character-Aware Neural Language Models
  url: https://www.aclweb.org/anthology/K19-1086
  year: '2019'
K19-1087:
  abstract: "In this paper, we focus on quantifying model stability as a function\
    \ of random seed by investigating the effects of the induced randomness on model\
    \ performance and the robustness of the model in general. We specifically perform\
    \ a controlled study on the effect of random seeds on the behaviour of attention,\
    \ gradient-based and surrogate model based (LIME) interpretations. Our analysis\
    \ suggests that random seeds can adversely affect the consistency of models resulting\
    \ in counterfactual interpretations. We propose a technique called Aggressive\
    \ Stochastic Weight Averaging (ASWA) and an extension called Norm-filtered Aggressive\
    \ Stochastic Weight Averaging (NASWA) which improves the stability of models over\
    \ random seeds. With our ASWA and NASWA based optimization, we are able to improve\
    \ the robustness of the original model, on average reducing the standard deviation\
    \ of the model\u2019s performance by 72%."
  address: Hong Kong, China
  attachment:
  - filename: K19-1087.Supplementary_Material.zip
    type: supplementary-material
    url: https://www.aclweb.org/anthology/attachments/K19-1087.Supplementary_Material.zip
  - filename: K19-1087.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-1087.Attachment.zip
  author:
  - first: Pranava
    full: Pranava Madhyastha
    id: pranava-swaroop-madhyastha
    last: Madhyastha
  - first: Rishabh
    full: Rishabh Jain
    id: rishabh-jain
    last: Jain
  author_string: Pranava Madhyastha, Rishabh Jain
  bibkey: madhyastha-jain-2019-model
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1087
  month: November
  page_first: '929'
  page_last: '939'
  pages: "929\u2013939"
  paper_id: '87'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1087.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1087.jpg
  title: On Model Stability as a Function of Random Seed
  title_html: On Model Stability as a Function of Random Seed
  url: https://www.aclweb.org/anthology/K19-1087
  year: '2019'
K19-1088:
  abstract: Work on Abusive Language Detection has tackled a wide range of subtasks
    and domains. As a result of this, there exists a great deal of redundancy and
    non-generalisability between datasets. Through experiments on cross-dataset training
    and testing, the paper reveals that the preconceived notion of including more
    non-abusive samples in a dataset (to emulate reality) may have a detrimental effect
    on the generalisability of a model trained on that data. Hence a hierarchical
    annotation model is utilised here to reveal redundancies in existing datasets
    and to help reduce redundancy in future efforts.
  address: Hong Kong, China
  author:
  - first: Steve Durairaj
    full: Steve Durairaj Swamy
    id: steve-durairaj-swamy
    last: Swamy
  - first: Anupam
    full: Anupam Jamatia
    id: anupam-jamatia
    last: Jamatia
  - first: "Bj\xF6rn"
    full: "Bj\xF6rn Gamb\xE4ck"
    id: bjorn-gamback
    last: "Gamb\xE4ck"
  author_string: "Steve Durairaj Swamy, Anupam Jamatia, Bj\xF6rn Gamb\xE4ck"
  bibkey: swamy-etal-2019-studying
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1088
  month: November
  page_first: '940'
  page_last: '950'
  pages: "940\u2013950"
  paper_id: '88'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1088.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1088.jpg
  title: Studying Generalisability across Abusive Language Detection Datasets
  title_html: Studying Generalisability across Abusive Language Detection Datasets
  url: https://www.aclweb.org/anthology/K19-1088
  year: '2019'
K19-1089:
  abstract: Authorship attribution is an active research area which has been prevalent
    for many decades. Nevertheless, the majority of approaches consider problem sizes
    of a few candidate authors only, making them difficult to apply to recent scenarios
    incorporating thousands of authors emerging due to the manifold means to digitally
    share text. In this study, we focus on such large-scale problems and propose to
    effectively reduce the number of candidate authors before applying common attribution
    techniques. By utilizing document embeddings, we show on a novel, comprehensive
    dataset collection that the set of candidate authors can be reduced with high
    accuracy. Moreover, we show that common authorship attribution methods substantially
    benefit from a preliminary reduction if thousands of authors are involved.
  address: Hong Kong, China
  author:
  - first: Michael
    full: Michael Tschuggnall
    id: michael-tschuggnall
    last: Tschuggnall
  - first: Benjamin
    full: Benjamin Murauer
    id: benjamin-murauer
    last: Murauer
  - first: "G\xFCnther"
    full: "G\xFCnther Specht"
    id: gunther-specht
    last: Specht
  author_string: "Michael Tschuggnall, Benjamin Murauer, G\xFCnther Specht"
  bibkey: tschuggnall-etal-2019-reduce
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1089
  month: November
  page_first: '951'
  page_last: '960'
  pages: "951\u2013960"
  paper_id: '89'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1089.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1089.jpg
  title: 'Reduce & Attribute: Two-Step Authorship Attribution for Large-Scale Problems'
  title_html: 'Reduce &amp; Attribute: Two-Step Authorship Attribution for Large-Scale
    Problems'
  url: https://www.aclweb.org/anthology/K19-1089
  year: '2019'
K19-1090:
  abstract: Aspect-term sentiment analysis (ATSA) is a long-standing challenge in
    natural language process. It requires fine-grained semantical reasoning about
    a target entity appeared in the text. As manual annotation over the aspects is
    laborious and time-consuming, the amount of labeled data is limited for supervised
    learning. This paper proposes a semi-supervised method for the ATSA problem by
    using the Variational Autoencoder based on Transformer. The model learns the latent
    distribution via variational inference. By disentangling the latent representation
    into the aspect-specific sentiment and the lexical context, our method induces
    the underlying sentiment prediction for the unlabeled data, which then benefits
    the ATSA classifier. Our method is classifier-agnostic, i.e., the classifier is
    an independent module and various supervised models can be integrated. Experimental
    results are obtained on the SemEval 2014 task 4 and show that our method is effective
    with different the five specific classifiers and outperforms these models by a
    significant margin.
  address: Hong Kong, China
  author:
  - first: Xingyi
    full: Xingyi Cheng
    id: xingyi-cheng
    last: Cheng
  - first: Weidi
    full: Weidi Xu
    id: weidi-xu
    last: Xu
  - first: Taifeng
    full: Taifeng Wang
    id: taifeng-wang
    last: Wang
  - first: Wei
    full: Wei Chu
    id: wei-chu
    last: Chu
  - first: Weipeng
    full: Weipeng Huang
    id: weipeng-huang
    last: Huang
  - first: Kunlong
    full: Kunlong Chen
    id: kunlong-chen
    last: Chen
  - first: Junfeng
    full: Junfeng Hu
    id: junfeng-hu
    last: Hu
  author_string: Xingyi Cheng, Weidi Xu, Taifeng Wang, Wei Chu, Weipeng Huang, Kunlong
    Chen, Junfeng Hu
  bibkey: cheng-etal-2019-variational
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1090
  month: November
  page_first: '961'
  page_last: '969'
  pages: "961\u2013969"
  paper_id: '90'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1090.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1090.jpg
  title: Variational Semi-Supervised Aspect-Term Sentiment Analysis via Transformer
  title_html: Variational Semi-Supervised Aspect-Term Sentiment Analysis via Transformer
  url: https://www.aclweb.org/anthology/K19-1090
  year: '2019'
K19-1091:
  abstract: Aspect-based sentiment analysis (ABSA) is to predict the sentiment polarity
    towards a particular aspect in a sentence. Recently, this task has been widely
    addressed by the neural attention mechanism, which computes attention weights
    to softly select words for generating aspect-specific sentence representations.
    The attention is expected to concentrate on opinion words for accurate sentiment
    prediction. However, attention is prone to be distracted by noisy or misleading
    words, or opinion words from other aspects. In this paper, we propose an alternative
    hard-selection approach, which determines the start and end positions of the opinion
    snippet, and selects the words between these two positions for sentiment prediction.
    Specifically, we learn deep associations between the sentence and aspect, and
    the long-term dependencies within the sentence by leveraging the pre-trained BERT
    model. We further detect the opinion snippet by self-critical reinforcement learning.
    Especially, experimental results demonstrate the effectiveness of our method and
    prove that our hard-selection approach outperforms soft-selection approaches when
    handling multi-aspect sentences.
  address: Hong Kong, China
  author:
  - first: Mengting
    full: Mengting Hu
    id: mengting-hu
    last: Hu
  - first: Shiwan
    full: Shiwan Zhao
    id: shiwan-zhao
    last: Zhao
  - first: Honglei
    full: Honglei Guo
    id: honglei-guo
    last: Guo
  - first: Renhong
    full: Renhong Cheng
    id: renhong-cheng
    last: Cheng
  - first: Zhong
    full: Zhong Su
    id: zhong-su
    last: Su
  author_string: Mengting Hu, Shiwan Zhao, Honglei Guo, Renhong Cheng, Zhong Su
  bibkey: hu-etal-2019-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1091
  month: November
  page_first: '970'
  page_last: '979'
  pages: "970\u2013979"
  paper_id: '91'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1091.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1091.jpg
  title: Learning to Detect Opinion Snippet for Aspect-Based Sentiment Analysis
  title_html: Learning to Detect Opinion Snippet for Aspect-Based Sentiment Analysis
  url: https://www.aclweb.org/anthology/K19-1091
  year: '2019'
K19-1092:
  abstract: "In this article we present an extended version of PolEmo \u2013 a corpus\
    \ of consumer reviews from 4 domains: medicine, hotels, products and school. Current\
    \ version (PolEmo 2.0) contains 8,216 reviews having 57,466 sentences. Each text\
    \ and sentence was manually annotated with sentiment in 2+1 scheme, which gives\
    \ a total of 197,046 annotations. We obtained a high value of Positive Specific\
    \ Agreement, which is 0.91 for texts and 0.88 for sentences. PolEmo 2.0 is publicly\
    \ available under a Creative Commons copyright license. We explored recent deep\
    \ learning approaches for the recognition of sentiment, such as Bi-directional\
    \ Long Short-Term Memory (BiLSTM) and Bidirectional Encoder Representations from\
    \ Transformers (BERT)."
  address: Hong Kong, China
  attachment:
  - filename: K19-1092.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-1092.Attachment.zip
  author:
  - first: Jan
    full: "Jan Koco\u0144"
    id: jan-kocon
    last: "Koco\u0144"
  - first: Piotr
    full: "Piotr Mi\u0142kowski"
    id: piotr-milkowski
    last: "Mi\u0142kowski"
  - first: Monika
    full: "Monika Za\u015Bko-Zieli\u0144ska"
    id: monika-zasko-zielinska
    last: "Za\u015Bko-Zieli\u0144ska"
  author_string: "Jan Koco\u0144, Piotr Mi\u0142kowski, Monika Za\u015Bko-Zieli\u0144\
    ska"
  bibkey: kocon-etal-2019-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1092
  month: November
  page_first: '980'
  page_last: '991'
  pages: "980\u2013991"
  paper_id: '92'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1092.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1092.jpg
  title: 'Multi-Level Sentiment Analysis of PolEmo 2.0: Extended Corpus of Multi-Domain
    Consumer Reviews'
  title_html: 'Multi-Level Sentiment Analysis of <span class="acl-fixed-case">P</span>ol<span
    class="acl-fixed-case">E</span>mo 2.0: Extended Corpus of Multi-Domain Consumer
    Reviews'
  url: https://www.aclweb.org/anthology/K19-1092
  year: '2019'
K19-1093:
  abstract: "In this paper, we look beyond the traditional population-level sentiment\
    \ modeling and consider the individuality in a person\u2019s expressions by discovering\
    \ both textual and contextual information. In particular, we construct a hierarchical\
    \ neural network that leverages valuable information from a person\u2019s past\
    \ expressions, and offer a better understanding of the sentiment from the expresser\u2019\
    s perspective. Additionally, we investigate how a person\u2019s sentiment changes\
    \ over time so that recent incidents or opinions may have more effect on the person\u2019\
    s current sentiment than the old ones. Psychological studies have also shown that\
    \ individual variation exists in how easily people change their sentiments. In\
    \ order to model such traits, we develop a modified attention mechanism with Hawkes\
    \ process applied on top of a recurrent network for a user-specific design. Implemented\
    \ with automatically labeled Twitter data, the proposed model has shown positive\
    \ results employing different input formulations for representing the concerned\
    \ information."
  address: Hong Kong, China
  attachment:
  - filename: K19-1093.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-1093.Attachment.zip
  - filename: K19-1093.Supplementary_Material.zip
    type: supplementary-material
    url: https://www.aclweb.org/anthology/attachments/K19-1093.Supplementary_Material.zip
  author:
  - first: Siwen
    full: Siwen Guo
    id: siwen-guo
    last: Guo
  - first: Sviatlana
    full: "Sviatlana H\xF6hn"
    id: sviatlana-hohn
    last: "H\xF6hn"
  - first: Christoph
    full: Christoph Schommer
    id: christoph-schommer
    last: Schommer
  author_string: "Siwen Guo, Sviatlana H\xF6hn, Christoph Schommer"
  bibkey: guo-etal-2019-personalized
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1093
  month: November
  page_first: '992'
  page_last: '1001'
  pages: "992\u20131001"
  paper_id: '93'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1093.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1093.jpg
  title: A Personalized Sentiment Model with Textual and Contextual Information
  title_html: A Personalized Sentiment Model with Textual and Contextual Information
  url: https://www.aclweb.org/anthology/K19-1093
  year: '2019'
K19-1094:
  abstract: Text classification plays a crucial role for understanding natural language
    in a wide range of applications. Most existing approaches mainly focus on long
    text classification (e.g., blogs, documents, paragraphs). However, they cannot
    easily be applied to short text because of its sparsity and lack of context. In
    this paper, we propose a new model called cluster-gated convolutional neural network
    (CGCNN), which jointly explores word-level clustering and text classification
    in an end-to-end manner. Specifically, the proposed model firstly uses a bi-directional
    long short-term memory to learn word representations. Then, it leverages a soft
    clustering method to explore their semantic relation with the cluster centers,
    and takes linear transformation on text representations. It develops a cluster-dependent
    gated convolutional layer to further control the cluster-dependent feature flows.
    Experimental results on five commonly used datasets show that our model outperforms
    state-of-the-art models.
  address: Hong Kong, China
  author:
  - first: Haidong
    full: Haidong Zhang
    id: haidong-zhang
    last: Zhang
  - first: Wancheng
    full: Wancheng Ni
    id: wancheng-ni
    last: Ni
  - first: Meijing
    full: Meijing Zhao
    id: meijing-zhao
    last: Zhao
  - first: Ziqi
    full: Ziqi Lin
    id: ziqi-lin
    last: Lin
  author_string: Haidong Zhang, Wancheng Ni, Meijing Zhao, Ziqi Lin
  bibkey: zhang-etal-2019-cluster
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1094
  month: November
  page_first: '1002'
  page_last: '1011'
  pages: "1002\u20131011"
  paper_id: '94'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1094.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1094.jpg
  title: Cluster-Gated Convolutional Neural Network for Short Text Classification
  title_html: Cluster-Gated Convolutional Neural Network for Short Text Classification
  url: https://www.aclweb.org/anthology/K19-1094
  year: '2019'
K19-1095:
  abstract: In hospitals, critical care patients are often susceptible to various
    complications that adversely affect their morbidity and mortality. Digitized patient
    data from Electronic Health Records (EHRs) can be utilized to facilitate risk
    stratification accurately and provide prioritized care. Existing clinical decision
    support systems are heavily reliant on the structured nature of the EHRs. However,
    the valuable patient-specific data contained in unstructured clinical notes are
    often manually transcribed into EHRs. The prolific use of extensive medical jargon,
    heterogeneity, sparsity, rawness, inconsistent abbreviations, and complex structure
    of the clinical notes poses significant challenges, and also results in a loss
    of information during the manual conversion process. In this work, we employ two
    coherence-based topic modeling approaches to model the free-text in the unstructured
    clinical nursing notes and capture its semantic textual features with the emphasis
    on human interpretability. Furthermore, we present FarSight, a long-term aggregation
    mechanism intended to detect the onset of disease with the earliest recorded symptoms
    and infections. We utilize the predictive capabilities of deep neural models for
    the clinical task of risk stratification through ICD-9 code group prediction.
    Our experimental validation on MIMIC-III (v1.4) database underlined the efficacy
    of FarSight with coherence-based topic modeling, in extracting discriminative
    clinical features from the unstructured nursing notes. The proposed approach achieved
    a superior predictive performance when benchmarked against the structured EHR
    data based state-of-the-art model, with an improvement of 11.50% in AUPRC and
    1.16% in AUROC.
  address: Hong Kong, China
  author:
  - first: Tushaar
    full: Tushaar Gangavarapu
    id: tushaar-gangavarapu
    last: Gangavarapu
  - first: Gokul S
    full: Gokul S Krishnan
    id: gokul-s-krishnan
    last: Krishnan
  - first: Sowmya
    full: Sowmya Kamath S
    id: sowmya-kamath-s
    last: Kamath S
  author_string: Tushaar Gangavarapu, Gokul S Krishnan, Sowmya Kamath S
  bibkey: gangavarapu-etal-2019-coherence
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1095
  month: November
  page_first: '1012'
  page_last: '1022'
  pages: "1012\u20131022"
  paper_id: '95'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1095.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1095.jpg
  title: Coherence-based Modeling of Clinical Concepts Inferred from Heterogeneous
    Clinical Notes for ICU Patient Risk Stratification
  title_html: Coherence-based Modeling of Clinical Concepts Inferred from Heterogeneous
    Clinical Notes for <span class="acl-fixed-case">ICU</span> Patient Risk Stratification
  url: https://www.aclweb.org/anthology/K19-1095
  year: '2019'
K19-1096:
  abstract: "We investigate the political roles of \u201CInternet trolls\u201D in\
    \ social media. Political trolls, such as the ones linked to the Russian Internet\
    \ Research Agency (IRA), have recently gained enormous attention for their ability\
    \ to sway public opinion and even influence elections. Analysis of the online\
    \ traces of trolls has shown different behavioral patterns, which target different\
    \ slices of the population. However, this analysis is manual and labor-intensive,\
    \ thus making it impractical as a first-response tool for newly-discovered troll\
    \ farms. In this paper, we show how to automate this analysis by using machine\
    \ learning in a realistic setting. In particular, we show how to classify trolls\
    \ according to their political role \u2014left, news feed, right\u2014 by using\
    \ features extracted from social media, i.e., Twitter, in two scenarios: (i) in\
    \ a traditional supervised learning scenario, where labels for trolls are available,\
    \ and (ii) in a distant supervision scenario, where labels for trolls are not\
    \ available, and we rely on more-commonly-available labels for news outlets mentioned\
    \ by the trolls. Technically, we leverage the community structure and the text\
    \ of the messages in the online social network of trolls represented as a graph,\
    \ from which we extract several types of learned representations, i.e., embeddings,\
    \ for the trolls. Experiments on the \u201CIRA Russian Troll\u201D dataset show\
    \ that our methodology improves over the state-of-the-art in the first scenario,\
    \ while providing a compelling case for the second scenario, which has not been\
    \ explored in the literature thus far."
  address: Hong Kong, China
  attachment:
  - filename: K19-1096.Attachment.txt
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-1096.Attachment.txt
  author:
  - first: Atanas
    full: Atanas Atanasov
    id: atanas-atanasov
    last: Atanasov
  - first: Gianmarco
    full: Gianmarco De Francisci Morales
    id: gianmarco-de-francisci-morales
    last: De Francisci Morales
  - first: Preslav
    full: Preslav Nakov
    id: preslav-nakov
    last: Nakov
  author_string: Atanas Atanasov, Gianmarco De Francisci Morales, Preslav Nakov
  bibkey: atanasov-etal-2019-predicting
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1096
  month: November
  page_first: '1023'
  page_last: '1034'
  pages: "1023\u20131034"
  paper_id: '96'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1096.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1096.jpg
  title: Predicting the Role of Political Trolls in Social Media
  title_html: Predicting the Role of Political Trolls in Social Media
  url: https://www.aclweb.org/anthology/K19-1096
  year: '2019'
K19-1097:
  abstract: 'Sentiment analysis in low-resource languages suffers from the lack of
    training data. Cross-lingual sentiment analysis (CLSA) aims to improve the performance
    on these languages by leveraging annotated data from other languages. Recent studies
    have shown that CLSA can be performed in a fully unsupervised manner, without
    exploiting either target language supervision or cross-lingual supervision. However,
    these methods rely heavily on unsupervised cross-lingual word embeddings (CLWE),
    which has been shown to have serious drawbacks on distant language pairs (e.g.
    English - Japanese). In this paper, we propose an end-to-end CLSA model by leveraging
    unlabeled data in multiple languages and multiple domains and eliminate the need
    for unsupervised CLWE. Our model applies to two CLSA settings: the traditional
    cross-lingual in-domain setting and the more challenging cross-lingual cross-domain
    setting. We empirically evaluate our approach on the multilingual multi-domain
    Amazon review dataset. Experimental results show that our model outperforms the
    baselines by a large margin despite its minimal resource requirement.'
  address: Hong Kong, China
  author:
  - first: Yanlin
    full: Yanlin Feng
    id: yanlin-feng
    last: Feng
  - first: Xiaojun
    full: Xiaojun Wan
    id: xiaojun-wan
    last: Wan
  author_string: Yanlin Feng, Xiaojun Wan
  bibkey: feng-wan-2019-towards
  bibtype: inproceedings
  booktitle: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  booktitle_html: Proceedings of the 23rd Conference on Computational Natural Language
    Learning (CoNLL)
  doi: 10.18653/v1/K19-1097
  month: November
  page_first: '1035'
  page_last: '1044'
  pages: "1035\u20131044"
  paper_id: '97'
  parent_volume_id: K19-1
  pdf: https://www.aclweb.org/anthology/K19-1097.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-1097.jpg
  title: Towards a Unified End-to-End Approach for Fully Unsupervised Cross-Lingual
    Sentiment Analysis
  title_html: Towards a Unified End-to-End Approach for Fully Unsupervised Cross-Lingual
    Sentiment Analysis
  url: https://www.aclweb.org/anthology/K19-1097
  year: '2019'
K19-2000:
  address: Hong Kong
  author:
  - first: Stephan
    full: Stephan Oepen
    id: stephan-oepen
    last: Oepen
  - first: Omri
    full: Omri Abend
    id: omri-abend
    last: Abend
  - first: Jan
    full: Jan Hajic
    id: jan-hajic
    last: Hajic
  - first: Daniel
    full: Daniel Hershcovich
    id: daniel-hershcovich
    last: Hershcovich
  - first: Marco
    full: Marco Kuhlmann
    id: marco-kuhlmann
    last: Kuhlmann
  - first: Tim
    full: "Tim O\u2019Gorman"
    id: tim-ogorman
    last: "O\u2019Gorman"
  - first: Nianwen
    full: Nianwen Xue
    id: nianwen-xue
    last: Xue
  author_string: "Stephan Oepen, Omri Abend, Jan Hajic, Daniel Hershcovich, Marco\
    \ Kuhlmann, Tim O\u2019Gorman, Nianwen Xue"
  bibkey: conll-2019-shared
  bibtype: proceedings
  booktitle: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  month: November
  paper_id: '0'
  parent_volume_id: K19-2
  pdf: https://www.aclweb.org/anthology/K19-2000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-2000.jpg
  title: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  title_html: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  url: https://www.aclweb.org/anthology/K19-2000
  year: '2019'
K19-2001:
  abstract: 'The 2019 Shared Task at the Conference for Computational Language Learning
    (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks.
    Five distinct approaches to the representation of sentence meaning in the form
    of directed graph were represented in the training and evaluation data for the
    task, packaged in a uniform abstract graph representation and serialization. The
    task received submissions from eighteen teams, of which five do not participate
    in the official ranking because they arrived after the closing deadline, made
    use of additional training data, or involved one of the task co-organizers. All
    technical information regarding the task, including system submissions, official
    results, and links to supporting resources and software are available from the
    task web site at: http://mrp.nlpl.eu'
  address: Hong Kong
  attachment:
  - filename: K19-2001.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-2001.Attachment.pdf
  author:
  - first: Stephan
    full: Stephan Oepen
    id: stephan-oepen
    last: Oepen
  - first: Omri
    full: Omri Abend
    id: omri-abend
    last: Abend
  - first: Jan
    full: Jan Hajic
    id: jan-hajic
    last: Hajic
  - first: Daniel
    full: Daniel Hershcovich
    id: daniel-hershcovich
    last: Hershcovich
  - first: Marco
    full: Marco Kuhlmann
    id: marco-kuhlmann
    last: Kuhlmann
  - first: Tim
    full: "Tim O\u2019Gorman"
    id: tim-ogorman
    last: "O\u2019Gorman"
  - first: Nianwen
    full: Nianwen Xue
    id: nianwen-xue
    last: Xue
  - first: Jayeol
    full: Jayeol Chun
    id: jayeol-chun
    last: Chun
  - first: Milan
    full: Milan Straka
    id: milan-straka
    last: Straka
  - first: Zdenka
    full: Zdenka Uresova
    id: zdenka-uresova
    last: Uresova
  author_string: "Stephan Oepen, Omri Abend, Jan Hajic, Daniel Hershcovich, Marco\
    \ Kuhlmann, Tim O\u2019Gorman, Nianwen Xue, Jayeol Chun, Milan Straka, Zdenka\
    \ Uresova"
  bibkey: oepen-etal-2019-mrp
  bibtype: inproceedings
  booktitle: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  booktitle_html: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  doi: 10.18653/v1/K19-2001
  month: November
  page_first: '1'
  page_last: '27'
  pages: "1\u201327"
  paper_id: '1'
  parent_volume_id: K19-2
  pdf: https://www.aclweb.org/anthology/K19-2001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-2001.jpg
  title: 'MRP 2019: Cross-Framework Meaning Representation Parsing'
  title_html: '<span class="acl-fixed-case">MRP</span> 2019: Cross-Framework Meaning
    Representation Parsing'
  url: https://www.aclweb.org/anthology/K19-2001
  year: '2019'
K19-2002:
  abstract: This paper describes the TUPA system submission to the shared task on
    Cross-Framework Meaning Representation Parsing (MRP) at the 2019 Conference for
    Computational Language Learning (CoNLL). Because it was prepared by one of the
    task co-organizers, TUPA provides a baseline point of comparison and is not considered
    in the official ranking of participating systems. While originally developed for
    UCCA only, TUPA has been generalized to support all MRP frameworks included in
    the task, and trained using multi-task learning to parse them all with a shared
    model. It is a transition-based parser with a BiLSTM encoder, augmented with BERT
    contextualized embeddings.
  address: Hong Kong
  attachment:
  - filename: K19-2002.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-2002.Attachment.zip
  author:
  - first: Daniel
    full: Daniel Hershcovich
    id: daniel-hershcovich
    last: Hershcovich
  - first: Ofir
    full: Ofir Arviv
    id: ofir-arviv
    last: Arviv
  author_string: Daniel Hershcovich, Ofir Arviv
  bibkey: hershcovich-arviv-2019-tupa
  bibtype: inproceedings
  booktitle: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  booktitle_html: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  doi: 10.18653/v1/K19-2002
  month: November
  page_first: '28'
  page_last: '39'
  pages: "28\u201339"
  paper_id: '2'
  parent_volume_id: K19-2
  pdf: https://www.aclweb.org/anthology/K19-2002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-2002.jpg
  title: 'TUPA at MRP 2019: A Multi-Task Baseline System'
  title_html: '<span class="acl-fixed-case">TUPA</span> at <span class="acl-fixed-case">MRP</span>
    2019: A Multi-Task Baseline System'
  url: https://www.aclweb.org/anthology/K19-2002
  year: '2019'
K19-2003:
  abstract: The English Resource Grammar (ERG) is a broad-coverage computational grammar
    of English that outputs underspecified logical-form representations of meaning
    in a framework dubbed English Resource Semantics (ERS). Two of the target representations
    in the the 2019 Shared Task on Cross-Framework Meaning Representation Parsing
    (MRP 2019) derive graph-based simplifications of ERS, viz. Elementary Dependency
    Structures (EDS) and DELPH-IN MRS Bi-Lexical Dependencies (DM). As a point of
    reference outside the official MRP competition, we parsed the evaluation strings
    using the ERG and converted the resulting meaning representations to EDS and DM.
    These graphs yield higher evaluation scores than the purely data-driven parsers
    in the actual shared task, suggesting that the general-purpose linguistic knowledge
    about English grammar encoded in the ERG can add value when parsing into these
    meaning representations.
  address: Hong Kong
  author:
  - first: Stephan
    full: Stephan Oepen
    id: stephan-oepen
    last: Oepen
  - first: Dan
    full: Dan Flickinger
    id: dan-flickinger
    last: Flickinger
  author_string: Stephan Oepen, Dan Flickinger
  bibkey: oepen-flickinger-2019-erg
  bibtype: inproceedings
  booktitle: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  booktitle_html: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  doi: 10.18653/v1/K19-2003
  month: November
  page_first: '40'
  page_last: '44'
  pages: "40\u201344"
  paper_id: '3'
  parent_volume_id: K19-2
  pdf: https://www.aclweb.org/anthology/K19-2003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-2003.jpg
  title: 'The ERG at MRP 2019: Radically Compositional Semantic Dependencies'
  title_html: 'The <span class="acl-fixed-case">ERG</span> at <span class="acl-fixed-case">MRP</span>
    2019: Radically Compositional Semantic Dependencies'
  url: https://www.aclweb.org/anthology/K19-2003
  year: '2019'
K19-2004:
  abstract: "This paper describes our SJTU-NICT\u2019s system for participating in\
    \ the shared task on Cross-Framework Meaning Representation Parsing (MRP) at the\
    \ 2019 Conference for Computational Language Learning (CoNLL). Our system uses\
    \ a graph-based approach to model a variety of semantic graph parsing tasks. Our\
    \ main contributions in the submitted system are summarized as follows: 1. Our\
    \ model is fully end-to-end and is capable of being trained only on the given\
    \ training set which does not rely on any other extra training source including\
    \ the companion data provided by the organizer; 2. We extend our graph pruning\
    \ algorithm to a variety of semantic graphs, solving the problem of excessive\
    \ semantic graph search space; 3. We introduce multi-task learning for multiple\
    \ objectives within the same framework. The evaluation results show that our system\
    \ achieved second place in the overall F1 score and achieved the best score and\
    \ achieved the best F1 score on the DM framework. score on the DM framework."
  address: Hong Kong
  author:
  - first: Zuchao
    full: Zuchao Li
    id: zuchao-li
    last: Li
  - first: Hai
    full: Hai Zhao
    id: hai-zhao
    last: Zhao
  - first: Zhuosheng
    full: Zhuosheng Zhang
    id: zhuosheng-zhang
    last: Zhang
  - first: Rui
    full: Rui Wang
    id: rui-wang
    last: Wang
  - first: Masao
    full: Masao Utiyama
    id: masao-utiyama
    last: Utiyama
  - first: Eiichiro
    full: Eiichiro Sumita
    id: eiichiro-sumita
    last: Sumita
  author_string: Zuchao Li, Hai Zhao, Zhuosheng Zhang, Rui Wang, Masao Utiyama, Eiichiro
    Sumita
  bibkey: li-etal-2019-sjtu
  bibtype: inproceedings
  booktitle: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  booktitle_html: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  doi: 10.18653/v1/K19-2004
  month: November
  page_first: '45'
  page_last: '54'
  pages: "45\u201354"
  paper_id: '4'
  parent_volume_id: K19-2
  pdf: https://www.aclweb.org/anthology/K19-2004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-2004.jpg
  title: 'SJTU-NICT at MRP 2019: Multi-Task Learning for End-to-End Uniform Semantic
    Graph Parsing'
  title_html: '<span class="acl-fixed-case">SJTU</span>-<span class="acl-fixed-case">NICT</span>
    at <span class="acl-fixed-case">MRP</span> 2019: Multi-Task Learning for End-to-End
    Uniform Semantic Graph Parsing'
  url: https://www.aclweb.org/anthology/K19-2004
  year: '2019'
K19-2005:
  abstract: 'This paper presents the system used in our submission to the CoNLL 2019
    shared task: Cross-Framework Meaning Representation Parsing. Our system is a graph-based
    parser which combines an extended pointer-generator network that generates nodes
    and a second-order mean field variational inference module that predicts edges.
    Our system achieved 1st and 2nd place for the DM and PSD frameworks respectively
    on the in-framework ranks and achieved 3rd place for the DM framework on the cross-framework
    ranks.'
  address: Hong Kong
  author:
  - first: Xinyu
    full: Xinyu Wang
    id: xinyu-wang
    last: Wang
  - first: Yixian
    full: Yixian Liu
    id: yixian-liu
    last: Liu
  - first: Zixia
    full: Zixia Jia
    id: zixia-jia
    last: Jia
  - first: Chengyue
    full: Chengyue Jiang
    id: chengyue-jiang
    last: Jiang
  - first: Kewei
    full: Kewei Tu
    id: kewei-tu
    last: Tu
  author_string: Xinyu Wang, Yixian Liu, Zixia Jia, Chengyue Jiang, Kewei Tu
  bibkey: wang-etal-2019-shanghaitech
  bibtype: inproceedings
  booktitle: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  booktitle_html: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  doi: 10.18653/v1/K19-2005
  month: November
  page_first: '55'
  page_last: '65'
  pages: "55\u201365"
  paper_id: '5'
  parent_volume_id: K19-2
  pdf: https://www.aclweb.org/anthology/K19-2005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-2005.jpg
  title: 'ShanghaiTech at MRP 2019: Sequence-to-Graph Transduction with Second-Order
    Edge Inference for Cross-Framework Meaning Representation Parsing'
  title_html: '<span class="acl-fixed-case">S</span>hanghai<span class="acl-fixed-case">T</span>ech
    at <span class="acl-fixed-case">MRP</span> 2019: Sequence-to-Graph Transduction
    with Second-Order Edge Inference for Cross-Framework Meaning Representation Parsing'
  url: https://www.aclweb.org/anthology/K19-2005
  year: '2019'
K19-2006:
  abstract: We describe the Saarland University submission to the shared task on Cross-Framework
    Meaning Representation Parsing (MRP) at the 2019 Conference on Computational Natural
    Language Learning (CoNLL).
  address: Hong Kong
  attachment:
  - filename: K19-2006.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-2006.Attachment.zip
  author:
  - first: Lucia
    full: Lucia Donatelli
    id: lucia-donatelli
    last: Donatelli
  - first: Meaghan
    full: Meaghan Fowlie
    id: meaghan-fowlie
    last: Fowlie
  - first: Jonas
    full: Jonas Groschwitz
    id: jonas-groschwitz
    last: Groschwitz
  - first: Alexander
    full: Alexander Koller
    id: alexander-koller
    last: Koller
  - first: Matthias
    full: Matthias Lindemann
    id: matthias-lindemann
    last: Lindemann
  - first: Mario
    full: Mario Mina
    id: mario-mina
    last: Mina
  - first: Pia
    full: "Pia Wei\xDFenhorn"
    id: pia-weissenhorn
    last: "Wei\xDFenhorn"
  author_string: "Lucia Donatelli, Meaghan Fowlie, Jonas Groschwitz, Alexander Koller,\
    \ Matthias Lindemann, Mario Mina, Pia Wei\xDFenhorn"
  bibkey: donatelli-etal-2019-saarland
  bibtype: inproceedings
  booktitle: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  booktitle_html: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  doi: 10.18653/v1/K19-2006
  month: November
  page_first: '66'
  page_last: '75'
  pages: "66\u201375"
  paper_id: '6'
  parent_volume_id: K19-2
  pdf: https://www.aclweb.org/anthology/K19-2006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-2006.jpg
  title: 'Saarland at MRP 2019: Compositional parsing across all graphbanks'
  title_html: '<span class="acl-fixed-case">S</span>aarland at <span class="acl-fixed-case">MRP</span>
    2019: Compositional parsing across all graphbanks'
  url: https://www.aclweb.org/anthology/K19-2006
  year: '2019'
K19-2007:
  abstract: 'This paper describes our system (HIT-SCIR) for CoNLL 2019 shared task:
    Cross-Framework Meaning Representation Parsing. We extended the basic transition-based
    parser with two improvements: a) Efficient Training by realizing Stack LSTM parallel
    training; b) Effective Encoding via adopting deep contextualized word embeddings
    BERT. Generally, we proposed a unified pipeline to meaning representation parsing,
    including framework-specific transition-based parsers, BERT-enhanced word representation,
    and post-processing. In the final evaluation, our system was ranked first according
    to ALL-F1 (86.2%) and especially ranked first in UCCA framework (81.67%).'
  address: Hong Kong
  attachment:
  - filename: K19-2007.Attachment.pdf
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-2007.Attachment.pdf
  author:
  - first: Wanxiang
    full: Wanxiang Che
    id: wanxiang-che
    last: Che
  - first: Longxu
    full: Longxu Dou
    id: longxu-dou
    last: Dou
  - first: Yang
    full: Yang Xu
    id: yang-xu
    last: Xu
  - first: Yuxuan
    full: Yuxuan Wang
    id: yuxuan-wang
    last: Wang
  - first: Yijia
    full: Yijia Liu
    id: yijia-liu
    last: Liu
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  author_string: Wanxiang Che, Longxu Dou, Yang Xu, Yuxuan Wang, Yijia Liu, Ting Liu
  bibkey: che-etal-2019-hit
  bibtype: inproceedings
  booktitle: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  booktitle_html: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  doi: 10.18653/v1/K19-2007
  month: November
  page_first: '76'
  page_last: '85'
  pages: "76\u201385"
  paper_id: '7'
  parent_volume_id: K19-2
  pdf: https://www.aclweb.org/anthology/K19-2007.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-2007.jpg
  title: 'HIT-SCIR at MRP 2019: A Unified Pipeline for Meaning Representation Parsing
    via Efficient Training and Effective Encoding'
  title_html: '<span class="acl-fixed-case">HIT</span>-<span class="acl-fixed-case">SCIR</span>
    at <span class="acl-fixed-case">MRP</span> 2019: A Unified Pipeline for Meaning
    Representation Parsing via Efficient Training and Effective Encoding'
  url: https://www.aclweb.org/anthology/K19-2007
  year: '2019'
K19-2008:
  abstract: 'This paper describes the system of our team SJTU for our participation
    in the CoNLL 2019 Shared Task: Cross-Framework Meaning Representation Parsing.
    The goal of the task is to advance data-driven parsing into graph-structured representations
    of sentence meaning. This task includes five meaning representation frameworks:
    DM, PSD, EDS, UCCA, and AMR. These frameworks have different properties and structures.
    To tackle all the frameworks in one model, it is needed to find out the commonality
    of them. In our work, we define a set of the transition actions to once-for-all
    tackle all the frameworks and train a transition-based model to parse the meaning
    representation. The adopted multi-task model also can allow learning for one framework
    to benefit the others. In the final official evaluation of the shared task, our
    system achieves 42% F1 unified MRP metric score.'
  address: Hong Kong
  author:
  - first: Hongxiao
    full: Hongxiao Bai
    id: hongxiao-bai
    last: Bai
  - first: Hai
    full: Hai Zhao
    id: hai-zhao
    last: Zhao
  author_string: Hongxiao Bai, Hai Zhao
  bibkey: bai-zhao-2019-sjtu
  bibtype: inproceedings
  booktitle: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  booktitle_html: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  doi: 10.18653/v1/K19-2008
  month: November
  page_first: '86'
  page_last: '94'
  pages: "86\u201394"
  paper_id: '8'
  parent_volume_id: K19-2
  pdf: https://www.aclweb.org/anthology/K19-2008.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-2008.jpg
  title: 'SJTU at MRP 2019: A Transition-Based Multi-Task Parser for Cross-Framework
    Meaning Representation Parsing'
  title_html: '<span class="acl-fixed-case">SJTU</span> at <span class="acl-fixed-case">MRP</span>
    2019: A Transition-Based Multi-Task Parser for Cross-Framework Meaning Representation
    Parsing'
  url: https://www.aclweb.org/anthology/K19-2008
  year: '2019'
K19-2009:
  abstract: "This paper describes Jeonbuk National University (JBNU)\u2019s system\
    \ for the 2019 shared task on Cross-Framework Meaning Representation Parsing (MRP\
    \ 2019) at the Conference on Computational Natural Language Learning. Of the five\
    \ frameworks, we address only the DELPH-IN MRS Bi-Lexical Dependencies (DP), Prague\
    \ Semantic Dependencies (PSD), and Universal Conceptual Cognitive Annotation (UCCA)\
    \ frameworks. We propose a unified parsing model using biaffine attention (Dozat\
    \ and Manning, 2017), consisting of 1) a BERT-BiLSTM encoder and 2) a biaffine\
    \ attention decoder. First, the BERT-BiLSTM for sentence encoder uses BERT to\
    \ compose a sentence\u2019s wordpieces into word-level embeddings and subsequently\
    \ applies BiLSTM to word-level representations. Second, the biaffine attention\
    \ decoder determines the scores for an edge\u2019s existence and its labels based\
    \ on biaffine attention functions between roledependent representations. We also\
    \ present multi-level biaffine attention models by combining all the role-dependent\
    \ representations that appear at multiple intermediate layers."
  address: Hong Kong
  author:
  - first: Seung-Hoon
    full: Seung-Hoon Na
    id: seung-hoon-na
    last: Na
  - first: Jinwoon
    full: Jinwoon Min
    id: jinwoon-min
    last: Min
  - first: Kwanghyeon
    full: Kwanghyeon Park
    id: kwanghyeon-park
    last: Park
  - first: Jong-Hun
    full: Jong-Hun Shin
    id: jong-hun-shin
    last: Shin
  - first: Young-Kil
    full: Young-Kil Kim
    id: young-gil-kim
    last: Kim
  author_string: Seung-Hoon Na, Jinwoon Min, Kwanghyeon Park, Jong-Hun Shin, Young-Kil
    Kim
  bibkey: na-etal-2019-jbnu
  bibtype: inproceedings
  booktitle: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  booktitle_html: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  doi: 10.18653/v1/K19-2009
  month: November
  page_first: '95'
  page_last: '103'
  pages: "95\u2013103"
  paper_id: '9'
  parent_volume_id: K19-2
  pdf: https://www.aclweb.org/anthology/K19-2009.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-2009.jpg
  title: 'JBNU at MRP 2019: Multi-level Biaffine Attention for Semantic Dependency
    Parsing'
  title_html: '<span class="acl-fixed-case">JBNU</span> at <span class="acl-fixed-case">MRP</span>
    2019: Multi-level Biaffine Attention for Semantic Dependency Parsing'
  url: https://www.aclweb.org/anthology/K19-2009
  year: '2019'
K19-2010:
  abstract: This paper describes our system (RESOLVER) submitted to the CoNLL 2019
    shared task on Cross-Framework Meaning Representation Parsing (MRP). Our system
    implements a transition-based parser with a directed acyclic graph (DAG) to tree
    preprocessor and a novel cross-framework variable-arity resolve action that generalizes
    over five different representations. Although we ranked low in the competition,
    we have shown the current limitations and potentials of including variable-arity
    action in MRP and concluded with directions for improvements in the future.
  address: Hong Kong
  attachment:
  - filename: K19-2010.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-2010.Attachment.zip
  author:
  - first: Sunny
    full: Sunny Lai
    id: sunny-lai
    last: Lai
  - first: Chun Hei
    full: Chun Hei Lo
    id: chun-hei-lo
    last: Lo
  - first: Kwong Sak
    full: Kwong Sak Leung
    id: kwong-sak-leung
    last: Leung
  - first: Yee
    full: Yee Leung
    id: yee-leung
    last: Leung
  author_string: Sunny Lai, Chun Hei Lo, Kwong Sak Leung, Yee Leung
  bibkey: lai-etal-2019-cuhk
  bibtype: inproceedings
  booktitle: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  booktitle_html: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  doi: 10.18653/v1/K19-2010
  month: November
  page_first: '104'
  page_last: '113'
  pages: "104\u2013113"
  paper_id: '10'
  parent_volume_id: K19-2
  pdf: https://www.aclweb.org/anthology/K19-2010.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-2010.jpg
  title: 'CUHK at MRP 2019: Transition-Based Parser with Cross-Framework Variable-Arity
    Resolve Action'
  title_html: '<span class="acl-fixed-case">CUHK</span> at <span class="acl-fixed-case">MRP</span>
    2019: Transition-Based Parser with Cross-Framework Variable-Arity Resolve Action'
  url: https://www.aclweb.org/anthology/K19-2010
  year: '2019'
K19-2011:
  abstract: "This paper describes the proposed system of the Hitachi team for the\
    \ Cross-Framework Meaning Representation Parsing (MRP 2019) shared task. In this\
    \ shared task, the participating systems were asked to predict nodes, edges and\
    \ their attributes for five frameworks, each with different order of \u201Cabstraction\u201D\
    \ from input tokens. We proposed a unified encoder-to-biaffine network for all\
    \ five frameworks, which effectively incorporates a shared encoder to extract\
    \ rich input features, decoder networks to generate anchorless nodes in UCCA and\
    \ AMR, and biaffine networks to predict edges. Our system was ranked fifth with\
    \ the macro-averaged MRP F1 score of 0.7604, and outperformed the baseline unified\
    \ transition-based MRP. Furthermore, post-evaluation experiments showed that we\
    \ can boost the performance of the proposed system by incorporating multi-task\
    \ learning, whereas the baseline could not. These imply efficacy of incorporating\
    \ the biaffine network to the shared architecture for MRP and that learning heterogeneous\
    \ meaning representations at once can boost the system performance."
  address: Hong Kong
  author:
  - first: Yuta
    full: Yuta Koreeda
    id: yuta-koreeda
    last: Koreeda
  - first: Gaku
    full: Gaku Morio
    id: gaku-morio
    last: Morio
  - first: Terufumi
    full: Terufumi Morishita
    id: terufumi-morishita
    last: Morishita
  - first: Hiroaki
    full: Hiroaki Ozaki
    id: hiroaki-ozaki
    last: Ozaki
  - first: Kohsuke
    full: Kohsuke Yanai
    id: kohsuke-yanai
    last: Yanai
  author_string: Yuta Koreeda, Gaku Morio, Terufumi Morishita, Hiroaki Ozaki, Kohsuke
    Yanai
  bibkey: koreeda-etal-2019-hitachi
  bibtype: inproceedings
  booktitle: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  booktitle_html: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  doi: 10.18653/v1/K19-2011
  month: November
  page_first: '114'
  page_last: '126'
  pages: "114\u2013126"
  paper_id: '11'
  parent_volume_id: K19-2
  pdf: https://www.aclweb.org/anthology/K19-2011.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-2011.jpg
  title: 'Hitachi at MRP 2019: Unified Encoder-to-Biaffine Network for Cross-Framework
    Meaning Representation Parsing'
  title_html: 'Hitachi at <span class="acl-fixed-case">MRP</span> 2019: Unified Encoder-to-Biaffine
    Network for Cross-Framework Meaning Representation Parsing'
  url: https://www.aclweb.org/anthology/K19-2011
  year: '2019'
K19-2012:
  abstract: We present a system description of our contribution to the CoNLL 2019
    shared task, CrossFramework Meaning Representation Parsing (MRP 2019). The proposed
    architecture is our first attempt towards a semantic parsing extension of the
    UDPipe 2.0, a lemmatization, POS tagging and dependency parsing pipeline. For
    the MRP 2019, which features five formally and linguistically different approaches
    to meaning representation (DM, PSD, EDS, UCCA and AMR), we propose a uniform,
    language and framework agnostic graph-tograph neural network architecture. Without
    any knowledge about the graph structure, and specifically without any linguistically
    or framework motivated features, our system implicitly models the meaning representation
    graphs. After fixing a human error (we used earlier incorrect version of provided
    test set analyses), our submission would score third in the competition evaluation.
    The source code of our system is available at https://github.com/ufal/mrpipe-conll2019.
  address: Hong Kong
  author:
  - first: Milan
    full: Milan Straka
    id: milan-straka
    last: Straka
  - first: Jana
    full: "Jana Strakov\xE1"
    id: jana-strakova
    last: "Strakov\xE1"
  author_string: "Milan Straka, Jana Strakov\xE1"
  bibkey: straka-strakova-2019-ufal
  bibtype: inproceedings
  booktitle: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  booktitle_html: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  doi: 10.18653/v1/K19-2012
  month: November
  page_first: '127'
  page_last: '137'
  pages: "127\u2013137"
  paper_id: '12'
  parent_volume_id: K19-2
  pdf: https://www.aclweb.org/anthology/K19-2012.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-2012.jpg
  title: "\xDAFAL MRPipe at MRP 2019: UDPipe Goes Semantic in the Meaning Representation\
    \ Parsing Shared Task"
  title_html: "<span class=\"acl-fixed-case\">\xDAFAL</span> <span class=\"acl-fixed-case\"\
    >MRP</span>ipe at <span class=\"acl-fixed-case\">MRP</span> 2019: <span class=\"\
    acl-fixed-case\">UDP</span>ipe Goes Semantic in the Meaning Representation Parsing\
    \ Shared Task"
  url: https://www.aclweb.org/anthology/K19-2012
  year: '2019'
K19-2013:
  abstract: 'This paper describes the system submission of our team Amazon to the
    shared task on Cross Framework Meaning Representation Parsing (MRP) at the 2019
    Conference for Computational Language Learning (CoNLL). Via extensive analysis
    of implicit alignments in AMR, we recategorize five meaning representations (MRs)
    into two classes: Lexical- Anchoring and Phrasal-Anchoring. Then we propose a
    unified graph-based parsing framework for the lexical-anchoring MRs, and a phrase-structure
    parsing for one of the phrasal- anchoring MRs, UCCA. Our system submission ranked
    1st in the AMR subtask, and later improvements show promising results on other
    frameworks as well.'
  address: Hong Kong
  author:
  - first: Jie
    full: Jie Cao
    id: jie-cao
    last: Cao
  - first: Yi
    full: Yi Zhang
    id: yi-zhang
    last: Zhang
  - first: Adel
    full: Adel Youssef
    id: adel-youssef
    last: Youssef
  - first: Vivek
    full: Vivek Srikumar
    id: vivek-srikumar
    last: Srikumar
  author_string: Jie Cao, Yi Zhang, Adel Youssef, Vivek Srikumar
  bibkey: cao-etal-2019-amazon
  bibtype: inproceedings
  booktitle: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  booktitle_html: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  doi: 10.18653/v1/K19-2013
  month: November
  page_first: '138'
  page_last: '148'
  pages: "138\u2013148"
  paper_id: '13'
  parent_volume_id: K19-2
  pdf: https://www.aclweb.org/anthology/K19-2013.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-2013.jpg
  title: 'Amazon at MRP 2019: Parsing Meaning Representations with Lexical and Phrasal
    Anchoring'
  title_html: '<span class="acl-fixed-case">A</span>mazon at <span class="acl-fixed-case">MRP</span>
    2019: Parsing Meaning Representations with Lexical and Phrasal Anchoring'
  url: https://www.aclweb.org/anthology/K19-2013
  year: '2019'
K19-2014:
  abstract: In this paper, we describe our participating systems in the shared task
    on Cross- Framework Meaning Representation Parsing (MRP) at the 2019 Conference
    for Computational Language Learning (CoNLL). The task includes five frameworks
    for graph-based meaning representations, i.e., DM, PSD, EDS, UCCA, and AMR. One
    common characteristic of our systems is that we employ graph-based methods instead
    of transition-based methods when predicting edges between nodes. For SDP, we jointly
    perform edge prediction, frame tagging, and POS tagging via multi-task learning
    (MTL). For UCCA, we also jointly model a constituent tree parsing and a remote
    edge recovery task. For both EDS and AMR, we produce nodes first and edges second
    in a pipeline fashion. External resources like BERT are found helpful for all
    frameworks except AMR. Our final submission ranks the third on the overall MRP
    evaluation metric, the first on EDS and the second on UCCA.
  address: Hong Kong
  author:
  - first: Yue
    full: Yue Zhang
    id: yue-zhang
    last: Zhang
  - first: Wei
    full: Wei Jiang
    id: wei-jiang
    last: Jiang
  - first: Qingrong
    full: Qingrong Xia
    id: qingrong-xia
    last: Xia
  - first: Junjie
    full: Junjie Cao
    id: junjie-cao
    last: Cao
  - first: Rui
    full: Rui Wang
    id: rui-wang
    last: Wang
  - first: Zhenghua
    full: Zhenghua Li
    id: zhenghua-li
    last: Li
  - first: Min
    full: Min Zhang
    id: min-zhang
    last: Zhang
  author_string: Yue Zhang, Wei Jiang, Qingrong Xia, Junjie Cao, Rui Wang, Zhenghua
    Li, Min Zhang
  bibkey: zhang-etal-2019-suda
  bibtype: inproceedings
  booktitle: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  booktitle_html: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  doi: 10.18653/v1/K19-2014
  month: November
  page_first: '149'
  page_last: '157'
  pages: "149\u2013157"
  paper_id: '14'
  parent_volume_id: K19-2
  pdf: https://www.aclweb.org/anthology/K19-2014.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-2014.jpg
  title: 'SUDA-Alibaba at MRP 2019: Graph-Based Models with BERT'
  title_html: '<span class="acl-fixed-case">SUDA</span>-<span class="acl-fixed-case">A</span>libaba
    at <span class="acl-fixed-case">MRP</span> 2019: Graph-Based Models with <span
    class="acl-fixed-case">BERT</span>'
  url: https://www.aclweb.org/anthology/K19-2014
  year: '2019'
K19-2015:
  abstract: "This paper describes the \xDAFAL--Oslo system submission to the shared\
    \ task on Cross-Framework Meaning Representation Parsing (MRP, Oepen et al. 2019).\
    \ The submission is based on several third-party parsers. Within the official\
    \ shared task results, the submission ranked 11th out of 13 participating systems."
  address: Hong Kong
  attachment:
  - filename: K19-2015.Attachment.zip
    type: attachment
    url: https://www.aclweb.org/anthology/attachments/K19-2015.Attachment.zip
  author:
  - first: Kira
    full: Kira Droganova
    id: kira-droganova
    last: Droganova
  - first: Andrey
    full: Andrey Kutuzov
    id: andrey-kutuzov
    last: Kutuzov
  - first: Nikita
    full: Nikita Mediankin
    id: nikita-mediankin
    last: Mediankin
  - first: Daniel
    full: Daniel Zeman
    id: daniel-zeman
    last: Zeman
  author_string: Kira Droganova, Andrey Kutuzov, Nikita Mediankin, Daniel Zeman
  bibkey: droganova-etal-2019-ufal
  bibtype: inproceedings
  booktitle: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  booktitle_html: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  doi: 10.18653/v1/K19-2015
  month: November
  page_first: '158'
  page_last: '165'
  pages: "158\u2013165"
  paper_id: '15'
  parent_volume_id: K19-2
  pdf: https://www.aclweb.org/anthology/K19-2015.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-2015.jpg
  title: "\xDAFAL-Oslo at MRP 2019: Garage Sale Semantic Parsing"
  title_html: "<span class=\"acl-fixed-case\">\xDAFAL</span>-Oslo at <span class=\"\
    acl-fixed-case\">MRP</span> 2019: Garage Sale Semantic Parsing"
  url: https://www.aclweb.org/anthology/K19-2015
  year: '2019'
K19-2016:
  abstract: 'We design, implement and evaluate two semantic parsers, which represent
    factorization- and composition-based approaches respectively, for Elementary Dependency
    Structures (EDS) at the CoNLL 2019 Shared Task on Cross-Framework Meaning Representation
    Parsing. The detailed evaluation of the two parsers gives us a new perception
    about parsing into linguistically enriched meaning representations: current neural
    EDS parsers are able to reach an accuracy at the inter-annotator agreement level
    in the same-epoch-and-domain setup.'
  address: Hong Kong
  author:
  - first: Yufei
    full: Yufei Chen
    id: yufei-chen
    last: Chen
  - first: Yajie
    full: Yajie Ye
    id: yajie-ye
    last: Ye
  - first: Weiwei
    full: Weiwei Sun
    id: weiwei-sun
    last: Sun
  author_string: Yufei Chen, Yajie Ye, Weiwei Sun
  bibkey: chen-etal-2019-peking
  bibtype: inproceedings
  booktitle: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  booktitle_html: Proceedings of the Shared Task on Cross-Framework Meaning Representation
    Parsing at the 2019 Conference on Natural Language Learning
  doi: 10.18653/v1/K19-2016
  month: November
  page_first: '166'
  page_last: '176'
  pages: "166\u2013176"
  paper_id: '16'
  parent_volume_id: K19-2
  pdf: https://www.aclweb.org/anthology/K19-2016.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K19-2016.jpg
  title: 'Peking at MRP 2019: Factorization- and Composition-Based Parsing for Elementary
    Dependency Structures'
  title_html: 'Peking at <span class="acl-fixed-case">MRP</span> 2019: Factorization-
    and Composition-Based Parsing for Elementary Dependency Structures'
  url: https://www.aclweb.org/anthology/K19-2016
  year: '2019'
