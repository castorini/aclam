K18-1000:
  address: Brussels, Belgium
  author:
  - first: Anna
    full: Anna Korhonen
    id: anna-korhonen
    last: Korhonen
  - first: Ivan
    full: Ivan Titov
    id: ivan-titov
    last: Titov
  author_string: Anna Korhonen, Ivan Titov
  bibkey: conll-2018-natural
  bibtype: proceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  month: October
  paper_id: '0'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1000.jpg
  title: Proceedings of the 22nd Conference on Computational Natural Language Learning
  title_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  url: https://www.aclweb.org/anthology/K18-1000
  year: '2018'
K18-1001:
  abstract: Complex textual information extraction tasks are often posed as sequence
    labeling or shallow parsing, where fields are extracted using local labels made
    consistent through probabilistic inference in a graphical model with constrained
    transitions. Recently, it has become common to locally parametrize these models
    using rich features extracted by recurrent neural networks (such as LSTM), while
    enforcing consistent outputs through a simple linear-chain model, representing
    Markovian dependencies between successive labels. However, the simple graphical
    model structure belies the often complex non-local constraints between output
    labels. For example, many fields, such as a first name, can only occur a fixed
    number of times, or in the presence of other fields. While RNNs have provided
    increasingly powerful context-aware local features for sequence tagging, they
    have yet to be integrated with a global graphical model of similar expressivity
    in the output distribution. Our model goes beyond the linear chain CRF to incorporate
    multiple hidden states per output label, but parametrizes them parsimoniously
    with low-rank log-potential scoring matrices, effectively learning an embedding
    space for hidden states. This augmented latent space of inference variables complements
    the rich feature representation of the RNN, and allows exact global inference
    obeying complex, learned non-local output constraints. We experiment with several
    datasets and show that the model outperforms baseline CRF+RNN models when global
    output constraints are necessary at inference-time, and explore the interpretable
    latent structure.
  address: Brussels, Belgium
  author:
  - first: Dung
    full: Dung Thai
    id: dung-thai
    last: Thai
  - first: Sree Harsha
    full: Sree Harsha Ramesh
    id: sree-harsha-ramesh
    last: Ramesh
  - first: Shikhar
    full: Shikhar Murty
    id: shikhar-murty
    last: Murty
  - first: Luke
    full: Luke Vilnis
    id: luke-vilnis
    last: Vilnis
  - first: Andrew
    full: Andrew McCallum
    id: andrew-mccallum
    last: McCallum
  author_string: Dung Thai, Sree Harsha Ramesh, Shikhar Murty, Luke Vilnis, Andrew
    McCallum
  bibkey: thai-etal-2018-embedded
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1001
  month: October
  page_first: '1'
  page_last: '10'
  pages: "1\u201310"
  paper_id: '1'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1001.jpg
  title: Embedded-State Latent Conditional Random Fields for Sequence Labeling
  title_html: Embedded-State Latent Conditional Random Fields for Sequence Labeling
  url: https://www.aclweb.org/anthology/K18-1001
  year: '2018'
K18-1002:
  abstract: Word embeddings have become a mainstream tool in statistical natural language
    processing. Practitioners often use pre-trained word vectors, which were trained
    on large generic text corpora, and which are readily available on the web. However,
    pre-trained word vectors oftentimes lack important words from specific domains.
    It is therefore often desirable to extend the vocabulary and embed new words into
    a set of pre-trained word vectors. In this paper, we present an efficient method
    for including new words from a specialized corpus, containing new words, into
    pre-trained generic word embeddings. We build on the established view of word
    embeddings as matrix factorizations to present a spectral algorithm for this task.
    Experiments on several domain-specific corpora with specialized vocabularies demonstrate
    that our method is able to embed the new words efficiently into the original embedding
    space. Compared to competing methods, our method is faster, parameter-free, and
    deterministic.
  address: Brussels, Belgium
  author:
  - first: Tianfan
    full: Tianfan Fu
    id: tianfan-fu
    last: Fu
  - first: Cheng
    full: Cheng Zhang
    id: cheng-zhang
    last: Zhang
  - first: Stephan
    full: Stephan Mandt
    id: stephan-mandt
    last: Mandt
  author_string: Tianfan Fu, Cheng Zhang, Stephan Mandt
  bibkey: fu-etal-2018-continuous
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1002
  month: October
  page_first: '11'
  page_last: '20'
  pages: "11\u201320"
  paper_id: '2'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1002.jpg
  title: Continuous Word Embedding Fusion via Spectral Decomposition
  title_html: Continuous Word Embedding Fusion via Spectral Decomposition
  url: https://www.aclweb.org/anthology/K18-1002
  year: '2018'
K18-1003:
  abstract: "Recent deep learning models have shown improving results to natural language\
    \ generation (NLG) irrespective of providing sufficient annotated data. However,\
    \ a modest training data may harm such models\u2019 performance. Thus, how to\
    \ build a generator that can utilize as much of knowledge from a low-resource\
    \ setting data is a crucial issue in NLG. This paper presents a variational neural-based\
    \ generation model to tackle the NLG problem of having limited labeled dataset,\
    \ in which we integrate a variational inference into an encoder-decoder generator\
    \ and introduce a novel auxiliary auto-encoding with an effective training procedure.\
    \ Experiments showed that the proposed methods not only outperform the previous\
    \ models when having sufficient training dataset but also demonstrate strong ability\
    \ to work acceptably well when the training data is scarce."
  address: Brussels, Belgium
  author:
  - first: Van-Khanh
    full: Van-Khanh Tran
    id: van-khanh-tran
    last: Tran
  - first: Le-Minh
    full: Le-Minh Nguyen
    id: minh-le-nguyen
    last: Nguyen
  author_string: Van-Khanh Tran, Le-Minh Nguyen
  bibkey: tran-nguyen-2018-dual
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1003
  month: October
  page_first: '21'
  page_last: '30'
  pages: "21\u201330"
  paper_id: '3'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1003.jpg
  title: Dual Latent Variable Model for Low-Resource Natural Language Generation in
    Dialogue Systems
  title_html: Dual Latent Variable Model for Low-Resource Natural Language Generation
    in Dialogue Systems
  url: https://www.aclweb.org/anthology/K18-1003
  year: '2018'
K18-1004:
  abstract: Measuring entity relatedness is a fundamental task for many natural language
    processing and information retrieval applications. Prior work often studies entity
    relatedness in a static setting and unsupervised manner. However, entities in
    real-world are often involved in many different relationships, consequently entity
    relations are very dynamic over time. In this work, we propose a neural network-based
    approach that leverages public attention as supervision. Our model is capable
    of learning rich and different entity representations in a joint framework. Through
    extensive experiments on large-scale datasets, we demonstrate that our method
    achieves better results than competitive baselines.
  address: Brussels, Belgium
  author:
  - first: Tu
    full: Tu Nguyen
    id: tu-nguyen
    last: Nguyen
  - first: Tuan
    full: Tuan Tran
    id: tuan-tran
    last: Tran
  - first: Wolfgang
    full: Wolfgang Nejdl
    id: wolfgang-nejdl
    last: Nejdl
  author_string: Tu Nguyen, Tuan Tran, Wolfgang Nejdl
  bibkey: nguyen-etal-2018-trio
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1004
  month: October
  page_first: '31'
  page_last: '41'
  pages: "31\u201341"
  paper_id: '4'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1004.jpg
  title: A Trio Neural Model for Dynamic Entity Relatedness Ranking
  title_html: A Trio Neural Model for Dynamic Entity Relatedness Ranking
  url: https://www.aclweb.org/anthology/K18-1004
  year: '2018'
K18-1005:
  abstract: "Locations of social media users are important to many applications such\
    \ as rapid disaster response, targeted advertisement, and news recommendation.\
    \ However, many users do not share their exact geographical coordinates due to\
    \ reasons such as privacy concerns. The lack of explicit location information\
    \ has motivated a growing body of research in recent years looking at different\
    \ automatic ways of determining the user\u2019s primary location. In this paper,\
    \ we propose a unified user geolocation method which relies on a fusion of neural\
    \ networks. Our joint model incorporates different types of available information\
    \ including tweet text, user network, and metadata to predict users\u2019 locations.\
    \ Moreover, we utilize a bidirectional LSTM network augmented with an attention\
    \ mechanism to identify the most location indicative words in textual content\
    \ of tweets. The experiments demonstrate that our approach achieves state-of-the-art\
    \ performance over two Twitter benchmark geolocation datasets. We also conduct\
    \ an ablation study to evaluate the contribution of each type of information in\
    \ user geolocation performance."
  address: Brussels, Belgium
  author:
  - first: Mohammad
    full: Mohammad Ebrahimi
    id: mohammad-ebrahimi
    last: Ebrahimi
  - first: Elaheh
    full: Elaheh ShafieiBavani
    id: elaheh-shafieibavani
    last: ShafieiBavani
  - first: Raymond
    full: Raymond Wong
    id: raymond-wong
    last: Wong
  - first: Fang
    full: Fang Chen
    id: fang-chen
    last: Chen
  author_string: Mohammad Ebrahimi, Elaheh ShafieiBavani, Raymond Wong, Fang Chen
  bibkey: ebrahimi-etal-2018-unified
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1005
  month: October
  page_first: '42'
  page_last: '53'
  pages: "42\u201353"
  paper_id: '5'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1005.jpg
  title: A Unified Neural Network Model for Geolocating Twitter Users
  title_html: A Unified Neural Network Model for Geolocating <span class="acl-fixed-case">T</span>witter
    Users
  url: https://www.aclweb.org/anthology/K18-1005
  year: '2018'
K18-1006:
  abstract: Thematic role hierarchy is a widely used linguistic tool to describe interactions
    between semantic roles and their syntactic realizations. Despite decades of dedicated
    research and numerous thematic hierarchy suggestions in the literature, this concept
    has not been used in NLP so far due to incompatibility and limited scope of existing
    hierarchies. We introduce an empirical framework for thematic hierarchy induction
    and evaluate several role ranking strategies on English and German full-text corpus
    data. We hypothesize that global thematic hierarchy induction is feasible, that
    a hierarchy can be induced from just fractions of training data and that resulting
    hierarchies apply cross-lingually. We evaluate these assumptions empirically.
  address: Brussels, Belgium
  author:
  - first: Ilia
    full: Ilia Kuznetsov
    id: ilia-kuznetsov
    last: Kuznetsov
  - first: Iryna
    full: Iryna Gurevych
    id: iryna-gurevych
    last: Gurevych
  author_string: Ilia Kuznetsov, Iryna Gurevych
  bibkey: kuznetsov-gurevych-2018-corpus
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1006
  month: October
  page_first: '54'
  page_last: '64'
  pages: "54\u201364"
  paper_id: '6'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1006.jpg
  title: Corpus-Driven Thematic Hierarchy Induction
  title_html: Corpus-Driven Thematic Hierarchy Induction
  url: https://www.aclweb.org/anthology/K18-1006
  year: '2018'
K18-1007:
  abstract: "Adversarial examples are inputs to machine learning models designed to\
    \ cause the model to make a mistake. They are useful for understanding the shortcomings\
    \ of machine learning models, interpreting their results, and for regularisation.\
    \ In NLP, however, most example generation strategies produce input text by using\
    \ known, pre-specified semantic transformations, requiring significant manual\
    \ effort and in-depth understanding of the problem and domain. In this paper,\
    \ we investigate the problem of automatically generating adversarial examples\
    \ that violate a set of given First-Order Logic constraints in Natural Language\
    \ Inference (NLI). We reduce the problem of identifying such adversarial examples\
    \ to a combinatorial optimisation problem, by maximising a quantity measuring\
    \ the degree of violation of such constraints and by using a language model for\
    \ generating linguistically-plausible examples. Furthermore, we propose a method\
    \ for adversarially regularising neural NLI models for incorporating background\
    \ knowledge. Our results show that, while the proposed method does not always\
    \ improve results on the SNLI and MultiNLI datasets, it significantly and consistently\
    \ increases the predictive accuracy on adversarially-crafted datasets \u2013 up\
    \ to a 79.6% relative improvement \u2013 while drastically reducing the number\
    \ of background knowledge violations. Furthermore, we show that adversarial examples\
    \ transfer among model architectures, and that the proposed adversarial training\
    \ procedure improves the robustness of NLI models to adversarial examples."
  address: Brussels, Belgium
  author:
  - first: Pasquale
    full: Pasquale Minervini
    id: pasquale-minervini
    last: Minervini
  - first: Sebastian
    full: Sebastian Riedel
    id: sebastian-riedel
    last: Riedel
  author_string: Pasquale Minervini, Sebastian Riedel
  bibkey: minervini-riedel-2018-adversarially
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1007
  month: October
  page_first: '65'
  page_last: '74'
  pages: "65\u201374"
  paper_id: '7'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1007.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1007.jpg
  title: Adversarially Regularising Neural NLI Models to Integrate Logical Background
    Knowledge
  title_html: Adversarially Regularising Neural <span class="acl-fixed-case">NLI</span>
    Models to Integrate Logical Background Knowledge
  url: https://www.aclweb.org/anthology/K18-1007
  year: '2018'
K18-1008:
  abstract: "We investigate the relation between the transposition and deletion effects\
    \ in word reading, i.e., the finding that readers can successfully read \u201C\
    SLAT\u201D as \u201CSALT\u201D, or \u201CWRK\u201D as \u201CWORK\u201D, and the\
    \ neighborhood effect. In particular, we investigate whether lexical orthographic\
    \ neighborhoods take into account transposition and deletion in determining neighbors.\
    \ If this is the case, it is more likely that the neighborhood effect takes place\
    \ early during processing, and does not solely rely on similarity of internal\
    \ representations. We introduce a new neighborhood measure, rd20, which can be\
    \ used to quantify neighborhood effects over arbitrary feature spaces. We calculate\
    \ the rd20 over large sets of words in three languages using various feature sets\
    \ and show that feature sets that do not allow for transposition or deletion explain\
    \ more variance in Reaction Time (RT) measurements. We also show that the rd20\
    \ can be calculated using the hidden state representations of an Multi-Layer Perceptron,\
    \ and show that these explain less variance than the raw features. We conclude\
    \ that the neighborhood effect is unlikely to have a perceptual basis, but is\
    \ more likely to be the result of items co-activating after recognition. All code\
    \ is available at: www.github.com/clips/conll2018"
  address: Brussels, Belgium
  author:
  - first: "St\xE9phan"
    full: "St\xE9phan Tulkens"
    id: stephan-tulkens
    last: Tulkens
  - first: Dominiek
    full: Dominiek Sandra
    id: dominiek-sandra
    last: Sandra
  - first: Walter
    full: Walter Daelemans
    id: walter-daelemans
    last: Daelemans
  author_string: "St\xE9phan Tulkens, Dominiek Sandra, Walter Daelemans"
  bibkey: tulkens-etal-2018-strings
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1008
  month: October
  page_first: '75'
  page_last: '85'
  pages: "75\u201385"
  paper_id: '8'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1008.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1008.jpg
  title: 'From Strings to Other Things: Linking the Neighborhood and Transposition
    Effects in Word Reading'
  title_html: 'From Strings to Other Things: Linking the Neighborhood and Transposition
    Effects in Word Reading'
  url: https://www.aclweb.org/anthology/K18-1008
  year: '2018'
K18-1009:
  abstract: Many name tagging approaches use local contextual information with much
    success, but can fail when the local context is ambiguous or limited. We present
    a new framework to improve name tagging by utilizing local, document-level, and
    corpus-level contextual information. For each word, we retrieve document-level
    context from other sentences within the same document and corpus-level context
    from sentences in other documents. We propose a model that learns to incorporate
    document-level and corpus-level contextual information alongside local contextual
    information via document-level and corpus-level attentions, which dynamically
    weight their respective contextual information and determines the influence of
    this information through gating mechanisms. Experiments on benchmark datasets
    show the effectiveness of our approach, which achieves state-of-the-art results
    for Dutch, German, and Spanish on the CoNLL-2002 and CoNLL-2003 datasets. We will
    make our code and pre-trained models publicly available for research purposes.
  address: Brussels, Belgium
  author:
  - first: Boliang
    full: Boliang Zhang
    id: boliang-zhang
    last: Zhang
  - first: Spencer
    full: Spencer Whitehead
    id: spencer-whitehead
    last: Whitehead
  - first: Lifu
    full: Lifu Huang
    id: lifu-huang
    last: Huang
  - first: Heng
    full: Heng Ji
    id: heng-ji
    last: Ji
  author_string: Boliang Zhang, Spencer Whitehead, Lifu Huang, Heng Ji
  bibkey: zhang-etal-2018-global
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1009
  month: October
  page_first: '86'
  page_last: '96'
  pages: "86\u201396"
  paper_id: '9'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1009.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1009.jpg
  title: Global Attention for Name Tagging
  title_html: Global Attention for Name Tagging
  url: https://www.aclweb.org/anthology/K18-1009
  year: '2018'
K18-1010:
  abstract: Current state-of-the-art machine translation systems are based on encoder-decoder
    architectures, that first encode the input sequence, and then generate an output
    sequence based on the input encoding. Both are interfaced with an attention mechanism
    that recombines a fixed encoding of the source tokens based on the decoder state.
    We propose an alternative approach which instead relies on a single 2D convolutional
    neural network across both sequences. Each layer of our network re-codes source
    tokens on the basis of the output sequence produced so far. Attention-like properties
    are therefore pervasive throughout the network. Our model yields excellent results,
    outperforming state-of-the-art encoder-decoder systems, while being conceptually
    simpler and having fewer parameters.
  address: Brussels, Belgium
  author:
  - first: Maha
    full: Maha Elbayad
    id: maha-elbayad
    last: Elbayad
  - first: Laurent
    full: Laurent Besacier
    id: laurent-besacier
    last: Besacier
  - first: Jakob
    full: Jakob Verbeek
    id: jakob-verbeek
    last: Verbeek
  author_string: Maha Elbayad, Laurent Besacier, Jakob Verbeek
  bibkey: elbayad-etal-2018-pervasive
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1010
  month: October
  page_first: '97'
  page_last: '107'
  pages: "97\u2013107"
  paper_id: '10'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1010.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1010.jpg
  title: 'Pervasive Attention: 2D Convolutional Neural Networks for Sequence-to-Sequence
    Prediction'
  title_html: 'Pervasive Attention: 2<span class="acl-fixed-case">D</span> Convolutional
    Neural Networks for Sequence-to-Sequence Prediction'
  url: https://www.aclweb.org/anthology/K18-1010
  year: '2018'
K18-1011:
  abstract: We propose a machine reading comprehension model based on the compare-aggregate
    framework with two-staged attention that achieves state-of-the-art results on
    the MovieQA question answering dataset. To investigate the limitations of our
    model as well as the behavioral difference between convolutional and recurrent
    neural networks, we generate adversarial examples to confuse the model and compare
    to human performance. Furthermore, we assess the generalizability of our model
    by analyzing its differences to human inference, drawing upon insights from cognitive
    science.
  address: Brussels, Belgium
  author:
  - first: Matthias
    full: Matthias Blohm
    id: matthias-blohm
    last: Blohm
  - first: Glorianna
    full: Glorianna Jagfeld
    id: glorianna-jagfeld
    last: Jagfeld
  - first: Ekta
    full: Ekta Sood
    id: ekta-sood
    last: Sood
  - first: Xiang
    full: Xiang Yu
    id: xiang-yu
    last: Yu
  - first: Ngoc Thang
    full: Ngoc Thang Vu
    id: ngoc-thang-vu
    last: Vu
  author_string: Matthias Blohm, Glorianna Jagfeld, Ekta Sood, Xiang Yu, Ngoc Thang
    Vu
  bibkey: blohm-etal-2018-comparing
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1011
  month: October
  page_first: '108'
  page_last: '118'
  pages: "108\u2013118"
  paper_id: '11'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1011.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1011.jpg
  title: 'Comparing Attention-Based Convolutional and Recurrent Neural Networks: Success
    and Limitations in Machine Reading Comprehension'
  title_html: 'Comparing Attention-Based Convolutional and Recurrent Neural Networks:
    Success and Limitations in Machine Reading Comprehension'
  url: https://www.aclweb.org/anthology/K18-1011
  year: '2018'
K18-1012:
  abstract: Existing research on question answering (QA) and comprehension reading
    (RC) are mainly focused on the resource-rich language like English. In recent
    times, the rapid growth of multi-lingual web content has posed several challenges
    to the existing QA systems. Code-mixing is one such challenge that makes the task
    more complex. In this paper, we propose a linguistically motivated technique for
    code-mixed question generation (CMQG) and a neural network based architecture
    for code-mixed question answering (CMQA). For evaluation, we manually create the
    code-mixed questions for Hindi-English language pair. In order to show the effectiveness
    of our neural network based CMQA technique, we utilize two benchmark datasets,
    SQuAD and MMQA. Experiments show that our proposed model achieves encouraging
    performance on CMQG and CMQA.
  address: Brussels, Belgium
  author:
  - first: Deepak
    full: Deepak Gupta
    id: deepak-gupta
    last: Gupta
  - first: Pabitra
    full: Pabitra Lenka
    id: pabitra-lenka
    last: Lenka
  - first: Asif
    full: Asif Ekbal
    id: asif-ekbal
    last: Ekbal
  - first: Pushpak
    full: Pushpak Bhattacharyya
    id: pushpak-bhattacharyya
    last: Bhattacharyya
  author_string: Deepak Gupta, Pabitra Lenka, Asif Ekbal, Pushpak Bhattacharyya
  bibkey: gupta-etal-2018-uncovering
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1012
  month: October
  page_first: '119'
  page_last: '130'
  pages: "119\u2013130"
  paper_id: '12'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1012.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1012.jpg
  title: 'Uncovering Code-Mixed Challenges: A Framework for Linguistically Driven
    Question Generation and Neural Based Question Answering'
  title_html: 'Uncovering Code-Mixed Challenges: A Framework for Linguistically Driven
    Question Generation and Neural Based Question Answering'
  url: https://www.aclweb.org/anthology/K18-1012
  year: '2018'
K18-1013:
  abstract: 'While learning embedding models has yielded fruitful results in several
    NLP subfields, most notably Word2Vec, embedding correspondence has relatively
    not been well explored especially in the context of natural language understanding
    (NLU), a task that typically extracts structured semantic knowledge from a text.
    A NLU embedding model can facilitate analyzing and understanding relationships
    between unstructured texts and their corresponding structured semantic knowledge,
    essential for both researchers and practitioners of NLU. Toward this end, we propose
    a framework that learns to embed semantic correspondence between text and its
    extracted semantic knowledge, called semantic frame. One key contributed technique
    is semantic frame reconstruction used to derive a one-to-one mapping between embedded
    vectors and their corresponding semantic frames. Embedding into semantically meaningful
    vectors and computing their distances in vector space provides a simple, but effective
    way to measure semantic similarities. With the proposed framework, we demonstrate
    three key areas where the embedding model can be effective: visualization, semantic
    search and re-ranking.'
  address: Brussels, Belgium
  author:
  - first: Sangkeun
    full: Sangkeun Jung
    id: sangkeun-jung
    last: Jung
  - first: Jinsik
    full: Jinsik Lee
    id: jinsik-lee
    last: Lee
  - first: Jiwon
    full: Jiwon Kim
    id: jiwon-kim
    last: Kim
  author_string: Sangkeun Jung, Jinsik Lee, Jiwon Kim
  bibkey: jung-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1013
  month: October
  page_first: '131'
  page_last: '140'
  pages: "131\u2013140"
  paper_id: '13'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1013.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1013.jpg
  title: Learning to Embed Semantic Correspondence for Natural Language Understanding
  title_html: Learning to Embed Semantic Correspondence for Natural Language Understanding
  url: https://www.aclweb.org/anthology/K18-1013
  year: '2018'
K18-1014:
  abstract: This study focuses on acquisition of commonsense knowledge. A previous
    study proposed a commonsense knowledge base completion (CKB completion) method
    that predicts a confidence score of for triplet-style knowledge for improving
    the coverage of CKBs. To improve the accuracy of CKB completion and expand the
    size of CKBs, we formulate a new commonsense knowledge base generation task (CKB
    generation) and propose a joint learning method that incorporates both CKB completion
    and CKB generation. Experimental results show that the joint learning method improved
    completion accuracy and the generation model created reasonable knowledge. Our
    generation model could also be used to augment data and improve the accuracy of
    completion.
  address: Brussels, Belgium
  author:
  - first: Itsumi
    full: Itsumi Saito
    id: itsumi-saito
    last: Saito
  - first: Kyosuke
    full: Kyosuke Nishida
    id: kyosuke-nishida
    last: Nishida
  - first: Hisako
    full: Hisako Asano
    id: hisako-asano
    last: Asano
  - first: Junji
    full: Junji Tomita
    id: junji-tomita
    last: Tomita
  author_string: Itsumi Saito, Kyosuke Nishida, Hisako Asano, Junji Tomita
  bibkey: saito-etal-2018-commonsense
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1014
  month: October
  page_first: '141'
  page_last: '150'
  pages: "141\u2013150"
  paper_id: '14'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1014.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1014.jpg
  title: Commonsense Knowledge Base Completion and Generation
  title_html: Commonsense Knowledge Base Completion and Generation
  url: https://www.aclweb.org/anthology/K18-1014
  year: '2018'
K18-1015:
  abstract: We study the application of active learning techniques to the translation
    of unbounded data streams via interactive neural machine translation. The main
    idea is to select, from an unbounded stream of source sentences, those worth to
    be supervised by a human agent. The user will interactively translate those samples.
    Once validated, these data is useful for adapting the neural machine translation
    model. We propose two novel methods for selecting the samples to be validated.
    We exploit the information from the attention mechanism of a neural machine translation
    system. Our experiments show that the inclusion of active learning techniques
    into this pipeline allows to reduce the effort required during the process, while
    increasing the quality of the translation system. Moreover, it enables to balance
    the human effort required for achieving a certain translation quality. Moreover,
    our neural system outperforms classical approaches by a large margin.
  address: Brussels, Belgium
  author:
  - first: "\xC1lvaro"
    full: "\xC1lvaro Peris"
    id: alvaro-peris
    last: Peris
  - first: Francisco
    full: Francisco Casacuberta
    id: francisco-casacuberta
    last: Casacuberta
  author_string: "\xC1lvaro Peris, Francisco Casacuberta"
  bibkey: peris-casacuberta-2018-active
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1015
  month: October
  page_first: '151'
  page_last: '160'
  pages: "151\u2013160"
  paper_id: '15'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1015.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1015.jpg
  title: Active Learning for Interactive Neural Machine Translation of Data Streams
  title_html: Active Learning for Interactive Neural Machine Translation of Data Streams
  url: https://www.aclweb.org/anthology/K18-1015
  year: '2018'
K18-1016:
  abstract: We propose a new method to detect when users express the intent to leave
    a service, also known as churn. While previous work focuses solely on social media,
    we show that this intent can be detected in chatbot conversations. As companies
    increasingly rely on chatbots they need an overview of potentially churny users.
    To this end, we crowdsource and publish a dataset of churn intent expressions
    in chatbot interactions in German and English. We show that classifiers trained
    on social media data can detect the same intent in the context of chatbots. We
    introduce a classification architecture that outperforms existing work on churn
    intent detection in social media. Moreover, we show that, using bilingual word
    embeddings, a system trained on combined English and German data outperforms monolingual
    approaches. As the only existing dataset is in English, we crowdsource and publish
    a novel dataset of German tweets. We thus underline the universal aspect of the
    problem, as examples of churn intent in English help us identify churn in German
    tweets and chatbot conversations.
  address: Brussels, Belgium
  author:
  - first: Christian
    full: Christian Abbet
    id: christian-abbet
    last: Abbet
  - first: Meryem
    full: "Meryem M\u2019hamdi"
    id: meryem-mhamdi
    last: "M\u2019hamdi"
  - first: Athanasios
    full: Athanasios Giannakopoulos
    id: athanasios-giannakopoulos
    last: Giannakopoulos
  - first: Robert
    full: Robert West
    id: robert-west
    last: West
  - first: Andreea
    full: Andreea Hossmann
    id: andreea-hossmann
    last: Hossmann
  - first: Michael
    full: Michael Baeriswyl
    id: michael-baeriswyl
    last: Baeriswyl
  - first: Claudiu
    full: Claudiu Musat
    id: claudiu-musat
    last: Musat
  author_string: "Christian Abbet, Meryem M\u2019hamdi, Athanasios Giannakopoulos,\
    \ Robert West, Andreea Hossmann, Michael Baeriswyl, Claudiu Musat"
  bibkey: abbet-etal-2018-churn
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1016
  month: October
  page_first: '161'
  page_last: '170'
  pages: "161\u2013170"
  paper_id: '16'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1016.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1016.jpg
  title: Churn Intent Detection in Multilingual Chatbot Conversations and Social Media
  title_html: Churn Intent Detection in Multilingual Chatbot Conversations and Social
    Media
  url: https://www.aclweb.org/anthology/K18-1016
  year: '2018'
K18-1017:
  abstract: Named Entity Disambiguation algorithms typically learn a single model
    for all target entities. In this paper we present a word expert model and train
    separate deep learning models for each target entity string, yielding 500K classification
    tasks. This gives us the opportunity to benchmark popular text representation
    alternatives on this massive dataset. In order to face scarce training data we
    propose a simple data-augmentation technique and transfer-learning. We show that
    bag-of-word-embeddings are better than LSTMs for tasks with scarce training data,
    while the situation is reversed when having larger amounts. Transferring a LSTM
    which is learned on all datasets is the most effective context representation
    option for the word experts in all frequency bands. The experiments show that
    our system trained on out-of-domain Wikipedia data surpass comparable NED systems
    which have been trained on in-domain training data.
  address: Brussels, Belgium
  author:
  - first: Ander
    full: Ander Barrena
    id: ander-barrena
    last: Barrena
  - first: Aitor
    full: Aitor Soroa
    id: aitor-soroa
    last: Soroa
  - first: Eneko
    full: Eneko Agirre
    id: eneko-agirre
    last: Agirre
  author_string: Ander Barrena, Aitor Soroa, Eneko Agirre
  bibkey: barrena-etal-2018-learning
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1017
  month: October
  page_first: '171'
  page_last: '180'
  pages: "171\u2013180"
  paper_id: '17'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1017.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1017.jpg
  title: Learning Text Representations for 500K Classification Tasks on Named Entity
    Disambiguation
  title_html: Learning Text Representations for 500<span class="acl-fixed-case">K</span>
    Classification Tasks on Named Entity Disambiguation
  url: https://www.aclweb.org/anthology/K18-1017
  year: '2018'
K18-1018:
  abstract: Aspect-level sentiment analysis aims to identify the sentiment of a specific
    target in its context. Previous works have proved that the interactions between
    aspects and the contexts are important. On this basis, we also propose a succinct
    hierarchical attention based mechanism to fuse the information of targets and
    the contextual words. In addition, most existing methods ignore the position information
    of the aspect when encoding the sentence. In this paper, we argue that the position-aware
    representations are beneficial to this task. Therefore, we propose a hierarchical
    attention based position-aware network (HAPN), which introduces position embeddings
    to learn the position-aware representations of sentences and further generate
    the target-specific representations of contextual words. The experimental results
    on SemEval 2014 dataset show that our approach outperforms the state-of-the-art
    methods.
  address: Brussels, Belgium
  author:
  - first: Lishuang
    full: Lishuang Li
    id: lishuang-li
    last: Li
  - first: Yang
    full: Yang Liu
    id: yang-liu
    last: Liu
  - first: AnQiao
    full: AnQiao Zhou
    id: anqiao-zhou
    last: Zhou
  author_string: Lishuang Li, Yang Liu, AnQiao Zhou
  bibkey: li-etal-2018-hierarchical
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1018
  month: October
  page_first: '181'
  page_last: '189'
  pages: "181\u2013189"
  paper_id: '18'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1018.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1018.jpg
  title: Hierarchical Attention Based Position-Aware Network for Aspect-Level Sentiment
    Analysis
  title_html: Hierarchical Attention Based Position-Aware Network for Aspect-Level
    Sentiment Analysis
  url: https://www.aclweb.org/anthology/K18-1018
  year: '2018'
K18-1019:
  abstract: 'Generative Adversarial Network (GAN) has been proposed to tackle the
    exposure bias problem of Neural Machine Translation (NMT). However, the discriminator
    typically results in the instability of the GAN training due to the inadequate
    training problem: the search space is so huge that sampled translations are not
    sufficient for discriminator training. To address this issue and stabilize the
    GAN training, in this paper, we propose a novel Bidirectional Generative Adversarial
    Network for Neural Machine Translation (BGAN-NMT), which aims to introduce a generator
    model to act as the discriminator, whereby the discriminator naturally considers
    the entire translation space so that the inadequate training problem can be alleviated.
    To satisfy this property, generator and discriminator are both designed to model
    the joint probability of sentence pairs, with the difference that, the generator
    decomposes the joint probability with a source language model and a source-to-target
    translation model, while the discriminator is formulated as a target language
    model and a target-to-source translation model. To further leverage the symmetry
    of them, an auxiliary GAN is introduced and adopts generator and discriminator
    models of original one as its own discriminator and generator respectively. Two
    GANs are alternately trained to update the parameters. Experiment results on German-English
    and Chinese-English translation tasks demonstrate that our method not only stabilizes
    GAN training but also achieves significant improvements over baseline systems.'
  address: Brussels, Belgium
  author:
  - first: Zhirui
    full: Zhirui Zhang
    id: zhirui-zhang
    last: Zhang
  - first: Shujie
    full: Shujie Liu
    id: shujie-liu
    last: Liu
  - first: Mu
    full: Mu Li
    id: mu-li
    last: Li
  - first: Ming
    full: Ming Zhou
    id: ming-zhou
    last: Zhou
  - first: Enhong
    full: Enhong Chen
    id: enhong-chen
    last: Chen
  author_string: Zhirui Zhang, Shujie Liu, Mu Li, Ming Zhou, Enhong Chen
  bibkey: zhang-etal-2018-bidirectional
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1019
  month: October
  page_first: '190'
  page_last: '199'
  pages: "190\u2013199"
  paper_id: '19'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1019.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1019.jpg
  title: Bidirectional Generative Adversarial Networks for Neural Machine Translation
  title_html: Bidirectional Generative Adversarial Networks for Neural Machine Translation
  url: https://www.aclweb.org/anthology/K18-1019
  year: '2018'
K18-1020:
  abstract: "Named-entity Recognition (NER) is an important task in the NLP field\
    \ , and is widely used to solve many challenges. However, in many scenarios, not\
    \ all of the entities are explicitly mentioned in the text. Sometimes they could\
    \ be inferred from the context or from other indicative words. Consider the following\
    \ sentence: \u201CCMA can easily hydrolyze into free acetic acid.\u201D Although\
    \ water is not mentioned explicitly, one can infer that H2O is an entity involved\
    \ in the process. In this work, we present the problem of Latent Entities Extraction\
    \ (LEE). We present several methods for determining whether entities are discussed\
    \ in a text, even though, potentially, they are not explicitly written. Specifically,\
    \ we design a neural model that handles extraction of multiple entities jointly.\
    \ We show that our model, along with multi-task learning approach and a novel\
    \ task grouping algorithm, reaches high performance in identifying latent entities.\
    \ Our experiments are conducted on a large biological dataset from the biochemical\
    \ field. The dataset contains text descriptions of biological processes, and for\
    \ each process, all of the involved entities in the process are labeled, including\
    \ implicitly mentioned ones. We believe LEE is a task that will significantly\
    \ improve many NER and subsequent applications and improve text understanding\
    \ and inference."
  address: Brussels, Belgium
  author:
  - first: Eylon
    full: Eylon Shoshan
    id: eylon-shoshan
    last: Shoshan
  - first: Kira
    full: Kira Radinsky
    id: kira-radinsky
    last: Radinsky
  author_string: Eylon Shoshan, Kira Radinsky
  bibkey: shoshan-radinsky-2018-latent
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1020
  month: October
  page_first: '200'
  page_last: '210'
  pages: "200\u2013210"
  paper_id: '20'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1020.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1020.jpg
  title: 'Latent Entities Extraction: How to Extract Entities that Do Not Appear in
    the Text?'
  title_html: 'Latent Entities Extraction: How to Extract Entities that Do Not Appear
    in the Text?'
  url: https://www.aclweb.org/anthology/K18-1020
  year: '2018'
K18-1021:
  abstract: Most recent approaches to bilingual dictionary induction find a linear
    alignment between the word vector spaces of two languages. We show that projecting
    the two languages onto a third, latent space, rather than directly onto each other,
    while equivalent in terms of expressivity, makes it easier to learn approximate
    alignments. Our modified approach also allows for supporting languages to be included
    in the alignment process, to obtain an even better performance in low resource
    settings.
  address: Brussels, Belgium
  author:
  - first: Yova
    full: Yova Kementchedjhieva
    id: yova-kementchedjhieva
    last: Kementchedjhieva
  - first: Sebastian
    full: Sebastian Ruder
    id: sebastian-ruder
    last: Ruder
  - first: Ryan
    full: Ryan Cotterell
    id: ryan-cotterell
    last: Cotterell
  - first: Anders
    full: "Anders S\xF8gaard"
    id: anders-sogaard
    last: "S\xF8gaard"
  author_string: "Yova Kementchedjhieva, Sebastian Ruder, Ryan Cotterell, Anders S\xF8\
    gaard"
  bibkey: kementchedjhieva-etal-2018-generalizing
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1021
  month: October
  page_first: '211'
  page_last: '220'
  pages: "211\u2013220"
  paper_id: '21'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1021.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1021.jpg
  title: Generalizing Procrustes Analysis for Better Bilingual Dictionary Induction
  title_html: Generalizing <span class="acl-fixed-case">P</span>rocrustes Analysis
    for Better Bilingual Dictionary Induction
  url: https://www.aclweb.org/anthology/K18-1021
  year: '2018'
K18-1022:
  abstract: "Keyphrase extraction is the task of automatically selecting a small set\
    \ of phrases that best describe a given free text document. Supervised keyphrase\
    \ extraction requires large amounts of labeled training data and generalizes very\
    \ poorly outside the domain of the training data. At the same time, unsupervised\
    \ systems have poor accuracy, and often do not generalize well, as they require\
    \ the input document to belong to a larger corpus also given as input. Addressing\
    \ these drawbacks, in this paper, we tackle keyphrase extraction from single documents\
    \ with EmbedRank: a novel unsupervised method, that leverages sentence embeddings.\
    \ EmbedRank achieves higher F-scores than graph-based state of the art systems\
    \ on standard datasets and is suitable for real-time processing of large amounts\
    \ of Web data. With EmbedRank, we also explicitly increase coverage and diversity\
    \ among the selected keyphrases by introducing an embedding-based maximal marginal\
    \ relevance (MMR) for new phrases. A user study including over 200 votes showed\
    \ that, although reducing the phrases\u2019 semantic overlap leads to no gains\
    \ in F-score, our high diversity selection is preferred by humans."
  address: Brussels, Belgium
  author:
  - first: Kamil
    full: Kamil Bennani-Smires
    id: kamil-bennani-smires
    last: Bennani-Smires
  - first: Claudiu
    full: Claudiu Musat
    id: claudiu-musat
    last: Musat
  - first: Andreea
    full: Andreea Hossmann
    id: andreea-hossmann
    last: Hossmann
  - first: Michael
    full: Michael Baeriswyl
    id: michael-baeriswyl
    last: Baeriswyl
  - first: Martin
    full: Martin Jaggi
    id: martin-jaggi
    last: Jaggi
  author_string: Kamil Bennani-Smires, Claudiu Musat, Andreea Hossmann, Michael Baeriswyl,
    Martin Jaggi
  bibkey: bennani-smires-etal-2018-simple
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1022
  month: October
  page_first: '221'
  page_last: '229'
  pages: "221\u2013229"
  paper_id: '22'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1022.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1022.jpg
  title: Simple Unsupervised Keyphrase Extraction using Sentence Embeddings
  title_html: Simple Unsupervised Keyphrase Extraction using Sentence Embeddings
  url: https://www.aclweb.org/anthology/K18-1022
  year: '2018'
K18-1023:
  abstract: Timeline summarization (TLS) creates an overview of long-running events
    via dated daily summaries for the most important dates. TLS differs from standard
    multi-document summarization (MDS) in the importance of date selection, interdependencies
    between summaries of different dates and by having very short summaries compared
    to the number of corpus documents. However, we show that MDS optimization models
    using submodular functions can be adapted to yield well-performing TLS models
    by designing objective functions and constraints that model the temporal dimension
    inherent in TLS. Importantly, these adaptations retain the elegance and advantages
    of the original MDS models (clear separation of features and inference, performance
    guarantees and scalability, little need for supervision) that current TLS-specific
    models lack.
  address: Brussels, Belgium
  author:
  - first: Sebastian
    full: Sebastian Martschat
    id: sebastian-martschat
    last: Martschat
  - first: Katja
    full: Katja Markert
    id: katja-markert
    last: Markert
  author_string: Sebastian Martschat, Katja Markert
  bibkey: martschat-markert-2018-temporally
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1023
  month: October
  page_first: '230'
  page_last: '240'
  pages: "230\u2013240"
  paper_id: '23'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1023.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1023.jpg
  title: A Temporally Sensitive Submodularity Framework for Timeline Summarization
  title_html: A Temporally Sensitive Submodularity Framework for Timeline Summarization
  url: https://www.aclweb.org/anthology/K18-1023
  year: '2018'
K18-1024:
  abstract: As a precious part of the human cultural heritage, Chinese poetry has
    influenced people for generations. Automatic poetry composition is a challenge
    for AI. In recent years, significant progress has been made in this area benefiting
    from the development of neural networks. However, the coherence in meaning, theme
    or even artistic conception for a generated poem as a whole still remains a big
    problem. In this paper, we propose a novel Salient-Clue mechanism for Chinese
    poetry generation. Different from previous work which tried to exploit all the
    context information, our model selects the most salient characters automatically
    from each so-far generated line to gradually form a salient clue, which is utilized
    to guide successive poem generation process so as to eliminate interruptions and
    improve coherence. Besides, our model can be flexibly extended to control the
    generated poem in different aspects, for example, poetry style, which further
    enhances the coherence. Experimental results show that our model is very effective,
    outperforming three strong baselines.
  address: Brussels, Belgium
  author:
  - first: Xiaoyuan
    full: Xiaoyuan Yi
    id: xiaoyuan-yi
    last: Yi
  - first: Ruoyu
    full: Ruoyu Li
    id: ruoyu-li
    last: Li
  - first: Maosong
    full: Maosong Sun
    id: maosong-sun
    last: Sun
  author_string: Xiaoyuan Yi, Ruoyu Li, Maosong Sun
  bibkey: yi-etal-2018-chinese
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1024
  month: October
  page_first: '241'
  page_last: '250'
  pages: "241\u2013250"
  paper_id: '24'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1024.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1024.jpg
  title: Chinese Poetry Generation with a Salient-Clue Mechanism
  title_html: <span class="acl-fixed-case">C</span>hinese Poetry Generation with a
    Salient-Clue Mechanism
  url: https://www.aclweb.org/anthology/K18-1024
  year: '2018'
K18-1025:
  abstract: Natural human communication is nuanced and inherently multi-modal. Humans
    possess specialised sensoria for processing vocal, visual, and linguistic, and
    para-linguistic information, but form an intricately fused percept of the multi-modal
    data stream to provide a holistic representation. Analysis of emotional content
    in face-to-face communication is a cognitive task to which humans are particularly
    attuned, given its sociological importance, and poses a difficult challenge for
    machine emulation due to the subtlety and expressive variability of cross-modal
    cues. Inspired by the empirical success of recent so-called End-To-End Memory
    Networks and related works, we propose an approach based on recursive multi-attention
    with a shared external memory updated over multiple gated iterations of analysis.
    We evaluate our model across several large multi-modal datasets and show that
    global contextualised memory with gated memory update can effectively achieve
    emotion recognition.
  address: Brussels, Belgium
  author:
  - first: Rory
    full: Rory Beard
    id: rory-beard
    last: Beard
  - first: Ritwik
    full: Ritwik Das
    id: ritwik-das
    last: Das
  - first: Raymond W. M.
    full: Raymond W. M. Ng
    id: raymond-w-m-ng
    last: Ng
  - first: P. G. Keerthana
    full: P. G. Keerthana Gopalakrishnan
    id: p-g-keerthana-gopalakrishnan
    last: Gopalakrishnan
  - first: Luka
    full: Luka Eerens
    id: luka-eerens
    last: Eerens
  - first: Pawel
    full: Pawel Swietojanski
    id: pawel-swietojanski
    last: Swietojanski
  - first: Ondrej
    full: Ondrej Miksik
    id: ondrej-miksik
    last: Miksik
  author_string: Rory Beard, Ritwik Das, Raymond W. M. Ng, P. G. Keerthana Gopalakrishnan,
    Luka Eerens, Pawel Swietojanski, Ondrej Miksik
  bibkey: beard-etal-2018-multi
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1025
  month: October
  page_first: '251'
  page_last: '259'
  pages: "251\u2013259"
  paper_id: '25'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1025.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1025.jpg
  title: Multi-Modal Sequence Fusion via Recursive Attention for Emotion Recognition
  title_html: Multi-Modal Sequence Fusion via Recursive Attention for Emotion Recognition
  url: https://www.aclweb.org/anthology/K18-1025
  year: '2018'
K18-1026:
  abstract: Distributional models provide a convenient way to model semantics using
    dense embedding spaces derived from unsupervised learning algorithms. However,
    the dimensions of dense embedding spaces are not designed to resemble human semantic
    knowledge. Moreover, embeddings are often built from a single source of information
    (typically text data), even though neurocognitive research suggests that semantics
    is deeply linked to both language and perception. In this paper, we combine multimodal
    information from both text and image-based representations derived from state-of-the-art
    distributional models to produce sparse, interpretable vectors using Joint Non-Negative
    Sparse Embedding. Through in-depth analyses comparing these sparse models to human-derived
    behavioural and neuroimaging data, we demonstrate their ability to predict interpretable
    linguistic descriptions of human ground-truth semantic knowledge.
  address: Brussels, Belgium
  author:
  - first: Steven
    full: Steven Derby
    id: steven-derby
    last: Derby
  - first: Paul
    full: Paul Miller
    id: paul-miller
    last: Miller
  - first: Brian
    full: Brian Murphy
    id: brian-murphy
    last: Murphy
  - first: Barry
    full: Barry Devereux
    id: barry-devereux
    last: Devereux
  author_string: Steven Derby, Paul Miller, Brian Murphy, Barry Devereux
  bibkey: derby-etal-2018-using
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1026
  month: October
  page_first: '260'
  page_last: '270'
  pages: "260\u2013270"
  paper_id: '26'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1026.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1026.jpg
  title: Using Sparse Semantic Embeddings Learned from Multimodal Text and Image Data
    to Model Human Conceptual Knowledge
  title_html: Using Sparse Semantic Embeddings Learned from Multimodal Text and Image
    Data to Model Human Conceptual Knowledge
  url: https://www.aclweb.org/anthology/K18-1026
  year: '2018'
K18-1027:
  abstract: We present and evaluate two similarity dependent Chinese Restaurant Process
    (sd-CRP) algorithms at the task of automated cognate detection. The sd-CRP clustering
    algorithms do not require any predefined threshold for detecting cognate sets
    in a multilingual word list. We evaluate the performance of the algorithms on
    six language families (more than 750 languages) and find that both the sd-CRP
    variants performs as well as InfoMap and better than UPGMA at the task of inferring
    cognate clusters. The algorithms presented in this paper are family agnostic and
    can be applied to any linguistically under-studied language family.
  address: Brussels, Belgium
  author:
  - first: Taraka
    full: Taraka Rama
    id: taraka-rama
    last: Rama
  author_string: Taraka Rama
  bibkey: rama-2018-similarity
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1027
  month: October
  page_first: '271'
  page_last: '281'
  pages: "271\u2013281"
  paper_id: '27'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1027.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1027.jpg
  title: Similarity Dependent Chinese Restaurant Process for Cognate Identification
    in Multilingual Wordlists
  title_html: Similarity Dependent <span class="acl-fixed-case">C</span>hinese Restaurant
    Process for Cognate Identification in Multilingual Wordlists
  url: https://www.aclweb.org/anthology/K18-1027
  year: '2018'
K18-1028:
  abstract: Following the recent success of word embeddings, it has been argued that
    there is no such thing as an ideal representation for words, as different models
    tend to capture divergent and often mutually incompatible aspects like semantics/syntax
    and similarity/relatedness. In this paper, we show that each embedding model captures
    more information than directly apparent. A linear transformation that adjusts
    the similarity order of the model without any external resource can tailor it
    to achieve better results in those aspects, providing a new perspective on how
    embeddings encode divergent linguistic information. In addition, we explore the
    relation between intrinsic and extrinsic evaluation, as the effect of our transformations
    in downstream tasks is higher for unsupervised systems than for supervised ones.
  address: Brussels, Belgium
  author:
  - first: Mikel
    full: Mikel Artetxe
    id: mikel-artetxe
    last: Artetxe
  - first: Gorka
    full: Gorka Labaka
    id: gorka-labaka
    last: Labaka
  - first: "I\xF1igo"
    full: "I\xF1igo Lopez-Gazpio"
    id: inigo-lopez-gazpio
    last: Lopez-Gazpio
  - first: Eneko
    full: Eneko Agirre
    id: eneko-agirre
    last: Agirre
  author_string: "Mikel Artetxe, Gorka Labaka, I\xF1igo Lopez-Gazpio, Eneko Agirre"
  bibkey: artetxe-etal-2018-uncovering
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1028
  month: October
  page_first: '282'
  page_last: '291'
  pages: "282\u2013291"
  paper_id: '28'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1028.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1028.jpg
  title: Uncovering Divergent Linguistic Information in Word Embeddings with Lessons
    for Intrinsic and Extrinsic Evaluation
  title_html: Uncovering Divergent Linguistic Information in Word Embeddings with
    Lessons for Intrinsic and Extrinsic Evaluation
  url: https://www.aclweb.org/anthology/K18-1028
  year: '2018'
K18-1029:
  abstract: "Simple reference games are of central theoretical and empirical importance\
    \ in the study of situated language use. Although language provides rich, compositional\
    \ truth-conditional semantics to facilitate reference, speakers and listeners\
    \ may sometimes lack the overall lexical and cognitive resources to guarantee\
    \ successful reference through these means alone. However, language also has rich\
    \ associational structures that can serve as a further resource for achieving\
    \ successful reference. Here we investigate this use of associational information\
    \ in a setting where only associational information is available: a simplified\
    \ version of the popular game Codenames. Using optimal experiment design techniques,\
    \ we compare a range of models varying in the type of associative information\
    \ deployed and in level of pragmatic sophistication against human behavior. In\
    \ this setting we find that listeners\u2019 behavior reflects direct bigram collocational\
    \ associations more strongly than word-embedding or semantic knowledge graph-based\
    \ associations and that there is little evidence for pragmatically sophisticated\
    \ behavior on the part of either speakers or listeners. More generally, we demonstrate\
    \ the effective use of simple tasks to derive insights into the nature of complex\
    \ linguistic phenomena."
  address: Brussels, Belgium
  author:
  - first: Judy Hanwen
    full: Judy Hanwen Shen
    id: judy-hanwen-shen
    last: Shen
  - first: Matthias
    full: Matthias Hofer
    id: matthias-hofer
    last: Hofer
  - first: Bjarke
    full: Bjarke Felbo
    id: bjarke-felbo
    last: Felbo
  - first: Roger
    full: Roger Levy
    id: roger-levy
    last: Levy
  author_string: Judy Hanwen Shen, Matthias Hofer, Bjarke Felbo, Roger Levy
  bibkey: shen-etal-2018-comparing
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1029
  month: October
  page_first: '292'
  page_last: '301'
  pages: "292\u2013301"
  paper_id: '29'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1029.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1029.jpg
  title: 'Comparing Models of Associative Meaning: An Empirical Investigation of Reference
    in Simple Language Games'
  title_html: 'Comparing Models of Associative Meaning: An Empirical Investigation
    of Reference in Simple Language Games'
  url: https://www.aclweb.org/anthology/K18-1029
  year: '2018'
K18-1030:
  abstract: Learning attention functions requires large volumes of data, but many
    NLP tasks simulate human behavior, and in this paper, we show that human attention
    really does provide a good inductive bias on many attention functions in NLP.
    Specifically, we use estimated human attention derived from eye-tracking corpora
    to regularize attention functions in recurrent neural networks. We show substantial
    improvements across a range of tasks, including sentiment analysis, grammatical
    error detection, and detection of abusive language.
  address: Brussels, Belgium
  author:
  - first: Maria
    full: Maria Barrett
    id: maria-barrett
    last: Barrett
  - first: Joachim
    full: Joachim Bingel
    id: joachim-bingel
    last: Bingel
  - first: Nora
    full: Nora Hollenstein
    id: nora-hollenstein
    last: Hollenstein
  - first: Marek
    full: Marek Rei
    id: marek-rei
    last: Rei
  - first: Anders
    full: "Anders S\xF8gaard"
    id: anders-sogaard
    last: "S\xF8gaard"
  author_string: "Maria Barrett, Joachim Bingel, Nora Hollenstein, Marek Rei, Anders\
    \ S\xF8gaard"
  bibkey: barrett-etal-2018-sequence
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1030
  month: October
  page_first: '302'
  page_last: '312'
  pages: "302\u2013312"
  paper_id: '30'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1030.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1030.jpg
  title: Sequence Classification with Human Attention
  title_html: Sequence Classification with Human Attention
  url: https://www.aclweb.org/anthology/K18-1030
  year: '2018'
K18-1031:
  abstract: Motivated by recent findings on the probabilistic modeling of acceptability
    judgments, we propose syntactic log-odds ratio (SLOR), a normalized language model
    score, as a metric for referenceless fluency evaluation of natural language generation
    output at the sentence level. We further introduce WPSLOR, a novel WordPiece-based
    version, which harnesses a more compact language model. Even though word-overlap
    metrics like ROUGE are computed with the help of hand-written references, our
    referenceless methods obtain a significantly higher correlation with human fluency
    scores on a benchmark dataset of compressed sentences. Finally, we present ROUGE-LM,
    a reference-based metric which is a natural extension of WPSLOR to the case of
    available references. We show that ROUGE-LM yields a significantly higher correlation
    with human judgments than all baseline metrics, including WPSLOR on its own.
  address: Brussels, Belgium
  author:
  - first: Katharina
    full: Katharina Kann
    id: katharina-kann
    last: Kann
  - first: Sascha
    full: Sascha Rothe
    id: sascha-rothe
    last: Rothe
  - first: Katja
    full: Katja Filippova
    id: katja-filippova
    last: Filippova
  author_string: Katharina Kann, Sascha Rothe, Katja Filippova
  bibkey: kann-etal-2018-sentence
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1031
  month: October
  page_first: '313'
  page_last: '323'
  pages: "313\u2013323"
  paper_id: '31'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1031.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1031.jpg
  title: 'Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!'
  title_html: 'Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!'
  url: https://www.aclweb.org/anthology/K18-1031
  year: '2018'
K18-1032:
  abstract: Inducing sparseness while training neural networks has been shown to yield
    models with a lower memory footprint but similar effectiveness to dense models.
    However, sparseness is typically induced starting from a dense model, and thus
    this advantage does not hold during training. We propose techniques to enforce
    sparseness upfront in recurrent sequence models for NLP applications, to also
    benefit training. First, in language modeling, we show how to increase hidden
    state sizes in recurrent layers without increasing the number of parameters, leading
    to more expressive models. Second, for sequence labeling, we show that word embeddings
    with predefined sparseness lead to similar performance as dense embeddings, at
    a fraction of the number of trainable parameters.
  address: Brussels, Belgium
  author:
  - first: Thomas
    full: Thomas Demeester
    id: thomas-demeester
    last: Demeester
  - first: Johannes
    full: Johannes Deleu
    id: johannes-deleu
    last: Deleu
  - first: "Fr\xE9deric"
    full: "Fr\xE9deric Godin"
    id: frederic-godin
    last: Godin
  - first: Chris
    full: Chris Develder
    id: chris-develder
    last: Develder
  author_string: "Thomas Demeester, Johannes Deleu, Fr\xE9deric Godin, Chris Develder"
  bibkey: demeester-etal-2018-predefined
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1032
  month: October
  page_first: '324'
  page_last: '333'
  pages: "324\u2013333"
  paper_id: '32'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1032.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1032.jpg
  title: Predefined Sparseness in Recurrent Sequence Models
  title_html: Predefined Sparseness in Recurrent Sequence Models
  url: https://www.aclweb.org/anthology/K18-1032
  year: '2018'
K18-1033:
  abstract: Traditional active learning (AL) methods for machine translation (MT)
    rely on heuristics. However, these heuristics are limited when the characteristics
    of the MT problem change due to e.g. the language pair or the amount of the initial
    bitext. In this paper, we present a framework to learn sentence selection strategies
    for neural MT. We train the AL query strategy using a high-resource language-pair
    based on AL simulations, and then transfer it to the low-resource language-pair
    of interest. The learned query strategy capitalizes on the shared characteristics
    between the language pairs to make an effective use of the AL budget. Our experiments
    on three language-pairs confirms that our method is more effective than strong
    heuristic-based methods in various conditions, including cold-start and warm-start
    as well as small and extremely small data conditions.
  address: Brussels, Belgium
  author:
  - first: Ming
    full: Ming Liu
    id: ming-liu
    last: Liu
  - first: Wray
    full: Wray Buntine
    id: wray-buntine
    last: Buntine
  - first: Gholamreza
    full: Gholamreza Haffari
    id: gholamreza-haffari
    last: Haffari
  author_string: Ming Liu, Wray Buntine, Gholamreza Haffari
  bibkey: liu-etal-2018-learning-actively
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1033
  month: October
  page_first: '334'
  page_last: '344'
  pages: "334\u2013344"
  paper_id: '33'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1033.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1033.jpg
  title: Learning to Actively Learn Neural Machine Translation
  title_html: Learning to Actively Learn Neural Machine Translation
  url: https://www.aclweb.org/anthology/K18-1033
  year: '2018'
K18-1034:
  abstract: We propose a post-OCR text correction approach for digitising texts in
    Romanised Sanskrit. Owing to the lack of resources our approach uses OCR models
    trained for other languages written in Roman. Currently, there exists no dataset
    available for Romanised Sanskrit OCR. So, we bootstrap a dataset of 430 images,
    scanned in two different settings and their corresponding ground truth. For training,
    we synthetically generate training images for both the settings. We find that
    the use of copying mechanism (Gu et al., 2016) yields a percentage increase of
    7.69 in Character Recognition Rate (CRR) than the current state of the art model
    in solving monotone sequence-to-sequence tasks (Schnober et al., 2016). We find
    that our system is robust in combating OCR-prone errors, as it obtains a CRR of
    87.01% from an OCR output with CRR of 35.76% for one of the dataset settings.
    A human judgement survey performed on the models shows that our proposed model
    results in predictions which are faster to comprehend and faster to improve for
    a human than the other systems.
  address: Brussels, Belgium
  author:
  - first: Amrith
    full: Amrith Krishna
    id: amrith-krishna
    last: Krishna
  - first: Bodhisattwa P.
    full: Bodhisattwa P. Majumder
    id: bodhisattwa-p-majumder
    last: Majumder
  - first: Rajesh
    full: Rajesh Bhat
    id: rajesh-bhat
    last: Bhat
  - first: Pawan
    full: Pawan Goyal
    id: pawan-goyal
    last: Goyal
  author_string: Amrith Krishna, Bodhisattwa P. Majumder, Rajesh Bhat, Pawan Goyal
  bibkey: krishna-etal-2018-upcycle
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1034
  month: October
  page_first: '345'
  page_last: '355'
  pages: "345\u2013355"
  paper_id: '34'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1034.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1034.jpg
  title: 'Upcycle Your OCR: Reusing OCRs for Post-OCR Text Correction in Romanised
    Sanskrit'
  title_html: 'Upcycle Your <span class="acl-fixed-case">OCR</span>: Reusing <span
    class="acl-fixed-case">OCR</span>s for Post-<span class="acl-fixed-case">OCR</span>
    Text Correction in <span class="acl-fixed-case">R</span>omanised <span class="acl-fixed-case">S</span>anskrit'
  url: https://www.aclweb.org/anthology/K18-1034
  year: '2018'
K18-1035:
  abstract: 'Weakly-supervised semantic parsers are trained on utterance-denotation
    pairs, treating logical forms as latent. The task is challenging due to the large
    search space and spuriousness of logical forms. In this paper we introduce a neural
    parser-ranker system for weakly-supervised semantic parsing. The parser generates
    candidate tree-structured logical forms from utterances using clues of denotations.
    These candidates are then ranked based on two criterion: their likelihood of executing
    to the correct denotation, and their agreement with the utterance semantics. We
    present a scheduled training procedure to balance the contribution of the two
    objectives. Furthermore, we propose to use a neurally encoded lexicon to inject
    prior domain knowledge to the model. Experiments on three Freebase datasets demonstrate
    the effectiveness of our semantic parser, achieving results within the state-of-the-art
    range.'
  address: Brussels, Belgium
  author:
  - first: Jianpeng
    full: Jianpeng Cheng
    id: jianpeng-cheng
    last: Cheng
  - first: Mirella
    full: Mirella Lapata
    id: mirella-lapata
    last: Lapata
  author_string: Jianpeng Cheng, Mirella Lapata
  bibkey: cheng-lapata-2018-weakly
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1035
  month: October
  page_first: '356'
  page_last: '367'
  pages: "356\u2013367"
  paper_id: '35'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1035.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1035.jpg
  title: Weakly-Supervised Neural Semantic Parsing with a Generative Ranker
  title_html: Weakly-Supervised Neural Semantic Parsing with a Generative Ranker
  url: https://www.aclweb.org/anthology/K18-1035
  year: '2018'
K18-1036:
  abstract: Neural morphological tagging has been regarded as an extension to POS
    tagging task, treating each morphological tag as a monolithic label and ignoring
    its internal structure. We propose to view morphological tags as composite labels
    and explicitly model their internal structure in a neural sequence tagger. For
    this, we explore three different neural architectures and compare their performance
    with both CRF and simple neural multiclass baselines. We evaluate our models on
    49 languages and show that the neural architecture that models the morphological
    labels as sequences of morphological category values performs significantly better
    than both baselines establishing state-of-the-art results in morphological tagging
    for most languages.
  address: Brussels, Belgium
  author:
  - first: Alexander
    full: Alexander Tkachenko
    id: alexander-tkachenko
    last: Tkachenko
  - first: Kairit
    full: Kairit Sirts
    id: kairit-sirts
    last: Sirts
  author_string: Alexander Tkachenko, Kairit Sirts
  bibkey: tkachenko-sirts-2018-modeling
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1036
  month: October
  page_first: '368'
  page_last: '379'
  pages: "368\u2013379"
  paper_id: '36'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1036.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1036.jpg
  title: Modeling Composite Labels for Neural Morphological Tagging
  title_html: Modeling Composite Labels for Neural Morphological Tagging
  url: https://www.aclweb.org/anthology/K18-1036
  year: '2018'
K18-1037:
  abstract: Classification tasks are usually analysed and improved through new model
    architectures or hyperparameter optimisation but the underlying properties of
    datasets are discovered on an ad-hoc basis as errors occur. However, understanding
    the properties of the data is crucial in perfecting models. In this paper we analyse
    exactly which characteristics of a dataset best determine how difficult that dataset
    is for the task of text classification. We then propose an intuitive measure of
    difficulty for text classification datasets which is simple and fast to calculate.
    We empirically prove that this measure generalises to unseen data by comparing
    it to state-of-the-art datasets and results. This measure can be used to analyse
    the precise source of errors in a dataset and allows fast estimation of how difficult
    a dataset is to learn. We searched for this measure by training 12 classical and
    neural network based models on 78 real-world datasets, then use a genetic algorithm
    to discover the best measure of difficulty. Our difficulty-calculating code and
    datasets are publicly available.
  address: Brussels, Belgium
  author:
  - first: Edward
    full: Edward Collins
    id: edward-collins
    last: Collins
  - first: Nikolai
    full: Nikolai Rozanov
    id: nikolai-rozanov
    last: Rozanov
  - first: Bingbing
    full: Bingbing Zhang
    id: bingbing-zhang
    last: Zhang
  author_string: Edward Collins, Nikolai Rozanov, Bingbing Zhang
  bibkey: collins-etal-2018-evolutionary
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1037
  month: October
  page_first: '380'
  page_last: '391'
  pages: "380\u2013391"
  paper_id: '37'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1037.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1037.jpg
  title: 'Evolutionary Data Measures: Understanding the Difficulty of Text Classification
    Tasks'
  title_html: 'Evolutionary Data Measures: Understanding the Difficulty of Text Classification
    Tasks'
  url: https://www.aclweb.org/anthology/K18-1037
  year: '2018'
K18-1038:
  abstract: 'Despite their practical success and impressive performances, neural-network-based
    and distributed semantics techniques have often been criticized as they remain
    fundamentally opaque and difficult to interpret. In a vein similar to recent pieces
    of work investigating the linguistic abilities of these representations, we study
    another core, defining property of language: the property of long-distance dependencies.
    Human languages exhibit the ability to interpret discontinuous elements distant
    from each other in the string as if they were adjacent. This ability is blocked
    if a similar, but extraneous, element intervenes between the discontinuous components.
    We present results that show, under exhaustive and precise conditions, that one
    kind of word embeddings and the similarity spaces they define do not encode the
    properties of intervention similarity in long-distance dependencies, and that
    therefore they fail to represent this core linguistic notion.'
  address: Brussels, Belgium
  author:
  - first: Paola
    full: Paola Merlo
    id: paola-merlo
    last: Merlo
  - first: Francesco
    full: Francesco Ackermann
    id: francesco-ackermann
    last: Ackermann
  author_string: Paola Merlo, Francesco Ackermann
  bibkey: merlo-ackermann-2018-vectorial
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1038
  month: October
  page_first: '392'
  page_last: '401'
  pages: "392\u2013401"
  paper_id: '38'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1038.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1038.jpg
  title: Vectorial Semantic Spaces Do Not Encode Human Judgments of Intervention Similarity
  title_html: Vectorial Semantic Spaces Do Not Encode Human Judgments of Intervention
    Similarity
  url: https://www.aclweb.org/anthology/K18-1038
  year: '2018'
K18-1039:
  abstract: Recent work has shown how to learn better visual-semantic embeddings by
    leveraging image descriptions in more than one language. Here, we investigate
    in detail which conditions affect the performance of this type of grounded language
    learning model. We show that multilingual training improves over bilingual training,
    and that low-resource languages benefit from training with higher-resource languages.
    We demonstrate that a multilingual model can be trained equally well on either
    translations or comparable sentence pairs, and that annotating the same set of
    images in multiple language enables further improvements via an additional caption-caption
    ranking objective.
  address: Brussels, Belgium
  author:
  - first: "\xC1kos"
    full: "\xC1kos K\xE1d\xE1r"
    id: akos-kadar
    last: "K\xE1d\xE1r"
  - first: Desmond
    full: Desmond Elliott
    id: desmond-elliott
    last: Elliott
  - first: Marc-Alexandre
    full: "Marc-Alexandre C\xF4t\xE9"
    id: marc-alexandre-cote
    last: "C\xF4t\xE9"
  - first: Grzegorz
    full: "Grzegorz Chrupa\u0142a"
    id: grzegorz-chrupala
    last: "Chrupa\u0142a"
  - first: Afra
    full: Afra Alishahi
    id: afra-alishahi
    last: Alishahi
  author_string: "\xC1kos K\xE1d\xE1r, Desmond Elliott, Marc-Alexandre C\xF4t\xE9\
    , Grzegorz Chrupa\u0142a, Afra Alishahi"
  bibkey: kadar-etal-2018-lessons
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1039
  month: October
  page_first: '402'
  page_last: '412'
  pages: "402\u2013412"
  paper_id: '39'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1039.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1039.jpg
  title: Lessons Learned in Multilingual Grounded Language Learning
  title_html: Lessons Learned in Multilingual Grounded Language Learning
  url: https://www.aclweb.org/anthology/K18-1039
  year: '2018'
K18-1040:
  abstract: In sentence compression, the task of shortening sentences while retaining
    the original meaning, models tend to be trained on large corpora containing pairs
    of verbose and compressed sentences. To remove the need for paired corpora, we
    emulate a summarization task and add noise to extend sentences and train a denoising
    auto-encoder to recover the original, constructing an end-to-end training regime
    without the need for any examples of compressed sentences. We conduct a human
    evaluation of our model on a standard text summarization dataset and show that
    it performs comparably to a supervised baseline based on grammatical correctness
    and retention of meaning. Despite being exposed to no target data, our unsupervised
    models learn to generate imperfect but reasonably readable sentence summaries.
    Although we underperform supervised models based on ROUGE scores, our models are
    competitive with a supervised baseline based on human evaluation for grammatical
    correctness and retention of meaning.
  address: Brussels, Belgium
  author:
  - first: Thibault
    full: Thibault Fevry
    id: thibault-fevry
    last: Fevry
  - first: Jason
    full: Jason Phang
    id: jason-phang
    last: Phang
  author_string: Thibault Fevry, Jason Phang
  bibkey: fevry-phang-2018-unsupervised
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1040
  month: October
  page_first: '413'
  page_last: '422'
  pages: "413\u2013422"
  paper_id: '40'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1040.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1040.jpg
  title: Unsupervised Sentence Compression using Denoising Auto-Encoders
  title_html: Unsupervised Sentence Compression using Denoising Auto-Encoders
  url: https://www.aclweb.org/anthology/K18-1040
  year: '2018'
K18-1041:
  abstract: "Word embeddings are powerful tools that facilitate better analysis of\
    \ natural language. However, their quality highly depends on the resource used\
    \ for training. There are various approaches relying on n-gram corpora, such as\
    \ the Google n-gram corpus. However, n-gram corpora only offer a small window\
    \ into the full text \u2013 5 words for the Google corpus at best. This gives\
    \ way to the concern whether the extracted word semantics are of high quality.\
    \ In this paper, we address this concern with two contributions. First, we provide\
    \ a resource containing 120 word-embedding models \u2013 one of the largest collection\
    \ of embedding models. Furthermore, the resource contains the n-gramed versions\
    \ of all used corpora, as well as our scripts used for corpus generation, model\
    \ generation and evaluation. Second, we define a set of meaningful experiments\
    \ allowing to evaluate the aforementioned quality differences. We conduct these\
    \ experiments using our resource to show its usage and significance. The evaluation\
    \ results confirm that one generally can expect high quality for n-grams with\
    \ n > 3."
  address: Brussels, Belgium
  author:
  - first: "\xC1bel"
    full: "\xC1bel Elekes"
    id: abel-elekes
    last: Elekes
  - first: Adrian
    full: Adrian Englhardt
    id: adrian-englhardt
    last: Englhardt
  - first: Martin
    full: "Martin Sch\xE4ler"
    id: martin-schaler
    last: "Sch\xE4ler"
  - first: Klemens
    full: "Klemens B\xF6hm"
    id: klemens-bohm
    last: "B\xF6hm"
  author_string: "\xC1bel Elekes, Adrian Englhardt, Martin Sch\xE4ler, Klemens B\xF6\
    hm"
  bibkey: elekes-etal-2018-resources
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1041
  month: October
  page_first: '423'
  page_last: '432'
  pages: "423\u2013432"
  paper_id: '41'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1041.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1041.jpg
  title: Resources to Examine the Quality of Word Embedding Models Trained on n-Gram
    Data
  title_html: Resources to Examine the Quality of Word Embedding Models Trained on
    n-Gram Data
  url: https://www.aclweb.org/anthology/K18-1041
  year: '2018'
K18-1042:
  abstract: In this paper, we propose a new linguistically-based approach to answering
    non-factoid open-domain questions from unstructured data. First, we elaborate
    on an architecture for textual encoding based on which we introduce a deep end-to-end
    neural model. This architecture benefits from a bilateral attention mechanism
    which helps the model to focus on a question and the answer sentence at the same
    time for phrasal answer extraction. Second, we feed the output of a constituency
    parser into the model directly and integrate linguistic constituents into the
    network to help it concentrate on chunks of an answer rather than on its single
    words for generating more natural output. By optimizing this architecture, we
    managed to obtain near-to-human-performance results and competitive to a state-of-the-art
    system on SQuAD and MS-MARCO datasets respectively.
  address: Brussels, Belgium
  author:
  - first: Ahmad
    full: Ahmad Aghaebrahimian
    id: ahmad-aghaebrahimian
    last: Aghaebrahimian
  author_string: Ahmad Aghaebrahimian
  bibkey: aghaebrahimian-2018-linguistically
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1042
  month: October
  page_first: '433'
  page_last: '443'
  pages: "433\u2013443"
  paper_id: '42'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1042.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1042.jpg
  title: Linguistically-Based Deep Unstructured Question Answering
  title_html: Linguistically-Based Deep Unstructured Question Answering
  url: https://www.aclweb.org/anthology/K18-1042
  year: '2018'
K18-1043:
  abstract: Phonetic similarity algorithms identify words and phrases with similar
    pronunciation which are used in many natural language processing tasks. However,
    existing approaches are designed mainly for Indo-European languages and fail to
    capture the unique properties of Chinese pronunciation. In this paper, we propose
    a high dimensional encoded phonetic similarity algorithm for Chinese, DIMSIM.
    The encodings are learned from annotated data to separately map initial and final
    phonemes into n-dimensional coordinates. Pinyin phonetic similarities are then
    calculated by aggregating the similarities of initial, final and tone. DIMSIM
    demonstrates a 7.5X improvement on mean reciprocal rank over the state-of-the-art
    phonetic similarity approaches.
  address: Brussels, Belgium
  author:
  - first: Min
    full: Min Li
    id: min-li
    last: Li
  - first: Marina
    full: Marina Danilevsky
    id: marina-danilevsky
    last: Danilevsky
  - first: Sara
    full: Sara Noeman
    id: sara-noeman
    last: Noeman
  - first: Yunyao
    full: Yunyao Li
    id: yunyao-li
    last: Li
  author_string: Min Li, Marina Danilevsky, Sara Noeman, Yunyao Li
  bibkey: li-etal-2018-dimsim
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1043
  month: October
  page_first: '444'
  page_last: '453'
  pages: "444\u2013453"
  paper_id: '43'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1043.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1043.jpg
  title: 'DIMSIM: An Accurate Chinese Phonetic Similarity Algorithm Based on Learned
    High Dimensional Encoding'
  title_html: '<span class="acl-fixed-case">DIMSIM</span>: An Accurate <span class="acl-fixed-case">C</span>hinese
    Phonetic Similarity Algorithm Based on Learned High Dimensional Encoding'
  url: https://www.aclweb.org/anthology/K18-1043
  year: '2018'
K18-1044:
  abstract: "News editorials are said to shape public opinion, which makes them a\
    \ powerful tool and an important source of political argumentation. However, rarely\
    \ do editorials change anyone\u2019s stance on an issue completely, nor do they\
    \ tend to argue explicitly (but rather follow a subtle rhetorical strategy). So,\
    \ what does argumentation quality mean for editorials then? We develop the notion\
    \ that an effective editorial challenges readers with opposing stance, and at\
    \ the same time empowers the arguing skills of readers that share the editorial\u2019\
    s stance \u2014 or even challenges both sides. To study argumentation quality\
    \ based on this notion, we introduce a new corpus with 1000 editorials from the\
    \ New York Times, annotated for their perceived effect along with the annotators\u2019\
    \ political orientations. Analyzing the corpus, we find that annotators with different\
    \ orientation disagree on the effect significantly. While only 1% of all editorials\
    \ changed anyone\u2019s stance, more than 5% meet our notion. We conclude that\
    \ our corpus serves as a suitable resource for studying the argumentation quality\
    \ of news editorials."
  address: Brussels, Belgium
  author:
  - first: Roxanne
    full: Roxanne El Baff
    id: roxanne-el-baff
    last: El Baff
  - first: Henning
    full: Henning Wachsmuth
    id: henning-wachsmuth
    last: Wachsmuth
  - first: Khalid
    full: Khalid Al-Khatib
    id: khalid-al-khatib
    last: Al-Khatib
  - first: Benno
    full: Benno Stein
    id: benno-stein
    last: Stein
  author_string: Roxanne El Baff, Henning Wachsmuth, Khalid Al-Khatib, Benno Stein
  bibkey: el-baff-etal-2018-challenge
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1044
  month: October
  page_first: '454'
  page_last: '464'
  pages: "454\u2013464"
  paper_id: '44'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1044.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1044.jpg
  title: 'Challenge or Empower: Revisiting Argumentation Quality in a News Editorial
    Corpus'
  title_html: 'Challenge or Empower: Revisiting Argumentation Quality in a News Editorial
    Corpus'
  url: https://www.aclweb.org/anthology/K18-1044
  year: '2018'
K18-1045:
  abstract: Word order is clearly a vital part of human language, but it has been
    used comparatively lightly in distributional vector models. This paper presents
    a new method for incorporating word order information into word vector embedding
    models by combining the benefits of permutation-based order encoding with the
    more recent method of skip-gram with negative sampling. The new method introduced
    here is called Embeddings Augmented by Random Permutations (EARP). It operates
    by applying permutations to the coordinates of context vector representations
    during the process of training. Results show an 8% improvement in accuracy on
    the challenging Bigger Analogy Test Set, and smaller but consistent improvements
    on other analogy reference sets. These findings demonstrate the importance of
    order-based information in analogical retrieval tasks, and the utility of random
    permutations as a means to augment neural embeddings.
  address: Brussels, Belgium
  author:
  - first: Trevor
    full: Trevor Cohen
    id: trevor-cohn
    last: Cohen
  - first: Dominic
    full: Dominic Widdows
    id: dominic-widdows
    last: Widdows
  author_string: Trevor Cohen, Dominic Widdows
  bibkey: cohen-widdows-2018-bringing
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1045
  month: October
  page_first: '465'
  page_last: '475'
  pages: "465\u2013475"
  paper_id: '45'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1045.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1045.jpg
  title: Bringing Order to Neural Word Embeddings with Embeddings Augmented by Random
    Permutations (EARP)
  title_html: Bringing Order to Neural Word Embeddings with Embeddings Augmented by
    Random Permutations (<span class="acl-fixed-case">EARP</span>)
  url: https://www.aclweb.org/anthology/K18-1045
  year: '2018'
K18-1046:
  abstract: The task of entity linking aims to identify concepts mentioned in a text
    fragments and link them to a reference knowledge base. Entity linking in long
    text has been well studied in previous work. However, short text entity linking
    is more challenging since the text are noisy and less coherent. To better utilize
    the local information provided in short texts, we propose a novel neural network
    framework, Aggregated Semantic Matching (ASM), in which two different aspects
    of semantic information between the local context and the candidate entity are
    captured via representation-based and interaction-based neural semantic matching
    models, and then two matching signals work jointly for disambiguation with a rank
    aggregation mechanism. Our evaluation shows that the proposed model outperforms
    the state-of-the-arts on public tweet datasets.
  address: Brussels, Belgium
  author:
  - first: Feng
    full: Feng Nie
    id: feng-nie
    last: Nie
  - first: Shuyan
    full: Shuyan Zhou
    id: shuyan-zhou
    last: Zhou
  - first: Jing
    full: Jing Liu
    id: jing-liu
    last: Liu
  - first: Jinpeng
    full: Jinpeng Wang
    id: jinpeng-wang
    last: Wang
  - first: Chin-Yew
    full: Chin-Yew Lin
    id: chin-yew-lin
    last: Lin
  - first: Rong
    full: Rong Pan
    id: rong-pan
    last: Pan
  author_string: Feng Nie, Shuyan Zhou, Jing Liu, Jinpeng Wang, Chin-Yew Lin, Rong
    Pan
  bibkey: nie-etal-2018-aggregated
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1046
  month: October
  page_first: '476'
  page_last: '485'
  pages: "476\u2013485"
  paper_id: '46'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1046.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1046.jpg
  title: Aggregated Semantic Matching for Short Text Entity Linking
  title_html: Aggregated Semantic Matching for Short Text Entity Linking
  url: https://www.aclweb.org/anthology/K18-1046
  year: '2018'
K18-1047:
  abstract: 'We present two categories of model-agnostic adversarial strategies that
    reveal the weaknesses of several generative, task-oriented dialogue models: Should-Not-Change
    strategies that evaluate over-sensitivity to small and semantics-preserving edits,
    as well as Should-Change strategies that test if a model is over-stable against
    subtle yet semantics-changing modifications. We next perform adversarial training
    with each strategy, employing a max-margin approach for negative generative examples.
    This not only makes the target dialogue model more robust to the adversarial inputs,
    but also helps it perform significantly better on the original inputs. Moreover,
    training on all strategies combined achieves further improvements, achieving a
    new state-of-the-art performance on the original task (also verified via human
    evaluation). In addition to adversarial training, we also address the robustness
    task at the model-level, by feeding it subword units as both inputs and outputs,
    and show that the resulting model is equally competitive, requires only 1/4 of
    the original vocabulary size, and is robust to one of the adversarial strategies
    (to which the original model is vulnerable) even without adversarial training.'
  address: Brussels, Belgium
  author:
  - first: Tong
    full: Tong Niu
    id: tong-niu
    last: Niu
  - first: Mohit
    full: Mohit Bansal
    id: mohit-bansal
    last: Bansal
  author_string: Tong Niu, Mohit Bansal
  bibkey: niu-bansal-2018-adversarial
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1047
  month: October
  page_first: '486'
  page_last: '496'
  pages: "486\u2013496"
  paper_id: '47'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1047.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1047.jpg
  title: Adversarial Over-Sensitivity and Over-Stability Strategies for Dialogue Models
  title_html: Adversarial Over-Sensitivity and Over-Stability Strategies for Dialogue
    Models
  url: https://www.aclweb.org/anthology/K18-1047
  year: '2018'
K18-1048:
  abstract: Building systems that can communicate with humans is a core problem in
    Artificial Intelligence. This work proposes a novel neural network architecture
    for response selection in an end-to-end multi-turn conversational dialogue setting.
    The architecture applies context level attention and incorporates additional external
    knowledge provided by descriptions of domain-specific words. It uses a bi-directional
    Gated Recurrent Unit (GRU) for encoding context and responses and learns to attend
    over the context words given the latent response representation and vice versa.
    In addition, it incorporates external domain specific information using another
    GRU for encoding the domain keyword descriptions. This allows better representation
    of domain-specific keywords in responses and hence improves the overall performance.
    Experimental results show that our model outperforms all other state-of-the-art
    methods for response selection in multi-turn conversations.
  address: Brussels, Belgium
  author:
  - first: Debanjan
    full: Debanjan Chaudhuri
    id: debanjan-chaudhuri
    last: Chaudhuri
  - first: Agustinus
    full: Agustinus Kristiadi
    id: agustinus-kristiadi
    last: Kristiadi
  - first: Jens
    full: Jens Lehmann
    id: jens-lehmann
    last: Lehmann
  - first: Asja
    full: Asja Fischer
    id: asja-fischer
    last: Fischer
  author_string: Debanjan Chaudhuri, Agustinus Kristiadi, Jens Lehmann, Asja Fischer
  bibkey: chaudhuri-etal-2018-improving
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1048
  month: October
  page_first: '497'
  page_last: '507'
  pages: "497\u2013507"
  paper_id: '48'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1048.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1048.jpg
  title: Improving Response Selection in Multi-Turn Dialogue Systems by Incorporating
    Domain Knowledge
  title_html: Improving Response Selection in Multi-Turn Dialogue Systems by Incorporating
    Domain Knowledge
  url: https://www.aclweb.org/anthology/K18-1048
  year: '2018'
K18-1049:
  abstract: Tree-structured neural network architectures for sentence encoding draw
    inspiration from the approach to semantic composition generally seen in formal
    linguistics, and have shown empirical improvements over comparable sequence models
    by doing so. Moreover, adding multiplicative interaction terms to the composition
    functions in these models can yield significant further improvements. However,
    existing compositional approaches that adopt such a powerful composition function
    scale poorly, with parameter counts exploding as model dimension or vocabulary
    size grows. We introduce the Lifted Matrix-Space model, which uses a global transformation
    to map vector word embeddings to matrices, which can then be composed via an operation
    based on matrix-matrix multiplication. Its composition function effectively transmits
    a larger number of activations across layers with relatively few model parameters.
    We evaluate our model on the Stanford NLI corpus, the Multi-Genre NLI corpus,
    and the Stanford Sentiment Treebank and find that it consistently outperforms
    TreeLSTM (Tai et al., 2015), the previous best known composition function for
    tree-structured models.
  address: Brussels, Belgium
  author:
  - first: WooJin
    full: WooJin Chung
    id: woojin-chung
    last: Chung
  - first: Sheng-Fu
    full: Sheng-Fu Wang
    id: sheng-fu-wang
    last: Wang
  - first: Samuel
    full: Samuel Bowman
    id: samuel-bowman
    last: Bowman
  author_string: WooJin Chung, Sheng-Fu Wang, Samuel Bowman
  bibkey: chung-etal-2018-lifted
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1049
  month: October
  page_first: '508'
  page_last: '518'
  pages: "508\u2013518"
  paper_id: '49'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1049.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1049.jpg
  title: The Lifted Matrix-Space Model for Semantic Composition
  title_html: The Lifted Matrix-Space Model for Semantic Composition
  url: https://www.aclweb.org/anthology/K18-1049
  year: '2018'
K18-1050:
  abstract: Entity Linking (EL) is an essential task for semantic text understanding
    and information extraction. Popular methods separately address the Mention Detection
    (MD) and Entity Disambiguation (ED) stages of EL, without leveraging their mutual
    dependency. We here propose the first neural end-to-end EL system that jointly
    discovers and links entities in a text document. The main idea is to consider
    all possible spans as potential mentions and learn contextual similarity scores
    over their entity candidates that are useful for both MD and ED decisions. Key
    components are context-aware mention embeddings, entity embeddings and a probabilistic
    mention - entity map, without demanding other engineered features. Empirically,
    we show that our end-to-end method significantly outperforms popular systems on
    the Gerbil platform when enough training data is available. Conversely, if testing
    datasets follow different annotation conventions compared to the training set
    (e.g. queries/ tweets vs news documents), our ED model coupled with a traditional
    NER system offers the best or second best EL accuracy.
  address: Brussels, Belgium
  author:
  - first: Nikolaos
    full: Nikolaos Kolitsas
    id: nikolaos-kolitsas
    last: Kolitsas
  - first: Octavian-Eugen
    full: Octavian-Eugen Ganea
    id: octavian-eugen-ganea
    last: Ganea
  - first: Thomas
    full: Thomas Hofmann
    id: thomas-hofmann
    last: Hofmann
  author_string: Nikolaos Kolitsas, Octavian-Eugen Ganea, Thomas Hofmann
  bibkey: kolitsas-etal-2018-end
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1050
  month: October
  page_first: '519'
  page_last: '529'
  pages: "519\u2013529"
  paper_id: '50'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1050.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1050.jpg
  title: End-to-End Neural Entity Linking
  title_html: End-to-End Neural Entity Linking
  url: https://www.aclweb.org/anthology/K18-1050
  year: '2018'
K18-1051:
  abstract: In this paper we consider semantic spaces consisting of objects from some
    particular domain (e.g. IMDB movie reviews). Various authors have observed that
    such semantic spaces often model salient features (e.g. how scary a movie is)
    as directions. These feature directions allow us to rank objects according to
    how much they have the corresponding feature, and can thus play an important role
    in interpretable classifiers, recommendation systems, or entity-oriented search
    engines, among others. Methods for learning semantic spaces, however, are mostly
    aimed at modelling similarity. In this paper, we argue that there is an inherent
    trade-off between capturing similarity and faithfully modelling features as directions.
    Following this observation, we propose a simple method to fine-tune existing semantic
    spaces, with the aim of improving the quality of their feature directions. Crucially,
    our method is fully unsupervised, requiring only a bag-of-words representation
    of the objects as input.
  address: Brussels, Belgium
  author:
  - first: Thomas
    full: Thomas Ager
    id: thomas-ager
    last: Ager
  - first: "Ond\u0159ej"
    full: "Ond\u0159ej Ku\u017Eelka"
    id: ondrej-kuzelka
    last: "Ku\u017Eelka"
  - first: Steven
    full: Steven Schockaert
    id: steven-schockaert
    last: Schockaert
  author_string: "Thomas Ager, Ond\u0159ej Ku\u017Eelka, Steven Schockaert"
  bibkey: ager-etal-2018-modelling
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1051
  month: October
  page_first: '530'
  page_last: '540'
  pages: "530\u2013540"
  paper_id: '51'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1051.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1051.jpg
  title: Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces
  title_html: Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces
  url: https://www.aclweb.org/anthology/K18-1051
  year: '2018'
K18-1052:
  abstract: This paper investigates learning methods for multi-class classification
    using labeled data for the target classification scheme and another labeled data
    for a similar but different classification scheme (support scheme). We show that
    if we have prior knowledge about the relation between support and target classification
    schemes in the form of a class correspondence table, we can use it to improve
    the model performance further than the simple multi-task learning approach. Instead
    of learning the individual classification layers for the support and target schemes,
    the proposed method converts the class label of each example on the support scheme
    into a set of candidate class labels on the target scheme via the class correspondence
    table, and then uses the candidate labels to learn the classification layer for
    the target scheme. We evaluate the proposed method on two tasks in NLP. The experimental
    results show that our method effectively learns the target schemes especially
    for the classes that have a tight connection to certain support classes.
  address: Brussels, Belgium
  author:
  - first: Hiyori
    full: Hiyori Yoshikawa
    id: hiyori-yoshikawa
    last: Yoshikawa
  - first: Tomoya
    full: Tomoya Iwakura
    id: tomoya-iwakura
    last: Iwakura
  author_string: Hiyori Yoshikawa, Tomoya Iwakura
  bibkey: yoshikawa-iwakura-2018-model
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1052
  month: October
  page_first: '541'
  page_last: '550'
  pages: "541\u2013550"
  paper_id: '52'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1052.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1052.jpg
  title: Model Transfer with Explicit Knowledge of the Relation between Class Definitions
  title_html: Model Transfer with Explicit Knowledge of the Relation between Class
    Definitions
  url: https://www.aclweb.org/anthology/K18-1052
  year: '2018'
K18-1053:
  abstract: There have been several attempts to define a plausible motivation for
    a chit-chat dialogue agent that can lead to engaging conversations. In this work,
    we explore a new direction where the agent specifically focuses on discovering
    information about its interlocutor. We formalize this approach by defining a quantitative
    metric. We propose an algorithm for the agent to maximize it. We validate the
    idea with human evaluation where our system outperforms various baselines. We
    demonstrate that the metric indeed correlates with the human judgments of engagingness.
  address: Brussels, Belgium
  author:
  - first: Yury
    full: Yury Zemlyanskiy
    id: yury-zemlyanskiy
    last: Zemlyanskiy
  - first: Fei
    full: Fei Sha
    id: fei-sha
    last: Sha
  author_string: Yury Zemlyanskiy, Fei Sha
  bibkey: zemlyanskiy-sha-2018-aiming
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1053
  month: October
  page_first: '551'
  page_last: '561'
  pages: "551\u2013561"
  paper_id: '53'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1053.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1053.jpg
  title: Aiming to Know You Better Perhaps Makes Me a More Engaging Dialogue Partner
  title_html: Aiming to Know You Better Perhaps Makes Me a More Engaging Dialogue
    Partner
  url: https://www.aclweb.org/anthology/K18-1053
  year: '2018'
K18-1054:
  abstract: We present experiments for cross-domain semantic dependency analysis with
    a neural Maximum Subgraph parser. Our parser targets 1-endpoint-crossing, pagenumber-2
    graphs which are a good fit to semantic dependency graphs, and utilizes an efficient
    dynamic programming algorithm for decoding. For disambiguation, the parser associates
    words with BiLSTM vectors and utilizes these vectors to assign scores to candidate
    dependencies. We conduct experiments on the data sets from SemEval 2015 as well
    as Chinese CCGBank. Our parser achieves very competitive results for both English
    and Chinese. To improve the parsing performance on cross-domain texts, we propose
    a data-oriented method to explore the linguistic generality encoded in English
    Resource Grammar, which is a precisionoriented, hand-crafted HPSG grammar, in
    an implicit way. Experiments demonstrate the effectiveness of our data-oriented
    method across a wide range of conditions.
  address: Brussels, Belgium
  author:
  - first: Yufei
    full: Yufei Chen
    id: yufei-chen
    last: Chen
  - first: Sheng
    full: Sheng Huang
    id: sheng-huang
    last: Huang
  - first: Fang
    full: Fang Wang
    id: fang-wang
    last: Wang
  - first: Junjie
    full: Junjie Cao
    id: junjie-cao
    last: Cao
  - first: Weiwei
    full: Weiwei Sun
    id: weiwei-sun
    last: Sun
  - first: Xiaojun
    full: Xiaojun Wan
    id: xiaojun-wan
    last: Wan
  author_string: Yufei Chen, Sheng Huang, Fang Wang, Junjie Cao, Weiwei Sun, Xiaojun
    Wan
  bibkey: chen-etal-2018-neural
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1054
  month: October
  page_first: '562'
  page_last: '572'
  pages: "562\u2013572"
  paper_id: '54'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1054.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1054.jpg
  title: Neural Maximum Subgraph Parsing for Cross-Domain Semantic Dependency Analysis
  title_html: Neural Maximum Subgraph Parsing for Cross-Domain Semantic Dependency
    Analysis
  url: https://www.aclweb.org/anthology/K18-1054
  year: '2018'
K18-1055:
  abstract: Dropout is used to avoid overfitting by randomly dropping units from the
    neural networks during training. Inspired by dropout, this paper presents GI-Dropout,
    a novel dropout method integrating with global information to improve neural networks
    for text classification. Unlike the traditional dropout method in which the units
    are dropped randomly according to the same probability, we aim to use explicit
    instructions based on global information of the dataset to guide the training
    process. With GI-Dropout, the model is supposed to pay more attention to inapparent
    features or patterns. Experiments demonstrate the effectiveness of the dropout
    with global information on seven text classification tasks, including sentiment
    analysis and topic classification.
  address: Brussels, Belgium
  author:
  - first: Hengru
    full: Hengru Xu
    id: hengru-xu
    last: Xu
  - first: Shen
    full: Shen Li
    id: shen-li
    last: Li
  - first: Renfen
    full: Renfen Hu
    id: renfen-hu
    last: Hu
  - first: Si
    full: Si Li
    id: si-li
    last: Li
  - first: Sheng
    full: Sheng Gao
    id: sheng-gao
    last: Gao
  author_string: Hengru Xu, Shen Li, Renfen Hu, Si Li, Sheng Gao
  bibkey: xu-etal-2018-random
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1055
  month: October
  page_first: '573'
  page_last: '582'
  pages: "573\u2013582"
  paper_id: '55'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1055.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1055.jpg
  title: 'From Random to Supervised: A Novel Dropout Mechanism Integrated with Global
    Information'
  title_html: 'From Random to Supervised: A Novel Dropout Mechanism Integrated with
    Global Information'
  url: https://www.aclweb.org/anthology/K18-1055
  year: '2018'
K18-1056:
  abstract: Sequence to sequence (SEQ2SEQ) models lack diversity in their generated
    translations. This can be attributed to their limitations in capturing lexical
    and syntactic variations in parallel corpora, due to different styles, genres,
    topics, or ambiguity of human translation process. In this paper, we develop a
    novel sequence to sequence mixture (S2SMIX) model that improves both translation
    diversity and quality by adopting a committee of specialized translation models
    rather than a single translation model. Each mixture component selects its own
    training dataset via optimization of the marginal log-likelihood, which leads
    to a soft clustering of the parallel corpus. Experiments on four language pairs
    demonstrate the superiority of our mixture model compared to SEQ2SEQ model with
    the standard and diversity encouraged beam search. Our mixture model incurs negligible
    additional parameters and no extra computation in the decoding time.
  address: Brussels, Belgium
  author:
  - first: Xuanli
    full: Xuanli He
    id: xuanli-he
    last: He
  - first: Gholamreza
    full: Gholamreza Haffari
    id: gholamreza-haffari
    last: Haffari
  - first: Mohammad
    full: Mohammad Norouzi
    id: mohammad-norouzi
    last: Norouzi
  author_string: Xuanli He, Gholamreza Haffari, Mohammad Norouzi
  bibkey: he-etal-2018-sequence
  bibtype: inproceedings
  booktitle: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  booktitle_html: Proceedings of the 22nd Conference on Computational Natural Language
    Learning
  doi: 10.18653/v1/K18-1056
  month: October
  page_first: '583'
  page_last: '592'
  pages: "583\u2013592"
  paper_id: '56'
  parent_volume_id: K18-1
  pdf: https://www.aclweb.org/anthology/K18-1056.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-1056.jpg
  title: Sequence to Sequence Mixture Model for Diverse Machine Translation
  title_html: Sequence to Sequence Mixture Model for Diverse Machine Translation
  url: https://www.aclweb.org/anthology/K18-1056
  year: '2018'
K18-2000:
  address: Brussels, Belgium
  author:
  - first: Daniel
    full: Daniel Zeman
    id: daniel-zeman
    last: Zeman
  - first: Jan
    full: "Jan Haji\u010D"
    id: jan-hajic
    last: "Haji\u010D"
  author_string: "Daniel Zeman, Jan Haji\u010D"
  bibkey: conll-2018-conll
  bibtype: proceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  month: October
  paper_id: '0'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2000.jpg
  title: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw
    Text to Universal Dependencies'
  title_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span class="acl-fixed-case">NLL</span>
    2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies'
  url: https://www.aclweb.org/anthology/K18-2000
  year: '2018'
K18-2001:
  abstract: "Every year, the Conference on Computational Natural Language Learning\
    \ (CoNLL) features a shared task, in which participants train and test their learning\
    \ systems on the same data sets. In 2018, one of two tasks was devoted to learning\
    \ dependency parsers for a large number of languages, in a real-world setting\
    \ without any gold-standard annotation on test input. All test sets followed a\
    \ unified annotation scheme, namely that of Universal Dependencies. This shared\
    \ task constitutes a 2nd edition\u2014the first one took place in 2017 (Zeman\
    \ et al., 2017); the main metric from 2017 has been kept, allowing for easy comparison,\
    \ also in 2018, and two new main metrics have been used. New datasets added to\
    \ the Universal Dependencies collection between mid-2017 and the spring of 2018\
    \ have contributed to increased difficulty of the task this year. In this overview\
    \ paper, we define the task and the updated evaluation methodology, describe data\
    \ preparation, report and analyze the main results, and provide a brief categorization\
    \ of the different approaches of the participating systems."
  address: Brussels, Belgium
  author:
  - first: Daniel
    full: Daniel Zeman
    id: daniel-zeman
    last: Zeman
  - first: Jan
    full: "Jan Haji\u010D"
    id: jan-hajic
    last: "Haji\u010D"
  - first: Martin
    full: Martin Popel
    id: martin-popel
    last: Popel
  - first: Martin
    full: Martin Potthast
    id: martin-potthast
    last: Potthast
  - first: Milan
    full: Milan Straka
    id: milan-straka
    last: Straka
  - first: Filip
    full: Filip Ginter
    id: filip-ginter
    last: Ginter
  - first: Joakim
    full: Joakim Nivre
    id: joakim-nivre
    last: Nivre
  - first: Slav
    full: Slav Petrov
    id: slav-petrov
    last: Petrov
  author_string: "Daniel Zeman, Jan Haji\u010D, Martin Popel, Martin Potthast, Milan\
    \ Straka, Filip Ginter, Joakim Nivre, Slav Petrov"
  bibkey: zeman-etal-2018-conll
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2001
  month: October
  page_first: '1'
  page_last: '21'
  pages: "1\u201321"
  paper_id: '1'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2001.jpg
  title: 'CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal
    Dependencies'
  title_html: '<span class="acl-fixed-case">C</span>o<span class="acl-fixed-case">NLL</span>
    2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies'
  url: https://www.aclweb.org/anthology/K18-2001
  year: '2018'
K18-2002:
  abstract: We summarize empirical results and tentative conclusions from the Second
    Extrinsic Parser Evaluation Initiative (EPE 2018). We review the basic task setup,
    downstream applications involved, and end-to-end results for seventeen participating
    teams. Based on in-depth quantitative and qualitative analysis, we correlate intrinsic
    evaluation results at different layers of morph-syntactic analysis with observed
    downstream behavior.
  address: Brussels, Belgium
  author:
  - first: Murhaf
    full: Murhaf Fares
    id: murhaf-fares
    last: Fares
  - first: Stephan
    full: Stephan Oepen
    id: stephan-oepen
    last: Oepen
  - first: Lilja
    full: "Lilja \xD8vrelid"
    id: lilja-ovrelid
    last: "\xD8vrelid"
  - first: Jari
    full: "Jari Bj\xF6rne"
    id: jari-bjorne
    last: "Bj\xF6rne"
  - first: Richard
    full: Richard Johansson
    id: richard-johansson
    last: Johansson
  author_string: "Murhaf Fares, Stephan Oepen, Lilja \xD8vrelid, Jari Bj\xF6rne, Richard\
    \ Johansson"
  bibkey: fares-etal-2018-2018
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2002
  month: October
  page_first: '22'
  page_last: '33'
  pages: "22\u201333"
  paper_id: '2'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2002.jpg
  title: 'The 2018 Shared Task on Extrinsic Parser Evaluation: On the Downstream Utility
    of English Universal Dependency Parsers'
  title_html: 'The 2018 Shared Task on Extrinsic Parser Evaluation: On the Downstream
    Utility of <span class="acl-fixed-case">E</span>nglish Universal Dependency Parsers'
  url: https://www.aclweb.org/anthology/K18-2002
  year: '2018'
K18-2003:
  abstract: In this paper, we describe the system used for our first participation
    at the CoNLL 2018 shared task. The submitted system largely reused the state of
    the art parser from CoNLL 2017 (https://github.com/tdozat/Parser-v2). We enhanced
    this system for morphological features predictions, and we used all available
    resources to provide accurate models for low-resource languages. We ranked 5th
    of 27 participants in MLAS for building morphology aware dependency trees, 2nd
    for morphological features only, and 3rd for tagging (UPOS) and parsing (LAS)
    low-resource languages.
  address: Brussels, Belgium
  author:
  - first: Elie
    full: Elie Duthoo
    id: elie-duthoo
    last: Duthoo
  - first: Olivier
    full: Olivier Mesnard
    id: olivier-mesnard
    last: Mesnard
  author_string: Elie Duthoo, Olivier Mesnard
  bibkey: duthoo-mesnard-2018-cea
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2003
  month: October
  page_first: '34'
  page_last: '44'
  pages: "34\u201344"
  paper_id: '3'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2003.jpg
  title: 'CEA LIST: Processing Low-Resource Languages for CoNLL 2018'
  title_html: '<span class="acl-fixed-case">CEA</span> <span class="acl-fixed-case">LIST</span>:
    Processing Low-Resource Languages for <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018'
  url: https://www.aclweb.org/anthology/K18-2003
  year: '2018'
K18-2004:
  abstract: 'This paper describes the ICS PAS system which took part in CoNLL 2018
    shared task on Multilingual Parsing from Raw Text to Universal Dependencies. The
    system consists of jointly trained tagger, lemmatizer, and dependency parser which
    are based on features extracted by a biLSTM network. The system uses both fully
    connected and dilated convolutional neural architectures. The novelty of our approach
    is the use of an additional loss function, which reduces the number of cycles
    in the predicted dependency graphs, and the use of self-training to increase the
    system performance. The proposed system, i.e. ICS PAS (Warszawa), ranked 3th/4th
    in the official evaluation obtaining the following overall results: 73.02 (LAS),
    60.25 (MLAS) and 64.44 (BLEX).'
  address: Brussels, Belgium
  author:
  - first: Piotr
    full: Piotr Rybak
    id: piotr-rybak
    last: Rybak
  - first: Alina
    full: "Alina Wr\xF3blewska"
    id: alina-wroblewska
    last: "Wr\xF3blewska"
  author_string: "Piotr Rybak, Alina Wr\xF3blewska"
  bibkey: rybak-wroblewska-2018-semi
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2004
  month: October
  page_first: '45'
  page_last: '54'
  pages: "45\u201354"
  paper_id: '4'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2004.jpg
  title: Semi-Supervised Neural System for Tagging, Parsing and Lematization
  title_html: Semi-Supervised Neural System for Tagging, Parsing and Lematization
  url: https://www.aclweb.org/anthology/K18-2004
  year: '2018'
K18-2005:
  abstract: "This paper describes our system (HIT-SCIR) submitted to the CoNLL 2018\
    \ shared task on Multilingual Parsing from Raw Text to Universal Dependencies.\
    \ We base our submission on Stanford\u2019s winning system for the CoNLL 2017\
    \ shared task and make two effective extensions: 1) incorporating deep contextualized\
    \ word embeddings into both the part of speech tagger and parser; 2) ensembling\
    \ parsers trained with different initialization. We also explore different ways\
    \ of concatenating treebanks for further improvements. Experimental results on\
    \ the development data show the effectiveness of our methods. In the final evaluation,\
    \ our system was ranked first according to LAS (75.84%) and outperformed the other\
    \ systems by a large margin."
  address: Brussels, Belgium
  author:
  - first: Wanxiang
    full: Wanxiang Che
    id: wanxiang-che
    last: Che
  - first: Yijia
    full: Yijia Liu
    id: yijia-liu
    last: Liu
  - first: Yuxuan
    full: Yuxuan Wang
    id: yuxuan-wang
    last: Wang
  - first: Bo
    full: Bo Zheng
    id: bo-zheng
    last: Zheng
  - first: Ting
    full: Ting Liu
    id: ting-liu
    last: Liu
  author_string: Wanxiang Che, Yijia Liu, Yuxuan Wang, Bo Zheng, Ting Liu
  bibkey: che-etal-2018-towards
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2005
  month: October
  page_first: '55'
  page_last: '64'
  pages: "55\u201364"
  paper_id: '5'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2005.jpg
  title: 'Towards Better UD Parsing: Deep Contextualized Word Embeddings, Ensemble,
    and Treebank Concatenation'
  title_html: 'Towards Better <span class="acl-fixed-case">UD</span> Parsing: Deep
    Contextualized Word Embeddings, Ensemble, and Treebank Concatenation'
  url: https://www.aclweb.org/anthology/K18-2005
  year: '2018'
K18-2006:
  abstract: 'This paper describes the system of team LeisureX in the CoNLL 2018 Shared
    Task: Multilingual Parsing from Raw Text to Universal Dependencies. Our system
    predicts the part-of-speech tag and dependency tree jointly. For the basic tasks,
    including tokenization, lemmatization and morphology prediction, we employ the
    official baseline model (UDPipe). To train the low-resource languages, we adopt
    a sampling method based on other richresource languages. Our system achieves a
    macro-average of 68.31% LAS F1 score, with an improvement of 2.51% compared with
    the UDPipe.'
  address: Brussels, Belgium
  author:
  - first: Zuchao
    full: Zuchao Li
    id: zuchao-li
    last: Li
  - first: Shexia
    full: Shexia He
    id: shexia-he
    last: He
  - first: Zhuosheng
    full: Zhuosheng Zhang
    id: zhuosheng-zhang
    last: Zhang
  - first: Hai
    full: Hai Zhao
    id: hai-zhao
    last: Zhao
  author_string: Zuchao Li, Shexia He, Zhuosheng Zhang, Hai Zhao
  bibkey: li-etal-2018-joint
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2006
  month: October
  page_first: '65'
  page_last: '73'
  pages: "65\u201373"
  paper_id: '6'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2006.jpg
  title: Joint Learning of POS and Dependencies for Multilingual Universal Dependency
    Parsing
  title_html: Joint Learning of <span class="acl-fixed-case">POS</span> and Dependencies
    for Multilingual Universal Dependency Parsing
  url: https://www.aclweb.org/anthology/K18-2006
  year: '2018'
K18-2007:
  abstract: 'This paper describes the system of our team Phoenix for participating
    CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies.
    Given the annotated gold standard data in CoNLL-U format, we train the tokenizer,
    tagger and parser separately for each treebank based on an open source pipeline
    tool UDPipe. Our system reads the plain texts for input, performs the pre-processing
    steps (tokenization, lemmas, morphology) and finally outputs the syntactic dependencies.
    For the low-resource languages with no training data, we use cross-lingual techniques
    to build models with some close languages instead. In the official evaluation,
    our system achieves the macro-averaged scores of 65.61%, 52.26%, 55.71% for LAS,
    MLAS and BLEX respectively.'
  address: Brussels, Belgium
  author:
  - first: Yingting
    full: Yingting Wu
    id: yingting-wu
    last: Wu
  - first: Hai
    full: Hai Zhao
    id: hai-zhao
    last: Zhao
  - first: Jia-Jun
    full: Jia-Jun Tong
    id: jia-jun-tong
    last: Tong
  author_string: Yingting Wu, Hai Zhao, Jia-Jun Tong
  bibkey: wu-etal-2018-multilingual
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2007
  month: October
  page_first: '74'
  page_last: '80'
  pages: "74\u201380"
  paper_id: '7'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2007.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2007.jpg
  title: Multilingual Universal Dependency Parsing from Raw Text with Low-Resource
    Language Enhancement
  title_html: Multilingual Universal Dependency Parsing from Raw Text with Low-Resource
    Language Enhancement
  url: https://www.aclweb.org/anthology/K18-2007
  year: '2018'
K18-2008:
  abstract: "We propose a novel neural network model for joint part-of-speech (POS)\
    \ tagging and dependency parsing. Our model extends the well-known BIST graph-based\
    \ dependency parser (Kiperwasser and Goldberg, 2016) by incorporating a BiLSTM-based\
    \ tagging component to produce automatically predicted POS tags for the parser.\
    \ On the benchmark English Penn treebank, our model obtains strong UAS and LAS\
    \ scores at 94.51% and 92.87%, respectively, producing 1.5+% absolute improvements\
    \ to the BIST graph-based parser, and also obtaining a state-of-the-art POS tagging\
    \ accuracy at 97.97%. Furthermore, experimental results on parsing 61 \u201Cbig\u201D\
    \ Universal Dependencies treebanks from raw texts show that our model outperforms\
    \ the baseline UDPipe (Straka and Strakova, 2017) with 0.8% higher average POS\
    \ tagging score and 3.6% higher average LAS score. In addition, with our model,\
    \ we also obtain state-of-the-art downstream task scores for biomedical event\
    \ extraction and opinion analysis applications. Our code is available together\
    \ with all pre-trained models at: https://github.com/datquocnguyen/jPTDP"
  address: Brussels, Belgium
  author:
  - first: Dat Quoc
    full: Dat Quoc Nguyen
    id: dat-quoc-nguyen
    last: Nguyen
  - first: Karin
    full: Karin Verspoor
    id: karin-verspoor
    last: Verspoor
  author_string: Dat Quoc Nguyen, Karin Verspoor
  bibkey: nguyen-verspoor-2018-improved
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2008
  month: October
  page_first: '81'
  page_last: '91'
  pages: "81\u201391"
  paper_id: '8'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2008.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2008.jpg
  title: An Improved Neural Network Model for Joint POS Tagging and Dependency Parsing
  title_html: An Improved Neural Network Model for Joint <span class="acl-fixed-case">POS</span>
    Tagging and Dependency Parsing
  url: https://www.aclweb.org/anthology/K18-2008
  year: '2018'
K18-2009:
  abstract: This paper presents the IBM Research AI submission to the CoNLL 2018 Shared
    Task on Parsing Universal Dependencies. Our system implements a new joint transition-based
    parser, based on the Stack-LSTM framework and the Arc-Standard algorithm, that
    handles tokenization, part-of-speech tagging, morphological tagging and dependency
    parsing in one single model. By leveraging a combination of character-based modeling
    of words and recursive composition of partially built linguistic structures we
    qualified 13th overall and 7th in low resource. We also present a new sentence
    segmentation neural architecture based on Stack-LSTMs that was the 4th best overall.
  address: Brussels, Belgium
  author:
  - first: Hui
    full: Hui Wan
    id: hui-wan
    last: Wan
  - first: Tahira
    full: Tahira Naseem
    id: tahira-naseem
    last: Naseem
  - first: Young-Suk
    full: Young-Suk Lee
    id: young-suk-lee
    last: Lee
  - first: Vittorio
    full: Vittorio Castelli
    id: vittorio-castelli
    last: Castelli
  - first: Miguel
    full: Miguel Ballesteros
    id: miguel-ballesteros
    last: Ballesteros
  author_string: Hui Wan, Tahira Naseem, Young-Suk Lee, Vittorio Castelli, Miguel
    Ballesteros
  bibkey: wan-etal-2018-ibm
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2009
  month: October
  page_first: '92'
  page_last: '102'
  pages: "92\u2013102"
  paper_id: '9'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2009.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2009.jpg
  title: IBM Research at the CoNLL 2018 Shared Task on Multilingual Parsing
  title_html: <span class="acl-fixed-case">IBM</span> Research at the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task on Multilingual Parsing
  url: https://www.aclweb.org/anthology/K18-2009
  year: '2018'
K18-2010:
  abstract: This paper presents our experiments with applying TUPA to the CoNLL 2018
    UD shared task. TUPA is a general neural transition-based DAG parser, which we
    use to present the first experiments on recovering enhanced dependencies as part
    of the general parsing task. TUPA was designed for parsing UCCA, a cross-linguistic
    semantic annotation scheme, exhibiting reentrancy, discontinuity and non-terminal
    nodes. By converting UD trees and graphs to a UCCA-like DAG format, we train TUPA
    almost without modification on the UD parsing task. The generic nature of our
    approach lends itself naturally to multitask learning.
  address: Brussels, Belgium
  attachment:
  - filename: K18-2010.Poster.pdf
    type: poster
    url: https://www.aclweb.org/anthology/attachments/K18-2010.Poster.pdf
  author:
  - first: Daniel
    full: Daniel Hershcovich
    id: daniel-hershcovich
    last: Hershcovich
  - first: Omri
    full: Omri Abend
    id: omri-abend
    last: Abend
  - first: Ari
    full: Ari Rappoport
    id: ari-rappoport
    last: Rappoport
  author_string: Daniel Hershcovich, Omri Abend, Ari Rappoport
  bibkey: hershcovich-etal-2018-universal
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2010
  month: October
  page_first: '103'
  page_last: '112'
  pages: "103\u2013112"
  paper_id: '10'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2010.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2010.jpg
  title: Universal Dependency Parsing with a General Transition-Based DAG Parser
  title_html: Universal Dependency Parsing with a General Transition-Based <span class="acl-fixed-case">DAG</span>
    Parser
  url: https://www.aclweb.org/anthology/K18-2010
  year: '2018'
K18-2011:
  abstract: 'We present the Uppsala system for the CoNLL 2018 Shared Task on universal
    dependency parsing. Our system is a pipeline consisting of three components: the
    first performs joint word and sentence segmentation; the second predicts part-of-speech
    tags and morphological features; the third predicts dependency trees from words
    and tags. Instead of training a single parsing model for each treebank, we trained
    models with multiple treebanks for one language or closely related languages,
    greatly reducing the number of models. On the official test run, we ranked 7th
    of 27 teams for the LAS and MLAS metrics. Our system obtained the best scores
    overall for word segmentation, universal POS tagging, and morphological features.'
  address: Brussels, Belgium
  author:
  - first: Aaron
    full: Aaron Smith
    id: aaron-smith
    last: Smith
  - first: Bernd
    full: Bernd Bohnet
    id: bernd-bohnet
    last: Bohnet
  - first: Miryam
    full: Miryam de Lhoneux
    id: miryam-de-lhoneux
    last: de Lhoneux
  - first: Joakim
    full: Joakim Nivre
    id: joakim-nivre
    last: Nivre
  - first: Yan
    full: Yan Shao
    id: yan-shao
    last: Shao
  - first: Sara
    full: Sara Stymne
    id: sara-stymne
    last: Stymne
  author_string: Aaron Smith, Bernd Bohnet, Miryam de Lhoneux, Joakim Nivre, Yan Shao,
    Sara Stymne
  bibkey: smith-etal-2018-82
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2011
  month: October
  page_first: '113'
  page_last: '123'
  pages: "113\u2013123"
  paper_id: '11'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2011.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2011.jpg
  title: '82 Treebanks, 34 Models: Universal Dependency Parsing with Multi-Treebank
    Models'
  title_html: '82 Treebanks, 34 Models: Universal Dependency Parsing with Multi-Treebank
    Models'
  url: https://www.aclweb.org/anthology/K18-2011
  year: '2018'
K18-2012:
  abstract: "We introduce tree-stack LSTM to model state of a transition based parser\
    \ with recurrent neural networks. Tree-stack LSTM does not use any parse tree\
    \ based or hand-crafted features, yet performs better than models with these features.\
    \ We also develop new set of embeddings from raw features to enhance the performance.\
    \ There are 4 main components of this model: stack\u2019s \u03C3-LSTM, buffer\u2019\
    s \u03B2-LSTM, actions\u2019 LSTM and tree-RNN. All LSTMs use continuous dense\
    \ feature vectors (embeddings) as an input. Tree-RNN updates these embeddings\
    \ based on transitions. We show that our model improves performance with low resource\
    \ languages compared with its predecessors. We participate in CoNLL 2018 UD Shared\
    \ Task as the \u201CKParse\u201D team and ranked 16th in LAS, 15th in BLAS and\
    \ BLEX metrics, of 27 participants parsing 82 test sets from 57 languages."
  address: Brussels, Belgium
  author:
  - first: "\xD6mer"
    full: "\xD6mer K\u0131rnap"
    id: omer-kirnap
    last: "K\u0131rnap"
  - first: Erenay
    full: "Erenay Dayan\u0131k"
    id: erenay-dayanik
    last: "Dayan\u0131k"
  - first: Deniz
    full: Deniz Yuret
    id: deniz-yuret
    last: Yuret
  author_string: "\xD6mer K\u0131rnap, Erenay Dayan\u0131k, Deniz Yuret"
  bibkey: kirnap-etal-2018-tree
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2012
  month: October
  page_first: '124'
  page_last: '132'
  pages: "124\u2013132"
  paper_id: '12'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2012.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2012.jpg
  title: Tree-Stack LSTM in Transition Based Dependency Parsing
  title_html: Tree-Stack <span class="acl-fixed-case">LSTM</span> in Transition Based
    Dependency Parsing
  url: https://www.aclweb.org/anthology/K18-2012
  year: '2018'
K18-2013:
  abstract: In this paper we describe the TurkuNLP entry at the CoNLL 2018 Shared
    Task on Multilingual Parsing from Raw Text to Universal Dependencies. Compared
    to the last year, this year the shared task includes two new main metrics to measure
    the morphological tagging and lemmatization accuracies in addition to syntactic
    trees. Basing our motivation into these new metrics, we developed an end-to-end
    parsing pipeline especially focusing on developing a novel and state-of-the-art
    component for lemmatization. Our system reached the highest aggregate ranking
    on three main metrics out of 26 teams by achieving 1st place on metric involving
    lemmatization, and 2nd on both morphological tagging and parsing.
  address: Brussels, Belgium
  author:
  - first: Jenna
    full: Jenna Kanerva
    id: jenna-kanerva
    last: Kanerva
  - first: Filip
    full: Filip Ginter
    id: filip-ginter
    last: Ginter
  - first: Niko
    full: Niko Miekka
    id: niko-miekka
    last: Miekka
  - first: Akseli
    full: Akseli Leino
    id: akseli-leino
    last: Leino
  - first: Tapio
    full: Tapio Salakoski
    id: tapio-salakoski
    last: Salakoski
  author_string: Jenna Kanerva, Filip Ginter, Niko Miekka, Akseli Leino, Tapio Salakoski
  bibkey: kanerva-etal-2018-turku
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2013
  month: October
  page_first: '133'
  page_last: '142'
  pages: "133\u2013142"
  paper_id: '13'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2013.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2013.jpg
  title: 'Turku Neural Parser Pipeline: An End-to-End System for the CoNLL 2018 Shared
    Task'
  title_html: '<span class="acl-fixed-case">T</span>urku Neural Parser Pipeline: An
    End-to-End System for the <span class="acl-fixed-case">C</span>o<span class="acl-fixed-case">NLL</span>
    2018 Shared Task'
  url: https://www.aclweb.org/anthology/K18-2013
  year: '2018'
K18-2014:
  abstract: "We describe the SEx BiST parser (Semantically EXtended Bi-LSTM parser)\
    \ developed at Lattice for the CoNLL 2018 Shared Task (Multilingual Parsing from\
    \ Raw Text to Universal Dependencies). The main characteristic of our work is\
    \ the encoding of three different modes of contextual information for parsing:\
    \ (i) Treebank feature representations, (ii) Multilingual word representations,\
    \ (iii) ELMo representations obtained via unsupervised learning from external\
    \ resources. Our parser performed well in the official end-to-end evaluation (73.02\
    \ LAS \u2013 4th/26 teams, and 78.72 UAS \u2013 2nd/26); remarkably, we achieved\
    \ the best UAS scores on all the English corpora by applying the three suggested\
    \ feature representations. Finally, we were also ranked 1st at the optional event\
    \ extraction task, part of the 2018 Extrinsic Parser Evaluation campaign."
  address: Brussels, Belgium
  author:
  - first: KyungTae
    full: KyungTae Lim
    id: kyungtae-lim
    last: Lim
  - first: Cheoneum
    full: Cheoneum Park
    id: cheoneum-park
    last: Park
  - first: Changki
    full: Changki Lee
    id: changki-lee
    last: Lee
  - first: Thierry
    full: Thierry Poibeau
    id: thierry-poibeau
    last: Poibeau
  author_string: KyungTae Lim, Cheoneum Park, Changki Lee, Thierry Poibeau
  bibkey: lim-etal-2018-sex
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2014
  month: October
  page_first: '143'
  page_last: '152'
  pages: "143\u2013152"
  paper_id: '14'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2014.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2014.jpg
  title: 'SEx BiST: A Multi-Source Trainable Parser with Deep Contextualized Lexical
    Representations'
  title_html: '<span class="acl-fixed-case">SE</span>x <span class="acl-fixed-case">B</span>i<span
    class="acl-fixed-case">ST</span>: A Multi-Source Trainable Parser with Deep Contextualized
    Lexical Representations'
  url: https://www.aclweb.org/anthology/K18-2014
  year: '2018'
K18-2015:
  abstract: 'This paper describes our system (SLT-Interactions) for the CoNLL 2018
    shared task: Multilingual Parsing from Raw Text to Universal Dependencies. Our
    system performs three main tasks: word segmentation (only for few treebanks),
    POS tagging and parsing. While segmentation is learned separately, we use neural
    stacking for joint learning of POS tagging and parsing tasks. For all the tasks,
    we employ simple neural network architectures that rely on long short-term memory
    (LSTM) networks for learning task-dependent features. At the basis of our parser,
    we use an arc-standard algorithm with Swap action for general non-projective parsing.
    Additionally, we use neural stacking as a knowledge transfer mechanism for cross-domain
    parsing of low resource domains. Our system shows substantial gains against the
    UDPipe baseline, with an average improvement of 4.18% in LAS across all languages.
    Overall, we are placed at the 12th position on the official test sets.'
  address: Brussels, Belgium
  author:
  - first: Riyaz A.
    full: Riyaz A. Bhat
    id: riyaz-ahmad-bhat
    last: Bhat
  - first: Irshad
    full: Irshad Bhat
    id: irshad-bhat
    last: Bhat
  - first: Srinivas
    full: Srinivas Bangalore
    id: srinivas-bangalore
    last: Bangalore
  author_string: Riyaz A. Bhat, Irshad Bhat, Srinivas Bangalore
  bibkey: bhat-etal-2018-slt
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2015
  month: October
  page_first: '153'
  page_last: '159'
  pages: "153\u2013159"
  paper_id: '15'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2015.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2015.jpg
  title: The SLT-Interactions Parsing System at the CoNLL 2018 Shared Task
  title_html: The <span class="acl-fixed-case">SLT</span>-Interactions Parsing System
    at the <span class="acl-fixed-case">C</span>o<span class="acl-fixed-case">NLL</span>
    2018 Shared Task
  url: https://www.aclweb.org/anthology/K18-2015
  year: '2018'
K18-2016:
  abstract: "This paper describes Stanford\u2019s system at the CoNLL 2018 UD Shared\
    \ Task. We introduce a complete neural pipeline system that takes raw text as\
    \ input, and performs all tasks required by the shared task, ranging from tokenization\
    \ and sentence segmentation, to POS tagging and dependency parsing. Our single\
    \ system submission achieved very competitive performance on big treebanks. Moreover,\
    \ after fixing an unfortunate bug, our corrected system would have placed the\
    \ 2nd, 1st, and 3rd on the official evaluation metrics LAS, MLAS, and BLEX, and\
    \ would have outperformed all submission systems on low-resource treebank categories\
    \ on all metrics by a large margin. We further show the effectiveness of different\
    \ model components through extensive ablation studies."
  address: Brussels, Belgium
  author:
  - first: Peng
    full: Peng Qi
    id: peng-qi
    last: Qi
  - first: Timothy
    full: Timothy Dozat
    id: timothy-dozat
    last: Dozat
  - first: Yuhao
    full: Yuhao Zhang
    id: yuhao-zhang
    last: Zhang
  - first: Christopher D.
    full: Christopher D. Manning
    id: christopher-d-manning
    last: Manning
  author_string: Peng Qi, Timothy Dozat, Yuhao Zhang, Christopher D. Manning
  bibkey: qi-etal-2018-universal
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2016
  month: October
  page_first: '160'
  page_last: '170'
  pages: "160\u2013170"
  paper_id: '16'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2016.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2016.jpg
  title: Universal Dependency Parsing from Scratch
  title_html: Universal Dependency Parsing from Scratch
  url: https://www.aclweb.org/anthology/K18-2016
  year: '2018'
K18-2017:
  abstract: "We introduce NLP-Cube: an end-to-end Natural Language Processing framework,\
    \ evaluated in CoNLL\u2019s \u201CMultilingual Parsing from Raw Text to Universal\
    \ Dependencies 2018\u201D Shared Task. It performs sentence splitting, tokenization,\
    \ compound word expansion, lemmatization, tagging and parsing. Based entirely\
    \ on recurrent neural networks, written in Python, this ready-to-use open source\
    \ system is freely available on GitHub. For each task we describe and discuss\
    \ its specific network architecture, closing with an overview on the results obtained\
    \ in the competition."
  address: Brussels, Belgium
  author:
  - first: Tiberiu
    full: Tiberiu Boros
    id: tiberiu-boros
    last: Boros
  - first: Stefan Daniel
    full: Stefan Daniel Dumitrescu
    id: stefan-daniel-dumitrescu
    last: Dumitrescu
  - first: Ruxandra
    full: Ruxandra Burtica
    id: ruxandra-burtica
    last: Burtica
  author_string: Tiberiu Boros, Stefan Daniel Dumitrescu, Ruxandra Burtica
  bibkey: boros-etal-2018-nlp
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2017
  month: October
  page_first: '171'
  page_last: '179'
  pages: "171\u2013179"
  paper_id: '17'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2017.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2017.jpg
  title: 'NLP-Cube: End-to-End Raw Text Processing With Neural Networks'
  title_html: '<span class="acl-fixed-case">NLP</span>-Cube: End-to-End Raw Text Processing
    With Neural Networks'
  url: https://www.aclweb.org/anthology/K18-2017
  year: '2018'
K18-2018:
  abstract: This paper describes our submission to CoNLL UD Shared Task 2018. We have
    extended an LSTM-based neural network designed for sequence tagging to additionally
    generate character-level sequences. The network was jointly trained to produce
    lemmas, part-of-speech tags and morphological features. Sentence segmentation,
    tokenization and dependency parsing were handled by UDPipe 1.2 baseline. The results
    demonstrate the viability of the proposed multitask architecture, although its
    performance still remains far from state-of-the-art.
  address: Brussels, Belgium
  author:
  - first: Gor
    full: Gor Arakelyan
    id: gor-arakelyan
    last: Arakelyan
  - first: Karen
    full: Karen Hambardzumyan
    id: karen-hambardzumyan
    last: Hambardzumyan
  - first: Hrant
    full: Hrant Khachatrian
    id: hrant-khachatrian
    last: Khachatrian
  author_string: Gor Arakelyan, Karen Hambardzumyan, Hrant Khachatrian
  bibkey: arakelyan-etal-2018-towards
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2018
  month: October
  page_first: '180'
  page_last: '186'
  pages: "180\u2013186"
  paper_id: '18'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2018.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2018.jpg
  title: 'Towards JointUD: Part-of-speech Tagging and Lemmatization using Recurrent
    Neural Networks'
  title_html: 'Towards <span class="acl-fixed-case">J</span>oint<span class="acl-fixed-case">UD</span>:
    Part-of-speech Tagging and Lemmatization using Recurrent Neural Networks'
  url: https://www.aclweb.org/anthology/K18-2018
  year: '2018'
K18-2019:
  abstract: This is a system description paper for the CUNI x-ling submission to the
    CoNLL 2018 UD Shared Task. We focused on parsing under-resourced languages, with
    no or little training data available. We employed a wide range of approaches,
    including simple word-based treebank translation, combination of delexicalized
    parsers, and exploitation of available morphological dictionaries, with a dedicated
    setup tailored to each of the languages. In the official evaluation, our submission
    was identified as the clear winner of the Low-resource languages category.
  address: Brussels, Belgium
  author:
  - first: Rudolf
    full: Rudolf Rosa
    id: rudolf-rosa
    last: Rosa
  - first: David
    full: "David Mare\u010Dek"
    id: david-marecek
    last: "Mare\u010Dek"
  author_string: "Rudolf Rosa, David Mare\u010Dek"
  bibkey: rosa-marecek-2018-cuni
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2019
  month: October
  page_first: '187'
  page_last: '196'
  pages: "187\u2013196"
  paper_id: '19'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2019.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2019.jpg
  title: 'CUNI x-ling: Parsing Under-Resourced Languages in CoNLL 2018 UD Shared Task'
  title_html: '<span class="acl-fixed-case">CUNI</span> x-ling: Parsing Under-Resourced
    Languages in <span class="acl-fixed-case">C</span>o<span class="acl-fixed-case">NLL</span>
    2018 <span class="acl-fixed-case">UD</span> Shared Task'
  url: https://www.aclweb.org/anthology/K18-2019
  year: '2018'
K18-2020:
  abstract: 'UDPipe is a trainable pipeline which performs sentence segmentation,
    tokenization, POS tagging, lemmatization and dependency parsing. We present a
    prototype for UDPipe 2.0 and evaluate it in the CoNLL 2018 UD Shared Task: Multilingual
    Parsing from Raw Text to Universal Dependencies, which employs three metrics for
    submission ranking. Out of 26 participants, the prototype placed first in the
    MLAS ranking, third in the LAS ranking and third in the BLEX ranking. In extrinsic
    parser evaluation EPE 2018, the system ranked first in the overall score.'
  address: Brussels, Belgium
  author:
  - first: Milan
    full: Milan Straka
    id: milan-straka
    last: Straka
  author_string: Milan Straka
  bibkey: straka-2018-udpipe
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2020
  month: October
  page_first: '197'
  page_last: '207'
  pages: "197\u2013207"
  paper_id: '20'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2020.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2020.jpg
  title: UDPipe 2.0 Prototype at CoNLL 2018 UD Shared Task
  title_html: <span class="acl-fixed-case">UDP</span>ipe 2.0 Prototype at <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 <span class="acl-fixed-case">UD</span>
    Shared Task
  url: https://www.aclweb.org/anthology/K18-2020
  year: '2018'
K18-2021:
  abstract: "We present the contribution of the ONLP lab at the Open University of\
    \ Israel to the UD shared task on multilingual parsing from raw text to Universal\
    \ Dependencies. Our contribution is based on a transition-based parser called\
    \ \u2018yap \u2013 yet another parser\u2019, which includes a standalone morphological\
    \ model, a standalone dependency model, and a joint morphosyntactic model. In\
    \ the task we used yap\u2018s standalone dependency parser to parse input morphologically\
    \ disambiguated by UDPipe, and obtained the official score of 58.35 LAS. In our\
    \ follow up investigation we use yap to show how the incorporation of morphological\
    \ and lexical resources may improve the performance of end-to-end raw-to-dependencies\
    \ parsing in the case of a morphologically-rich and low-resource language, Modern\
    \ Hebrew. Our results on Hebrew underscore the importance of CoNLL-UL, a UD-compatible\
    \ standard for accessing external lexical resources, for enhancing end-to-end\
    \ UD parsing, in particular for morphologically rich and low-resource languages.\
    \ We thus encourage the community to create, convert, or make available more such\
    \ lexica in future tasks."
  address: Brussels, Belgium
  author:
  - first: Amit
    full: Amit Seker
    id: amit-seker
    last: Seker
  - first: Amir
    full: Amir More
    id: amir-more
    last: More
  - first: Reut
    full: Reut Tsarfaty
    id: reut-tsarfaty
    last: Tsarfaty
  author_string: Amit Seker, Amir More, Reut Tsarfaty
  bibkey: seker-etal-2018-universal
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2021
  month: October
  page_first: '208'
  page_last: '215'
  pages: "208\u2013215"
  paper_id: '21'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2021.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2021.jpg
  title: 'Universal Morpho-Syntactic Parsing and the Contribution of Lexica: Analyzing
    the ONLP Lab Submission to the CoNLL 2018 Shared Task'
  title_html: 'Universal Morpho-Syntactic Parsing and the Contribution of Lexica:
    Analyzing the <span class="acl-fixed-case">ONLP</span> Lab Submission to the <span
    class="acl-fixed-case">C</span>o<span class="acl-fixed-case">NLL</span> 2018 Shared
    Task'
  url: https://www.aclweb.org/anthology/K18-2021
  year: '2018'
K18-2022:
  abstract: 'We present SParse, our Graph-Based Parsing model submitted for the CoNLL
    2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies
    (Zeman et al., 2018). Our model extends the state-of-the-art biaffine parser (Dozat
    and Manning, 2016) with a structural meta-learning module, SMeta, that combines
    local and global label predictions. Our parser has been trained and run on Universal
    Dependencies datasets (Nivre et al., 2016, 2018) and has 87.48% LAS, 78.63% MLAS,
    78.69% BLEX and 81.76% CLAS (Nivre and Fang, 2017) score on the Italian-ISDT dataset
    and has 72.78% LAS, 59.10% MLAS, 61.38% BLEX and 61.72% CLAS score on the Japanese-GSD
    dataset in our official submission. All other corpora are evaluated after the
    submission deadline, for whom we present our unofficial test results.'
  address: Brussels, Belgium
  author:
  - first: Berkay
    full: "Berkay \xD6nder"
    id: berkay-furkan-onder
    last: "\xD6nder"
  - first: Can
    full: "Can G\xFCmeli"
    id: can-gumeli
    last: "G\xFCmeli"
  - first: Deniz
    full: Deniz Yuret
    id: deniz-yuret
    last: Yuret
  author_string: "Berkay \xD6nder, Can G\xFCmeli, Deniz Yuret"
  bibkey: onder-etal-2018-sparse
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2022
  month: October
  page_first: '216'
  page_last: '222'
  pages: "216\u2013222"
  paper_id: '22'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2022.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2022.jpg
  title: "SParse: Ko\xE7 University Graph-Based Parsing System for the CoNLL 2018\
    \ Shared Task"
  title_html: "<span class=\"acl-fixed-case\">SP</span>arse: <span class=\"acl-fixed-case\"\
    >K</span>o\xE7 University Graph-Based Parsing System for the <span class=\"acl-fixed-case\"\
    >C</span>o<span class=\"acl-fixed-case\">NLL</span> 2018 Shared Task"
  url: https://www.aclweb.org/anthology/K18-2022
  year: '2018'
K18-2023:
  abstract: "In this paper, we present the details of the neural dependency parser\
    \ and the neural tagger submitted by our team \u2018ParisNLP\u2019 to the CoNLL\
    \ 2018 Shared Task on parsing from raw text to Universal Dependencies. We augment\
    \ the deep Biaffine (BiAF) parser (Dozat and Manning, 2016) with novel features\
    \ to perform competitively: we utilize an indomain version of ELMo features (Peters\
    \ et al., 2018) which provide context-dependent word representations; we utilize\
    \ disambiguated, embedded, morphosyntactic features from lexicons (Sagot, 2018),\
    \ which complements the existing feature set. Henceforth, we call our system \u2018\
    ELMoLex\u2019. In addition to incorporating character embeddings, ELMoLex benefits\
    \ from pre-trained word vectors, ELMo and morphosyntactic features (whenever available)\
    \ to correctly handle rare or unknown words which are prevalent in languages with\
    \ complex morphology. ELMoLex ranked 11th by Labeled Attachment Score metric (70.64%),\
    \ Morphology-aware LAS metric (55.74%) and ranked 9th by Bilexical dependency\
    \ metric (60.70%)."
  address: Brussels, Belgium
  author:
  - first: Ganesh
    full: Ganesh Jawahar
    id: ganesh-jawahar
    last: Jawahar
  - first: Benjamin
    full: Benjamin Muller
    id: benjamin-muller
    last: Muller
  - first: Amal
    full: Amal Fethi
    id: amal-fethi
    last: Fethi
  - first: Louis
    full: Louis Martin
    id: louis-martin
    last: Martin
  - first: "\xC9ric"
    full: "\xC9ric Villemonte de la Clergerie"
    id: eric-villemonte-de-la-clergerie
    last: Villemonte de la Clergerie
  - first: "Beno\xEEt"
    full: "Beno\xEEt Sagot"
    id: benoit-sagot
    last: Sagot
  - first: "Djam\xE9"
    full: "Djam\xE9 Seddah"
    id: djame-seddah
    last: Seddah
  author_string: "Ganesh Jawahar, Benjamin Muller, Amal Fethi, Louis Martin, \xC9\
    ric Villemonte de la Clergerie, Beno\xEEt Sagot, Djam\xE9 Seddah"
  bibkey: jawahar-etal-2018-elmolex
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2023
  month: October
  page_first: '223'
  page_last: '237'
  pages: "223\u2013237"
  paper_id: '23'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2023.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2023.jpg
  title: 'ELMoLex: Connecting ELMo and Lexicon Features for Dependency Parsing'
  title_html: '<span class="acl-fixed-case">ELM</span>o<span class="acl-fixed-case">L</span>ex:
    Connecting <span class="acl-fixed-case">ELM</span>o and Lexicon Features for Dependency
    Parsing'
  url: https://www.aclweb.org/anthology/K18-2023
  year: '2018'
K18-2024:
  abstract: We propose two word representation models for agglutinative languages
    that better capture the similarities between words which have similar tasks in
    sentences. Our models highlight the morphological features in words and embed
    morphological information into their dense representations. We have tested our
    models on an LSTM-based dependency parser with character-based word embeddings
    proposed by Ballesteros et al. (2015). We participated in the CoNLL 2018 Shared
    Task on multilingual parsing from raw text to universal dependencies as the BOUN
    team. We show that our morphology-based embedding models improve the parsing performance
    for most of the agglutinative languages.
  address: Brussels, Belgium
  author:
  - first: "\u015Eaziye Bet\xFCl"
    full: "\u015Eaziye Bet\xFCl \xD6zate\u015F"
    id: saziye-betul-ozates
    last: "\xD6zate\u015F"
  - first: Arzucan
    full: "Arzucan \xD6zg\xFCr"
    id: arzucan-ozgur
    last: "\xD6zg\xFCr"
  - first: Tunga
    full: "Tunga G\xFCng\xF6r"
    id: tunga-gungor
    last: "G\xFCng\xF6r"
  - first: "Balk\u0131z"
    full: "Balk\u0131z \xD6zt\xFCrk"
    id: balkiz-ozturk
    last: "\xD6zt\xFCrk"
  author_string: "\u015Eaziye Bet\xFCl \xD6zate\u015F, Arzucan \xD6zg\xFCr, Tunga\
    \ G\xFCng\xF6r, Balk\u0131z \xD6zt\xFCrk"
  bibkey: ozates-etal-2018-morphology
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2024
  month: October
  page_first: '238'
  page_last: '247'
  pages: "238\u2013247"
  paper_id: '24'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2024.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2024.jpg
  title: A Morphology-Based Representation Model for LSTM-Based Dependency Parsing
    of Agglutinative Languages
  title_html: A Morphology-Based Representation Model for <span class="acl-fixed-case">LSTM</span>-Based
    Dependency Parsing of Agglutinative Languages
  url: https://www.aclweb.org/anthology/K18-2024
  year: '2018'
K18-2025:
  abstract: We describe the graph-based dependency parser in our system (AntNLP) submitted
    to the CoNLL 2018 UD Shared Task. We use bidirectional lstm to get the word representation,
    then a bi-affine pointer networks to compute scores of candidate dependency edges
    and the MST algorithm to get the final dependency tree. From the official testing
    results, our system gets 70.90 LAS F1 score (rank 9/26), 55.92 MLAS (10/26) and
    60.91 BLEX (8/26).
  address: Brussels, Belgium
  author:
  - first: Tao
    full: Tao Ji
    id: tao-ji
    last: Ji
  - first: Yufang
    full: Yufang Liu
    id: yufang-liu
    last: Liu
  - first: Yijun
    full: Yijun Wang
    id: yijun-wang
    last: Wang
  - first: Yuanbin
    full: Yuanbin Wu
    id: yuanbin-wu
    last: Wu
  - first: Man
    full: Man Lan
    id: man-lan
    last: Lan
  author_string: Tao Ji, Yufang Liu, Yijun Wang, Yuanbin Wu, Man Lan
  bibkey: ji-etal-2018-antnlp
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2025
  month: October
  page_first: '248'
  page_last: '255'
  pages: "248\u2013255"
  paper_id: '25'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2025.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2025.jpg
  title: 'AntNLP at CoNLL 2018 Shared Task: A Graph-Based Parser for Universal Dependency
    Parsing'
  title_html: '<span class="acl-fixed-case">A</span>nt<span class="acl-fixed-case">NLP</span>
    at <span class="acl-fixed-case">C</span>o<span class="acl-fixed-case">NLL</span>
    2018 Shared Task: A Graph-Based Parser for Universal Dependency Parsing'
  url: https://www.aclweb.org/anthology/K18-2025
  year: '2018'
K18-2026:
  abstract: "This paper describes Fudan\u2019s submission to CoNLL 2018\u2019s shared\
    \ task Universal Dependency Parsing. We jointly train models when two languages\
    \ are similar according to linguistic typology and then ensemble the models using\
    \ a simple re-parse algorithm. We outperform the baseline method by 4.4% (2.1%)\
    \ on average on development (test) set in CoNLL 2018 UD Shared Task."
  address: Brussels, Belgium
  author:
  - first: Danlu
    full: Danlu Chen
    id: danlu-chen
    last: Chen
  - first: Mengxiao
    full: Mengxiao Lin
    id: mengxiao-lin
    last: Lin
  - first: Zhifeng
    full: Zhifeng Hu
    id: zhifeng-hu
    last: Hu
  - first: Xipeng
    full: Xipeng Qiu
    id: xipeng-qiu
    last: Qiu
  author_string: Danlu Chen, Mengxiao Lin, Zhifeng Hu, Xipeng Qiu
  bibkey: chen-etal-2018-simple
  bibtype: inproceedings
  booktitle: 'Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  booktitle_html: 'Proceedings of the <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span> 2018 Shared Task: Multilingual Parsing from
    Raw Text to Universal Dependencies'
  doi: 10.18653/v1/K18-2026
  month: October
  page_first: '256'
  page_last: '263'
  pages: "256\u2013263"
  paper_id: '26'
  parent_volume_id: K18-2
  pdf: https://www.aclweb.org/anthology/K18-2026.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-2026.jpg
  title: A Simple yet Effective Joint Training Method for Cross-Lingual Universal
    Dependency Parsing
  title_html: A Simple yet Effective Joint Training Method for Cross-Lingual Universal
    Dependency Parsing
  url: https://www.aclweb.org/anthology/K18-2026
  year: '2018'
K18-3000:
  address: Brussels
  author:
  - first: Mans
    full: Mans Hulden
    id: mans-hulden
    last: Hulden
  - first: Ryan
    full: Ryan Cotterell
    id: ryan-cotterell
    last: Cotterell
  author_string: Mans Hulden, Ryan Cotterell
  bibkey: conll-2018-conll-sigmorphon
  bibtype: proceedings
  booktitle: "Proceedings of the CoNLL\u2013SIGMORPHON 2018 Shared Task: Universal\
    \ Morphological Reinflection"
  month: October
  paper_id: '0'
  parent_volume_id: K18-3
  pdf: https://www.aclweb.org/anthology/K18-3000.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-3000.jpg
  title: "Proceedings of the CoNLL\u2013SIGMORPHON 2018 Shared Task: Universal Morphological\
    \ Reinflection"
  title_html: "Proceedings of the <span class=\"acl-fixed-case\">C</span>o<span class=\"\
    acl-fixed-case\">NLL</span>\u2013<span class=\"acl-fixed-case\">SIGMORPHON</span>\
    \ 2018 Shared Task: Universal Morphological Reinflection"
  url: https://www.aclweb.org/anthology/K18-3000
  year: '2018'
K18-3001:
  address: Brussels
  author:
  - first: Ryan
    full: Ryan Cotterell
    id: ryan-cotterell
    last: Cotterell
  - first: Christo
    full: Christo Kirov
    id: christo-kirov
    last: Kirov
  - first: John
    full: John Sylak-Glassman
    id: john-sylak-glassman
    last: Sylak-Glassman
  - first: "G\xE9raldine"
    full: "G\xE9raldine Walther"
    id: geraldine-walther
    last: Walther
  - first: Ekaterina
    full: Ekaterina Vylomova
    id: ekaterina-vylomova
    last: Vylomova
  - first: Arya D.
    full: Arya D. McCarthy
    id: arya-d-mccarthy
    last: McCarthy
  - first: Katharina
    full: Katharina Kann
    id: katharina-kann
    last: Kann
  - first: Sebastian
    full: Sebastian Mielke
    id: sebastian-j-mielke
    last: Mielke
  - first: Garrett
    full: Garrett Nicolai
    id: garrett-nicolai
    last: Nicolai
  - first: Miikka
    full: Miikka Silfverberg
    id: miikka-silfverberg
    last: Silfverberg
  - first: David
    full: David Yarowsky
    id: david-yarowsky
    last: Yarowsky
  - first: Jason
    full: Jason Eisner
    id: jason-eisner
    last: Eisner
  - first: Mans
    full: Mans Hulden
    id: mans-hulden
    last: Hulden
  author_string: "Ryan Cotterell, Christo Kirov, John Sylak-Glassman, G\xE9raldine\
    \ Walther, Ekaterina Vylomova, Arya D. McCarthy, Katharina Kann, Sebastian Mielke,\
    \ Garrett Nicolai, Miikka Silfverberg, David Yarowsky, Jason Eisner, Mans Hulden"
  bibkey: cotterell-etal-2018-conll
  bibtype: inproceedings
  booktitle: "Proceedings of the CoNLL\u2013SIGMORPHON 2018 Shared Task: Universal\
    \ Morphological Reinflection"
  booktitle_html: "Proceedings of the <span class=\"acl-fixed-case\">C</span>o<span\
    \ class=\"acl-fixed-case\">NLL</span>\u2013<span class=\"acl-fixed-case\">SIGMORPHON</span>\
    \ 2018 Shared Task: Universal Morphological Reinflection"
  doi: 10.18653/v1/K18-3001
  month: October
  page_first: '1'
  page_last: '27'
  pages: "1\u201327"
  paper_id: '1'
  parent_volume_id: K18-3
  pdf: https://www.aclweb.org/anthology/K18-3001.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-3001.jpg
  title: "The CoNLL\u2013SIGMORPHON 2018 Shared Task: Universal Morphological Reinflection"
  title_html: "The <span class=\"acl-fixed-case\">C</span>o<span class=\"acl-fixed-case\"\
    >NLL</span>\u2013<span class=\"acl-fixed-case\">SIGMORPHON</span> 2018 Shared\
    \ Task: Universal Morphological Reinflection"
  url: https://www.aclweb.org/anthology/K18-3001
  year: '2018'
K18-3002:
  address: Brussels
  author:
  - first: Manex
    full: Manex Agirrezabal
    id: manex-agirrezabal
    last: Agirrezabal
  author_string: Manex Agirrezabal
  bibkey: agirrezabal-2018-ku
  bibtype: inproceedings
  booktitle: "Proceedings of the CoNLL\u2013SIGMORPHON 2018 Shared Task: Universal\
    \ Morphological Reinflection"
  booktitle_html: "Proceedings of the <span class=\"acl-fixed-case\">C</span>o<span\
    \ class=\"acl-fixed-case\">NLL</span>\u2013<span class=\"acl-fixed-case\">SIGMORPHON</span>\
    \ 2018 Shared Task: Universal Morphological Reinflection"
  doi: 10.18653/v1/K18-3002
  month: October
  page_first: '28'
  page_last: '32'
  pages: "28\u201332"
  paper_id: '2'
  parent_volume_id: K18-3
  pdf: https://www.aclweb.org/anthology/K18-3002.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-3002.jpg
  title: "KU-CST at CoNLL\u2013SIGMORPHON 2018 Shared Task: a Tridirectional Model"
  title_html: "<span class=\"acl-fixed-case\">KU</span>-<span class=\"acl-fixed-case\"\
    >CST</span> at <span class=\"acl-fixed-case\">C</span>o<span class=\"acl-fixed-case\"\
    >NLL</span>\u2013<span class=\"acl-fixed-case\">SIGMORPHON</span> 2018 Shared\
    \ Task: a Tridirectional Model"
  url: https://www.aclweb.org/anthology/K18-3002
  year: '2018'
K18-3003:
  address: Brussels
  author:
  - first: Rashel
    full: Rashel Fam
    id: rashel-fam
    last: Fam
  - first: Yves
    full: Yves Lepage
    id: yves-lepage
    last: Lepage
  author_string: Rashel Fam, Yves Lepage
  bibkey: fam-lepage-2018-ips
  bibtype: inproceedings
  booktitle: "Proceedings of the CoNLL\u2013SIGMORPHON 2018 Shared Task: Universal\
    \ Morphological Reinflection"
  booktitle_html: "Proceedings of the <span class=\"acl-fixed-case\">C</span>o<span\
    \ class=\"acl-fixed-case\">NLL</span>\u2013<span class=\"acl-fixed-case\">SIGMORPHON</span>\
    \ 2018 Shared Task: Universal Morphological Reinflection"
  doi: 10.18653/v1/K18-3003
  month: October
  page_first: '33'
  page_last: '42'
  pages: "33\u201342"
  paper_id: '3'
  parent_volume_id: K18-3
  pdf: https://www.aclweb.org/anthology/K18-3003.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-3003.jpg
  title: "IPS-WASEDA system at CoNLL\u2013SIGMORPHON 2018 Shared Task on morphological\
    \ inflection"
  title_html: "<span class=\"acl-fixed-case\">IPS</span>-<span class=\"acl-fixed-case\"\
    >WASEDA</span> system at <span class=\"acl-fixed-case\">C</span>o<span class=\"\
    acl-fixed-case\">NLL</span>\u2013<span class=\"acl-fixed-case\">SIGMORPHON</span>\
    \ 2018 Shared Task on morphological inflection"
  url: https://www.aclweb.org/anthology/K18-3003
  year: '2018'
K18-3004:
  address: Brussels
  author:
  - first: Andreas
    full: Andreas Madsack
    id: andreas-madsack
    last: Madsack
  - first: Alessia
    full: Alessia Cavallo
    id: alessia-cavallo
    last: Cavallo
  - first: Johanna
    full: Johanna Heininger
    id: johanna-heininger
    last: Heininger
  - first: Robert
    full: "Robert Wei\xDFgraeber"
    id: robert-weissgraeber
    last: "Wei\xDFgraeber"
  author_string: "Andreas Madsack, Alessia Cavallo, Johanna Heininger, Robert Wei\xDF\
    graeber"
  bibkey: madsack-etal-2018-ax
  bibtype: inproceedings
  booktitle: "Proceedings of the CoNLL\u2013SIGMORPHON 2018 Shared Task: Universal\
    \ Morphological Reinflection"
  booktitle_html: "Proceedings of the <span class=\"acl-fixed-case\">C</span>o<span\
    \ class=\"acl-fixed-case\">NLL</span>\u2013<span class=\"acl-fixed-case\">SIGMORPHON</span>\
    \ 2018 Shared Task: Universal Morphological Reinflection"
  doi: 10.18653/v1/K18-3004
  month: October
  page_first: '43'
  page_last: '47'
  pages: "43\u201347"
  paper_id: '4'
  parent_volume_id: K18-3
  pdf: https://www.aclweb.org/anthology/K18-3004.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-3004.jpg
  title: "AX Semantics\u2019 Submission to the CoNLL\u2013SIGMORPHON 2018 Shared Task"
  title_html: "<span class=\"acl-fixed-case\">AX</span> Semantics\u2019 Submission\
    \ to the <span class=\"acl-fixed-case\">C</span>o<span class=\"acl-fixed-case\"\
    >NLL</span>\u2013<span class=\"acl-fixed-case\">SIGMORPHON</span> 2018 Shared\
    \ Task"
  url: https://www.aclweb.org/anthology/K18-3004
  year: '2018'
K18-3005:
  address: Brussels
  author:
  - first: Rishabh
    full: Rishabh Jain
    id: rishabh-jain
    last: Jain
  - first: Anil Kumar
    full: Anil Kumar Singh
    id: anil-kumar-singh
    last: Singh
  author_string: Rishabh Jain, Anil Kumar Singh
  bibkey: jain-singh-2018-experiments
  bibtype: inproceedings
  booktitle: "Proceedings of the CoNLL\u2013SIGMORPHON 2018 Shared Task: Universal\
    \ Morphological Reinflection"
  booktitle_html: "Proceedings of the <span class=\"acl-fixed-case\">C</span>o<span\
    \ class=\"acl-fixed-case\">NLL</span>\u2013<span class=\"acl-fixed-case\">SIGMORPHON</span>\
    \ 2018 Shared Task: Universal Morphological Reinflection"
  doi: 10.18653/v1/K18-3005
  month: October
  page_first: '48'
  page_last: '57'
  pages: "48\u201357"
  paper_id: '5'
  parent_volume_id: K18-3
  pdf: https://www.aclweb.org/anthology/K18-3005.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-3005.jpg
  title: 'Experiments on Morphological Reinflection: CoNLL-2018 Shared Task'
  title_html: 'Experiments on Morphological Reinflection: <span class="acl-fixed-case">C</span>o<span
    class="acl-fixed-case">NLL</span>-2018 Shared Task'
  url: https://www.aclweb.org/anthology/K18-3005
  year: '2018'
K18-3006:
  address: Brussels
  author:
  - first: Katharina
    full: Katharina Kann
    id: katharina-kann
    last: Kann
  - first: Stanislas
    full: Stanislas Lauly
    id: stanislas-lauly
    last: Lauly
  - first: Kyunghyun
    full: Kyunghyun Cho
    id: kyunghyun-cho
    last: Cho
  author_string: Katharina Kann, Stanislas Lauly, Kyunghyun Cho
  bibkey: kann-etal-2018-nyu
  bibtype: inproceedings
  booktitle: "Proceedings of the CoNLL\u2013SIGMORPHON 2018 Shared Task: Universal\
    \ Morphological Reinflection"
  booktitle_html: "Proceedings of the <span class=\"acl-fixed-case\">C</span>o<span\
    \ class=\"acl-fixed-case\">NLL</span>\u2013<span class=\"acl-fixed-case\">SIGMORPHON</span>\
    \ 2018 Shared Task: Universal Morphological Reinflection"
  doi: 10.18653/v1/K18-3006
  month: October
  page_first: '58'
  page_last: '63'
  pages: "58\u201363"
  paper_id: '6'
  parent_volume_id: K18-3
  pdf: https://www.aclweb.org/anthology/K18-3006.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-3006.jpg
  title: "The NYU System for the CoNLL\u2013SIGMORPHON 2018 Shared Task on Universal\
    \ Morphological Reinflection"
  title_html: "The <span class=\"acl-fixed-case\">NYU</span> System for the <span\
    \ class=\"acl-fixed-case\">C</span>o<span class=\"acl-fixed-case\">NLL</span>\u2013\
    <span class=\"acl-fixed-case\">SIGMORPHON</span> 2018 Shared Task on Universal\
    \ Morphological Reinflection"
  url: https://www.aclweb.org/anthology/K18-3006
  year: '2018'
K18-3007:
  address: Brussels
  author:
  - first: Stefan Daniel
    full: Stefan Daniel Dumitrescu
    id: stefan-daniel-dumitrescu
    last: Dumitrescu
  - first: Tiberiu
    full: Tiberiu Boros
    id: tiberiu-boros
    last: Boros
  author_string: Stefan Daniel Dumitrescu, Tiberiu Boros
  bibkey: dumitrescu-boros-2018-attention
  bibtype: inproceedings
  booktitle: "Proceedings of the CoNLL\u2013SIGMORPHON 2018 Shared Task: Universal\
    \ Morphological Reinflection"
  booktitle_html: "Proceedings of the <span class=\"acl-fixed-case\">C</span>o<span\
    \ class=\"acl-fixed-case\">NLL</span>\u2013<span class=\"acl-fixed-case\">SIGMORPHON</span>\
    \ 2018 Shared Task: Universal Morphological Reinflection"
  doi: 10.18653/v1/K18-3007
  month: October
  page_first: '64'
  page_last: '68'
  pages: "64\u201368"
  paper_id: '7'
  parent_volume_id: K18-3
  pdf: https://www.aclweb.org/anthology/K18-3007.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-3007.jpg
  title: Attention-free encoder decoder for morphological processing
  title_html: Attention-free encoder decoder for morphological processing
  url: https://www.aclweb.org/anthology/K18-3007
  year: '2018'
K18-3008:
  address: Brussels
  author:
  - first: Peter
    full: Peter Makarov
    id: peter-makarov
    last: Makarov
  - first: Simon
    full: Simon Clematide
    id: simon-clematide
    last: Clematide
  author_string: Peter Makarov, Simon Clematide
  bibkey: makarov-clematide-2018-uzh
  bibtype: inproceedings
  booktitle: "Proceedings of the CoNLL\u2013SIGMORPHON 2018 Shared Task: Universal\
    \ Morphological Reinflection"
  booktitle_html: "Proceedings of the <span class=\"acl-fixed-case\">C</span>o<span\
    \ class=\"acl-fixed-case\">NLL</span>\u2013<span class=\"acl-fixed-case\">SIGMORPHON</span>\
    \ 2018 Shared Task: Universal Morphological Reinflection"
  doi: 10.18653/v1/K18-3008
  month: October
  page_first: '69'
  page_last: '75'
  pages: "69\u201375"
  paper_id: '8'
  parent_volume_id: K18-3
  pdf: https://www.aclweb.org/anthology/K18-3008.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-3008.jpg
  title: "UZH at CoNLL\u2013SIGMORPHON 2018 Shared Task on Universal Morphological\
    \ Reinflection"
  title_html: "<span class=\"acl-fixed-case\">UZH</span> at <span class=\"acl-fixed-case\"\
    >C</span>o<span class=\"acl-fixed-case\">NLL</span>\u2013<span class=\"acl-fixed-case\"\
    >SIGMORPHON</span> 2018 Shared Task on Universal Morphological Reinflection"
  url: https://www.aclweb.org/anthology/K18-3008
  year: '2018'
K18-3009:
  address: Brussels
  author:
  - first: Fynn
    full: "Fynn Schr\xF6der"
    id: fynn-schroder
    last: "Schr\xF6der"
  - first: Marcel
    full: Marcel Kamlot
    id: marcel-kamlot
    last: Kamlot
  - first: Gregor
    full: Gregor Billing
    id: gregor-billing
    last: Billing
  - first: Arne
    full: "Arne K\xF6hn"
    id: arne-kohn
    last: "K\xF6hn"
  author_string: "Fynn Schr\xF6der, Marcel Kamlot, Gregor Billing, Arne K\xF6hn"
  bibkey: schroder-etal-2018-finding
  bibtype: inproceedings
  booktitle: "Proceedings of the CoNLL\u2013SIGMORPHON 2018 Shared Task: Universal\
    \ Morphological Reinflection"
  booktitle_html: "Proceedings of the <span class=\"acl-fixed-case\">C</span>o<span\
    \ class=\"acl-fixed-case\">NLL</span>\u2013<span class=\"acl-fixed-case\">SIGMORPHON</span>\
    \ 2018 Shared Task: Universal Morphological Reinflection"
  doi: 10.18653/v1/K18-3009
  month: October
  page_first: '76'
  page_last: '85'
  pages: "76\u201385"
  paper_id: '9'
  parent_volume_id: K18-3
  pdf: https://www.aclweb.org/anthology/K18-3009.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-3009.jpg
  title: "Finding the way from \xE4 to a: Sub-character morphological inflection for\
    \ the SIGMORPHON 2018 shared task"
  title_html: "Finding the way from \xE4 to a: Sub-character morphological inflection\
    \ for the <span class=\"acl-fixed-case\">SIGMORPHON</span> 2018 shared task"
  url: https://www.aclweb.org/anthology/K18-3009
  year: '2018'
K18-3010:
  address: Brussels
  author:
  - first: Ling
    full: Ling Liu
    id: ling-liu
    last: Liu
  - first: Ilamvazhuthy
    full: Ilamvazhuthy Subbiah
    id: ilamvazhuthy-subbiah
    last: Subbiah
  - first: Adam
    full: Adam Wiemerslage
    id: adam-wiemerslage
    last: Wiemerslage
  - first: Jonathan
    full: Jonathan Lilley
    id: jonathan-lilley
    last: Lilley
  - first: Sarah
    full: Sarah Moeller
    id: sarah-moeller
    last: Moeller
  author_string: Ling Liu, Ilamvazhuthy Subbiah, Adam Wiemerslage, Jonathan Lilley,
    Sarah Moeller
  bibkey: liu-etal-2018-morphological
  bibtype: inproceedings
  booktitle: "Proceedings of the CoNLL\u2013SIGMORPHON 2018 Shared Task: Universal\
    \ Morphological Reinflection"
  booktitle_html: "Proceedings of the <span class=\"acl-fixed-case\">C</span>o<span\
    \ class=\"acl-fixed-case\">NLL</span>\u2013<span class=\"acl-fixed-case\">SIGMORPHON</span>\
    \ 2018 Shared Task: Universal Morphological Reinflection"
  doi: 10.18653/v1/K18-3010
  month: October
  page_first: '86'
  page_last: '92'
  pages: "86\u201392"
  paper_id: '10'
  parent_volume_id: K18-3
  pdf: https://www.aclweb.org/anthology/K18-3010.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-3010.jpg
  title: "Morphological Reinflection in Context: CU Boulder\u2019s Submission to CoNLL\u2013\
    SIGMORPHON 2018 Shared Task"
  title_html: "Morphological Reinflection in Context: <span class=\"acl-fixed-case\"\
    >CU</span> Boulder\u2019s Submission to <span class=\"acl-fixed-case\">C</span>o<span\
    \ class=\"acl-fixed-case\">NLL</span>\u2013<span class=\"acl-fixed-case\">SIGMORPHON</span>\
    \ 2018 Shared Task"
  url: https://www.aclweb.org/anthology/K18-3010
  year: '2018'
K18-3011:
  address: Brussels
  author:
  - first: Yova
    full: Yova Kementchedjhieva
    id: yova-kementchedjhieva
    last: Kementchedjhieva
  - first: Johannes
    full: Johannes Bjerva
    id: johannes-bjerva
    last: Bjerva
  - first: Isabelle
    full: Isabelle Augenstein
    id: isabelle-augenstein
    last: Augenstein
  author_string: Yova Kementchedjhieva, Johannes Bjerva, Isabelle Augenstein
  bibkey: kementchedjhieva-etal-2018-copenhagen
  bibtype: inproceedings
  booktitle: "Proceedings of the CoNLL\u2013SIGMORPHON 2018 Shared Task: Universal\
    \ Morphological Reinflection"
  booktitle_html: "Proceedings of the <span class=\"acl-fixed-case\">C</span>o<span\
    \ class=\"acl-fixed-case\">NLL</span>\u2013<span class=\"acl-fixed-case\">SIGMORPHON</span>\
    \ 2018 Shared Task: Universal Morphological Reinflection"
  doi: 10.18653/v1/K18-3011
  month: October
  page_first: '93'
  page_last: '98'
  pages: "93\u201398"
  paper_id: '11'
  parent_volume_id: K18-3
  pdf: https://www.aclweb.org/anthology/K18-3011.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-3011.jpg
  title: "Copenhagen at CoNLL\u2013SIGMORPHON 2018: Multilingual Inflection in Context\
    \ with Explicit Morphosyntactic Decoding"
  title_html: "Copenhagen at <span class=\"acl-fixed-case\">C</span>o<span class=\"\
    acl-fixed-case\">NLL</span>\u2013<span class=\"acl-fixed-case\">SIGMORPHON</span>\
    \ 2018: Multilingual Inflection in Context with Explicit Morphosyntactic Decoding"
  url: https://www.aclweb.org/anthology/K18-3011
  year: '2018'
K18-3012:
  address: Brussels
  author:
  - first: Alexey
    full: Alexey Sorokin
    id: alexey-sorokin
    last: Sorokin
  author_string: Alexey Sorokin
  bibkey: sorokin-2018-gain
  bibtype: inproceedings
  booktitle: "Proceedings of the CoNLL\u2013SIGMORPHON 2018 Shared Task: Universal\
    \ Morphological Reinflection"
  booktitle_html: "Proceedings of the <span class=\"acl-fixed-case\">C</span>o<span\
    \ class=\"acl-fixed-case\">NLL</span>\u2013<span class=\"acl-fixed-case\">SIGMORPHON</span>\
    \ 2018 Shared Task: Universal Morphological Reinflection"
  doi: 10.18653/v1/K18-3012
  month: October
  page_first: '99'
  page_last: '104'
  pages: "99\u2013104"
  paper_id: '12'
  parent_volume_id: K18-3
  pdf: https://www.aclweb.org/anthology/K18-3012.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-3012.jpg
  title: What can we gain from language models for morphological inflection?
  title_html: What can we gain from language models for morphological inflection?
  url: https://www.aclweb.org/anthology/K18-3012
  year: '2018'
K18-3013:
  address: Brussels
  author:
  - first: Abhishek
    full: Abhishek Sharma
    id: abhishek-sharma
    last: Sharma
  - first: Ganesh
    full: Ganesh Katrapati
    id: ganesh-katrapati
    last: Katrapati
  - first: Dipti Misra
    full: Dipti Misra Sharma
    id: dipti-misra-sharma
    last: Sharma
  author_string: Abhishek Sharma, Ganesh Katrapati, Dipti Misra Sharma
  bibkey: sharma-etal-2018-iit
  bibtype: inproceedings
  booktitle: "Proceedings of the CoNLL\u2013SIGMORPHON 2018 Shared Task: Universal\
    \ Morphological Reinflection"
  booktitle_html: "Proceedings of the <span class=\"acl-fixed-case\">C</span>o<span\
    \ class=\"acl-fixed-case\">NLL</span>\u2013<span class=\"acl-fixed-case\">SIGMORPHON</span>\
    \ 2018 Shared Task: Universal Morphological Reinflection"
  doi: 10.18653/v1/K18-3013
  month: October
  page_first: '105'
  page_last: '111'
  pages: "105\u2013111"
  paper_id: '13'
  parent_volume_id: K18-3
  pdf: https://www.aclweb.org/anthology/K18-3013.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-3013.jpg
  title: "IIT(BHU)\u2013IIITH at CoNLL\u2013SIGMORPHON 2018 Shared Task on Universal\
    \ Morphological Reinflection"
  title_html: "<span class=\"acl-fixed-case\">IIT</span>(<span class=\"acl-fixed-case\"\
    >BHU</span>)\u2013<span class=\"acl-fixed-case\">IIITH</span> at <span class=\"\
    acl-fixed-case\">C</span>o<span class=\"acl-fixed-case\">NLL</span>\u2013<span\
    \ class=\"acl-fixed-case\">SIGMORPHON</span> 2018 Shared Task on Universal Morphological\
    \ Reinflection"
  url: https://www.aclweb.org/anthology/K18-3013
  year: '2018'
K18-3014:
  address: Brussels
  author:
  - first: Taraka
    full: Taraka Rama
    id: taraka-rama
    last: Rama
  - first: "\xC7a\u011Fr\u0131"
    full: "\xC7a\u011Fr\u0131 \xC7\xF6ltekin"
    id: cagri-coltekin
    last: "\xC7\xF6ltekin"
  author_string: "Taraka Rama, \xC7a\u011Fr\u0131 \xC7\xF6ltekin"
  bibkey: rama-coltekin-2018-tubingen
  bibtype: inproceedings
  booktitle: "Proceedings of the CoNLL\u2013SIGMORPHON 2018 Shared Task: Universal\
    \ Morphological Reinflection"
  booktitle_html: "Proceedings of the <span class=\"acl-fixed-case\">C</span>o<span\
    \ class=\"acl-fixed-case\">NLL</span>\u2013<span class=\"acl-fixed-case\">SIGMORPHON</span>\
    \ 2018 Shared Task: Universal Morphological Reinflection"
  doi: 10.18653/v1/K18-3014
  month: October
  page_first: '112'
  page_last: '115'
  pages: "112\u2013115"
  paper_id: '14'
  parent_volume_id: K18-3
  pdf: https://www.aclweb.org/anthology/K18-3014.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-3014.jpg
  title: "T\xFCbingen-Oslo system at SIGMORPHON shared task on morphological inflection.\
    \ A multi-tasking multilingual sequence to sequence model."
  title_html: "T\xFCbingen-Oslo system at <span class=\"acl-fixed-case\">SIGMORPHON</span>\
    \ shared task on morphological inflection. A multi-tasking multilingual sequence\
    \ to sequence model."
  url: https://www.aclweb.org/anthology/K18-3014
  year: '2018'
K18-3015:
  address: Brussels
  author:
  - first: Saeed
    full: Saeed Najafi
    id: saeed-najafi
    last: Najafi
  - first: Bradley
    full: Bradley Hauer
    id: bradley-hauer
    last: Hauer
  - first: Rashed Rubby
    full: Rashed Rubby Riyadh
    id: rashed-rubby-riyadh
    last: Riyadh
  - first: Leyuan
    full: Leyuan Yu
    id: leyuan-yu
    last: Yu
  - first: Grzegorz
    full: Grzegorz Kondrak
    id: grzegorz-kondrak
    last: Kondrak
  author_string: Saeed Najafi, Bradley Hauer, Rashed Rubby Riyadh, Leyuan Yu, Grzegorz
    Kondrak
  bibkey: najafi-etal-2018-combining
  bibtype: inproceedings
  booktitle: "Proceedings of the CoNLL\u2013SIGMORPHON 2018 Shared Task: Universal\
    \ Morphological Reinflection"
  booktitle_html: "Proceedings of the <span class=\"acl-fixed-case\">C</span>o<span\
    \ class=\"acl-fixed-case\">NLL</span>\u2013<span class=\"acl-fixed-case\">SIGMORPHON</span>\
    \ 2018 Shared Task: Universal Morphological Reinflection"
  doi: 10.18653/v1/K18-3015
  month: October
  page_first: '116'
  page_last: '120'
  pages: "116\u2013120"
  paper_id: '15'
  parent_volume_id: K18-3
  pdf: https://www.aclweb.org/anthology/K18-3015.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-3015.jpg
  title: Combining Neural and Non-Neural Methods for Low-Resource Morphological Reinflection
  title_html: Combining Neural and Non-Neural Methods for Low-Resource Morphological
    Reinflection
  url: https://www.aclweb.org/anthology/K18-3015
  year: '2018'
K18-3016:
  address: Brussels
  author:
  - first: Judit
    full: "Judit \xC1cs"
    id: judit-acs
    last: "\xC1cs"
  author_string: "Judit \xC1cs"
  bibkey: acs-2018-bme
  bibtype: inproceedings
  booktitle: "Proceedings of the CoNLL\u2013SIGMORPHON 2018 Shared Task: Universal\
    \ Morphological Reinflection"
  booktitle_html: "Proceedings of the <span class=\"acl-fixed-case\">C</span>o<span\
    \ class=\"acl-fixed-case\">NLL</span>\u2013<span class=\"acl-fixed-case\">SIGMORPHON</span>\
    \ 2018 Shared Task: Universal Morphological Reinflection"
  doi: 10.18653/v1/K18-3016
  month: October
  page_first: '121'
  page_last: '126'
  pages: "121\u2013126"
  paper_id: '16'
  parent_volume_id: K18-3
  pdf: https://www.aclweb.org/anthology/K18-3016.pdf
  publisher: Association for Computational Linguistics
  thumbnail: https://www.aclweb.org/anthology/thumb/K18-3016.jpg
  title: "BME-HAS System for CoNLL\u2013SIGMORPHON 2018 Shared Task: Universal Morphological\
    \ Reinflection"
  title_html: "<span class=\"acl-fixed-case\">BME</span>-<span class=\"acl-fixed-case\"\
    >HAS</span> System for <span class=\"acl-fixed-case\">C</span>o<span class=\"\
    acl-fixed-case\">NLL</span>\u2013<span class=\"acl-fixed-case\">SIGMORPHON</span>\
    \ 2018 Shared Task: Universal Morphological Reinflection"
  url: https://www.aclweb.org/anthology/K18-3016
  year: '2018'
